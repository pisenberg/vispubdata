{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5875d0f0-849f-4889-bf31-f94db4ff00e4",
   "metadata": {},
   "source": [
    "# TVCG and CG&A Papers presented at VIS\n",
    "\n",
    "Here we create a new sheet on vispubdata that contains articles accepted directly to the IEEE TVCG journal and the IEEE CG&A magazine. These articles were presented at VIS but were not reviewed at VIS. Instead, they were reviewed by the regular journal/magazine review process.\n",
    "\n",
    "The list of journal papers is kept up-to-date by Tobias Isenberg in this location: https://docs.google.com/spreadsheets/d/1I6n4a6xvmoanAIDiSsGlaOVljAJ5IkT2C_naI-dStNo/\n",
    "\n",
    "If you have new journal articles you would like to add that are not yet in this spreadsheet then, ideally, contact [Tobias](https://tobias.isenberg.cc/) and get him to do it. Only if this does not work or you are in a rush, copy the spreadsheet, add the dois of the new articles, then change the url below to your new spreadsheet (or read from a csv file).\n",
    "\n",
    "Run this code after you extracted data from the DBLP (see its respective subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b7e32-19a6-4003-98a9-b29797b3718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First load what we need to load\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request, json \n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from crossref.restful import Works, Etiquette\n",
    "\n",
    "with open('ieeexplore-apikey.txt') as f:\n",
    "    apikey = f.readline()\n",
    "    \n",
    "youremail = \"petra.isenberg@inria.fr\" #replace this with your own email address. This is required for querying CrossRef\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90710cd-b6d1-4698-bd5c-ed5866f2d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_sheet_url = \"https://docs.google.com/spreadsheets/d/1I6n4a6xvmoanAIDiSsGlaOVljAJ5IkT2C_naI-dStNo/gviz/tq?tqx=out:csv\"\n",
    "journals_df = pd.read_csv(journals_sheet_url, keep_default_na=False)\n",
    "journals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e37d9b-fb55-4bee-a7f4-3715f5f28ecc",
   "metadata": {},
   "source": [
    "## Download data from IEEEXplore\n",
    "\n",
    "Make sure you have an API key first (and loaded above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331eeb8-2729-462a-a38c-8c0c19213a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl =  \"https://ieeexploreapi.ieee.org/api/v1/search/articles?parameter&apikey=\"+apikey+\"&max_records=200&doi=\"\n",
    "\n",
    "#https://ieeexploreapi.ieee.org/api/v1/search/articles?querytext=(rfid%20OR%20%22internet%20of%20things%22)&apikey="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c6cfa7-1f4d-40c9-a720-0eb0962b484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to generate a search string with 200 DOIs at a time\n",
    "\n",
    "# Function to concatenate DOIs in blocks of 200 with \"%20OR%20%\". \n",
    "def concatenate_dois(dois, block_size=200, separator=\"%20OR%20%\"):\n",
    "    concatenated_list = []\n",
    "    for i in range(0, len(dois), block_size):\n",
    "        block = dois[i:i + block_size]\n",
    "        concatenated_string = separator.join(block)\n",
    "        concatenated_list.append(concatenated_string)\n",
    "    return concatenated_list\n",
    "\n",
    "# Concatenate the DOIs\n",
    "concatenated_dois = concatenate_dois(journals_df[\"doi\"].tolist())\n",
    "\n",
    "# ask IEEEXplore and then save the results\n",
    "# we have to do this blocking because there is a rate limit that is lower than the number of articles we have\n",
    "\n",
    "i = 0\n",
    "for block in concatenated_dois:\n",
    "    fullurl = baseurl + block\n",
    "    full_filename = \"journal-data/original-data/ieeexplore-article-data-\"+str(i)+\".json\"\n",
    "    i = i + 1\n",
    "    with open(full_filename, 'w+', encoding=\"utf-8\") as f:\n",
    "            resp = requests.get(fullurl, verify=True)\n",
    "            f.write(resp.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626150f2-a333-4c88-b19a-a0e504e4df61",
   "metadata": {},
   "source": [
    "## Convert the data to vispubdata format\n",
    "\n",
    "Similar to how it's done for the vis papers ...\n",
    "Here we convert the .json file into something more flat and similar to vispubdata. Run this code if you updated the .json files above.\n",
    "\n",
    "This code comes from here:\n",
    "https://github.com/vinay20045/json-to-csv/blob/master/json_to_csv.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a517d-53cc-472d-81ef-eb2556659c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Convert to string keeping encoding in mind...\n",
    "##\n",
    "def to_string(s):\n",
    "    \n",
    "    try:\n",
    "        return str(s)\n",
    "    except:\n",
    "        #Change the encoding type if needed\n",
    "        return s.encode('utf-8')\n",
    "\n",
    "    \n",
    "def reduce_item(key, value):\n",
    "    global reduced_item\n",
    "    \n",
    "    #Reduction Condition 1\n",
    "    if type(value) is list:\n",
    "        i=0\n",
    "        for sub_item in value:\n",
    "            reduce_item(key+'_'+to_string(i), sub_item)\n",
    "            i=i+1\n",
    "\n",
    "    #Reduction Condition 2\n",
    "    elif type(value) is dict:\n",
    "        sub_keys = value.keys()\n",
    "        for sub_key in sub_keys:\n",
    "            reduce_item(key+'_'+to_string(sub_key), value[sub_key])\n",
    "    \n",
    "    #Base Condition\n",
    "    else:\n",
    "        reduced_item[to_string(key)] = to_string(value)\n",
    "\n",
    "folderdir = 'journal-data/original-data'\n",
    "generated_data_path = 'journal-data/generated-data'\n",
    " \n",
    "# giving file extension\n",
    "ext = ('.json')\n",
    "\n",
    "\n",
    " \n",
    "# iterating over all files\n",
    "# iterating over directory and subdirectory to get desired result\n",
    "for path, dirc, files in os.walk(folderdir):\n",
    "    for name in files:\n",
    "        if name.endswith(ext):\n",
    "            csv_file_path = generated_data_path+\"/\"+name+\".csv\"\n",
    "            node = \"articles\"\n",
    "        \n",
    "            pathExists = os.path.exists(generated_data_path)\n",
    "            if not pathExists:\n",
    "                os.makedirs(generated_data_path)\n",
    "        \n",
    "            json_file_path = path + \"/\" + name\n",
    "            fp = open(json_file_path, mode = 'rb')\n",
    "            json_value = fp.read()\n",
    "            raw_data = json.loads(json_value)        \n",
    "        \n",
    "            print(json_file_path)  # printing file name of desired extension\n",
    "            \n",
    "            try:\n",
    "                data_to_be_processed = raw_data[node]\n",
    "            except:\n",
    "                data_to_be_processed = raw_data\n",
    "\n",
    "            processed_data = []\n",
    "            header = []\n",
    "            for item in data_to_be_processed:\n",
    "                reduced_item = {}\n",
    "                reduce_item(node, item)\n",
    "\n",
    "                header += reduced_item.keys()\n",
    "\n",
    "                processed_data.append(reduced_item)\n",
    "\n",
    "            header = list(set(header))\n",
    "            header.sort()\n",
    "\n",
    "            with open(csv_file_path, 'w+',encoding=\"utf8\") as f:\n",
    "                writer = csv.DictWriter(f, header, dialect='excel')\n",
    "                writer.writeheader()\n",
    "                for row in processed_data:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "        #print (\"Just completed writing \"+csv_file_path+\" file with %d columns\" % len(header))\n",
    "\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421fc9f-cd82-4c60-8a58-b920604a1b5a",
   "metadata": {},
   "source": [
    "## Convert IEEEXplore csvs to Vispubdata\n",
    "\n",
    "Here we convert the data from the .csv files into the vispubdata format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0afbba-bd6a-4a3f-9184-8008182d3bd7",
   "metadata": {},
   "source": [
    "\n",
    "#### Read the latest vispubdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17457d54-87dd-4c5a-8ae0-8a11044aaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Google Sheet URL\n",
    "vispubdata_sheet_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQhVtLqK8w-28-V8tr6xYkNAWubNrXQ1KuWtQKOkIzTUJ0pUGXcsbkkIUSh006tfk3bk6s3gSHmSx0V/pub?gid=1573772404&single=true&output=csv\" #\"https://docs.google.com/spreadsheets/d/1xgoOPu28dQSSGPIp_HHQs0uvvcyLNdkMF9XtRajhhxU/gviz/tq?gid=1573772404&tqx=out:csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "vispubdata = pd.read_csv(vispubdata_sheet_url, keep_default_na=False)\n",
    "\n",
    "print(vispubdata.columns)\n",
    "\n",
    "#if you ever want to introduce new columns you could try to do it here. I usually add them on the Google sheet. \n",
    "final_columns = vispubdata.columns\n",
    "#final_columns = ['Conference', 'Year', 'Title', 'DOI', 'Link', 'FirstPage','LastPage','PaperType','Abstract','AuthorNames-Deduped','AuthorNames','AuthorAffiliation','InternalReferences','AuthorKeywords','AminerCitationCount','CitationCount_CrossRef','PubsCited_CrossRef',' Downloads_Xplore','Award','GraphicsReplicabilityStamp']\n",
    "\n",
    "#double check that the names are correct here\n",
    "crossRefCitation_column = \"CitationCount_CrossRef\"\n",
    "crossRefPubsCited_column = 'PubsCited_CrossRef'\n",
    "downloads_column = \"Downloads_Xplore\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56199904-e0ac-45d9-8709-b25e9ab1fcab",
   "metadata": {},
   "source": [
    "#### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74f9e4-b281-449d-8033-79a54904cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need this later to sort the author columns by the number hidden in its name  \n",
    "def num_sort(test_string):\n",
    "    return list(map(int, re.findall(r'\\d+', test_string)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe3c38-950d-41a4-b143-5b824fc0e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we prepare the data structure that will resemble the final vispubdata table\n",
    "def prepareXploreDFTable(xplore_df):\n",
    "    \n",
    "    #get all column names\n",
    "    columns = xplore_df.columns\n",
    "\n",
    "    #remove all the columns we don't need\n",
    "    #-------------------------------------------------------------\n",
    "    xplore_df.drop('articles_access_type', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_abstract_url', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_publisher', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_article_number', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_volume', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_rank', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_publication_number', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_publication_date', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_pdf_url', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_partnum', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_issue', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_issn', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_is_number', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_html_url', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_publication_title', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_citing_patent_count', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_conference_location', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_conference_dates', axis=1, inplace=True,errors='ignore')\n",
    " \n",
    "\n",
    "    #WARNING: I assume the file is correctly ordered. If not we need to do something more fancy\n",
    "    columns_to_drop = columns[columns.str.contains(\"author_order\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    columns_to_drop = columns[columns.str.contains(\"ieee_terms\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    columns_to_drop = columns[columns.str.contains(\"_id\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    columns_to_drop = columns[columns.str.contains(\"authorUrl\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    columns_to_drop = columns[columns.str.contains(\"isbn\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    #rename the columns we want to keep\n",
    "    xplore_df.rename(index=str, inplace=True, columns={\"articles_title\":\"Title\",\n",
    "                                                       \"articles_start_page\":\"FirstPage\",\n",
    "                                                       \"articles_abstract\":\"Abstract\",\n",
    "                                                       \"articles_publication_year\": \"Year\",\n",
    "                                                       \"articles_content_type\": \"PaperType\",\n",
    "                                                       \"articles_end_page\":\"LastPage\",\n",
    "                                                       \"articles_doi\":\"DOI\",\n",
    "                                                       \"articles_citing_paper_count\":\"CitationCount_CrossRef\",\n",
    "                                                       \"articles_download_count\":\"Downloads_Xplore\"})\n",
    "\n",
    "\n",
    "    #put all author full names together\n",
    "    #----------------------------------------\n",
    "    author_columns = columns[columns.str.contains(\"full_name\")].tolist()\n",
    "    author_columns.sort(key=num_sort)\n",
    "    \n",
    "    xplore_df[author_columns].fillna(value=\"\")\n",
    "    authors = xplore_df[author_columns].apply(lambda x: ';'.join(x.dropna().values.tolist()), axis=1)\n",
    "    authors = authors.str.rstrip(\";\")\n",
    "    #we have the authors put together, now add them to the df\n",
    "    xplore_df['AuthorNames'] = authors\n",
    "    #now remove all the individual columns that we no longer need\n",
    "    xplore_df.drop(author_columns, axis=1, inplace=True)\n",
    "\n",
    "    #put all affiliations together\n",
    "    #------------------------------------------\n",
    "    ##We have to do something more complicated for the affiliations because the csv file does not contain an affiliation column if none of the authors in a given position has an affiliation\n",
    "    \n",
    "    #Careful. IEEEXplore seems to change the way it handles the affiliations. Their json changes each year. \n",
    "    #Also, the original json can now handle multiple affiliations - todo for the future. Requires an update to vispubdata.\n",
    "    \n",
    "    xplore_df['AuthorAffiliation'] = \"\" #make an empty column first\n",
    "    xplore_df[\"AuthorCount\"] =  xplore_df['AuthorNames'].str.count(';') + 1\n",
    "    \n",
    "    for index, row in xplore_df.iterrows():\n",
    "        #we need to find out how many authors a paper has first\n",
    "        authorCount = row[\"AuthorCount\"]\n",
    "        affiliations = \"\"\n",
    "        for i in range(0,authorCount):\n",
    "            author_column_name = author_columns[i]\n",
    "            affcolumn = author_column_name.replace(\"full_name\",\"affiliation\")\n",
    "            if affcolumn in xplore_df.columns:\n",
    "                #we're doing this a few too many times here but we don't care for speed, yet\n",
    "                affiliations = affiliations + \";\" + row[affcolumn]\n",
    "            else:\n",
    "                affiliations = affiliations + \";\" + \"\"\n",
    "        affiliations = affiliations[1:]\n",
    "        #print(affiliations)\n",
    "        xplore_df.at[index,'AuthorAffiliation'] = affiliations\n",
    "    \n",
    "    \n",
    "    xplore_df.drop([\"AuthorCount\"], axis=1, inplace=True)\n",
    "    \n",
    "    affiliation_columns = columns[columns.str.contains(\"affiliation\")]\n",
    "    #now remove all the individual columns that we no longer need\n",
    "    xplore_df.drop(affiliation_columns, axis=1, inplace=True)\n",
    "\n",
    "    #put all author keywords together\n",
    "    #------------------------------------------\n",
    "    kw_columns = columns[columns.str.contains(\"author_terms\")]\n",
    "    xplore_df[kw_columns].fillna(value=\"\")\n",
    "    keywords = xplore_df[kw_columns].apply(lambda x: ','.join(x.dropna().values.tolist()), axis=1)\n",
    "    keywords = keywords.apply(lambda x: x.rstrip(','))\n",
    "\n",
    "    #we have the keywords put together, now add them to the df\n",
    "    xplore_df['AuthorKeywords'] = keywords\n",
    "    #now remove all the individual columns that we no longer need\n",
    "    xplore_df.drop(kw_columns, axis=1, inplace=True)\n",
    "\n",
    "    #create the link column\n",
    "    #--------------------------------------------\n",
    "    xplore_df[\"Link\"] = 'http://dx.doi.org/' + xplore_df[\"DOI\"]\n",
    "\n",
    "    #now add columns that are missing\n",
    "    xplore_df_columns = xplore_df.columns\n",
    "    \n",
    "    for c in final_columns:\n",
    "        if not c in xplore_df_columns:\n",
    "            if c == \"Conference\":\n",
    "                xplore_df[\"Conference\"] = [\"conference_external\"] * len(xplore_df.index)\n",
    "            else:\n",
    "                xplore_df[c] = [\"\"] * len(xplore_df.index)\n",
    "    \n",
    "    for c in xplore_df_columns:\n",
    "        if not c in final_columns:\n",
    "            #we remove all columns that we haven't captured yet\n",
    "            xplore_df.drop([c], axis=1, inplace=True)\n",
    "            \n",
    "    #now reorder the columns\n",
    "    xplore_df = xplore_df[final_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9901d3f-807b-43f4-82f1-03091064a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkJournalTitle(xplore_df):\n",
    "    #DOIs are not case_sensitive!\n",
    "    journals_df['doi'] = journals_df['doi'].str.lower()\n",
    "    xplore_df['DOI'] = xplore_df['DOI'].str.lower()\n",
    "    \n",
    "    for index, row in xplore_df.iterrows():\n",
    "        xplore_doi = row['DOI']\n",
    "        journal_name = journals_df.loc[journals_df['doi'] == xplore_doi,\"journal\"].tolist()[0]\n",
    "        xplore_df.loc[index,'Journal'] = journal_name\n",
    "        \n",
    "    \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0ee47-d48b-4bd3-91e8-a18809851097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertFile(csv_filename):\n",
    "    path = \"journal-data/generated-data/\"+csv_filename\n",
    "    \n",
    "    #double-checking that the file exists\n",
    "    if(os.path.isfile(path)):\n",
    "        print(\"Your file \" + path + \" exists\")\n",
    "    else:\n",
    "        print(\"Your file \" + path + \" does not exist\")\n",
    "        \n",
    "    #now we load it\n",
    "    xplore_df = pd.read_csv(path,dtype=object, encoding ='utf-8',keep_default_na=False,dialect='excel')\n",
    "    #now prepare it\n",
    "    prepareXploreDFTable(xplore_df)\n",
    "    \n",
    "    checkJournalTitle(xplore_df)\n",
    "    \n",
    "    return xplore_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f766ba-f4d1-4c3e-b847-acc029f0b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "folderdir = 'journal-data/generated-data'\n",
    " \n",
    "ext = ('.csv')\n",
    "xploredfs = []\n",
    "\n",
    "for path, dirc, files in os.walk(folderdir):\n",
    "    for name in files:\n",
    "        if \"ieeexplore\" in name:\n",
    "            xdf = convertFile(name)\n",
    "            xploredfs.append(xdf)\n",
    "\n",
    "ieeexplore_vispub_df =  pd.concat(xploredfs, ignore_index=True)\n",
    "#full_xplore_df.to_csv(\"results/Vispubdata-journals.csv\",index =False)\n",
    "#print(\"File saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5479c-472a-4757-a94a-d79fbece7768",
   "metadata": {},
   "source": [
    "#### Now fill missing entries and fix some others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09065f-2eb8-49d9-a4d5-e911bb8d26a5",
   "metadata": {},
   "source": [
    "##### Fixing the Publication Type\n",
    "\n",
    "- TVCG papers are Journals, so they will get a \"J\" label\n",
    "- CG&A papers are magazine articles, they will get a \"MAG\" label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3f91b-369a-4e5a-9366-a8fe4aa732f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ieeexplore_vispub_df.loc[ieeexplore_vispub_df[\"Journal\"] == \"TVCG\",\"PaperType\"] = \"J\"\n",
    "ieeexplore_vispub_df.loc[ieeexplore_vispub_df[\"Journal\"] == \"CG&A\",\"PaperType\"] = \"MAG\"\n",
    "ieeexplore_vispub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0604d89-d4a5-4f4b-8088-7dacb528993c",
   "metadata": {},
   "outputs": [],
   "source": [
    " ieeexplore_vispub_df.to_csv(\"results/DEBUG_ieeexplore_journals_df.csv\") #some debugging output if you'd like to check that so far things are in order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c48bd-a64c-4ac0-998d-d91fe3deebda",
   "metadata": {},
   "source": [
    "### Merging with the current vispubdata journal spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c806113-ad13-4ac1-b76c-b392c123182a",
   "metadata": {},
   "source": [
    "Now we merge the current vispubdata and the new table.\n",
    "We want to keep our current version of vispubdata and add the new papers, then we need to copy over the new citation and download counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf76b30-0e92-4611-8797-1e671eeeaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vispubdata #the current version of vispubdata journal articles (loaded above)\n",
    "#we probably already did this but just in case\n",
    "\n",
    "ieeexplore_vispub_df #all vis papers with data from IEEEXplore -> that is, some papers will be missing but there will be some new ones in here\n",
    "ieeexplore_vispub_df['DOI'] = ieeexplore_vispub_df['DOI'].str.lower()\n",
    "vispubdata['DOI'] = vispubdata['DOI'].str.lower()\n",
    "\n",
    "#we first add the new papers to vispubdata\n",
    "new_papers = ieeexplore_vispub_df[~ieeexplore_vispub_df['DOI'].isin(vispubdata['DOI'])]\n",
    "\n",
    "print(new_papers['Year'].unique()) #hopefully we're only adding papers from one new year here\n",
    "\n",
    "vispubdata_new = pd.concat([vispubdata,new_papers],ignore_index = True)\n",
    "\n",
    "#ok, I've learned that DOIs are not case-sensitive. Merde...\n",
    "vispubdata_new[\"DOI\"] = vispubdata_new[\"DOI\"].str.lower()\n",
    "\n",
    "\n",
    "#print(vispubdata_new.info())\n",
    "vispubdata_new.to_csv(\"results/DEBUG_vispubdata_new.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2990b-2493-4779-b1ba-f7050ea35a90",
   "metadata": {},
   "source": [
    "### Copy over the download data\n",
    "\n",
    "The IEEEXplore API provides information on how many times a paper has been downloaded. We copy this over here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06966fa4-46c3-4b40-a2c2-e7300b393125",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_df = pd.DataFrame({'DOI':ieeexplore_vispub_df['DOI'],downloads_column:ieeexplore_vispub_df[downloads_column]})\n",
    "download_df[\"DOI\"] = download_df[\"DOI\"].str.lower()\n",
    "\n",
    "for index,row in download_df.iterrows():\n",
    "    downloads = row[downloads_column]\n",
    "    doi = row['DOI']\n",
    "\n",
    "    vispubdata_new.loc[vispubdata_new['DOI']== doi,downloads_column] = downloads\n",
    "\n",
    "#check that now we have download counts updated\n",
    "vispubdata_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e8406-de5c-4de5-adea-fe7059776a9e",
   "metadata": {},
   "source": [
    "### Add Awards\n",
    "\n",
    "At this point we're not recording paper awards. This is a TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381d4c4-c4d7-49d0-8c55-6902d01f49de",
   "metadata": {},
   "source": [
    "### Add Replicability Stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf82a9-68b7-4ba3-abca-cf0a8f3b4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicablePapers = pd.read_csv(\"tvcg-dois-with-stamp.csv\", keep_default_na=False)\n",
    "replicablePapers[\"doi\"] = replicablePapers[\"doi\"].str.lower()\n",
    "#replicablePapers.head()\n",
    "\n",
    "stampMarker = \"X\"\n",
    "\n",
    "for index,row in replicablePapers.iterrows():\n",
    "    \n",
    "    doi = row['doi']\n",
    "\n",
    "    vispubdata_new.loc[vispubdata_new['DOI'] == doi,'GraphicsReplicabilityStamp'] = stampMarker\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a9d0a-eeca-4255-9b8a-ac27c3c08321",
   "metadata": {},
   "source": [
    "### Get CrossRef Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cef90d-5275-4c89-b3a1-d9b58ffcb72c",
   "metadata": {},
   "source": [
    "The output from this operation is a file named  **citations_references_journals_vispubdata.csv**. You can jump over this section if this file is relatively recent since the next chunk of code takes a bit of time to run. The file will be read in by the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9683995-3501-4225-abb0-b417afb71184",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_etiquette = Etiquette('Vispubdata', '9.02', 'https://sites.google.com/site/vispubdata/home', youremail)\n",
    "str(my_etiquette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd4e78-7520-4c94-a1b3-50b329fa35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "works = Works(etiquette=my_etiquette)\n",
    "\n",
    "crossRefCitations = [ ]\n",
    "crossRefPubsCited = [ ]\n",
    "dois = [ ] \n",
    "referenced_papers = [ ]\n",
    "referenced_papers_without_DOI = []\n",
    "\n",
    "print(\"Starting to work on the citation counts\")\n",
    "\n",
    "for index,row in vispubdata_new.iterrows():\n",
    "    doi = row['DOI']\n",
    "    print(str(index) + \" \" + doi)\n",
    "    \n",
    "    paper = works.doi(doi)\n",
    "    \n",
    "    if (paper is None):\n",
    "        print(\"CrossRef does not know: \" + doi)\n",
    "        dois.append(doi)\n",
    "        crossRefCitations.append(\"\")\n",
    "        crossRefPubsCited.append(\"\")\n",
    "        referenced_papers_without_DOI.append(\"\")\n",
    "        referenced_papers.append(\"\")\n",
    "        continue\n",
    "        \n",
    "    isreferencedby = paper['is-referenced-by-count']\n",
    "    \n",
    "    if (isreferencedby is None):\n",
    "        isreferencedby = \"\"\n",
    "    \n",
    "    references = paper['references-count']\n",
    "    \n",
    "    if (references is None):\n",
    "        references = \"\"\n",
    "\n",
    "    if 'reference' in paper:\n",
    "        cited_papers = paper['reference']\n",
    "\n",
    "        citedpapers = \"\"\n",
    "        citedpapers_withoutDOI = \"\"\n",
    "\n",
    "        if(cited_papers is None):\n",
    "            citedpapers = \"\"\n",
    "            print(\"No cited papers found for: \" + doi)\n",
    "        else:\n",
    "            for ref in cited_papers:\n",
    "                if \"DOI\" in ref:\n",
    "                    cited_doi = ref['DOI'].lower().strip()\n",
    "                    citedpapers =  citedpapers + \";\" + cited_doi\n",
    "\n",
    "                elif \"article-title\" in ref:\n",
    "                    citedpapers_withoutDOI = citedpapers_withoutDOI + \":::\" + ref['article-title'].strip()\n",
    "                \n",
    "\n",
    "        if len(citedpapers) > 0:\n",
    "            citedpapers = citedpapers[1:]\n",
    "        \n",
    "        if len(citedpapers_withoutDOI) > 0:\n",
    "            citedpapers_withoutDOI = citedpapers_withoutDOI[3:]\n",
    "    else:\n",
    "        print(\"No cited papers found for: \" + doi + \": Crossref does not have the reference list for this paper. This should be flagged to the IEEE Xplore people for them to fix.\")\n",
    "        citedpapers_withoutDOI = \"\"\n",
    "        citedpapers = \"\"\n",
    "        #referenced_papers_without_DOI.append(\"\")\n",
    "        #referenced_papers.append(\"\")\n",
    "                \n",
    "        \n",
    "    crossRefCitations.append(isreferencedby)\n",
    "    crossRefPubsCited.append(references)\n",
    "    referenced_papers_without_DOI.append(citedpapers_withoutDOI)\n",
    "    referenced_papers.append(citedpapers)\n",
    "\n",
    "    dois.append(doi)\n",
    "\n",
    "\n",
    "citationdf = pd.DataFrame({'DOI':dois,crossRefCitation_column:crossRefCitations,crossRefPubsCited_column:crossRefPubsCited,\"citedPapers\":referenced_papers,\"citedPapersWithoutDOI\":referenced_papers_without_DOI})\n",
    "citationdf.to_csv(\"results/citations_references_journals_vispubdata.csv\",index=False)\n",
    "citationdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed67ba-30d4-49c1-9d3e-cc32f1f1817d",
   "metadata": {},
   "source": [
    "### Integrate the Citation data\n",
    "\n",
    "This part you shouldn't skip over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47708f1-dff6-4788-822c-221a269b903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "citationdf = pd.read_csv(\"results/citations_references_journals_vispubdata.csv\", keep_default_na=False)\n",
    "citationdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ae59c-8c71-4dad-ada3-968000834300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we copy the citation data over\n",
    "#it would be much smarter to do this with a merge TODO later since I don't care for speed yet....\n",
    "\n",
    "for index,row in citationdf.iterrows():\n",
    "    \n",
    "    doi = row['DOI']\n",
    "    pubscited = row[crossRefPubsCited_column]\n",
    "    citation = row[crossRefCitation_column]\n",
    "\n",
    "    vispubdata_new.loc[vispubdata_new['DOI'] == doi,crossRefPubsCited_column] = pubscited\n",
    "    vispubdata_new.loc[vispubdata_new['DOI'] == doi,crossRefCitation_column] = citation\n",
    "\n",
    "\n",
    "\n",
    "vispubdata_new.to_csv(\"DEBUG_vispubdata_new.csv\")\n",
    "\n",
    "vispubdata_new.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ac733-ad84-465f-97c9-a870ded14a42",
   "metadata": {},
   "source": [
    "### Getting the internal citations\n",
    "\n",
    "What should the internal references represent? Any references to VIS papers? Or also references to other journal papers presented at VIS? Maybe let's go for all of VIS + journals. Although this is not what I'm currently doing for vispubdata where references are only to other VIS papers.\n",
    "\n",
    "For this to work correctly the online version of vispubdata should have been updated - since I'm reading from there directly. Otherwise you'll have to load the latest local version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff215c10-af2a-4343-8c3d-4d4ebad9d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheet URL\n",
    "vispubdata_main_sheet_url = \"https://docs.google.com/spreadsheets/d/1xgoOPu28dQSSGPIp_HHQs0uvvcyLNdkMF9XtRajhhxU/gviz/tq?tqx=out:csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "vispubdata_main = pd.read_csv(vispubdata_main_sheet_url, keep_default_na=False)\n",
    "vispubdata_main_DOIs = vispubdata_main['DOI'].str.lower().tolist()\n",
    "\n",
    "vispubdata_journal_DOIs = vispubdata_new['DOI'].str.lower().tolist()\n",
    "\n",
    "for index,row in citationdf.iterrows():\n",
    "    \n",
    "    doi = row['DOI'].lower()\n",
    "\n",
    "    crossRefReferences = row['citedPapers']\n",
    "    if pd.isna(crossRefReferences):\n",
    "         #if crossref didn't return any references, for now we just continue. Later we can write code to check if it returned any titles of papers\n",
    "         continue\n",
    "    \n",
    "    \n",
    "    internalReferences_current = vispubdata_new.loc[vispubdata_new['DOI'] == doi,'InternalReferences']\n",
    "    currentreflist = internalReferences_current #.tolist()[0].split(\";\")\n",
    "    \n",
    "    if not pd.isna(internalReferences_current).values[0]:\n",
    "         currentreflist = currentreflist.tolist()[0].split(\";\")\n",
    "    else:\n",
    "         currentreflist = []\n",
    "    \n",
    "    #print(\"This many internal refs before: \" + str(len(currentreflist)))\n",
    "    refsbefore = len(currentreflist)\n",
    "    currentreflist = [s.strip() for s in currentreflist]\n",
    "    currentreflist = [s.lower() for s in currentreflist]\n",
    "     \n",
    "       \n",
    "    crossRefList = crossRefReferences.split(\";\")\n",
    "    crossRefList = [s.strip() for s in crossRefList]\n",
    "    crossRefList = [s.lower() for s in crossRefList]\n",
    "\n",
    "    for crossRefRef in crossRefList:\n",
    "         if crossRefRef in currentreflist:\n",
    "              #in this case the citation is already in our list\n",
    "              #print(\"already found: \" + crossRefRef)\n",
    "              continue\n",
    "         else:\n",
    "              #the citation is not already on vispubdata. There are two reasons. Either the reference is not to a VIS paper, or it was forgotten\n",
    "              #so let's check if the reference is a vis paper\n",
    "              if (crossRefRef in vispubdata_journal_DOIs) or (crossRefRef in vispubdata_main_DOIs):\n",
    "                   #yes, we've found a VIS paper. So now we need to add it to our list:\n",
    "                   currentreflist.append(crossRefRef)\n",
    "\n",
    "    #print(\"This many internal refs after: \" + str(len(currentreflist)))\n",
    "    #if len(currentreflist) > refsbefore:\n",
    "    #     print(\"found something new\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #print(crossRefReferences)\n",
    "    \n",
    "    vispubdata_new.loc[vispubdata_new['DOI'] == doi,'InternalReferences'] = ';'.join(currentreflist)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42b626-b128-40fc-b291-6fbdb64a23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vispubdata_new.head()\n",
    "#so we can double-check that things look good\n",
    "vispubdata_new.to_csv(\"results/DEBUG_vispubdata_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b176f5-0de3-478c-99ca-d0e4334abfcd",
   "metadata": {},
   "source": [
    "### Include deduped authors from DBLP\n",
    "\n",
    "For the following code I expect that you already ran preparation 2 from all the way at the top of the document. That's the step where you extracted potential VIS authors from the DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca0d44-c07a-4db9-8a3e-40e9f8fd13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doi_substring(text):\n",
    "    # Define the regex pattern to find the DOI that starts with doi.org\n",
    "    pattern = r'doi\\.org/([^:]+?)(::|$)'\n",
    "    \n",
    "    # Search for the pattern in the given text\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    # If a match is found, return the captured group (the DOI substring)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a73e61-b19e-4a54-b6fa-c7942f7fea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblpauthors = pd.read_csv(\"../dblp-data-extraction/data/VIS-author-articles.csv\",keep_default_na=False)\n",
    "\n",
    "\n",
    "# Define the regex pattern to find the doi out of the list of electronic identifiers\n",
    "dblpauthors['DOI'] = dblpauthors['ee'].apply(find_doi_substring)\n",
    "dblpauthors['DOI'] = dblpauthors['DOI'].str.lower()\n",
    "\n",
    "dblpauthors.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693eb722-8112-4c9d-b127-ce1efc4c2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prepare to copy things over\n",
    "\n",
    "vispubdata_new['AuthorNames-Deduped'] = vispubdata_new['AuthorNames-Deduped'].astype(str)\n",
    "\n",
    "dois_not_in_dblp = []\n",
    "\n",
    "\n",
    "for index,row in vispubdata_new.iterrows():\n",
    "\n",
    "    doi = row['DOI']\n",
    "    #find the DOI in the dblpauthors\n",
    "    \n",
    "    #print(doi)\n",
    "    found = dblpauthors['DOI'].eq(doi).any() \n",
    "    #print(found)\n",
    "   \n",
    "    if(found != False):\n",
    "\n",
    "        dblp_rowindex = dblpauthors.index[dblpauthors['DOI'] == doi].tolist()[0]\n",
    "        #print(dblp_rowindex)\n",
    "        vispubdata_rowindex = index #pd.Index(vispubdata.DOI).get_loc(doi)\n",
    "        #print(vispubdata_rowindex)\n",
    "        \n",
    "        vispubdata_deduped_authors = vispubdata_new.at[vispubdata_rowindex,'AuthorNames-Deduped']\n",
    "        authors_on_dblp = dblpauthors.at[dblp_rowindex,'author']\n",
    "        authors_on_dblp = authors_on_dblp.replace(\"::\",\";\")\n",
    "        \n",
    "\n",
    "        if vispubdata_deduped_authors != authors_on_dblp:\n",
    "            dblp_deduped_authors = authors_on_dblp\n",
    "            print(\"DOI: \" +dblpauthors.at[dblp_rowindex,'DOI'] + \" \" + vispubdata_new.at[vispubdata_rowindex,'DOI'])\n",
    "            print(\"Existing authors do not match new authors:\")\n",
    "            print(\" old: \"+str(vispubdata_deduped_authors))\n",
    "            print(\" new: \"+str(dblp_deduped_authors))\n",
    "            vispubdata_new.at[vispubdata_rowindex,'AuthorNames-Deduped'] = dblp_deduped_authors\n",
    "        #else:\n",
    "            #print(\"Old authors match new authors\")\n",
    "    else:\n",
    "        dois_not_in_dblp.append(doi)\n",
    "\n",
    "print(len(dois_not_in_dblp))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c8dc6-a67c-4cb1-ab79-eccf98b1ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dois_not_in_dblp)\n",
    "\n",
    "\n",
    "#for those papers we just copy over their IEEEXplore author list\n",
    "for doi in dois_not_in_dblp:\n",
    "    vispubdata_rowindex = pd.Index(vispubdata_new.DOI).get_loc(doi)\n",
    "    authors = vispubdata_new.at[vispubdata_rowindex,'AuthorNames']\n",
    "    vispubdata_new.loc[vispubdata_rowindex, 'AuthorNames-Deduped'] = authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f85786-a8c1-4ee4-a5c3-6008d43e724c",
   "metadata": {},
   "source": [
    "All articles above are not (yet) in DBLP. They may be early access articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7ddbc-6192-46fd-9823-a43bdbcf10e9",
   "metadata": {},
   "source": [
    "## The Finale\n",
    "We save the file to disk that can be copied over to vispubdata. However, at this point the Aminer citations are still missing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8fbde-2f41-493e-b3b4-8e3dbf3b72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vispubdata_new.to_csv(\"results/vispubdata-update-journals.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d5795-991c-4f14-b345-76e4e99cd3ec",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Now we generate a few files that are handy for data analysis about authors. This is not strictly necessary for the vispubdata update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f9e40-c6d3-42f8-bec8-896ea9e196b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_authors = []\n",
    "\n",
    "authors = vispubdata_new['AuthorNames-Deduped'].dropna()\n",
    "\n",
    "for author in authors:\n",
    "    authors = author.split(';')\n",
    "    for a in authors:\n",
    "        deduped_authors.append(a)\n",
    "    \n",
    "\n",
    "deduped_unique_authors = list(set(deduped_authors))\n",
    "\n",
    "#print(deduped_unique_authors)    \n",
    "\n",
    "pd.DataFrame({'Authors':deduped_unique_authors}).to_csv('results/Deduped-Authors-journals.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c6635-c769-415a-b6d4-3cd5d865eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#create the papers -> author names df\n",
    "papersAuthorsTemp = pd.DataFrame({'Paper DOI':vispubdata_new['DOI'],'Author Names':vispubdata_new['AuthorNames-Deduped']})\n",
    "papersAuthors = pd.DataFrame(papersAuthorsTemp['Author Names'].str.split(';').tolist(), index=papersAuthorsTemp['Paper DOI']).stack()\n",
    "papersAuthors = papersAuthors.reset_index([0, 'Paper DOI'])\n",
    "papersAuthors.columns = ['Paper DOI', 'Author Names']\n",
    "\n",
    "papersAuthors.to_csv(\"results/DedupedAuthors-Papers-journals.csv\",index=False)\n",
    "\n",
    "papersAuthors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e35e68-e016-481c-9490-c5fd51c68d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "papersAuthorsPosition = papersAuthors\n",
    "papersAuthorsPosition['PositionNumber'] = -1\n",
    "papersAuthorsPosition['PositionCode'] = ''\n",
    "papersAuthorsPosition['Affiliation'] = ''\n",
    "\n",
    "papersAuthorsPosition.head()\n",
    "lastDOI = ''\n",
    "lastPosition = 0\n",
    "lastindex = -1\n",
    "\n",
    "#Warning, this assumes that the df was created in sorted order\n",
    "for index, row in papersAuthorsPosition.iterrows():\n",
    "    doi = row['Paper DOI']\n",
    "    position = -1\n",
    "    affiliation = ''\n",
    "    \n",
    "    allAffs = vispubdata_new.loc[vispubdata_new['DOI'] == doi,'AuthorAffiliation'].values[0]\n",
    "    #print(allAffs)\n",
    "    \n",
    "    affList = allAffs.split(\";\")\n",
    "    \n",
    "    if doi != lastDOI:\n",
    "        #we've reached a new paper\n",
    "        position = 1\n",
    "        positionCode = 'F'\n",
    "        lastDOI = doi\n",
    "        if len(affList) > 0:\n",
    "            affiliation = affList[0]\n",
    "        \n",
    "        #if we're not at the very first row\n",
    "        if lastindex != -1:\n",
    "            #we need to update the positionCode of the last author\n",
    "            papersAuthorsPosition.loc[lastindex,'PositionCode'] = 'L'\n",
    "            #if it was a single author, we set them to 'F'\n",
    "            if papersAuthorsPosition.loc[lastindex,'PositionNumber'] == 1:\n",
    "                papersAuthorsPosition.loc[lastindex,'PositionCode'] = 'F'\n",
    "\n",
    "    elif doi == lastDOI:\n",
    "        #we're still at the same paper\n",
    "        if len(affList) > lastPosition:\n",
    "            affiliation = affList[lastPosition]\n",
    "        position = lastPosition + 1\n",
    "        positionCode = 'M'\n",
    "        \n",
    "        \n",
    "    papersAuthorsPosition.loc[index,'PositionNumber'] = position\n",
    "    papersAuthorsPosition.loc[index,'PositionCode'] = positionCode\n",
    "    papersAuthorsPosition.loc[index,'Affiliation'] = affiliation\n",
    "    lastindex = index\n",
    "    lastPosition = position\n",
    "        \n",
    "        \n",
    "papersAuthorsPosition.to_csv(\"results/DedupedAuthors-Papers-Position-Affiliation-journals.csv\",index=False)\n",
    "\n",
    "papersAuthorsPosition.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc502d-e857-4328-ae8d-37be3331b3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0fcf75-6f54-482d-9962-af1815e4fa21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
