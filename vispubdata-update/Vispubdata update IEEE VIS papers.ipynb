{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5bd2a2",
   "metadata": {},
   "source": [
    "# Updating Vispubdata with new IEEE VIS papers\n",
    "\n",
    "This notebook allows you to create an update of vispubdata. It will walk you through the different steps that are needed. \n",
    "\n",
    "This code was written by Petra Isenberg (petra.isenberg@inria.fr) and is update regularly.\n",
    "If you find better or more elegant ways to implement what I did below, don't hesitate to suggest improvements!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e779e",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "### Get an IEEE Xplore API Key\n",
    "https://ieeexplore.ieee.org/Xplorehelp/administrators-and-librarians/api-portal\n",
    "\n",
    "Put the API key into the file called \"ieeexplore-apikey.txt\" in the main folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5406a",
   "metadata": {},
   "source": [
    "### 1) Finding the issue number\n",
    "\n",
    "Find the issue number for the latest TVCG issue that holds the new VIS papers you would like to add. To do so, do the following: \n",
    "    - Go to the TVCG IEEEXplore entry: https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=2945. Find the issue that holds the IEEE VIS papers of the year. At the time of writing, this was typicaly issue 1 of each year. Open any paper of this issue. Then check the source of the html page for this paper and search in the source file for \"isnumber\".\n",
    "    - Open the file \"xplore-isnumbers.csv\". Add a new line at the bottom with the year, [YOUR ISNUMBER] ,TVCGSI,articles-SI. TVCGSI stands for \"special issue of TVCG\" and \"articles-SI\" stands for \"article in the special issue\". In the earlier years you can find slightly different codes but those don't have to concern you if you are updating more recent articles.\n",
    "\n",
    "**Example: \n",
    "\n",
    "The IEEE VIS 2023 papers can be found here in the digital library: https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=10373160&punumber=2945. Open a random paper from the issue. For example this one: https://ieeexplore.ieee.org/document/10301796. Check the source. In the document metadata you will find the isnumber: isnumber=10373160. Add the following line to xplore-isnumbers.csv: **2023,10373160,TVCGSI,\n",
    "\n",
    "#### Documentation for xplore-isnumbers.csv\n",
    "\n",
    "The columns of this file are:\n",
    "1. year: the year of the conference. E.g.1990 for Vis 1990\n",
    "2. isnumber: the issue number as given by IEEEXplore. See above\n",
    "3. conference: the conference that belongs to the issue number. Anything after 2021 should be like the last entry below.\n",
    "   - **Vis-conf** = the IEEE Visualization conference when papers were still published as conference papers\n",
    "   - **InfoVis-conf** = the IEEE Information Visualization conference when papers were still published as conference papers\n",
    "   - **VAST-conf** = the IEEE VAST conference when papers were still published as conference papers\n",
    "   - **SciVis-conf** = the IEEE SciVis conference when papers were still published as conference papers\n",
    "   - **TVCGSI** = the papers that are published as a special issue in TVCG\n",
    "5. jason_filename = the name of the file that will hold the paper info once downloaded. Anything after 2021 should be like the last entry below.\n",
    "   - **articles-vis** for **Vis-conf**\n",
    "   - **articles-infovis** for **InfoVis-conf**\n",
    "   - **articles-vast** for **VAST-conf**\n",
    "   - **articles-scivis** for **SciVis-conf**\n",
    "   - **articles-SI** for **TVCGSI**\n",
    "\n",
    "### 2) Get the IEEE VIS full paper titles from the IEEE VIS program chairs\n",
    "Every year there are a few errors on the IEEE digital library. In addition, the special issue of TVCG that includes the IEEE VIS full papers (usually the first issue of the year) sometimes contains additional papers such as best papers from associated events such as LDAV. \n",
    "To chatch errors and exclude papers that do not belong on vispubdata, I have data files with the correct titles of each IEEE VIS full paper. These are the titles listed on vispubdata. To get the correct titles email (mailto:program@ieeevis.org)[program@ieeevis.org] to ask for the file. Then go to the folder for the year that you are updating. For example, to add paper titles for VIS 2023, **create a folder** called 2023 and in it a file called **VisTitles.txt**. Copy the titles in there, one title per line.\n",
    "\n",
    "### 3) Get the awarded papers\n",
    "Open the file **awardedPapers.csv** and add the information for the paper awards of the conference year you are adding. The paper award types are:\n",
    "- **BP** = Best paper award\n",
    "- **HM** = Honorable mention award\n",
    "- **TT** = Test of Time award\n",
    "- **BCS** = Best case study award (was given out in the early years, not recently)\n",
    "\n",
    "If a paper received more than one award then separate them by a semicolon. You can still do this step after you've done the conversion of IEEEXplore data to vispubdata format. That way you can easily copy the DOIs of the awarded papers from the `generated_data` file called `Vispubdata-Vis.csv` if you're adding a new year - if you are adding an older award then search for the DOI on IEEEXplore or vispubdata.\n",
    "\n",
    "### 4) Extract a VIS Authors file from DBLP\n",
    "We assume that this step is already done. If not, go to the [`../dblp-data-extraction/`](../dblp-data-extraction/) folder in this repo and follow the instructions in the `ParseDBLP-VIS-Authors` notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0057de-bfec-4c63-8d27-93496eca067e",
   "metadata": {},
   "source": [
    "### Libraries \n",
    "Make sure to have these libraries. Install them if not. Run the code below before starting to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891382d-2923-48a4-b9fa-e83ae1bd85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "from crossref.restful import Works, Etiquette\n",
    "\n",
    "\n",
    "with open('ieeexplore-apikey.txt') as f:\n",
    "    apikey = f.readline()\n",
    "\n",
    "youremail = \"petra.isenberg@inria.fr\" #replace this with your own email address. This is required for querying CrossRef\n",
    "\n",
    "xplore_numbers = pd.read_csv(\"xplore-isnumbers.csv\", dtype=object,keep_default_na=False) #readings all as strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27663d68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the paper metadata from IEEEXplore\n",
    "\n",
    "These next code sections will download and check that there is a json file with data about papers from each year. If your data files are up to date you can skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ccc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#set whether or not to download all files again or just check for new ones\n",
    "# --> if you want updated download counts, then set this to true\n",
    "renewall = True \n",
    "\n",
    "print(xplore_numbers.head(60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6936783",
   "metadata": {},
   "source": [
    "##### Check if we have all files we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9425cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in xplore_numbers.iterrows():\n",
    "    year = row['year']\n",
    "    if not pd.isnull(row['json_filename']):\n",
    "        filename = row['json_filename']+\".json\"\n",
    "        \n",
    "        my_file = Path(year+\"/\"+filename)\n",
    "        if renewall == True:\n",
    "            row['json_filename'] = np.NaN\n",
    "        elif not my_file.is_file():\n",
    "            row['json_filename'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163831c1",
   "metadata": {},
   "source": [
    "##### Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0fec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we download all the files we don't have  or we want to renew  \n",
    "baseurl =  \"https://ieeexploreapi.ieee.org/api/v1/search/articles?parameter&apikey=\"+apikey+\"&max_records=200&is_number=\"\n",
    "\n",
    "for index, row in xplore_numbers.iterrows():\n",
    "    year = row['year']\n",
    "    print(year + \" \" + row['conference'])\n",
    "    \n",
    "    if pd.isnull(row['json_filename']):\n",
    "        fullurl = baseurl + row['isnumber']\n",
    "        \n",
    "        filename = \"articles-SI\"\n",
    "        if(row['conference'] == \"InfoVis-conf\"): \n",
    "            filename = \"articles-infovis\"\n",
    "        elif (row['conference'] == 'VAST-conf'):\n",
    "            filename = \"articles-vast\"\n",
    "        elif (row['conference'] == 'SciVis-conf'):\n",
    "            filename = \"articles-scivis\"\n",
    "        elif (row['conference'] == 'Vis-conf'):\n",
    "            filename = \"articles-vis\"\n",
    "            \n",
    "        full_filename = year + \"/original_data/\"+filename + \".json\"\n",
    "        \n",
    "        # Create target Directory if it doesn't exist\n",
    "        directory = year+\"/original_data\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "            print(\"Directory \" , directory ,  \" Created \")\n",
    "        else:    \n",
    "            print(\"Directory \" , directory ,  \" already exists\")\n",
    "        \n",
    "        with open(full_filename, 'w+', encoding=\"utf-8\") as f:\n",
    "            resp = requests.get(fullurl, verify=True)\n",
    "            f.write(resp.text)\n",
    "        \n",
    "        xplore_numbers.at[index,'json_filename'] = filename\n",
    "\n",
    "xplore_numbers.to_csv(\"xplore-isnumbers.csv\",index=False)\n",
    "\n",
    "print('Processing done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806210d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert downloaded paper metadata from json to csv\n",
    "\n",
    "Here we convert the .json file into something more flat and similar to vispubdata. Run this code if you updated the .json files above.\n",
    "\n",
    "This code comes from here:\n",
    "https://github.com/vinay20045/json-to-csv/blob/master/json_to_csv.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Convert to string keeping encoding in mind...\n",
    "##\n",
    "def to_string(s):\n",
    "    \n",
    "    try:\n",
    "        return str(s)\n",
    "    except:\n",
    "        #Change the encoding type if needed\n",
    "        return s.encode('utf-8')\n",
    "\n",
    "    \n",
    "def reduce_item(key, value):\n",
    "    global reduced_item\n",
    "    \n",
    "    #Reduction Condition 1\n",
    "    if type(value) is list:\n",
    "        i=0\n",
    "        for sub_item in value:\n",
    "            reduce_item(key+'_'+to_string(i), sub_item)\n",
    "            i=i+1\n",
    "\n",
    "    #Reduction Condition 2\n",
    "    elif type(value) is dict:\n",
    "        sub_keys = value.keys()\n",
    "        for sub_key in sub_keys:\n",
    "            reduce_item(key+'_'+to_string(sub_key), value[sub_key])\n",
    "    \n",
    "    #Base Condition\n",
    "    else:\n",
    "        reduced_item[to_string(key)] = to_string(value)\n",
    "\n",
    "        \n",
    "\n",
    "for index, row in xplore_numbers.iterrows():\n",
    "    year = row['year']\n",
    "    \n",
    "    if not pd.isnull(row['json_filename']):\n",
    "        node = \"articles\"\n",
    "        \n",
    "        generated_data_path = year +\"/generated_data/\"\n",
    "        pathExists = os.path.exists(generated_data_path)\n",
    "        if not pathExists:\n",
    "            os.makedirs(generated_data_path)\n",
    "        \n",
    "        json_file_path = year + \"/original_data/\" + row['json_filename']+\".json\"\n",
    "        csv_file_path = generated_data_path + row['json_filename']+\".csv\"\n",
    "        \n",
    "        #print (\"Now working on \"+csv_file_path)\n",
    "        \n",
    "        #TODO: We could check if the .csv file already exists and then decide not to do anything\n",
    "\n",
    "        #fp = open(json_file_path, 'r',encoding=\"utf8\")\n",
    "        fp = open(json_file_path, mode = 'rb')\n",
    "        json_value = fp.read()\n",
    "        raw_data = json.loads(json_value)\n",
    "\n",
    "        try:\n",
    "            data_to_be_processed = raw_data[node]\n",
    "        except:\n",
    "            data_to_be_processed = raw_data\n",
    "\n",
    "        processed_data = []\n",
    "        header = []\n",
    "        for item in data_to_be_processed:\n",
    "            reduced_item = {}\n",
    "            reduce_item(node, item)\n",
    "\n",
    "            header += reduced_item.keys()\n",
    "\n",
    "            processed_data.append(reduced_item)\n",
    "\n",
    "        header = list(set(header))\n",
    "        header.sort()\n",
    "\n",
    "        with open(csv_file_path, 'w+',encoding=\"utf8\") as f:\n",
    "            writer = csv.DictWriter(f, header, dialect='excel')\n",
    "            writer.writeheader()\n",
    "            for row in processed_data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "        #print (\"Just completed writing \"+csv_file_path+\" file with %d columns\" % len(header))\n",
    "        \n",
    "    else:\n",
    "        print(\"We are missing a filename for: \" + year + \" \" + row['conference']+ \". Did you run the previous step already?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8404cbe",
   "metadata": {},
   "source": [
    "## Convert IEEEXplore csvs to Vispubdata\n",
    "\n",
    "Here we convert the data from the .json file into the vispubdata format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f89da",
   "metadata": {},
   "source": [
    "#### Read the latest vispubdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheet URL\n",
    "vispubdata_sheet_url = \"https://docs.google.com/spreadsheets/d/1xgoOPu28dQSSGPIp_HHQs0uvvcyLNdkMF9XtRajhhxU/gviz/tq?tqx=out:csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "vispubdata = pd.read_csv(vispubdata_sheet_url,keep_default_na=False)\n",
    "\n",
    "print(vispubdata.columns)\n",
    "\n",
    "#if you ever want to introduce new columns you could try to do it here. I usually add them on the Google sheet. \n",
    "final_columns = vispubdata.columns\n",
    "#final_columns = ['Conference', 'Year', 'Title', 'DOI', 'Link', 'FirstPage','LastPage','PaperType','Abstract','AuthorNames-Deduped','AuthorNames','AuthorAffiliation','InternalReferences','AuthorKeywords','AminerCitationCount','CitationCount_CrossRef','PubsCited_CrossRef',' Downloads_Xplore','Award','GraphicsReplicabilityStamp']\n",
    "\n",
    "#double check that the names are correct here\n",
    "crossRefCitation_column = \"CitationCount_CrossRef\"\n",
    "crossRefPubsCited_column = 'PubsCited_CrossRef'\n",
    "downloads_column = \"Downloads_Xplore\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610b291",
   "metadata": {},
   "source": [
    "#### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c07a3-13e9-4e2b-97ff-f6ca19db53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need this later to sort the author columns by the number hidden in its name  \n",
    "def num_sort(test_string):\n",
    "    return list(map(int, re.findall(r'\\d+', test_string)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855ebfa-5591-4ca2-bdc9-54737bf28cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we prepare the data structure that will resemble the final vispubdata table\n",
    "def prepareXploreDFTable(xplore_df):\n",
    "    \n",
    "    #get all column names\n",
    "    columns = xplore_df.columns\n",
    "\n",
    "    #remove all the columns we don't need\n",
    "    #-------------------------------------------------------------\n",
    "    xplore_df.drop('articles_access_type', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_abstract_url', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_publisher', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_article_number', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_volume', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_rank', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_publication_number', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_publication_date', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_pdf_url', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_partnum', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_issue', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_issn', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_is_number', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_html_url', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_publication_title', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_citing_patent_count', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_conference_location', axis=1, inplace=True,errors='ignore')\n",
    "    xplore_df.drop('articles_conference_dates', axis=1, inplace=True,errors='ignore')\n",
    " \n",
    "\n",
    "    #WARNING: I assume the file is correctly ordered. If not we need to do something more fancy\n",
    "    columns_to_drop = columns[columns.str.contains(\"author_order\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    columns_to_drop = columns[columns.str.contains(\"ieee_terms\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    columns_to_drop = columns[columns.str.contains(\"_id\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    columns_to_drop = columns[columns.str.contains(\"authorUrl\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    columns_to_drop = columns[columns.str.contains(\"isbn\")] \n",
    "    xplore_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    #rename the columns we want to keep\n",
    "    xplore_df.rename(index=str, inplace=True, columns={\"articles_title\":\"Title\",\n",
    "                                                       \"articles_start_page\":\"FirstPage\",\n",
    "                                                       \"articles_abstract\":\"Abstract\",\n",
    "                                                       \"articles_publication_year\": \"Year\",\n",
    "                                                       \"articles_content_type\": \"PaperType\",\n",
    "                                                       \"articles_end_page\":\"LastPage\",\n",
    "                                                       \"articles_doi\":\"DOI\",\n",
    "                                                       \"articles_citing_paper_count\":\"CitationCount_CrossRef\",\n",
    "                                                       \"articles_download_count\":\"Downloads_Xplore\"})\n",
    "\n",
    "\n",
    "    #put all author full names together\n",
    "    #----------------------------------------\n",
    "    author_columns = columns[columns.str.contains(\"full_name\")].tolist()\n",
    "    author_columns.sort(key=num_sort)\n",
    "    \n",
    "    xplore_df[author_columns].fillna(value=\"\")\n",
    "    authors = xplore_df[author_columns].apply(lambda x: ';'.join(x.dropna().values.tolist()), axis=1)\n",
    "    authors = authors.str.rstrip(\";\")\n",
    "    #we have the authors put together, now add them to the df\n",
    "    xplore_df['AuthorNames'] = authors\n",
    "    #now remove all the individual columns that we no longer need\n",
    "    xplore_df.drop(author_columns, axis=1, inplace=True)\n",
    "\n",
    "    #put all affiliations together\n",
    "    #------------------------------------------\n",
    "    ##We have to do something more complicated for the affiliations because the csv file does not contain an affiliation column if none of the authors in a given position has an affiliation\n",
    "    \n",
    "    #Careful. IEEEXplore seems to change the way it handles the affiliations. Their json changes each year. \n",
    "    #Also, the original json can now handle multiple affiliations - todo for the future. Requires an update to vispubdata.\n",
    "    \n",
    "    xplore_df['AuthorAffiliation'] = \"\" #make an empty column first\n",
    "    xplore_df[\"AuthorCount\"] =  xplore_df['AuthorNames'].str.count(';') + 1\n",
    "    \n",
    "    for index, row in xplore_df.iterrows():\n",
    "        #we need to find out how many authors a paper has first\n",
    "        authorCount = row[\"AuthorCount\"]\n",
    "        affiliations = \"\"\n",
    "        for i in range(0,authorCount):\n",
    "            author_column_name = author_columns[i]\n",
    "            affcolumn = author_column_name.replace(\"full_name\",\"affiliation\")\n",
    "            if affcolumn in xplore_df.columns:\n",
    "                #we're doing this a few too many times here but we don't care for speed, yet\n",
    "                affiliations = affiliations + \";\" + row[affcolumn]\n",
    "            else:\n",
    "                affiliations = affiliations + \";\" + \"\"\n",
    "        affiliations = affiliations[1:]\n",
    "        #print(affiliations)\n",
    "        xplore_df.at[index,'AuthorAffiliation'] = affiliations\n",
    "    \n",
    "    \n",
    "    xplore_df.drop([\"AuthorCount\"], axis=1, inplace=True)\n",
    "    \n",
    "    affiliation_columns = columns[columns.str.contains(\"affiliation\")]\n",
    "    #now remove all the individual columns that we no longer need\n",
    "    xplore_df.drop(affiliation_columns, axis=1, inplace=True)\n",
    "\n",
    "    #put all author keywords together\n",
    "    #------------------------------------------\n",
    "    kw_columns = columns[columns.str.contains(\"author_terms\")]\n",
    "    xplore_df[kw_columns].fillna(value=\"\")\n",
    "    keywords = xplore_df[kw_columns].apply(lambda x: ','.join(x.dropna().values.tolist()), axis=1)\n",
    "    keywords = keywords.apply(lambda x: x.rstrip(','))\n",
    "    #we have the keywords put together, now add them to the df\n",
    "    xplore_df['AuthorKeywords'] = keywords\n",
    "    #now remove all the individual columns that we no longer need\n",
    "    xplore_df.drop(kw_columns, axis=1, inplace=True)\n",
    "\n",
    "    #create the link column\n",
    "    #--------------------------------------------\n",
    "    xplore_df[\"Link\"] = 'http://dx.doi.org/' + xplore_df[\"DOI\"]\n",
    "\n",
    "    #now add columns that are missing\n",
    "    xplore_df_columns = xplore_df.columns\n",
    "    \n",
    "    for c in final_columns:\n",
    "        if not c in xplore_df_columns:\n",
    "            if c == \"Conference\":\n",
    "                xplore_df[\"Conference\"] = [\"conference_external\"] * len(xplore_df.index)\n",
    "            else:\n",
    "                xplore_df[c] = [\"\"] * len(xplore_df.index)\n",
    "    \n",
    "    for c in xplore_df_columns:\n",
    "        if not c in final_columns:\n",
    "            #we remove all columns that we haven't captured yet\n",
    "            xplore_df.drop([c], axis=1, inplace=True)\n",
    "            \n",
    "    #now reorder the columns\n",
    "    xplore_df = xplore_df[final_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkConferenceTitleFiles(conf_title_file,xplore_df,year_to_import,conference_external):\n",
    "    \n",
    "    #For a couple of years InfoVis, SciVis, and VAST were all published in the same special issue of TVCG. \n",
    "    #In order to separate them we need to check which title in that special issue belongs to which conference.\n",
    "    \n",
    "\n",
    "    conf_titles = pd.read_csv(conf_title_file,header=None,names=['Title'],sep=\"\\t\",encoding ='utf-8',quoting=csv.QUOTE_NONE,keep_default_na=False)\n",
    "    conf_titles = conf_titles['Title'].str.lower()\n",
    "    titles_not_found = [ ]\n",
    "    reasons_found = 0\n",
    "    xplore_df = xplore_df[final_columns]\n",
    "    xPloreTitles = xplore_df['Title'].str.lower()\n",
    "\n",
    "    reasonsFileName = str(year_to_import)+\"/missingTitles-reason.txt\"\n",
    "    \n",
    "    for t in conf_titles:\n",
    "        #we should just get the titles from df that match the conference\n",
    "        #now check if the title exists in the read out data\n",
    "        found = xPloreTitles[xPloreTitles == t].any()\n",
    "            \n",
    "                \n",
    "        if(found != False):\n",
    "            #rowindex = pd.Index(xPloreTitles).get_loc(t)\n",
    "            #xplore_df.at[str(rowindex),'Conference'] = conference_external.replace('-conf', '')\n",
    "            confname = conference_external.replace('-conf', '')\n",
    "            xplore_df.loc[xplore_df['Title'].str.lower() == t,\"Conference\"] = confname\n",
    "            \n",
    "        else:\n",
    "            titles_not_found.append(t)\n",
    "            if(os.path.isfile(reasonsFileName)):\n",
    "                reason_df = pd.read_csv(reasonsFileName,dtype=object, sep=\"\\t\",encoding ='utf-8',keep_default_na=False)\n",
    "                reasontitles = reason_df[\"Title\"].str.lower().tolist()\n",
    "                if t in reasontitles:\n",
    "                    reasons_found = reasons_found + 1\n",
    "                else:\n",
    "                    print(\"No reason found for: \"+t)\n",
    "\n",
    "    \n",
    "    if len(titles_not_found) > 0:\n",
    "        print(\"For \"+str(year_to_import)+ \" \" + str(conference_external) + \" I couldn't find \" + str(len(titles_not_found)) + \" titles. \"+str(reasons_found) + \" reasons for missings titles were recorded.\")\n",
    "    pd.DataFrame({'Conference':conference_external,'Title':titles_not_found}).to_csv(year_to_import+'/generated_data/TitlesNotFound-'+conference_external+'.csv',index=False)\n",
    "    \n",
    "    xplore_df.to_csv(year_to_import+\"/generated_data/Vispubdata-\"+conference_external+\".csv\",index =False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workOnYear(year_to_import,csv_filename,conference_external):\n",
    "\n",
    "    #we'll get the .csv file for the conference and year we're interested in\n",
    "    path = year_to_import+\"/generated_data/\"+csv_filename\n",
    "\n",
    "    #double-checking that the file exists\n",
    "    if(os.path.isfile(path)):\n",
    "        print(\"Your file \" + path + \" exists\")\n",
    "    else:\n",
    "        print(\"Your file \" + path + \" does not exist\")\n",
    "\n",
    "    #print(path)\n",
    "    \n",
    "    #now we load it\n",
    "    xplore_df = pd.read_csv(path,dtype=object, encoding ='utf-8',keep_default_na=False,dialect='excel')\n",
    "    #now prepare it\n",
    "    prepareXploreDFTable(xplore_df)\n",
    "    \n",
    "    \n",
    "    ###Now we have the xplore_df file ready for this particular year and conference\n",
    "    \n",
    "    conf_title_file = year_to_import+\"/\"+conference_external+\"Titles.txt\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    if Path(conf_title_file).is_file():\n",
    "        checkConferenceTitleFiles(conf_title_file,xplore_df,year_to_import,conference_external)\n",
    "\n",
    "    else:\n",
    "        #print(\"Title file: \" + conf_title_file + \" not found\")\n",
    "        \n",
    "        #Most likely we've encountered the special issue link that we need to split into InfoVis, Vis, SciVis, or VAST\n",
    "        if \"TVCGSI\" in conference_external:\n",
    "            possibleConferences = [\"InfoVis\",\"SciVis\",\"VAST\"]\n",
    "            titleFilesFound = []\n",
    "            filesChecked = 0\n",
    "            \n",
    "            #here we check if this special issue contained papers from InfoVis, VAST, or SciVis\n",
    "            for pc in possibleConferences:\n",
    "                conf_title_file = year_to_import+\"/\"+pc+\"Titles.txt\"\n",
    "                if Path(conf_title_file).is_file():\n",
    "                    checkConferenceTitleFiles(conf_title_file,xplore_df,year_to_import,pc)\n",
    "                    filesChecked = filesChecked + 1\n",
    "                    \n",
    "            #now we check if this special issue is from the combined conference that no longer has these sub-confereces\n",
    "            conf_title_file = year_to_import+\"/VisTitles.txt\" #from 2021 onwards\n",
    "            if Path(conf_title_file).is_file():\n",
    "                    checkConferenceTitleFiles(conf_title_file,xplore_df,year_to_import,\"Vis\")\n",
    "                    filesChecked = filesChecked + 1\n",
    "            \n",
    "            if filesChecked == 0:\n",
    "                print(\"Warning: For this special issue there is no file with paper titles!\")\n",
    "                    \n",
    "            \n",
    "        else:\n",
    "            print(\"Some problem here. I don't know what to do with \" + conf_title_file)\n",
    "        \n",
    "    print(\"------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2dd8fc",
   "metadata": {},
   "source": [
    "#### Start the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets work on all years\n",
    "\n",
    "xplore_numbers = pd.read_csv(\"xplore-isnumbers.csv\", dtype=object) #readings all as strings\n",
    "\n",
    "for index, row in xplore_numbers.iterrows():\n",
    "    year = row['year']\n",
    "    issue_number = row['isnumber']\n",
    "    conference_external = row['conference']\n",
    "\n",
    "    csv_filename = xplore_numbers.loc[xplore_numbers['isnumber'] == issue_number, 'json_filename']+\".csv\"\n",
    "    csv_filename = csv_filename.iloc[0]\n",
    "    \n",
    "    print(\"Looking for: \" + conference_external + \" \" + year + \" at \" + csv_filename)\n",
    "    \n",
    "    workOnYear(year,csv_filename,conference_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04222b6d-ac1d-467a-a9f8-4dd97e12ad2b",
   "metadata": {},
   "source": [
    "### Double-check correctness of the data manually\n",
    "\n",
    "#### Check for titles that were not found\n",
    "Now we have the data in vispubdata format.  Please read the output above. It's important that each time when you see that a specific title hasn't been found, a reason has been recorded. If a reason has not been recorded then there is either an error in the IEEEXplore DL or in our list of titles. You need to check where the paper is manually. The main reference should be the paper itself. Try to find it on the DL and open the pdf. You can find the titles that have not been found by going to the year's generated_data folder and opening the respective `TitlesNotFound.txt` file. If you found out where the problem, do the following:\n",
    "- if the title is incorrect in the respective `VisTitles.txt` file, fix it there\n",
    "- if the title is incorrect on the IEEEXplore digital library, then create a file called `missingTitles-reason.txt`. The header is \"Title\" \"DOI\"  \"Reason\" -> all tab separated. On line 2 copy the title from our titles list (which has to be the correct one), the DOI of the paper, and explain why it's different from the one in the IEEE DL. You can follow the example in the 2018 folder. You can also try to email the IEEEXplore DL folks to tell them about the error (or email the publications chairs of IEEEVIs who might have a more direct connection to them).\n",
    "\n",
    "#### Check if you missed any titles in our list\n",
    "Go to the `generated_data` folder of the year that you are trying to add. Open all files that have the format `Vispubdata-*.csv`. Check all the papers that have a **conference_external** label in the first column. If you suspect that they might be actual IEEE VIS papers, then investigate why their title is not in your list. Reasons can be:\n",
    "- Many of the entries labeled as **conference_external** will be editorials, the list of PC members, the OC, or other front/back matter. You can ignore them\n",
    "- Every once in a while there is something that looks like an actual paper. Don't assume immediately that you missed the paper. Check the program of https://ieeevis.org of the year you are adding. In the last couple of years, there has been the occasional best paper from an associated event such as LDAV or VDS included in the special issue of TVCG that also includes all VIS papers. We don't want these papers in vispubdata, so ignore them. If you find one of these, create a file called `non-vis titles in SI.txt` in the main folder of the year you are adding. Add the paper title. This helps other people to avoid looking for this paper again.\n",
    "- If, however, you find an actual Vis paper with the **conference_external** label, it means that you didn't have its title in the `VisTitles.txt` file. Add it and rerun the code above.\n",
    "\n",
    "Note, if you are re-checking older years, there is a peculiarity: The special issue contained papers from all 3 conferences (InfoVis, SciVis, and VAST) - so you will see a lot of conference_external labels in the respective files. So you'll have to do a merge of the three vispubdata files and find out what is consistently labeled as **conference_external**. I haven't written code for that yet.\n",
    "\n",
    "#### Fixing the publication year\n",
    "\n",
    "If you are adding a new special issue published after 2015 you have to manually fix the date. Since 2015 VIS papers are published in the first issue of the next calendar year of IEEE TVCG but in vispubdata we record the papers by when they were presented at the conference and not by when they appeared in TVCG.  For simplicity we'll just fix the code of every year to be the year of the folder the data is in\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ca0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"./\"):\n",
    "    for file in files:\n",
    "        \n",
    "        filepath = os.path.join(root, file)\n",
    "        if (\"generated_data\\Vispubdata-\" in filepath):\n",
    "            year = root[2:6]  #this may be a source of error in other operating systems. To check...\n",
    "            df = pd.read_csv(filepath,dtype=object, encoding ='utf-8',keep_default_na=False)\n",
    "            df['Year'] = year\n",
    "            df.to_csv(filepath,index =False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c55ec7",
   "metadata": {},
   "source": [
    "##### Fixing the Publication Type\n",
    "\n",
    "If you are adding VIS papers that are part of a special issue of TVCG then we can simply replace the paper type from \"journals\" to \"J\" (=vispubdata notation). If you, however, you are adding conference papers or posters you likely need to do some manual fixing of this data since IEEEXplore tags both as \"Conferences\" while vispubdata marks posters, panels, VAST challenges etc. as \"M\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd6cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"./\"):\n",
    "    for file in files:\n",
    "        \n",
    "        filepath = os.path.join(root, file)\n",
    "        if (\"generated_data\\Vispubdata-\" in filepath):\n",
    "            year = root[2:6]  #this may be a source of error in other operating systems. To check...\n",
    "            df = pd.read_csv(filepath,dtype=object, encoding ='utf-8',keep_default_na=False)\n",
    "            df.loc[df['PaperType'] == 'Journals', 'PaperType'] = \"J\"\n",
    "            df.to_csv(filepath,index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9263f-3f32-40b3-9534-abf323caed4d",
   "metadata": {},
   "source": [
    "## Make sure that you have done all the steps above\n",
    "\n",
    "Next, we automatically remove all entries with the conference_external label. So it's really important that you finished careful error checking at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa996f4b-0966-4b11-aab6-dcee654ec424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "years = xplore_numbers.year.unique()\n",
    "dataframes = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"./\"):\n",
    "    for file in files:\n",
    "        filepath = os.path.join(root, file)\n",
    "        if (\"generated_data\\Vispubdata-\" in filepath):\n",
    "            df = pd.read_csv(filepath,dtype=object, encoding ='utf-8',keep_default_na=False)\n",
    "            df.drop(df[df.Conference == \"conference_external\"].index,inplace=True)\n",
    "            df.to_csv(filepath,index =False)\n",
    "            dataframes.append(df)\n",
    "            \n",
    "#this df will hold all ieeevis papers with data from ieeexplore. Note, that the online version of vispubdata contains error fixes that are available only on the spreadsheet\n",
    "#so NEVER just copy this df below over onto vispubdata. We should only copy over certain columns like the ones about downloads\n",
    "ieeexplore_vispub_df = pd.concat(dataframes, ignore_index=True)   \n",
    "            \n",
    "ieeexplore_vispub_df.info()\n",
    "\n",
    "ieeexplore_vispub_df.to_csv(\"results/DEBUG_ieeexplore_vispub_df.csv\")\n",
    "\n",
    "#Petra: I usually take a quick scan through this DEBUG-file to see if everything looks more or less ok\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b1f06-8742-4bb7-9911-60d2a4542187",
   "metadata": {},
   "source": [
    "Now we merge the current vispubdata and the new table.\n",
    "We want to keep our current version of vispubdata and add the new papers, then we need to copy over the new citation and download counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd427e-6a72-47a2-b474-170a4af0a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################CONTINUE HERE#################################\n",
    "#somewhere above we've loaded the current vispubdata into a dataframe\n",
    "vispubdata #the current version of vispubdata\n",
    "ieeexplore_vispub_df #all vis papers with data from IEEEXplore -> that is, some papers will be missing but there will be some new ones in here\n",
    "ieeexplore_vispub_df['DOI'] = ieeexplore_vispub_df['DOI'].str.lower()\n",
    "vispubdata['DOI'] = vispubdata['DOI'].str.lower()\n",
    "\n",
    "#we first add the new papers to vispubdata\n",
    "new_papers = ieeexplore_vispub_df[~ieeexplore_vispub_df['DOI'].isin(vispubdata['DOI'])]\n",
    "\n",
    "print(new_papers['Year']) #hopefully we're only adding papers from one new year here\n",
    "\n",
    "vispubdata_new = pd.concat([vispubdata,new_papers],ignore_index = True)\n",
    "\n",
    "\n",
    "#print(vispubdata_new.info())\n",
    "vispubdata_new.to_csv(\"results/DEBUG_vispubdata_new.csv\")\n",
    "\n",
    "vispubdata_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff5bff",
   "metadata": {},
   "source": [
    "### Copy over the download data\n",
    "\n",
    "The IEEEXplore API provides information on how many times a paper has been downloaded. We copy this over here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_df = pd.DataFrame({'DOI':ieeexplore_vispub_df['DOI'],downloads_column:ieeexplore_vispub_df[downloads_column]})\n",
    "download_df[\"DOI\"] = download_df[\"DOI\"].str.lower()\n",
    "\n",
    "for index,row in download_df.iterrows():\n",
    "    downloads = row[downloads_column]\n",
    "    doi = row['DOI']\n",
    "\n",
    "    vispubdata_new.loc[vispubdata_new['DOI']== doi,downloads_column] = downloads\n",
    "\n",
    "#check that now we have download counts updated\n",
    "vispubdata_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab9d28",
   "metadata": {},
   "source": [
    "## Add Awards\n",
    "\n",
    "Make sure that you have updated **awardedPapers.csv**. See the instructions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf831ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "awardedPapers = pd.read_csv(\"awardedPapers.csv\",keep_default_na=False)\n",
    "awardedPapers[\"DOI\"] = awardedPapers[\"DOI\"].str.lower()\n",
    "\n",
    "for index,row in awardedPapers.iterrows():\n",
    "    award = row['Award']\n",
    "    doi = row['DOI']\n",
    "\n",
    "    vispubdata_new.loc[vispubdata_new['DOI']== doi,'Award'] = award\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaba6c5",
   "metadata": {},
   "source": [
    "## Add Replicability Stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b531c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicablePapers = pd.read_csv(\"tvcg-dois-with-stamp.csv\",keep_default_na=False)\n",
    "replicablePapers[\"doi\"] = replicablePapers[\"doi\"].str.lower()\n",
    "\n",
    "stampMarker = \"X\"\n",
    "\n",
    "for index,row in replicablePapers.iterrows():\n",
    "    \n",
    "    doi = row['doi']\n",
    "\n",
    "    vispubdata_new.loc[vispubdata_new['DOI'] == doi,'GraphicsReplicabilityStamp'] = stampMarker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a8d14",
   "metadata": {},
   "source": [
    "## Add CrossRef Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6adecc7",
   "metadata": {},
   "source": [
    "### Download new Citation data.\n",
    "\n",
    "The output from this operation is a file named  **citations_vispubdata.csv**. You can jump over this section if this file is relatively recent since the next chunk of code takes around 30 minutes to run. The file will be read in by the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_etiquette = Etiquette('Vispubdata', '9.02', 'https://sites.google.com/site/vispubdata/home', youremail)\n",
    "str(my_etiquette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "works = Works(etiquette=my_etiquette)\n",
    "\n",
    "crossRefCitations = [ ]\n",
    "crossRefPubsCited = [ ]\n",
    "dois = [ ] \n",
    "referenced_papers = [ ]\n",
    "referenced_papers_without_DOI = []\n",
    "\n",
    "print(\"Starting to work on the citation counts\")\n",
    "\n",
    "for index,row in vispubdata_new.iterrows():\n",
    "    doi = row['DOI']\n",
    "    print(str(index) + \" \" + doi)\n",
    "    \n",
    "    paper = works.doi(doi)\n",
    "    \n",
    "    if (paper is None):\n",
    "        print(\"CrossRef does not know: \" + doi)\n",
    "        dois.append(doi)\n",
    "        crossRefCitations.append(\"\")\n",
    "        crossRefPubsCited.append(\"\")\n",
    "        referenced_papers_without_DOI.append(\"\")\n",
    "        referenced_papers.append(\"\")\n",
    "        continue\n",
    "        \n",
    "    isreferencedby = paper['is-referenced-by-count']\n",
    "    \n",
    "    if (isreferencedby is None):\n",
    "        isreferencedby = \"\"\n",
    "    \n",
    "    references = paper['references-count']\n",
    "    \n",
    "    if (references is None):\n",
    "        references = \"\"\n",
    "\n",
    "    if 'reference' in paper:\n",
    "        cited_papers = paper['reference']\n",
    "\n",
    "        citedpapers = \"\"\n",
    "        citedpapers_withoutDOI = \"\"\n",
    "\n",
    "        if(cited_papers is None):\n",
    "            citedpapers = \"\"\n",
    "            print(\"No cited papers found for: \" + doi)\n",
    "        else:\n",
    "            for ref in cited_papers:\n",
    "                if \"DOI\" in ref:\n",
    "                    cited_doi = ref['DOI'].lower().strip()\n",
    "                    citedpapers =  citedpapers + \";\" + cited_doi\n",
    "\n",
    "                elif \"article-title\" in ref:\n",
    "                    citedpapers_withoutDOI = citedpapers_withoutDOI + \":::\" + ref['article-title'].strip()\n",
    "                \n",
    "\n",
    "        if len(citedpapers) > 0:\n",
    "            citedpapers = citedpapers[1:]\n",
    "        \n",
    "        if len(citedpapers_withoutDOI) > 0:\n",
    "            citedpapers_withoutDOI = citedpapers_withoutDOI[3:]\n",
    "    else:\n",
    "        print(\"No cited papers found for: \" + doi + \": Crossref does not have the reference list for this paper. This should be flagged to the IEEE Xplore people for them to fix.\")\n",
    "        citedpapers_withoutDOI = \"\"\n",
    "        citedpapers = \"\"\n",
    "        #referenced_papers_without_DOI.append(\"\")\n",
    "        #referenced_papers.append(\"\")\n",
    "                \n",
    "        \n",
    "    crossRefCitations.append(isreferencedby)\n",
    "    crossRefPubsCited.append(references)\n",
    "    referenced_papers_without_DOI.append(citedpapers_withoutDOI)\n",
    "    referenced_papers.append(citedpapers)\n",
    "\n",
    "    dois.append(doi)\n",
    "\n",
    "\n",
    "citationdf = pd.DataFrame({'DOI':dois,crossRefCitation_column:crossRefCitations,crossRefPubsCited_column:crossRefPubsCited,\"citedPapers\":referenced_papers,\"citedPapersWithoutDOI\":referenced_papers_without_DOI})\n",
    "citationdf.to_csv(\"results/citations_references_vispubdata.csv\",index=False)\n",
    "citationdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c961fc",
   "metadata": {},
   "source": [
    "### Integrate the Citation data\n",
    "\n",
    "This part you shouldn't skip over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error checking\n",
    "#Check for which DOI we don't have data\n",
    "citationdf = pd.read_csv(\"results/citations_references_vispubdata.csv\", keep_default_na=False)\n",
    "citationdf.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0982a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For debugging the API\n",
    "# doi = '10.0000/00000001'\n",
    "\n",
    "# works = Works(etiquette=my_etiquette)\n",
    "# paper = works.doi(doi)\n",
    "# isreferencedby = \"\"\n",
    "# if(paper is not None):\n",
    "#         isreferencedby = works.doi(doi)['is-referenced-by-count']\n",
    "#         if (isreferencedby is None):\n",
    "#                 isreferencedby = \"\"\n",
    "\n",
    "# print(\"Citations: \" + isreferencedby)\n",
    "# #pubscited = works.doi(doi)['references-count']\n",
    "\n",
    "# works.doi(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we copy the citation data over\n",
    "#it would be much smarter to do this with a merge but I am currently worried of getting it wrong and merging rows and values in that I don't want. Since I don't care for speed yet....\n",
    "\n",
    "for index,row in citationdf.iterrows():\n",
    "    \n",
    "    doi = row['DOI']\n",
    "    pubscited = row[crossRefPubsCited_column]\n",
    "    citation = row[crossRefCitation_column]\n",
    "\n",
    "    vispubdata_new.loc[vispubdata_new['DOI'] == doi,crossRefPubsCited_column] = pubscited\n",
    "    vispubdata_new.loc[vispubdata_new['DOI'] == doi,crossRefCitation_column] = citation\n",
    "\n",
    "\n",
    "\n",
    "vispubdata_new.to_csv(\"results/DEBUG_vispubdata_new.csv\")\n",
    "\n",
    "vispubdata_new.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb95c3d",
   "metadata": {},
   "source": [
    "### Getting the internal citations\n",
    "\n",
    "In previous versions of vispubdata we've extracted the internal references using other tools. For example, a tool called Grobid (https://github.com/kermitt2/grobid) converted the pdf to xml and then we went through a paper title matching process, following by manual checking of the results. That all is to say that the old references are likely pretty correct and I don't want to change them. The following code is therefore just about adding potentially missing internal references as exposed by crossref.  If you want to manually correct this data, then do so on vispubdata online. In future years we'll only be adding potentially missing references if crossref has found new ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vispubdataDOIs = vispubdata_new['DOI'].str.lower().tolist()\n",
    "\n",
    "for index,row in citationdf.iterrows():\n",
    "    \n",
    "    doi = row['DOI'].lower()\n",
    "\n",
    "    crossRefReferences = row['citedPapers']\n",
    "    if pd.isna(crossRefReferences):\n",
    "         #if crossref didn't return any references, for now we just continue. Later we can write code to check if it returned any titles of papers\n",
    "         continue\n",
    "    \n",
    "    \n",
    "    internalReferences_current = vispubdata_new.loc[vispubdata_new['DOI'] == doi,'InternalReferences']\n",
    "    currentreflist = internalReferences_current #.tolist()[0].split(\";\")\n",
    "    \n",
    "    if not pd.isna(internalReferences_current).values[0]:\n",
    "         currentreflist = currentreflist.tolist()[0].split(\";\")\n",
    "    else:\n",
    "         currentreflist = []\n",
    "    \n",
    "    #print(\"This many internal refs before: \" + str(len(currentreflist)))\n",
    "    refsbefore = len(currentreflist)\n",
    "    currentreflist = [s.strip() for s in currentreflist]\n",
    "    currentreflist = [s.lower() for s in currentreflist]\n",
    "     \n",
    "       \n",
    "    crossRefList = crossRefReferences.split(\";\")\n",
    "    crossRefList = [s.strip() for s in crossRefList]\n",
    "    crossRefList = [s.lower() for s in crossRefList]\n",
    "\n",
    "    for crossRefRef in crossRefList:\n",
    "         if crossRefRef in currentreflist:\n",
    "              #in this case the citation is already in our list\n",
    "              #print(\"already found: \" + crossRefRef)\n",
    "              continue\n",
    "         else:\n",
    "              #the citation is not already on vispubdata. There are two reasons. Either the reference is not to a VIS paper, or it was forgotten\n",
    "              #so let's check if the reference is a vis paper\n",
    "              if crossRefRef in vispubdataDOIs:\n",
    "                   #yes, we've found a VIS paper. So now we need to add it to our list:\n",
    "                   currentreflist.append(crossRefRef)\n",
    "\n",
    "    #print(\"This many internal refs after: \" + str(len(currentreflist)))\n",
    "    #if len(currentreflist) > refsbefore:\n",
    "    #     print(\"found something new\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #print(crossRefReferences)\n",
    "    \n",
    "    vispubdata_new.loc[vispubdata_new['DOI'] == doi,'InternalReferences'] = ';'.join(currentreflist)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f613df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's debug\n",
    "#so we can double-check that things look good\n",
    "vispubdata_new.to_csv(\"results/DEBUG_vispubdata_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e3869-36ee-4a4e-8044-206e85dd56c6",
   "metadata": {},
   "source": [
    "## Include deduped authors from DBLP\n",
    "\n",
    "For the following code I expect that you already ran preparation 2 from all the way at the top of the document. That's the step where you extracted potential VIS authors from the DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doi_substring(text):\n",
    "    # Define the regex pattern to find the DOI that starts with doi.org\n",
    "    pattern = r'doi\\.org/([^:]+?)(::|$)'\n",
    "    \n",
    "    # Search for the pattern in the given text\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    # If a match is found, return the captured group (the DOI substring)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77de551-7584-4093-9613-a2f1effd109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblpauthors = pd.read_csv(\"../dblp-data-extraction/data/VIS-author-articles.csv\",keep_default_na=False)\n",
    "\n",
    "# Define the regex pattern to find the doi out of the list of electronic identifiers\n",
    "dblpauthors['DOI'] = dblpauthors['ee'].apply(find_doi_substring)\n",
    "dblpauthors['DOI'] = dblpauthors['DOI'].str.lower()\n",
    "\n",
    "dblpauthors.head(20)\n",
    "\n",
    "\n",
    "#if this fails you didn't do step 2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e7599-33d2-4a7a-b5ab-c99a6b641062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prepare to copy things over\n",
    "\n",
    "vispubdata_new['AuthorNames-Deduped'] = vispubdata_new['AuthorNames-Deduped'].astype(str)\n",
    "\n",
    "dois_not_in_dblp = []\n",
    "\n",
    "\n",
    "for index,row in vispubdata_new.iterrows():\n",
    "\n",
    "    doi = row['DOI']\n",
    "    #find the DOI in the dblpauthors\n",
    "    \n",
    "    #print(doi)\n",
    "    found = dblpauthors['DOI'].eq(doi).any() \n",
    "    #print(found)\n",
    "   \n",
    "    if(found != False):\n",
    "\n",
    "        dblp_rowindex = dblpauthors.index[dblpauthors['DOI'] == doi].tolist()[0]\n",
    "        #print(dblp_rowindex)\n",
    "        vispubdata_rowindex = index #pd.Index(vispubdata.DOI).get_loc(doi)\n",
    "        #print(vispubdata_rowindex)\n",
    "        \n",
    "        vispubdata_deduped_authors = vispubdata_new.at[vispubdata_rowindex,'AuthorNames-Deduped']\n",
    "        authors_on_dblp = dblpauthors.at[dblp_rowindex,'author']\n",
    "        authors_on_dblp = authors_on_dblp.replace(\"::\",\";\")\n",
    "        \n",
    "\n",
    "        if vispubdata_deduped_authors != authors_on_dblp:\n",
    "            dblp_deduped_authors = authors_on_dblp\n",
    "            print(\"DOI: \" +dblpauthors.at[dblp_rowindex,'DOI'] + \" \" + vispubdata_new.at[vispubdata_rowindex,'DOI'])\n",
    "            print(\"Existing authors do not match new authors:\")\n",
    "            print(\" old: \"+str(vispubdata_deduped_authors))\n",
    "            print(\" new: \"+str(dblp_deduped_authors))\n",
    "            vispubdata_new.at[vispubdata_rowindex,'AuthorNames-Deduped'] = dblp_deduped_authors\n",
    "        #else:\n",
    "            #print(\"Old authors match new authors\")\n",
    "    else:\n",
    "        dois_not_in_dblp.append(doi)\n",
    "\n",
    "print(len(dois_not_in_dblp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dois_not_in_dblp)\n",
    "\n",
    "#DOIs regularly  not found:\n",
    "#10.0000/00000001 -> M paper\n",
    "#10.0000/00000002 -> M paper\n",
    "#10.1109/VISUAL.1991.175767 --> M paper\n",
    "#10.1109/VISUAL.1991.175823 -- 28 --> M papers (panels etc.)\n",
    "#10.1109/VISUAL.1992.235182 -- 87 --> M papers\n",
    "#10.1109/INFVIS.2003.1249000 --> M paper\n",
    "#10.1109/VISUAL.2003.1250348 --> M paper\n",
    "#10.1109/INFVIS.2004.20 --> M paper, missing in DBLP\n",
    "#10.1109/INFVIS.2004.73 --> M paper, missing in DBLP\n",
    "#10.1109/INFVIS.2005.1532153 --> M paper\n",
    "\n",
    "#for those papers we just copy over their IEEEXplore author list\n",
    "for doi in dois_not_in_dblp:\n",
    "    vispubdata_rowindex = pd.Index(vispubdata_new.DOI).get_loc(doi)\n",
    "    authors = vispubdata_new.at[vispubdata_rowindex,'AuthorNames']\n",
    "    vispubdata_new.loc[vispubdata_rowindex, 'AuthorNames-Deduped'] = authors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e96dc3",
   "metadata": {},
   "source": [
    "## The Finale\n",
    "We save the file to disk that can be copied over to vispubdata. However, at this point the Aminer citations are still missing, they are not updated that often. If you do want to update them head over to th Aminer code once you have finished running the code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "vispubdata_new.to_csv(\"results/vispubdata-update.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb7198",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Now we generate a few files that are handy for data analysis about authors. This is not strictly necessary for the vispubdata update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5763ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_authors = []\n",
    "\n",
    "authors = vispubdata_new['AuthorNames-Deduped'].dropna()\n",
    "\n",
    "for author in authors:\n",
    "    authors = author.split(';')\n",
    "    for a in authors:\n",
    "        deduped_authors.append(a)\n",
    "    \n",
    "\n",
    "deduped_unique_authors = list(set(deduped_authors))\n",
    "\n",
    "#print(deduped_unique_authors)    \n",
    "\n",
    "pd.DataFrame({'Authors':deduped_unique_authors}).to_csv('results/Deduped-Authors.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#create the papers -> author names df\n",
    "papersAuthorsTemp = pd.DataFrame({'Paper DOI':vispubdata_new['DOI'],'Author Names':vispubdata_new['AuthorNames-Deduped']})\n",
    "\n",
    "authors = papersAuthorsTemp['Author Names'].str.split(';').tolist()\n",
    "print(authors)\n",
    "papersAuthors = pd.DataFrame(authors, index=papersAuthorsTemp['Paper DOI']).stack()\n",
    "papersAuthors = papersAuthors.reset_index([0, 'Paper DOI'])\n",
    "papersAuthors.columns = ['Paper DOI', 'Author Names']\n",
    "\n",
    "papersAuthors.to_csv(\"results/DedupedAuthors-Papers.csv\",index=False)\n",
    "\n",
    "papersAuthors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "papersAuthorsPosition = papersAuthors\n",
    "papersAuthorsPosition['PositionNumber'] = -1\n",
    "papersAuthorsPosition['PositionCode'] = ''\n",
    "papersAuthorsPosition['Affiliation'] = ''\n",
    "\n",
    "papersAuthorsPosition.head()\n",
    "lastDOI = ''\n",
    "lastPosition = 0\n",
    "lastindex = -1\n",
    "\n",
    "#Warning, this assumes that the df was created in sorted order\n",
    "for index, row in papersAuthorsPosition.iterrows():\n",
    "    doi = row['Paper DOI']\n",
    "    position = -1\n",
    "    affiliation = ''\n",
    "    \n",
    "    allAffs = vispubdata_new.loc[vispubdata_new['DOI'] == doi,'AuthorAffiliation'].values[0]\n",
    "    #print(allAffs)\n",
    "    \n",
    "    affList = allAffs.split(\";\")\n",
    "    \n",
    "    if doi != lastDOI:\n",
    "        #we've reached a new paper\n",
    "        position = 1\n",
    "        positionCode = 'F'\n",
    "        lastDOI = doi\n",
    "        if len(affList) > 0:\n",
    "            affiliation = affList[0]\n",
    "        \n",
    "        #if we're not at the very first row\n",
    "        if lastindex != -1:\n",
    "            #we need to update the positionCode of the last author\n",
    "            papersAuthorsPosition.loc[lastindex,'PositionCode'] = 'L'\n",
    "            #if it was a single author, we set them to 'F'\n",
    "            if papersAuthorsPosition.loc[lastindex,'PositionNumber'] == 1:\n",
    "                papersAuthorsPosition.loc[lastindex,'PositionCode'] = 'F'\n",
    "\n",
    "    elif doi == lastDOI:\n",
    "        #we're still at the same paper\n",
    "        if len(affList) > lastPosition:\n",
    "            affiliation = affList[lastPosition]\n",
    "        position = lastPosition + 1\n",
    "        positionCode = 'M'\n",
    "        \n",
    "        \n",
    "    papersAuthorsPosition.loc[index,'PositionNumber'] = position\n",
    "    papersAuthorsPosition.loc[index,'PositionCode'] = positionCode\n",
    "    papersAuthorsPosition.loc[index,'Affiliation'] = affiliation\n",
    "    lastindex = index\n",
    "    lastPosition = position\n",
    "        \n",
    "        \n",
    "papersAuthorsPosition.to_csv(\"results/DedupedAuthors-Papers-Position-Affiliation.csv\",index=False)\n",
    "\n",
    "papersAuthorsPosition.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193071a2",
   "metadata": {},
   "source": [
    "And now we're done...now update the journal papers by heading over to **journals-at-vis-update.ipynb**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
