<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">H-BLOB: A Hierarchical Visual Clustering Method Using Implicit Surfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Sprenger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Swiss Federal Institute of Technology (ETH) Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brunella</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Swiss Federal Institute of Technology (ETH) Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Swiss Federal Institute of Technology (ETH) Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">H-BLOB: A Hierarchical Visual Clustering Method Using Implicit Surfaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>clustering</term>
					<term>categorization</term>
					<term>partitioning</term>
					<term>information visualization</term>
					<term>non-linear dimensionality reduction</term>
					<term>physics-based graph layout</term>
					<term>cluster visualization</term>
					<term>multidimensional information visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we present a new hierarchical clustering and visualization algorithm called H-BLOB, which groups and visualizes cluster hierarchies at multiple levels-of-detail. Our method is fundamentally different to conventional clustering algorithms, such as C-means, K-means, or linkage methods that are primarily designed to partition a collection of objects into subsets sharing similar attributes. These approaches usually lack an efficient level-ofdetail strategy that breaks down the visual complexity of very large datasets for visualization. In contrast, our method combines grouping and visualization in a two stage process constructing a hierarchical setting. In the first stage a cluster tree is computed making use of an edge contraction operator. Exploiting the inherent hierarchical structure of this tree, a second stage visualizes the clusters by computing a hierarchy of implicit surfaces. We believe that H-BLOB is especially suited for the visualization of very large datasets and for visual decision making in information visualization. The versatility of the algorithm is demonstrated using examples from visual data mining.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The term clustering refers to the process of grouping similar objects, where similarity is captured by a metric function <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>.</p><p>Clustering methods have been a hot topic in different research fields such as: statistics, pattern recognition, machine learning, etc. Because of the constantly increasing size of datasets over the last years, clustering also has advanced to a key technology in the area of information visualization and data mining. In fact, with the use of today's technology for data generation and collection, typical datasets have grown by magnitudes. Since the human cognitive system is limited to recognize only a very small number of objects at once (around 7 objects) as well as due to performance restrictions of today's graphics hardware we are forced to the use an efficient level-of-detail strategy. Consequently, literature describes various interesting data clustering approaches including their efficient and refined implementations <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b24">[24]</ref>.</p><p>Because our main interest lies in visualizing clusters, we focus on the problem of clustering large data sets in coordinate space <ref type="bibr" target="#b6">[7]</ref>, also referred to as the Euclidian space, in which data objects can be represented as vectors . Unlike data sets in a distance space <ref type="bibr" target="#b6">[7]</ref>, also referred to as the data domain or the arbitrary metric space, the vector representation gives access to various efficiently implemented vector operations (e.g. addition, multiplication, dot-product, etc.), which enables one to calculate simplified representations of complex data subregions at interactive rates. Similar operations are not defined in distance space. The only pos-sible operation is the computation of a distance function between two data objects, thus rendering the problem of clustering much more complex.</p><p>Since many problems in information visualization are located in distance space, and thus non-accessible for our methods, a projection from distance space into coordinate space has to be defined. Such a projection operator maps each data object from distance space to an -dimensional vector in coordinate space while preserving relative distances between objects. Thereafter, vector-based clustering methods may be applied and their results can be visualized in 2D or 3D space.</p><p>This approach entails an additional advantage. Once the projection operator has been applied, the objects have become dataindependent, i.e. the clustering algorithm operating on those objects is highly reusable for a large variety of data clustering tasks.</p><p>There exist several techniques for topology-preserving transformations <ref type="bibr" target="#b19">[19]</ref>. One of them is called multidimensional scaling (MDS) <ref type="bibr" target="#b23">[23]</ref>. Other widely spread methods are employing with neural networks, namely with topology-preserving Kohonen networks <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b8">[9]</ref>, which belong to the group of self-organizing features maps (SOM). As a third technique spring-embedding systems (SES) perform the desired transformation by running a physicsbased simulation process <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>Our clustering research activities take place in the context of the IVORY project, where we develop a JAVA-based framework for physics-based visualization and analysis of multidimensional data relations <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The system is based on quantifying the similarity of related objects, which governs the parameters of a springembedding system. Since the spring stiffnesses correspond to the computed similarity measures, the system will converge into an energy minimum, which reveals multidimensional relations and adjacencies in terms of spatial neighborhood. In our research work, IVORY serves as a versatile information visualization environment to explore visual metaphors and advanced interaction paradigms.</p><p>In order to simplify the geometry and topology of complex object setups, IVORY already provides a set of clustering algorithms for postprocessing. In contrast to many other cluster-based  systems, IVORY not only calculates clustered object layouts including corresponding one-level partitions (as a group of cluttered single objects) but also computes an enfolding surface (ellipsoids, BLOBS (implicit surfaces), etc.) for each cluster <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Aiming at a reduction of complexity, such a surface can replace a large group of single objects in a higher level of representation. Without losing significant visual information, the scene can drastically diminish in complexity. At the same time, the visual distinctness increases.</p><p>In this paper we introduce the concept of H-BLOB clustering. Our new technique discovers and visualizes clusters by a two-stage procedure. During the first stage, an agglomerative hierarchical algorithm computes a cluster tree, partitioning data objects into a nested sequence of subsets. This is what we call the analytical clustering step. In a second stage, the intrinsic visualization takes place. We compute a single enclosing shape for each cluster which approximates the outline of the included data objects as closely as possible. For the visualization we propose a new technique called H-BLOBS, which is a direct improvement to the BLOB clustering algorithm presented in <ref type="bibr" target="#b4">[5]</ref>.</p><p>The remainder of the paper is organized as follows. In Section 2, we discuss related work on clustering and some of our initial approaches. In Section 3, we present the technique we use for fast analytical clustering and introduce the H-BLOB algorithm dedicated to visualize cluster hierarchies using implicit surfaces. The paper closes with Section 4 describing the implementation issues and its versatility on the basis of a real world example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK AND FUNDAMENTAL APPROACHES</head><p>Clustering algorithms can be roughly divided into two categories: partitioning and hierarchical methods. In the following two subsections we present a variety of widely used partitioning, respectively hierarchical clustering algorithms, followed by a description of different advanced cluster visualization techniques. The following list is far from being complete, but it should point out the main clustering techniques, most of today's clustering algorithm are based upon. Mainly, this section conduces to set our work into context and better understand our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Partitioning Methods</head><p>Partitioning cluster methods (PCM) attempt to analytically subdivide a set of data objects into a certain number of clusters, whereupon they assume that clusters are of hyper-ellipsoidal shape and of similar size. Like other centroid-based techniques they generally fail, if clusters differ significantly in shape or size. We will have a closer look at two representative algorithms and their qualities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-Means</head><p>The basic idea of the C-means method is to join an object obj i to a cluster clust j if the distance between the position x i of the data object obj i and the center c j of the cluster clust j is less than a threshold value :</p><p>The center position c j of cluster clust j is defined by the arithmetic average of the positions of all data objects x i enclosed by cluster clust j <ref type="bibr" target="#b1">(2)</ref> where N designates the number of data objects within the current cluster.</p><p>The C-means algorithm iterates over all data objects obj i and verifies for each object obj i if there exists a cluster clust j the center c j of which is closer to x i than . If there are such clusters the object will be added to the cluster that is closest to the object. Otherwise a new cluster is generated with the object x i as its only member. After assigning the object to the cluster's center position will be updated, i.e. the center will shift.</p><p>A major disadvantage of the C-means method is the user defined selection of the cluster threshold value . Eventually, the determination of a proper value for could be very difficult. With too large a value clusters will contain objects which do not correspond. On the other hand, too small a value will result in clusters each holding only one single object. Another drawback is the sensitivity of the algorithm to the order of traversal of given objects. In particular, the choice of the starting object has a great influence on the resulting cluster distribution.</p><p>The cost of the C-means algorithm is of order O(n 2 ) being defined by the worst case scenario, with each object located in its own cluster. But due to the very simple operations the C-means method relies on, it is very fast in general. K-means' iterative behavior and the apriori unknown number of iterations makes the cost estimation more difficult than for the C-means algorithm. In each step, the algorithm calculates the distances between all n object and the k cluster centers, i.e. calculates nk distances. Since k is constant, the costs are of order O(n) per iteration step.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K-Means</head><formula xml:id="formula_1">δ abs x i c j - ( ) δ ≤ c j 1 N ---- x i i 1 = N ∑ ⋅ =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Methods</head><p>Hierarchical clustering methods (HCM) are commonly used in the area of information visualization and data mining. In contrast to partitional clustering methods, that subdivide a set of objects into a certain number of clusters, hierarchical clustering generates a nested sequence of partitions. We call this a cluster tree (as shown in <ref type="figure" target="#fig_5">fig. 4</ref>).</p><p>An agglomerative hierarchical clustering algorithm starts with n atomic clusters, each containing exactly one object. At each step, the algorithm merges the two most similar 1 clusters and thus decreases the total number of clusters by one. These steps recur until only one single cluster, containing all objects, remains. Any two clusters generated by such a procedure are either nested or disjoint. In contrast, divisive hierarchical clustering reverses the process by starting with a single cluster holding all objects and subdividing it into smaller sets <ref type="bibr" target="#b15">[16]</ref>.</p><p>Many variants of agglomerative hierarchical clustering methods are known, mainly differing in the definition of the metric applied in updating the similarity between existing and merged clusters.</p><p>Along with the incremental algorithms mentioned above, there is a group of non-incremental clustering methods (e.g. CLUS-TER/S <ref type="bibr" target="#b22">[22]</ref>). The discussion of those algorithms is beyond the scope of this paper, and their methods are not considered in the following.</p><p>In the remainer of the section we shall discuss two different hierarchical clustering methods: the single linkage method and the complete linkage method. For an in-depth description we refer to <ref type="bibr" target="#b25">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Linkage Method</head><p>Another straightforward and quick clustering technique is called single linkage method (SLM) or nearest neighbor technique. For this algorithm we define the distance between two clusters as the minimal spacing between two arbitrary objects, each located in two different clusters. Assume that d ij is the distance between object obj i from cluster clust i and object obj j from cluster clust j . Then, the distance D ij between clusters clust i and clust j is defined as .</p><p>That means we measure distances between two clusters as the distance of the closest pair of objects each belonging to a different cluster. The SLM synthesizes clusters analogous to the general description found at the beginning of this section. A problem of SLM is the algorithm's tendence to generously accept object chains as clusters. Assume we have an object configuration like the one shown in <ref type="figure" target="#fig_6">fig. 5</ref>. The SLM would string objects between A and B to a chain. Thus, objects A and B will be assigned to the same cluster. SLM generates three clusters (drawn with a solid line). Building only two clusters (shown with a dotted line) would be a superior solution.</p><p>Unlike centroid-based algorithms, this method could discover clusters of arbitrary shape and different size. Unfortunately, the procedure is highly susceptible to noise and outliers.</p><p>To build up the cluster tree, the single linkage method has to compute the pairwise distance between every two objects, i.e. supposed we have n objects, we have to perform distance evaluations per iteration, which dearly is of order over all n iteration steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complete Linkage Method</head><p>Another clustering method, the complete linkage method (CLM), takes into account the chain formation and defines the distance between two clusters D ij as the maximal distance between two of their objects <ref type="formula">4</ref>Supposed we run the CLM on an object topology that already contains two shorter cluster chains, the distance between the two clusters is now defined by the two furthest away objects not located in the same cluster. This is equal to the distance of the outermost object on the one side of a chain and the outermost object on the other side of the other chain. Thus, chain formation is suppressed.</p><p>As mentioned at the beginning of this section, there are many other well known clustering algorithms, i.e. BIRCH <ref type="bibr" target="#b24">[24]</ref>, which is basically an extension of the K-means clustering, but adequately addresses the problem of large datasets. CURE <ref type="bibr" target="#b10">[11]</ref> remedies the drawback of single centroid representation by taking advantage of a multi-centroid representation of clusters. Hence this algorithm is more robust to outliers and identifies clusters varying in size and having non-spherical shapes. A recent approach is called CHAME-LEON <ref type="bibr" target="#b16">[17]</ref>, a hierarchical clustering algorithm that measures inter-cluster similarity based on a dynamic model. In addition to other algorithms, CHAMELEON clustering is based not only on vicinity of objects but also considers corresponding connectivity information. This combination results in a robust handling of data that consists of clusters being of different shape, size or density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cluster Visualization Methods</head><p>There is quite a large number of algorithms and systems treating the subject of cluster visualization. Practically all of them take the problem of cluster visualization simply as a layout problem, thus focusing on optimizing the computation and spatial grouping of crowds of single data objects. The visualization then is limited to drawing just a simple shape (dot, icon, glyph, etc.) for each data cluster tree with 4 levels generated by an agglomerative, hierarchical clustering algorithm <ref type="bibr" target="#b0">1</ref> In the current context similarity of two objects is defined by the inverse of their distance. Thus the algorithm merges the two closest clusters in each step.  <ref type="figure" target="#fig_7">fig. 6a</ref>). Thus, the actual visual clustering process is rather done by the user's perceptual system than by the visualization system itself.</p><formula xml:id="formula_3">A B C D E F G H F G H D E A B C A B C D E F G H A B C D E F G H A B C D E F G H a) b) D ij min d ij ( ) =</formula><formula xml:id="formula_4">B A n n 1 - ( ) 2 ⁄ ⋅ O n 3 ( ) D ij max d ij ( ) = object (shown in</formula><p>There are two reasons to go a step further: first today's graphics hardware, though current progress in this area is tremendous, is not yet ready for the data volumes we would like to address with present data management systems (i.e. data warehouses). Second, the user's perceptual system should be relieved of gathering single points to a cluster object. In order to speed up the decision making process and to increase the decision's quality, cluster visualization has to take the step to the next higher level of visual representation.</p><p>Only a few approaches make an effort in this direction. Some of the systems attempt to break down complexity by running a preclustering algorithm on the initial dataset. Afterwards the system confines itself to displaying only objects on a chosen clustering level, where clusters are represented by a simple shape at the position of their centroids. Doing so, we lose most of the information contained in a cluster. Only the cluster's position is visible to the user. Information about the internal object distribution, including size, orientation and variation is visually not available to the user.</p><p>Initial work about a more powerful visualization method is reported in <ref type="bibr" target="#b12">[13]</ref>, where wrapping hyperspheres accomplish the clustering of data objects. Furthermore, some of the authors of this paper proposed a PCA-based technique in <ref type="bibr" target="#b20">[20]</ref> where the basic idea was to wrap ellipsoids around each object group whose shape is controlled by the principal components of the respective cluster (shown in <ref type="figure" target="#fig_7">fig. 6b</ref>). In either approach restriction to a quadric surface representation of the clustering hull represents an unnecessary restriction. The internal object distribution is only rough approximated, as well in size as in orientation. This drawback gets addressed by an algorithm called BLOB-clustering <ref type="bibr" target="#b20">[20]</ref>, the fundamental idea of which is to use blob functions combined with a marching cube <ref type="bibr" target="#b2">[3]</ref> algorithm to represent the enfolding cluster surface (see <ref type="figure" target="#fig_7">fig. 6c</ref>). The generated shape represents the distribution of the included data objects in the best possible manner.</p><p>However, all of the cluster visualization methods mentioned above are limited to work only based on partitioning clustering algorithms. Non of them takes advantage of the hierarchical information cluster structures inherently contain. Therefore, we propose a new simple and fast clustering technique that has its strength in the visualization of hierarchical clustering structures, say cluster trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">H-BLOB: HIERARCHICAL CLUSTER VISUALIZATION USING ISOSURFACES</head><p>The H-BLOB (Hierarchical BLOB) algorithm is considered to be a direct derivative of the BLOB clustering method, extended by the capability to handle hierarchical settings. In fact, it is a combina-tion of techniques and algorithms described in preceding sections, each one applied on a preferable subtask corresponding to their strengths. The algorithm can be split into two stages, starting with an analytical clustering process building up a cluster tree, which is followed by the hierarchical cluster surface computation in combination with the visualization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stage I: Edge Collapse Clustering</head><p>Inspired by the persuasive idea of the edge collapsing algorithm presented in <ref type="bibr" target="#b14">[15]</ref>, we propose a new simple and efficient clustering method, called edge collapse clustering (ECC).</p><p>The algorithm we present, belongs to the category of agglomerative hierarchical clustering methods. Thus, the general structure is very similar to the methods presented in Section 2.2.</p><p>In contrast to the linkage methods the ECC bases on centroids; hence, it only works in coordinate space. We define the distance D ij between two clusters clust i and clust j as the distance between their centroids c i and c j .</p><p>The process of cluster merging works analogous to the process shown in Section 2.2, but with the following extension:</p><p>All clusters clust i obtain a weight w i corresponding to the number of objects contained in clust i . The weight w i is initialized with a value of one. With each iteration, the algorithm merges the two closest clusters, i.e. the pair of clusters with minimal distance D ij , into a new one, called clust new with centroid c new . At the same time, the parameters of the new cluster are updated corresponding to the formulas below:</p><formula xml:id="formula_6">(6)<label>(7)</label></formula><p>If the two clusters are of different weight, the new cluster will be located closer to the heavier, i.e. larger cluster, which is desirable in praxis. <ref type="figure" target="#fig_8">Fig. 7</ref> illustrates the algorithm by means of an example with 5 objects spread on a plane. Each iteration step is shown on a separate line, with the actual object arrangement in the left half and the current cluster tree on the opposite side. Starting with 5 single objects, the ECC algorithm merges them into a single cluster after the same number of iteration steps. The thicker line, highlights the edge to be collapsed next.</p><p>Since each cluster is defined by its centroid only and as the distance metric depends only on the centroid's coordinates, every two clusters are virtually interconnected with exactly one edge of length </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ij abs c i c j -</head><formula xml:id="formula_7">( ) = c new c i w i c j w j + w i w j + -------------------------- = w new w i w j + =</formula><p>hierarchical structure of a cluster tree. The computational complexity for each iteration step is defined by the corresponding number of clusters. This is an advantage compared to the linkage algorithms, which always operate on the initial set of all single objects. Hence, the ECC algorithm is computational less complex than linkage methods.</p><p>The disadvantages concerning the fragile user-driven parameter preselection of the C-and K-means methods do not apply for ECC. Although this technique is partly based on centroids, it is more stable with respect to unconstrained shapes and different cluster sizes than C-and K-means. The effect of chain formation does not occur for ECC.</p><p>Unfortunately, the ECC is still in the same polynomial order as the linkage techniques. It also preforms n iterations steps and computes in each of the steps distances. Since ECC computes distances based on centroids we get a triangular cost scheme over all iterations, which results in an complexity of order regarding the number of computed distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stage II: Cluster Tree Visualization</head><p>The cluster tree generated as a result of the first stage must now be visualized, Each hierarchy level should be handled separately, i.e. we compute a separate surrounding surface for each cluster at a specific hierarchy level.</p><p>As a basic idea we devote resources to the BLOB algorithm described in <ref type="bibr" target="#b9">[10]</ref>. The fundamental idea of BLOB clustering is to give each object a spatial extension by attaching a spherical primitive to its center. In general a primitive is a working model comprising a parameterized oriented shape and a corresponding 3D field function . Primitives and their parameterization will be explained in more detail in the next section.</p><p>To compute a BLOB surface, we superimpose all field functions in space and accordingly run a marching cube algorithm <ref type="bibr" target="#b2">[3]</ref> to extract the implicit surface at a given isovalue. The subsequent sections explain how we extend this algorithm in order to handle hierarchical cluster structures efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization using BLOBS</head><p>As a straightforward approach to visualize a single cluster on a given cluster level, we could assume a scenario where a primitive is attached to each of the cluster's objects. Supposed we choose a skillful parameterization of those primitives, we could accomplish an isosurface, that fully encloses all objects and the visualization problem would be superficially solved.</p><p>Even if this approach results in fair visual results, it has a tremendous handicap. For very large clusters holding a huge number of single objects the computational cost rises excessively. That effect occurs because in order to perform an isosurface extraction we have to evaluate the superimposed field at given points in space which involves the evaluation of the field equation for every single primitive. The problem could be eased if we find a way to limit the number of primitives during visualization.</p><p>We consider the cluster tree shown in <ref type="figure" target="#fig_9">fig. 8</ref>, subdivided into 3 hierarchical levels. The topmost cluster on level I contains all 5 objects (ABCDE). If we intend to visualize this cluster, we have to take into account five different primitives -one for each object.</p><p>To limit the number of primitives we propose the following approach: instead of attaching primitives to every single object, we just consider the objects one level below the level of interest. Thus, in order to visualize the cluster in level I we attach primitives to the level II cluster objects, i.e. to the clusters (ABC), (D) and (E). Or, if we aim to visualize clusters of level II, we utilize cluster objects from level III and so forth.</p><p>To provide for satisfactory results, we need to extend the characteristics of the primitives used, which -in the original BLOB paper <ref type="bibr" target="#b9">[10]</ref> -were restricted to be of radial symmetric shape. This is due to the fact that in contrast to the previous BLOB clustering algorithm primitives now have to account for the properties of a whole object set rather than of only one single object. We suggest the extension of our concept of a primitive to an elliptical feature, the so called ellipsoidal primitive. The following sections will give a more exact definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extension to Ellipsoidal Primitives</head><p>Ellipsoidal primitives are a direct extension to the common primitives determined in <ref type="bibr" target="#b9">[10]</ref>. The characteristics of an ellipsoidal primitive is specified by an ellipsoidal shape and the field function . For the definition of the shape and the computation of its size, orientation and position we refer to <ref type="bibr" target="#b20">[20]</ref>. The definition of is (8)  <ref type="table">I  II  III  IV  V   I  II  III  IV  V   I  II  III  IV  V   I  II  III  IV  V   I  II  III  IV</ref>  f i x y z , , ( )</p><formula xml:id="formula_8">V n n 1 - ( ) 2 ⁄ ⋅ O 1 6 ---n 3 1 6 ---n -     f i x y z , , ( )</formula><formula xml:id="formula_9">A B C D E ABCDE ABC D E I II III f i f i f i x y z , , ( ) b i if x y z , , ( )lies inside ellipsoid b i e a i d i x y z ,<label>, ( ) 2 -</label></formula><p>⋅ else      = where is the distance to ellipsoidal surface, defines the maximal magnitude of the function inside the ellipsoid, and influences the descent of the field function. <ref type="figure" target="#fig_10">Fig. 9</ref> compares the fields of a spherical symmetric primitive to the field of a new ellipsoidal primitive defined by eqn. (8) on the basis of their isolines. Inside the red area the field has a value of . The field of a single ellipsoidal primitive could be described as follows: for all points inside the ellipsoid the value of the field is uniformly . Starting at the surface of the ellipsoid the field descents exponentially and monotonously as a function of the distance to the surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation of Ellipsoidal Gaussian Fields</head><p>An ellipsoid is defined by its scaling matrix S, its rotation matrix R and its center . From the diagonal elements of the scaling matrix result the three half axes , and . Transforming the ellipsoid into the origin will simplify subsequent formulas. In order to compute the value of the field function at a point from eqn. <ref type="bibr" target="#b7">(8)</ref>, the coordinates of have to be transformed: first, is translated by the negative values of vector according to .</p><p>Then, is rotated by the inverse rotation matrix R:</p><p>To gather the distance between the transformed point and the surface of the ellipsoid, it is necessary to intersect the connecting line between the center of the ellipsoid -which is equal to the origin -and the point with the ellipsoidal surface. To this aim the line is parametrized with running from 0 to 1.</p><p>A point is located on the surface of the ellipsoid, if the ellipsoidal equation evaluates to 1:</p><formula xml:id="formula_13">(12)</formula><p>Substituting eqn. <ref type="bibr" target="#b10">(11)</ref> into eqn. (12) yields for the intersection point :</p><formula xml:id="formula_14">(13) If</formula><p>, then the point lies within the ellipsoid. With it could be computed using transformed coordinates:</p><p>Parameter Definition for Ellipsoidal Primitives</p><p>The ellipsoidal primitives contain the two parameters and , which control the descent and magnitude of the corresponding field function. These two parameters should be determined automatically, because a configuration by the user may be longsome and instable. Whenever possible, the algorithm should disburden the user from such decisions.</p><p>The simplest approach would be a static setting for these two parameters. Unfortunately, this idea is not acceptable because the visualized clusters vary too much in both scale and position. Thus, it is impossible to find values that delivering satisfactory results under all circumstances. The parameters have to set in context with the underlying ellipsoid. We will discuss two possible approaches solving this problem:</p><p>1. The heavier a cluster is, i.e. the more objects it contains, the larger becomes the value of the magnitude of the ellipsoid primitive's field function.</p><p>2. The larger the maximum extension of the ellipsoid is, the weaker becomes the descent of the ellipsoid primitive's field function.</p><p>Experiments have shown, rule one can lead to very big BLOB surfaces, e.g. if the object distribution in space is dense. Hence, this rule was dropped and a fixed value is assigned to (e.g. =1.0). The second rule on the other hand is considered to provide an relevant visual feedback. The parameter is defined as <ref type="bibr" target="#b14">(15)</ref> where the value for the constant factor must be determined experimentally, yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Determination of Isovalues to ensure connected BLOB-Surfaces</head><p>According to <ref type="bibr" target="#b9">[10]</ref> a BLOB's shape is strongly influenced by the corresponding isovalue . The smaller this value, the larger the BLOB's extension will get. In order to ensure that a BLOB encloses all its objects the correct choice of c is crucial. In this section, heuristics for the automatic determination of isovalues is presented.</p><p>Take the example of <ref type="figure" target="#fig_0">fig. 10a</ref> where an enclosing BLOB surface for three objects A, B and C has to be computed. The indicated number on the connecting edges illustrates the minimal value of the superimposed field along the edge. In order to assure as tight a BLOB as possible we have to look for the largest iso-value which still guarantees that the BLOB does not break apart. <ref type="figure" target="#fig_0">Fig. 11</ref> shows three possible cases for the choice of an isovalue. On the left hand side, the chosen value results in the illustrated split-up into two subclusters because is bigger than the minimal field value on edges AB and BC. On the right hand side, too small an isovalue does not provide for a distinctive shape.   </p><formula xml:id="formula_16">p t t ( ) x t y t z t , , ( ) T t x″ ⋅ t y″ ⋅ t z″ ⋅ , , ( ) T = = p t x t 2 H a ------ y t 2 H b ------ z t 2 H c ------ + + 1 = t</formula><formula xml:id="formula_17">a i b i b i a i b i b i a i a i a 0 ellipsoid′s dimensions -------------------------------------------------------- = a 0 c 0 &gt; A C B 0.5 0.7 0.9 b) c 0.8 = a)</formula><p>The case illustrated in the middle seems ideal. Choosing -bigger than the minimum on edge AB but smaller than the minimal value on BC -results in a tight single BLOB surface enclosing all objects.</p><p>This example shows how to find an ideal isovalue: look for the biggest value that still guarantees for a single enclosing surface. This is equivalent to choosing a value such that all objects are connected by edges with minimal field value bigger or equal to the isovalue.</p><p>There are two problems in this approach: first, graph theory shows that it is very expensive to find a minimal spanning tree, at least if cluster sizes approach several hundred objects. Second, finding the minimal field value on interconnecting lines is expensive too, as it is impossible to find an analytic solution for arbitrarily superimposed fields. In the remainder of the section, we present an approach which in most cases yields suited isovalues. <ref type="figure" target="#fig_0">Fig. 10b</ref> shows a constellation of several objects of a cluster for which an enclosing BLOB surface has to be found. The red dot marks the center of the cluster. Intuitively, objects close to cluster center will not cause problems. In contrast thereof, it is troublesome to account for outliers -objects which are far apart from the cluster's center. Instead of looking for a minimal spanning tree for all of the cluster's objects we concentrate on the outliers. Therefore, we look for the minimal field value on the interconnecting lines between the outlier and the cluster center. <ref type="figure" target="#fig_0">Fig. 10b</ref> shows these lines highlighted in red. The smallest value found is regarded as a good approximation to the ideal isovalue.</p><p>We are left with the problem of finding the minimal field value on the lines between outliers and the cluster center. To this aim, we employ a Newton iteration scheme in order to find the zero crossings of the first derivative of the superimposed field function with regard to parametrization t of the interconnecting line .</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">6 )</head><p>The corresponding Newton iteration step is given by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. ( 1 7 )</head><p>As it is hardly possible to find symbolic expressions for the first and second derivative of the field function f, they are approximated in terms of central differences as follows: <ref type="bibr" target="#b17">(18)</ref> As the reader may have noticed, this procedure is not guaranteed to find the global minimum but is highly dependent on the choice of a favorable initial value . In order to find a good value for , we sample the value of the field function on equidistant points on the interconnecting line and choose to be the smallest value found during the sampling procedure. As a matter of fact, the outlined procedure still does not provide for finding the global minimum. However, practice has shown, that it yields suitable isovalues for non-pathological cases. For clusters of less than five objects the minimal spanning tree is computed which guarantees for the optimal isovalue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION AND RESULTS</head><p>This section documents a concrete implementation of the H-BLOB algorithm in the context of our information visualization research project, called IVORY. Following, on the basis of two examples we illustrate the visual performance and versatility of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>The algorithm has been fully implemented as a class library in Java2. For the domain of 3D visualization we apply Java3D in the version 1.1.2. All computational work is done on a standard PC completed with a hardware accelerated graphics subsystem (Open GL). Even for more complex examples we still get interactive frame rates.</p><p>Concerning an implementation of the H-BLOB algorithm there are two main issues. The first one affects the data structure used for the edge collapse clustering. Since this stage of the algorithm makes heavy use of point-to-point distance calculations and cluster merging, together with the higher order characteristic of the problem, makes a good choice difficult. Employing standard data structures quickly leads to a performance bottleneck, mostly because of memory shortage. Some promising work addressing this type of problems could be found in <ref type="bibr" target="#b5">[6]</ref> The second issue is about the isosurface extraction. In spite of the multi-resolution approach it remains the most time consuming part of the algorithm. Implicit surfaces may provide very nice shapes, but are computational very expensive. There are many sources available to this topic, but for our prototype implementation our choice was <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Small World Example</head><p>This first and small example illustrates the basic properties of the H-BLOB clustering algorithm. The scene consists of 5 single objects each represented by a colored sphere. We present two snapshots of the cluster tree buildup sequence including the corresponding implicit cluster surfaces generated by the H-BLOB algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Document Retrieval Visualization</head><p>This example is a from a real document retrieval research project. We applied our new technique to a hit list (result list) originate from an intranet document query. The number of single objects is 100. For the clustering stage a maximum of 20 clusters has been defined. From one picture to the next we respectively merge 50% of the clusters, what results in 6 hierarchy levels with 20, 10, 5, 3, 2, and 1 clusters. We show 4 selected images from this session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>The main contributions of this paper is a new hierarchical clustering algorithm called H-BLOB, which provides an efficient levelof-detail strategy and is consequently capable to cluster and visualize very large and complex data volumes. The algorithm is subdivided into two stages: Firstly, a simple and fast clustering strategy -based on edge collapsing -computes a cluster hierarchy. Secondly, improving this hierarchical structure, the next stage visualizes the clusters with nested implicit shapes. The key concept is an efficient multi-resolution setup, breaking down the structural and visual complexity of scenes. We have shown the algorithm's versatility by experimental results, demonstrating H-BLOB's capability to simplify and enhance the feasibility of cluster visualization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Clustering of a subset of objects performed with BLOBS. a) Initial object layout b) Clustered configuration with enclosing BLOB surface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>K</head><label></label><figDesc>-means belongs to the class of iterative clustering techniques. Choosing the K-means method we have to preselect the number k of clusters, the algorithm would generate. First k initial cluster centers are defined. An object obj i is assigned to the cluster clust j when its center c j is closest to the object position x i . In such a way, all objects are associated to exactly one cluster. At the beginning of the next iteration, the cluster centers c j of all k clusters are updated to the arithmetical average of all positions x i of associated objects. Thereafter, another assignment round starts using the recently computed cluster centers. The iteration loop stops if all cluster centers have converged into a stable position. The K-means method poses a problem concerning the selection of the initial positioning of the k Clusters. A unlucky choice could have great influence on the resulting object clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>a) Partitioning using C-means method with threshold , where the assignment of object x is undetermined. Object y, on the other hand, could not be assigned to any existing cluster. Therefore, it generates a new one. b) Completely clustered scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The same scene as infig. 2clustered with the K-means algorithm a) The iteration steps for the 3 cluster centroids. b) Resulting clustered layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>a) Probable object arrangement with 8 objects. b) Corresponding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Generation of chains applying the single linkage method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>D ij . Consequently, ECC takes advantage of the inherent bDifferent techniques to visualize clusters of data objects. a) cluster represented by a cluttered group of single objects b) visualization with ellipsoidal surfaces wrapped around clusters c) objects visually combined by a BLOB surface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>a) -e) Progressive edge-collapse algorithm. Red line indicates edge to be collapsed next. Current cluster tree levels (I-V) are shown on the righthand side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Cluster tree with three levels. It is a condensed view on the corresponding tree shown infig. 7ewithout displaying level II and IV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Isolines of a) a spherical, symmetric primitive and b) a new ellipsoidal primitive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>--------------------------------------------------------------------------------------------------a)Three objects for which an enclosing tight BLOB surface has to be found. b) Objects of a cluster with so-called outlier objects. The interconnecting lines between outliers and the cluster center are marked in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :t 0 t 0 Figure 12 :</head><label>11012</label><figDesc>left: iso-value too big, BLOB breaks apart middle: optimal iso-value, tight BLOB enclosing all objects right: iso-value too small, non-distinctive shapec ----------------------------------------------------------------------------------------------------------------------------------------------------------------= t 0 Small example showing the clustering process by means of 5 simple objects. Snapshot with 4 and 2 clusters are shown. Level indicates the hierarchy level in respect to the cluster tree.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">objects -4 clusters -level 2 5 objects -2 clusters -level</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This research has been made possible by the Advanced Engineering Center (AEC) of the UBS, Basel, Switzerland. Many thanks to Martin Roth and Andreas Hubeli for their extraordinary efforts in text editing and proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Background: mathematical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abramsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dov</forename><forename type="middle">M</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S E</forename><surname>Maibaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename></persName>
		</author>
		<idno>ISBN 0-19-853735-2</idno>
	</analytic>
	<monogr>
		<title level="j">Handbook of Logic in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1992" />
			<publisher>Clarendon Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Arrows, Structures, and Functors: The Categorical Imperative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Manes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="93" to="106" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Implicit Surface Polygonizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bloomenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Gems IV</title>
		<editor>P. Heckbert</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="324" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast Interactive 3-D Graph Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bruss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graph Drawing 95</title>
		<meeting>Graph Drawing 95</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1027</biblScope>
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster Analysis: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Odell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Econimics and Mathematical Systems</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<date type="published" when="1974" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic Euclidean minimum spanning trees and extrema of binary functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eppstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="1995-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Clustering Large Datasets in Arbitrary Metric Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>French</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining Very Large Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="1999-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Visualization of Multidimensional Data Sets using a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seibert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="145" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visualizing Information on a Sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Information Visualization &apos;97</title>
		<meeting>IEEE Information Visualization &apos;97<address><addrLine>Phoenix AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-10-24" />
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CURE: An efficient clustering algorithm for large databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualization of cluster hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Society for Optical Engineering</title>
		<editor>Erbacher, R. F. and Pang, A.</editor>
		<meeting><address><addrLine>Bellingham, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">3298</biblScope>
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
	<note>Visual Data Exploration and Analysis V, SPIE</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Case Study -Narcissus: Visualizing Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hendley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Information Visual-ization 95</title>
		<meeting>the IEEE Information Visual-ization 95</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="90" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive Graph Layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH Symposium, Proceedings ACM Siggraph Symposium on UI Soft-ware</title>
		<meeting>the ACM SIGGRAPH Symposium, Proceedings ACM Siggraph Symposium on UI Soft-ware</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH 1996 Proceedings)</title>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CHAMELEON: A Hierarchical Clustering Algorithm Using Dynamic Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eui-Hong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="1999-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-Organizing Maps Second Extended Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Series in Information Sciences</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Berlin, Heidelberg, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Comparative performance analysis of non-linear dimensionality reduction methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coomans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>James Cook University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Framework for Physically-Based Information Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eggenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics Workshop on Visualization &apos;97</title>
		<meeting>Eurographics Workshop on Visualization &apos;97<address><addrLine>Boulogne sur Mer, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">IVORY -An Object-Oriented Framework for Physics-Based Information Visualization in Java</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bielser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Information Visualization &apos;98</title>
		<meeting>IEEE Information Visualization &apos;98<address><addrLine>Research Triangle Park, NC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conceptual clustering of structured objects: A goal-oriented approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="43" to="69" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multidimensional scaling: history, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Lawrence Erlbaum associates</publisher>
			<pubPlace>Hillsdale, New jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BIRCH: An Efficient Data Clustering Method for Very Large Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 1996 ACM SIGMOD International Conference on Management of Data<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Clustering of Large Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chemometrics Research Studies Series</title>
		<imprint>
			<publisher>Research Studies Press</publisher>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
