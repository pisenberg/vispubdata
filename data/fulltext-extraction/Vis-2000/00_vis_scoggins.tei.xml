<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enabling Level-of-Detail Matching for Exterior Scene Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">K</forename><surname>Scoggins</surname></persName>
							<email>scoggir@wes.army.mil</email>
							<affiliation key="aff0">
								<orgName type="laboratory">U S Army Corps of Engineers Engineer Research and Develop-ment Center</orgName>
								<address>
									<addrLine>3909 Halls Ferry Road</addrLine>
									<settlement>Vicksburg</settlement>
									<region>Mississippi</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">NSF Engineering Research Center, 2 Research Boulevard</orgName>
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<region>Mississippi</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Machiraju</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Information and Science</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<addrLine>2015 Neil Avenue, Columbus</addrLine>
									<region>Ohio</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Moorhead</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">NSF Engineering Research Center, 2 Research Boulevard</orgName>
								<orgName type="institution">Mississippi State University</orgName>
								<address>
									<region>Mississippi</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enabling Level-of-Detail Matching for Exterior Scene Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>1.3.3 [Computer Graphics]: Picture/Image Generation -Viewing Algorithms</term>
					<term>1.3.6 [Computer Graphics]: Methodology and Techniques -Graphics data structures and data types</term>
					<term>1.4.7 [Image Processing and Computer Vision]: Feature Measurement -Feature Representation multiresolution model, level-of-detail, rendering, image metrics, perception, terrain visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This work presents a method to enable matching of level-of-detail (LOD) models to image-plane resolution over large variations in viewing distances often present in exterior images. A relationship is developed between image sampling rate, viewing distance, object projection, and expected image error due to LOD approximations. This is employed in an error metric to compute error profiles for LOD models. Multirate filtering in the frequency space of a reference object image is utilized to approximate multiple distant views over a range of orientations. An importance sampling method is described to better characterize perspective projection over view distance. A contrast sensitivity function (CSF) is employed to approximate the response of the vision system. Examples are presented for multiresolution spheres and a terrain height field feature. Future directions for extending this method are described.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many disciplines such as remote sensing, forestry management, military battlespace visualization, and environmental planning, use synthetic imagery at various levels of realism to visualize natural landscapes. McGaughey <ref type="bibr" target="#b11">[12]</ref> presents a survey of current landscape visualization software, oriented to forest management, which provide previously impossible levels of realism and rendering speed. Premoze et al. <ref type="bibr" target="#b17">[18]</ref> have recently created strikingly realistic forested alpine landscape images by texture mapping over many scales obtained from real imagery. Incorporating the complex structures found in nature is important. Daniel <ref type="bibr" target="#b3">[4]</ref> and Bergen et al. <ref type="bibr" target="#b1">[2]</ref> saw a strong correlation between increased complexity in many scene elements and realistic human observer response to synthetic forested landscape images compared to real images.</p><p>A hierarchy of complexity exists in unbounded outdoor landscape scenes. For example, only the largest features on the distant mountain in <ref type="figure">Figure 1</ref> are visible. At closer ranges more detailed surface topography, trees, rocks, and even blades of grass become important. The great amount of detail in natural environments requires a multiresolution approach to image synthesis in which components of the scene are modeled at resolutions matched to their resolution in the final image.</p><p>Recently level-of-detail (LOD) methods have been presented to continuously simplify surface meshes, a common object representation, by measuring errors due to object-space mesh simplifications projected to image-space <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>. Other approaches develop profiles of multi-state objects by pre-processing sample images from many views and computing an error profile from image features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>. During rendering, these methods select LOD by referencing the object error profiles. However, error metrics have been based on heuristics, such as image edge-content. The profilegeneration method presented here is based on more rigorous signal processing concepts.</p><p>Real images of distant objects lose detail as progressively Real fewer pixels sample the object. However, far away objects can be better resolved if a magnified image is used, such as with a telescope. An innovation described here proposes filtering a magnified synthetic view of an object, as in <ref type="figure">Figure 1</ref>, to remove high frequency content for use as an approximation to the true distinct view synthetic image. A frequency space error metric can then be used to characterize feature content in these image spectra for various LOD object models. Rather than employing this as an absolute error measure, we propose that the metric be normalized by the spectral content of high-resolution object images. In this way, metric values can be compared on a common scale and provide a ranking of LOD similarity to the highest resolution object for enabling LOD selection. The basic components of the process can be summarized as follows steps:</p><p>1. Identify a reference view distance z 0 for which object perspective distortion is not significant. This is generally obtained at the closest possible viewing distance.</p><p>2. Derive a view transform that magnifies the scene at the reference viewing distance so that it covers the maximum possible extent in the reference image.</p><p>3. Develop an error profile by rendering the scene from many view-points. Apply the multirate filtering method presented herewith to simulate the effect of many increased viewing distances. <ref type="bibr" target="#b3">4</ref>. Compute an error metric from the spectra of these images relative to images of a high-resolution object rendered under the same viewing and lighting conditions.</p><p>5. Utilize the resulting error profile during rendering to select LOD models that maintain error below a userspecified value.</p><p>Since the perspective transform can distort object geometry resulting in changes in object self-occlusion and projected area, it is possible that filtering will not always accurately reflect changes in image content with LOD and distance. This is the reason behind step 1, so that objects at even greater distances than z 0 will undergo progressively less perspective distortion. We also describe a method for adapting the error profile developed in Steps 3-4 to arbitrary view transforms for use in Step 5. The reliability of this filtering approach will also be discussed. While discrete LOD rendering methods have been criticized due to problems associated with LOD changes, they do provide opportunities for performance enhancements by cache optimization. A subset of LOD meshes can be brought into the cache based on predictions of viewing orientation and distance for future frames. We show that our method works well for discrete LODs in practice.</p><p>Section 2 reviews previous work in error-directed LOD. Section 3 develops the image error metric as a function of viewing distance, while Section 4 presents examples of multiresolution rendering where entity state selection is based on error metric values. Finally, Section 5 discusses future work expanding on current developments oriented to image synthesis of outdoor environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>Researchers have proposed <ref type="bibr" target="#b9">[10]</ref> that a hierarchy of scale exists in complex environments which could be employed to optimize image synthesis algorithms. An early implementation of hierarchical rendering is described by Becker and Max <ref type="bibr" target="#b0">[1]</ref> where bump maps, displacement maps, and bidirectional reflection distribution functions represent scene content at different scales. Maciel and Shirley <ref type="bibr" target="#b10">[11]</ref> implemented a LOD rendering method with simplified object impostors based on edge content in prototype object images. However the need for more sophisticated measures was expressed by <ref type="bibr" target="#b10">[11]</ref> as the basis for selecting LOD models. Projecting error due to local modifications of polygonal mesh objects to screen-space has been addressed by Hoppe <ref type="bibr" target="#b8">[9]</ref> for LOD selection. However, with few exceptions <ref type="bibr" target="#b20">[21]</ref>, image-space metrics which consider perception have not been applied to multiresolution rendering.</p><p>Sources of error in digital images have been studied in relation to image compression algorithms <ref type="bibr" target="#b16">[17]</ref> and perception metrics have been implemented for early termination of expensive global illumination <ref type="bibr" target="#b19">[20]</ref> and volume rendering <ref type="bibr" target="#b4">[5]</ref>. Rushmeier et al. <ref type="bibr" target="#b21">[22]</ref> employed frequency-space image metrics for comparing images from different global illumination algorithms to images of real spaces. Neumann et al. <ref type="bibr" target="#b14">[15]</ref> describe an image-space frequency analysis using a Monte Carlo sampling method.</p><p>Reddy <ref type="bibr" target="#b20">[21]</ref> has presented an image-space analysis method incorporating a measure of perceptually significant image change introduced by LOD polygonal objects. A pre-processing phase produces sample images of an object rendered from multiple view points using a range of LOD models. Image frequency content is estimated and used to develop an LOD error profile associated with the object, just as with color and other attributes. During interactive rendering, the error profile is referenced to select a LOD model such that the detectable image error is kept below a user-specified level.</p><p>The relation between the scaling of scene objects due to perspective and image-plane sample rate has been addressed for volume rendering by Mueller et al. <ref type="bibr" target="#b12">[13]</ref>. As pointed out in <ref type="bibr" target="#b12">[13]</ref>, the perspective projection imposes multiple sample rates as the distance between volume data grid point and the image-plane increases. Volume data is reconstructed on a progressively lowerresolution grid to remove high-frequency content beyond the Nyquist frequency of the perspective image-plane sample rate. Our method also studies the change in sample rate with distance.</p><p>In many surface-based perspective rendering methods, the view frustum is mapped to a cubic volume followed by an orthographic projection onto the image plane. As in volume rendering, surfaces at greater distances from the view point are sampled at progressively lower rates so that features beyond a high-frequency limit are not needed. This can be particularly evident in exterior scenes, where great viewing distances can result in very small image projections of important objects. The method described here proposes that LOD models can be selected by matching filtered images of each LOD object's continuous luminance signal to that of high-resolution objects similar in concept to that presented in <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MATCHING LOD APPROXIMATIONS TO IMAGE RESOLUTION</head><p>As the extent of real objects in an image decrease with viewing distance, pixels represent increasingly larger portion of the object. This produces a lowpass filtering effect in camera pixel values resulting from integration over the solid angle subtended by each image pixel. When sufficient frequencies have been removed from the highest resolution object's image due to distance, differences in spatial image content becomes minimal. Under such conditions the low-resolution approximations can better match the resolution of the object in the image. Pre-computing an error profile for multiple views and approximations allows selecting the best error match during normal rendering and results in reduced model complexity. Reddy has presented object error profiles by employing image segmentation to compute fundamental spatial frequencies in object sample images. During normal rendering, LOD models are switched when the fundamental spatial frequency of a neighboring LOD exceeds (or falls below) the threshold contrast sensitivity of human vision for the current resolution of the object in the image plane <ref type="bibr" target="#b20">[21]</ref>. Our approach differs in that we apply the Fourier transform to compute the image spectra. Spectra from low-resolution object images are then compared, in the mean-squared sense, to that of the highest-resolution object image. A frequency space error allows the CSF to be applied as a modulation transfer function for a comparison that is significant to the visual system. During rendering an approximation is selected that does not exceed a user-specified threshold for a given view and distance. This method is a more rigorous signal processing approach than previously taken.</p><p>The framework of computing the LOD error profile is now described. A conceptual image-based object replacement is first made and resolution limits on the image projection determined. Limits of the filter passband in frequency space are then related to resolution. An error metric is presented and a method to produce image sample viewing distances for perspective projected objects are also addressed in the remainder of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approximating View Dependent Loss of Detail by Filtering</head><p>Reduced resolution objects are most useful when viewed from a significant distance. Under such conditions, an orthographic closeup image of the object can be a good approximation to the perspective image <ref type="bibr" target="#b10">[11]</ref>. Let an object in world space (after applying the model transform matrix) be replaced by a local reference image of that object, as shown in <ref type="figure" target="#fig_0">Figure 2a</ref>. This is formed on a plane that is centered on and perpendicular to the view vector and tangent to the object point closest to the view point. The local image therefore replaces the volume of a trapezoid enclosing the world space near the object with an orthographic image oriented in the direction of the view vector. For simplicity, assume the view vector is along -z. This introduces some limitations but a subsequent analysis can still provide less than optimal bounds for simplification of off-axis objects, as described by Mueller et al. <ref type="bibr" target="#b12">[13]</ref>. The Shannon-Hartley sampling theorem states that frequency content in a continuous signal that can be correctly reproduced with sampling is related to the sampling rate <ref type="bibr" target="#b2">[3]</ref>. Also the Nyquist frequency, or highest frequency that can be represented in the discrete Fourier transform, cannot exceed one half the sampling rate. Real images are bandlimited by the image acquisition device. However synthetic images are not and aliasing can occur because frequencies are present in the continuous scene that exceed the Nyquist frequency, even for dense sampling.</p><p>To minimize this, the view transform for the reference image should be set so that the object occupies the largest possible portion of the image without clipping, thus providing the highest possible sampling rate. The object's 3-D convex hull, bounding box or sphere would be useful for automatically determining appropriate l, r, t, and b values shown in <ref type="figure" target="#fig_0">Figure 2b</ref>. However, in this work these were determined interactively by noting when the above conditions were met in sample images. As images are filtered, their frequency spectrum should be similar to that of a real image or a synthetic image in which anti-aliasing methods are employed.</p><p>(1)</p><p>Consider the conditions described above in the context of the OpenGL perspective transform <ref type="bibr" target="#b13">[14]</ref> in Equation 1. At z = z 0 the local image will map to the full extent of the near plane on the Normalized Device Coordinates (NDC) cube, shown in <ref type="figure" target="#fig_0">Figure 2b</ref>. Objects between the n and f planes of the view frustum will be reduced in size due to perspective scaling as z increases. In the horizontal direction, the mapping of local image corners (x l , y, z 0 ) and (x r , y, z 0 ), where y b ≤ y ≤ y t , follows from similar triangles in <ref type="figure" target="#fig_0">Figure 2a</ref> as:</p><formula xml:id="formula_0">(2)</formula><p>At z = z 0 , the local image will be equal to the actual image. For z&gt;z 0 the corners of the local image will map to image coordinates u l , u r , u t , and u b and be less than the full extent of the actual image. The horizontal image length d(z)=(u r -u l ) on the NDC near face occupied by a perspectively scaled local image at distance z&gt;z 0 can be found by applying Equation 1 to (x r , y , z , 1) T and (x l , y , z , 1) T and taking the difference of the resulting u components:</p><formula xml:id="formula_1">(3)</formula><p>Substituting the values for r and l from Equation 2 into Equation 3 produces:</p><formula xml:id="formula_2">(4)</formula><p>At the reference distance z 0 , the local image extends over the full width of the NDC cube face <ref type="figure" target="#fig_0">(Figure 2b</ref>), which has a total hor- </p><formula xml:id="formula_3">(x l , y, z 0 ) (x r , y, z 0 ) -z -1,-1,-1 1,-1,-1 1,-1,1 1,1,1 -1,1,1 -1,1,-1 far plane f near plane n v u left l right r (a) (b) top t bottom b -z { d sample image distance z 0 2n r l - ----------0 r l + r l - ---------- 0 0 2n t b - ---------- t b + t b - ----------- 0 0 0 f n + ( ) - f n - ------------------- 2nf - f n - ----------- 0 0 1 - 0 r x r n z 0 ---- l - x l n z 0 ---- - = = d z ( ) 2 -n r l - ( ) -------------- x r x l - ( ) z ------------------- = d z ( ) 2n z ------x r x l - ( )     n z 0 ----x r x l - ( )     1 - 2 z 0 z ---- = =</formula><p>izontal width of two (2) in NDC coordinates. During rendering, the perspective transform scales the local image, which is then orthographically projected onto this NDC face. This image will therefore occupy a smaller portion relative to the image at the reference distance z 0 . The ratio of the sample rate R(z), in samples per image, of the scaled local image to that of the full sample grid rate R(z 0 ) is thus in proportion to d/2:</p><p>Given this sampling rate, the horizontal Nyquist frequency of the perspective scaled local image's Fourier transform is:</p><p>This implies that the local image's frequency spectrum at z can be found by applying multiple box filters in frequency space to the reference image spectrum at z 0 , with passbands defined by Equation 6. <ref type="figure">Figure 1</ref> illustrates this process with rectangles, representing multirate box filters, that partition the image spectrum into progressively smaller regions, corresponding to spatially scaled images. As described later in Section 4.1, the sum of the magnitude of the image's Fourier coefficients in a partition is used as an approximate measure of feature content in images of the distant object. The spectrum is found without rendering the object at large viewing distances, which can introduce aliasing errors, and repeatedly performing the frequency transform. The same analysis follows for vertical frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Frequency Space Error</head><p>Equation 6 defines the maximum usable frequency of an image at z in terms of a reference Nyquist frequency and view distance z 0 . A measure is now required to quantify the difference between LOD images and corresponding views using the high-resolution object. Perception research has shown the vision system to be sensitive to spatial frequency content <ref type="bibr" target="#b15">[16]</ref>. Because of this, and the duality of spatial and frequency energy measures stated in Parseval's theorem, vision research, image compression, early termination of raytracing, and volume rendering have applied a mean squared error in frequency space to compare images. This work employs a similar formulation to <ref type="bibr" target="#b4">[5]</ref> as:</p><formula xml:id="formula_6">(7)</formula><p>where F L and F H are the power spectra of the LOD and high-resolution objects respectively. Dividing by the power in the high resolution object image normalizes the metric for meaningful inter-LOD comparisons.</p><p>Since they are relative to the scaled image's size, the full range of f ij is constant irrespective of the passband of the box filter and falls within -π to π. However, the actual memory arrays containing the power spectra remain at the size of the reference images and contain the passband as well as the truncated portion of the reference spectrum (see <ref type="figure">Figure 1</ref>). To sum only that portion containing a spectrum for a given scaled image, the limits of summation in Equation 7 must be reduced to not include that portion of the arrays outside the passband.</p><p>The limits of summation of i and j in Equation 7 for a reference image of M by M pixels are thus:</p><p>This corresponds to the reference image's Nyquist frequency. The Nyquist frequency of the scaled image is reduced by the factor z 0 /z in Equation 6. Therefore, the summation of i and j for scaled images must be adjusted to match the new Nyquist frequency. The new limits of summation are therefore given by:</p><formula xml:id="formula_7">(8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Profile Image Samples</head><p>Section 3.1 provides frequency spectra of displaced local images by multirate filtering to simulate increased viewing distances. Section 3.2 describes an error metric utilizing these spectra. Combining both with a sequence of reference images, produced for view points uniformly distributed over spherical coordinates (φ,θ) directed at the object, generates the error profile for a single LOD model. While the effect of view orientation is model dependent, the effect of perspective with viewing distance z can be predicted. In this case an importance sampling method is desirable to concentrate image samples at viewing distances with more significant effects.</p><p>Importance sampling as employed here selects independent variables (z) that segment a known function into equal area segments. Where the function changes rapidly the segments are narrow and samples are more closely spaced, and where the function changes slowly the segments are wider and further apart. The general effect of the perspective projection on an object space feature of size d o can be expressed by the function d(z): <ref type="bibr" target="#b8">(9)</ref> where d is the projected image extent of the object feature d o . The area under d from z i to z i+1 is given by the integral <ref type="bibr" target="#b9">(10)</ref> Solving for z i+1 yields: <ref type="bibr" target="#b10">(11)</ref> where is the constant area. Given an initial viewing distance z 0 and second viewing distance z 1 , the constant can be found by substituting and solving for . View distance values representing equal area segments in Equation 10 can now be computed with a recursive equation as: <ref type="bibr" target="#b11">(12)</ref> Computing the relative change in image-plane feature size (d in Equation 9) from the i th viewing distance to the next gives: <ref type="bibr" target="#b12">(13)</ref> Therefore the relative change in the screen size of a perspective projected object feature depends on the ratio z 0 / z 1 . This method provides z values that produce image scaling which is constant relative to the previous projected object size rather than previous viewing distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incorporating Contrast Sensitivity</head><p>The CSF defines visual contrast and spatial frequency threshold</p><formula xml:id="formula_8">R z ( ) R z 0 ( ) d 2 --- R z 0 ( ) z 0 z ---- = = f N z ( ) R z ( ) 2 ----------- R z 0 ( ) 2 ------------- z 0 z ---- f N z 0 ( ) z 0 z ---- = = = ε z F L f ij ( ) F H f ij ( ) - ( ) 2 j ∑ i ∑ F H f ij ( ) 2 j ∑ i ∑ ----------------------------------------------------------------- = M 2 ----- - i j , M 2 ----- ≤ ≤ M 2 ----- z 0 z ----i j , M 2 ----- z 0 z ---- ≤ ≤ - d z ( ) d o z ----- = d z ( ) z d z i z i 1 + ∫ d o z i 1 + z i log - log ( ) = z i 1 + z i κ d o -----     exp = κ κ z i 1 + z i z 1 z 0 ---- = d z i 1 + ( ) d z i ( ) - d z i ( ) z 0 z 1 ----1 -     ∆d d ------- ⇒ z 0 z 1 ----1 -     = =</formula><p>levels of the human cortex. Mannos and Sakrison <ref type="bibr" target="#b15">[16]</ref>, who measured the CSF by conducting a series of psycho-physical experiments on human subjects, developed the model given in Equation 14:</p><formula xml:id="formula_9">(14)</formula><p>where f s is angular spatial frequency in cycles per degree. The Fourier transform produces frequencies relative to the image size so a conversion is necessary from cycles per pixel (or image). In image acquisition systems Equation 14 corresponds to the system modulation transfer functions or MTF. Equation 14 is evaluated at spatial frequencies relative to the image. Since the spectrum used is that of the reference image frequency indices must be scaled when computing the CSF to account for the implicit scaling of the reference frequency spectrum as viewing distance increases. The scale factor for viewing distance is:</p><formula xml:id="formula_10">(15)</formula><p>The CSF modulated error metric is now given by a modified Equation 7:</p><formula xml:id="formula_11">(16)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>Object profiles are developed with respect to the object's coordinate system. To be useful with general view transforms, view vectors in world space must be transformed back into the object's model space for quantization into sample view orientations. The view vector can be found by transforming the object's local coordinate center to world coordinates and forming a vector from that point to the eye point at the origin. This vector is then transformed back into the object's model coordinate system, where φ, θ, and z are computed and associated with the nearest sample view. The quantized coordinates are used as indices into the LOD error table.</p><p>Profile images are produced using a view transform that magnifies the object to reduce aliasing. For a general non-magnified field-of-view (fov) not equal to that of the reference image, the relationship between viewing distance and metric error changes. This can be accounted for by noting how changing the values of r, l, t, and b changes the projection d on the NDC cube in Equation 3. If r and l are multiplied by a magnification factor m = r' / r = l' / l the new projection d' is given by: <ref type="bibr" target="#b16">(17)</ref> and similarly for t and b. Therefore, the profile can be employed with a different fov by use of an effective view distance rather than z when referencing the LOD error table.</p><p>An LOD error table is an array of error values stored in order of LOD number, spherical coordinates (φ,θ), and distance z. Calculation of the error metric value err, shown as pseudo-code in <ref type="figure">Figure 3</ref>, is performed for a user-specified range of spherical coordinates, for each LOD available, and for z-values generated by the method of Section 3.3. The maximum z-value is determined by the minimum value that the frequency index i can take in Equation 8. A value of five (5) was selected as the minimum value for |i| in the test cases presented here, which corresponds to a 10x10 pixel image impostor for the object at the most distant view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>The algorithm in <ref type="figure">Figure 3</ref> presents the metric calculation for a single view orientation (φ,θ). First a reference image of the highest resolution object available is rendered at distance z 0 , multiplied by a Hanning window <ref type="bibr" target="#b18">[19]</ref>, and subject to a Fourier Transform to yield the image spectrum F H . A loop over the range of available LOD object models is then entered. In each iteration the LOD image is rendered at distance z 0 and the corresponding power spectrum computed just as for the high-resolution image. Frequency indices f and f i are then initialized to the maximum index of the reference image's spectrum, corresponding to its Nyquist frequency. The following while-loop performs the scaling, measuring power in progressively smaller partitions of the spectrum to simulating distant views as described in Section 3.1. An array C of CSF values, with magnitudes corrected to correspond to the scaled image frequency spectrum, is then computed using an appropriate correction factor (see <ref type="bibr">Equation 15)</ref>.</p><p>Only the total signal power in the passband of the box filter is required for the metric. Therefore summations are performed over the passband (in horizontal and vertical directions) of the high-resolution image and the LOD image multiplied by corresponding C values. This step is indicated by the left bracket in <ref type="figure">Figure 3</ref>. However in practice real-valued images have symmetric spectra reducing the summation by one-half. The current error metric value err is computed and the next z-value in the view distance importance sampling sequence is calculated as described in Section 3.3. The size of the frequency-space box filter passband is reduced by computing a new value of frequency f based on the reference viewing distance z 0 and the new distance z. In this way, multiple box filters, corresponding to multiple sampling rates, partition the spectrum into increasingly smaller regions about the frequency-plane origin as shown in <ref type="figure">Figure 1</ref>. The while-loop terminates when the minimum desired frequency index f min is reached. The process is repeated for each LOD model and then for all view orientations to develop the LOD error profile table. Directional lighting effects were incorporated into each set of the reference images by an object order rendering algorithm using OpenGL. Also, flat shading was utilized here, which introduces increased frequency content in images and is taken to represent a worst-case condition. In the current implementation, profiles are light-direction dependent and must be re-created if lighting orientation changes. However, the metric should be insensitive to lighting intensity because profile images are normalized by a reference high-resolution object model image. Non-directional light sources can induce illumination variations over the surface which are not accounted for in the current method. However such an approximation in outdoor environments may be acceptable and this limits the number of image samples required for a single lighting condition. Ways to overcome this limitation are discussed under future work.</p><formula xml:id="formula_12">C f s ( ) 2.6 0.0192 0.114f s + [ ] 0.114f s ( ) 1.1 - [ ] exp ( ) = α f N z ( ) f N z 0 ( ) --------------- = ε z C f ij α ⁄ ( ) 2 F L f ij ( ) F H f ij ( ) - ( ) 2 j ∑ i ∑ C f ij α ⁄ ( ) 2 F H f ij ( ) 2 j ∑ i ∑ ------------------------------------------------------------------------------------------ = d' 2n r l - ( ) -------------- x r x l - ( ) mz ( ) ------------------- = mz</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sphere Snowflakes</head><p>Two examples are now presented to illustrate the metric-based LOD rendering algorithm. While these are oriented to surface meshes the error metric profiling is independent of how the reference image is created. The first example is the sphere snowflake, from the Standard Procedural Database <ref type="bibr" target="#b7">[8]</ref>. The snowflake object is composed of multiple spheres individually selected from a set of LOD sphere models. Examples of individual LOD spheres and a snowflake object are shown in <ref type="figure">Figure 4</ref>. An error profile was developed of a single sphere view(φ=0, θ=0) but with multiple distance from the method described above. The time required for profile calculation was 17 minutes on a dual processor 600MHz Pentium III with 128K of memory running Linux. <ref type="figure">Figure 7a</ref>-b (see color plate section) are perspective images consisting of many snowflakes objects arranged in a spiral pattern about the view (-z) axis. The reference viewing distance z 0 at which perspective became insignificant was selected interactively to be nine (9) sphere diameters. A fov of ten (10) degrees was selected for rendering, providing a magnified view of distant snowflakes. Each snowflake in the spiral is drawn at a geometrically increasing view distance as described for importance sampling in Section 3.3. The snowflake consists of multiple spheres selected from a suite of nineteen <ref type="bibr" target="#b18">(19)</ref> LODs. Since individual spheres are scaled down in size at each higher level of the snowflake, the rendering algorithm incorporates a size correction factor similar to the distance factor in Equation 17. The scale factor produces an effective viewing distance for selecting an appropriate sphere model from the LOD error table during rendering. <ref type="figure">Figure 7a</ref> is the snowflake spiral rendered using only the high resolution sphere, composed of 394,212 triangles, for a total of 9,451,008 triangles per high-resolution snowflake. <ref type="figure">Figure 7b</ref> is the same sequence with snowflakes composed of LOD sphere models selected from the profile that includes CSF. <ref type="figure">Figure 5</ref> is a plot of polygon count for each snowflake rendered in the <ref type="figure">Figure 7b</ref>, also including results without use of the CSF. A profile error of 10 -6 was selected for the NO CSF image and 10 -5 for the CSF image. Error thresholds were selected as the error value of the most detailed LOD model at the closest range. While ad hoc this method attempts to identify the maximum error that is acceptable, given the number of LOD models available. Note that even the nearest snowflake in the LOD model has fewer faces due to the reduced size of higher level spheres. Also the polygon count decreases monotonically with increasing viewing distance. Rapid variation of error at nearer viewing distances is captured by our importance sampling method, with relatively constant LOD values selected at increased viewing distance.</p><p>Comparing <ref type="figure">Figure 7b</ref> to <ref type="figure">Figure 7a</ref> reveals that the smallest spheres are maintained longer in the LOD image compared to the full resolution sphere image, where the projected image size the small faces approaches zero. To better illustrate how LOD models change with size and distance, <ref type="figure">Figure 7c</ref> is a color-coded reproduction of <ref type="figure">Figure 7b</ref> based on LOD number. Snowflake spheres with red hues are composed of higher complexity LOD models while green hues indicate lower complexity spheres. <ref type="figure">Figure 7d</ref> is the error image of <ref type="figure">Figure 7a</ref> and 7b, illustrating how LOD spheres better maintain the integrity of the snowflake at greater distances. Supersampling can also reduce the loss of spheres due to small projected polygon size. However, by better matching the size of object elements to the image resolution our technique attacks the root of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Terrain Data Set</head><p>To illustrate the application of this method to an exterior scene object, a feature in digital elevation data for an area near Flagstaff, Arizona was selected for use as a example data set. The feature is a prominent mountain in a triangulated irregular network (TIN) created from US Geological Survey 7.5 degree DEM data using freely distributed software <ref type="bibr" target="#b5">[6]</ref>. The mountain's peak vertex was identified and a region growing method utilized to separate mountain faces from that of the surrounding lower elevation. The mountain grid was then projected into lower complexity LOD models using the quadric error simplification software made available by <ref type="bibr">1 6</ref> 16 snowflake Garland <ref type="bibr" target="#b6">[7]</ref>. A set of seven (7) LOD mountain grids were produced from the mountain grid extracted from the terrain TIN with simplification software <ref type="bibr" target="#b6">[7]</ref>. The original high resolution model consisted of 8,538 faces. LOD models were created ranging from 1,000 faces for LOD 1 to 7,000 faces for LOD 7 in steps of 1,000. LOD error profiles were generated for zenith angle θ = 0 to 90 in steps of 15 degrees and azimuth angle φ = 0 to 360 in steps of 30 degrees. This resulted in 6 and 12 quantization levels respectively for zenith and azimuth spherical coordinates. While view sampling in this manner is straight-forward, it results in a non-uniform sampling of the object surface. More sophisticated view sampling methods are under consideration for future work. The method of Section 3 was applied to compute distant view errors for multiple z values. The total time for generating error profiles was 100 minutes on a dual processor 600MHz Pentium III with 128K of memory running Linux.</p><p>The reference viewing distance z 0 was selected interactively by observing at what distance orthographic and perspective images were judged to be similar. This was taken to be z 0 = 135km. The first distance in the importance sampling sequence, which determines the z-spacing for box filtering, was selected as z 1 = z 0 + 50km. This was done primarily to limit the number of error metric calculation to 12 for a given view orientation. No specific guidance was developed for selecting this initial sample spacing however. Using Section 3.1 to compute the view frustum planes from the desired z 0 and the object size resulted in a fov of 7 degrees for profile development. For <ref type="figure">Figure 8</ref> (see color plate section) a 30 degree fov was employed during rendering. Effective zvalues, relative to the profile image 7 degree fov, were computed for the figure image fov by applying Equation 17 to profile image view frustum coordinates.</p><p>Since the metric is computed for many view orientations, LOD selection should be sensitive to image viewing orientation as well as distance. <ref type="figure" target="#fig_2">Figure 6a</ref> plots the error value versus distance, averaged over all LOD models, in the error quantization table for two (2) viewing angles. Clearly the graph indicates that the error has a view-dependence, resulting from variations in surface structure and lighting. Also faces oriented away from the light source can result in lower error. View-dependent error can result in lower complexity models being selected for fixed z as a function of view angle. <ref type="figure">Figure 8</ref> was produced to illustrate the LOD rendering method in action. The mountains in <ref type="figure">Figure 8a</ref> were selected from a sequence in which the frames were uniformly spaced over the viewing distance. The graph in of <ref type="figure" target="#fig_2">Figure 6b</ref> was plotted using the full sequence of frames. Discontinuities in the curve denote changes from one LOD to the next. Also note the effect of using a large error threshold (given to the right of the NO CSF and CSF labels) for CSF rendering, selected in the same way as with the snowflake images. Also, because quantized viewing distance levels were selected by importance sampling the effect of perspective projection, more levels were available at closer distances where the effect is greatest, as the graph in <ref type="figure" target="#fig_2">Figure 6b</ref> illustrates. The complete animation sequence is available on the world wide web at http://www.erc.msstate.edu/~randy/Vis2000.html.</p><p>It should be noted while the mountain surface grid served to illustrate the LOD rendering process presented here, it may not represent the best technique for multiresolution rendering of continuous height fields. One implementation approach that has been considered for continuous fields is subdivision of the terrain TIN into discrete surface meshes based on height or some other distinguishing feature. Individual LOD surface meshes could then be developed. However the most advantageous use of this method may be rendering of landscape surface objects, particularly objects that are replicated many times such as individual tree elements, buildings, and prominent localized landscape features of high complexity. Since the LOD method presented here is based on images, non-mesh LOD objects can also be utilized in error-directed rendering including hand-crafted LOD objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>We have presented a method to address problems associated with scene synthesis involving large variations in viewing distance and orientation. A quantitative analysis of image sample rate, viewing distance, and the resolution of features on an object has been derived by approximating objects with images. This has led to the use of a frequency-space image error metric, as a function of viewing distance, based on multirate frequency-space filtering of high and low complexity object images. A CSF has been incorporated into the metric to modulate the frequency transfer characteristics of the visual system. Finally, examples of our LOD matching rendering technique have been presented to illustrate rendering cost savings by reducing polygon count.</p><p>Several aspects of a full analysis of this method have been left to future work, such as comparison to other screen-space LOD selection methods. The error metric presented here does not include many important features of the human vision system which may increase the effectiveness of object resolution matching. Also, other frequency transform and non-transform methods should be investigated for identifying pertinent image frequency content. The method presented utilizes discrete LOD models, a commonly employed technique but one susceptible to visual artifacts. How- Actual Image Viewing Distance z (km) ever discrete LODs lend themselves to cache optimization. By extending importance sampling methods to view orientation, i.e. (φ,θ), as well as z, fewer LOD models may be found that can better represent the object.</p><p>Flat shading employed here may introduce relatively large amounts of high-frequencies in reference images and may represent a worst-case error analysis. Smooth shading acts to pre-filter polygonal object radiance, removing many high frequencies and possibly leading to fewer LOD models. Changes in occlusion, silhouette and other LOD and perspective effects also should be incorporated into the error profiles. However, at large viewing distances addressed here the benefits may be minimal. Also, methods to decouple geometry and illumination such as shading in frequency space should be investigated. In this way existing LOD error profiles may be adapted to new lighting orientations without requiring a new pre-processing phase. Also, alternative techniques for merging multiple object models into a single, combined object with minimal image error can also be judged using the method described herein. Finally, application of this method to non-mesh objects is straight-forward because of the use of reference images as object substitutes, which can be created thorough a variety of rendering methods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) View frustum shown in the xz plane with y out of page. (b) Frustum mapped to Normalized Device Coordinates cube by perspective projection matrix, and sampled by uniform grid on uv image plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 . 5 Figure 5 .</head><label>455</label><figDesc>Three (3) example sphere LOD models taken from a set of 19 and the sphere snowflake. Viewing Distance (in sphere diameters) Polygon count for LOD snowflake images. Snowflakes with full-resolution spheres contain 9,461,088 triangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>(a) mean LOD error illustrating view-dependence for zenith angle θ = 0 and azimuth angle φ = 0 and 90 degrees. (b) Polygon count versus distance for a sequence of views with and without CSF. A full animated sequence is available at http://www.erc.msstate.edu/~randy/Viz2000.html.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Snowflake sequence (a) -(b) drawn in a spiral pattern with increasing viewing distance. Nearest snowflake appears in the upper-left corner and furthest near image center; (c) is the CSF sequence color-coded to indicate LOD number. Red indicates high LOD number and green low LOD number; (d) is the error image of (a) and (b). Mountains rendered with (a) high-resolution model; (b) LOD error profiles without CSF referenced after correction for field-ofview; (c) LOD with CSF. Lower right inserts for (b) and(c)are color-coded by LOD number. Red indicates high LOD, green low LOD, and monochrome the highest-resolution version. Note that using the CSF decreases LOD number more rapidly than no CSF. For animations visit http://www.erc.msstate.edu/~randy/Viz2000.html.V ie w in g D is ta n c e (k m )</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>The first author wishes to thank Brad Carter, NSF Engineering Research Center at Mississippi State University, for his continuing support. Also support was provided from the US Army Corps of Engineers Engineer Research and Development Center (ERDC) through the ILIR Research program. Thanks also go to Ken Hall of the Structures Laboratory, ERDC. The second author wishes to acknowledge the support of the NSF through CAREER award number 9734483. The work of the third author was supported by the DoD High Performance Computing Modernization Office through the HPVCI, the NAVO/MSRC PET program, and the ARL/MSRC PET program, by NASA/SSC, and by a DEPSCoR grant from ONR. We thank Michael Garland and Paul Heckbert for making their quadric simplification and TIN software freely available. Finally, thanks to the anonymous reviewers for their valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Smooth transitions between bump rendering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-GRAPH &apos;93</title>
		<meeting>SIG-GRAPH &apos;93</meeting>
		<imprint>
			<biblScope unit="page" from="183" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The validity of computer-generated graphic images of forest landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Ulricht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Fridley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ganter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Environmental Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multirate digital signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">E</forename><surname>Crochiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<idno>pp. 19. ISBN 0-136-05162-6</idno>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Human response-based evaluation of environmental data visualization: final report. Cooperative Research Agreement between United States Department of Agriculture Forest Service, FPM/MAG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><forename type="middle">C</forename><surname>Daniel</surname></persName>
		</author>
		<ptr target="http://tdserv.psych.arizona.edu/dataviz/overview.html" />
		<imprint>
			<date type="published" when="1997-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Steering image generation with wavelet based perception metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajeetkumar</forename><surname>Gaddipatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Yagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics&apos;97</title>
		<meeting>Eurographics&apos;97</meeting>
		<imprint>
			<biblScope unit="page" from="241" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast polygonal approximation of terrains and height fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Heckbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Carnegie Mellon U</title>
		<imprint>
			<date type="published" when="1995-09" />
		</imprint>
	</monogr>
	<note type="report_type">CMU-CS-95-181</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quadric-based polygon simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CS-99-105</title>
		<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="15213" to="3891" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Dissertation. Research Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A proposal for standard graphics environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Haines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3" to="5" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Smooth view-dependent level-of-detail control and its application to terrain rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization&apos;98</title>
		<meeting>IEEE Visualization&apos;98</meeting>
		<imprint>
			<biblScope unit="volume">516</biblScope>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anisotropic reflection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Kajiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;85</title>
		<meeting>SIGGRAPH&apos;85</meeting>
		<imprint>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual aviation of large environments using textured clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Palo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1995 Symposium on 3D Graphics</title>
		<meeting>1995 Symposium on 3D Graphics</meeting>
		<imprint>
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Techniques for visualizing the appearance of forestry operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Mcgaughey</surname></persName>
		</author>
		<ptr target="http://gordon.cfr.washing-ton.edu/viztools.htm" />
	</analytic>
	<monogr>
		<title level="m">The Journal of Forestry Val</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Splatting errors and antialiasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Swan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Crawfis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naeem</forename><surname>Shareef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Yagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="178" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">OpenGL programming guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Woo</surname></persName>
		</author>
		<idno>0- 201-63274-8</idno>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Addison Wesley</publisher>
			<biblScope unit="page">480</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perception based color image difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Laszlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kresimir</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Matkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Purgathofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The effects of visual fidelity criterion on the encoding of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mannos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="525" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The visibility of DCT quantization noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ahumada</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Society for Information Display Digest Technical Papers</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="942" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geospecific rendering of alpine terrain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Premoze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Eurographics Rendering Workshop</title>
		<meeting>the 10th Eurographics Rendering Workshop</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="115" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Numerical recipes in C second edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saul</forename><forename type="middle">A</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">P</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flannery</surname></persName>
		</author>
		<idno>554. ISBN 0- 521-43108-5</idno>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A perceptually based physical error metric for realistic image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Ramasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sumanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">P</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-GRAPH&apos;99</title>
		<meeting>SIG-GRAPH&apos;99</meeting>
		<imprint>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SCROOGE: Perceptually-driven polygon reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="191" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparing real and synthetic images: Some ideas about metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holly</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics Rendering Workshop</title>
		<meeting>Eurographics Rendering Workshop</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic view-dependent simplification for polygonal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><forename type="middle">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitabh</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;96</title>
		<meeting>IEEE Visualization &apos;96</meeting>
		<imprint>
			<biblScope unit="page" from="327" to="334" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
