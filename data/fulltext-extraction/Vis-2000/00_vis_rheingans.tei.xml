<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualizing High-Dimensional Predictive Model Quality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penny</forename><surname>Rheingans</surname></persName>
							<email>rheingan@cs.umbc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Electrical Engineering</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Marie desJardins SRI International Artificial Intelligence Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visualizing High-Dimensional Predictive Model Quality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Using inductive learning techniques to construct classification models from large, high-dimensional data sets is a useful way to make predictions in complex domains. However, these models can be difficult for users to understand. We have developed a set of visualization methods that help users to understand and analyze the behavior of learned models, including techniques for high-dimensional data space projection, display of probabilistic predictions, variable/class correlation, and instance mapping. We show the results of applying these techniques to models constructed from a benchmark data set of census data, and draw conclusions about the utility of these methods for model understanding.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discovering interesting information in large, high-dimensional data spaces is a challenging problem. Using inductive machine learning techniques to construct classification models has proven to be one useful approach for solving this problem. A typical machine learning application involves a great deal of manual effort to iteratively construct a representation of the domain (feature engineering), set the parameters of the learning algorithm, induce a set of models, and analyze the resulting models. To support this process, we have developed a set of visualization methods with the goal of improving a user's ability to evaluate the quality of learned models.</p><p>Traditional model analysis methods primarily consist of numerical and statistical tools for assessing the quality of a learned model. These tools include classification accuracy, confusion matrices, and receiver operating characteristic (ROC) curves. Our visualization techniques provide a richer representation of the information that the statistical tools summarize by a single number or curve, and are meant to augment, not replace, these statistical tools. To that end, we discuss in this paper how the visualization methods can be used to gain insights into how the behavior of the model varies across the data space. These insights could be used to guide the application development process by pinpointing, for example, regions of the data space (groups of individuals) with high misclassification rates, thus helping the user to determine what additional data to gather, or how to modify the set of features to improve differentiation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Induced Predictive Models</head><p>A model is a description of how the world is expected to behave. Typically a model describes the aspects of the world that are relevant to a specific task: e.g., diagnosing a disease, predicting credit risks, or classifying documents by topic area. Here we focus on classification tasks, which have the form "Given an object description, classify it into one of k classes." Classification methods can be used for both prediction and diagnosis (e.g., "Given an applicant's characteristics, predict whether they will default on a loan," or "Given a patient's symptoms, determine what disease is affecting them"). Probabilistic classification methods give the probability of class membership, which is particularly useful in domains containing uncertainty, noisy data, or incomplete object descriptions.</p><p>In classification problems, one of the variables is a distinguished class variable; we refer to the other variables as input variables. (The class variable can be thought of as the dependent variable; the input variables as the independent variables.) The data space is the n-dimensional space defined by the n input variables. In a classification task, the goal is to derive the class probabilities, i.e., the marginal probabilities that an instance belongs to each class, given values for some (or all) of the input variables.</p><p>The problem of accurately predicting class membership from available information is a key challenge of knowledge discovery. A wide variety of methods have been developed by machine learning and data mining researchers to solve this problem, ranging from decision-tree learning algorithms to nearest-neighbor techniques to Bayesian learning methods. The visualization techniques we have developed are applicable to any learning methods whose output makes predictions that can be interpreted as probabilities, such as probabilistic decision trees or Bayesian networks. In the examples given in this paper, we used the ADULT data set from the UCI Machine Learning Repository <ref type="bibr">[UCI 1999]</ref>, which is derived from U.S. Census data, to construct classification models. We applied Tree-Augmented Naive Bayes (TAN) <ref type="bibr" target="#b3">[Friedman and Goldszmidt 1996]</ref>, a Bayesian network learning system that is tailored for classification, to construct the models. Data instances contain fourteen variables (six continuous and eight nominal) and a binary class label indicating income level ( ¢ ¡ ¤ £ K (higher-income) or ¥ § ¦ ¡ ¤ £ K (lower-income)). Input variables include age, sex, race, education, occupation, hours worked per week, native country, type of employer, marital status, and household type. Using a subset of eight of the input variables, we used TAN to construct (from the training data) a model to predict income.</p><p>We have identified a set of model characteristics that may be visualized in order to understand and analyze a model's behavior. Some of the characteristics we have explored are class probability at each point in the data space; the decision boundary that delineates the region of instances that are predicted to be members of a class; misclassifications, which correspond to instances whose actual class label does not match the predicted class label; misclassification types, e.g., false positives and false negatives for binary classes; and meta-attributes of the model, such as the distribution and density of the training data used to build the model, and the confidence assigned to each estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Traditional Model Analysis</head><p>A number of measurements have been developed by machine learning and statistics researchers to assess model performance. The most commonly used are classification accuracy, confusion matrices, and receiver operating characteristic (ROC) curves. In most machine learning research, model evaluation focuses on a single metric, such as classification accuracy. Classification accuracy is a single number that indicates the percentage of correctly classified instances in a test set. For the example model shown here, predictive accuracy on the test set is 81 percent.  A confusion matrix is often used to show the types of misclassifications made by a model. The confusion matrix <ref type="table">(Table 1)</ref> is a two-dimensional table that indicates actual class label along one dimension and predicted class label along the other dimension. Each matrix entry indicates how many instances with the corresponding actual class label were predicted by the model to have the corresponding predicted class label. Entries along the diagonal correspond to correctly classified instances. For a binary class, there are two off-diagonal entries, corresponding to false positives (negative instances with a predicted positive label) and false negatives (positive instances with a predicted negative label).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test data</head><formula xml:id="formula_0">¢ ¡£ ¢ ¡¤ ¢ ¡¥ ¢ ¡¦ § ¡ © £ ¡¤ ¢ ¡¥ ¢ ¡¦ § " ! $ # ! &amp; % ( ' # ) 0 ' 2 1 " ! $ # ! &amp; % 3 ' #</formula><p>ROC curves are used to assess the performance of the model as misclassification costs are varied. By changing the prediction threshold, a given model can be biased towards making more false positive predictions (lowering the threshold) or more false negative predictions (raising the threshold). The ROC curve <ref type="figure" target="#fig_0">(Figure 1</ref>) plots the false positive rate against the false negative rate.</p><p>None of these methods really address the question of when the model performs poorly, specifically what sort of instances tend to be misclassified. Visualization of model characteristics in the context of the data space complements the statistical tools by providing better insights into the nature of the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Although many researchers have studied techniques for visualizing data sets, and others have developed techniques to view model structure directly, there has been relatively little effort focused on visualizing learned models in the data space. A notable exception is the MineSet data mining package <ref type="bibr">[SGI 1999]</ref>, which includes several techniques for visualizing models, such as scatterplotting of misclassified instances. The display space is generated by manual feature selection, so the behavior of the complete model can be difficult to perceive. Visualization of classifiers in the MineSet paradigm was described by <ref type="bibr" target="#b1">[Becker 1998</ref>].</p><p>A wide variety of techniques have been developed to perform dimension reduction of high-dimensional data. These include parallel coordinates <ref type="bibr" target="#b4">[Inselberg and Dimsdale 1990]</ref>, multiparameter icons <ref type="bibr" target="#b6">[Pickett et al. 1990]</ref>, and a host of interactive techniques developed by dynamic statistics researchers <ref type="bibr" target="#b1">[Cleveland and McGill 1988]</ref>. Many of these approaches only work for discrete-valued variables.</p><p>There are also other techniques that produce clusters in 2D space based on the similarity of data instances have been used to perform the projection of high-dimensional data to 2D. These techniques include multi-dimensional scaling <ref type="bibr" target="#b2">[Cox and Cox 1994</ref>] and relevance maps <ref type="bibr" target="#b0">[Assa et al. 1997]</ref>. Other applications of SOM techniques to information visualization include the visualization of customer characteristics <ref type="bibr" target="#b8">[Rushmeier et al. 1997]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Visualization</head><p>Information visualization applications are typically characterized by non-spatial, high-dimensional, non-continuous data spaces. Visualizing such data requires transforming the data to a spatial display space, then applying representation techniques to the transformed data points. The model characteristics we are visualizing are non-spatial and high-dimensional, but the dimensions are a mixture of continuous and nominal variables. Thus, while this application has a number of typical information visualization characteristics, the continuous nature of the data necessitates some additional design choices, particularly in the projection process.</p><p>In all of the visualizations shown here, we use a background colormap in saturations of green to show the class probabilities at each point in the display space. Locations with high class probabilities are bright green; locations with low class probabilities are black.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Space Projection</head><p>A computer monitor or printed page can only display two dimensions. These two dimensions can be intuitively extended to three with the addition of computer graphics 3D shape cues such as obscuration, perspective, and view point control. While we can represent additional high-dimensional coordinate information using other display parameters, such as time, color, size, or opacity, these more abstract display parameters are not as easily or directly interpreted as spatial position. In practice, the display space is only three-dimensional; therefore, high-dimensional data must be projected down to three dimensions to be visually represented.</p><p>One obvious, and common, method for the projection of highdimensional data into a lower-dimensional display space is the selection of a subset of available dimensions. The plane of the projected space corresponds to an axis-parallel plane through the data space, with all points orthogonally projected onto the plane. <ref type="figure" target="#fig_1">Figure 2</ref> shows a 2D display space created using feature selection. The vertical axis is education; the horizontal axis, hours worked. Each location in this display space represents a highdimensional subspace where education and hours worked are fixed, but other values can vary over their entire range. Feature selection has the advantages of being simple to perform and intuitive to understand. Unfortunately, the straightforward feature selection display often does not adequately capture the complex structure of the model in the high-dimensional data space, since instances with very different characteristics along other dimensions are aggregated.</p><p>In order to achieve a display space that better represents the high-dimensional structure of the data space, we have also used a set of projection techniques based on self-organizing maps (SOM) <ref type="bibr" target="#b5">[Kohonen 1997]</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> shows a representation of the model where the data space has been projected to two dimensions using a SOM. In a SOM, neighboring locations in the display space correspond to neighboring locations in the data space, unlike feature selection where points far apart in the data space can map to identical locations in the display space. We are currently performing data space projection using a public-domain package that implements self organizing maps <ref type="bibr" target="#b5">[Kohonen et al. 1996]</ref>.</p><p>A SOM is constructed from a set of discrete instances, commonly the data set to be explored. In our application, the continuous data space must be sampled to yield a discrete set of instances.</p><p>We have experimented with constructing SOMs from the input instance set used to construct the model, the test set of instances used to perform traditional model analyses, and a set of samples generated from the model, which reflects the population characteristics of the input set. The latter was used to create the SOM in <ref type="figure" target="#fig_2">Figure  3</ref>. Each of these choices produces a map that is specific to a particular model. Alternatively, one can build a "model-neutral" SOM using a set of constructed instances that regularly sample the entire data space, as seen in <ref type="figure" target="#fig_3">Figure 4</ref>. This includes instances that, while not impossible, may be extremely unlikely. Note that the high-probability region is much less contiguous in this visualization than in <ref type="figure" target="#fig_2">Figure 3</ref>. The model-neutral map allocates display space relatively equally to all regions of the data space, rather than predominantly to regions of the data space corresponding to instances that are more likely to appear in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Display of Model Characteristics</head><p>The decision boundary (in this case, the boundary between positive and negative predictions when a prediction threshold of 50% is used) is shown by a white line. <ref type="figure" target="#fig_1">Figure 2</ref> shows the probability distribution for 2D feature selection. Since each point in the display space is equivalent to an attribute vector with missing values, the class probability can be computed by marginalizing over the missing values. <ref type="figure" target="#fig_2">Figure 3</ref> shows the probability distribution for the SOM projection. When building the SOM from the model, probability is treated as just another variable for similarity clustering, resulting in maps that typically have high coherence of predicted probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Display Space Interpretation</head><p>The most difficult aspect of projecting high-dimensional spaces into a 2D display space is understanding the correspondence of the projected space to the original data space. In <ref type="figure" target="#fig_4">Figures 5 and 6</ref>, attribute contours are overlaid on the probability distribution to show how the input variables correlate with the predicted class and with each other. These contours show how the Cartesian grid of the data space has been warped by the projection process. The 2D feature selection projection ( <ref type="figure" target="#fig_5">Figure 6</ref>) is easy to understand, but the attribute contours are useful as an indicator of the scale of the variables. In this projection, each attribute contour represents a data space hyperplane that is orthogonal to the 2D display plane. The correlations of the selected input variables with the class are apparent: individuals with more education (towards the top) and who work more hours (towards the right) tend to make more money, with the education correlation being somewhat stronger. (Notice that working too hard doesn't really pay off: a good lesson for us all.)</p><p>The attribute contours are even more important in the SOM projection, since there is no intuitive interpretation of the projected space. <ref type="figure" target="#fig_4">Figure 5</ref> shows the education and hours worked contours. These contours show how severely the hyperplanes of constant attribute value in the data space are distorted through the projection process. In this projection, the hyperplanes represented by the attribute contours are not generally orthogonal to the display space, and may not even be represented by contiguous contour lines. As before, notice that predicted high-income earners tend to have greater education levels (higher saturation of blue in the contours) and work more hours (red saturation). <ref type="figure" target="#fig_6">Figure 7</ref> shows contours of another pair of attributes, those of education and sex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Visual Model Analysis</head><p>By providing a visual representation of the model characteristics, our visualization methods add depth to a user's understanding of traditional statistical model analysis measurements. Seeing the number of instances and their class labels against the background of the predicted probability distribution gives the user a visual understanding of the number and types of misclassifications. The graded color representing the probability distribution and the instance classifications shown against this background gives the user a visual interpretation of the information conveyed by the ROC curve. The visual display, however, also provides information not present in the ROC curve. Specifically, when the decision threshold is increased (decreased), where are false negatives (positives) introduced?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Instance Mapping</head><p>Test instances can be plotted in the display space in order to compare model predictions to actual classifications. Each test instance is displayed in the projected space as a small sphere-shaped glyph. We use glyph size to indicate the number of instances at a given point. (A linear scale is used, but is clamped at 10 instances, since otherwise large glyphs dominate the picture.) A continuous color map is used to show the proportion of class labels in the set of collocated instances. Yellow shows predominantly positive instances, red shows predominantly negative instances. Orange glyphs indicate points where there are roughly equal numbers of positive and negative instances. This representation allows a user to easily identify false positives (red glyphs inside the decision boundary) and false negatives (yellow glyphs outside the decision boundary). <ref type="figure" target="#fig_7">Figure 8</ref> shows the test instances on the 2D feature selection projection. Notice that because the projection groups very different locations together at each projected point, a high proportion of the instance glyphs are orange (indicating a mix of positive and negative instances at a location). Also, a relatively small percentage of the space is used for the high-class-probability part of the model, so the visual impact is that most of the space is taken up by negative instances (in fact, about 25% of the training instances are positive). <ref type="figure" target="#fig_8">Figure 9</ref> shows the test instances on the SOM projection. Notice that many of the misclassified instances are located in the vicinity of decision boundary, a region of the data space where their presence is expected and is not a major reason for concern. Comparing this pictures to <ref type="figure" target="#fig_3">Figure 4</ref>, one can see that the model-neutral SOM has much tighter instance clustering, because much of the display space represents regions of the data space that do not appear in the data (for example, very young and very old people). The model-neutral SOM can be useful in understanding how the data and model are biased towards particular regions of the data space; the model-dependent SOM in <ref type="figure" target="#fig_8">Figure 9</ref> is more useful in understanding the model itself, since differences between high-probability and low-probability regions are emphasized in the SOM construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Display Space Queries</head><p>The visualizations can be queried interactively (by clicking at a point on the map) to generate a summary of the instances that correspond to any given region. For example, near the upper right of <ref type="figure" target="#fig_8">Figure 9</ref>, there is a region with two large orange glyphs (mixed positive and negative instances). This region corresponds to a group of males in private industry who work long hours (60 hours a week), have moderate education (typically some college), and work as professionals or managers. Querying further, we can get a summary of the positive instances (true positives) and negative instances (false positives) in the region. Upon inspection, there are few differences between these two groups along the dimensions included in this data set. A knowledge engineer could use this analysis process to identify groups of individuals who are not easily differentiated with respect to the class of interest. Such a conclusion might lead to further data gathering (to identify features that would differentiate the high-and low-income earners), or might simply indicate that the model was not reliable for that particular group. Existing data mining software packages primarily use feature subset projection techniques for visualizing high-dimensional datasets and model predictions. Our investigation has shown that this approach is inadequate for complex, high-dimensional domains, because the 2D (or even 3D) feature selection display does not adequately capture the structure of the domain.</p><p>The SOM-based displays are promising in that the display space is utilized more efficiently, and the actual model behavior is reflected more completely than in feature selection. However, the correspondence of the SOM displays to the data space is also more difficult to understand. The attribute contours and querying techniques we have developed provide some initial tools for interpreting the SOM display, but further work is needed in this area.</p><p>The nature of the SOM display space depends heavily on the particular process used to build it (e.g., which instances are used to seed the SOM construction process and which similarity metric is used). Although this could be seen as a disadvantage, it provides more flexibility in constructing the SOM to meet particular projection criteria. We are currently exploring different projection metrics (such as region preservation and decision boundary smoothness) and how the SOM construction process affects these metrics.</p><p>We are exploring other dimension reduction techniques, such as principal components analysis (PCA), as alternatives to the SOM approach. However, while these techniques may provide useful for certain types of domains, the key issues of representing complexity and of data space/display space correspondence will remain the same. Similarly, we are investigating 3D visualization methods to add richness to the visualizations, but adding another display dimension does not solve the fundamental challenges of highdimensional model visualization. We believe that the key to making these methods widely useful lies in developing additional annotation techniques to clarify the correspondence between the data space and the projected display space.</p><p>Many of the techniques that we have presented here are also completely applicable to the direct visualization of data instances without the initial creation of a model. Some of the techniques used here, in particular the display of instance glyphs using feature selection or SOMs, are already in common use for that purpose. The annotation of high-dimensional projections using attribute contours can be done with direct visualization of discrete data instances, since the projection is continuous even if the data is not, but contours have not previously been used this way, to our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>The construction of induced predictive models can aid in the process of knowledge discovery, but can be limited by the complexity of such models. We have presented visualization techniques that assist a user in understanding and analyzing a predictive model. These techniques augment standard statistical tools by allowing the user to see multiple characteristics of the model's behavior across the data space. Using a benchmark data set, we have demonstrated how visualization of model characteristics facilitates both model understanding and analysis, indicating aspects of model performance not available from standard statistical tools.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ROC curve for the census domain</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Probability distribution for 2D feature selection. Vertical axis = education; horizonal = hours worked.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>SOM probability map constructed from model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>SOM "model-neutral" probability map constructed from instances spanning the data space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>SOM probability map with education and hours worked contours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Probability distribution for 2D feature selection with attribute contours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>SOM probability map with education and sex contours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>2D feature selection projection with test instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>SOM probability map with test instances.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NSF CAREER Grant 9996043 (Dr. Rheingans) and DARPA's HPKB program (Dr. des-Jardins).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Displaying data in multidimensional relevance space with 2D visualization maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Assa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;97</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Research report: Visualizing decision table classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">W</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Wadsworth and Brooks/Cole</publisher>
			<pubPlace>Becker; Los Alamitos CA</pubPlace>
		</imprint>
	</monogr>
	<note>Dynamic Graphics for Statistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multidimensional Scaling, Monographs on Statistics and Applied Probability</title>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Chapman &amp; Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building classifiers using Bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1277" to="1284" />
		</imprint>
	</monogr>
	<note>Friedman and Goldszmidt</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parallel coordinates: A tool for visualizing multidimensional geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dimsdale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;90</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="361" to="375" />
		</imprint>
	</monogr>
	<note>Inselberg and Dimsdale</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SOM PAK: The Self-Organizing Map program package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen ; Kohonen</surname></persName>
		</author>
		<idno>CBM- TR-117</idno>
	</analytic>
	<monogr>
		<title level="m">Self-Organizing Maps, Second Edition</title>
		<meeting><address><addrLine>Berlin; Finland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1980-05" />
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology, Laboratory of Computer and Information Science, FIN-02150 Espoo ; Rutgers University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>The need for biases in learning generalizations</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iconographic displays of multiparameter and multimodality images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pickett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Visualization in Biomedical Computing</title>
		<meeting>of Visualization in Biomedical Computing</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Third International Conference on Knowledge Discovery and Data Mining<address><addrLine>Huntington Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>Provost and Fawcett</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Case Study: Visualizing Customer Segmentations Produced by Self Organizing Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Almasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;97</title>
		<meeting><address><addrLine>Los Alamitos CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="463" to="466" />
		</imprint>
	</monogr>
	<note>Rushmeier et al. 1997</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MineSet User&apos;s Guide, SGI Document</title>
	</analytic>
	<monogr>
		<title level="j">Silicon Graphics, Inc</title>
		<imprint>
			<biblScope unit="page" from="7" to="3214" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>SGI 1999</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="http://www.ics.uci.edu/˜mlearn/MLRepository.html" />
		<title level="m">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of California, Irvine</orgName>
		</respStmt>
	</monogr>
	<note>UCI 1999</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
