<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hardware-Accelerated Texture Advection For Unsteady Flow Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Jobard</surname></persName>
							<email>jobard@csit.fsu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Erlebacher</surname></persName>
							<email>erlebach@csit.fsu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousuff Hussaini</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computational Science &amp; Information Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dirac Science Library</orgName>
								<address>
									<postCode>411, 32306-4120</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hardware-Accelerated Texture Advection For Unsteady Flow Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Computer Graphics]: Bitmap and framebuffer operations -Display algorithms; I.3.6 [Computer Graphics]: Graphics data structure and data types unsteady</term>
					<term>vector field</term>
					<term>pathlines</term>
					<term>streakline</term>
					<term>advection</term>
					<term>texture</term>
					<term>hardware</term>
					<term>OpenGL</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present a novel hardware-accelerated texture advection algorithm to visualize the motion of two-dimensional unsteady flows. Making use of several proposed extensions to the OpenGL-1.2 specification, we demonstrate animations of over 65,000 particles at 2 frames/sec on an SGI Octane with EMXI graphics. High image quality is achieved by careful attention to edge effects, noise frequency, and image enhancement. We provide a detailed description of the hardware implementation, including temporal and spatial coherence techniques, dye advection techniques, and feature extraction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Traditionally, unsteady flow fields are visualized by time integration of a collection of pathlines that originate from user-defined seed points <ref type="bibr" target="#b11">[12]</ref>. Many experimental techniques are based on a combination of pathlines, streaklines, and timelines <ref type="bibr" target="#b1">[2]</ref>. It is well known however, that the resulting structures are strongly dependent on the initial seed points. Following this realization, an increasing body of work aims to remove the influence of initial conditions on the final display. This led to a several approaches such as spot noise <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref>, LIC <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, texture advection <ref type="bibr" target="#b12">[13]</ref>, and most recently, equally-spaced streamlines <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, all developed for steady flows. Some of these approaches have recently been extended to the exploration of unsteady vector fields <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref>. Most of these techniques are based on a dense representation of the flow. While the resulting displayed flow fields are more realistic and are to a large degree void of computational artifacts, they are very expensive to compute. The fastest algorithm developed to date for steady flows is fastLIC <ref type="bibr" target="#b14">[15]</ref>. Straightforward generalizations of steady algorithms are even more expensive to compute since a large collection of particles is advected at each time step.</p><p>As the performance of commodity 3D graphics cards continues to increase at a rate substantially faster than that dictated by Moore's law (factor 2 every 18 months), an increasing fraction of the visual computations will be subsumed by the graphics processor, freeing the central processing unit for other tasks. Many algorithms currently written in software will be reexamined to take advantage of the latest hardware features. These algorithms will, within a couple of years, perform far faster than their older software-based siblings.</p><p>The trend towards increased reliance on hardware is clearly demonstrated in the evolution of OpenGL, a graphic standard introduced in 1992. Since then, a large number of extensions have been proposed, and a subset of them adopted. Among the more interesting proposed extensions advanced in 1997 is the notion of a pixel texture <ref type="bibr" target="#b0">[1]</ref>, a form of indirect addressing that allows many old algorithms to be recast on per pixel basis that were not possible before <ref type="bibr" target="#b8">[9]</ref>, an enhancement not previously possible.</p><p>In this paper, we propose a hardware-accelerated algorithm, based on texture advection, to animate a dense set of particles in two-dimensional unsteady flows. Based on a texture advection scheme, the algorithm is highly accelerated by available hardware features on advanced graphic workstations. Use is made of texture maps, hardware frame buffers, pixel textures, and blending. Several issues that plague texture advection methods are addressed: the treatment of domain boundaries, temporal and spatial correlation, and the loss of high frequency information. The animations simultaneously display velocity direction, velocity magnitude, particle path segments, and the trajectories that result from the continuous injection of colored dye into the flow. A dense collection of over 256 256 × particles can be advected at a rate of two frames per second on an SGI Octane with EMXI graphics.</p><p>The rest of the paper is organized as follows. Section 2 gives an overview of related work. Texture advection is described in Section 3. The hardware implementation is detailed in Section 4. Several extensions of the algorithms are suggested in Section 5, and finally, we draw some conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Several techniques have been proposed to produce dense representations of unsteady vector fields. Best known is perhaps UFLIC (Unsteady Flow LIC) developed by Shen <ref type="bibr" target="#b13">[14]</ref>, based on the Line Integral Convolution (LIC) technique <ref type="bibr" target="#b3">[4]</ref>. The algorithm achieves good spatial and temporal correlation although the images are difficult to interpret. The paths are blurred in regions of rapid change of direction and are thick where the flow is almost uniform.</p><p>The spot noise technique, initially developed for the visualization of steady flowfields, has been extended to unsteady flows <ref type="bibr" target="#b6">[7]</ref> by advecting the spots along pathlines. Control of the frame rate is possible by varying the number and size of the spots.</p><p>Max and Becker <ref type="bibr" target="#b12">[13]</ref> proposed an alternative texture-based algorithm to represent steady and unsteady flow fields. The basic idea is to advect a texture along the flow either by advecting the vertices of a triangular mesh or by integrating the texture coordinates associated with each triangle backward in time. In both cases, if the flow has rotational shear, the advected texture eventually becomes excessively distorted. To counter this distortion, Max periodically reinitializes the texture coordinates to their initial values and blends the texture with a second advecting texture offset by half a period. When texture coordinates leave (or particles enter) the physical domain, an external velocity field is linearly extrapolated from the boundary. Rather than encode the direction of the velocity and its magnitude in a single frame, they are visualized through time animation. This technique attains interactive frame rates by controlling the underlying mesh resolution.</p><p>Heidrich et al. <ref type="bibr" target="#b8">[9]</ref> described the first hardware-accelerated implementation of LIC to depict the directional information of 2D steady flow fields. A white noise texture is successively advected along streamlines, forward and backward, to generate N advected textures. When blended together, these textures produce the desired LIC image. Two major contributions of this algorithm are the delegation of the numerical integration of texture coordinates to the graphics hardware and the use of pixel textures to handle indirect addressing on a per-pixel basis. The exclusive use of graphics hardware results in a LIC algorithm that is several times the speed of fastLIC <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TEXTURE ADVECTION</head><p>We are interested in computing the temporal evolution of particles in an Eulerian frame of reference. A fluid particle at position x and time t is tagged by the value of a function ( , ) N t</p><p>x , encoded as a two-color noise texture. This particle describes a trajectory <ref type="bibr">( , , )</ref> t τ</p><p>[ as a function of τ , called a pathline. At each point along the pathline, the velocity of the particle is ( ( , , ), )</p><formula xml:id="formula_0">t τ τ v [</formula><p>; the trajectory satisfies the evolution equation</p><formula xml:id="formula_1">( , , ) ( ( , , ), ) d t t d τ τ τ τ = [ v [<label>(1)</label></formula><p>From (1), if a particle passes through the point x at time t and the point '</p><p>x at time ' t , the coordinates of these points are related by</p><formula xml:id="formula_2">' ' ( ( ' , ' , ), ) t t t d τ τ τ = + ∫ x x v [<label>(2)</label></formula><p>Since ( , ) N t x describes an invariant particle property, it is constant along a pathline:</p><formula xml:id="formula_3">( , ) ( ', ') N t N t = x x<label>(3)</label></formula><p>A Taylor expansion of ( , ') N t x about time t shows that ( , ) N t x satisfies the advection equation</p><formula xml:id="formula_4">0 N N t ∂ + ⋅∇ = ∂ v<label>(4)</label></formula><p>valid for both steady and unsteady flows if ( , ) N t x is a continuous function of x . In this work, we consider discontinuous functions of the spatial coordinates so that (4) cannot be used.</p><p>To obtain an advected texture at any time t , equation <ref type="formula" target="#formula_3">3</ref>is solved at the center x of each texel of the noise texture by determining the property value at a previous location '</p><p>x at time ' t . This approach is a common starting point for the algorithms of Max <ref type="bibr" target="#b12">[13]</ref>, Heidrich <ref type="bibr" target="#b8">[9]</ref> and the one presented in this paper, all based on texture advection.</p><p>The two former methods compute an advected texture at time t from the initial property texture at time ' 0 t = . Consequently, they require either an extrapolation of the vector field outside the physical domain <ref type="bibr" target="#b12">[13]</ref> or the limitation of the advection to a few time steps to minimize artifacts in regions of incoming flow <ref type="bibr" target="#b8">[9]</ref>. Our algorithm, dependent on a random property function, computes successive textures incrementally and suppresses artifacts by generating random property values for particles that enter the physical domain.</p><p>As in <ref type="bibr" target="#b8">[9]</ref>, our method advects textures on a per-pixel basis rather than on a coarse triangular mesh <ref type="bibr" target="#b12">[13]</ref>. It also extends the implementation of Heidrich to track particles in unsteady flows over indeterminate time periods. This is made possible by an innovative treatment of incoming particles, compensating for the nonzero divergence of the flow, and a corrective procedure to address the loss of accuracy that results from the discrete nature of the algorithm.</p><p>In the following sections, we describe a new algorithm that computes ( , ) N t</p><p>x based only on OpenGL routines that directly access the hardware available on an Indigo 2 SGI with a Maximum Impact graphics board or on an SGI Octane with EMXI graphics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">HARDWARE IMPLEMENTATION</head><p>Our implementation largely capitalizes on new per-pixel operations and other recent OpenGL extensions provided by some SGI graphics boards. The core of the texture advection process relies mainly on two hardware features: 1) additive and subtractive blending between framebuffer content and incoming fragments from textured polygons or pixel arrays, and 2) an indirection operation, called pixel texture, that uses a buffer as a lookup table into a texture.</p><p>These hardware operations are further detailed in Section 4.1. Section 4.2 summarizes the different steps of the algorithm before Sections 4.3 through 4.8 describe them in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Notation</head><p>This section introduces a simplified notation that maps to the hardware operations used in this paper. In our algorithm, data is drawn from, read to, and copied between a combination of buffers and textures. During these operations, incoming data can be blended into the destination buffer, colored using per-pixel color tables, and color transformed using color matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Buffers and Textures.</head><p>The physical variables used are coordinates, velocity, and particle property. They are stored either in a 2D/3D texture or in a 2D hardware RGB framebuffer. However, rather than using an entire visual allocated by the X window system for this purpose, the hardware back and front buffers are divided into several sub-buffers from which data can be read and to which data can be written. In the remainder of the paper, a buffer refers to any subset of a hardware framebuffer used as a storage area. Buffers and textures are denoted by B and T respectively with subscripts that characterize their function or content. All buffers and textures have the resolution of the discretized physical domain.</p><p>Blending operations. Blending is a per-pixel operation executed when an incoming fragment merges with the corresponding pixel in the destination buffer. Additive ( B + ) and subtractive ( B − ) blending of a texture T into a buffer B are denoted by</p><formula xml:id="formula_5">( , ) B α β ± B T Ã Ã equivalent to α β ← ± B B T Ã Ã<label>(5)</label></formula><p>The first argument of B ± is always a buffer; the second argument can be a texture, a pixel texture, or another buffer.</p><p>Pixel texture. Proposed by SGI in 1997 as an extension to OpenGL <ref type="bibr" target="#b2">[3]</ref>, pixel textures have been used to advantage in a variety of algorithms ranging from steady-state LIC to a wide range of sophisticated lighting models <ref type="bibr" target="#b8">[9]</ref>. Pixel textures allow the projection of a texture onto the framebuffer through the intermediary of a texture coordinate map <ref type="bibr" target="#b2">[3]</ref>. Rather than directly affecting the color in the framebuffer (see <ref type="figure" target="#fig_0">Figure 1</ref>, left), the color components of the incoming fragment are interpreted as texture coordinates. The texel color at these coordinates is then sent to the framebuffer <ref type="figure" target="#fig_0">(Figure 1, right)</ref>. Let A be an array of pixels and T be a texture. The action of a pixel texture operation, denoted by ( , ) P A T , can be viewed as the construction of an intermediate array of pixels ( ) T A , where the RGB components of the pixels in A , acting like texture coordinates <ref type="bibr">( , , )</ref> r s t , are replaced by the corresponding texel values of T . The resulting pixel array can be stored or blended with the contents of a buffer B . If a pixel array A is contained in a buffer ' B , the composite blending operation is expressed as</p><formula xml:id="formula_6">( , ( ', )) B P ± B B T<label>(6)</label></formula><p>Read, draw, and copy. A draw operation, denoted by ( , ) D B T , copies the contents of a texture T into a buffer B . In practice, a polygon, texture-mapped with T , is drawn into B . A read operation, denoted by <ref type="bibr">( , )</ref> R T B , takes the contents of a buffer, and transfers it to a subset of a texture, called a sub-texture, of equal size. In practice, we use the OpenGL extension glCopyTexSub-ImageEXT() to directly write to texture memory. Finally, a copy operation from a buffer 1 B to a buffer 2 B is denoted by 2 1</p><p>( , ) C B B . Although a part of the proposed SGI extensions to OpenGL, the copy operation does not work when the second argument is a pixel texture. In practice, the copy operator is replaced by the combination glReadPixels() and glDrawPixels() at the cost of accessing conventional memory. Both these routines work with pixel textures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithm Overview</head><p>The first phase of the algorithm implements a hardware version of the advection component described by equations <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_3">3</ref>  <ref type="figure">4)</ref>, while the loss of spatial frequency due to the nonzero divergence of the flow is compensated for by a random injection of noise (Section 4.5). The corrected advected texture is then blended with the last blended frame to produce an animation frame with an acceptable level of spatio-temporal correlation (Section 4.6). Finally, the coordinate buffer x B is reinitialized in preparation for the next iteration. This initialization takes into account constraints imposed by the discrete nature of the algorithm (Section 4.7). Images are enhanced by additional post processing such as masking and dye advection (Section 4.8).</p><p>We will often refer to the different steps of the algorithm. They are numbered and summarized in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_3">Figure 2</ref> (see color plate). <ref type="table">Table 1</ref> gathers the complete set of operations written using the notation described in Section 4.1, while <ref type="figure" target="#fig_3">Figure 2</ref> represents all hardware resources as a list of buffers and textures along with the operations that link them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Basic Advection</head><p>In this section, we discuss the hardware implementation of the advection component of the algorithm described by equations <ref type="bibr" target="#b1">(2)</ref> and <ref type="bibr" target="#b2">(3)</ref>. As proposed in <ref type="bibr" target="#b8">[9]</ref>, the red and blue components of buffers and textures encode both velocity and coordinate data. We store a time series of 2D vector fields, which cover the entire physical domain, in two 3D velocity textures whose third dimension represents time. The velocity components are normalized by the infinity norm over all the field slices. To accommodate the fact that texture values can only be positive, the velocity field is split into its negative and positive components ( )</p><formula xml:id="formula_7">+ − = − v v v and stored in two separate 3D textures, − v T and + v</formula><p>T . Furthermore, since the entire 3D vector field, <ref type="bibr">( , , )</ref> x y t , is often too large to completely reside in texture memory, only two time slices of the Coordinate update. Texture coordinates at time ' t t h = − are computed from a first order discretization of (2):</p><formula xml:id="formula_8">' [ ( ,) ( ,) ] h t t + − = − − x x v x v x<label>(7)</label></formula><p>Two buffers, Since each velocity component is the range <ref type="bibr">[0.,1.]</ref> , h is related to the maximum possible displacement p (in pixels) of a particle between two consecutive positions by / h p N = . To achieve a sufficient degree of spatio-temporal correlation during an animation sequence h must be sufficiently small. We find that [0.5,3] p ∈ yields good results.</p><p>Noise update. The second part of the advection process (step 4) computes ( ', ) N t x using the pixel texture '</p><p>N P x B T . In principle, any texture can be used for the advection. However, we use a noise texture for its lack of spatial correlation. This property is a necessary requirement for the treatment of particles entering the physical domain (Section 4.4), noise injection (Section 4.5), and noise blending (Section 4.6).</p><p>With the x buffer reinitialized between successive iterations, first four steps implement a basic texture advection. However, several issues must be addressed to correct and enhance the advected textures. They are explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Edge Correction</head><p>A common problem with texture advection techniques is the inadequate treatment of particles that originate from outside the physical domain <ref type="bibr" target="#b12">[13]</ref>. A proper treatment of edge effects requires that these particles be identified and new property values assigned to them without introducing extraneous visual artifacts. We capitalize on the OpenGL property that states that before storage into a buffer (or into a texture), floating point color values are clamped to the range <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref> . Whatever the particle referenced outside the domain, its coordinates reference an edge texel. As an illustration, <ref type="figure" target="#fig_4">Figure 3 (a,b)</ref> shows the black and white striations, which result from the clamp operation, on a circular flow defined by <ref type="bibr">( , )</ref> </p><formula xml:id="formula_10">( , ) u v y x = −</formula><p>. In this example, a particle at <ref type="bibr">( , )</ref> x y originates from ' x and ' 1 y &gt; acquire the value of the noise texture at ( ',1)</p><p>x . These particles lie on a straight line with positive slope h , clearly seen in <ref type="figure" target="#fig_4">Figure 3 (a,b)</ref>.   12</p><formula xml:id="formula_11">B − C B − % N T C B + v T − v T •</formula><p>Spatio-temporal correlation is enhanced by applying a temporal filter on advected textures. As an additional bonus, flow direction is available in static frames.</p><p>We demonstrated how to use masks to control the regions of interest through the intermediary of a control function stored in the velocity texture. Finally, we capitalized on the possibility of long time integration to transport dye and visualize streaklines. <ref type="figure">Figure 8</ref> displays three frames of a 1000 frame animation of unsteady wind patterns over Europe using all the above. Dye is released both from a point, and from a line segment. Regions of rotation are easily discerned, along with the regions of high velocity.</p><p>This paper further demonstrates the usefulness of new hardware capabilities and advanced graphic functionality, such as the SGI pixel texture extension. Interactive frame rates are achieved with buffer sizes of 256 256 × and over 65,000 individual particles. The small texture size led to a novel algorithm for long time advection. At present, only the Maximum Impact and the Octane have the required hardware in their graphics engines to implement the algorithm. However, these features deserve to be incorporated into a wider class of machines. Each 256 256 × frame takes 0.4 seconds to compute on an Octane with EMXI graphics. Although 18 texture applications per step are required, we expect the algorithm to be increasingly superior to the best software implementations. On the other hand, the precise control afforded by a software implementation will most probably lead to higher quality images. (see <ref type="bibr" target="#b4">[5]</ref>). <ref type="figure">Figure 8</ref>. Two frames from a 1000 frame animation (computed in 11 minutes) sequence of wind currents over Europe <ref type="bibr" target="#b16">[17]</ref>. Note the lack of artifacts at the domain boundaries. Dye is released from a point and from a line segment. Vortical patterns are evident in the lower frame.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .T</head><label>1</label><figDesc>Pixel Texture. (left) Color triplets are transferred directly to the framebuffer. (right) When a pixel texture is applied, color triplets are used to address its texels, whose values are sent to the framebuffer. velocity texture are stored at any given time. They are updated whenever the current time is outside the range encompassed by these slices.The texel coordinates of NT are initially stored in the texture references the center of the corresponding texel in N T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>B</head><label></label><figDesc>, are used for this operation. x B , which initially contains the initial texture coordinates stored in 0 x T , is blended with texels from the velocity textures, and the result is stored in ' x B (steps 1-3 in table 1 and Figure 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>'</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Algorithm for a single time step. The numbers indicate the operation number in the algorithm, which matches the line number in table 1. Pixel texture operations are shown as circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>(a),(b) noise texture advected by a circular flow ( , ) ( , ) u v y x = − . (c) regions from particles exterior to the domain at the previous step are black 1 ( ) I . (d) new noise is injected into the edge region; complement region is black 2 ( ) I . (e) composite of 1 I and 2 I .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Three streaklines in the rotating uniform flow ( , ) (cos( ),sin( )) u v t t =</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>We would like to thank David Banks for lively discussions in all areas of visualization, including several valuable suggestions to improve the quality of this paper, and van Liere for the use of the wind data from CWI. We also appreciate the comments of the reviewers, which led to improved clarity of exposition. We acknowledge the support of NSF under grant NSF-9872140.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and is given by N T elsewhere. The second image, <ref type="bibr" target="#b1">2</ref> I , has new noise in B S , and is black elsewhere. Adding the two images produces a noise texture with new values only on B S . There are no visible artifacts at the juncture between the images since we use a spatially uncorrelated noise. Next, we describe the construction of these two images.</p><p>As seen above, the spurious streaks at the edges of the domain take on the color of some boundary texel. We capitalize on this property by placing a one texel wide black border along the perimeter of N T (initially and between steps 8 and 9). Consequently, the output of step 4 is a noise buffer that contains <ref type="bibr" target="#b0">1</ref> I . The second image is constructed with the help of a white mask texture M T whose border texels are black. </p><p>T is a 3D texture with two layers in the third dimension. The bottom layer has random noise; the top layer is black at coordinate (1,1,1). Thus, where the mask buffer is white, the color of ( , ) R M N P B T is black. At black texels, the mask buffer has the original coordinate values and the pixel texture returns a random noise. Finally, in step 7, the image 2 I output by the pixel texture is added to 1 I , stored in the noise buffer generated in step 4. To insure that the new noise generated in step 7 has no temporal correlation 0 x T is randomly translated at each iteration. This is accomplished using a texture transformation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Noise Injection</head><p>In regions of positive flow divergence, adjacent pixels in ' x B that reference the same texel location in N T after the backward integration step share the same color. Therefore, the overall frequency of the successive noise textures decreases. <ref type="figure">Figure 4</ref> clearly demonstrates this decrease for a source flow after several time steps. To maintain a constant noise frequency, a small amount of new noise is injected into N B at every iteration (step 8). Through experimentation, we found that randomly inverting the color of two to three percent of the noise texels at each time step is enough to maintain a high frequency noise that is approximately constant without a significant loss of temporal correlation.</p><p>In practice, an invariant black texture with a 2-3 percent random distribution of white texels, % N T , is XORed into N B with an OpenGL blending mode. The injection process affects a different set of texels at each time step by applying a random texture translation matrix to % N T . The content of the noise buffer is read back into the noise texture N T in step 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Noise Blending</head><p>We introduce an acceptable level of spatial and temporal correlation into each frame by applying a one-sided exponential filter to the sequence of frames. This effect is implemented with standard alpha blending (step 10):</p><p>(1 )</p><p>The use of noise textures implies that the only spatial correlation after filtering is along a pathline segment. Besides smoothing the animation, the blending process adds directional information to static frames, a feature not present in <ref type="bibr" target="#b12">[13]</ref> for example. A twocolor black and white noise maximizes the contrast of the final blended image. Good visual results are obtained with 0.1 α = . The image in C B can be saved as a final animation frame or be used for further image enhancements (Section 4.8). . Notice the gaps of increasing size that result from the constant divergence of the particle paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Coordinate Reinitialization</head><p>During the texture advection phase, the coordinate buffer ' x B was updated to reference the location of incoming particle properties and a new noise texture was computed from the advection of the current noise texture. The coordinate buffers must now be reinitialized in preparation for the next iteration, taking into account certain constraints imposed by the discrete nature of the algorithm. The displacement of the particle property between successive frames must be small enough to maintain a good spatio-temporal correlation. However, if the displacement of a particle is such that both old and new positions lie within the same pixel, the updated noise texel remains unchanged. Even worse, once the coordinates are reinitialized to their initial values (stored in 0 x T ) in step 11, any subpixel displacement (also called fractional displacement) is lost and cannot be recovered: the motion of the particle property is suppressed (step 5).</p><p>The above discussion suggests that the fractional displacements of particles be accumulated, and the noise texture be updated, once the accumulated displacement exceeds the width p w of a pixel. The distance from 0</p><p>x to '</p><p>x is the sum of an integer displacement vector</p><p>whose components are each an integral number of pixel widths, and a fractional displacement vector  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Image Enhancement</head><p>We propose two techniques to augment the information content of the animations. First, the brightness of regions of low velocity is reduced to better identify strong currents in the flow. Second, colored dye is introduced into the flow to visualize streaklines. <ref type="figure">Figure 6</ref> shows a single frame of wind patterns over Europe <ref type="bibr" target="#b16">[17]</ref>. High and low velocity regions are discerned thanks to the spatial correlation of the velocity field. However, the high frequency noise associated with regions of low velocity detracts the user from regions of interest related to higher-speed currents. To increase the contrast between these regions, we subtract the linear function </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1">Velocity Mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2">Dye Advection</head><p>Experimentalists have long tracked tracer particles, dye, and smoke to help understand the structure of unsteady flows <ref type="bibr" target="#b1">[2]</ref>. In our implementation of dye advection, the advected texture acts like a physical surface upon which dye is released. Dye is introduced into the flow between steps 8 and 9 by drawing geometric primitives (dots, lines, etc.) into the noise buffer N B . The algorithm then automatically advects the dye at no additional cost. In the final image, the dye is automatically subject to a temporal convolution of successive frames for increased smoothness. The dye is stored in the green and blue texture components while the noise is stored in the red component. Thus, multiple colored streaks can be tracked. <ref type="figure">Figures 5, 7</ref>, and 8 were produced in this way.</p><p>It is well known that in unsteady flows, streaklines, streamlines, and particle paths are different from one another. We test the validity of the dye advection algorithm on the uniformly rotating uniform flow ( , ) (cos( ),sin( )) u v t t =</p><p>.</p><p>In this flow, streamlines are straight, while streaklines and particle paths have circular trajectories. <ref type="figure">Figure 7</ref> shows two frames of an animation in which dye is released at three points in the flow. As expected, the streaklines are circular.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>The use of graphics hardware implies some constraints and restrictions due to the limited number of bits available to encode data.</p><p>Resolution limitation. The depth of the framebuffer has a direct impact on the spatial and temporal accuracy of the texture advection. In particular, it affects the number of bits that encode the fractional part of the coordinate displacement in the coordinate buffers. <ref type="figure">Figure 5</ref> shows the extreme case when no bits are available for the fractional part. Through experimentation, we found that four bits are necessary for good visual results. Under this constraint, a visual of 12 bits per color component only provides 8 bits to encode texture coordinates, which then limits the advection to 256 256 × textures. We have since addressed this limitation using a tiling algorithm <ref type="bibr" target="#b10">[11]</ref>.</p><p>Hardware resources. Hardware buffers and texture memory are limited resources. Our algorithm uses seven textures (see <ref type="figure">Figure 2</ref>). Using internal texture formats available in OpenGL that require the minimum amount of memory, the advection of 256 by 256 textures consumes less than 1.2 Mbytes of texture memory. In practice, the incomplete implementation of the pixel texture extension on SGI graphic boards requires that the textures mapped by this operation be 3D and RGBA (see manual page for glPixelTexGenSGIX). This limitation increases the required texture memory to 2.3 Mbytes, which can still reside in the four Mbyte texture memory of the Octane.</p><p>Multiple buffers are stored in each hardware framebuffer. They are arranged in a single hardware buffer as a non-overlapping array of 3 by 2 buffers of size N N × . We used the fact that ∆x B , M B , can share in turn the same space without conflict to reduce the required number of stored buffers from seven to six. 256 pixels wide buffers are easily accommodated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>This paper describes the first complete hardware-accelerated implementation of an algorithm to visualize unsteady flow based on a per-texel advection technique. It can simultaneously display velocity direction, velocity magnitude, and dye advection. A major advantage of this system is its ability to interactively compute long animation sequences.</p><p>We solved intrinsic problems that plague texture advection algorithms, particularly when they are applied to time-dependent data over extended periods:</p><p>• Incoming flow regions are handled with uncorrelated noise textures and image compositing.</p><p>• Long time advection is achieved through a restoration of the texture frequency at each time step without significant loss of temporal correlation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">OpenGL Specification, Version 1.2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arb</forename><surname>Opengl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">Visualized flow : Fluid Motion in Basic and Engineering Situations Revealed by Flow Visualization</title>
		<imprint>
			<publisher>Pergamon Press</publisher>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://www.sgi.com/-software/opengl/advanced98/notes/notes.html" />
		<title level="m">Advanced Graphics Programming Techniques Using OpenGL. SIGGRAPH &apos;98 Course</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imaging Vector Fields Using Line Integral Convolution. Computer Graphics Proceedings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Leedom</surname></persName>
		</author>
		<idno>0-201- 58889-7</idno>
	</analytic>
	<monogr>
		<title level="m">Annual Conference Series</title>
		<editor>James T. Kajiya</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993-08" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Comparing LIC and Spot Noise. IEEE Visualization &apos;98</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>De Leeuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Liere</surname></persName>
		</author>
		<editor>David Ebert, Hans Hagen, and Holly Rushmeier</editor>
		<imprint>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
	<note>ISBN 0-8186-9176-X</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced Spot Noise for Vector Field Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>De Leeuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Liere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Visualization &apos;95</title>
		<editor>D. Silver and G. M. Nielson</editor>
		<meeting>Visualization &apos;95</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spotting Structure in Complex Time Dependent Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>De Leeuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Liere</surname></persName>
		</author>
		<idno>SEN-R9823</idno>
		<imprint>
			<date type="published" when="1998-09" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using Line Integral Convolution for Flow Visualization: Curvilinear Grids, Variable-Speed Animation, and Unsteady Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Forssell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="141" />
			<date type="published" when="1995-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<title level="m">Applications of Pixel Textures in Visualization and Realistic Image Synthesis. ACM Symposium on Interactive 3D Graphics</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999-04" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Creating Evenly-Spaced Streamlines of Arbitrary Density. Visualization in Scientific Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jobard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lefer</surname></persName>
		</author>
		<editor>W. Lefer and M. Grave</editor>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer Verlag</publisher>
			<biblScope unit="page" from="43" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tiled Hardware-Accelerated Texture Advection for Unsteady Flow Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jobard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erlebacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hussaini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visualizing Time-Varying Phenomena In Numerical Simulations Of Unsteady Flows NASA Ames Research Center, Visualizing Time-Varying Phenomena In Numerical Simulations Of Unsteady Flows NAS-96-001</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flow visualization using moving textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASE/LaRC Symposium on Visualizing Time Varying Data</title>
		<editor>David C. Banks, Tom W. Crockett, and Stacy Kathy</editor>
		<meeting>ICASE/LaRC Symposium on Visualizing Time Varying Data</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">3321</biblScope>
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
	<note>NASA Conference Publication</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A New Line Integral Convolution Algorithm for Visualizing Time-Varying Flow Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and Resolution Independent Line Integral Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stalling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics Proceedings. Annual Conference Series</title>
		<imprint>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-Guided Streamline Placement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banks</surname></persName>
		</author>
		<idno>ISBN 0-201-94800-1</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings, Annual Conference Series</title>
		<editor>Holly Rushmeier</editor>
		<imprint>
			<biblScope unit="page" from="453" to="460" />
		</imprint>
	</monogr>
	<note>Proceedings of SIGGRAPH 96</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Liere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cwi</surname></persName>
		</author>
		<imprint>
			<pubPlace>Amsterdam, the Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spot Noise-Texture Synthesis for Data Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIG-GRAPH 91)</title>
		<imprint>
			<date type="published" when="1991-07" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
