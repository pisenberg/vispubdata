<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward a Compelling Sensation of Telepresence: Demonstrating a portal to a distant (static) office</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Towles</surname></persName>
							<email>towles@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Nyland</surname></persName>
							<email>nyland@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Welch</surname></persName>
							<email>welch@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
							<email>fuchs@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Toward a Compelling Sensation of Telepresence: Demonstrating a portal to a distant (static) office</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Telepresence</term>
					<term>Tele-Immersion</term>
					<term>Virtual Reality</term>
					<term>Collaborative Visualization</term>
					<term>Immersive Display</term>
					<term>Augmented Reality</term>
					<term>Human-Computer Interface</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In 1998 we introduced the idea for a project we call the Office of the Future. Our long-term vision is to provide a better everyday working environment, with high-fidelity scene reconstruction for life-sized 3D tele-collaboration. In particular, we want a true sense of presence with our remote collaborator and their real surroundings. The challenges related to this vision are enormous and involve many technical tradeoffs. This is true in particular for scene reconstruction. Researchers have been striving to achieve real-time approaches, and while they have made respectable progress, the limitations of conventional technologies relegate them to relatively low resolution in a restricted volume. In this paper we present a significant step toward our ultimate goal, via a slightly different path. In lieu of low-fidelity dynamic scene modeling we present an exceedingly high fidelity reconstruction of a real but static office. By assembling the best of available hardware and software technologies in static scene acquisition, modeling algorithms, rendering, tracking and stereo projective display, we are able to demonstrate a portal to a real office, occupied today by a mannequin, and in the future by a real remote collaborator. We now have both a compelling sense of just how good it could be, and a framework into which we will later incorporate dynamic scene modeling, as we continue to head toward our ultimate goal of 3D collaborative, telepresence.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite several decades of unprecedented advances in computing technology, there has been little change in the number of pixels we see, or in the way we interact with the computer. Our dream, depicted in <ref type="figure">Figure 1</ref>, is that some day your office would be equipped with ceiling-mounted digital cameras and "smart" projectors that work together to support high-resolution projected imagery, human-computer interaction, and dynamic image-based modeling throughout the office <ref type="bibr" target="#b21">[22]</ref>. We foresee a future when we will be able to interact with our colleagues in any locale just as if they were across the table in our office.</p><p>However we are still years away from the full realization of our long-term vision. Network transport issues aside, a primary challenge in realizing our vision is real-time, high quality scene reconstruction (acquisition/display of geometry and textures) of real environments. While system design is a multi-dimensional optimization task, <ref type="figure" target="#fig_0">Figure 3</ref> illustrates the fundamental design space tradeoff between scene quality and acquisition performance. To date, researchers have largely explored the lower left quadrant of this space. One would naturally expect this activity to expand along the diagonal as the research community pushes toward the ultimate goal of high quality and real-time scene reconstruction.</p><p>We have chosen to take a slight "detour" from the normal diagonal development thrust of <ref type="figure" target="#fig_0">Figure 3</ref>. In order to capture a glimpse today of what the future may hold, we chose to remove the realtime capture constraint and to focus our efforts on recreating the most compelling visual experience possible. Even though the realization of a natural and convincing immersive environment is still quite complex, we believe that we have reached a milestone in our research by demonstrating for the first time a truly compelling portal to a distant office <ref type="figure">(Figure 2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For many years researchers have been using two-dimensional video textures and projector displays to realize immersive telepresence. Two relevant examples include <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b10">[11]</ref>. They use carefully constructed geometric models for the office environment, and video-based human avatars obtained by separating the remote participants from the original background (via delta-keying). The results are presented to a head-tracked user. The TelePort display is built into a room that is carefully designed to match the rendered room <ref type="bibr" target="#b10">[11]</ref>. The goal is for the virtual room to seem as an extension of the real room. In both instances, because the background is synthetically modeled, it is not a faithful representation of the remote collaborator's real office. In <ref type="bibr" target="#b26">[27]</ref>, a perspectively-correct 2D live video conferencing system is proposed. This system offers high-resolution projected imagery, but because the user is not tracked the images appear geometrically distorted when the viewer is not in the system's "sweet spot". Billinghurst et al. have developed a very unusual and compelling Augmented and Virtual Reality tele-collaborative system that attaches miniature video avatars of remote collaborators to small cards in the local viewer's real space <ref type="bibr" target="#b2">[4]</ref>. While this may be effective for some applications, we are interested primarily in 3D reconstructions that are scaled and located to give the appearance of sitting across the table.</p><p>Three-dimensional computer vision-based techniques typically seek to use multi-baseline correspondence to estimate a model of the scene <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b8">9]</ref>. However, given the limitations of today's technology, these methods generate only low-resolution depth maps. Visual hull techniques <ref type="bibr" target="#b15">[16]</ref> can deliver approximate geometry in real time, but are primarily suited for outside-looking-in camera configurations. There is no clear extension to an inside-lookingout configuration, which would be necessary for reconstructing an environment such as an office.</p><p>The primary benefit of using some form of image-based rendering <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref> is that it is targeted at the reconstruction of real objects or environments, and does not require an explicit 3D representation of the scene. However, given the display resolution and working volume we desire, the image-based rendering cost would be quite significant compared to the use of conventional textured polygons.</p><p>Finally, one might imagine mounting a remote stereo camera rig on a head-slaved robotic arm. However, electro-mechanical limitations introduce significant latency which would result in geometric mis-registration between the local and remote scenes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>In analyzing the task to demonstrate a portal to another office, we identified five key components of technology needed:</p><p>Acquisition of a remote scene with color and depth information. Modeling and conversion of the acquired data into textured 3D geometry. Tracking of the local user's eye positions.</p><p>Rendering of the models based on the tracked user position. Presentation in stereo for the user.</p><p>In considering these needs, we further established a number of implementation priorities. First among them was the remote office environment. It had to be a real office and not a mixture of real and synthetic components. It should be acquired and presented with the best visual fidelity and resolution possible. In fact, we wanted the scene definition to exceed the display resolution so as the display resolution improved we could repeat the experiment with the same acquisition data and/or techniques. Of course, as previously discussed we knew we wanted to explore the operating space that non-realtime acquisition would afford.</p><p>In addition, our previous virtual reality work taught us the importance of accurate and low latency tracking in creating the correct visual cues without lag or swim effects. As for rendering, we wanted to achieve 30fps, and we knew we would apply the projection technology we had been working with in our Office of the Future research. We also hoped to apply the dual-projector, passive stereo techniques we had recently developed.</p><p>The following sections individually address the five technology areas and detail our implementation decisions and methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Acquisition</head><p>For scene acquisition, we selected a 3D scanning laser rangefinder system <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">1]</ref> because of the unmatched quality and density of data provided compared to other camera-only, vision-based techniques <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>This system <ref type="figure" target="#fig_1">(Figure 4</ref>), which was developed in-house, captures range data in spherical coordinates -scanning the room in elevation passes before stepping to the next azimuth position. Range data accuracy is approximately 4mm RMS at 1 milliradian angular resolution in azimuth and elevation. Scene color is acquired by replacing the scanner with a calibrated high-resolution digital camera matched to the same center of projection. Color images are taken  at azimuthal steps of 24 degrees. Acquisition time for a scene scan of 180 degrees azimuth is approximately 30 minutes. <ref type="figure" target="#fig_2">Figure 5</ref> illustrates the layout of the office to be scanned. We positioned a table in the middle of the room with a mannequin seated behind it. The table effectively divided the 16 ft. x 12 ft. office into two parts -an 8 ft. x 12 ft. area to be scanned and later rendered (upper portion of figure), and the area from which the laser rangefinder system scanned the scene (lower portion of figure).</p><p>To produce a complete scene definition of everything behind the table, we took four scans from different positions to minimize the possibility of missing scene data due to occlusions. The scanning positions were chosen to achieve full scene definition, making use of the knowledge that the environment would be viewed from the eye height of a seated adult, through a 4 ft. x 3 ft. portal, from a distance of approximately three feet opposite the table and mannequin. To further aid in determining the scan positions we created a temporary cardboard portal mockup by cutting out an appropriately sized opening in a large piece of cardboard. We hung the cardboard portal mockup from the ceiling over the table (in front of the mannequin) so that we could clearly ascertain what would be visible through the virtual portal when we projected the reconstructed scene onto a display of the same dimensions. <ref type="figure" target="#fig_3">Figure 6</ref> shows a snapshot of the office scene with the cardboard portal mockup hung from the ceiling. Each of the four scans produced a 2924 x 1754 resolution range image with color images of similar angular resolution that covered 150 degrees in azimuth and 90 degrees in elevation. The color and range images were registered by interactively selecting correspondences between the range image and each of the color images. Because the instrinsic parameters of the color camera were already known from a priori calibration, after obtaining the extrinsic parameters with the correspondences, we were able to properly map depth values onto the color images. Final depth images were then generated by sampling depth values for each pixels in the color images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling</head><p>Several methods have been proposed for model reconstruction from range images <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6]</ref>. We adapted methods to combine the data from multiple range scans and high-resolution images to construct a unified polygonal model of the remote office with minimum occlusions.</p><p>After collecting the four scans of the office-mannequin scene, we employed reconstruction and simplification steps to reduce the data from more than 20 million points to a model of approximately 30K triangles ( <ref type="figure" target="#fig_4">Figure 7</ref>). High-resolution color images from the scanned scene were then textured on the polygonal mesh. The images required approximately 48 MBytes of texture memory in the rendering system. Details of the geometric simplification methodology follow.</p><p>Reconstruction Since each physical object in the office can be seen from more than one scan locations, duplicate samples can be removed for faster rendering. We choose to keep samples with better scan-quality, and discard the ones with worse quality. Samples with local surface normals closer to the gazing direction of the scanner are considered to have higher quality <ref type="bibr" target="#b16">[17]</ref>. As the registration among different scans is not exact, some of the redundant triangles are not removed successfully. To improve rendering speed, we manually intervened to eliminate many of these duplications. After redundant samples are removed, an input mesh is generated for each depth image using regular meshing techniques <ref type="bibr" target="#b14">[15]</ref>. These input meshes need to be processed before simplification. First, physically disjoint objects can be falsely connected on silhouette edges. These false triangles can be removed by detecting empty space using scans from multiple locations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b5">6]</ref>. Although no triangles are removed incorrectly, not all false triangles can be removed successfully if the number of acquisition scans is insufficient. Therefore, we also removed triangles based on several heuristics including triangle orientation (with respect to the corresponding laser scan beam) and triangle aspect ratio <ref type="bibr" target="#b18">[19]</ref>. Simplification After false triangles and redundant triangles are removed, the resulting mesh is simplified. We have adapted a fast and robust quadric simplification method <ref type="bibr" target="#b9">[10]</ref> to generate the smaller dataset. The density of triangles closer to the user is made higher by assigning higher quadric weights during simplification. In the end, the degree of mesh simplification is driven by the desire to maintain a high rendering frame rate. To further increase the rendering speed, the triangles are processed into triangle strips <ref type="bibr" target="#b7">[8]</ref>. <ref type="figure" target="#fig_5">Figure 8</ref> illustrates the steps for constructing the geometric model from the depth images. Since the complexity of our model is driven by the rendering performance, the modeling process is very lengthy and currently requires much user intervention. As stated, the majority of time was spent on manually removing duplicated samples that were not automatically deleted due to registration errors among the different scanned views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tracking</head><p>Our application is more sensitive to tracking than typical VR applications, as the rendered scene is observed together with the real environment. In this situation, tracker latency and error can translate into dynamic and static registration error, respectively <ref type="bibr" target="#b0">[2]</ref>. Dynamic registration errors cause the moving user to perceive shearing in the rendered scene, and these factors destroy the illusion of presence.</p><p>We chose to use the UNC-developed HiBall TM Wide-Area Tracking System <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr">1]</ref> because of its superior performance characteristics compared to other wide-area trackers such as magnetic based systems. The HiBall tracker provides new estimates of tracker position and orientation over 1500 times per second with a measured RMS position and orientation noise of less than 0.5 millimeters and 0.02 degrees, respectively. The system uses a array of infrared LED beacons mounted in the overhead ceiling panels and an optical sensor (the HiBall) mounted to an adjustable headband that the user wears <ref type="figure" target="#fig_6">(Figure 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Rendering</head><p>The triangles that have been processed into triangle strips are rendered using projective textures to address texture distortion. Since our scene is largely diffuse, in order to increase the rendering speed, we currently do not exploit view-dependent texture-mapping techniques such as <ref type="bibr" target="#b6">[7]</ref>. The simplified model is currently rendered in stereo on two pipes of SGI InfiniteReality2, one for each eye. On two genlocked graphics pipes, the current frame rate is 30fps for each eye.</p><p>Since we are using roughly-aligned projector pairs, the displayed images of the two projectors are not aligned and need to be calibrated. By finding at least four corresponding pixels between the two projectors, we can evaluate the 3D collineation/homography matrix <ref type="bibr" target="#b20">[21]</ref> that relates the two images. In practice, we find the image coordinates for each projector that map to the same four points on the corners of the display screen and solve for two collineation matrices. These matrices, À Ð Ø Ý and À Ö Ø Ý , are then used to modify the viewing transformation of their respective view transform so as to achieve projector pair alignment to within a sub-pixel error threshold. This step introduces no extra rendering cost.</p><p>Coordinate systems need to be registered among modeling, tracking and physical world spaces. The transformation from the tracker to the world space is solved by finding the tracker readings on the corners of the physical screen. Scene modeling space is registered to the world coordinate with the transformation by specifying the location and orientation of the physical window inside the model. While orientation and positioning of the virtual office is critical, equally important is accurate scale. Inter-pupiliary distance (IPD) and eye positions are critical in resolving the scale of the scene. We currently measure the user's IPD before using our system. Combined with the knowledge of the tracker's orientation and position on the headband, we can estimate the position of each eye relative to the tracker.</p><p>Given the above transformation, at each screen update we read the most recent tracker position/orientation from the networked HiBall TM tracker server and calculate the virtual eye position inside the model coordinate system. A view frustum for each eye is formed by connecting each virtual eye position to the four corners of the screen and making the projection plane parallel to the virtual location of the screen. Finally, since the collineation matrices effectively map the corners of the computed images to the pixel coordinate corresponding to the physical corners of the screens, the two matrices À Ð Ø Ý and À Ö Ø Ý are applied (respectively) to the left-eye and right-eye transformation matrix stacks.</p><p>To verify the registration accuracy of our system, we rendered virtual spheres relative to know positions of a set of hanging plumb weights as shown in <ref type="figure">Figure 10</ref>. As a result of remaining rendering latency, some small dynamic mis-registration is observable for a fast-moving user, but the static registration is quite accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Presentation</head><p>To convincingly present the virtual office scene, it must be displayed in life-size, stereo, and properly registered with the real world. We chose to render the stereo image with two LCD pro- <ref type="figure">Figure 10</ref>: Registration verification between real physical objects (plumb weights) and computer-generated spheres below them. jectors, one for each eye, fitted with circularly polarizing filters. This approach simultaneously presents left and right eye views at the maximum rate, rather than in a time-division multiplexed manner <ref type="bibr" target="#b3">[5]</ref> or by sharing the spatial resolution <ref type="bibr" target="#b23">[24]</ref> of a single projector. In our solution, the user wears only a lightweight pair of passively polarized glasses <ref type="figure" target="#fig_6">(Figure 9</ref>). Though all LCD projectors are linearly polarized, not all projectors are built with the RGB imagers polarized in the same linear plane. By selecting a projector that does, we were simply able to add a ½ retarder filter in front of the lens to produce circularly polarized light. By orienting the filter on the respective projectors 90 degrees apart, the polarization can be made clockwise for one eye and counter-clockwise for the other -thus creating the needed stereo separation. Stereo with circular polarization, rather than simple linear polarization, provides the advantage that left-right eye discrimination is independent of the head angle of the user and passive glasses. We used a pair of Sharp XG-3000U projectors augmented with polarization filters we assembled, and polarized glasses from StereoGraphics Corporation. To preserve polarization, the display surface is covered with a silver lenticular screen fabric (GN92448) from Draper, Inc.</p><p>Our rendering process does not account for nonlinear distortions (such as radial distortions) in the projected imagery. However we are able to adjust the projectors to minimize such distortions before calibration. In particular, we observed that while adjusting the zoom for the projectors, the type of radial distortion changed from barrel-type to pincushion-type, with a "sweet spot" in between where minimal distortion can be reached. Such an approach in practice alleviated the problem significantly.</p><p>Other aspects of the viewing/presentation environment that called for some judicious decisions were related to the size and position of the projected window. The size of 4 ft. ¢ 3 ft. was selected to enable life-size display of the mannequin and provide a reasonable viewing window while maintaining high display pixel density and image brightness. The desk used under the window in the real office is the exact same desk that was scanned with the mannequin. This further ensured an additional degree of continuity from the real to the virtual worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Initial Reactions and Lessons Learned</head><p>We were so encouraged by the initial reactions of our departmental colleagues that we immediately wanted to get a more unbiased reaction from unsuspecting outsiders. So we invited two members of the university community, both professionally experienced with conventional video teleconferencing, to evaluate our system. To the The second subject was more emotional: "It's a bit eerie it's so realistic... it looks like somebody took a chain saw and cut a hole in the wall and he's on the other side..."</p><p>Since these initial tests and encouraging words, twenty-five or more persons have also had an opportunity to experience the demo. From these sessions, we have learned several important lessons that will be relevant when we conduct scientifically-controlled user studies of the system's effectiveness. First, we have seen evidence that a person's initial impressions of the system may be negatively affected by whether their first exposure is from the head-tracked, stereo position of the active participant or as an observer standing in the background only wearing stereo glasses. We speculate that if the initial experience is that of a casual observer one may become pre-biased compared to the person that first or only has a headtracked experience. This is based on the knowledge that only the head-tracked user is seeing a perspectively correct view and probably more importantly a view that is responding naturely to their head-motions.</p><p>Secondly, we have found that the quality of depth perception varies significantly between individuals and is truly impacted by our estimate of interpupiliary distance and geometric relationship to the head-mounted tracking system.</p><p>Lastly, when we run the stereo display with one SGI graphics pipe and the frame rate drops from 30fps to 15fps, the experience is much less compelling. Some of us speculate that the perceived effectiveness may fall apart rapidly when certain performance levels (e.g., frame rate) are not maintained. In fact, we are concerned that some researchers who have built similar systems in the past may not have been as encouraged simply because system performance fell below some acceptable psychological threshold. Brooks, Whitton, et al. here at UNC are developing sound scientific methods for measuring perceptual effectiveness and sensitivity to variations in a variety of system parameters. <ref type="figure" target="#fig_7">Figure 11</ref> and the short compressed movie on the CD-ROM distribution show the system in operation.</p><p>When the first subject immediately started to move into the scene and the second subject spontaneously described it as cutting a hole into the wall of the next office, we realized that we may have reached our goal -that of giving a glimpse of what future, naturally immersive environments are going to be like.</p><p>The major challenge now is to advance this research from a proof-of-concept demo of a high quality, static scene to a fully working high-quality, dynamic 3D telepresence system. In that spirit, we are currently:</p><p>Developing real-time point cloud, geometric meshing algorithms Investigating fast, intuitive methods for calibrating the eye positions of individual users with respect to head-mounted trackers Exploring tracker technology which does not encumber the user (no headware and wireless) Applying multiple, intensity blended, front-projector arrays for increased resolution, area and brightness Seeking new stereo projectors to replace our current solution that is dependent on a projector polarization characteristic that is no longer available. Exploring new algorithms and hardware architectures for realtime image-based renderinḡ Discussing more extensive user-studies for quantifying the perceptual quality of our results and sensitivity to various system parameters But first and foremost, we realize that the laser scanning technology we used in this experiment does not directly promise offer a real-time solution. Therefore, as part of the National Tele-Immersion Initiative we are actively incorporating real-time reconstruction techniques, developed at the University of Pennsylvania's GRASP Lab <ref type="bibr" target="#b17">[18]</ref> under the leadership of Ruzena Bajcsy and Kostas Daniilidis, with our high-fidelity, non-realtime scanned office environments. We are also exploring the application of real-time, scene acquisition technologies based on visual hull algorithms developed by Leonard McMillan and his research team at MIT <ref type="bibr" target="#b15">[16]</ref>. While today's real-time capabilities support only limited working volumes and resolution, these investigations are necessary steps toward our ultimate goal.</p><p>Although many years of work remain, we are increasingly optimistic that the user-computer interface will no longer be limited to a desktop monitor, but will provide a more compelling, immersive environment for an increasing variety of applications, in particular those that bring distant people closer together.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>An important tele-immersion design space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Laser scanner used for scene acquisition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The layout of the office and four scanning positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The real office before scanning, with a cardboard portal mockup hung from the ceiling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>A portion of the polygon mesh after simplification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Steps of geometric modeling from depth images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Left: Head-mounted optical tracker and separate passively-polarized stereo glasses. Right: Circularly-polarized stereo projector pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Working system with the tracker placed on the user's head and rendering in stereo mode. question of how this system compared with conventional tele-video, the first subject responded, "It's less clear in some respects, but of course, one of the things that's different is I can lean into[it, and]  look into the picture where in teleconferencing you can't."</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>Support for this research is generously provided by the National Tele-Immersion Initiative (NTII) sponsored by Advanced Network and Services, Inc. and NSF Cooperative Agreement no. ASC-8920219: "Science and Technology Center for Computer Graphics and Scientific Visualization". We also thank the Intel Corporation for their generous equipment donations as part of Technology for Education 2000.</p><p>We would also acknowledge the support of our fellow research team members at UNC-CH, listed at www.cs.unc.edu/Research/stc/office/index.html, and graciously credit and thank David McAllister and Steven Matuszek for their time and software utilities used in processing the rangefinder data. We thank Andrew Nashel for his help in putting together the video for the paper. We thank John Thomas and Kurtis Keller for their engineering assistance, and finally we thank Andrei State for his visionary sketch <ref type="figure">(Figure 1</ref>).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving Static and Dynamic Registration in an Optical See-Through HMD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH &apos;94 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1994-07" />
			<biblScope unit="page" from="197" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3-D Reconstruction of Environments for Virtual Collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruzena</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reyes</forename><surname>Enciso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerda</forename><surname>Kamberova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucien</forename><surname>Nocera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE Workshop on Applications of Computer Vision</title>
		<meeting>4th IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shared Space: Collaborative Augmented Reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;99 Conference abstracts and applications</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surround-screen Projection-based Virtual Reality: The Design and Implementation of the CAVE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Cruz-Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Sandin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>De-Fanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;93</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m">Conference Proceedings</title>
		<imprint>
			<date type="published" when="1993-08" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Volumetric Method for Building Complex Models from Range Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH &apos;96 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient View-Dependent Image-Based Rendering with Projective Texture-Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">D</forename><surname>Borshukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Rendering Workshop</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing Triangle Strips for Fast Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitabh</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;96</title>
		<imprint>
			<date type="published" when="1996-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Three-Dimensional Computer Vision: A Geometric Viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Surface Simplification Using Quadric Error Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Heckbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;97 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1997-08" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Arapis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">J</forename><surname>Breiteneder</surname></persName>
		</author>
		<title level="m">TELEPORT-Towards Immersive Copresence. Multimedia Systems</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">F</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lumigraph</surname></persName>
		</author>
		<title level="m">SIGGRAPH &apos;96 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1996-08" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Virtual Meeting in Cyberstage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vali</forename><surname>Lalioti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hasenbrink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Virtual reality software and technology</title>
		<meeting>the ACM Symposium on Virtual reality software and technology</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Light Field Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;96 Conference Proceedings</title>
		<imprint>
			<publisher>August</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Post-Rendering 3D Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="1997-04" />
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-based Visual Hulls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 2000 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2000-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Real-Time Rendering of Real-World Environments. Eurographics Rendering Workshop 1999</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">F</forename><surname>Nyland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Voicu</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Lastra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mccue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">View-independent Scene Acquisition for Tele-Presence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Augmented Reality</title>
		<meeting>International Symposium on Augmented Reality</meeting>
		<imprint>
			<date type="published" when="2000-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">View-based Rendering: Visualizing Real Objects from Scanned Range and Color Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kari</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Rendering Workshop</title>
		<imprint>
			<date type="published" when="1997-06" />
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Virtualized Reality: Constructing Time-Varying Virtual Worlds from Real World Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;97</title>
		<imprint>
			<date type="published" when="1997-11" />
			<biblScope unit="page" from="277" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Immersive Planar Display using Roughly Aligned Projectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Virtual Reality</title>
		<imprint>
			<date type="published" when="2000-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Office of the Future: A Unified Approach to Image-Based Modeling and Spatially Immersive Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Cutts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Stesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;98 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zippered Polygon Meshes from Range Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;94 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1994-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Vrex</surname></persName>
		</author>
		<ptr target="http://www.vrex.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SCAAT: Incremental Tracking with Incomplete Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;97 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1997-08" />
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The HiBall Tracker: High-Performance Wide-Area Tracking for Virtual and Augmented Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leandra</forename><surname>Vicci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Brumback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurtis</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Virtual Reality Software and Technology</title>
		<imprint>
			<date type="published" when="1999-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geometrically Correct Imagery for Teleconferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Brent</forename><surname>Seales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia &apos;99 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
