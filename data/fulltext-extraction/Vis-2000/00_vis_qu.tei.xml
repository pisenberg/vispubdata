<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Based Rendering With Stable Frame Rates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huamin</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Computing (CVC)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Stony Brook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Computing (CVC)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Stony Brook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafa</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Computing (CVC)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Stony Brook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Kaufman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Visual Computing (CVC)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Stony Brook</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Based Rendering With Stable Frame Rates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image-based rendering</term>
					<term>ray casting</term>
					<term>voxel-based modeling</term>
					<term>terrain rendering</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper presents an efficient keyframeless image-based rendering technique. An intermediate image is used to exploit the coherences among neighboring frames. The pixels in the intermediate image are first rendered by a ray-casting method and then warped to the intermediate image at the current viewpoint and view direction. We use an offset buffer to record the precise positions of these pixels in the intermediate image. Every frame is generated in three steps: warping the intermediate image onto the frame, filling in holes, and selectively rendering a group of &quot;old&quot; pixels. By dynamically adjusting the number of those &quot;old&quot; pixels in the last step, the workload at every frame can be balanced. The pixels generated by the last two steps make contributions to the new intermediate image. Unlike occasional keyframes in conventional image-based rendering which need to be totally rerendered, intermediate images only need to be partially updated at every frame. In this way, we guarantee more stable frame rates and more uniform image qualities. The intermediate image can be warped efficiently by a modified incremental 3D warp algorithm. As a specific application, we demonstrate our technique with a voxel-based terrain rendering system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Real time rendering in a walk-through or fly-through system has been a challenge in the computer graphics field for decades. The dataset of the scene model is often so large that even the most advanced rendering hardware cannot provide interactive rates. Various methods have been used to accelerate the rendering. A great deal of previous work has focused on visibility culling and levelof-detail management.</p><p>More recently, image-based rendering (IBR) techniques are used in walk-through or fly-through systems. IBR techniques exploit the frame-to-frame coherence by reusing the previously rendered images. This generally involves keyframes (or reference frames) and warping processes. Key frames are usually rendered by conventional rendering techniques from the original dataset and have high quality. The derived frames can be generated by warping the keyframes and filling in holes if needed. Key frames can be either generated in a preprocessing phase or rendered on the fly. However, in some applications, keyframes can only be rendered on the fly. For example, in a fly-through system, users fly over a large terrain and the fly routes change interactively. It is almost impractical to pregenerate all keyframes and use them because of the huge number of keyframes needed. Thus, for such applications, keyframes are generally rendered on the fly.</p><p>However, there are some disadvantages to this keyframe method. Generally, the time to render a keyframe is higher than the time to generate a derived frame. This causes the unstable frame rates between keyframes and derived frames. The user can feel the "pause" or dramatic change in the frame rate during the fly-through. Another problem of the keyframe method is the nonuniform image quality among frames. Key frames have better image qualities than derived frames. The derived images generated at the viewpoint near the keyframes' viewpoint have better qualities than the images generated at the viewpoint far from the keyframes' viewpoint.</p><p>Most of the previous work uses this keyframe IBR technique on surface models. When we try to use this keyframe technique in a voxel-based scene model, problems become more serious. The conventional rendering technique in a voxel-based scene model is ray casting. The ray-casting method can generate a high quality image, but it is time-consuming. Thus, the cost to generate a keyframe on the fly is high, and the unstable frame rate problem becomes more serious.</p><p>One advantage of the ray-casting rendering method is that it is easy to render any pixel in an image by casting one ray. The holes in the derived frames can be filled in by the ray-casting method. Actually, the ray-casting method can easily render any part, even scattered pixels of an image. We try to take advantage of this feature in this work. The basic idea is to use an image warping technique to gain a high frame rate, and to use the ray-casting method to selectively rerender part of the image to guarantee image quality.</p><p>We present an efficient image-based rendering technique without keyframes in the context of voxel-based scene modeling. We use an intermediate image to exploit the coherence in frames. The pixels in the intermediate image are all first generated by the ray-casting method in previous frames and then warped to the intermediate image at the current viewpoint and view direction. We relax the requirement that all pixels in an image are located in a regular rectangular grid. We use an offset buffer to record the precise position of these pixels in the intermediate image. Every frame is generated in three steps : warping the intermediate image, filling in holes and selectively rendering some other pixels. This last step can guarantee the image quality. By dynamically adjusting the number of those pixels in the last step, we can balance the workload at every frame. The pixels generated by the last two steps make contributions to the new intermediate image.</p><p>McMillan and Bishop's 3D warp algorithm <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> can be successfully adapted to warp the intermediate image. We show that the intermediate image can still be warped in McMillan's occlusion compatible order <ref type="bibr" target="#b8">[9]</ref>. By quantizing the offset of pixels in the intermediate image and using lookup tables, the advantage of the incremental computation of McMillan and Bishop's algorithm can be preserv ¥ ed. The advantage of our method over the keyframe method is that the keyframe needs to be rerendered totally, but the intermediate image only needs to be updated partially at every frame. By combining the advantages of the image-based rendering techniques and the ray-casting method, we can get more stable frame rates and more uniform image qualities. Our method is especially useful in a real time system when the keyframe rendering is time consuming. We demonstrate our method with a voxel-based terrain rendering system. This paper is organized as follows. Section 2 reviews the previous work in image-based rendering techniques and voxel-based terrain modeling. Our keyframeless image-based rendering technique is described in detail in Section 3, which focuses on the construction and warping of the intermediate image. Section 4 presents results from our implementation. Conclusions and future work are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image-based rendering</head><p>In recent years, there have been many papers on image-based rendering. Mark's <ref type="bibr" target="#b7">[8]</ref> is a good survey. Some researchers have used the keyframe method to accelerate rendering in a walk-through or fly-through system.</p><p>Mark <ref type="bibr" target="#b6">[7]</ref> has warped two different reference images and composited the results to avoid occlusion-related artifacts. The reference images are generated on the fly based on the prediction of the future viewpoint and view direction. The reference frames are rendered at 5 frame/sec, and derived frames can be generated at 30 frame/sec.</p><p>Popescu et al. <ref type="bibr" target="#b11">[12]</ref> have warped layered depth images in the context of an architectural walk-through system. The layered depth images are synthesized in a preprocessing phase for every portal.</p><p>Chen et al. <ref type="bibr" target="#b1">[2]</ref> used a hybrid LOD-Sprite technique to accelerate terrain rendering. A sprite image is generated from high-resolution scene geometry at every keyframe. Then the sprites are reused by texture mapping in the following frame using low-resolution scene geometry. The keyframes are generated on the fly. However, the time to render a keyframe can be as high as three times more than the time to render a derived frame.</p><p>Instead of using an image to represent the entire scene, the imposter methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> uses image to represent remote objects in a scene.</p><p>Among various warping algorithms, McMillan and Bishop's 3D warp method <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> is an efficient algorithm. It warps the image with depth information, so it can provide proper parallax and can be implemented efficiently by incremental computation of the 3D warp equation and by using an occlusion compatible ordering algorithm to resolve occlusion without z-buffering. McMillan and Bishop's algorithm relies on the epipolar geometry. Given two images, the epipole in the second image is the projection of the camera location of the first image into the second image. The epipolar lines are the image-space lines that pass through the epipole. The relative warp order for different epipolar lines does not matter, but the points on a single epipolar line must be warped in a particular order. Thus, the input image is split into sheets horizontally and vertically at the epipolar point. Every sheet is processed in a different scan line order. Popescu et al. <ref type="bibr" target="#b11">[12]</ref> pointed out that special attentions should be paid to the pixels on the row and column of the epipole because pixels in discrete images have a non-negligible area. They have to be warped either first or last, depending on the sign of the epipole.</p><p>Our work is based on McMillan and Bishop's algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Voxel-based terrain modeling</head><p>Most of the current flight simulators are based on the surface model. However the voxel-based approach has many advantages compared to a surface-based one. For example, the format of the elevation map lends itself to generating a very high resolution 3D volume of terrain and multi-resolution volumes. Also, texture mapping for voxels is much simpler, of higher quality, and can be preprocessed. More importantly, the voxel-based model is somewhat scene complexity independent, and it is easy to incorporate clouds, haze, flames, and other amorphous phenomena and volumetric objects <ref type="bibr" target="#b3">[4]</ref>. Therefore, in recent years, the voxel-based approach has become a new research issue for height-field visualization systems <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref>. Most of the published voxel-based terrain rendering algorithms directly use the 2D elevation map to represent 3D values, so that both memory size and accessing time for the terrain model can be greatly reduced <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref>. However, the quality of images generated from this model is not very good. Thus, a true 3D voxel-based terrain model has been generated in our previous voxel-based terrain visualization system <ref type="bibr" target="#b15">[16]</ref>. The terrain model consists of a 3D volumetric terrain dataset and a corresponding aerial photograph or satellite image of the terrain. There are also some disadvantages of this 3D voxel-based terrain model. The storage requirement for a 3D voxel-based terrain is high, but memories are becoming larger and relatively inexpensive. If there is no hardware support, it usually takes longer time to render a 3D voxel-based terrain by raycasting than to render a surface-based one with the hardware support for texture mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">3D Warp Without Keyframe</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Keyframeless rendering</head><p>In a walk-through or fly-through system, the viewpoint changes gradually. There is substantial coherence in adjacent frames. A straightforward way to take advantage of this is to render some keyframes by the ray-casting method and then warp the keyframes to get the next frames. When the error is beyond a threshold, the keyframe is rerendered. Because of the change of viewpoint, some new part of the scene enters the user's view and some occluded part of the scene in the keyframe can pop up in the new frame. This causes holes in the resulting image. These holes can be filled in by ray casting. <ref type="figure" target="#fig_4">Figure 1</ref> shows this method.</p><p>As we mentioned before, this keyframe method has the disadvantages of unstable frame rates and nonuniform image qualities. This method only takes advantage of the coherence between the keyframe and the derived frame. There are also coherences in the derived frames. Let's take a look at <ref type="figure" target="#fig_4">Figure 1</ref>. Frame 1 is the keyframe generated by using the ray-casting method. Frame 2 to Frame n are derived frames by warping Frame 1. Actually, at every derived frame, except the pixels warped from the keyframe, a certain amount of pixels have to be rendered by the ray-casting method because of the holes in the warped images. These pixels have a high image quality and likely will appear in the next frames because of the coherence in the adjacent derived frames. To make a difference, we use the term "high quality pixels" to refer to the pixels generated by the ray-casting method in a derived frame and "low quality pixels" to refer to pixels reconstructed from the warped image. Traditionally, the keyframe method does not take advantage of this.  <ref type="figure" target="#fig_4">Figure 1</ref>: A straightforward keyframe technique. The keyframe is rendered by the ray-casting method. Frames 1-n are rendered by warping the keyframe. The holes in frames 1-n are filled in by ray casting. When the error is beyond a threshold, the keyframe is rerendered. quickly after several such warps because most pixels in frame m-1 are from reconstruction and have low image quality.</p><p>2) Warping the high quality pixels from the previous m-1 frames to get frame m. Then, the Z-buffer has to be used to resolve the occlusion. We have to warp scattered points in m-1 frames. The advantage of the incremental computation does not exist.</p><p>We try to design an image-based rendering technique which exploits the coherences in the multi-frames which meet the following requirements : 1) Only high quality pixels can be warped. We do not warp the pixels from reconstruction. 2) No Z-buffer is needed to resolve the occlusion problem 3) Preserve the advantage of incremental computation. 4) No keyframe is needed.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, when a pixel is warped from the keyframe to frame 1, the center of the pixel generally does not fall exactly on the grid of the derived frame. Thus a reconstruction process is needed. However, if we store the exact position of the center of this pixel in frame 1, warping this pixel from frame 1 to frame2 will have the same effect as warping the original pixel from the keyframe to frame 2. We refer to the final reconstructed image as the displayed image, and refer to the warped image before reconstruction as the intermediate image. The exact positions of warped pixels in the interme- In order to make a difference between these two types of pixels, we use an age buffer to record the age of a pixel. If a pixel is just rendered in this image by the ray-casting method, its age is 0. If a pixel is warped from the previous intermediate image and is still visible at the current intermediate image, then its age is incremented by 1. By using the age buffer, we know exactly how many times a pixel has been warped from the intermediate image where this pixel is first generated by the ray-casting method to the current intermediate image. The older the pixel is and the longer the pixel stays in the intermediate image, the more likely it will disappear in the next frame, and more important, the more likely the image quality around this pixel will be degraded. Thus, the age of a pixel is a nice criteria for the image quality around this pixel.</p><p>In order to guarantee the image quality, we set a threshold for the ages of the pixels in the intermediate image. If the age of a pixel is older than this threshold, then we do not warp this pixel. This probably causes holes in the next intermediate image and these holes are filled by the ray-casting method.</p><p>Therefore ity pixels. By using the offset buffer to record the exact positions of these pixels, warping these pixels from the intermediate image has the same effect as warping the original pixels from the image where these pixels were first generated by the ray-casting method. By using the age buffer, we can control the image quality by setting a threshold for ages of pixels in the intermediate image. <ref type="figure">Figure 3</ref> shows our keyframeless IBR method.</p><p>Please note that Popescu et al. <ref type="bibr" target="#b12">[13]</ref> have used the offset buffer for a different purpose. They used the offset buffer to resolve the visibility of the pixels. We use the offset buffer to record the precise positions of the pixels in the intermediate image for further warping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Warping the intermediate image</head><p>The pixels in the intermediate image no longer fall in the regular rectangular grid. However, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the intermediate image can still be warped by McMillan and Bishop's ordering algorithm. As we mentioned before, there are two properties of the occlusion-compatible order <ref type="bibr" target="#b7">[8]</ref>: (1) the relative warp order for two reference-image points on different reference-image epipolar line does not matter; (2) the points on a single reference-image epipolar line must be warped in a particular order. Even though the scan line in the intermediate image is no longer a straight line, these two properties are still preserved. The formal proof of McMillan <ref type="bibr" target="#b10">[11]</ref> applies, so the ordering algorithm still works.</p><p>One advantage of McMillan and Bishop's 3D warp algorithm is the incremental computation. We use the notation of the pinhole camera model adopted by McMillan <ref type="bibr" target="#b10">[11]</ref>. Then the 3D warp equation is <ref type="bibr" target="#b7">[8]</ref> :</p><formula xml:id="formula_0">z 2 S 2 S 1 z 1 u 2 ¦ P § 1 2 P 1 u 1¨P § 1 2 S 1 z 1 © Ċ 1 Ċ 2 S ¦ a b a b c u 1 ¦ u 1 v 1 1 ! u 2 ¦ u 2 v 2 1 !</formula><p>The vectors a b c are the basis vectors for the camera coordinate system. P andĊ represent the pinhole camera viewing parameter and center of projection respectively for the images. The u 1 , u 2 are the image coordinates of the reference frame and the derived frame. z 1 and z 2 are the reference frame and derived frame depth values.</p><p>To compute the warped position of the next pixel along a scan line, incremental computation can be used : As mentioned before, we set a threshold for the ages of the pixels in the intermediate images. That means the number of times a pixel can be warped is bounded by a threshold. We can use a fixed point instead of a float point to improve the efficiency. We can quantize the offset of pixels into levels. Let's suppose at image space the one pixel distance is quantized into L levels. At every frame, the warped pixel position error introduced by the quantization is 0 , with c as a constant. Because the offset is quantized, we can precompute P § <ref type="bibr" target="#b0">1</ref> 2 </p><formula xml:id="formula_1">P § 1 2 P 1 u 1¨1 v 1 1 ! ¦ P § 1 2 P 1 u 1 v 1 1 ! P § 1 2 P 1 1 0 0 ! ¦</formula><formula xml:id="formula_2">P 1 $ # o f f setu 0 0 % and P § 1 2 P 1 &amp; # 0 o f f</formula><formula xml:id="formula_3">P § 1 2 P 1 u 1¨1¨o f f setu v 1¨o f f setv 1 ! ¦ P § 1 2 P 1 u 1 v 1 1 ! P § 1 2 P 1 1 0 0 ! P § 1 2 P 1 o f f setu 0 0 ! P § 1 2 P 1 0 o f f setv 0 ! ¦ start¨uincr¨table u ' o f f setu(¨table v ' o f f setv(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Filling in holes</head><p>The holes in the intermediate image are filled in by the ray-casting method. It can be done efficiently by exploiting the coherence between rays. For example, we show how to fill in holes in a voxel-based terrain rendering system. We fill in the holes in the warped image column by column. In each column, we cast one ray through each hole pixel bottom up, in order to speed up ray traversal, by exploiting the specific vertical ray coherence <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref> in terrain scenes. As shown in <ref type="figure">Figure 5</ref>, the basic idea of ray coherence is that for two viewing rays from the same camera that project onto the same line on the base plane of the terrain, the higher ray hits the terrain surface at a position farther away from the viewpoint. By exploiting this ray coherence, ray casting can be dramatically accelerated by skipping most of the empty space above the terrain surface.  <ref type="figure">Figure 5</ref>: The ray to fill in a hole can emanate from the hit point of the pixel just below it</p><p>During our hole filling procedure, ray casting for each image column is completed in the following steps: first, find the location of the lowest hole pixel and the depth of the pixel just below this pixel from the current intermediate image buffer. Cast a ray through this pixel and use the corresponding depth to skip the empty space along the ray. In a special case when the lowest hole pixel is at the bottom of the column, we have to traverse the ray from the image without any space leaping. Once the hit position is found along the ray, save the new depth value. Second, move upward along the column to the next hole pixel and cast a ray through it. If its lower pixel is a ray-casting pixel, we use its lower pixel depth for space leaping; otherwise, its lower pixel is a warped pixel, and we use the lower pixel depth from warping for space leaping. Third, save the new depth once the hit point is found. Fourth, repeat the second step, until no hole pixels are left or the current ray does not intersect with the terrain surface. Finally, search upward along the column for all remaining ray-casting pixels, and directly assign their colors to be background color. No ray casting is performed through these pixels, since they have no chance of intersecting with the terrain surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Balancing the workload</head><p>At every viewpoint, we need to warp the intermediate image at the previous viewpoint and fill in the holes. The time to warp an image is independent of the scene complexity and almost a constant. The time to fill in the holes by ray casting depends on the number of holes, which can vary from frame to frame. As we mentioned before, the holes are caused by : 1) Some region of terrain just enters the current view.</p><p>2) Some region of terrain occluded in the previous frame now pops up.</p><p>3) The ages of some pixels at the intermediate image are older than a threshold, so we do not warp these pixels.</p><p>The total number of these pixels should be controlled by our computational ability. We also hope that the numbers of pixels needed to be rerendered by ray casting at every frame are almost the same. We cannot control the number of holes caused by (1) (2). However, we can control the number of holes caused by (3) by adjusting the threshold. The way to balance the workload at every frame is to dynamically adjust the number of pixels caused by (3).</p><p>We set a threshold for the total number of pixels we can afford to rerender by ray casting. Then, at every frame, after warping the intermediate image, we count the number of holes. If the number is beyond our threshold, we can do nothing else except fill in these holes. However, if the number is below our threshold, we compute the difference of the threshold and the number of our current holes. Then we can select that number of pixels from the pixels which are the oldest in the intermediate image and rerender them by ray casting. In this way we can balance the workload and guarantee the image quality at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Reconstruction</head><p>Reconstruction is a difficult problem in image-based rendering techniques. Chang et al. <ref type="bibr" target="#b0">[1]</ref> use a bilinear kernel. Four LDI pixels are updated for each pixel of a reference image. Shade et al. <ref type="bibr" target="#b14">[15]</ref> use a rough approximation to the footprint evaluation optimized for speed. The image size is approximated by using a lookup table. The four splat sizes they used have 1 1,3 3, 5 5, and 7 7 pixel footprints. The alpha values are rounded to 1, 1/2, or 1/4. Therefore, the alpha blending can be done with integer shifts and adds. Mark et al. <ref type="bibr" target="#b6">[7]</ref> use a technique to treat the reference frame as a mesh. The 3D warp perturbs the vertices of the mesh. Reconstruction occurs by rendering the perturbed mesh triangles into the derived frame.</p><p>For simplicity, we use one pixel-wide reconstruction kernel. We just write a single pixel in the intermediate image in the nearest neighbor position of the derived image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>Our method was implemented on a Silicon Graphics Onyx 2 (1024MB RAM, four R1000 processors) with Infinite Reality graphics. However, we did not exploit its parallel processing capability in our implementation. Only one processor was used.</p><p>We demonstrated our method with a voxel-based rendering system <ref type="bibr" target="#b15">[16]</ref>. Our terrain model consists of a 3D terrain volume with a resolution of 512 512 64 and a corresponding registered aerial photo. <ref type="figure">Figures 7</ref> give the experiment results of our algorithm for a camera path shown at <ref type="figure" target="#fig_7">Figure 6</ref> which produces more than 300  frame images. The image size is 500 400. The ray-casting rendering method used in this paper is based on our earlier work <ref type="bibr" target="#b15">[16]</ref>. <ref type="figure">Figures 7 and 8</ref> show the amount of time required to render each frame. We compared our algorithm with the keyframe algorithm. <ref type="figure">Figure 7</ref> shows the result of the keyframe method. We rendered the keyframe by the ray-casting method on the voxel terrain model. Ten derived frames were gotten by warping one keyframe. We can see that the time to render keyframes is about two times longer than the time to render the derived frames. Even the time to render derived frames can be quite different. <ref type="figure">Figure 8</ref> shows the result of our method. The threshold for the age of pixels is 10. The number of pixels needed to be rerendered is about 14 percent of the total number of pixels in the intermediate image. <ref type="figure">Comparing Figure 7</ref> to <ref type="figure">Figure 8</ref>, we can see that our method can get a more stable frame rate than the keyframe method. <ref type="figure" target="#fig_1">Figures 11 and 12</ref> show a view of the terrain (Frame No. 100). <ref type="figure" target="#fig_4">Figure 11</ref> shows the terrain rendered by the ray-casting method. <ref type="figure" target="#fig_1">Figure 12</ref> shows the terrain rendered by our keyframeless IBR method. <ref type="figure" target="#fig_4">Figure 13</ref> gives the absolute value of the difference between these two images. We quantized the offset of pixels into 1024 levels. Because the threshold for the age of pixels is 10, we think that the position errors of pixels in the intermediate image caused by quantization and at most 10 consecutive warpings are negligible. The image quality of our algorithm depends on the reconstruction method. We used one pixel-wide reconstruction kernel and got sat-  isfying results. If more precise reconstruction methods are used the image quality can be further improved. We also demonstrated our method on another terrain. The terrain size is 468 693 64. <ref type="figure" target="#fig_4">Figures 9 and 10</ref> show the amount of time required to render each frame. <ref type="figure">Figure 9</ref> shows the result of the keyframe method. <ref type="figure" target="#fig_4">Figure 10</ref> shows the result of our method. <ref type="figure" target="#fig_3">Figure 14</ref> shows the terrain rendered by the ray-casting method. <ref type="figure" target="#fig_4">Figure 15</ref> shows the terrain rendered by our keyframeless IBR method. <ref type="figure" target="#fig_4">Figure 16</ref> shows the difference between these two images. From these figures we got the following observations: our method get a more stable frame rate than the keyframe method. The quality of the images generated by our method is comparable to that of ray casting and can be sustained at that level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We have presented a novel image-based rendering technique used in voxel-based scene modeling. Our contributions are: 1) We designed a keyframeless image-based rendering technique which combines the advantages of the 3D warp algorithm and the ray-casting method. The 3D image warp algorithm is used to gain a high frame rate, and the ray-casting method is used to fill in holes and to selectively rerender part of the image to guarantee the image quality.</p><p>2) An offset buffer is used to record the precise position of a pixel, so pixels can be consecutively warped from a single intermediate image without loss of accuracy. McMillan and Bishop's warp algorithm is adapted to warp the intermediate image. By quantizing the offset, we can preserve the incremental computation.</p><p>3) An age buffer is used to record the age of a pixel. This provides a nice criteria for image quality. By slightly adjusting the threshold for the ages of pixels in the intermediate image, we can balance the workload at every frame. Our method indeed obtains more stable frame rates and more uniform image qualities.</p><p>We demonstrate our method with a voxel-based terrain rendering system. However, our method can be applied to many other applications. It is especially useful in the applications which have the following features : 1) The keyframes have to be rendered on the fly and the time to render one keyframe is much higher than the time to generate one derived frame.</p><p>2) Ray casting can be used as a rendering method.</p><p>There are also some limitations to our method. First, the irregular distribution of pixels in the intermediate image can cause extra difficulty for reconstruction. If the nearest neighbor is used as the reconstruction method, we found that warping the intermediate image can cause more holes than warping the keyframe. Second, like other image-based rendering methods the performance of our method depends on the frame-to-frame coherence. If there is nice frame-to-frame coherence, such as in the terrain dataset, our method can give pretty stable frame rates and uniform image qualities. If the frame-to-frame coherence is not that good, such as in a dataset with a very complicated background scenery, then it is possible that the number of holes per frame can be vary substantially, and the ray casting may take longer than expected to fill in these holes. This may lead to inconsistent frame rates.</p><p>Some work need to be done in the future. We want to find a more accurate and fast reconstruction method for our intermediate image. We plan to use a splatting method instead of the current nearest neighbor method, so the number of holes can be decreased and the image quality can be improved. We also plan to parallelize our method to achieve higher frame rates. . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>¡</head><label></label><figDesc>Department of Computer Science, State University of NewYork at Stony Brook, Stony Brook, NY 11794-4400, USA. Email: @cs.sunysb.edu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Offset buffer. The exact positions of the warped pixels are recorded by an offset buffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>diate image are recorded by an offset buffer which represents the offset of the center of the pixels from their nearest neighborhood. The pixels in the intermediate image are all first rendered by ray casting in a previous frame and then warped into the intermediate image at the current viewpoint and view direction. At the beginning, the first frame is generated by the ray-casting method. Thus, the intermediate image is exactly the same as the displayed image. The offsets of all pixels at the intermediate image are 0. Then, we warp the intermediate image to get the next intermediate image. The offsets of warped pixels are recorded by an offset buffer. Now there are some holes in the intermediate image. We fill in these holes by the ray-casting method. The displayed image is reconstructed from this intermediate image. Then, we warp this intermediate image to get the next intermediate image and reconstruct the next displayed image from the next intermediate image, and so on. Therefore, at any intermediate image some pixels are newly rendered by the ray-casting method. Some pixels are warped from the previous intermediate image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>, the pixels in the intermediate image are all high qual-Warp order of the intermediate image for a negative epipole.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>start¨uincr© u¨1 v 1 .</head><label>1</label><figDesc>For an intermediate image, the next pixel coordinates at the image space along a scan line is © u¨1¨o f f setu v¨o f f setv 1 instead of The incremental computation cannot be directly used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>total possible error introduced by the quantization after m warps is bounded by c m © 1" L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Camera Path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Time to render each frame by the keyframe IBR method. Time to render each frame by our keyframeless IBR method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Time to render each frame by the keyframe IBR method. Time to render each frame by our keyframeless IBR method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :Figure 13 :Figure 14 :Figure 16 :</head><label>11131416</label><figDesc>A California terrain (size: 512 512 64) rendered by the ray-casting method. (See also Color Plate.) Figure 12: A California terrain (size: 512 512 64) rendered by our keyframeless IBR method. (See also Color Plate.) The difference between Figure 11 and Figure 12. (See also Color Plate.) . A Los Angeles terrain (size: 468 693 64) rendered by the ray-casting method. (See also Color Plate.) Figure 15: A Los Angeles terrain (size: 468 693 64) rendered by our keyframeless IBR method. (See also Color Plate.) The difference between Figure 14 and Figure 15. (See also Color Plate.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>There are two intuitive methods that can take advantage of coherences in derived frames: 1) Warping frame m-1 to get frame m. Image quality degrades</figDesc><table><row><cell>Keyframe</cell><cell>Frame 1</cell><cell>Frame2</cell></row><row><cell>Frame n-2</cell><cell>Frame n-1</cell><cell>Frame n</cell></row><row><cell></cell><cell cols="2">Pixels Rendered by Ray Casting</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The keyframeless IBR method. The intermediate image i+1 is rendered by warping the intermediate image i. The exact positions of warped pixels in the intermediate image i+1 are recorded by the offset buffer. After that, the intermediate image i+1 is partially updated by ray casting according to the ages of the pixels. The displayed image i+1 is reconstructed from the intermediate image i+1. The first intermediate image is rendered by ray casting.</figDesc><table><row><cell>Intermediate</cell><cell>Warp</cell><cell>Intermediate</cell><cell>Warp</cell><cell>Intermediate</cell></row><row><cell>Image i</cell><cell>Update</cell><cell>Image i+1</cell><cell>Update</cell><cell>Image i+2</cell></row><row><cell cols="2">Partially</cell><cell></cell><cell>Partially</cell><cell></cell></row><row><cell>Reconstruct</cell><cell></cell><cell>Reconstruct</cell><cell></cell><cell>Reconstruct</cell></row><row><cell>Displayed</cell><cell></cell><cell>Displayed</cell><cell></cell><cell>Displayed</cell></row><row><cell>Image i</cell><cell></cell><cell>Image i+1</cell><cell></cell><cell>Image i+2</cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>setv 0 % for all possible levels of the offset and store them in two lookup tables table u and table v . Before rendering each new image, we use the new camera information to precompute values for all lookup table indices. Then, McMillan and Bishop's incremental computation method can be adapted by adding two items to compensate for the offset of the pixels from the regular grid:</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by ONR grant N000149710402. Thanks to Nan Zhang, Baoquan Chen, and other members of the virtual fly-through project. Thanks to Manuel Menezes de Oliveira Neto and anonymous reviewers for their valuable comments. We would also like to thank Marianne Catalano for proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LDI Tree: A Hierarchical Representation for Image-based Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fa</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Lastra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GRAPH 99 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1999-08" />
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arie Kaufman. A Hybrid LOD-Sprite Technique for Accelerated Terrain Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Bauquan Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Swan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;99</title>
		<meeting>IEEE Visualization &apos;99</meeting>
		<imprint>
			<date type="published" when="1999-10" />
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Real-Time Photo-Realistic Visual Fly-through</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shenkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="265" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">Graphics. Computer</biblScope>
			<biblScope unit="page" from="51" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An Efficient Ray Tracing Method for Terrain Rendering. Pacific Graphics &apos;95</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="181" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual navigation of large environments using textured clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Paulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 Symposium on Interactive 3D Graphics</title>
		<meeting>the 1995 Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1995-04" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Post-Rendering 3D Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 Symposium on Interactive 3D Graphics</title>
		<meeting>the 1997 Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1997-04" />
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Post-Rendering 3D Image Warping: Visibility, Reconstruction, and Performance for Depth-Image Warping. Ph.D. Dissertation, University of North Carolina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Mark</surname></persName>
		</author>
		<idno>TR99-022</idno>
		<imprint>
			<date type="published" when="1999-04" />
		</imprint>
		<respStmt>
			<orgName>UNC Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Head-tracked stereoscopic display using image warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings SPIE</title>
		<imprint>
			<biblScope unit="volume">2409</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="1995-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Plenoptic Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95 Conference Proceedings</title>
		<imprint>
			<publisher>August</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An Image-Based Approach to Three-Dimensional Computer Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<idno>97-013</idno>
		<imprint>
			<date type="published" when="1997-04" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Dissertation. Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient Warping for Architectural Walkthroughs using Layered Depth Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Voicu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><forename type="middle">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">G</forename><surname>Lastra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Aliaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;98</title>
		<imprint>
			<date type="published" when="1998-10" />
			<biblScope unit="page" from="211" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">High Quality 3D Image Warping by Separating Visibility from Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Voicu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselmo</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lastra</surname></persName>
		</author>
		<idno>TR99-017</idno>
		<imprint>
			<date type="published" when="1999-04-05" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">UNC Computer Science Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamically Generated Impostors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schaufler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling Virtual-Worlds-Distributed Graphics, MVD 95 Workshop</title>
		<editor>D. W. Fellner</editor>
		<imprint>
			<date type="published" when="1995-11" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Layered Depth Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1998-07" />
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Virtual Flythrough over a Voxel-Based Terrain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huamin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Virtual Reality</title>
		<meeting>IEEE Virtual Reality</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Footprint evaluation for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 90 Conference Proceedings)</title>
		<imprint>
			<date type="published" when="1990-08" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Voxel-Based, Forward Projection Algorithm for Rendering Surface and Volumetric Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;92</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
