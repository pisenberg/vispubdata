<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Case Study: Observing a Volume Rendered Fetus within a Pregnant Patient</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>State</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tector</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutarou</forename><surname>Ohbuchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Bajura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Case Study: Observing a Volume Rendered Fetus within a Pregnant Patient</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Augmented reality systems with see-through headmounted displays have been used primarily for applications that are possible with today&apos;s computational capabilities. We explore possibilities for a particular application-in-place, real-time 3 0 ultrasound visualization-without concem for such limitations. The question is not &quot;How well could we currently visualize the fetus in real time, &apos;&apos; but &quot;How well could we see the fetus if we had suficient compute power?&quot; Our video sequence shows a 3 0 fetus within a pregnant woman&apos;s abdomen-the way this would look to a HMD user. Technical problems in making the sequence are discussed. This experience exposed limitations of current augmented reality systems; it may help define the capabilities of future systems needed for applications as demanding as real-time medical visualization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interpreting 3D radiological data is difficult for nonexperts because understanding spatial relationships between patient and data requires mental fusion of the two. Volume rendering has been useful for visualization but has typically been viewed separately from the patient.</p><p>Ideally one would like to see directly inside a patient. Ultrasound echography allows dynamic live scanning and patient-doctor interaction; an augmented reality system displaying live ultrasound data in real time and properly registered in 3D space within a scanned subject would be a powerful and intuitive tool; it could be used for needleguided biopsies, obstetrics, cardiology, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous work</head><p>Many researchers have attempted visualization of 3D echography data <ref type="bibr">[ 1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4]</ref>; some have volume visualized data sets that were acquired as a series of hand-guided 2D echography slices with 6DOF [5, 61. Compared to echography imaging by current state-of-the-art 2D scanners, such volume visualizations promise to reduce the difficulty of mentally combining 2D echography slices into a coherent 3D volume.</p><p>However, almost all of these systems have used conventional stationary video monitors for presentation, so that a user must still mentally fuse 3D volume images on the monitor with the 3D volume of the patient.</p><p>One system [7] tried to visualize live 2D echography images in-place within the patient using a see-through HMD system. While that system demonstrated the initial concept of "augmented reality," it could show only a few image slices (no 3D volume) at a relatively low frame rate Those few ultrasound images appeared to be pasted in front of the patient's body rather than fixed within it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Near-real-time visualization system</head><p>In January 1993 we attempted to improve upon [7] with a system designed to perform real-time, in-place volume visualization of a live human subject. It contained two major real-time features: a continuously updated and rendered volume, and an image compositor. The volume was updated from a series of 2D echography images acquired by a tracked ultrasound transducer. The image compositor combined each frame of the volume rendering with a live HMD video image of the patient (Plate 1).</p><p>Unfortunately, due to the requirements of real-time operation, the performance was seriously inadequate. The system's major shortcomings were: the ultrasound acquisition rate was only 3 ultrasound frames per second, causing both temporal and spatial undersampling of the volume, the reconstructed volume was crudely sampled (to 100 x 100 x loo), and could not be updated at more than 1 ultrasound slice per second, the volume was rendered at low resolution (65 x 8 1 rays cast) and interpolated to the display resolution of (512 x 640) to achieve 10 fps, the tracking system resolution and accuracy were poor, with significant lag and noise in the data. As a result of these problems, the (interactive) volume renderings displayed by this system were unrecognizable. The scans of a nearly full-term fetus did not reveal human-like forms to any member of the research team other than the M.D. ultrasonographer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hybrid real-time / off-line system</head><p>To answer the question "How much better would the visualization be if the real-time computational demands were met by the available resources," this system was designed so that the most computationally expensive tasks (volume reconstruction and rendering) are done off-line. <ref type="figure" target="#fig_0">Figure 1</ref> shows the steps involved in generating a video sequence that combines volume rendered echography data with HMD camera visuals. The figure shows the dependency of various tasks on one another. The tasks can be grouped roughly into calibration tasks performed prior to scanning a subject, data acquisition tasks performed during the scan, and image generation tasks which are a post process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Calibration</head><p>Calibration procedures compute several sets of parameters. One such set relates echography pixels to the transducer tracker origin; together with the transducer tracker position, it determines the location of the echography pixels in 3D world space. Similarly, the camera position and orientation relative to the HMD tracker origin must be determined; the optical distortion of the camera lens must be modeled so that the CG imagery can be made to match the camera images.</p><p>Echography pixel to tracker calibration. This function (which is not necessarily linear in pixel space) is measured by imaging a point target (a 4 mm bead obtained from GE medical systems and suspended at the tip of a pin in a water tank) at a known location relative to the transducer. Plate 2 shows the calibration setup for the ultrasound transducer. The transducer is attached to a precision translation stage which moves under computer control to chart out the point spread functions of the echography pixels (this function grows with distance from the transducer tip). The 2D transducer slice was measured to be rotated 2.3 degrees from the transducer's axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera calibration.</head><p>Position and orientation of the HMD camera relative to the HMD tracking origin are determined by an iterative semi-automatic method. The optical distortion of the lens is determined by imaging a grid pattern (Plate 2, inset). A circularly symmetric model based on a 5th degree polynomial is used.</p><p>Our optical tracking system is described in [8]; the calibration methods used for it are described in [9].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real-time acquisition</head><p>Unburdened by visualization processing needs, this phase takes place in true real-time. Both the ultrasound images and the HMD camera images are recorded at 30 fps on Sony D2 digital tape recorders. At the same time, tracking data for the ultrasound transducer and for the HMD is saved on a UNIXTM workstation which controls the D2 recorders. With each tracker record the time code of the corresponding video frame is stored for later synchronization, thus establishing correspondence between the tracking and video data streams.</p><p>To create an illusion of the visualized volume residing inside the abdomen, a (polygonal) model of a "pit" must be created; the pit must conform to the shape of the abdomen and be placed at the correct position. To achieve this, the geometry of the abdomen is acquired by making a zig-zag sweep of the abdomen. The tip of the tracked transducer is used as a 3D digitizing stylus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Off-line image generation</head><p>In this phase volume visualized echography images and the images captured by the HMD-mounted camera are combined into composite HMD viewpoint imagery. The major steps in generating the composite are:</p><p>Tracking noise filtering. The tracking data we acquire fluctuates even if the tracked target remains stationary. Such noise causes misregistration between video and CG imagery. To reduce this effect, a noncausal low-pass filter without phase shift is used [lo], with cut-off frequencies of 1 Hz for the transducer tracker and 6 Hz for the HMD tracker.</p><p>Reconstruction. The echography pixels are positioned and resampled into a regularly gridded volume, using a simple approximation algorithm based on a linear combination of Gaussian weighting functions which are translated and scaled to minimize artifacts [6, 111. Size and shape of an echography pixel in world (or tracker) space are approximated by a point spread function which falls off away from the world space position as a nonspherical Gaussian. Since an image frame and its tracking information are related by the frame's time code, digitization of echography frames from video tape and volume reconstruction can take place automatically on a workstation under program control.</p><p>Visualization. A volume renderer v012 [ 121 running on a graphics multicomputer Pixel-Planes 5 is used to render the reconstructed volume(s) from viewpoints matching those of the HMD-mounted camera. By modulating the direction of rays, the images are distorted based on the model described above; the polygonal pit, built using the data from the abdomen geometry sweep, is included in the rendering. The images are recorded in single-frame mode onto digital video tape.</p><p>Compositing. Camera and CG images are mixed by chroma-keying on a Sony video mixer. The mixer replaces blue background in the CG frames by HMD frames; the time codes recorded during HMD acquisition are used to ensure synchronization of the 2 elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Live subject experiment and results</head><p>In January 1994 we scanned two pregnant subjects with the hybrid system. Since the current tracking setup allows only one target at a time, the abdomen sweep data, the ultrasound data (Plate 3, left) and the head camera data (Plate 4) had to be recorded in 3 successive passes. The patients had to remain motionless throughout the acquisition phase.</p><p>From the acquired ultrasound imagery, we selected and digitized a short sequence of about 15 seconds, during which the ultrasonographer had made a continuous sweep (455 slices) of the fetus from the middle of the skull down to the bottom of the hip <ref type="figure">(Plate 3, right)</ref>. The slices were reconstructed into a 165 x 165 x 150 volume with a resolution of 8.2 voxelskm (or a voxel size of .122 cm3), which is better than the highest resolution of the ultrasound machinehansducer combination.</p><p>After reconstruction, objects such as uterus and placenta were edited out manually using an editing mask with Gaussian fall-off to avoid introducing artifacts in the volume. Finally a small 3D Gaussian filter (standard deviation 2 voxels) was applied to the volume.</p><p>The abdomen geometry sweep had failed due to a minor technical problem during the live scan; we derived abdomen geometry data by triangulating a number of small structures visible on the skin of the abdomen in frames videographed from different viewpoints and extracted from the HMD video sequence.</p><p>The reconstructed and edited volume (Plate 5 ) and the pit were rendered by v012 with optical distortion (Plate 5, inset); the CG sequence was combined with the HMD camera sequence (Plate 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future directions</head><p>Many aspects of opr experiment suffered from lack of immediate, real-time 3D feedback. During HMD video acquisition, the HMD wearer could not really see inside the patient; thus we unfortunately ended up looking at the patient from the "wrong" side and thus viewing the fetus from behind in the resulting composite video sequence. During echography acquisition, we were unable to gather enough echography slices to reconstruct a complete fetus due to lack of real-time 3D feedback on the geometry of the scanned anatomy. Still, in separately generated images from viewpoints chosen more advantageously than those of the HMD camera during the HMD video acquisition, one can recognize more anatomical features (Plate 5).</p><p>What resources would be required to present an online visualization of comparable quality in an augmentedreality system? We expect advances in volume rendering software and hardware to soon provide high-speed stereoscopic rendering of volumetric data sets (see for <ref type="bibr">example [ 131)</ref>. As for reconstruction, our volume contained nearly 4 times as many voxels as the one used in the 1993 experiment. Since the latter was being reconstructed at a rate of about 1 Hz, we need an increase of 2 orders of magnitude in computational speed for the reconstruction subsystem.</p><p>We learned from the January 1993 experiment and others that, besides the image generation frame rate, short lag in both volume reconstruction and visualization is very important. Our hybrid system has avoided this problem through off-line processing. For an on-line realtime system, however, we need to design and implement hardware and algorithms that provide not only high throughput but also short lag. In addition, we need fast, minimal-lag, high-precision tracking.</p><p>In the area of our application-visualizing ultrasound as a "flashlight" into the body-we conclude that a step forward has been achieved. The sequence showing a fetus registered inside the pregnant subject provides a "brass standard" (if not a gold one) as a target for our next real-time efforts. One way to acquire the high amounts of computing resources needed for such efforts is through the current research on high-bandwidth links between powerful computing stations, which hints at computational capabilities that might be available in the next few years and are within the desired range of power.</p><p>In general, complex visualizations presented within augmented vision systems make greater application demands than either virtual environments or scientific visualization individually.</p><p>Any closed virtual environment or scientific visualization system lacks the error-emphasizing cues that a combined system provides. An augmented reality application provides sufficient information to enable the user to easily notice registration errors, tracker lag, computational errors, calibration errors and real time delays which destroy the attempted illusion. Augmented systems or any virtual reality system dealing directly with the real world will not be easy to create. Simple applications with little computational demand, such as overlays for wiring guides or informational pointers, will be able to get by, but applications with complex visualization goals will be heavily burdened by the demands. Researchers should be sensitive to these issues and attempt to evaluate carefully their impact in the intended application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flow diagram for hybrid experiment combining real-time acquisition with off-line visualization. The top row shows the "basic ingredients" of our system.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">i s r e colorplntes, p q e </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>Jack Goldfeather provided mathematical models for the echography calibration procedure. Gary Bishop wrote filtering software for the tracking data. Nancy Chescheir, M.D. and Vern Katz, M.D. were our ultrasonographers. Kathryn Y. Tesh and Eddie Saxe provided experimental assistance. Scott Shauf created an early version of figure 1. We thank our anonymous subject for her patience.</p><p>Funding was provided by ARPA (ISTO DABT 63-92-C-0048 and ISTO DAEA 18-90-C-0044) and by CNRI / ARPA / NSF / BellSouth / GTE (NCR-8919038).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multidimensional Ultrasonic Imaging for Cardiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Kinter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Mcewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barillot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Greenleaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Echocardiographic Three-Dimensional Visualization of the Heart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Lalouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bickmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Mankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kangaraloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Monnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Masotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Novins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Greppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cerofolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Devereux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AS1 Series</title>
		<editor>Fuchs, H. Hohne, K. H., and Pizer, S. M. NATO</editor>
		<meeting><address><addrLine>Travemunde, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
	<note>3 0 Imaging in Medicine. F Tomographic Technologies, GMBH. 4 0 Tomographic Ultrasound, A clinical study</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D acquisition and visualization of ultrasound data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Biomedical Computing 1992. Chapel Hill</title>
		<meeting><address><addrLine>NC</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Merging Virtual Objects with the Real World: Seeing Ultrasound Imagery within the Patient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bajura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Biomedical Computing 1992. Chapel Hill</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
	<note>Computer Graphics (Proceedings of SIGGRAPH&apos;92)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Real-Time Optical 3D Tracker for Head-Mounted Display System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of 1990 Symposium on Interactive</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Graphics)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="205" to="215" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autocalibration for Virtual Environment Tracking Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gottschalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIGGRAPH&apos;93)</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Digital Low-Pass Filter Without Phase Shift</title>
	</analytic>
	<monogr>
		<title level="m">NASA Tech Briefs KSC-11471. John F. Kennedy Space Center</title>
		<meeting><address><addrLine>Florida</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Incremental Acquisition and Visualization of 3D Ultrasound Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill, Computer Science Department</orgName>
		</respStmt>
	</monogr>
	<note>Doctoral dissertation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Interactive Multimodal Volume Visualization for a Distributed Radiation-Treatment Planning Simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cullip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lavoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rhoades</surname></persName>
		</author>
		<idno>TR94-040</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill, Computer Science Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Accelerating Volume Reconstruction With 3D Texture Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cullip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<idno>TR93-027</idno>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="535" to="545" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill, Computer Science Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Plate 1 : HMD view in near-real-time system with on-line concurrent processing. Plate 2: Ultrasound transducer calibration setup. Inset-HMD view during camera calibration run</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Left-ultrasonographer acquiring echography slices. Right-4 of the 455 slices used for volume reconstruction. Plate 4: Acquiring HMD camera video</title>
	</analytic>
	<monogr>
		<title level="j">Plate</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reconstructed fetus data set. Inset-CG frame matching the HMD camera view in Plate 4. Plate 6: View of volume rendered fetus within pregnant subject</title>
	</analytic>
	<monogr>
		<title level="j">Plate</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
