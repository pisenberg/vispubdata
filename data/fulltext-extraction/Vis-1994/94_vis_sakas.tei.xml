<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Case Study: Visualization of 3D Ultrasonic Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Sakas</surname></persName>
							<email>gsakas@igd.fhg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Fraunhofer Institute for Computer Graphics</orgName>
								<address>
									<addrLine>Wilhelminenstr. 7</addrLine>
									<postCode>64283</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars-Arne</forename><surname>Schreyer</surname></persName>
							<email>schreyer@igd.fhg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Fraunhofer Institute for Computer Graphics</orgName>
								<address>
									<addrLine>Wilhelminenstr. 7</addrLine>
									<postCode>64283</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Grimm</surname></persName>
							<email>mgrimm@igd.fhg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Fraunhofer Institute for Computer Graphics</orgName>
								<address>
									<addrLine>Wilhelminenstr. 7</addrLine>
									<postCode>64283</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Case Study: Visualization of 3D Ultrasonic Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>3D Uhasound is one of the most interesting noninvasive, non-radiative tomographic techniques. Rendering 3D modells from such data is not straightforward due to the noisy, fuzzy nature of ultrasound imaging containing a lot of artefacts. In this paper we first apply speckle, median and gaussaan prefiltering to improve the image quality. Using several semi-automatic segmentation tools we isolate interesting features within a few minutes. Our improved surface-extraction procedure enables volume rendering of high quality within a few seconds on a normal workstation, thus making the complete system suitable for routine clinical applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Motivation</head><p>The 'tomographic' medical imaging techniques, such as CT, MR, nuclear medicine, etc., have in recent years experienced a break-through in several domains of diagnostic imaging. Such tomographic imaging techniques are of great interest, because they potentially enable a 3D reconstruction and examination of interesting parts, offering obvious benefits (reviewing from any desired angle, isolation of interesting locations, visualization of internal structures, 'fly by', accurate measurements of distances, angles, volumes, etc.).</p><p>3D Ultrasound is probably one of the most interesting applications in this area. In contrast to the usual the 2D case, a complete volume is covered with a whole series of 'cuts'. Thus 3D ultrasound can become a new, fast, non-radiative, non-invasive, and inexpensive, volumetric visualisation technique. Moreover for the localisation of vessels and tumors in soft tissue (spleen, kidneys, lever, etc.) the results gained by 3D ultrasound are sometimes considered to be better than those of other tomographic techniques.</p><p>One of the major reasons for the limited acceptance of 3D ultrasound so far is the complete lack of an appropriate visualisation technique, able to exploit the benefits of the acquisition of several slices. Although experimental systems for volumetric visualization of ultrasonic data have been proposed [NeE193], the most common method currently is to present each slice sequentially as a 2D image on the screen. Thus there is no significant benefit in comparison to normal (and 10 times less expensive) ultrasound equipment. A second reason is that the computation speed availble usually is limited and results in a low (or very low) frame rate, thus not allowing a 'natural', easy-to-understand, interactive working, on-the-fly parameter estimation, etc. Goal of the work is to develop specialised techniques, enabling a 3D reconstruction of the examined part from 3D ultrasonic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Acquisition Devices</head><p>For our tests we used datasets from two different devices, both showing fetuses 3 1/2 months before birth (25th week of pregnancy). The first dataset has been acquired in the Deutsche Klinik fur Diagnostik, Wiesbaden, with a prototype of a parallel-scan device. In this case the transducer is translated parallel to the surface, thus all images are axis-aligned to each other and the data density, i.e. the size of details represented by one pixel, is isotropic. Such data can be used directly and require no re-sampling. The data resolution of the dataset was 256' .96.</p><p>Our second device was a 'Kretz Voluson 550'. Voluson employs a more modern fan-scanning technique and uses currently a mechanically swinging transducer. Due to the fan-scanning, the data density close to the transducer is higher than that on the opposite side of the image. The result of the acquisition is a 3D curvi-linear sector, similar to a truncated pyramid.</p><p>Processing and volume rendering of non-rectlinear data is possible [Wi1192], but not straight-forward. Instead, visualisation of Cartesian data is usually much faster due to the exploitation of coherency [ScSt92], [Saka93d]. Therefore we choose to re-sample the original curvi-linear dataset to a Cartesian one. This re-sampling is already provided from the acquisition device and requires only a few seconds. The resulting cube has a resolution of 2563 or 1283 voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Input Data</head><p>Unlike MR and CT data, ultrasonic images can usually not be volume rendered without further processing. The images have: significant amount of noise much lower dynamic range as compared to tomographic images the regions representing boundaries are not sharp but show a width of several pixels depending on the orientation of a surface to the sound source, the intensity of the reflection, and thus the density of a boundary, varies significantly with the surface curvature. deeper surfaces are 'shadowed' from objects lying above, e.g., the hand shadows the face. Depending on the size and density of the above object, deeper surfaces can be partly (weak edges) or totally shadowed (surface gaps) in the case of the older parallel-scan device, the average lightness of subsequent images may vary signiin addition, the alignment between subsequent images is bad and shows sometimes variations of several pixels in the case of fan-scanning, the resolution represented by a pixel varies with the distance from the sound source.</p><p>All difficulties mentioned above are characteristic for ultrasonic images and represent a fundamental difference to CT or MR scanning devices. . One can immediately see errors caused by noise and surface artefacts. Such images can not be used without further processing. Thus the major difficulty here is to find ways for discriminating the tissue of interest from the huge amount of apparently noisy signal that is always present. This goal can be achieved over a combination of various approaches:</p><p>enhancing the quality of the original data (filtering) -separating noise from tissue (segmentation) -using adequate visualization methods (volume ren-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Filtering</head><p>We tried to improve the image quality by means of filtering. When filtering medical images, a trade-off between image quality and information loss due to filtering must always be taken into account. Physicians ficantly dering) are usually very reluctant and reserved when their 'originals' are manipulated. We employed three different filters: gaussian for noise reduction, speckle removal for contour smoothing, and median for both, noise reduction and closing of small gaps caused by differences in the average luminosity between subsequent es. Especially the median filter has been proven to be a good tool, since it effectively reduces noise and small surface artefacts, while maintaining the sharpness of contours.</p><p>In the first implementation filtering has been applied on each 2D slice using the tools implemented in Khoros Version 1.0 (public domain). The results for each image are satisfactory and can be seen on <ref type="figure">fig. 2</ref>. However, when these images are volume rendered, the misalignments between individual slices become visible, see <ref type="figure">fig. 3</ref> lower left. Therefore we implemented 3D versions of the gaussian and median filters (speckle removal is still applied on slices). The results can be seen on images 3 right and 5. Both datasets have been processed with a 3D-median filter with a width of 33, 53, and 73, as well as with a gaussian one with a width of 33 and 53. In both cases the appearance is significantly improved. However, with increasing size of filtering, the small details are almost completely smoothed out and the fetuses show a rather unnatural overly smooth appearance. The best compromise for the current datasets was a median of 33 or 53, followed by a gaussian of 33. Due to the improved overall image quality (less noise, sharper edges, no variation of luminosity, good alignment), the data of Kretz Voluson require less filtering than those of the older parallel-scan device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Interactive Segmentation</head><p>Filtering improves the image quality, but several artefacts still remain in the image. These are either noise clusters too big to be smoothed or removed by filtering, or objects such as hands and tissue lying on top of the dataset and covering the face below them. Noise clusters build 'satellite blobs' in front of the face, sometimes of considerable size (see <ref type="figure">fig. 5</ref>). Hands or tissue show the same characteristics as the fetus and therefore can not be distinguished and removed or filtered out automatically. In order to separate the interesting parts, image segmentation is necessary.</p><p>The image processing and the computer vision community has produced a rich literature about image segmentation, see [Niem81] and [PratSl] for an overview. On the other hand, proposed techniques lack either in generality, or in the required computation time.</p><p>In order to implement a system to be used in the clinical routine in the next future, we decided to apply interactive segmentation. Such tools must be intuitive, uncomplicated, interactive and very fast in order to be used by physicians. Interactivity means they have to show their results in near real-time. As an example, defining an exact contour on several slices (or even on a single slice) requires a lot of precision and several seconds, or even minutes, of manual input, a time not available in the clinical routine. We implemented four different segmentation methods to tackle all these requirements.</p><p>In the first case we segmented the image in object space. Here the data volume was originally bounded by a cube (or prism). By moving the surfaces of the cube, parts of the dataset could be clipped. Such a translation can be performed either perpendicular to the axes of the coordinate system, or by moving each surface freely in 3D space. In the latter case the shape of the original cube is changing to another convex solid (hexahedron, pentaedron, or tetrahedron). As demonstrated on the video-tape, such a clipping can be performed within seconds and is generally completed within a minute. The contours of the new solid are continuously displayed on the 3D and the three 2D windows. This clipping mode allows only crude data segmentation. To remove the remaining parts of the noise and/or unwanted parts of the object itself, i.e. the arm, we implemented three additional techniques allowing more accurate segmentation. In the first approach the user can draw an arbitrarily shaped boundary on either one of the three 2D windows or on the volume window as shown on <ref type="figure">fig. 1</ref>. In case of the 2D windows, this screen boundary is pixel-wise back-projected onto the corresponding face of the data-cube and sweeped along the depth direction forming an axis-aligned sweeping solid (see <ref type="figure">fig. 1 left)</ref>. The validity of all voxels within this solid is seted to false. The complete clipping process requires 1-2 seconds, depending on the resolution and the size of the deleted volume.</p><p>Identification of voxels projecting within the erasing area is more complicated when drawing a boundary in the 3D window. In this case the data-cube is scanned in object order, every voxel is projected (splatted) [West901 onto the window, and if its pixel-coordinates are included in the boundary, the voxel is deleted. The inclusion test can be speeded up by using an inclusion mask equal to the screen resolution and check this mask for the projected coordinates of each splatted voxel (see <ref type="figure">fig. 1 right)</ref>. Further we splat only voxels included in the cube bounding the intersection of the voxel-field and the projection of the erase-boundary along the eye-direction. This algorithm avoids oversampling in the voxel space as well and requires in average 2-3 seconds for erasing a region.</p><p>The last approach enables a very accurate erasing in 3D space. Here the user is identifying objects (e.g., noise clusters in front of the face of the fetus) in the 3D volume window just by pointing on them with the left mouse button. Now, with every click on the middle button, an 'erasing wave front' propagates in the selected object forming more or less co-centrical semispheres and deleting all touched voxels. The wave front propagation stops when reaching voxels with opacity values lying outside a pre-defined range (usually when reaching voxels of low opacity). Thus, isolated 'satellites' in 3D space can be deleted in a very intuitive and fast way. After each new deleting step, the portions of the image affected by the process are automatically re-rendered, thus providing an immediate feed-back. Each erase &amp; re-render step requires in average 1/2 to 1 seconds. The procedure is also demonstrated on the video-tape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Visualisation</head><p>We implemented all features mentioned above in our volume visualisation system InViVo [Saka93d]. For the visualisation of CT and MRI images a surface reconstruction method is usually employed similar to that described in <ref type="bibr">[Levo88]</ref>. By interactively choosing an appropriate grey level value and tolerance, different types of surfaces may become visible. The image shown on <ref type="figure">fig.7</ref> has been produced by means of this method.</p><p>Of particular interest is the linking between 2D slices and 3D anatomy supported by InViVo: First, the user is positioning the cursor on a particular anatomic detail of the 3D image. By pressing the left, middle, or right mouse button he can see in real time a coronal, transversal or sagital cut of the 3D model through that particular point, as well as the corresponding 2D slice. Such a real-time positioning and moving of arbitrary cuts (axis aligned as well as oblique) helps understan-ding the relationship and relative position between a 2D slice and the 3D anatomy.</p><p>For the visualisation of the interior of a volume in addition to the surface we employ MIP, semitransparent, or X-ray illumination models. Changing between all three models can be performed in real time and requires not a new rendering. An additional feature is that we can extract both, surface as well as volumetric information during a single traversing. Extracted features are depth-sorted and stored together with their individual depth in a structure similar to a . After traversing and during visualisation one can select the type of desired features interactively. The visualisation evaluates the values of the ZZ-buffer and therefore can be performed in interactive rates since it requires no new traversing. Also mixed values (e.g., 40% surface, 60% X-ray) can be displayed, see <ref type="figure">fig. 6</ref>. Additional features include measurements of distances and angles, parallel and perspective projections, stereo, depth cueing, software video recorder, etc. <ref type="figure">Fig. 7</ref> compares an image reconstructed from data acquired in the 25th pregnancy week with a photo of the baby 24 hours after birth. The resolution of the data was 2562 . 128 (8 MBytes), the time for volume rendering one image with resolution of 5122 pixels was ca. 10 seconds. A rotation sequence of this fetus required only 3 minutes on an SGI R4400. Diagnosis of abnormal cases can be definitely improved. In addition, due to their high emotional value, we expect that such reconstructions will be highly desired by parents as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion &amp; Further Works</head><p>The major advantages of InViVo in visualizing ultrasonic data are:</p><p>-Understanding the content of 3D ultrasound images has been significantly improved. Our visualisation methods provide a true 'added value' for diagnosis. -Noise included in the 3D ultrasound data has been significantly reduced. -The speed of visualisation enables an immediate e v e luation of the acquired data during examination of the patient. -InViVo runs on commercial modern workstations.</p><p>Thus the (rather low-cost) visualisation subsystem is separated from the more expensive acquisition equipment allowing a faster system update.</p><p>The images in this paper have been generated by first filtering the data and then applying Levoy's gradient shading method. This study demonstrated that this approach is promissing and can be used to implement practical systems. Currently we work on several improvements in order to enable a faster, high-quality rendering of fuzzy, noisy surfaces. These improvements include better and faster pre-filtering, an automatic pre-segmentation of tissue and noisy areas, a multiresolution surface identification procedure, a more accurate surface identification avoiding oversampling, a modified gradient shading method, and a technique to compensate fluctuations in the intensity of the boundary caused by the varying orientation to the sound source. These results will be reported in [Saka94c] in more detail. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 3 upper left and fig. 5 upper left show volume renderings of the unprocessed data from both devices employing methods similar to those mentioned in [Levo88]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 6 :Figure 4 :</head><label>264</label><figDesc>Images from the parallel-scan device before and after filteringFigure 5: Upper: unfilbered, median of 33, 53, T3. Lower: gaiissian of 5'Figure 3: Parallel-scan data before (upper left) and after processing Transition between MIP and siirface rendering Data before and aft,er removing the left, hand Figure 7: Left dat,a of the 25th pregnancy week, right a photograph 21 hours after birth (See color plates, page CP-42.)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Display of Surfaces from Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualization of SD Ultrasound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elvis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data IEEE Computer Graphics &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Niem81 Niemann</surname></persName>
		</author>
		<title level="m">Pattern Analysts Springer-Verlag</title>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Pratt</surname></persName>
		</author>
		<title level="m">Digital Image Processing 2nd Edition</title>
		<imprint>
			<publisher>Wiley &amp; Sons</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The 22-Bufer: A Simple and Eficient Rendering Algorithm with Reliable Antialiazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sast89 Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stolfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings &apos;PIXIM&apos;89 Conference</title>
		<meeting>&apos;PIXIM&apos;89 Conference<address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-09" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intemctive Volume Rendering of Large Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting Surfacesfrom SD Ultrasonic Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saka94c Sakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">; P</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Parallel Volume Rendering as Line Drawing, ACM SIGGRAPH Proceedings 1992 Workshop on Volume Visualization</title>
		<meeting><address><addrLine>Boston-MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
	<note>To appear ScSt92 Schrtider</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Footprint Evaluation for Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Graphics</title>
		<imprint>
			<date type="published" when="1990-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive Splatttng of Non-Rectlinear Volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization&apos;92</title>
		<meeting>IEEE Visualization&apos;92<address><addrLine>Boston-MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-10-22" />
			<biblScope unit="page" from="425" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m">SIGGRAPH-90</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
