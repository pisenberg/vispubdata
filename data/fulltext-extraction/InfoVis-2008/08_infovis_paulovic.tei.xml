<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HiPP: A Novel Hierarchical Point Placement Strategy and its Application to the Exploration of Document Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosane</forename><surname>Minghim</surname></persName>
						</author>
						<title level="a" type="main">HiPP: A Novel Hierarchical Point Placement Strategy and its Application to the Exploration of Document Collections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text and document visualization</term>
					<term>hierarchical multidimensional visualization</term>
					<term>visual knowledge discovery</term>
					<term>highdimensional data</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Point placement strategies aim at mapping data points represented in higher dimensions to bi-dimensional spaces and are frequently used to visualize relationships amongst data instances. They have been valuable tools for analysis and exploration of data sets of various kinds. Many conventional techniques, however, do not behave well when the number of dimensions is high, such as in the case of documents collections. Later approaches handle that shortcoming, but may cause too much clutter to allow flexible exploration to take place. In this work we present a novel hierarchical point placement technique that is capable of dealing with these problems. While good grouping and separation of data with high similarity is maintained without increasing computation cost, its hierarchical structure lends itself both to exploration in various levels of detail and to handling data in subsets, improving analysis capability and also allowing manipulation of larger data sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many applications these days need to make sense of large collections of documents. Textual data sets are considered to have high dimensionality. Conventional vector representations are built from term counting after pre-processing to handle mining and visualization of such text collections. Visual analysis of text collections can, in this context, make use of many of the techniques available for multidimensional data visualization.</p><p>From those, projections or point placement strategies, that target at placing points representing individual data instances in a visualization space (a one, two or three dimensional space), have been shown to support many analysis tasks for document sets. The visual output is a set of points in the visualization space and proximity in this space is meant to reflect some sort of relationship (such as similarity) in the original representational space.</p><p>Data mappings of this kind may be very useful, but, in the resulting visualizations, some problems persist, such as overlap of points caused by the attempt of grouping highly related instances and the cognitive overload, since all elements are shown at once on the resulting layout. Another important fact is that not all grouping and separation (i.e. neighborhood) properties of the overall display hold when analyzing the map in detail. In addition, handling large data sets is very costly in computing time if accurate layouts are required -some approximations can be used to reduce the computational requirements but tend to result in poorer layouts, particularly for high-dimensional data sets.</p><p>This scenario suggests a hierarchical approach for the mapping and exploration of text and other multi-dimensional data sets that share the same features (high dimensionality, large amount of instances and non-trivial similarity relationships). This paper presents a novel approach to handle hierarchical point-placement and exploration of multi-dimensional data sets in general, and of document collections in particular. The target of the technique is to be able to explore a larger number of elements in one display, by allowing multiple levels of detail, that is, in the first level the entities are clusters of instances, and the expansion of any part of the display happens on demand. The placement is done on a bi-dimensional plane, where groups and subgroups of data instances are represented using circles inside circles,</p><p>• Fernando V. <ref type="bibr">Paulovich</ref>  and the position of such circles is given by the similarity between the (sub-)groups they represent. Together with this hierarchical technique, the main contributions of this paper are:</p><p>• a strategy to help a user to change the hierarchy in order to change the organization of the data set as exploration progresses; • a technique to extract topics following the same multi-level strategy of the hierarchical placement technique; • a novel, intuitive approach to evaluate point placement layouts based on the preservation of neighborhood; • a freely available system (tool and code available at http: //infoserver.lcad.icmc.usp.br/) that implements the technique presented here and other mechanisms to create and explore hierarchical representations. The next section describes point placement techniques and their application to visualization of document collections and contrasts those with the method presented here. Section 3 describes the new visualization approach in detail. Section 4 presents a technique which enables the user to re-organize the hierarchy (consequently the final layout). Section 5 describes the multi-level topic detection strategy. Section 6 exemplifies the application of the technique for different collections of documents, and in Section 7 we draw our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Point placement techniques seek to create visual representations of multi-dimensional data sets, enabling users to employ their visual ability to recognize structures or patterns present in the data. In this representation, each data instance is mapped to a visual element, such as a circle, point, or sphere embedded in a visualization space (1D, 2D, or 3D). Their relative position may reflect some type of relationship amongst data instances, the most common being similarity or neighborhood relationships given by their proximity in the visualization space.</p><p>One class of similarity-based point placements is Multidimensional Scaling (MDS) <ref type="bibr" target="#b2">[3]</ref>. MDS techniques aim at projecting instances belonging to an m-dimensional space into instances in a d-dimensional space (d &lt; m), striving to maintain, in the projected space, the distance relationships of the original space.</p><p>Amongst the various MDS approaches, Force-Directed Placement (FDP) <ref type="bibr" target="#b4">[5]</ref> is commonly used in visualizations. An FDP model, originally created for graph drawing, aims at bringing a system composed by instances connected by "virtual" springs into an equilibrium state. Instances are initially placed randomly and the spring forces iteratively push and pull them until reaching equilibrium. The forces are propor-tional to the difference between the dissimilarity amongst the instances in the m-dimensional space and the distances between points in the ddimensional space.</p><p>In the original FDP model an iteration is O(n 2 ). Since at least n iterations are necessary to reach an equilibrium state, its final complexity is O(n 3 ). In order to reduce such complexity, Tejada et. al <ref type="bibr" target="#b19">[20]</ref> proposed an approach, called Force Scheme, which needs less iterations to reach the final state, defining an O(n 2 ) model. Employing a different approach, Chalmers <ref type="bibr" target="#b1">[2]</ref> creates an O(n 2 ) technique defining iterations with linear complexity, and Morrison et. al <ref type="bibr" target="#b12">[13]</ref> reduce the global complexity to O(n 5 <ref type="bibr" target="#b3">4</ref> ) by creating a hybrid model based on approximations using samples and interpolation.</p><p>Although these approximations can reduce the computational requirements effectively, they can also impair the final layout, particularly for high-dimensional data sets (such as the vector representation of document collections). The technique presented here, instead of been based on approximations, reduces the computational costs by splitting the data into clusters, so that each cluster can be projected separately using a high precision approach, therefore resulting in more accurate layouts.</p><p>Similarity-based point placement techniques (also referred to as projection techniques) have also been used to create visual representation of document collections, i.e., document maps. Two typical approaches are the Infosky system <ref type="bibr" target="#b0">[1]</ref> and the Galaxies <ref type="bibr" target="#b22">[23]</ref> technique, the latter employed by the IN-SPIRE T M system 1 .</p><p>Infosky is a hierarchical approach, where the user can focus in and out, analogously to manipulating a telescope, in order to explore the document collection in different levels of details. However, different from the technique presented here, it assumes that the documents are already organized in a hierarchy of collections and subcollections, therefore not suitable to explore sets of documents without prior knowledge of content.</p><p>The Galaxies approach can be used on general collections but it can result in poor layouts due to the projection technique it employs, called Anchored Least Stress (ALS). ALS first projects a small set of documents (the anchors) onto the plane using Principal Components Analysis (PCA) <ref type="bibr" target="#b10">[11]</ref>. Then, making use of the anchors position, projects the remaining points using an interpolation strategy. Since only two principal components are used to represent the data on the plane, only two directions of large variance can be properly represented. As a consequence, the resulting layouts generally present an "elbow" where the two components cross, and where most data instances that are not well represented by those particular components are placed.</p><p>Other recently developed approaches have been successfully employed for mapping documents, such as Least Square Projection (LSP) <ref type="bibr" target="#b14">[15]</ref> and the NJ-tree <ref type="bibr" target="#b3">[4]</ref>. Different from most other projections, LSP aims at keeping neighborhood relationships instead of the similarity relationships in the final layout. Although LSP can group and separate documents well, it tends to cause a high degree of overlapping within groups of highly similar documents, impairing focussing on smaller sub-groups and hindering visual density estimation. NJtree construct high precision hierarchical similarity trees using an algorithm for phylogenetic tree reconstruction. In an NJ-tree overlapping is reduced and it is very useful to find groupings in various level of details. However, its complexity is O(n 3 ). The technique presented here reduces overlap by making a better use of the visual space in a cost effective way. On the layout, groups of highly related data instances are represented by circles. Dense areas can be easily located using color and size mappings based on the number of instances in the circles.</p><p>Hierarchical visualization and layout have been largely deployed to analyze data. They are normally classified in two different groups: space-filling techniques (such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>), and node-link techniques (such as <ref type="bibr" target="#b15">[16]</ref>). Although space filling techniques make the best use of the visual space and both groups of techniques are very effective on revealing the hierarchical structure of a data set, the resulting visual representations misses an important element of the techniques discussed 1 see http://in-spire.pnl.gov/ in this paper, which is the capability of conveying similarity between the elements (nodes or leaves) by their position on the layout. Hierarchical Parallel Coordinates <ref type="bibr" target="#b7">[8]</ref> is one hierarchical technique that represents some similarity information based on coloring, but it imposes a linear ordering of the dissimilarities amongst the clusters, which is not always possible to assume. It is, therefore very restrictive for the purpose of investigating similarity in various levels of detail. The focus of the technique presented here is the preservation of similarity relationships, reflect by grouping of similar instances while also visually separating dissimilar groups, thus adding a different constraint to the process of generating visual representations.</p><p>Another related work, though developed to solve a different problem in a different area, was presented in <ref type="bibr" target="#b20">[21]</ref>. In this work, a hierarchical FDP strategy is used to speed up the process of embedding a graph on a plane. Although FDP strategies can be used for graph layout, normally they cannot handle nodes with different sizes. This is an important feature the similarity hierarchy presented here, since node size is used to reflect number of elements in a node. FDP strategies can be adapted to consider nodes with different sizes, but normally they will be very slow to converge <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HIERARCHICAL POINT PLACEMENT (HIPP)</head><p>Aiming at tackling the problems of the existing projection techniques, we have developed a point placement technique called Hierarchical Point Placement (HiPP). HiPP defines a hierarchical cluster tree where visual elements can represent both individual data instances and clusters of highly-correlated instances. Its structure is constructed so that each level represents a clustered view of the data set. The first level presents the top level clusters, and the subsequent levels present more detailed sub-groups down to individual data instances. This an exploratory process by stepwise refinement, starting with an overview of the data set and focusing on clusters of interest, finally allowing examination of the content of the individual data instances. In addition, since only the explored clusters are projected, it is possible to employ a high-precision point placement technique within a cluster instead of working with approximations to reduce the computational complexity, resulting on very accurate placements.</p><p>This technique is detailed in the next sections. The first step is to create a hierarchical cluster tree, then the elements of that tree are mapped to the bi-dimensional space to create a visual representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Creating the Hierarchical Cluster Tree</head><p>The hierarchical cluster tree is constructed using a recursive partitioning process where the internal nodes are clusters and the leaves are individual data instances. This process starts with a node, ROOT , containing all data instances. Then, these instances are split into k = |ROOT | nodes, creating the ROOT children nodes C 1 ,C 2 ,...,C k (| • | denotes the number of data instances on a node). After that, each child node C i is split into |C i | nodes, and the resulting nodes are attached as their children -we use √ • to define the number of clusters to be created since in this technique creating more clusters than the real number of clusters is preferable than creating less than that, and this measure has been frequently accepted as a good upper bound for the number of clusters <ref type="bibr" target="#b13">[14]</ref>. This partitioning process is recursively applied for each new node until a minimum number of data instances in the nodes (min) is reached, at which point data instances are converted into leaves of the resulting tree. This procedure is described in Algorithm 1. The minimum number of data instances that a node must contain in order to be split influences user interaction and the amount of area used on the final visual representation. If min n, where n is the number of instances in the data set, the user needs several mouse clicks to expand the clusters until reaching the actual data instances. Additionally, since the visual representation draws a set of circles inside a parent circle, the area which is used is always smaller than the parent circle area. Here we use min = √ n, and w &gt; 2. When the tree is balanced, it will not have more than 3 levels. In our tests, the tree's depth never exceeds 5, and in average it is 4. Therefore, the user normally has to execute only 2 clicks on a cluster in order to reach actual data instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Hierarchical Cluster Tree Construction input:</head><p>-N: a node to be split.</p><p>min: minimum number of elements that a node must contain in order to be split. output: -a cluster hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>procedure Hierarchy(N)</head><p>1: if |N| &gt; min then <ref type="bibr" target="#b1">2</ref>:</p><formula xml:id="formula_0">CH ← Splitting(N, |N|) 3:</formula><p>for all C ∈ CH do  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Splitting Strategy</head><p>In order to split the data instances to create the children nodes, a partition clustering algorithm, called bisecting k-means <ref type="bibr" target="#b17">[18]</ref>, is employed.</p><p>Although there is a common belief that agglomerative algorithms perform better than the partition ones, Zhao and Karypis <ref type="bibr" target="#b23">[24]</ref> argue that this belief mainly occurs because the experiments are executed on data sets with lower dimensions, which is not the case of document collections and many other data sets. Additionally, partition approaches are very suitable for clustering large document data sets due to its low computational requirements <ref type="bibr" target="#b23">[24]</ref>.</p><p>The bisecting k-means algorithm works by splitting the data instances into hyperspheres so that each data instance is closer to the center of the hypersphere it belongs to than to all other hyperspheres centers. Clusters are formed successively bisecting the data into pairs of clusters. In each bisection, a cluster is chosen and divided into two new clusters until the required number of clusters is created.</p><p>There exist several ways to choose which cluster to split, but since the differences between these methods are small <ref type="bibr" target="#b17">[18]</ref>, we employ a simple one: choose the largest cluster (more data instances) to split. This approach is fast and it tends to produce clusters of similar sizes, resulting on a hierarchical cluster tree as balanced as possible <ref type="bibr" target="#b23">[24]</ref>, an important feature for visual analysis applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Projecting the Hierarchy</head><p>Once the hierarchical cluster tree has been assembled, its nodes are projected in order to create the visual representation. Projecting a node means that its children nodes are positioned on the plane inside the area defined for them, preserving as much as possible the their neighborhood relationships. Initially, the ROOT node is projected onto the plane. When required by the user, its children clusters can be projected, and so on, until reaching the leaf nodes, that is, the data instances. <ref type="figure" target="#fig_2">Figure 2</ref> shows an example of a visual representation generated for the hierarchy presented in <ref type="figure" target="#fig_0">Figure 1</ref>(b). In <ref type="figure" target="#fig_2">Figure 2</ref>(a) the projection of the ROOT node is presented. The size of each circle is proportional to the number of data instances that belong to it, and the numbers inside the circles indicate the data instances. <ref type="figure" target="#fig_2">Figure 2</ref>(b) shows the result if a user selects the node (0, <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9)</ref> to be expanded -this node is replaced by the projection of its children on the layout.</p><p>(a) Projection of ROOT node. (b) Projection of (0, <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9)</ref> node. Although the common choice to place the multi-dimensional points on the plane is a point placement strategy (see Section 2), since in our case we have circles (with a certain area), instead of points, these techniques cannot be directly applied. In addition, the overlap must be avoided and point placement techniques normally do not support it -the overlap must not occur since the children of a node are placed inside its area, and in the presence of overlap, data instances which are not related can be projected close on the final layout.</p><p>The approach proposed here splits the projection task into two main steps. First, the children nodes are projected to the plane using a point placement strategy without taking their sizes into consideration (they are represented by their centers). Then, these nodes are spread onto the plane in order to resolve overlaps.</p><p>For the projection we employ the Least-Square Projection (LSP) technique <ref type="bibr" target="#b14">[15]</ref>, which has shown good results for documents (see Section 3.4 for more details).</p><p>The algorithm for the distribution of points to avoid overlap was designed as follows: for each child node C i it is verified if an overlap occurs with all other children C j . For each node with which overlap happens, both nodes are moved in the opposite direction of each other, with C j giving a longer step than C i (without getting out of the area of its parent node). This step is proportional to the sizes of both nodes, but instead of taking the entire step to avoid the overlap -which is the sum of the nodes radius -a fraction of it is employed ( f rac). Running our algorithm for different values of f rac on various data sets we verified that 4 ≤ f rac ≤ 8 prevents that, in each iteration of the algorithm, the change in position is too large to maintain the perception of neighborhood between nodes. Using this fraction range, nodes are spread smoothly in each iteration. Nevertheless, other values can be chosen by the user.</p><p>If in the original projection no overlap between two nodes occurs, then a threshold (thresh) is added to the separation, which must exist between them after the spreader algorithm is applied. This threshold is proportional to the area of the parent node, and it aims at preserving, on the final layout, the separation between groups defined in the original projection. The order of execution of this algorithm is from the largest node (more data instances) to the smallest. Thus, larger nodes are more anchored than smaller nodes, assuring a faster convergency of the algorithm. This spreading process is executed until there is no more movements, or until a maximum number of iterations (max) is reached. This process is outlined in Algorithm 2. for all C j ∈ CH and C i = C j do 7:</p><formula xml:id="formula_1">d ← dist(C i ,C j ) 8: s ← size(C i ) + size(C j ) 9:</formula><p>if C i not originally overlaps C j then 10:</p><formula xml:id="formula_2">s ← s + thresh 11:</formula><p>end if 12:</p><p>if s &gt; d then <ref type="bibr" target="#b12">13</ref>:</p><formula xml:id="formula_3">Δ ← (s − d)/ f rac 14:</formula><p>Calculate a vector − → v from C i to C j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>Move C j in the direction of − → v a 3Δ 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Move C i in the opposite direction of − → v a Δ 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>changed ← true iteration ← iteration + 1 22: until changed and iteration ≤ max.</p><p>In Algorithm 2, dist(C i ,C j ) stands for the distance on the plane between the centers of C i and C j , and size(C i ) stands for the size (radius) of C i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Defining the size of nodes</head><p>Every time a node is projected, the radii of its children have to be defined. This is performed so that the area of a child node is proportional to the number of data instances belonging to it, and that the area of the parent node (the node being projected) is filled, up to an extent, without overlap. Since this is a difficult non-linear optimization problem, we use a naive solution.</p><p>Ignoring the non-overlapping constraint, this problem can be solved assigning a factor f i to each child node C i , with 0 ≤ f i ≤ 1 and ∑ f i = 1, calculating k according to the following equation:</p><formula xml:id="formula_4">π(k * f 1 ) 2 + π(k * f 2 ) 2 + ... + π(k * f n ) 2 = πr 2 that is, k = r 2 f 2 1 + f 2 1 + ... + f 2 n<label>(1)</label></formula><p>and computing the radius of a node C i as k * f i .</p><p>In this equation r is the radius of the parent node, and the factor f i is proportional to the number of data instances that belong to the child node C i . These factors are calculated as f i = |C i |/|P|, where P denotes the parent node.</p><p>If this approach is employed as is, the sum of the area of the child nodes is the same of the parent node, which leads to an overlap and a non-clear separation between the groups of related (similar) nodes. The solution adopted is to use only a fraction of the area of the parent node. In the results section we use 50% of the parent area. This value can be changed by the user but that may cause the non-overlapping condition to be violated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Projecting using anchors</head><p>When a node is projected, its child nodes are positioned on the plane so that similar children are placed close together. When the children are data instances, the dissimilarity amongst them are directly calculated since they are multi-dimensional vectors. In the case of clusters, the dissimilarity is calculated considering their centroids. Centroids are vectors embedded on the same space of the data instances, thus the same process used to calculate the dissimilarity between data instances can be applied to assess the dissimilarity between clusters, and between clusters and data instances.</p><p>In order to perform such projection we employ the Least-Square Projection (LSP) technique due to its ability to create useful layouts of high-dimensional spaces, being suitable to project document collections. In LSP, first a set of data instances is chosen, called control points, which represents as much as possible the groups of instances present in the data, and projected on the plane using a high-precision projection technique <ref type="bibr" target="#b14">[15]</ref>. Then, making use of a neighborhood graph (a graph which connects each instance to its nearest neighbor) and the 2D coordinates of the control points, the remaining data instances are projected following the distribution of the control points on the plane and seeking to place each data instance on the convex hull of its nearest neighbors.</p><p>LSP normally results on accurate layouts in terms of grouping of the similar instances and separating the dissimilar ones -although it tends to created overlapped layouts, in our case it is not a problem since the spreader algorithm is later applied. However, if the original LSP approach is employed to project the cluster hierarchy, a placement problem may occur. Suppose we have two sibling nodes very close to each other on the layout (sharing a boundary), and that both nodes are projected. In this case it is possible that the resulting projections place as neighbors children that are not closely related (children of sibling nodes placed close to the boundary between them).</p><p>To avoid that, when projecting a node N, first a Delaunay triangulation involving N and its siblings is created -considering their centers. Using this triangulation, it is determined which nodes compose the convex hull of N (if a node has less than 3 siblings, all siblings are used), and their centers are mapped onto its boundary. Then, these points are used in the LSP process as control points, and the centroids of the clusters they represent are employed when defining the neighborhood graph. Using this approach, information about the siblings that compose the convex hull of node N is taken into account when it is projected, reducing the problem of a final layout presenting nonrelated instances as neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RE-ARRANGING THE HIERARCHY</head><p>Clustering techniques aim at extracting possible groups of related data instances from a data set. To perform such task, a heuristic is applied, and depending on what is used and what is perceived as a cluster, different results can be attained <ref type="bibr" target="#b18">[19]</ref>. Additionally, if the number of clusters present in the data set is not known (which is normally the case), it must be defined using some approach, which can also lead to different sets of clusters. Therefore, it is possible that the resulting clusters do not match with what is expected by a user, possibly presenting clusters with instances that are not adequately related, or placing highly-related instances into different clusters.</p><p>In order to surpass this problem, a strategy to join clusters based on the visual inspection of the layout is suggested here. In this strategy, after the user has selected S clusters to join, a new node N is created containing the data instances of these nodes and it is attached to their common ancestor. After that, the node N is projected onto the plane, its topic is extracted, and all nodes which have instances removed are resized, and their topics renewed. In this way, a user can reconstruct the hierarchy of clusters using his or her own interpretation, re-organizing the data set based on his or her knowledge. In order to split a cluster it is only necessary to go down on the hierarchy, or to create a cluster selecting individual data instances. This joining strategy is summarized in Algorithm 3. Remove S i from C. A ← get the ancestor of node S i which is a child of C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Remove the instances of A which are common to the instances of S i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Decrease the size of A to be proportional to the number of data instances belongs to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Re-create the topic of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>end if 13: end for 14: Define the size of N proportional to the number of instances belonging to it. 15: Add N as a child of C. <ref type="bibr" target="#b15">16</ref>:</p><formula xml:id="formula_5">Spreader(C)</formula><p>The new node is positioned in order not to change too much the layout which is already constructed, only making local replacements (on the neighborhood of the selected clusters). This is an important feature, otherwise all information that the user has already learned from the visual representation, based on the position of the clusters, could be impaired. The new cluster N is positioned in the convex-hull of the selected clusters, closely placed to the clusters that present the larger number of data instances. To calculate the x coordinate of the center of N, the following equation is applied (the y coordinate is calculated the same way):</p><formula xml:id="formula_6">x = k ∑ i=1 ⎛ ⎜ ⎜ ⎝ |S i | k ∑ n=1 |S n | x i ⎞ ⎟ ⎟ ⎠<label>(2)</label></formula><p>where S = {S 1 , S 2 ,...,S k } denote the selected nodes, and x i is the x coordinate of node S i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXTRACTING MULTILEVEL TOPICS</head><p>In exploratory tasks involving documents there is need to support identification of content of groups of documents. In our approach a label is assigned to each cluster identifying the most relevant topic being tackled by its subsets of documents. To compose a label for a cluster, initially a "document x terms" matrix (see Section 6) is created considering the centroids of its child nodes. Based on this matrix, first the pair of words with the highest covariance in the vector representation is chosen. Then, for each remaining (non-selected) word it is computed the mean of the covariance relative to these first two words. If the resulting value is close to the highest covariance (in this paper we use 50% of that value) the word is added to the label. If this approach is applied to clusters in subsequent levels, it is likely that they present equal or very similar labels. To avoid such problem, when a label is created we chose the first two initial words, the words with the highest covariance, to be different from the first initial words of its ancestors nodes. In this way, a word can appear in subsequent labels, but final labels will never be the same. For convenience, the label of the ROOT node is empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>In order to apply HiPP (as well as many of the currently available point placement strategies), it is necessary to convert the initial set of documents into a vector representation, and choose a distance measure (between the vectors) to assess the dissimilarities amongst the documents. Here we employ a conventional vector space model <ref type="bibr" target="#b16">[17]</ref> to represent the documents as vectors, and a cosine-based distance, defined in <ref type="bibr" target="#b5">[6]</ref>, to determine dissimilarity. To construct the vector representation, initially relevant terms in the document collection are determined, and after that their frequencies are calculated for each document. The result is a "document x terms" matrix where the lines are the documents (vectors) and the columns are the frequency of the relevant terms in them (attributes). <ref type="figure" target="#fig_7">Figure 3</ref> shows a document map created using the technique presented here. This is a map of a document collection composed by 680 scientific papers, with title, authors, abstract and references in four different areas: Case-Based Reasoning (CBR), Inductive Logic Programming (ILP), Information Retrieval (IR), and Sonification (SON). The colors indicate these areas, with red for CBR, yellow for ILP, light blue for IR, and dark blue for SON. To color a cluster, it was used the most frequent subject area of the documents it contains -if the most frequent subject occurs in less than 70% of the documents, the cluster is colored using a neutral color (beige). That picture presents the toplevel view of the map (the ROOT node projection), where the circles represent high-level clusters of documents. The set of words placed over each node are the topics extracted from them. It can be seen that for each one of the four scientific papers areas there exist nodes representing them and that the nodes representing groups of papers with similar topics are laid out close together in the visual representation. In order to verify if the global accuracy in separating or grouping documents based on their similarity of content holds on small portions of the map, five documents belonging to the SON area, which are known to share a well related content, were intentionally added to the collection. <ref type="figure" target="#fig_8">Figure 4</ref> presents the resulting view when all clusters and sub-clusters of <ref type="figure" target="#fig_7">Figure 3</ref> are expanded showing the documents. In this figure these five documents are colored in green, and circled and labeled as "A", indicating that highly correlated documents are placed close together on the final layout. The documents circled and labeled as "B", which seem to be misplaced, are more related with "audio analysis", therefore better classified as SON papers. The documents in "C" are the only documents that deal with the "parallel programming" topic, being very focused on performance analysis. Although, from the color labeling, it would seem that they are poorly positioned, if their neighbors are further investigated, it can be noticed that they are also focused on performance analysis, but on different subjects, thus the documents in "C" are placed close to documents that share a common subject. Since IR and SON papers were collected from internet searches using only term queries belonging to those areas, the original labeling is not equivalent to an expert classification, therefore justifying the apparent visual misclassification according to color. The previous examples show that HiPP can separate well documents about different subjects based on their content. However, that data set is very heterogeneous, composed largely by documents that can be separated into one of distinct areas. In order to verify if the technique can also produce good results on a more homogeneous collection, a document map of the 2004 IEEE Information Visualization contest <ref type="bibr" target="#b6">[7]</ref> data set was created. This data set is composed by papers of ten years of the InfoVis conference and some other papers frequently cited by them. The resulting projection is presented in <ref type="figure" target="#fig_9">Figure 5</ref>. In this figure the documents are colored according to frequency of occurrence of some keywords -"debug program", "spreadsheets", "graph drawing", and "genomic sequence" -and the area where these subjects occur are circled and labeled, showing that the previous results on separation and grouping based on content also hold for more homogeneous collections. For scientific papers title, we used abstract, authors and references as content. Another example, given next, was built from RSS feeds of news, considerably smaller than papers abstracts. <ref type="figure" target="#fig_11">Figure 6</ref> presents a document map for RSS news from BBC, CNN, Reuters, and Associated Press, collected during two days in April 2006. In <ref type="figure" target="#fig_11">Figure 6</ref>(a) the most abstract view of the collection is shown (the high level clusters), and the topics of large clusters are presented, showing the common subjects handled by the different sources: "bird AND flu", "immigration AND senate", "game AND microsoft", and so on. Clusters are colored according to the percentage of documents belonging to a cluster that presents the topic words.</p><p>When creating the hierarchical cluster tree, the data instances of a node are clustered, creating √ n clusters (where n is the number of data instances on the node). As any other heuristic which is applied to set the number of clusters to be found on a data set, this can result in different clusters containing documents which share similar content (more clusters were chosen than the actual number in the data set). To handle this case, the user can select clusters and join them into a new cluster, re-organizing the hierarchy. <ref type="figure" target="#fig_11">Figure 6</ref>(b) presents the result of the map when clusters identified by the topic "bird AND flu" were joined into a single cluster. <ref type="figure" target="#fig_11">Figure 6</ref>(c) presents this document map after all clusters were expanded, showing that the joined documents share the same neighborhood. In that figure documents are colored according to the frequency of "bird AND flu".</p><p>The current visual representation adopted by almost all point placement techniques (except by the Infosky technique) map each data instance to a visual element and presented them at once to the user. In this way, the number of instances that can be laid out is limited by the visual area available, and if there is not enough space to plot the elements overlap will occur, impairing the user interpretation, and hindering the identification of dense regions. In addition, showing all data instances at once can result in cognitive overload. <ref type="figure" target="#fig_13">Figure 7</ref> presents document maps using the usual visual representation and the representation adopted by the HiPP technique. Both maps are projections of a document collection composed by 30,000 news articles extracted from the Reuters Corpus <ref type="bibr" target="#b11">[12]</ref>, and are colored according to the frequency of the word "Clinton". The map of <ref type="figure" target="#fig_13">Figure 7</ref>(a) was created using LSP. In this case there is severe overlap, and it is difficult to determine clusters of high-related elements; when the clusters are identified it is difficult to evaluate their density. With HiPP´s visual representation <ref type="figure" target="#fig_13">(Figure 7(b)</ref>) the groups of documents are explicitly represented, and their densities are mapped to their sizes (and possibly to color). In addition, the possibility of joining and splitting clusters of elements can be used to organize the data as it is analyzed, supporting an adequate overview of a document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation and Discussion</head><p>Usually distance-based point placement techniques are analytically evaluated using a stress function (a cost function that measures the information on distance lost during the projection process). However, since different techniques normally work by minimizing different stress functions, it is expected that a technique evaluated using its own function be better than other techniques which are based on other functions. Therefore, the result is always biased by the stress employed. In addition, it is common to find examples of incompatibility between the stress and the visual interpretation of layouts <ref type="bibr" target="#b14">[15]</ref>. Here, we define a simple approach to assess different layouts based on the neighborhood preservation. This is calculated by taking the k nearest neighbors of a multi-dimensional point (data instance), the k nearest points of its projection on the plane, and checking what proportion of the neighborhood is preserved. <ref type="figure">Figure 9</ref> employs this evaluation to compare different layouts produced by the hybrid model <ref type="bibr" target="#b12">[13]</ref>  <ref type="figure">(Figure 8(a)</ref>), the LSP technique <ref type="figure">(Figure 8(b)</ref>), and the HiPP technique <ref type="figure" target="#fig_8">(Figure 4)</ref>. Also it presents the result if the HiPP technique is applied without using the anchors on the projection (see Section 3.4). These layouts were generated using the CBR, ILP, IR and SON data set, and colors indicate subjects discussed in the documents.</p><p>The worst result is given by the hybrid technique, preserving less than 25% of the points neighborhood, matching a visual inspection. HiPP is the best technique for less than 70 neighbors, and for more than 85 LSP is better. HiPP is better than LSP for less neighbors because the points are clustered before the projection, resulting on layouts with more accurate local relations (relations inside the same cluster) since large distances are ignored. For more neighbors, LSP is better because all points are considered together on the projection process, therefore global relations are better preserved. If anchors are not used on HiPP, the results are as precise as the results using the anchors for small numbers of neighbors. In this case, since the neigh-  borhood of a point is normally defined inside the cluster it belongs to, ignoring information about other clusters does not affect the precision. However, as the number of neighbors increases, the neighborhood is defined over different clusters, so ignoring the information about other clusters when projecting one cluster decreases the accuracy of neighborhood preservation on the final layout. <ref type="figure">Figure 9</ref> can also be helpful to give insight into differences between: (1) first clustering and then projection; and (2) first projection and then clustering. In terms of neighborhood preservation, the former strategy preserves the local neighborhood information better than the latter; the reverse is true for preservation of global information. However the main difference is that, in the second case, the clustering is executed on a projected data set, being applied on a data set that does not contain all the information present in the original data set. In the first case, which is the one employed by HiPP, clustering and projection are executed considering the original data, therefore the distance relations used in both tasks are the same and the result of clustering is expected to be more coherent regarding the original similarity relationships.</p><p>A point to be made regarding the high degree of precision presented by HiPP is that this is so due to the fact that the similarity relationship employed is adequate for the data set. In other words, HiPP and related similarity based techniques are highly dependent on the accuracy of the distance calculations for a particular analysis task. For text, clearly the formation of the vector representation is crucial. That suggests that applications where the proper vocabulary can be defined (such as those properly served by a pre-defined vocabulary, taxonomy or ontology) will benefit greatly.</p><p>In terms of computational requirements, the complexity of HiPP can be calculated as O(C + P + S), where C is the complexity to create the hierarchical cluster tree, P is the complexity to project it, and S is the complexity to spread the projections. Considering the worst scenario where the tree is completely unbalanced -each level is composed by √ n − 1 singleton clusters (clusters with one element) and one large cluster -, it will have √ n levels. In this case, the value of C, which is the complexity of the clustering algorithm employed, is O(n √ n) <ref type="bibr" target="#b18">[19]</ref>. The complexity to project the points using LSP is O(n 2 ), but since the projection is applied at most in √ n instances when projecting a level (cluster), it is linear regarding the number of instances of the data set. Since it is necessary to project √ n levels, the whole complexity is O(n √ n). The same analysis can be applied to the complexity of the spreader algorithm. Therefore the overall complexity of HiPP is O(n √ n + n √ n + n √ n) = O(n √ n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>We have presented a technique called HiPP, a hierarchical point placement strategy for displaying, interacting, and organizing large multidimensional data sets, with particular attention to document collections. Different from the most common placement strategies, which present all data instances at once to the user, and clustering visualization techniques that handle the data in clusters, HiPP defines a hier-archical structure enabling the user to analyze multi-dimensional data sets on different levels of detail. The visual representation adopted is composed not only by individual data instances but also by circles representing clusters of instances. Circles are packed inside circles indicating sub-clusters inside clusters, and their position on the plane also reflect the similarity relationships amongst them.</p><p>In this paper we also present an strategy to extract topics from groups or clusters of documents -a topic being defined as a group of words that properly suggests the main subject handled by a group of documents -and an approach that allows users to join or split different clusters in the hierarchy. Thus, making use of the information conveyed by the topics, the user can re-organize the hierarchy, joining clusters which share a related topic, and adapting the visual representation to clusters of documents discovered during the analysis of the data set. Thus users can quickly focus on the parts of interest on the document map, reducing the problems related to the cognitive overload generated when analyzing the complete data set at once.</p><p>The results of the HiPP technique regarding preservation of neighborhood relationships are as good or better than other point placement strategies, and normally much more precise in small neighborhoods, assuring that the properties present in the overview analysis are preserved when exploring smaller groups of documents during focusing tasks. Although more precise, the computational complexity is similar to these techniques, and computational time is saved since the clusters are positioned on demand, and time is only spent calculating the parts of the data set that are in focus at any particular time.</p><p>In order to create the hierarchy, a partition clustering algorithm is employed. Although this algorithm produces good results for documents collections <ref type="bibr" target="#b17">[18]</ref>, for other types of data it maybe be preferable to use another clustering strategy. This is one of the ways the technique can be adapted to the domain on which it is applied (even for documents, other techniques can be employed that work well for particular types of textual information).</p><p>HiPP is accompanied by a topic extraction technique that can also be replaced by other summarization techniques when employed for other types of multidimensional data.   <ref type="figure">Fig. 8</ref>. Examples of document collections projections using different techniques. <ref type="figure">Fig. 9</ref>. Evaluation of neighborhood preservation for layouts generated using different techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>shows a simple example of 25 points on a plane (data instances) and a hierarchical cluster tree created for such points using the procedure explained above. The tree nodes are represented by boxes with numbers indicating the data instances that each node contains.(a) Points on a plane representing a group of bi-dimensional data instances. (b) Hierarchical cluster tree of the bi-dimensional data set. Leaves (gray circles) are individual data instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>A set of bi-dimensional instances and their hierarchical cluster tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Example of projecting the hierarchy defined on Figure 1(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 repeat 4 :← false 5 :</head><label>245</label><figDesc>Spreading the nodes to avoid overlapping. input:-N: the node to be displayed.max: maximum number of iterations. f rac: the fraction of movement to avoid overlap. thresh: separation between two clusters if they do not overlap. output: -the positioning of the children of N on the plane. procedure Spreader(N) 1: iteration ← 1 2: CH ← get the children of N. 3: changed for all C i ∈ CH do 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Joining nodes of a hierarchical cluster tree. input: -S: nodes selected to join. output: -a cluster hierarchy. procedure Join(S) 1: Create a new node N containing all data instances of the nodes in S. 2: Project N according to the nodes in S (using Equation 2). 3: C ← get the common ancestor of the nodes in S. 4: for all S i ∈ S do 5:if S i is a child of C then 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>A document map of a document collection composed by scientific papers from four distinct subject areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>All clusters of Figure 3 expanded showing the data instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>A document map of a more homogeneous scientific papers document collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Top abstraction of the collection. (b) Joining clusters about "bird AND flu". (c) View with all clusters expanded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 6 .</head><label>6</label><figDesc>Document map for RSS news feeds from various sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Least-Square Projection (LSP). (b) Hierarchical Point-Placement (HiPP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 7 .</head><label>7</label><figDesc>Comparing the visual result of a single level point placement strategy with the visual representation adopted here.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work is supported by FAPESP research financial agency, São Paulo, Brazil (proc. no. 04/07866-4 and 04/09888-5), CAPES research financial agency, Brazil (proc. no. 2214-07-5), and CNPq research financial, Brazil (proc. no. 304758/2005-1 and 484256/2007-6). We wish to acknowledge our research colleagues for some of the data and for fruitful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The infosky visual explorer: exploiting hierarchical structure and document similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sabol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Droschl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kappe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tochtermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="166" to="181" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A linear iteration time layout algorithm for visualising high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VIS&apos;96</title>
		<meeting>of VIS&apos;96</meeting>
		<imprint>
			<publisher>IEEE CS Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page">127</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A A</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Point placement by phylogenetic trees and its application for visual analysis of document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Cuadros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Telles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VAST&apos;07</title>
		<meeting>of VAST&apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A heuristic for graph drawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Eades</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fastmap: a fast algorithm for indexing, datamining and visualization of traditional and multimedia datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGMOD&apos;95</title>
		<meeting>of SIGMOD&apos;95</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="163" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<ptr target="http://www.cs.umd.edu/hcil/iv04contest" />
	</analytic>
	<monogr>
		<title level="j">IEEE InfoVis</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical parallel coordinates for exploration of large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Rundensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VIS&apos;99</title>
		<meeting>of VIS&apos;99</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Drawing graphs with non-uniform vertices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AVI&apos;02</title>
		<meeting>of AVI&apos;02</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tree-maps: a space-filling approach to the visualization of hierarchical information structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VIS&apos;91</title>
		<meeting>of VIS&apos;91</meeting>
		<imprint>
			<publisher>IEEE CS Press</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="284" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Principal Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>second (a) Hybrid model. (b) Least-Square Projection. edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RCV1: A New Benchmark Collection for Text Categorization Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A pivot-based routine for improved parent-finding in hybrid mds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On cluster validity for the fuzzy c-means model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TFS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="379" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Least square projection: a fast high precision multidimensional projection technique and its application to document mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levkowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="564" to="575" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cone trees: animated 3d visualizations of hierarchical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI&apos;91</title>
		<meeting>of CHI&apos;91</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Developments in automatic text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">253</biblScope>
			<biblScope unit="page" from="974" to="980" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparison of document clustering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD&apos;00</title>
		<meeting>of KDD&apos;00</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="109" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introduction to Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On improved projection techniques to support visual exploration of multidimensional data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tejada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="218" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multilevel algorithm for force-directed graph drawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Walshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of GD&apos;00</title>
		<meeting>of GD&apos;00</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualization of large hierarchical data by circle packing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI&apos;06</title>
		<meeting>of CHI&apos;06</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The ecological approach to text visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Wise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1224" to="1233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluation of hierarchical clustering algorithms for document datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CIKM&apos;02</title>
		<meeting>of CIKM&apos;02</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
