<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Large-Scale Video News via Interactive Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangzai</forename><surname>Luo</surname></persName>
							<email>hluo@uncc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
							<email>jfan@uncc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ribarsky</surname></persName>
							<email>rebarsky@uncc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><forename type="middle">'</forename><surname>Ichi Satoh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UNC-Charlotte Charlotte</orgName>
								<address>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<addrLine>UNC-Charlotte Charlotte</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<addrLine>UNC-Charlotte Charlotte</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<addrLine>UNC-Charlotte Charlotte</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">National Institute of Informatics Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Large-Scale Video News via Interactive Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>News Visualization, Semantic Video Classification I</term>
					<term>2</term>
					<term>6 [Artificial Intelligence]: Learning-Concept learning; I</term>
					<term>3</term>
					<term>6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we have developed a novel visualization framework to enable more effective visual analysis of large-scale news videos, where keyframes and keywords are automatically extracted from news video clips and visually represented according to their interestingness measurement to help audiences find news stories of interest at first glance. A computational approach is also developed to quantify the interestingness measurement of video clips. Our experimental results have shown that our techniques for intelligent news video analysis have the capacity to enable more effective visualization of large-scale news videos. Our news video visualization system is very useful for security applications and for general audiences to quickly find news topics of interest from among many channels.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The news industry is a large and profitable industry. Hundreds of news companies and producers report a tremendous amount of news stories in the format of text, audio or video everyday. Much of this content, including news from many countries, is now available to viewers via cable TV or Internet videocast. Intelligent analysis of such news stories can provide valuable information on science, politics, business and even military matters. On one hand, general audiences can have fun by reading, listening to or watching interesting news stories. On the other hand, security experts and business analysts can gather necessary information from news stories for decision making. Not only are news videos publicly available and rich in information, but international TV news provides a global overview of all news reports, which can represent important information about public opinion, discussions, and real thoughts from other countries in the world. Obviously, this information could be very useful for intelligence analysis, investment decision making, political or cultural analysis, and many other uses.</p><p>Due to the large amounts of video generated every day, discovering and analyzing news stories of interest is becoming an increasing problem. In news analysis, for example, it is becoming untenable to hire people to manually process all available news videos and produce summarization for them. Manual analysis of large-scale news videos is too expensive, and it may take long time for response. In addition, the manual summarization is generally biased and may mislead decision makers who use these summaries. On the other hand, providing summarization and visualization of largescale news videos is also very important for general audiences, i.e., to save them time in searching and reading news of interest. Based on these observations, there is an urgent need to develop new techniques for: (a) intelligent analysis of large-scale news videos to extract the news stories of interest; (b) more effective visualization of news video collection.</p><p>Targeting these requirements, some researchers have developed keyword-based news retrieval systems and these techniques have been widely used in news industry (i.e., news websites). However, the keyword-based news retrieval systems make the big assumption that the users have clear ideas about what are looking for, but this may not be true. The news video database has the following properties: (1) Dynamic, e.g. changing every day; (2) The topics have large diversity; (3) Unpredictable (or else it's no longer news). In addition there are broad correlations among events. For example, sales may be affected by new products, interest rate changes, new political policies or even large foreign investments. Thus neither experts nor general users may have pre-specified preferences when they explore the news database. They may not be able to decide what relevant keyword to search for because there are too many potential keywords. Some news systems have provided another approach for news retrieval by classifying the news stories into a set of categories for browsing, but they also suffer from the same problem as the keyword-based systems, i.e., the users do not know what the news stories of interest are during that day or time period. In addition, the lack of preference beforehand does not mean the users will have interest in every news story provided by the system. For a certain user, he/she may just have interest in a few news stories among thousands of available. Unfortunately, most existing systems for news retrieval, filtering, ranking and summarization have not explored user attention models to enable more effective news stories organization, indexing and visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>Based on these observations, we have developed a novel framework for news video analysis and visualization. Our framework has the following advantages: (a) First, a new algorithm for statistical video analysis is developed to extract the news stories of interest from large-scale TV news collections. (b) Second, a user attention model is developed by automatically assigning importance weights for the available news stories of interest. (c) Third, a novel news video visualization framework is developed to organize and visualize large-scale news stories more effectively and attract the  <ref type="figure" target="#fig_0">Figure 1</ref>, our new framework is able to bring together two research areas: visualization and statistical video analysis. We have observed that more effective techniques for statistical video analysis enable more efficient visualization of large-scale news stories. On the other hand, efficient visualization is able to provide useful feedback to improve our techniques for statistical video analysis.</p><p>By visualizing the semantic elements (such as keyframes and keywords) that are extracted from large-scale news video collections, our system is able to present abundant information to the users and attract their attention. By incorporating statistical video analysis for knowledge discovery (i.e., extracting news stories of interest), our system is able to relieve analysts from government or business from the burdensome task of watching and reading large volume of news videos and permit them to make decision based on valuable knowledge discovered by our system. In addition, more effective visualization of large-scale news stories of interest can also help general audiences find their personalized news easily. This integration of visual analytics capabilities is quite important in providing users with overviews plus appropriately accessible details for large amounts of streaming news video. Indeed, it is the first step towards an entirely scalable system where the coupling to interactive visualization (eventually with appropriate levels of abstraction) will be essential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>To support more efficient news browsing and organization <ref type="bibr" target="#b23">[23]</ref>, several systems have been proposed to enable news visualization, and they have received attention from media and researchers. Among them, some systems <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b7">7</ref>] adopt a world map or regional map to show some specific information. Statistical information of text news <ref type="bibr" target="#b16">[16]</ref> or video news <ref type="bibr" target="#b12">[12]</ref> is put on a world map to inform the audiences of the "hotness" of regions. Buzztracker <ref type="bibr" target="#b16">[16]</ref> also shows the correlations among different regions. Because only location related information is visualized in these systems, users may not have enough information to find the news stories of interest. IDVL <ref type="bibr" target="#b5">[5]</ref> can also visualize the result of keyword-based queries. ThemeView <ref type="bibr" target="#b10">[10]</ref> can visualize a large collection of documents with a predefined small keywords set. Because they require keywords input from the users, they are still far to be accepted. One commercial system, called 10 × 10 [9], organizes 100 icon images associated with 100 most important keywords from text news in a 10 by 10 icon grid. Because the icon grid cannot tell the users the real keywords and the importance weight between the keywords or the dynamic trend of the keywords along time, the naive users cannot figure out useful information from the icon grid easily. Another system, called newsmap <ref type="bibr" target="#b24">[24]</ref>, organizes news topics from Google news on a two dimensional rectangle, where each news story covers a visualization space that is proportional to the number of related news pages reported by Google. news titles are drawn in the corresponding visualization space allocated to them. It has several drawbacks: (1) The relative importance weights of news stories are assigned by the number of related news pages, and they may not be proportional to the interestingness of news stories; (2) news titles are folded in the visualization spaces assigned to the relevant topics and they may be difficult to read; (3) It lacks the capability to show the dynamic trend of news topics along time; (4) It cannot be directly extended for news video visualization. Thus there is an urgent need of developing new techniques for news video visualization.</p><p>Supporting efficient visualization of large-scale news video collections is very challenging. Even though there are many sophisticated text statistical analysis algorithms, performing statistical video analysis and understanding is still very hard if not impossible. The problem is caused by the semantic gap between the semantics of video clips from the human point of views and the low-level features that can be extracted by computers <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b17">17]</ref>. In addition, supporting statistical video analysis plays an important role in enabling more efficient visualization of large-scale news videos. Without extracting the semantic topics from large-scale news video collections automatically, it is very hard to visualize them effectively. On the other hand, visualization is also able to provide valuable feedback that can be used to improve the performance of the underlying techniques for statistical video analysis.</p><p>Our paper is organized as follows: Section 2 introduces our new framework for news video visualization. Section 3 presents our algorithms for statistical news video analysis and automatic assignment of importance weights for news stories. Section 4 gives the implementation details. We then provide conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VIDEO VISUALIZATION FRAMEWORK</head><p>There are two conflicting requirements for visualizing large-scale news video collection: (a) The visualization space is limited; (b) We need to show as many news stories of interest as possible in such limited space. Obviously, it is impossible to show all these news stories of interest in such limited space at the same time, we need to select and display only those most important news stories (i.e., most interesting news stories for the users) on the screen.</p><p>There are two critical issues that are very important to characterize the news stories and need to be detected and visualized:</p><p>(1) The relative ratios for the interestingness among the available news stories; (2) The dynamic trend of news stories along time. Based on this understanding, we have developed a good measurement to quantify the interestingness of news stories and this interestingness measurement is incorporated to assign the relative importance weights for the news stories automatically. In addition, we have also developed a new algorithm to detect the news stories of interest and their trends with time automatically from large-scale news video collections. In order to visualize the dynamic trends of the news stories with time, we have created a novel animation framework by automatically assigning an importance weight for each keyframe, so that the sizes of the keyframes can be made proportional to the interestingness of the corresponding news stories. When the interestingness of the relevant news stories changes with time, the sizes for the corresponding keyframes are changed adaptively and the corresponding keyframes are moved automatically to new places according to their importance weights. Thus our animation technique is able to visualize the changes of the news topics with time and provide valuable information for decision making.</p><p>By visualizing and animating the map of keyframes for the relevant news stories of interest, the users can have an overview of the daily news stories and their changing trend with time. An example of news video visualization is given in <ref type="figure" target="#fig_1">Figure 2</ref>. When an interesting keyframe (i.e., certain news story of interest) appears on the screen, the users can click the corresponding keyframe and watch the related news story. This click of the keyframes implies the user's preference of news stories, so we can build a personalized user attention model by using the semantics of the news stories that the user accessed. Our system can then retrieve the news video databases with the given user attention model and the relevant news stories will be returned to the user.</p><p>To implement such a system for large-scale news video visualization, we have addressed two critical issues:</p><p>(a) Interestingness measurement of news stories, which is related to the user preference and cannot be quantified accurately without users' inputs. To address this problem, we assume that each user wants to know as much information as possible when he/she does not have knowledge of video contents in the databases. Thus, our news video visualization framework should try to display as many news stories (certainly different) as possible within a limited space, and the interestingness for a certain news story should be characterized by the information (knowledge) it can provide to the users. The news stories that can provide more information to the users should be assigned with bigger importance weights and cover bigger visualization space.</p><p>(b) Informativeness measurement of news stories, which is used to characterize the information of news stories. Obviously, it is difficult to measure the total information that a specific news story can provide because every news reporters may give similar but not identical description for a specific event. Such a situation makes it difficult to perform statistical analysis of news videos. The users may pay attention to small items of news videos such as person's name, product's name or a specific video clip. The overall measurement of the whole news story can't characterize all aspects of the user attention model. Rather than quantifying the informativeness measurement of the whole news stories, we first partition the news videos into a set of semantic items and measure the information carried by each semantic item. For the closed captions, the video text and the audio of news videos, the semantic items are keywords. For the visual channel of news videos, the semantic items are the semantic concepts or the semantic objects (for example, human faces) carried by the corresponding video clips. These multi-modal semantics can be integrated to extract the news stories of interest and enable more effective knowledge-level visualization of large-scale news videos.</p><p>Based on these observations, we need to develop a good mea- surement to quantify the informativeness for a certain semantic item in a given news video clip, which is used to organize the visualization in Section 4. From information theory, the information that a semantic item carries largely depends on how well we can predict it. If we can predict a given semantic item completely by using our previous knowledge, this given semantic item may carry no information for us. Based on this observation, we use a global probability model which characterizes the distribution of all semantic items as the predictor:</p><formula xml:id="formula_0">G = {g (x) |x ∈ S} (1)</formula><p>where x is a given semantic item, S is the set of all semantic items and g (x) is the probability of the given semantic item x. Obviously, the probability distribution of semantic items may change over time, thus the local probability model for a specific time interval can be defined as:</p><formula xml:id="formula_1">L (t) = {l t (x) |x ∈ S t ⊆ S} (2)</formula><p>where t is the specific time interval of interest, such as one specific day. We have used different units in our experiments. For the general audiences, the most preferable unit is day. For some long term analysis, month is better. S t is the set of all semantic items in that specific time interval t and l t (x) is the probability of the given semantic item x. The difference between the local probability model L (t) for a specific time interval t and the global probability model G is able to tell us how much information we can obtained by knowing L (t). Because both the local probability model L (t) and the global probability model G are probability distributions, the widely used Kullback-Leibler divergence is able to characterize their difference:</p><formula xml:id="formula_2">D (L (t) G) = Σ x∈S t l t (x) log l t (x) g (x)<label>(3)</label></formula><p>The distance function D (L (t) G) is able to characterize the difference between L (t) and G, but we also need to evaluate the information carried by each semantic item x ∈ S t . By examining Eq. (3), one can observe that D (L (t) G) is composed of a set of components, and each semantic item x is only related to one single component in D (L (t) G). Thus the contribution for a certain semantic item x ∈ S t can be obtained by the relevant component of D (L (t) G) in Eq. (3). Based on this observation, we can define the interestingness of one certain semantic item x as:</p><formula xml:id="formula_3">w t (x) = l t (x) log l t (x) g (x)<label>(4)</label></formula><p>From Eq. (4), one can observe that the interestingness measurementŵ t (x) for one certain semantic item x depends on two factors: l t (x) and</p><formula xml:id="formula_4">l t (x) g(x)</formula><p>. The first factor l t (x) is used to characterize the local probability model L (t) and the second factor</p><formula xml:id="formula_5">l t (x) g(x)</formula><p>is used to characterize its difference with the global probability model G. Eq. (4) may emphasize the local probability too much in some situations. For example, in real news videos, an anchorperson may appear many times repeatedly in the same news program and may also appear in different news programs from the same TV channel. Thus the semantic item for him/her may have high frequency in the local probability model L (t). If Eq. (4) is directly used to organize the map of the keyframes (i.e., map of news stories of interest), we may select many anchor shots for visualizing the news stories of interest, which is unacceptable. Based on this observation, only the difference between the local probability model L (t) and the global probability model G should be used to characterize the interestingness measurement:</p><formula xml:id="formula_6">w t (x) = l t (x) g (x)<label>(5)</label></formula><p>The w t (x) in Eq. (5) can be normalized to simplify the multimodal data fusion:w</p><formula xml:id="formula_7">t (x) = w t (x) max x∈S t {w t (x)} (6)</formula><p>To enable more efficient visualization of large-scale news video collections, visual features should be considered. There are multiple types of visual features that may be important. Some types of visual features are similar to the text keywords and they can be processed by using Eq. <ref type="formula">6</ref>, such as the human faces and the semantic concepts of news video clips. Other types of visual features may not be characterized by using the same statistical analysis algorithms as described above. For examples, video production rules and text areas in news videos. To extract such kinds of visual features, we have also developed some specific statistical video analysis techniques as described later in this paper.</p><p>When the multi-modal importance weights (i.e., importance weights for video, audio, closed caption, special visual features) for all semantic items are computed by our system, they are combined to determine the overall weight for the given video clip. The unit of video clips (e.g. the partitioning over time) should be carefully selected to have best visualization. Too large unit may cause information loss; too small unit may carry too little information. Because of the natural properties of video, shot is the best suitable unit for visualization. A shot is a continuous set of frames captured by a camera for an uninterrupted period of time. Shot boundaries can be detected with high accuracy. Our system achieves 92% precision and 89% recall on Trec Video 2003 video database.</p><p>The overall weight for the given video shot is defined as:</p><formula xml:id="formula_8">w (i) = F W (S v (i)) ,W (S a (i)) ,W (S c (i)) ,W (V (i))<label>(7)</label></formula><p>Where i represents the i-th video shot, S v (i), S a (i) and S c (i) are the multi-modal semantic items (i.e., video, audio and closed caption) extracted from the i-th video shot, W ( * ) represents the set of weights determined by Eq. (6), V (i) is special visual feature set for the i-th video shot, W (V (i)) is the weight set assigned according to the video production rules and F ( * ) represents the fusion algorithm. Based on w (i), we can organize and animate the map of the keyframes more effectively.</p><p>With this foundation, we now turn our attention to a more detailed analysis of the multi-modal video clips. In the next sections, we describe the following techniques: (1) technique for extracting the semantic items from news video clips; (2) technique for extracting some special visual features for automatic weight assignment; (3) multi-modal data fusion technique for determining the final weight for each video shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STATISTICAL VIDEO ANALYSIS AND AUTOMATIC WEIGHT ASSIGNMENT</head><p>In order to determine the importance weight for each video shot, we have developed a novel algorithm to extract the multi-modal semantic items (i.e., video, audio, text) and important special visual features automatically and weights are assigned automatically by the proposed statistical video analysis algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic items extraction and statistical analysis of video</head><p>The basic unit for news video interpretation is the video shot. Unlike the keywords of text documents, a video shot may contain abundant information (i.e., an image is more than one thousand words). This specific property of video shot makes it hard to achieve effective statistical analysis on visual properties and assign importance weights to the corresponding video shots for news video visualization. To overcome this, we have developed a novel framework for statistical video analysis. There are three types of semantic units that are critical to determine the importance weights for the corresponding video shots: (a) The first type is the statistical properties of the video shots; (b) The second type is the special video objects that appear in the video shots; (c) The last type is the semantic concepts that are associated with the video shots. Because these three types of semantic units have different properties, different algorithms are needed to extract the relevant multi-modal semantic items by performing different statistical data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Statistical property analysis of video shots</head><p>The video shots are the basic unit for news video interpretation. Thus they can be treated as the semantic items for automatic weight assignment. One certain video shot may be repeated multiple times because of the following reasons: (1) video shots for the anchors may repeat multiple times in the same news program; (2) video shots for the participants of an interview may appear multiple times in the same program; (3) video shots for interpreting the important news may appear in both the news summary at the beginning and the detailed report later in the same program; (4) video shots for the important news may appear in different news programs of the same channel (at different time periods) or different TV channels. The last two situations of video shot repeating indicate the importance for the corresponding video shots. Nevertheless, the first two situations of video shot repeating may not indicate that the corresponding video shots are important. In addition, the repeating of video shots in news videos is very different from the repeating of keywords in text documents, and it cannot be detected automatically by using simple comparison of the video shots. Thus new techniques are desired for detecting the video shot repeating in news videos, such that we can assign the importance weights for the video shots automatically.</p><p>One of the authors of this paper has developed an algorithm for detecting the identical video shots from news videos <ref type="bibr" target="#b18">[18]</ref>. Even though this algorithm has high accuracy, the video shots repeated in news programs may have different properties due to captions, different capturing devices, different subpictures of anchor shots and different channel marks. To resolve this problem, the keyframes for the video shots are first partitioned into a set of sub-blocks as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Three 1-D color histograms are extracted from three color channels for each sub-block. The similarity between two keyframes is computed by the weighted sum of the similarities of the corresponding sub-blocks. The relative weights of sub-blocks are assigned by using numbers as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The purpose of the specific sub-blocks weights assignment is to help cluster anchor shots of the same anchor person together. The similarity of each pair of the sub-blocks is computed by the intersection of their color histograms. For each video shot, only 10 frames are selected and used as the keyframes to reduce the computation cost. The similarity of a pair of the video shots, ϕ (i 1 , i 2 ), is computed by the maximal similarity of all their keyframe pairs (i.e., 100 pairs).</p><p>Finally, a fixed threshold is used to detect the repeating of the video shots. If ϕ (i 1 , i 2 ) &gt; τ r , the corresponding video shots i 1 and i 2 are repeated over time. In our system, the threshold τ r is set to  0.9. We found the fixed threshold is good enough for most experiments thus we did not take further effort to design adaptive algorithm.</p><p>Since a given video shot may appear several times in the same program or in different programs, connected component analysis is performed by treating video shots as nodes and repeating video shot pairs as edges. The video shots in the same connected components are the multiple occurrence of one video shot. The intraprogram repeating number r intra (i) and inter-program repeating number r inter (i) for each video shot can be computed by counting the connected components. The two numbers r inter (i) and r intra (i) for most video shots are equal to 1 because they are not repeated along time. Obviously, some video shots may have these two numbers bigger than 1 and the different repeating modes (i.e., different repeating situations) may provide different semantics so that different weights should be assigned. The weights for different repeating numbers are approximated by using a bell shaped curve: </p><formula xml:id="formula_9">w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Video objects detection and statistical analysis</head><p>For news videos, text areas and human faces may provide important information of news stories of interest. By using the technique proposed in <ref type="bibr" target="#b13">[13]</ref>, text lines in news videos can be detected automatically. Human faces can also be detected by using the programs and models developed by OpenCV <ref type="bibr" target="#b4">[4]</ref>. Obviously, these automatic detection functions may fail in some cases. Thus the results that are detected by using a single video frame may not be reliable. To address such problem, the detection results for all the video frames within the same video shots are integrated and the relevant confidence maps for the detection results are calculated. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, such confidence maps can provide valuable information for evaluating the detection results. The confidence region is generated by transforming the relevant confidences for our detection results into a binary image via thresholding. The threshold for generating the confidence region of text is set to 0.5. The threshold for generating the confidence region of human faces is set to 0.35. Obviously, the size ratio between the confidence region and the size of video frames provides some valuable information for weight assignment, and thus the size ratios for text and human faces regions are obtained, α text (i) and α f ace (i). The sigmoid curve is used to determine the importance weights for the text regions and human faces:</p><formula xml:id="formula_10">w area (i) = 1 1 + e − max{α(i)−ν,0} λ<label>(9)</label></formula><p>where the parameters ν and λ are used to control the shape of the curve. In our current implementation, ν text = 0.05, λ text = 0.1593, ν f ace = 0.01 and λ f ace = 0.04096. For a given video shot, the importance weight for human faces w f aceArea (i) and the importance weight for text regions w textArea (i) can be determined by:</p><formula xml:id="formula_11">w textArea (i) = 1 1+e − max{αtext (i)−νtext ,0} λtext w f aceArea (i) = 1 1+e − max{α f ace (i)−ν f ace ,0}</formula><p>λ f ace <ref type="bibr" target="#b10">(10)</ref> By performing the face clustering technique developed in <ref type="bibr" target="#b19">[19]</ref>, face objects can be clustered to several groups and the human objects can be identified and be treated as the semantic items for weight assignment by using Eq. (6). The importance weight for human faces of shot i is computed by Eq. <ref type="formula" target="#formula_12">11</ref>:</p><formula xml:id="formula_12">w f ace (i) = max x {w t (x) |x ∈ f aces (i)} f aces (i) = / 0 0.5 f aces (i) = / 0<label>(11)</label></formula><p>Where f aces (i) is the set of all face objects of shot i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Semantic concept classification</head><p>The semantic concepts of the video shot may provide valuable information to enable more efficient visualization and retrieval of large-scale news video collections. Semantic video classification is one of the potential solutions to detect the semantic concepts of video shots. However, semantic video classification is still a challenging problem <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b21">21]</ref>. Many semantic video classification techniques have been proposed by different researchers. The related techniques can be classified into two categories:</p><p>(1) Rule-based (i.e., model-based) approach by using domain knowledge to define perceptual rules and achieve semantic video classification <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b8">8]</ref>. One advantage of the rule-based approach is the ease to insert, delete, and modify existing rules when the nature of the video classes changes. However, effective techniques for semantic video classification should be able to discover not only the perceptual rules that can be perceived by human inspection, but also the hidden significant correlations (i.e., hidden rules) among multi-modal inputs. Thus the rule-based methods can only detect the semantic concepts correlated to the video making rules. Unfortunately, most semantic concepts in news videos generally have little correlations with the underlying video making rules.</p><p>(2) Statistical approach by using statistical machine learning techniques to extract the semantic concepts [1]. The statistical approach can support more effective solutions for semantic video classification by discovering non-obvious correlations (i.e., hidden rules) among different video patterns. However, its performance largely depends on the success of the underlying framework for video content representation and feature extraction. The visual features, which are selected for video content representation, should have the ability to discriminate among various semantic concepts. The difficulty for the existing frameworks for video content representation is the lack of means to relate the low-level visual features to the high-level semantic concepts. Most existing systems for content-based video retrieval (CBVR) use the shot-based or objectbased (or, region-based) <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b17">17]</ref> visual features for video content indexing. Although the shot-based visual features are easy to be extracted, they are too general to be useful for semantic video classification, and thus the classification results are unreliable. Extracting the visual features by using video object regions may be able to capture the middle-level video semantics and thus provide more reliable classification results. However, automatic object extraction in general is a challenging problem because the homogeneous video regions in color, texture or motion do not correspond to the underlying semantic video objects directly.</p><p>Based on this understanding, we have proposed a new framework for video content representation, which is able to capture the middle-level semantics of video contents by using principal video shots. The principal video shots are defined as the video units by associating the video shots with the underlying concept-driven multimodal (visual, auditory and image-textual) salient objects. Thus the principal video shots are semantic-sensitive and have strong correlation with the semantic concepts for the relevant news video clips. In addition, the multi-modal features, which are extracted by using the principal video shots, can be used to discriminate different semantic concepts effectively.</p><p>The visual salient objects are not necessarily the semantic video objects but some concept-driven regions of interest that are effective to characterize the related semantic concepts. The auditory and image-textual salient objects are not necessarily the recognized speech and image-text but just some auditory and image-textual principal patterns that are related to the semantic concepts of interest. In addition, the salient objects can be extracted effectively by using low-level multi-modal features because they are relatively feature-invariant. Two examples for salient object detection are given in <ref type="figure" target="#fig_6">Figure 5</ref>.</p><p>To clarify this procedure, we generate the visual salient object "crowd regions" as an example to show how we can design our detection functions. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, our detection function consists of the following steps: (1) homogeneous image regions on color or texture are first obtained automatically; (2) homogeneous image regions are then classified into two classes that are relevant or irrelevant to the visual salient object "crowd regions"; (3) the visual salient object "crowd regions" is formed by combining all homogeneous regions classified to "crowd regions" in the same frame. The detected salient object is then tracked among frames within the same video shot to eliminate noise. A confidence map can then be generated for the detected salient object. The principal video shot is then defined as the video shot associated with the underlying salient object and its confidence map.</p><p>Even though the principal video shots may contain abundant semantics, they are not equivalent to the semantic concepts. For example, the principal video shot of "microphone" can be classified either "report" or "announcement". To interpret the contextual relationship between a specific semantic concept C j and the relevant principal video shots, the class distribution of the relevant principal video shots is approximated by using a finite mixture model with κ j mixture components:</p><formula xml:id="formula_13">P(X,C j , Θ c j ) = κ j ∑ i=1 ω i P(X,C j |θ i )<label>(12)</label></formula><p>In the above expression, P X,C j |θ i is the ith mixture component to interpret one relevant context class. Θ c j = κ j , θ c j , ω c j is the parameter tuple that includes the model structure, model parameters and weights, where κ j is the model structure (i.e., optimal number of mixture components), θ c j =</p><formula xml:id="formula_14">θ i = (µ i , σ i ) | i = 1, • • • , κ j</formula><p>is the model parameters (mean µ j and covariance σ j ) for κ j mixture components, ω c j = ω</p><formula xml:id="formula_15">i | i = 1, • • • , κ j</formula><p>is the relative weights among these κ j mixture components. Finally, X is the n-dimensional multi-modal features that are used for representing the relevant principal video shots. Maximum likelihood criterion can be used to determine the underlying model parameters in Eq. <ref type="bibr" target="#b12">(12)</ref>. The optimal parameter set of model structure, weights and model parametersΘ c j = κ j ,ω c j ,θ c j for the given concept C j is then determined by:</p><formula xml:id="formula_16">Θ c j = arg max Θ c j L C j , Θ c j<label>(13)</label></formula><p>where</p><formula xml:id="formula_17">L C j , Θ c j = ∑ X l ∈Ω c j log P X l ,C j , Θ c j</formula><p>is the objective function. To save cost for manually labeling the training samples, it is very important to integrate the unlabeled samples in the model training procedure. When the unlabeled samples are incorporated for classifier training, each unlabeled sample X u will be assigned a confidence score C j (X u ) ∈ [0, 1]. Thus, the unlabeled samples Ω c j = X u ,C j (X u ) |u = 1, • • • , N u are similar to the labeled samples and we can integrate them for classifier training by modifying the objective function:</p><formula xml:id="formula_18">L C j , Θ c j = ∑ X l ∈Ω c j log P X l ,C j , Θ c j +λ ∑ (Xu,Cj(Xu))∈Ωc j C j (X u ) log P X u ,C j , Θ c j<label>(14)</label></formula><p>where the discount factor λ ∈ [0, 1] is used to control the relative contribution of the unlabeled samples for semi-supervised classifier training. The EM algorithm <ref type="bibr" target="#b15">[15]</ref> is the traditional technique to optimize the model parameters toward Eq. <ref type="bibr" target="#b14">(14)</ref>. However, the traditional EM algorithm has two major problems: (1) it can't utilize the samples from other concepts to optimize the classifier; (2) it can't decide the optimal number of mixture component κ j . To resolve the two problems and integrate the unlabeled samples in the model training, the adaptive EM algorithm <ref type="bibr" target="#b6">[6]</ref> is used to train the optimal semantic concept models.</p><p>After the models for the semantic concepts of interest are formed, we can classify these principal video shots into the most relevant semantic concepts. Several most important concepts for visualization are implemented in our system. Shots of weather forecasting and sport are generally uninteresting. Thus they are detected and assigned smaller importance weights. Shots showing a person announcing something may be important. However, they will be weigh close to shots showing a reporter introducing the details, if we only use human face and text area objects. Both concepts are detected so that they can have different weights. It generally implies  important events that a lot of people gather together. The concept "gathered people" is detected so that shots related to this concept can be assigned a larger weights. The semantic classification accuracy of our system is in <ref type="table" target="#tab_0">Table 1</ref>. The semantic concepts contain two types of information that can be used for weight assignment: (1) the importance of the given semantic concepts; (2) the distribution of the semantic concepts in a given time interval of interest. The importance of the semantic concepts, w c (C (i)), is assigned as in <ref type="table" target="#tab_1">Table 2</ref>. Where C (i) is the semantic concept in the video shot i. The importance for the concept distribution can be determined by Eq. (6), w d (i) =w t (C (i)). Finally, the weight for the given semantic concept is determined by:</p><formula xml:id="formula_19">w concept (i) = w c (C (s)) × w d (i)<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic item extraction from audio and text</head><p>For news videos, the text documents for the closed captions match well with the news audio, and thus they can be integrated to take advantage of both media and remove the redundant information. The text documents for the closed captions may not synchronize with the video and generally have a delay of a few seconds. On the other hand, the audio generally synchronizes very well with the video but the accuracy of most existing techniques for speech recognition is still low. By integrating the results for speech recognition with the results of closed caption analysis, the closed captions can be synchronized with the video with high accuracy.</p><p>After the closed captions are synchronized to the relevant videos, we can determine the correlation between the closed captions and the video shots. To do this, the closed captions are first segmented to sentences, and the start time and the stop time for each text sentence can also be obtained automatically. All video shots that locate between the start time and the stop time for the same text sentence are associated with the corresponding text sentence. In addition, the text sentence is further segmented to keywords, all the video shots associated with the same text sentence are associated with all the keywords in the same text sentence.</p><p>In news videos, the news titles shown in video may provide very important keywords and thus they should be detected. Some special text sentences, such as "somebody, CNN, somewhere" and "ABC's somebody reports from somewhere", need to be processed separately. The names for news reporters in those text sentences are generally unattractive to the users. A context-free syntax parser is used to detect and mark this information.</p><p>All capital strings will fail most named entity detectors because initial capitalization is very important to achieve accurate named entity recognition. One way to resolve this problem is to train a detector with ground truth from closed caption text. However, it's very expensive to obtain the manually marked text material. Because the English has relatively strict grammar, it's possible to parse the sentence and recover most capital information by using part-of-speech and lemma information. We use TreeTagger <ref type="bibr" target="#b20">[20]</ref> to perform the part-of-speech tagging. Capital information will be recovered by TreeTagger automatically.</p><p>After special sentences are marked and capital information is recovered, LingPipe <ref type="bibr">[11]</ref> is used to perform the named entity detection and resolve cross reference. The model used is the news model of LingPipe. All parameters are set to default value.</p><p>Finally, LingPipe marked XML files are parsed to extract keywords and associated frequency information. Detected named entities are kept in their original format. Other text strings are segmented by non-alphabet characters and each segment is treated as a word. Because we will compute the weight by using frequency information with Eq. (6), we do not need to consider the stop words. The weight computation will automatically suppress the stop words.</p><p>The keyword weight of a shot is computed by Eq. <ref type="formula">16</ref>:</p><formula xml:id="formula_20">w keyword (i) = max x {w t (x)</formula><p>|x is a keyword of i} (16)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-modal data fusion</head><p>To enable more efficient visualization of large-scale news video collections, an overall weight is assigned with each video shot based on the weights described above. First, the w intra and the w inter are fused to compute the weight for the repeating video shot:</p><formula xml:id="formula_21">w repeat (i) = max {w intra (i) , w inter (i)}<label>(17)</label></formula><p>The w f aceArea and w textArea are fused to compute the object weight:</p><formula xml:id="formula_22">w ob ject (i) = max w f aceArea (i) , w textArea (i)<label>(18)</label></formula><p>The w f ace and w concept are both related to semantics of the corresponding video shot, thus they are integrated to determine the semantics weight:</p><formula xml:id="formula_23">w semantics (i) = max w f ace (i) , w concept (i)<label>(19)</label></formula><p>The reason to use max operation in above equations is that we want to detect the existence of interesting visual properties (e.g. the repeat pattern, the visual objects and the predefined semantics) of shots. The max operation assures the computed weights do not be suppressed a lot by the missing of a specific property.</p><p>The video importance weight for a given video shot is determined by the geometric average of above three weights:</p><formula xml:id="formula_24">w video (i) = 3 w repeat (i) × w ob ject (i) × w semantics (i)<label>(20)</label></formula><p>Finally, the overall weight for the given video shot is determined by averaging w video and w keyword :</p><formula xml:id="formula_25">w (i) = γ × w video (i) + (1 − γ) × w keyword (i)<label>(21)</label></formula><p>In our current experiments, we set γ = 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUALIZATION IMPLEMENTATION</head><p>After the importance weight for each video shot is computed, more efficient visualization of large-scale news video collections can be achieved by assigning different sizes for different news stories of interest. In addition, the video frame in the middle of each video shot is selected as the keyframe for the given video shot. There are several constraints for the layout and animation of the keyframes (i.e., news stories of interest). Firstly, the animation of the keyframes should be able to visualize the dynamic trend of news stories of interest (i.e., changing of news topics with time). Secondly, the sizes of the keyframes should be big enough so that the users can read the video content. In addition, the important keyframes (i.e., news stories of interest) are best visualized in the middle of the screen with larger size. Obviously, it is also very important to visualize as many keyframes (i.e., news stories of interest) as possible within a limited screen at the same time.</p><p>First, we introduce our algorithm for visualizing the map of the keyframes in a given time period. Because the aspect ratio of the keyframes is fixed, the treemap algorithm <ref type="bibr" target="#b14">[14]</ref> and its variants cannot effectively allocate the proper screen space for each keyframe. To address this problem, we have proposed a new visualization algorithm by using the column structure of screen layout. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a), 5 keyframes with largest weights are put in the middle of the screen and form a column. Then the other 14 keyframes are separated into two groups and form two columns at the left and right of the middle column. Finally, 22 other keyframes form the outermost two columns. This is a good layout that balances the constraints of overall screen size (standard desktop display in this case), the need to quickly apprehend individual keyframes, and the need to understand the flow of important stories within the context of other stories. If one had a significantly larger or smaller display, the layout could be adjusted accordingly.</p><p>To animate the keyframes (i.e., visualize changes of news topics) with time, the association of the keyframes with time should be extracted. Because news reports are rather condensed and tightly formatted, the video materials may not be repeated exactly, but similar video materials with some differences may used repeatedly over time. As we already perform sophisticated analysis on the video materials, we can just use the information obtained in the weight assignment procedure to extract the associations. Two video shots i 1 and i 2 are linked when one of the following three criteria is reached:</p><p>(1) Both of them contain the faces of the same person; (2) They are a repeating video shot pair, e.g. ϕ (i 1 , i 2 ) &gt; τ r ; (3) Both of them have at least 3 identical keywords.</p><p>Two keyframes maps are involved in the animation: the old map for the first time period and the new map for the following time period. The analysis of video shots separates the keyframes in the maps into two groups: keyframes without any link and others with links. For keyframes without links, the ones in the old map will gradually zoom to zero size and scroll up until they disappear from the screen; the ones in the new map will scroll up from the bottom until they reach their final positions. For keyframes with links, they will stay in the map. The resulting animation shows a flow of keyframes and some stationary "islands" in the flow. Thus dynamic topic trends are presented to the users, who can intuitively see which topics are growing or diminishing and which are retaining their importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORKS</head><p>By integrating several methods of statistical video analysis for knowledge discovery, our system is able to provide valuable information to users and enable significantly more effective and efficient visualization of news video from many broadcast channels. It can also help users find news stories of interest in a short time. These results demonstrate the power of integrating visualization with automated analysis techniques working together for a clear purpose. As a result, the complexities of the process are hidden from the user who receives meaningful results in an intuitive visual form.</p><p>In the future, visualization for single or several news stories, such as concept-oriented skimming, will be integrated into our system to help users examine specific news selected or retrieved via the visual interface. The system will also be scaled up to a much larger collection of broadcast channels and topics. To achieve this, new interactive visualization techniques that contain levels of abstraction will need to be derived.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The interaction between two relevant research areas users' attentions. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example of video news visualization. (a) Keyframes map for Japanese news on Nov. 12, 2005; (b) and (c) Intermediate animation; (d) Keyframes map for Japanese news on Nov. 13, 2005. The keyframes maps show the news topics on given day, the animation represents the trend of topic change over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Repeated shots detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Detected text lines (b) Confidence map of text (c) Detected faces (d) Confidence map of faces</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Text and face detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>intra (i) = e −</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Salient object examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Salient object detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Semantic Concept Classification Performance</figDesc><table><row><cell>Concept</cell><cell>Accuracy(%)</cell><cell cols="2">Concept Accuracy(%)</cell></row><row><cell>Announcement</cell><cell>75.35</cell><cell>Report</cell><cell>73.44</cell></row><row><cell>Sports</cell><cell>77.62</cell><cell>Weather</cell><cell>85.21</cell></row><row><cell>Gathered People</cell><cell>81.19</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Semantic Concept Importance</figDesc><table><row><cell>Concept</cell><cell>w c</cell><cell>Concept</cell><cell>w c</cell></row><row><cell>Announcement</cell><cell>0.9</cell><cell>Report</cell><cell>0.3</cell></row><row><cell>Sports</cell><cell>0.5</cell><cell>Weather</cell><cell>0.5</cell></row><row><cell>Gathered People</cell><cell>1</cell><cell cols="2">Unknown 0.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-10-31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic indexing of multimedia content using visual, audio and text cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><forename type="middle">Yung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chalapathy</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herriet</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Applied Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="170" to="185" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards automatic extraction of expressive elements from motion pictures: Tempo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitra</forename><surname>Dorai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic visual templates: linking visual features to semantics. IEEE Workshop on Content Based Video Search and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conjunction with IEEE ICIP &apos;98</title>
		<imprint>
			<date type="published" when="1998-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Open source computer vision library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Intel</forename><surname>Cop</surname></persName>
		</author>
		<ptr target="http://www.intel.com/technology/computing/opencv/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constant density displays using diversity sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Derthick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">G</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><forename type="middle">D</forename><surname>Wactlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InfoVis&apos;03</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning the semantics of images by using unlabeled samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangzai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuli</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Maps and cartograms of the 2004 us presidential election results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosma</forename><surname>Shalizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Newman</surname></persName>
		</author>
		<ptr target="http://www.cscs.umich.edu/~crshalizi/election/" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated highlevel movie segmentation for advanced video retrieval systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lagendijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biemond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on CSVT</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Harris</surname></persName>
		</author>
		<ptr target="http://tenbyten.org/10x10.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-faceted insight through interoperable visual information analysis paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InfoVis&apos;98</title>
		<meeting><address><addrLine>Research Triangle Park, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<ptr target="http://www.alias-i.com/lingpipe/" />
	</analytic>
	<monogr>
		<title level="j">Alias i Inc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Informedia-Ii</surname></persName>
		</author>
		<ptr target="http://www.informedia.cs.cmu.edu/dli2/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic text location in images and video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2055" to="2076" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tree maps: A space-filling approach to the visualization of hierarchical information structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 2nd International IEEE Visualization Conference</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="284" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The EM Algorithm and Extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thriyambakam</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Mod</surname></persName>
		</author>
		<ptr target="http://www.buzztracker.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A probabilistic framework for semantic video indexing, filtering, and retrival</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Milind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">News video analysis based on identical shot detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shin'ichi Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An efficient implementation and evaluation of robust face sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norio</forename><surname>Shin'ichi Satoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIAP&apos;99</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="266" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on New Methods in Language Processing</title>
		<meeting><address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image classification and querying using composite region template</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Image Understanding</title>
		<meeting>of Computer Vision and Image Understanding</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Determining computable scenes in films and their structures using audio-visual memory models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On news visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Wagstaff</surname></persName>
		</author>
		<ptr target="http://www.loosewireblog.com/2005/05/on_news_visuali.html" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Weskamp</surname></persName>
		</author>
		<ptr target="http://www.marumushi.com/apps/newsmap/index.cfm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic parsing and indexing of news video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang Yeo</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Smoliar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Hong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Systems</title>
		<imprint>
			<date type="published" when="1995-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="256" to="266" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
