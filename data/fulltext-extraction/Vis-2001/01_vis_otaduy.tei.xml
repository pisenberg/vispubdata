<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">User-Centric Viewpoint Computation for Haptic Exploration and Manipulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Otaduy</surname></persName>
							<email>otaduy@cs.unc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
							<email>lin¡@cs.unc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">User-Centric Viewpoint Computation for Haptic Exploration and Manipulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CR Categories: I</term>
					<term>3</term>
					<term>6 [Computer Graphics]: Methodology and Techniques -Interaction Techniques, Visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present several techniques for user-centric viewing of the virtual objects or datasets under haptic exploration and manipulation. Depending on the type of tasks performed by the user, our algorithms compute automatic placement of the user viewpoint to navigate through the scene, to display the near-optimal views, and to reposition the viewpoint for haptic visualization. This is accomplished by conjecturing the user&apos;s intent based on the user&apos;s actions, the object geometry, and intra-and inter-object occlusion relationships. These algorithms have been implemented and interfaced with both a 3-DOF and a 6-DOF PHANToM arms. We demonstrate their application on haptic exploration and visualization of a complex structure, as well as multiresolution modeling and 3D painting with a haptic interface.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Three-dimensional (3D) interaction has been explored in computer graphics, virtual reality (VR), user interface and scientific visualization. A number of techniques for 3D interaction have been developed, including object selection [PFC£ 97a], flying, grabbing and manipulating <ref type="bibr" target="#b24">[RH92]</ref>, worlds in miniature <ref type="bibr" target="#b19">[PBBW95]</ref>, combination of different modes of speech, gesture and gaze at the interface to allow real-time interaction with a graphics display, two-handed interaction <ref type="bibr" target="#b0">[ABF£ 97,</ref><ref type="bibr" target="#b3">CFH97]</ref>, and exploiting proprioception <ref type="bibr" target="#b16">[MBS97]</ref>. Among them, haptic visualization, as an augmentation to visual display, has the potential to further increase the understanding of complex datasets by enabling another modality of communication <ref type="bibr" target="#b1">[AS96,</ref><ref type="bibr" target="#b2">Bur96,</ref><ref type="bibr" target="#b4">Che99,</ref><ref type="bibr">DMW£ 98,</ref><ref type="bibr" target="#b9">Gib95,</ref><ref type="bibr" target="#b12">IN93,</ref><ref type="bibr" target="#b14">LLPN00,</ref><ref type="bibr" target="#b18">MPT99]</ref>.</p><p>3D manipulation of virtual objects or massive datasets can be simplified if the viewing of interaction is presented using multiple 2D views. This technique is commonly used in many commercial CAD/CAM and data visualization software systems. Typically the mouse cursor is free to move among the three or more views and constrained to act in only the two dimensions shown in each view. This requires the user to synthesize multiple 2D views to visualize the ma-nipulated objects or datasets. For most of the work in VR, 3D interaction techniques and haptic visualization, the user's viewpoint remains fixed at the same location, unless the user is head-or eye-tracked or has a dedicated input device (e.g. joystick or spaceball) to specifically indicate the direction of travel.</p><p>Viewpoint locations have a direct impact on the quality of the resulting graphical display accompanying the haptic visualization. However, to the best of our knowledge, the issues related to determining appropriate viewpoints in the graphical display that will be most suitable for haptic exploration and manipulation tasks have not been addressed. The disconnection between the graphical display and haptic visualization can result in poor or inconsistent presentation of information. For example, as the haptic probe moves between time steps while the viewpoint remains fixed, the probe may be occluded by other objects and not be visible from the viewpoint (as shown in <ref type="figure" target="#fig_0">Fig. 1)</ref>. A new camera position needs to be computed, in order to properly view the neighborhood under haptic exploration. . In order to properly view the probe and its nearby neighborhood, the camera must be repositioned at ¤ ¥ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Main Contribution</head><p>In this paper, we present algorithms for user-centric viewing of 3D virtual objects or datasets for haptic exploration and manipulation, by taking into account the user's intention in determining the viewpoint locations. In addition to using the force feedback device for haptic rendering, we also use it implicitly as a mechanism for the users to express their intent for the viewpoint location in graphical display, while simultaneously performing force display. Our algorithms generate automatic placement of the user viewpoint to navigate through the scene, to display the near-optimal views, and to reposition the viewpoint. This is accomplished by conjecturing the user's intent based on the user's actions, the object geometry, as well as inter-object and intra-object occlusion relationships. Our viewpoint computation techniques offer the following advantages:</p><p>Simple 3D interface to reset the view location for haptic visualization;</p><p>Proper viewing of the interesting events (e.g. interobject interaction) in the scene or explored salient features of the datasets at all time;</p><p>Automatic adjustment of the viewpoint locations without having the user to switch between haptic manipulation and camera repositioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Organization</head><p>The rest of the paper is organized as follows. Section 2 gives a brief survey of related work. Section 3 presents an overview of our approach. Our algorithm for view navigation of large, complex scenes for haptic exploration and visualization is explained in section 4. Section 5 discusses techniques to resolve the object-object occlusion problems and visibility issues in choosing near-optimal views. Section 6 presents a mechanism to automatically reposition the viewpoint for better viewing of manipulation results. Section 7 describes our prototype implementation and demonstrates their application on 6-DOF haptic interaction of complex structures, as well as haptic modeling and 3D painting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Camera control is a fundamental problem for 3D graphics applications. Several techniques on user interfaces for camera control have been proposed, including orbiting techniques mapping 2D motion into 3D interaction <ref type="bibr" target="#b6">[CMS88,</ref><ref type="bibr" target="#b20">PBG92,</ref><ref type="bibr" target="#b29">Wer94,</ref><ref type="bibr" target="#b32">ZF99]</ref>, use of image plane constraints <ref type="bibr" target="#b11">[GW92,</ref><ref type="bibr">PFC£ 97b]</ref>, and direct camera manipulation using a 6DOF input device <ref type="bibr" target="#b31">[WO90]</ref>. Our approach differs from many of the existing techniques on using 2D input devices to directly manipulate the viewpoint. It shares some similarity with <ref type="bibr" target="#b17">[MCR90]</ref> on moving the viewpoint toward a point of interest. However, our main focus here is to achieve automatic placement of viewpoint via implicit control based on user's manipulation of the haptic device. We also consider issues related to visibility and occlusion between objects.</p><p>Viewpoint computation is related to the problem of camera placement in computer vision, including sensor planning <ref type="bibr" target="#b28">[TAT95]</ref>, view planning <ref type="bibr" target="#b23">[Pit99]</ref>, and purposive viewpoint <ref type="bibr" target="#b13">[KD95]</ref>. These problems have different task requirements (e.g. placement of multiple sensors and cameras) and performance constraints (e.g. the execution can be carried out offline). This problem is also important to image-based rendering <ref type="bibr" target="#b5">[CL96,</ref><ref type="bibr" target="#b15">MB95]</ref>, where the viewpoint computation is crucial to model acquisition and scene modeling.</p><p>Our viewpoint computation problem can be considered as an inverse of the sensor planning problems in computer vision. Given the user's actions in exploring the complex environment or datasets, i.e. sequences of the input positions and orientations of the haptic device (robot arm used in reverse), we attempt to compute the near-optimal viewpoint for viewing the manipulated objects and/or features on the datasets. In contrast, camera placement for sensor planning takes a constraint based description of the vision task to be performed and synthesizes a "generalized viewpoint", which incorporates sensor location, orientation and lens parameters. Using the geometric description of the workspace, it is sometimes possible to construct occlusion-free visibility regions for viewing a particular feature of a given model. This region can then be used to find the best viewpoint for planning and accomplishing a certain vision task <ref type="bibr" target="#b27">[Tar91]</ref>.</p><p>Furthermore, the optimization process to compute the best viewpoint for performing certain tasks can often be highly non-linear and take at least several seconds upto a few minutes to perform the computation. In addition, it sometimes requires human assistance to provide good starting points. This is acceptable for one-shot off-line planning, but not suitable for interactive applications, such as haptic visualization and manipulation of complex datasets and objects at KHz rates.</p><p>The viewpoint computation problem we address in this paper is also somewhat related to the art gallery problem [O'R87] in computational geometry where the number and positions of watchmen in a 2D polygonal environment need to be determined. But, here we are concerned with a 3D environment and the placement of a single viewpoint. These requirements introduce a much higher combinatorial complexity than the classic art gallery problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Terminology</head><p>The position and directions are expressed using vector notation in bold face symbols. We use the quaternion algebra <ref type="bibr" target="#b25">[Sho87]</ref> to specify the relative orientation and transformation from one coordinate system to another.</p><p>The viewpoint (or the camera setting) is characterized by the following parameters: (a) e: the location of the viewpoint; (b) c: the location where the viewpoint is looking at; (c) u: the up vector that determines the pan and tilt angles along the viewing vector determined by e and c.</p><p>We use the term "probe object" for describing the object which the probe picks up, and "target object(s)" for referring to the object(s) that the probe object or the probe is interacting with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of Our Approach</head><p>Given the haptic inputs (i.e. positions &amp; orientations of the probe) from the user and the scene description, we wish to determine the viewpoint location that provides nearoptimal viewing of the "region of interest". The region of interest is changing dynamically and normally refers to the areas where the haptic manipulation is taking place or where the salient features of the datasets are under exploration. Optimality in this problem is measured in terms of object-object and self occlusion, given the user controlled focus distance and field-of-view of the camera.</p><p>We have designed several viewing algorithms that automatically compute the new viewpoint location based on the type of haptic tasks performed on the scene or the datasets. Our viewpoint computation techniques take into considera-tion the followings:</p><p>user's intention based on his/her manipulation of the haptic probe; the object geometry and the scene description;</p><p>intra-and inter-object occlusion; camera's field-of-view; camera's focus distance.</p><p>Given the haptic task, the viewpoint computation techniques we have developed include:</p><p>1. View navigation which provides the capability to recompute the viewpoint based on user's exploration of massive datasets (section 4);</p><p>2. Near-optimal views of the regions that the user is interacting with (section 5). This is applicable to all types of haptic exploration and manipulation;</p><p>3. Automatic repositioning of the viewpoint for any type of haptic interaction (section 6).</p><p>Next, we will describe each approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">View Navigation</head><p>In this section we present a view navigation technique for haptic exploration of massive datasets. Unlike traditional viewing systems for haptic visualization, the goal of this viewing functionality is to focus on the regions of potential interest in the virtual scene, as the user haptically explores different parts of the massive datasets in the environment.</p><p>In the typical viewing system for force display without head-tracking, as the user moves the haptic probe, the viewpoint normally stays fixed in the same location. The user often does not have the best viewing of the neighborhood where the haptic probe is exploring. We propose to adaptively change the viewpoint based on user's haptic exploration of the object(s) or datasets in a large, complex scene. The motion of the haptic probe provides excellent hints of the user's intentions. As the position and orientation of the haptic probe are changed based on user's gestures, viewing of the scene can be adapted to display the neighborhood that the virtual probe is interacting with, while preserving view coherence with the probe motion in the workspace.</p><p>The basic idea for our view navigation techniques is to apply appropriate transformations, taking into account the position of the haptic probe relative to a given reference point. Then, we attach the camera at the pre-defined offset distance (characterized by the camera parameters as explained in section 3) from the haptic probe to ensure proper viewing of the regions under haptic exploration. The position and orientation of the haptic device at each time instance are used as "velocity commands" for the virtual probe. The transformation between the local reference system and the global reference system is modified taking into account the velocity commands.</p><p>In a static situation, the motion of the virtual probe is confined to a virtual workspace. We use user's gestures to modify the position and orientation of that virtual workspace in the entire virtual world, thus allowing complete visual and haptic accessibility. Three invariants must be maintained in order to preserve coherence between the operations performed in the real workspace and the visual and haptic feedback:</p><p>1. The position and orientation of the virtual probe in the local reference system of the virtual workspace must mimic the position and orientation of the real haptic probe;</p><p>2. The camera must be anchored in the local reference system;</p><p>3. Force and torque must be displayed in local coordinates.</p><p>INVARIANT 1: aligning the local reference system with the real workspace Given the position, !</p><p>, and orientation, " # (expressed as a quaternion), of the haptic device in its workspace (see <ref type="figure" target="#fig_1">Figure 2</ref>), we compute the position, ! $</p><p>, and orientation, " % $ , of the virtual probe in the global coordinates of the virtual world. The first step is to align the position, ! &amp;</p><p>, and orientation,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>" &amp;</head><p>, of the virtual probe in its local reference frame with the state of the haptic device in the workspace. That is,</p><formula xml:id="formula_0">&amp; ¥ ( ' " &amp; ¥ " # )</formula><p>Next, the state of the haptic probe in the global reference system of the virtual world is computed. We denote the transformation from the local coordinates to the global coordinates with for the translation and " for the rotation. The conjugate of " is expressed as " # 0</p><formula xml:id="formula_1">. $ ¥ 2 1 3 " 5 4 6 ! &amp; 7 4 8 " 0' " $ ¥ " 5 4 8 " # &amp;)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computing the transformation between local and global reference systems</head><p>The velocity command is computed in terms of the difference between the current state of the haptic probe in its local reference frame (or the local state) and a virtual base state ( ! 9 and @ " % 9</p><p>). Emulating the operation modes of a joystick, we distinguish two possible modes that the user can select to manipulate the haptic probe and view the scene:</p><formula xml:id="formula_2">A C B E D G F ¤ F H B P I mode and Q S R P T U B W V X F ¤ H Y</formula><p>mode. In the position mode, the virtual base state is updated every frame and matches the current local state, so the velocity command is zero. 9 ¥ &amp;' " 9 ¥ " # &amp;)</p><p>In the velocity mode, the virtual base state remains static.</p><p>The sub-indices This implies that the base state "travels" along with the local state until we switch from the position mode to the velocity mode. At that moment the base will be anchored in the local reference system, and the difference between the local state and the virtual base state will set the velocity. This velocity is integrated at the current time step to yield an incremental transformation, p " and p q</p><p>. First, the rotation transformation is computed. Based on empirical observation, we conclude that the motion is more intuitive if the angular velocity is only allowed along the  In practice, the incremental transformation is only applied if the difference between the local state and the virtual base state is larger than a pre-defined threshold, and it is also clamped to a maximum value.</p><formula xml:id="formula_3">p " ¥ " q 4 8 " % ' " ¥ B ¤ u 'x S y b 4 u " # &amp; U à u " 9 ' " % ¥ B ¤ v 'x 4 v " &amp; ! a v " % 9 C ' " # e ¥ " # e g f ( h ¦ 4 p " ) where v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INVARIANT 2: setting the camera</head><p>After transforming the position and orientation of the virtual probe, we transform the settings of the camera and the output forces (and torque) so that coherence is maintained between the operations performed in the workspace of the device and their image in the local reference system of the virtual scenario. As described in section 3, the camera is characterized by means of its location, i , where it is looking at, j , and an up vector, k , to define the pan or tilt angle. Every frame they are defined with the initial settings (j</p><formula xml:id="formula_4">l , i S l , k l ): j ¥ 2 1 3 " m 4 8 j l 4 8 " 0' i ¥ 2 1 3 " m 4 8 i l 4 8 " 0' k ¥ " 5 4 8 k l 4 8 " 0)</formula><p>Given this mathematical formulation, the viewpoint is naturally controlled by the haptic probe to ensure proper viewing of the regions under haptic exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INVARIANT 3: transforming the forces</head><p>The forces and the torque are computed in the global coordinates, and they have to be displayed in local coordinates:</p><formula xml:id="formula_5">n &amp; ¥ " 0 4 n $ o 4 8 " ' p &amp; ¥ " 0 4 p $ 4 8 " )</formula><p>5 Near-Optimal Views</p><p>As the viewpoint moves with the haptic probe using view navigation described in section 4, it is possible that one object may occlude the view of another in the scene, or the visibility of the region of interest degrades due to self-occlusion. In this section, we describe a technique to compute nearoptimal views for haptic interaction and automatically provide the user with a proper view of the region of interest at every moment. This enables the user to manipulate target objects easily, without explicitly resetting the viewpoint location.</p><p>To present near-optimal views, we use two cameras to view the scene: the main camera and the detail camera. The main camera is typically positioned using the view navigation technique described in section 3; while the detail camera attempts to provide near-optimal views of the regions on the target object(s) that the probe is interacting with. An example is shown in Color Plate III and the corresponding views are shown in Color Plate IV. from the main camera (see <ref type="figure" target="#fig_4">Figure 3)</ref>. The first step in placing the detail camera is the computation of the view center j E s . Given an object touched by the haptic probe, we characterize the region of interest with a contact point, t , and a contact outward normal, u . We use penalty methods for the haptic rendering, and the contact normal is defined as the direction between the closest points in the virtual probe and the target object. The contact point is chosen to be the centroid of the contact area, when the probe is in contact with the object. When the haptic probe is free to move around, we set the contact point as the point on the target object that is closest to the probe. The contact point is selected as the view center. Then the translational vector from the main camera to the detail camera is expressed as:</p><p>! r ¥ t ¦ a j</p><p>Next, we need to compute the center of projection or the viewpoint, i s</p><p>. This is a problem with 3 degrees of freedom. A viewpoint location will be optimal if the contact region is visible from the user's view, at a user defined distance and at some offset angle from the normal direction. <ref type="figure" target="#fig_5">Figure 4</ref> shows schematically the location for the viewpoint of the near-optimal view in a scenario where the probe is a cylindrical object. In some scenarios this problem may not have an optimal solution, due to occlusion between the target object and the probe object (or the probe itself). In such cases, we may need to zoom in arbitrarily close to the contact point to minimize the amount of occlusion. Furthermore, an optimal solution may require global visibility information, such as an aspect graph. However, its computation has a runtime complexity of</p><formula xml:id="formula_6">v I ( w x</formula><p>, where I is the number of primitives (such as polygons). For most scenarios, it may not be possible at all to obtain the optimal solutions at interactive rates. Therefore, we do not attempt to compute a globally optimal view, but rather one that locally minimizes the amount of occlusion. We allow the user to interactively select the viewing distance y , thus adding one constraint to the problem. The camera can be positioned at any point on the surface of a sphere centered at the contact point with a radius of y . Assuming that our intention is to mainly visualize the target object, the location of the camera should be constrained to a cone around the contact normal. The angle of this cone is bounded to avoid tangential views of the contact location, which would not provide depth information (see <ref type="figure" target="#fig_6">Figure 5</ref>). The operation of placing the camera is decomposed into two steps: (a) an initial transformation where we place the camera along the contact normal, and (b) a local optimization that searches for the appropriate location. We call z " r the "initial rotation". In local coordinates, the main viewing direction is aligned with the z " % r ¥ " 5 4 8 " { 4 8 " 0)</p><p>The local optimization is based on the amount of occlusion from a successively refined camera position. The portions of the target and probe objects in front of the contact point are rendered separately from the current location of the detail camera. We compute amounts of overlap in two orthogonal directions in screen-space <ref type="bibr" target="#b33">[ZMHH97]</ref>. The camera is repositioned along those two directions following a bisection scheme, until no further reduction in the amount of occlusion is obtained. We are considering different approaches, which compute the amount of occlusion rendering the objects from the view center</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V } |</head><p>to reduce the number of rendering and readback operations. Eventually we obtain the rotation, " r , from the main camera to the detail camera. The region of interest can change suddenly, as well as the optimal placement of the camera to avoid occlusions. The translation and rotation to be applied to the camera are filtered to smooth these discontinuities. After motion smoothing, we can compute the settings for the detail camera by:</p><formula xml:id="formula_7">j s ¥ j 1 r # ' k h s ¥ " % r 4 8 k 4 8 " 0 r ' i s ¥ j s q 1 y © 4 8 " % r w 4 i a j i a j 4 8 " 0 r )</formula><p>where y is the distance between the contact point and the viewpoint.</p><p>Once the camera placement is computed, the scene is then rendered. Since occlusion might be impossible to avoid, we use transparency to make the neighborhood about the region of interest visible at every moment. First the target object is rendered, clipped with a view pyramid with its apex at the eye and its base at the contact point. This makes portions of the target object in front of the contact point visible within a "visibility window" to ensure the contact region is visible regardless of the occlusion. The size of the view pyramid can be set in terms of the size of the visibility window and the distance from the center of projection to the contact point. The next operation is to render the probe object and the rest of the target object blended. We finally render the rest of the objects blended as well, to avoid possible occlusions within the region of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Automatic Repositioning</head><p>In this section, we describe a viewing technique for automatically repositioning the viewpoint for any type of haptic tasks. During the haptic interaction, the region of interest or task space may become difficult to discern from the main view. And, the near-optimal view is intended as an aid for better viewing during haptic manipulation. However, due to the lack of coherence between the motion in the detail view and the motion in the workspace of the haptic device, the resulting graphical display may be misleading if the user tries to perform the operations based on the view projected on the detail camera. To solve these problems, we have designed a viewing functionality, where the main camera is progressively adapted to the location and orientation of the detail camera, allowing the user to automatically have a near-optimal and coherent vision of the task space (see <ref type="figure" target="#fig_8">Figure 6</ref>). In practice, this is carried out by composing the transformation from local to global coordinates with the transformation from the main camera to the detail camera. For the rotation, we use the initial rotation, z " % r , instead of the actual rotation, because it is independent of the position and orientation of the virtual probe.</p><p>Under the user's command to carry out automatic repositioning (e.g. pressing a button), the current transformation is stored as l and " l . The target is set to be the desired transformation, and linear interpolation is computed, updating at every step the weight of each term. In this section we describe the platforms and test scenarios where we have applied our algorithms, as well as the results that we have been to able to achieve. We used two different haptic devices: the 6-DOF PHANTOM Premium 1.5 and the 3-DOF PHANTOM Desktop device, both designed by SensAble Technologies, Inc.</p><p>We have integrated our algorithms with two applications: six degree-of-freedom haptic rendering of polygonal models [GME£ 00] and an interactive multiresolution modeling and 3D painting with haptic interface <ref type="bibr" target="#b8">[GEL00]</ref>. The latter was particularly valuable to test the performance of the automatic repositioning during a 3D painting or modeling operation, as opposed to the traditional technique of grabbing and repositioning the manipulated object. Using the framework for 6-DOF haptic rendering of arbitrary polygonal models we were able to test our view navigation algorithm and the nearoptimal view computation. The algorithm for view navigation was tested on the haptic exploration of a massive model, the Auxiliary Machine Room (AMR) of a submarine. In order to test the algorithm for near-optimal view computation, we used the model of a digestive tract, which has a notably irregular surface, and a CAD model, which shows clear examples of self-occlusion.</p><p>Please see the accompanied MPEGs that demonstrate the application of our algorithms on some test scenarios.</p><p>The video clips are also available at http://www.cs.unc.edu/ geom/HView.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">View Navigation</head><p>The AMR model consists of nearly half a million polygons and 3,000 parts. For the haptic exploration of the AMR we scaled the position of the haptic device in such a way that with the workspace of the haptic device we could only cover P E § § of the length of the AMR. This is a reasonable ratio for properly testing haptic exploration of the objects in the scenario using our view navigation algorithm. In Color Plate I we show a view of the haptic probe navigating along the AMR. In Color Plate II, we show the bird's eye view and the corresponding camera and probe positions for the situation in Color Plate I. View navigation is necessary if we want to be able to visualize all the objects in the scenario.</p><p>In this test scenario, the haptic device is efficiently used as an input device for tracking the intentions of the user, in addition to being a force feedback device. Both the haptic probe and the viewer follow the motion of the user's hand in a natural way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Near Optimal View</head><p>The model of the digestive tract (Color Plate III) is composed of approximately 53,000 triangles. Its surface is irregular at some places. This implies that the optimal viewing direction for perceiving surface details can change noticeably as the haptic probe traverses the surface.</p><p>Color Plate V shows the detail views when touching a point at the duodenum with different orientations of the probe. A change in the orientation of the probe moves the detail camera to a new location, in search of a lateral and occlusion-free view of the region of interest (i.e. the contact point).</p><p>There are situations when object-object or self occlusion cannot be avoided. Also, in the transition between two locations for the near-optimal views, occlusion may occur, such as in the situation where the haptic probe explores the inner side of the CAD model (See <ref type="figure">Figure 7)</ref>. The viewpoint location for the near-optimal views suddenly jumps trying to avoid self-occluding parts. Therefore, during the transition there is a lapse of time during which the contact point and the probe are occluded. In <ref type="figure">Figure 7</ref> we show how this problem is handled by rendering the objects with different levels of opacity. The region of interest becomes visible, yet we still have sufficient information about the part of the object that occludes the contact neighborhood. <ref type="figure">Figure 7</ref>: Rendering with different levels of opacity to avoid occlusion in the transition between two near-optimal views. Main view (left) and Detail view (right)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Automatic Repositioning</head><p>From the test scenarios using our interactive 3D multiresolution modeling and painting system, the users found that automatic repositioning is much easier to use than the tedious technique of grabbing the manipulated object and repositioning it whenever they want to modify parts of the object that are not visible.</p><p>During the manipulation task, the user can select a location on the target object at any time, make it the center of the workspace, and the viewpoint is automatically repositioned.</p><p>As an example, we show two different views of an object after having repositioned the main view to move a point in Color Plate VI and a point in Color Plate VII to the center of the workspace. As it can be inferred from the effect of the lighting, in both cases the normal of the object at those points has become the viewing direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary and Conclusions</head><p>We have presented several simple yet effective techniques for adaptively recomputing user-centric viewpoint locations based on the user's intentions, for haptic exploration and manipulation. These viewing techniques also take into consideration the object geometry, occlusion and visibility issues and camera parameters. They enable the users to better view the manipulated subjects during haptic interaction with complex structures or datasets, without head-or eye-tracking and additional 3D input devices. For future research, we plan to apply our techniques to higher dimensional volumetric datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>As the haptic probe moves from its location at occluded by the ellipsoidal object, viewing from the camera position at ¤ ¥ §</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>2D workspace (LEFT), global and local reference frames in the virtual world (RIGHT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the variables at the current time step and at the previous time step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Placement of the cameras for near-optimal view. W q e characterize the detail camera for projecting the nearoptimal views in terms of a rotation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Optimal view direction for scenarios with cylindrical probes and no self-occlusion of the target object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Location of the camera is constrainted to the surface of a sphere centered at the contact point with a radius y and to a cone around the contact normal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>s</head><label></label><figDesc>axis, and the up vector is aligned with the Y axis. The rotation from the main viewing direction to the normal direction of the contact can be found easily in local coordinates. This rotation " # { is computed as the composition of two rotations around the r and Y axes. The rotation is completed transforming back to the global coordinates:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Automatic repositioning of the viewpoint and the virtual probe.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research is supported in part by a fellowship of the Government of the Basque Country, NSF DMI-9900157, NSF IIS-9821067, ONR N00014-01-1-0067 and Intel. We would like to thank Stephen Ehmann for his help on integrating collision detection libraries, SWIFT and SWIFT++, with our 6-DOF haptic rendering algorithm. We are also grateful to Dinesh Manocha and the anonymous reviewers for their feedback on the earlier drafts of this paper, and Vincent Scheib for assisting with video editing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The two-user responsive workbench: Support for collaboration through independent views of a shared space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Beers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Fröhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Mcdowall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Bolas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="327" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A haptic interaction method for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Sobierajski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Visualization&apos;96</title>
		<meeting>Visualization&apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="197" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Force and Touch Feedback for Virtual Reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Burdea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Twohanded direct manipulation on the responsive workbench</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frolich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1997 Symposium on Interactive 3D Graphics</title>
		<meeting>of 1997 Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Six degree-of-freedom haptic system for desktop virtual prototyping applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Virtual Reality and Prototyping</title>
		<meeting>the First International Workshop on Virtual Reality and Prototyping</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 96 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A study in interactive 3-D rotation using 2-D control devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Joy</forename><surname>Michael Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>Mountford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sellen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;88 Proceedings)</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scirun haptic display for scientific visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Durbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Macias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hollerbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Phantom Users Group Meetings</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">in-Touch: Interactive multiresolution modeling and 3d painting with a haptic interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE VR Conference</title>
		<meeting>of IEEE VR Conference</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond volume rendering: Visualization, haptic exploration, an d physical modeling of element-based objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics workshop on Visualization in Scientific Computing</title>
		<meeting>Eurographics workshop on Visualization in Scientific Computing</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="10" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">6-dof haptic display of polygonal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mascarenhas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization Conference</title>
		<meeting>of IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Through-the-lens camera control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;92 Proceedings)</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Volume haptization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Noma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE VRAIS</title>
		<meeting>of IEEE VRAIS</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global surface reconstruction by purposive control of observer motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="147" to="177" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shock and vortex visualization using a combined visual/haptic interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Novoselov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization</title>
		<meeting>of IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="131" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Plenoptic modeling: An image-based rendering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95 Conference Proceedings</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Moving objects in space: Exploiting proprioception in virtual-environment interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><forename type="middle">P</forename><surname>Mine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><forename type="middle">H</forename><surname>Séquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 97 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rapid controlled movement through a virtual 3D workspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jock</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">G</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (SIGGRAPH &apos;90 Proceedings)</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="171" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Art Gallery Theorems and Algorithms. The International Series of Monographs on Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mcneely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puterbaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Troy ; J. O'rourke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGGRAPH</title>
		<meeting>of ACM SIGGRAPH<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
	<note>Six degree-of-freedom haptic rendering using voxel sampling</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Navigation and locomotion in virtual worlds via flight into Hand-Held miniatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Pausch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommy</forename><surname>Burnette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Brockway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Weiblen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 95 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="399" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic viewing control for 3d direct manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Granieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Symposium on Interactive 3D Graphics</title>
		<meeting>of ACM Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="71" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image plane interaction techniques in 3d immersive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeleznik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1997 Symposium on Interactive 3D Graphics</title>
		<meeting>of 1997 Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
	<note>PFC£ 97a</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image plane interaction techniques in 3d immersive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeleznik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Symposium on Interactive 3D Graphics</title>
		<meeting>of ACM Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
	<note>PFC£ 97b</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A solution to the next best view problem for automated surface acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1016" to="1030" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Implementation of flying, scaling, and grabbing in virtual worlds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Robinett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Holloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Interactive 3D Graphics)</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Quaternion calculus and fast animation, computer animation: 3-D motion specification and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Shoemake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tutorial</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="101" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sensor Planning and Modeling for Machine Vision Tasks. PhD thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Tarabanis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Columbia University ; Department of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey of sensor planning in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Tarabanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="86" to="104" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josie</forename><surname>Wernecke</surname></persName>
		</author>
		<title level="m">The Inventor Mentor</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Addison-Wesley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploration and virtual camera control in virtual three dimensional environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Symposium on Interactive 3D Graphics</title>
		<meeting>of ACM Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="175" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unicam 2d gestural camera controls for 3d environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Zeleznik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Symposium on Interactive 3D Graphics</title>
		<meeting>of ACM Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="169" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visibility culling using hierarchical occlusion maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGGRAPH</title>
		<meeting>of ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
