<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Shadow Removal from Front Projection Displays</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jaynes</surname></persName>
							<email>jaynes@metaverselab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Webb</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Matt</forename><surname>Steele</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Brent</forename><surname>Seales</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="laboratory">Metaverse Lab</orgName>
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Shadow Removal from Front Projection Displays</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large-scale display</term>
					<term>shadow removal</term>
					<term>immersive media</term>
					<term>calibration</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Front-projection display environments suffer from a fundamental problem: users and other objects in the environment can easily and inadvertently block projectors, creating shadows on the displayed image. We introduce a technique that detects and corrects transient shadows in a multi-projector display. Our approach is to minimize the difference between predicted (generated) and observed (camera) images by continuous modification of the projected image values for each display device. We are unaware of any other technique that directly addresses this problem. Furthermore, we speculate that the general predictive monitoring framework introduced here is capable of addressing more general radiometric consistency problems such as displaysurface inter-reflections and the changes in display color and intensity due to projector bulb temperature variation. Using an automatically-derived relative position of cameras and projectors in the display environment and a straightforward color correction scheme, the system renders an expected image for each camera location. Cameras observe the displayed image, which is compared with the expected image to detect shadowed regions. These regions are transformed to the appropriate projector frames, where corresponding pixel values are increased. In display regions where more than one projector contributes to the image, shadow regions are eliminated. We demonstrate an implementation of the technique to remove shadows in a multi-projector front projection system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The large-scale front-projection display, built to operate as a single logical display system from multiple projectors, has emerged in recent years as a compelling and feasible technology. Immersive teleconferencing applications <ref type="bibr" target="#b0">[1]</ref>, virtual reality environments <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, realistic simulation, and augmented reality <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> have all begun to explore the multi-projector paradigm. Researchers are systematically addressing the significant technical challenges associated with cooperative, projection-based rendering in somewhat unconstrained environments <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. These challenges include calibration, automatic and dynamic re-calibration, geometric registration, intensity blending and color correction, and synchronization of the rendering pipelines among individual components <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. Solutions to these challenges enable a loosely configured projection system to be rapidly arranged, calibrated and used as a coherent, scalable display to produce seamless, immersive imagery.</p><p>Despite advances, the goal of building a coherent and seamless display from a scalable number of individual projectors still faces significant obstacles. An important remaining challenge is related to the difficulty in achieving a radiometrically constant synthesis of the underlying projectors. Certain radiometric artifacts have already been addressed by pre-calibration techniques, such as color balancing <ref type="bibr" target="#b10">[11]</ref> intensity balancing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> and blending at overlap regions <ref type="bibr" target="#b8">[9]</ref>; others can be controlled through a careful construction of the environment, such as using special display materials with uniform reflectance properties <ref type="bibr" target="#b13">[14]</ref>, and physically aligning the projectors <ref type="bibr" target="#b2">[3]</ref>. But there are perceptually <ref type="figure">Figure 1</ref>: Front projection displays suffer from shadowing artifacts when users occlude projector frustums. This is particularly a problem for projector-based immersive environments that involve large numbers of projectors and encourage users to move within the display environment. significant radiometric effects that have not yet been addressed.</p><p>In front-projection systems, shadows are easily created and, though transient, are extremely distracting ( <ref type="figure">Fig. 1)</ref>. Shadows, regardless of position, provide a perceptual cue that removes the user from the visually immersive experience. While back-projection can be used to avoid shadows, it introduces other problems including space considerations, intensity and sharpness attenuation, and mechanical complexity. Constraining user movement to prevent shadows is not acceptable for interactive display environments that adaptively render a model based on the user's position. Requiring a user to move in order to avoid shadows forbids particular views of the model to be visualized.</p><p>Shadow correction takes place for display regions that are illuminated by at least two projectors and observed by at least one camera. Although several researchers have explored multi-projector environments for arbitrary configurations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, our constraints are reasonable and encountered in most common indoor architectural environments.</p><p>The technique requires that at least one camera is able to observe the screen surface at all times. This restricts the placement of the cameras in the display environment (for the experiments presented here a camera is mounted overhead to minimize the chance of occlusion by the user). We believe it is important that projectors can be placed arbitrarily, without regard for the potential for occlusion, so as to maximize the usefulness of the display environment (surface area coverage or resolution). Restricting the position of cameras, in order to support this type of reconfigurable immersive environment is appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CALIBRATION</head><p>Accurate calibration of the devices within the display environment is critical to both shadow detection and removal. Predicted images are constructed using the recovered position of the camera with respect to display projectors as well as a straightforward color transfer function. These predicted images can then be directly compared with observed images to detect unexpected artifacts. Detected shadows are warped from the camera frame to each contributing projector's frame to determine the appropriate projector pixels for adjustment.</p><p>Calibration is a two phase process that is performed prior to use of the display system. The relative geometry of both projectors and cameras is recovered. Next, the color differences between each projector and camera are estimated by iteratively projecting different color intensities and measuring each camera response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Geometric Calibration</head><p>Given a camera and projector pair, calibration determines the transform from pixels in the camera plane to their corresponding positions in the projectors' frame buffers. Given this transform, regions in shadow, observed in a camera, can then be correctly adjusted in the projected imagery. There have been a number of researchers who have used the controllable nature of a projector and camera pair to recover calibration information in a number of contexts <ref type="bibr" target="#b17">[18]</ref> and several different calibration techniques have been explicitly designed for front-projection display environments <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. In the interest of readability, we present one such calibration technique for the case in which the display surface is planar. The planar assumption is not a requirement, however, and other calibration techniques to derive a pointwise mapping between image and framebuffer pixels could be used <ref type="bibr" target="#b18">[19]</ref>. These approaches involve an extra rendering pass to implement the transform, however, and slow overall system performance.</p><p>If we assume that the devices observe a plane, the calibration problem becomes a matter of finding the collineation A such that:</p><formula xml:id="formula_0">i j p p A = Equation 2-1</formula><p>for all points p i in the camera and all p j in the projector. Because A is a planar projective transform (a collineation in P 2 ) it can be determined up to an unknown scale factor λ, by four pairs of matching points in general configuration <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Iteratively projecting a random point from the projector onto the display surface and observing that point in the camera generates matching points.</p><p>Each image point center p I is computed by fitting a 2D Gaussian to observed greyscale response whose variance is related to expected image noise. The resulting center point of the function is then stored with its matching projector pixel p j . Given at least four random pairs (for a set of degenerate cases see <ref type="bibr" target="#b20">[21]</ref>), we compute A up to an unknown scale factor λ. For the results shown in this paper, A is computed using 10 matching pairs. The accuracy of the recovered A can be measured as a pixel projection error on the projector's frame buffer for a number of matching points. Specifically, we make calibration error estimates by illuminating the scene with a known projector pixel p, observing its corresponding position in the camera, and then computing a (sub)pixel difference:</p><formula xml:id="formula_1">∑ − = N i p A p 2 ε Equation 2-2</formula><p>For the results contained in this paper, ε is measured by generating 50 points in the projector frame and calculating projection error in the camera as in Equation 2-2. To improve calibration accuracy, we employ a Monte Carlo technique that estimates A over many trials of randomly generated match points and measures ε for each trial. The recovered A that leads to the smallest ε is retained. Experimentation reveals that, for our situation, twenty trials are usually sufficient to recover accurate calibration. Mean re-projection error is reduced to subpixel accuracy, typically between 0.4 and 0.6 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Color Calibration</head><p>In addition to geometric calibration, the mapping between color values for each camera/projector pair must be estimated. Extensive research has been conducted to address problems associated with inter-projector color non-uniformity in multi-projector systems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>. For the technique presented here, only a rough estimate of the color transfer function between camera and projectors is required. We model this transfer function in order to generate a more accurate predicted image, so that shadow regions can be more robustly detected. We do not model the color differences between projectors in order to render a uniform image as is the goal in <ref type="bibr" target="#b10">[11]</ref>. Techniques that model the color differences between projectors to produce a more uniform color image can be directly incorporated with the framework presented here.</p><p>The true relationship between a projected color and the image captured by a camera pair is a complex multidimensional function including the projector's gamma curve, the camera's gain function, projection surface properties, and wavelength. For simplicity, the threecolor channels (Red, Green, Blue) are assumed to be independent, and are calibrated separately by approximating this complex function for each color channel.</p><p>A given camera C observes the display surface, while uniform color images of increasing intensity are iteratively projected from projector P. For each projected color image, the mean color intensity is computed over the corresponding observed image. This is computed for each color channel, holding the other two color values constant at zero. The mean value over ten trials is computed for each color channel. <ref type="figure" target="#fig_1">Figure 2</ref> shows the three transfer functions measured by the color calibration process for a single camera/projector pair. </p><formula xml:id="formula_2">+ + = − − ) ( 1 ) ( α Equation 2-3</formula><p>is fit to the measured values for each channel, where f C (x) is the color transfer function for color channel C. The four parameters are fit to the measured datapoints using the nonlinear optimization Levenberg-Marquardt technique <ref type="bibr" target="#b23">[24]</ref>. Initial estimates for the parameters are provided by the user, who is presented with the color response curves captured during the color calibration process. The mean squared fit error for all three color channels, for the results shown here, was 0.96 (on a 0-255 scale).</p><p>The resulting color transfer functions provide a straightforward way to predict how a color in projector space will appear in the camera image. By using this information, predicted color images can be more accurately compared with the observed images to detect shadow regions. <ref type="figure" target="#fig_7">Figure 6a</ref> shows a predicted image in a two-projector, one-camera display. <ref type="figure" target="#fig_7">Figure 6b</ref> shows the image as seen by the camera. Using the transfer functions fit to the data shown in <ref type="figure" target="#fig_1">Figure 2</ref>, a predicted image based on the relative position of the camera in the environment (geometric calibration) as well as the color mapping between the projector and camera (color calibration) is computed.</p><p>This image is shown in <ref type="figure" target="#fig_7">Figure 6c</ref>. Comparison of <ref type="figure" target="#fig_7">Figure 6c</ref> to <ref type="figure" target="#fig_7">Figure 6b</ref> versus <ref type="figure" target="#fig_7">Figure 6a</ref> to <ref type="figure" target="#fig_7">Figure 6b</ref> reveals that the color corrected predicted image is more accurate than the non-corrected image. Initial studies show that shadow detection is far more robust when comparing color-corrected imagery with observed imagery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SHADOW DETECTION, REMOVAL</head><p>Once the display environment has been calibrated, shadow correction should take place continuously and automatically.</p><p>Correction is a two-phase process. Initially, changes due to unexpected radiometric artifacts on the display surface are detected. Predicted imagery is constructed for a specific camera position and color Projected intensity values versus observed intensity for each color. Both axes range from 0 to 255. transfer function and compared to captured images. A filtering process produces a set of delta pixels that must be corrected in the next frame. Given a set of delta pixels, the system then determines corresponding projector pixels that are attenuated or intensified to bring the intensities of the observed image into agreement with the predicted imagery.</p><p>An alternative to this approach is a more straightforward, region based technique. Rather than perform a pixelwise compute a new region for which pixels should be either detection of the shadowed regions (represented in the delta image), a bounding box can be fit to detected shadowed regions. This bounding region can then be warped to the appropriate projector framebuffers to illuminated or darkened.</p><p>This approach has an advantage in that it is straightforward and easy to represent <ref type="bibr" target="#b24">[25]</ref>. In a multi-projector environment this more compressed representation can be efficiently transmitted to each rendering device in the display.</p><p>The pixelwise radiometric correction approach presented here has significant advantages. In contrast with region based shadow removal in which rectangular regions are either completely on (alpha=1) or off (alpha = 0), the pixel-wise approach operates on individual pixels and accommodates all intensity values by incrementally adjusting the alpha channel values. This more general framework enables future exploration of radiometric consistency problems such as ambient lighting, display inter-reflections, non-uniform inter-projector intensities, and subtle intensity variation due to non-uniform reflectance properties of the display surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Processing</head><p>It is important that cameras are able to detect radiometric changes on the display surface efficiently and accurately. We assume that the camera is able to observe the display surface without a significant chance of occlusion by the user. For example, the camera may be mounted overhead and oriented down onto a display wall. Ideally, each display camera should observe as large a screen area as possible.</p><p>An important aspect of our approach is that radiometric changes are detected directly in camera space. This removes the need for an explicit model of the occluding object.</p><p>In addition, image-based change detection removes the requirement for full Euclidean calibration of the camera and the position of the occluding object does not need to be known.</p><p>A predicted image must be made available to each camera so that it can be compared to the currently observed camera view. In situations where the image is fixed (exploration of a high-resolution still, for example), the predicted image for each display camera can be pre-computed prior to running the shadow removal algorithm. Predicted imagery may also be updated according to known mouse and keyboard input for scenarios where the system does not correct for underlying surface distortions. This approach is useful if the projected imagery is a simple desktop environment, and is easily extended to interactive display environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Camera View Prediction in a Dynamic Environment</head><p>In a dynamic display the imagery may change in an unpredictable way (user movement, simulations, video data). The predicted imagery must account for the changing display. This is accomplished by warping the rendered projector framebuffer to camera coordinates (through the recovered homography).</p><p>Our Given a desired image I, each projector in the display environment computes an appropriate framebuffer for projection in order to correctly contribute to the overall image I. This can be accomplished in a number of ways <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> and, for the results shown here, is accomplished in a manner that accounts for display surface geometry, relative projector positions, and the user's headtracked viewpoint in the display. For more details on how the individual projector images are constructed in the multi-projector environment used here, the reader is referred to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given the recovered collineation between the camera c and a projector p, c p</head><p>A a predicted image is recovered by warping all projector pixels into the camera frame. For a single projector, the predicted image is given by:</p><formula xml:id="formula_3">I I c p A = Equation 3-1</formula><p>Because the predicted image is the basis for subsequent modification of projector frame buffer pixels in the display environment, it is important that the image is as accurate as possible. In practice, we super-sample the predicted image by computing the position of each pixel corner on the projector plane to recover an interpolated subpixel estimate of the pixel value at its center. That is, for a pixel a=[i j 0] T on the camera plane, we compute where δa is the effective size of half a pixel in the camera. Each b is a vertex of a quadrilateral on the image plane of projector.</p><p>The correct pixel value I(i,j) for the predicted image I, is estimated as a weighted average of the pixels contained within the quadrilateral, weighed by the percentage of each pixel that is contained in the back-projected region:</p><formula xml:id="formula_4">∑ − = ⋅ = 1 0 ) ( 1 ) , ( N k k k p p N j i I λ Equation 3-3</formula><p>where p k is a projector pixel contained in the quadrilateral and λ(p k ) is a weighting factor equal to the total percentage of pixel p k contained in the quadrilateral. Finally, in the case where more than one projector contributes to a single camera pixel, the mean of all corresponding projector regions, computed using Equation 3-3, is stored in the predicted image.  <ref type="figure" target="#fig_3">Figure 3a</ref> shows the image captured by a camera hanging on a ceiling grid, observing a projection display. <ref type="figure" target="#fig_3">Figure  3b</ref> depicts the predicted image using the technique discussed in this section. In cases where the camera and projector were both mounted upside down (as was the case here), images have been flipped vertically for ease of understanding.</p><p>Note that the predicted image differs by the image actually captured by the camera due to sensor nonlinearities, and properties of the projection surface. The transfer functions discovered in the color calibration phase (see Equation 2-3) are applied to the predicted image I~to recover a color corrected, predicted image, Color corrected predicted images are compared to captured imagery by a subtraction of color components to derive two delta images, each representing pixels that are either too dark and should be intensified (the ∆ + I image) or pixels that are too bright and should be attenuated (∆ -I).</p><p>Each delta image, ∆I, is then filtered with a 3x3 median filter to remove spurious pixels that may emerge due to sensor noise. The size of the filter is directly related to the expected calibration error and expected image noise. Empirical studies have shown that, for the display environment used in this paper, a calibration error of less than a pixel and individual pixel variance can be eliminated with this simple filter. Finally, these delta pixels are converted to an alpha mask in each projector frame. <ref type="figure" target="#fig_5">Figure 4a</ref> depicts the ∆ + I image for the example shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Alpha Mask Generation</head><p>Delta images are directly related to the difference of the observed and predicted imagery for each camera in the display system. Therefore they are computed in the coordinate frame of the camera and must be warped to the reference frame of each projector for correction. This is accomplished using the recovered homography between the camera and projector.</p><p>In practice, camera devices and projectors are loosely coupled in the display environment through a communication protocol such as TCP/IP, and therefore recovered delta images must be transmitted as efficiently as possible to projectors for rendering.</p><p>In light of the need for transmission efficiency, a single delta image is constructed from the two images using a representation scheme that encodes the sign of a pixel value in the high order bit of a single byte. The remaining seven bits encode the difference (after the lambda gain has been applied) that should be integrated into the next display image. This image represents the alpha values that should be added to and subtracted from the current alpha mask to correct for observed differences. This delta image is then run-length encoded to improve transmission rates and reduce overhead bandwidth required by the radiometric correction algorithm. Due to the typical structure of a difference image (see <ref type="figure" target="#fig_5">Figure 4a)</ref>, the encoded image is typically reduced from approximately 500k bytes to less than 3k bytes. The encoded mask is multicast to all rendering client projectors that have an overlapping field of view with the camera that has detected the radiometric variation.</p><p>Each rendering client that receives a delta image, ∆I, first decodes the image and then warps the resulting alpha mask based on the relative position of the projector to the camera to recover the delta image in the projector coordinate frame, ∆I P .</p><p>Because the homography between the projector and camera has been recovered in the calibration phase, this warping is straightforward and is implemented as:</p><formula xml:id="formula_5">[ ] I I c p P ∆ = ∆ −1 A Equation 3-5</formula><p>Once a delta image has been aligned to a projector, an appropriate alpha mask is computed as follows:</p><formula xml:id="formula_6">[ ] ) , ( ) , ( ) , ( j i I j i I j i p p P − + ∆ − ∆ = ρ α Equation 3-6</formula><p>where ρ is the maximum allowed intensity change between any two frames and is used to avoid rapid fluctuations on the display surface. For the experiments presented here a ρ value of 25 was used. Although it may seem important to change the projected imagery as quickly as possible, sensor noise may lead to overcorrection for shadowed regions. Potentially, this can result in the system iteratively over-and under-correcting the shadowed region. Due to minor calibration error, this feedback may propagate the region of fluctuation on the display. The ρ parameter acts as a dampener to avoid this situation.</p><p>The alpha blending process takes into account whether incoming alpha values should be added or subtracted from the alpha channel currently being projected. <ref type="figure" target="#fig_5">Figure  4a</ref> shows the positive difference image, ∆I + . <ref type="figure" target="#fig_5">Figure 4b</ref> shows the resulting alpha mask, applied to a second projector for correction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENAL RESULTS</head><p>The radiometric correction algorithm has been integrated with a front-projection research display environment. The display is composed of four high-resolution projectors, two infrared head-tracking units and two digital video cameras. Each projector is connected to a single PC that contains a commodity, OpenGL-compliant graphics accelerator, and a standard network interface. The cameras are connected to a single PC that contains two frame-grabbers and a network interface. The PC rendering clients are connected via a 100Mb network hub. The display runs on the Linux operating system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Calibration Accuracy</head><p>The system was calibrated using the techniques described above and then used to demonstrate the shadow removal technique. In order to estimate the accuracy of our calibration approach, the mean error contained in the recovered homographies between each device and all other devices was estimated. This was accomplished by selecting 10 matching pairs for all devices with a view frustum overlap. Using these matching pairs, the mean pixel error for a particular device was computed by projecting known match points from other devices to the image plane and measuring a pixel disparity. Although we do not calibrate to a Euclidean coordinate system, metric errors on the screen can be estimated by backprojecting a line of known pixel length for each device and measuring the pixel sample distance on the screen for each device. The mean pixel error can then be multiplied by this scale to arrive at an estimate for calibration error in millimeters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shadow Removal Results</head><p>We have tested the system under a number of different conditions for a number of different applications. These include still objects placed in the path of a single projector, a person moving in front of the display, an object mounted to a controllable pan-tilt unit that is casting a shadow, and the casting of other light sources on the display (such as a flashlight) during its use. Due to space constraints, we only present the system being used by a single user who occludes a projector. <ref type="figure" target="#fig_8">Figure 7</ref> shows a user in a front-projection display environment. Clearly, both the subject's body and arm are casting on a shadow onto the image. The shadow precludes the user from interacting with the imagery in a meaningful manner.</p><p>Note also that an object (a stepladder) partially occludes the lowermost portion of the screen surface.</p><p>The system builds a predicted image and detects changes on the screen surface as they occur. Each incoming frame is then compared with the predicted image and differences are translated into modifications of the alpha mask that will correct the display. <ref type="figure" target="#fig_6">Figure 5a</ref> shows the ∆I + image for the occlusion event depicted in <ref type="figure" target="#fig_8">Figure 7</ref>.</p><p>The subtraction image is empty in this case and is not shown. <ref type="figure" target="#fig_6">Figure 5b</ref> shows the ∆I P image for a projector that is currently not occluded by the user. This image is then directly blended with that projector's current alpha mask to attenuate or intensify the image for radiometric correction. The newly blended alpha mask is rendered in the projector in concert with all other rendering devices on the next synchronized rendering pass. Typically, the system requires 3-4 frames to converge to a blended image. However, in an occlusion event such as the one depicted here, the algorithm will significantly brighten the shadowed region in the first frame after it is detected. <ref type="figure" target="#fig_10">Figure 8</ref> shows the result of blending based on the generated delta images from <ref type="figure" target="#fig_6">Figure 5</ref>. The information, previously lost in shadow, is now available to the user for inspection despite significant occlusion.</p><p>A close-up view of the corrected image is shown in <ref type="figure" target="#fig_9">Figure 9</ref>. Due to reprojection error as well as inter-frame latency between the detected position of the shadow and the update time, a small boundary between corrected shadows and blended image may exist as is seen in <ref type="figure" target="#fig_9">Figure 9</ref>. We have measured the border on the display surface by halting the system and inspecting the observable gaps. The widest point on the surface is approximately 11.5mm. Although a shadow boundary is visible and color differences between the different projectors make the shadowed region apparent, the information previously lost to the user is now present. Blending techniques, commonly used to remove the artifacts between adjacent projectors in multi-projector systems such as <ref type="bibr" target="#b8">[9]</ref> can be easily modified to smooth the observable shadow boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have shown that the framework introduced in this paper can be used to remove the appearance of shadows in front projection displays. Our technique does not depend on discovering a complex radiometric model of the scene, nor is it necessary to recover geometric or radiometric models of the objects or phenomena in the display area. Instead, the problem is solved in screen space using cameras, which observe the display and compare the observed image to an expected one. The primary constraint is that screen points are illuminated by more than one projector, and that the relative geometric relationships between cameras and projectors be known. Although the current implementation of the system is not at interactive rates. The algorithm can be implemented in real-time with appropriate rendering hardware. A similar algorithm, that does not require pixelwise updates has been implemented and runs at interactive speeds <ref type="bibr" target="#b24">[25]</ref>.</p><p>We are exploring the use of video cards that provide three alpha channels, one for each color band. This capability will allow correction for color band intensity with control over individual color bands. A second area of exploration is with algorithms for baseline color correction via observed color test patterns. We will also extend the framework to more general cases of radiometric inconsistencies such as dynamic ambient illumination, surface inter-reflection and nonuniformities due to projector and display surface variances.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Finally</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Measured transfer functions for each color channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Actual camera view versus predicted image. (a) Actual camera view.(b) Predicted image before color correction using calibrated camera information and the image interpolation technique discussed in this section. Obvious differences include the shadow on the screen surface. Other differences include sensor gain and color differences between the camera and projector (see Text).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>I</head><label></label><figDesc>&amp; , that can then be compared directly to the captured imagery. Each color component in the predicted image is adjusted according to:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Difference image and resulting alpha mask from the shadow event depicted in Figure 3. (a) Pixels detected in the ∆ + I image are the basis for brightening regions in display projectors. (b) Alpha mask after warping the difference image into the display projector's frame buffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Delta pixels used to correct for observed shadow changes. (a) Computed intensity differences based comparison of predicted versus observed image. (b) Alpha pixels warped to corresponding pixels in the projector's framebuffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of predicted image with and without color correction to the observed image. (a) Predicted image without color correction. (b) Image captured by camera while the display environment is in use. (c) Predicted image after three channel transfer functions have been applied. Predicted image is closer in color-space to the captured image than the predicted image without. Predicted imagery is the basis for detection of significant radiometric anomalies such as shadows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>User occludes a region of the desktop while using the large-scale visualization system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Close-up of the corrected image. Other than subjective analysis of the results, accuracy can be analyzed though direct measurement of remaining shadows on the display surface (see Text).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>View of the display surface with the radiometric uniformity algorithm running.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>reports these errors for all rendering clients in the display environment.</figDesc><table><row><cell>Rendering</cell><cell>Mean Pixel</cell><cell>Mean Screen</cell></row><row><cell>Client</cell><cell>Error</cell><cell>Error (mm)</cell></row><row><cell>Projector 1</cell><cell>0.583</cell><cell>1.23</cell></row><row><cell>Projector 2</cell><cell>0.603</cell><cell>1.61</cell></row><row><cell>Projector 3</cell><cell>0.616</cell><cell>1.64</cell></row><row><cell>Projector 4</cell><cell>0.664</cell><cell>1.72</cell></row><row><cell>Camera A</cell><cell>0.782</cell><cell>1.02</cell></row><row><cell>Camera B</cell><cell>0.793</cell><cell>1.18</cell></row><row><cell cols="3">Table 1: Calibration accuracy. Mean pixel error was measured</cell></row><row><cell cols="3">by reprojecting 10 known points in all other devices to</cell></row><row><cell cols="3">rendering clients through recovered homography and measuring</cell></row><row><cell cols="3">pixel disparity. Screen error was measured by back-projecting</cell></row><row><cell cols="3">error to display surface for each device independently.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward a Compelling Sensation of Telepresence: Demonstrating a portal to a distant (static) office. in 11th Ann</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference (Vis)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High Fidelity for Immersive Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schaufler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mazuryk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>in Short paper, ACM SIGCHI&apos;96 conference companion</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Surround-screen Projection-based Virtual Reality: The Design and Implementation of the CAVE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cruz-Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sandin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;93</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatially Augmented Reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Augmented Reality; Placing Artificial Objects in Real Scenes-Proceedings of the First IEEE Workshop on Augmented Reality (IWAR&apos;98)</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emancipated Pixels: Real-World Graphics In The Luminous Room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Underkoffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ullmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;99</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduciton to Building Projector-Based Tiled Display Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Judson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="22" to="28" />
			<date type="published" when="2000-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Load Balancing for Multi-Projector Rendering Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samanta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH/Eurographics Workshop on Graphics Hardware</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Distributed Graphics System for Large Tiled Displays. in IEEE Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seamless Projection Overlaps Using Image Warping and Intensity Blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on Virtual Systems and Multimedia</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Synchronization with Low Cost Networks in a Collaborative Rendering Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>University of Kentucky</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Achieving Color Uniformity Across Multi-Projector Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Ann. IEEE Visualization Conference (Vis)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scalable Self-Calibrating Display Technology for Seamless Large-Scale Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Surati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Developing Tiled Projection Display Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hereld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Mathamatics and Computer Science Division, Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="http://www.lcse.umn.edu/research/powerwall/powerwall.html" />
		<title level="m">Powerwall</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Image Generation for Multiprojector and Multisurface Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics Workshop</title>
		<meeting>the Eurographics Workshop<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Real-Time Depth Warping for 3-D Scene Reconstruction. in IEEE Aerospace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Seales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jaynes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>Boulder, CO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-Resolution Multiprojector Display Walls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schikore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="38" to="44" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">3D Computer Vision using Structured Light: Design, Calibration, and Implementation Issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Piero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="243" to="278" />
		</imprint>
	</monogr>
	<note>Advanced in Computers</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multiprojector Displays using Camera-based Registration. in IEEE Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic Alignment of High-Resolution Multi-Projector Displays Using an Un-Calibrated Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>TR--618-00. 2000</idno>
		<imprint/>
		<respStmt>
			<orgName>Dept. of Computer Science, Princeton University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Three-Dimensional Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>The MIT Press</publisher>
			<pubPlace>London, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanatini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Projective Geometry. CVGIP</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Radiometric Correction in the CoRE Display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jaynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seales</surname></persName>
		</author>
		<idno>TR313-01</idno>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>University of Kentucky, Dept. of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<title level="m">Numerical Recipes</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient Region-Based Shadow Correction in Multi-Projector Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jaynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of Kentucky, Dept. of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Oblique Projector Rendering on Planar Surfaces for a Tracked User</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>North Carolina</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill: Chapel Hill</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
