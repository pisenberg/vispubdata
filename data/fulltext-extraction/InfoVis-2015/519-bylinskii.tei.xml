<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Memorability: Visualization Recognition and Recall</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zoya</forename><surname>Bylinskii</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constance</forename><forename type="middle">May</forename><surname>Bainbridge</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><forename type="middle">S</forename><surname>Yeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Borkin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
						</author>
						<title level="a" type="main">Beyond Memorability: Visualization Recognition and Recall</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2015.2467732</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Senior Member</term>
					<term>IEEE</term>
					<term>and Aude Oliva Encoding Recognition Recall Encoding Recognition Recall Encoding Recognition Recall Encoding Recognition Recall Information visualization</term>
					<term>memorability</term>
					<term>recognition</term>
					<term>recall</term>
					<term>eye-tracking study</term>
				</keywords>
			</textClass>
			<abstract>
				<p>10 seconds / image 2 seconds / image 20 min-as many images as participant can complete OUTPUT: Eye-tracking fixation locations and durations. OUTPUT: Eye-tracking fixation locations and durations, and whether visualization is recognized. OUTPUT: Text descriptions of what participant recalls about the visualization. Correctly recognized blurred targets 100 &quot;target&quot; visualizations Same 100 targets + 100 &quot;fillers&quot; 393 visualizations Visualizations are taken from [8], and the label taxonomy described in Table 1 is applied. Fig. 1. Illustrative diagram of the experiment design. From left to right: the elements of the visualizations are labeled and categorized, eye-tracking fixations are gathered for 10 seconds of &quot;encoding&quot;, eye-tracking fixations are gathered while visualization recognizability is measured, and finally participants provide text descriptions of the visualizations based on blurred representations to gauge recall.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Understanding the perceptual and cognitive processing of a visualization is essential for effective data presentation as well as communication to the viewer. Memorability, a basic cognitive concept, has important implications for both the design of visualizations that will be remembered but also lays the groundwork for understanding higher cognitive functions such as comprehension. In our previous study <ref type="bibr" target="#b7">[8]</ref>, the memorability scores for hundreds of real-world visualizations were collected on Amazon's Mechanical Turk (AMT). The results of this research demonstrate that visualizations have inherent memorability, consistent across different groups of observers. We also found that the most memorable visualization types are those that are visually distinct (e.g., diagrams, tree and network diagrams, etc.), and that elements such as color, visual complexity, and recognizable objects increase a visualization's memorability. However a few questions remain: What visual elements do people actually pay attention to when examining a visualization? What are the differences in memorability when given more time to view a visualization? What information do people use to recognize a visualization? What exactly do people recall about a visualization?</p><p>To answer these questions, in this paper we present the results of a three-phase experiment (see experimental design in <ref type="figure">Fig. 1</ref>). In contrast to the previous study, where each visualization was presented for 1 second ("at-a-glance"), we presented each visualization for 10 seconds ("prolonged exposure") in the encoding phase of our experiment to allow participants more time to explore visualizations and to facilitate the collection of eye movements. During the recognition phase, participants viewed the target visualizations, mixed in with previouslyunseen (filler) visualizations for 2 seconds each and responded to indicate which visualizations they recognized. During both phases, we collected eye fixations to examine which elements a person focuses on when visually encoding and retrieving a visualization from memory. Finally, in the recall phase, participants were presented with the target visualizations correctly identified during the recognition phase in a randomized order and asked to "Describe the visualization in as much detail as possible." This last experiment phase provides insight into what visualization elements, types, and concepts were easily recalled from memory. Together, the three phases of the experiment, along with a new detailed labeling of the 393 target visualizations from <ref type="bibr" target="#b7">[8]</ref>, allow us to analyze and quantify what visualization elements people encode, retrieve, and recall from memory. Contributions: This work represents the first study incorporating eye-tracking as well as cognitive experimental techniques to investigate which elements of visualizations facilitate subsequent recognition and recall. In addition, we present an analysis of the new labeled visualizations in our database 1 in order to characterize visualization design characteristics, including data and message redundancy strategies, across different publication venues. Based on the results of our experiment, we are able to offer quantitative evidence in direct support of a number of existing conventional qualitative visualization design guidelines, including that: <ref type="bibr" target="#b0">(1)</ref> titles and supporting text should convey the message of a visualization, <ref type="bibr" target="#b1">(2)</ref> if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Perception and Memorability of Visualizations: Many important works in the visualization community have studied how different visualization types are perceived, and the effect of different data types and tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>. The effect of "visual embellishments" on the memorability and comprehensibility of visualizations is also an active area of research <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>. The effect and role of specific visual elements have also been investigated within the context of specific visualization types, e.g., attributes of node link diagrams <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, specific visual elements such as pictographs <ref type="bibr" target="#b17">[18]</ref>, visual distortions <ref type="bibr" target="#b36">[37]</ref>, and more broadly <ref type="bibr" target="#b18">[19]</ref>. It has been demonstrated that interactions with visualizations affect memory and recall (e.g., <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>). As described in Sec.1, we evaluated the memorability of visualizations using thousands of un-edited real-world visualizations from the web <ref type="bibr" target="#b7">[8]</ref>. We found that some visualization types are more memorable than others, and particular visual elements (e.g., human recognizable objects, color, etc.) seem to increase a visualization's memorability. In this study we build on all of this previous work and move beyond basic memorability by studying what visualization types and features contribute to visualization recognition and recall.</p><p>Visual Cognition and Memory: Work on image memorability has reported high consistency between participants and experimental settings with regards to which images are memorable and forgettable. Studies have shown that the memorability rank of images is stable over time <ref type="bibr" target="#b22">[23]</ref> and over experimental contexts <ref type="bibr" target="#b11">[12]</ref>. Memorability has been demonstrated to be an intrinsic property of scene images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>, faces <ref type="bibr" target="#b2">[3]</ref>, and graphs <ref type="bibr" target="#b7">[8]</ref>. Studies in visual cognition and memory have demonstrated high fidelity of memories over time and massive storage capacity in long-term memory <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47]</ref>. People remember objects that have defined meanings and use cases <ref type="bibr" target="#b9">[10]</ref>, people remember a lot of details from visual objects and scenes <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b46">47]</ref>, and people can easily distinguish between different exemplars of the same category <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>. Also, unique or distinct visual stimuli have been found to be easier to remember in massive memory studies <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. Brady et al. have shown that when an object is retrieved from memory, many details are retrieved along with it <ref type="bibr" target="#b9">[10]</ref>. In this paper we go beyond the memorability of visualizations <ref type="bibr" target="#b7">[8]</ref> and explore what details of visualizations are remembered along with it, and what elements of a visualization lead to better recognition and recall.</p><p>In the psychology literature, there is a dual process model of memory called process dissociation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b29">30]</ref>. The two memory processes include a fast, automatic "familiarity-driven" process, and a slower, conscious "recollection-driven" process. The former is thought to be the result of the perceptual system's quicker processing of stimuli <ref type="bibr" target="#b23">[24]</ref>. Thus, the experiments presented in this paper may be thought of as accessing two memory processes: single-response recognition and the longer, detail-retrieving recall. However, even with our 2-second recognition phase, the processes at play are not just perceptual, as participants have time to go back to some of the textual elements before making their response. We are thus likely capturing both memory processes during both experimental phases, and both play an important role in how and what people remember in visualizations.  <ref type="bibr" target="#b40">[41]</ref>. This pie chart from a government publication has 13 labeled elements including where the data, annotations, and title are located.</p><p>Eye-tracking Evaluations in Visualization: Eye-tracking evaluations can be an effective tool for understanding how a person views and visually explores a visualization <ref type="bibr" target="#b5">[6]</ref>. They have been used in the visualization community for evaluating specific visualization types such as graphs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>, tree diagrams <ref type="bibr" target="#b10">[11]</ref>, and parallel coordinates <ref type="bibr" target="#b41">[42]</ref>, for comparing multiple types of visualizations <ref type="bibr" target="#b16">[17]</ref>, and for evaluating cognitive processes in visualization interactions <ref type="bibr" target="#b24">[25]</ref>. There has also been research in the area of understanding different types of tasks and visual search strategies for visualizations through the analysis of eye-tracking fixation patterns as well as insights into cognitive processes <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. The work presented in this paper does not focus on specific tasks, nor on a specific type of visualization, but rather uses eye-tracking for fixation location and duration analysis on hundreds of labeled and categorized real-world visualizations from the web with dozens of study participants. Within the context of our experimental design, we are able to more deeply understand the specific cognitive processes involved in recognition and recall of visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA COLLECTION AND ANNOTATION</head><p>We used the database of visualizations from our previous memorability study <ref type="bibr" target="#b7">[8]</ref>. The database was generated by scraping multiple sources of real-world visualization publication venues online covering government reports, infographic blogs, news media, and scientific journals. The diversity and distribution of these visualizations represent a broad set of data visualizations "in the wild". For our present study, we used the same 393 target visualizations utilized in the previous study <ref type="bibr" target="#b7">[8]</ref> along with 393 visualizations selected from the remaining "single" (i.e., stand alone single-panel) visualizations in the collection as filler images (see Sec. 5.3).</p><p>To gain deeper insight into what elements of a visualization affect its recognition and recall, three experts manually labeled the polygons of each of the visual elements in the 393 target visualizations using the LabelMe system <ref type="bibr" target="#b40">[41]</ref>. All labels were reviewed for accuracy and consistency by three visualization experts. Examples of labeled visualizations are shown in the leftmost panel of <ref type="figure">Fig. 1</ref> and in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>The labeling taxonomy was based on the visualization taxonomy from <ref type="bibr" target="#b7">[8]</ref>. As described in <ref type="table">Table 1</ref>, the labels classify the visualization elements to be either data encoding, data-related components (e.g., axes, annotations, legends, etc.), textual elements (e.g., title, axis labels, paragraphs, etc.), human recognizable objects (HRO), or graphical elements with no data encoding function. Labels could overlap in that a single region could have a number of labels (e.g., an annotation on a graph has an annotation label and a graph label). Additionally, the title of each visualization was manually coded for further analysis.</p><p>We also documented whether each visualization exhibited one of two types of redundancies: data and message redundancy. A visualization exhibits data redundancy if the data being presented is visually encoded in more than one way. This can include the addition of quantitative values as labels (e.g., numbers on top of bars in a bar chart, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, or on sectors in a pie chart), or the use of channels <ref type="table">Table 1</ref>. The visualization labeling taxonomy used to annotate our target visualizations. The data subtypes taxonomy is taken from <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LABEL [OPTIONAL SUBTYPES] DESCRIPTION Annotation</head><p>[Arrow] Outline of any visual elements annotating the data. A specific subtype of "arrow" was included to denote whether the annotation was in the form of an arrow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Axis</head><p>[Time] Outline of where an axis is located including any tick marks and numeric values along the axis. A specific subtype of "time" was included to denote an axis involving time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Outline of where the full data plot area is located (e.g., the area between the x-axis and y-axis in a 2D plot  such as color, size, or opacity to represent a value already exhibited in a visualization such as the x-or y-axis values. In contrast, a visualization exhibits message redundancy if the main conclusion or message of the visualization is explicitly presented to the viewer in multiple ways: the addition of explanatory annotations, labels, text, and pictures. A visualization can exhibit both data and message redundancy (e.g., lower right panel of <ref type="figure" target="#fig_1">Fig. 3</ref>). Annotating redundancy for each target visualization allows us to evaluate whether redundancy enables more effective recognition and recall. Each of the two types of redundancies were recorded as present or absent, by three visualization experts. The visualizations were reviewed and discussed until unanimous consensus was found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSIS OF LABELED VISUALIZATIONS</head><p>The labeled visualizations enable us to gain insight into and study the distribution and type of visual elements employed across publication venues and visualization types. These insights also help us understand and put into context the observed trends and results of our study (Sec. 6). Examining the proportion of image area covered by the data label, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, we see that it is highest for the scientific journal visualizations. This is probably due to the publishing context of scientific journal figures in which the visualization occurs as part of a paper narrative. Additionally, authors commonly have enforced page limits and figure limit constraints, so maximizing information per unit area is a constraint.</p><p>Breaking down this measure of image area for data display by vi- sualization type, we see that diagrams, maps, and tables cover a larger percentage of the image area than other visualization types. These types of visualizations tend to have annotations and text labels incorporated into the data representation itself, thus requiring less of the image area around the data plot for additional explanations.</p><p>Another observation is the difference in the average total number of elements in a visualization across sources. Visualizations from government sources have on average 11.9 labeled elements per visualization, significantly fewer compared to other visualization sources (p &lt; 0.001,t(177) = 4.79 when compared 2 to the numerically closest visualization source, science). In contrast, visualizations from infographic sources have nearly twice as many elements (M = 38.7) as compared news media (M = 19.7, p &lt; 0.001, t(212) = 5.73) and scientific visualizations (M = 18.4, p &lt; 0.001, t(169) = 5.23). The additional elements in the infographic visualizations are mostly in the form of more text, objects, and graphical elements around the data.</p><p>Finally, there is a difference between publication venues in the percentage of the visualization's area covered by human recognizable objects (HRO). There are no such objects in the government visualizations, and the percentages are generally less for scientific journal visualizations (M = 14%) as compared to news media (M = 25%) and infographic (M = 29%) visualizations. The human recognizable objects are primarily in the form of company logos (McDonalds, Twitter, Apple, etc.), international flags commonly used in the news media visualizations, and pictograms or photographs of human representations and computer/technology depictions (see also the word cloud visualization in the Supplemental Material).</p><p>In addition to the labels, each visualization was examined to determine if it exhibited message redundancy, data redundancy, both, or neither. As shown in <ref type="figure">Fig</ref> izations with message redundancy, with the highest rates in the infographic and news media visualization. This is probably due to both venues prioritizing clear communication of the visualization message. In contrast, the scientific visualizations in our sample had the least message redundancy and no data redundancy. When examining redundancy across visualization types, the message redundancy rates are comparable across all visualization types but highest for circle (53% of circle visualizations contain message redundancy), table (51%), line (44%), and bar chart (39%) visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Set-up &amp; Participants</head><p>As discussed in Sec. 3, we labeled all the visual elements in the 393 target visualizations for which we have memorability scores <ref type="bibr" target="#b7">[8]</ref>. We also carefully selected 393 visualizations from the database of single visualizations <ref type="bibr" target="#b7">[8]</ref> to use as fillers during the recognition portion of the experiment. These visualizations match the exact distribution of visualization types and original sources as the target visualizations. All target and filler visualizations were resized, while preserving aspect ratios, so that their maximum dimension was 1000 pixels.</p><p>For the eye-tracking portions of our experiment, we used an SR Research EyeLink1000 desktop eye-tracking system with a chin-rest mount 22 inches from a 19 inch CRT monitor (resolution: 1280 x 1024 pixels). At the beginning of an experimental session, participants performed a randomized 9-point calibration and validation procedure. At regular intervals, a drift check was performed and, if necessary, recalibration took place. Optional breaks were offered to participants.</p><p>A total of 33 participants (17 females, 16 males) participated in the experiment. All of the participants were recruited from the local communities of Cambridge and Boston with an average age of 22.9 years (SD = 4.2). All participants had normal color vision. In a single experiment lasting 1 hour, a participant would see about 100 randomlyselected (out of a total of 393) target visualizations. A single session of the experiment lasted about 1 hour. Each participant was monetarily compensated $25 for their time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Encoding Phase</head><p>The first portion of the experiment, the encoding phase, lasted 20 minutes. As illustrated in <ref type="figure">Fig. 1</ref>, participants were each shown about 100 visualizations, randomly selected from the 393 labeled target visualizations. For this phase of the experiment, participants examined each visualization for 10 seconds while being eye tracked. Visualizations were separated by a 0.5 second fixation cross to clear their field of view. A 10 second duration proved to be of sufficient length for a participant to read the visualization's title, axes, annotations, etc. as well as explore the data encoding, and was short enough to avoid too much redundancy in refixations as well as explorative strategies. The mea-sures collected during this phase were the (x,y) fixation locations <ref type="bibr" target="#b2">3</ref> (in pixel coordinates on each visualization) and durations of each fixation (measured in ms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Recognition Phase</head><p>The recognition phase of the experiment, directly following the encoding phase, lasted 10 minutes. Participants were shown the same 100 visualizations they saw in the encoding phase as well as 100 filler visualizations. These 200 visualizations were presented in a random permutation for 2 seconds each with a 0.5 second fixation cross between visualizations. Participants pressed the spacebar to indicate recognition of a visualization from the previous experimental phase. A feedback message was presented (i.e., correct, incorrect) after each visualization. The 2 second duration time was chosen to be brief enough to quickly collect responses, but long enough to capture meaningful eye-tracking data. The measures collected during this phase were the (x,y) fixation locations and durations for each fixation (in ms), and the number of correctly-recognized visualizations (HITs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Recall Phase</head><p>The final portion of the experiment, the recall phase, lasted 20 minutes. Participants' gazes were not recorded. In this phase, all the visualizations the participant correctly recognized in the previous phase were presented in a randomized sequence. Each visualization was presented at 60% of its original size and blurred by a 40-pixel wide Gaussian filter to make the text unreadable. The purpose of blurring visualizations was to allow the visualization to be recognizable, but not contain enough visual detail to enable the extraction of any new information.</p><p>Next to each blurred visualization was an empty text box with the instruction: "Describe the visualization in as much detail as possible." The goal was to elicit from the participant as much information about the visualization as they could recall from memory. Participants were given 20 minutes in total to write as many descriptions as possible. There was no limit to how much time or text length was spent on each visualization, nor any requirement to complete a certain number of visualizations. The measures collected during this phase were the participant-generated text descriptions of what they could recall of a given visualization.</p><p>Displaying blurred visualizations at this phase of the experiment has its limitations, including the potential to bias participants to more easily recall visual elements. However, we chose to use this modality due to the following advantages: (1) the study can be performed at-scale, showing participants many visualizations and collecting text responses in a later phase, <ref type="bibr" target="#b1">(2)</ref> no assumptions are made about what participants can remember and at what level they extract the content, and thus free-form responses can be collected, and (3) participants can be queried on specific visualizations they have stored in long-term memory via visual hints (i.e., the blurred visualizations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Performance Metrics</head><p>We computed fixation measures by intersecting fixation locations with labeled visualization elements <ref type="table">(Table 1)</ref> to determine when fixations landed within each element. We analyze the total duration of a viewer's fixations landing on a given visual element throughout the entire viewing period. We also measure refixations -the total number of times a viewer returns to an element during the entire viewing period (including the first time the element is fixated). Consecutive fixations on the same element are not counted. Note that a single fixation can land on several elements at once (e.g., an annotation on a graph). In this case, we count the fixation as belonging to all of those elements. We collect all of a viewer's fixations during a particular viewing period (i.e., 10 seconds for encoding, 2 seconds for recognition), and we discard as noise fixations lasting less than 150 milliseconds. All fixation measures are averaged across viewers and different sets of visualizations, and compared using Bonferroni-corrected t-tests. Raw HR for "at-a-glance" study <ref type="bibr" target="#b7">[8]</ref> Fitted HR for prolonged exposure study <ref type="figure">Fig. 6</ref>. a) Plot comparing the raw HR (i.e., memorability) scores of target visualizations from the recognition phase of this experiment (after 10 seconds of encoding) to the raw HR scores of the same visualizations from the previous experiment <ref type="bibr" target="#b7">[8]</ref> (with 1 second of encoding). b) Summarizing the same data by box filtering the raw HR scores makes the main trends clearer. The most memorable "at-a-glance" visualizations are still the most memorable after prolonged exposure, likely due to visual associations. The increase in memorability across experiments for some visualizations can be explained by additional semantic associations (e.g., title, text, etc.) being invoked. c) The top and bottom ranked visualizations across description quality and memorability, for all four publication source categories. The y-axis represents recognition HR, and the x-axis represents average text description quality at recall. Description quality is also correlated with memorability (Sec. 6.1.4).</p><p>We also computed the recognition hit rate (HR) for each visualization. This is the fraction of participants who correctly recognized a visualization when they saw it during the recognition phase of the experiment. This value ranges from 0 to 1. See the Supplemental Material for a discussion of the measurement of memorability in this experiment compared to <ref type="bibr" target="#b7">[8]</ref>.</p><p>To quantify the qualitative aspects of the text descriptions collected during the recall phase of the experiment, three visualization experts went through all 2,449 descriptions. The description quality is a measure of how well a participant recalled the content and message of the visualization. Quality was rated from 0 to 3 where 0 was an incorrect or incoherent description, and 3 was a text description that touched upon the visualization topic, what data or information is presented in the visualization, the main message of the visualization, and one additional specific detail about the visualization. The quality ratings were assigned based on unanimous consensus. Each description was also reviewed for the visual components explicitly discussed or referenced from the visualization including the title and other textual elements. Finally, descriptions were flagged if they were not perfectly accurate (i.e., contained a factual error).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS AND DISCUSSION</head><p>In our previous study <ref type="bibr" target="#b7">[8]</ref>, we showed that people are highly consistent in which visualizations they recognize and which they forget. Visualizations were only shown for 1 second at a time, so we were able to measure how memorable visualizations are "at-a-glance". In this study we want to test not just recognizability, but we also want to discover which visualization elements people encode and are consecutively able to recall (top right quadrant of <ref type="figure">Fig. 6c</ref>). We also want to test what aspects of a visualization impact how well the main message of the visualization is understood. For this purpose we extended encoding time from 1 to 10 seconds and analyzed eye movements at encoding, responses at recognition, and textual descriptions at recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">"</head><p>At-a-glance" vs. "prolonged exposure" memorability 6.1.1 Does "at-a-glance" memorability generalize?</p><p>During the recognition phase of the current study, as discussed in Sec. 5, hit rates (HR) were generated for each visualization (i.e., what percentage of participants correctly identified the visualization as occurring during the encoding phase). For the current memorability study discussed in this paper (with 10 seconds of encoding), the mean HR is 79.70% (SD = 17.98%) as compared to a mean HR of 55.61%</p><p>(SD = 15.30%) from the previous study <ref type="bibr" target="#b7">[8]</ref> (with 1 second of encoding). We compare the memorability scores (HR) of both studies for all the target visualizations in <ref type="figure">Fig. 6a-b</ref>. When time to encode a visualization is increased from 1 to 10 seconds, it is natural for the absolute memorability scores to increase. However, what we are more interested in is the stability of the relative scores (i.e., ranks) of the visualizations across studies. We find a Spearman rank correlation of 0.62 (p &lt; 0.001) between the memorability scores of the two studies. Note that the inter-participant consistency in the initial study was not perfectly correlated either, with a rank correlation of 0.83 <ref type="bibr" target="#b7">[8]</ref>. Thus, the relatively high correlation between the two studies (despite the difference in experimental set-up, participant population, and encoding time) points to the stability of memorability ranks of the visualizations. We also see that this trend holds if we look separately within each source category. The Spearman rank correlations between the HR scores of the two studies are: 0.44 for infographics, 0.38 for news, 0.56 for government, and 0.59 for science (p &lt; 0.001 for all). Thus, visualizations that were memorable "at-a-glance" (after only 1 second of encoding) are often the same ones that are memorable after "prolonged exposure" (10 seconds of encoding). The same holds for the forgettable visualizations. Thus our initial study's findings generalize to a longer encoding time when people can process more of the visual and textual input. However, now that encoding time has increased, we also notice that 8% of the visualizations that were not previously in the top third most memorable visualizations move up in rank as compared to the previous study. In the next section we discuss why some visualizations become more memorable after 10 seconds of encoding than after 1 second, and why others remain forgettable in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">What are the differences between the most and least recognizable visualizations?</head><p>We use the eye movements of a participant at recognition time, just before a response is made, as indicators of which parts of the visualization trigger the response (i.e., help retrieve the memory of the visualization). We can compare the differences in eye movements for the visualizations that are "at-a-glance" the most and least recognizable <ref type="bibr" target="#b7">[8]</ref>. By considering the eye movements made on these visualizations during 10 seconds of encoding and at recognition time, we can see what parts of a visualization are encoded and what parts are required for recognition. As shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, the heat maps overlaid on the visualizations represent the average of all of the participants' encoding fixations on the visualization. The fixation patterns in the encoding phase demon- strate patterns of visual exploration (e.g., view graph, read title, look at legend, etc.), corresponding to the trends described in Sec. 6.2. These visual exploration patterns are seen in both the most and least recognizable visualizations. The heat maps generated for recognition fixations are obtained by taking only the fixations until a positive response is made, i.e., only the fixations that lead up to a successful recognition, so that we can determine which parts of a visualization participants look at to recognize a visualization. In <ref type="figure" target="#fig_5">Fig. 7</ref> we see a distinct difference between the fixation heat maps of the most and least recognizable visualizations in the recognition phase, where the most recognizable visualizations have a fixation bias towards the center of the visualization. This indicates that a fixation near the center, with the accompanying peripheral context, provides sufficient information to recognize the visualization without requiring further eye movements. In contrast, the least recognizable visualizations have fixation heat maps that look more like the fixation patterns of visual exploration in the encoding phase. These visualizations are not recognizable "at-a-glance" and participants visually search the visualization for an association, i.e., a component in the visualization that will help with recognition. The fixations along the top of the heat map for the least recognizable visualizations generally correspond to the location of the title and paragraph text describing the visualization in further detail. We conducted statistical tests to verify that the fixation patterns are significantly different between the most and least recognizable visualizations, and we describe these analyses in the Supplemental Material.</p><p>We see that the recognition fixations are significantly different between the most and least recognizable visualizations in that the least recognizable visualizations require more exploration before they can be retrieved from memory. There is more eye movement during recognition for the least recognizable visualizations indicating that people need to look at more parts of a visualization before they can recognize it. The visual appearance is no longer enough and people start exploring the text. In the following section we will discuss some possible explanations for this difference and what visualization elements people may be using for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Which visualization elements help recognition?</head><p>We have demonstrated that there is a distinct difference between the fixation patterns of the most and least recognizable visualizations. Which visual elements in the visualization contribute to this difference in fixation patterns, and which elements help explain the overall HR increase for recognition? We believe that people are utilizing two The visualizations that are not visually distinct must be recognizable due to semantic associations, such as the visualization's title or similar text. In fact, the elements with the highest total fixation time during recognition are the titles (M = 274ms) followed by human recognizable objects (M = 246ms, p &lt; 0.01). The titles serve as a semantic association, and the objects as a visual association for recognition. Note that in <ref type="figure" target="#fig_5">Fig. 7</ref> the fixations outside of the central focus in the least recognizable visualizations during recognition correspond to the visualizations' title and other textual elements near the top. Thus these types of semantic associations, i.e., textual components, are used for recognition if the visualization is not visually distinct or does not contain sufficient visual associations.</p><p>The least recognizable visualizations likely do not have a strong visual association nor a strong semantic association to assist with recognition <ref type="figure">(Fig. 6b)</ref>. The least recognizable visualizations after 10 seconds have significant overlap with the least memorable visualizations from <ref type="bibr" target="#b7">[8]</ref> (after 1 second). Also, of the 1/3 least recognizable visualizations 54% are from government sources (compare that to only 3% of government sources in the top 1/3 most recognizable visualizations), while of the 1/3 most recognizable visualizations, 49% are from infovis sources (and only 2% of the bottom 1/3 least recognizable visualizations are from infovis). Government visualizations tend to use the same templates and similar aesthetics, contributing to confusion between visualizations during recognition. The government visualizations are also heavily composed of the least memorable visualization types including bar and line graphs. The least memorable visualizations "at-a-glance" are also the least recognizable even after prolonged exposure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">What do people recall after 10 seconds of encoding?</head><p>Across 374 target visualizations <ref type="bibr" target="#b3">4</ref> , the study participants generated 2,733 text descriptions (see Supplemental Material for examples of participant-generated descriptions). The mean length of a descrip-  <ref type="figure">Fig. 8</ref>. The total number of mentions of specific visualization elements in the participant-generated recall text descriptions. Textual elements received the most mentions overall, and specially the title received the most mentions across all visualizations. <ref type="table">Table 3</ref>. Most frequently mentioned visualization elements by publication source (percent of time element was mentioned in the text description out of total number of times it was present). Titles, labels, and paragraphs are mentioned most often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank: 1st 2nd 3rd</head><p>Overall</p><formula xml:id="formula_0">Title (46%) Label (27%) Paragraph (24%) Infographics Title (72%) Label (28%) Paragraph (24%) News Paragraph (45%) Title (43%) Label (33%) Government Title (55%) Legend (26%) Data (21%) Science Label (27%) Axis (14%) Legend (13%)</formula><p>tion is 13.1 words (SD = 9.6), with an average of 7.3 descriptions (SD = 4.0) per visualization. <ref type="table">Table 2</ref> contains a breakdown of the description statistics per source category. We can see that infographics sources generated the most number of descriptions in total (877), with the largest average number of descriptions per visualization (M = 11.2, SD = 3.0) and the longest descriptions in terms of word count (M = 14.3, SD = 10.0). More importantly, they also correspond to the highest-quality descriptions (M = 2.09, SD = 0.79), statistically higher than the descriptions for the news media visualizations (M = 1.99, SD = 0.88,p &lt; 0.001). As discussed in Sec. 5, the description quality is a measure of how well participants recalled the content and message of the visualization. A higher description quality for infographics implies that participants recalled more details about the source that had the most recognizable visualizations. For reporting statistics related to description quality, we only consider visualizations with at least 3 participant-generated descriptions (so that computations of average description quality are more meaningful). The source with least recognizable visualizations, government, also had the fewest total descriptions (465), the fewest descriptions per visualization (M = 4.7, SD = 2.9), and the shortest descriptions (M = 10.8, SD = 8.4). The average description quality was 1.37 (SD = 0.93), similar to science visualizations. Thus participants were able to recall fewer things about the least recognizable visualizations. Overall, the Spearman rank correlation between "at-a-glance" HR <ref type="bibr" target="#b7">[8]</ref> and description quality (after prolonged exposure) is 0.34 (p &lt; 0.001), and within just infographics, the source with the highestquality descriptions, the correlation is 0.24 (p &lt; 0.01). The positive correlation is also present (though not significant) for the other 3 sources, possibly due to insufficient data (fewer visualizations and fewer descriptions per visualization than in the infographics category). <ref type="figure">Fig. 6c</ref> contains example visualizations categorized by how memorable they were and the quality of descriptions generated for them (i.e., whether what was memorable was the main content and message). Sample visualizations with descriptions, sorted by source, are included in the Supplemental Material.</p><p>At this point, although we cannot make any causality assumptions, the findings above demonstrate that visualization recognition is related to description quality. In other words, visualizations that were most recognizable after only 1 second of viewing also tend to be better described (after 10 second exposure). It might very well be that a third factor (e.g., relevance of topic to viewer) contributes both to recognizability and description quality. However, even in that case, knowing something about recognizability can tell us something about description quality. Thus, visualizations that were memorable for their visual content after 1 second of viewing are memorable after 10 seconds of viewing, and more importantly, their semantic content (the message of the visualizations) is correctly recalled by experimental participants (hence the higher description quality).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The effects of visualization elements on memorability and comprehension</head><p>In this section we investigate which visualization elements attract attention during encoding through the analysis of eye movements, and which elements are recalled when a visualization is described from memory through the analysis of textual descriptions. We use the eye movements on a visualization as a proxy for what information is being encoded into memory. From the content of the recall text descriptions, we can infer from which elements of a visualization a participant extracted information. As discussed in Sec. 5, the description quality score corresponds to the extent to which a participant recalled details and the main message of the visualization. These two metrics (eye movements and textual descriptions) together are evidence for which elements of a visualization contribute to a participant's memory and understanding of a visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Titles</head><p>When participants were shown visualizations with titles during encoding, the titles were fixated 59% of the time. Correspondingly, during the recall phase of the experiment, titles were the element most likely to be described if present (see <ref type="figure">Fig. 8</ref>). When presented visualizations with titles, 46% of the time the participants described the contents, or rewording, of the title. For the infographics sources, titles were fixated and described most often (80% and 72% of the time, respectively). A full breakdown by source category of elements described most often is presented in <ref type="table">Table 3</ref>.</p><p>What is the contribution of visualization titles to the quality of the textual descriptions? Across the 330 visualizations with at least 3 descriptions, the average description quality of visualizations with titles is 1.90 as compared to 1.30 for visualizations without titles (the difference in quality is statistically significant at p &lt; 0.001). This trend is partially driven by the absence of titles for the scientific visualizations. We believe this might be one explanatory factor as to why the scientific visualizations have the lowest quality descriptions.</p><p>Titles were also more likely to be fixated (76% of the time) when present at the top of the visualization than when present at the bottom (fixated 63% of the time). This trend holds across the source categories. Note that government visualizations, the least memorable category with poorer descriptions, contained the most occurrences of titles at the bottom of the visualization (73% of all visualizations that had a title at the bottom are from a government source). Interestingly, for government visualizations, table header rows were fixated more frequently and longer than titles (see Supplemental Material). For government sources, titles that were found at the top of the visualization were fixated 85% of the time and described 59% of the time, compared to titles that were found at the bottom of the visualization, which were fixated only 62% of the time and described 46% of the time (see <ref type="table">Table  2</ref> in the Supplemental Material for the complete breakdown). Thus, titles are more likely to be paid attention to and later recalled when present at the top of a visualization.</p><p>Across all textual elements, the title is among the most important. A good or a bad title can sometimes make all of the difference between a visualization that is recalled correctly from one that is not. We <ref type="table">Table 2</ref>. Summary of statistics related to visualization descriptions by source (** = p &lt; 0.001). Mean description quality is computed only on visualizations with at least 3 participant-generated descriptions. Infographics had the most, longest, and highest-quality descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Total # of vis. observed that when the title included the main message of the visualization (e.g., "66% of Americans feel Romney Performed Better Than Obama in Debates"), more participant-generated descriptions recalled the visualization's message compared to instances when the title was more generic or less clear (e.g., "Cities"). Select participant-generated descriptions for exemplar visualizations with good and bad titles are presented in the Supplemental Material. Note that prior psychology studies have demonstrated that concrete, imageable words (those that can be easily imagined or visualized) are easier to remember than abstract ones <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b32">33]</ref>. Recent work by Mahowald and colleagues <ref type="bibr" target="#b32">[33]</ref> has begun investigating the memorability of individual words, opening up future extensions to designing more memorable titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Pictograms</head><p>Pictograms, i.e., human recognizable objects (HRO), did not seem to distract participants during encoding. In fact, averaged over all visualization elements, the total fixation time spent on pictograms was less than all the other visualization elements (see Supplemental Material).</p><p>Infographics is the only source where pictograms were refixated the most and fixated for the longest time in total, but these are also the visualizations with the highest recognizability and quality descriptions. Overall (as in <ref type="table">Table 4</ref>), the average description quality for visualizations with pictograms (2.01) is statistically higher than for visualizations without pictograms (1.50, p &lt; 0.001). Within each source category, the mean description quality for visualizations with pictograms is always higher than for visualizations without pictograms. This is not always a significant difference, but what this demonstrates is that even if pictograms do not always help, they do not seem to hinder description quality. In fact, if we consider the top third best described visualizations, 20% of them have pictograms, compared to only 2% among the visualizations in the bottom third (Supplemental Material). Across all source categories, the visualizations with the best descriptions are more likely to contain pictograms than the visualizations with the lowest-quality descriptions. Thus, what participants recall about visualizations is not hindered by pictograms. In other words, with pictograms the message of the visualization is correctly recalled (and in fact, often better). Participants may use pictograms in these cases as visual associations (see Sec. 6.1.3). Pictograms might also provide a form of message redundancy (see Sec. 6.3).</p><p>Note, however, that some pictograms can hurt the understanding of a visualization. Any time people spend on a pictogram is time not spent on the data or the message (unless the pictogram contains part of the data or message), and if the link between the data/message and the pictogram is not clear, people may misunderstand and/or misrecall what the visualization was about. In the Supplemental Material, we qualitatively review the lowest quality text descriptions and see that pictograms that do not relate to the visualization's data or message can produce confusion and misdirect attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Other Elements</head><p>Across all visualizations, the elements that were refixated the most were the legend, table header row (often acting as a title), and title. <ref type="table">Table 4</ref>. Across all sources, visualizations with pictograms have similar or better quality descriptions than visualizations without pictograms (** = p &lt; 0.001 when comparing visualizations with and without pictograms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Mean description quality (0-3 scale) The elements that were fixated the longest were the paragraph, legend, and header row (a full breakdown by source is provided in the Supplemental Material.) We find that the elements that most commonly contribute to the textual descriptions, across all visualizations, include title, label, and paragraph (see <ref type="table">Table 3</ref>). For news media sources, the paragraph is used more often than the title and label in the recall descriptions. This may be due to the commonly short, and not necessarily informative, titles provided. For example, a viewer will gain more from the explanatory paragraph "Numbers of companies in the cotton market that 'defaulted' by ignoring arbitration decisions" instead of its title "Broken Promises". Recall that the science visualizations we have do not contain titles, a common convention in scientific journals, and so participants refer instead to the labels, axis, and legend in their descriptions (since those are often the only elements that contain any explanatory content). Thus, when the title is not sufficient, people turn to the other text in a visualization, including the explanatory paragraph and annotations (labels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">The Importance of Redundancy</head><p>The average description quality is higher for visualizations that contain message redundancy (M = 1.99) than for visualizations that do not (M = 1.59, p &lt; 0.001). Similarly, visualizations that contain data redundancy also have better quality descriptions (M = 2.01) than those that do not (M = 1.70, p &lt; 0.001). We can also see this trend by examining redundancy in the visualizations of the top third and bottom third of average description quality rankings. Of the visualizations in the top third of description quality ranks, 57% contain message redundancy and 34% contain data redundancy. In contrast, of the visualizations in the bottom third of description quality ranks, only 22% of visualizations contain message redundancy and 12% contain data redundancy. These trends hold within each of the source categories (see <ref type="table">Table 5</ref>) in that the mean description quality tends to be better for visualizations with message/data redundancy than without, and similarly, the visualizations with the best descriptions (the top third) are more likely to contain both forms of redundancy. Overall, if we compare the source categories to each other (see <ref type="table">Table 5</ref>), infographic visualizations contain the most message redundancy, followed by news media, government, and science. The exact same trend holds for data redundancy. Thus, although no causal <ref type="table">Table 5</ref>. The effect of redundancy on the description quality of visualizations: across all sources, the presence of message or data redundancy is linked to higher description quality on average (* = p &lt; 0.01 when comparing visualizations with and without message or data redundancy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Mean description quality (0-3 scale) % with msg redundancy % with data redundancy conclusions can be made at this point, we can see that redundancy is related to how well a visualization is recognized and recalled. The importance of data and message redundancy has also been observed for animated and video visualizations <ref type="bibr" target="#b1">[2]</ref>. Data and message redundancy in visualizations help people grasp on to the main trends and messages, and improves visualization recognition and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Based on the results presented in the preceding sections, we summarize below the key observations from our study. In addition to our experimental results shedding light on the fundamental theory of how visualizations are encoded, recognized, and recalled from memory, our results also provide direct quantitative empirical evidence in support of many conventional established qualitative visualization design and presentation guidelines. The conclusions listed below, with related established design principles where appropriate, relate to having a good and clear presentation, making effective use of text and annotations, drawing a viewer's attention to the important details, providing effective visual hooks for recall, and guiding the viewer through a visualization using effective composition and visual narrative.</p><p>Visualizations that are memorable "at-a-glance" have memorable content. Visualizations that are most memorable "at-a-glance" are those that can be quickly retrieved from memory (i.e., require less eye movements to recognize the visualization). Importantly, when these visualizations are retrieved from memory, many details of the visualization are retrieved as well. Thus, participant-generated descriptions tend to be higher quality for these visualizations (Sec. 6.1).</p><p>Titles and text are key elements in a visualization and help recall the message. Titles and text attract people's attention, are dwelled upon during encoding, and correspondingly contribute to recognition and recall. People spend the most amount of time looking at the text in a visualization, and more specifically, the title. If a title is not present, or is in an unexpected location (i.e., not at the top of the visualization), other textual elements receive attention. As exhibited by these results, the content of a title has a significant impact on what a person will take away from, and later recall, about a visualization (Sec. 6.2).</p><p>"Words on graphics are data-ink. It is nearly always helpful to write little messages on the plotting field to explain the data, to label outliers and interesting data points." (Edward Tufte <ref type="bibr" target="#b43">[44]</ref>)</p><p>Pictograms do not hinder the memory or understanding of a visualization. Visualizations that contain pictograms tend to be better recognized and described. Pictograms can often serve as visual hooks into memory, allowing a visualization to be retrieved from memory more effectively. If designed well, pictograms can help convey the message of the visualization, as an alternative, and addition to text (Sec. 6.2).</p><p>"The same ink should often serve more than one graphical purpose. A graphical element may carry data information and also perform a design function usually left to non-data-ink. Or it might show several different pieces of data. Such multi-functioning graphical elements, if designed with care and subtlety, can effectively display complex, multivariate data." (Edward Tufte <ref type="bibr" target="#b43">[44]</ref>)</p><p>Redundancy helps with visualization recall and understanding.</p><p>When redundancy is present, to communicate quantitative values (data redundancy) or the main trends or concepts of a visualization (message redundancy), the data is presented more clearly as measured through better-quality descriptions and a better understanding of the message of the visualization at recall (Sec. 6.3).</p><p>"Redundancy, upon occasion, has its uses; giving a context and order to complexity, facilitating comparisons over various parts of the data, perhaps creating an aesthetic balance." (Edward Tufte <ref type="bibr" target="#b43">[44]</ref>)</p><p>"Telling things once is often not enough: redundancy helps restore messages damaged by noise." (Jean-Luc Doumont <ref type="bibr" target="#b13">[14]</ref>)</p><p>All of these findings come down to the following well-phrased communication advice:</p><p>"Effective communication is getting messages across. Thus it implies someone else: it is about an audience, and it suggests that we get this audience to understand something. To ensure that they understand it, we must first get them to pay attention. In turn, getting them to understand is usually nothing but a means to an end: we may want them to remember the material communicated, be convinced of it, or ultimately, act or at least be able to act on the basis of it." (Jean-Luc Doumont <ref type="bibr" target="#b13">[14]</ref>)</p><p>The previous paper on visualization memorability <ref type="bibr" target="#b7">[8]</ref> presented findings on which visualizations are most memorable and which are most forgettable, when participants are only given 1 second to encode each visualization. We were able to show that this "at-a-glance" memorability is very consistent across participants, and thus likely a property of the visualizations themselves. In this paper, we extended encoding to give participants enough time to process the content (textual components, messages, and trends) of a visualization. We measured memorability and analyzed the information that participants were able to recall about the visualizations. Participants remembered and forgot the same visualizations whether given 1 or 10 seconds for encoding, and more importantly, participants better recalled the main message/content of the visualizations that were more memorable "at-a-glance". Thus, to get people to understand a visualization, "we must first get them to pay attention" -memorability is one way to get there. Then, with care, one can design and introduce as necessary text, pictograms, and redundancy into the visualization so as to get the messages across.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Example labeled visualization in the LabelMe system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustrative examples of data redundancy (i.e., additional quantitative encodings of the data) and message redundancy (i.e., additional qualitative encodings of the main trend or message of the data). More examples are provided in the Supplemental Material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Percentage of visualization average pixel area covered by the data label. Scientific visualizations on average had the highest percentage of image area devoted to data presentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>. 5, all publication venues included visual-The percent of visualizations that exhibit data and message redundancies by publication source. The largest percentage of visualizations with message redundancy are from the Infographic and News media venues. Overall and across all visualization sources, there is more frequent use of message redundancy as compared to data redundancy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Examples of the most and least recognizable visualizations from<ref type="bibr" target="#b7">[8]</ref>. TOP: Eye-tracking fixation heat maps (i.e., average of all participants' fixation locations) from the encoding phase of the experiment in which each visualization was presented for 10 seconds. The fixation patterns demonstrate visual exploration of the visualization. BOTTOM: Eye-tracking fixation heat maps from the recognition phase of the experiment in which each visualization was presented for 2 seconds or until response. The most recognizable visualizations all have a single focus in the center indicating quick recognition of the visualization, whereas the least recognizable visualizations have fixation patterns similar to the encoding fixations indicative of visual exploration (e.g., title, text, etc.) for recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>types of associations to help with visualization recognition: visual associations and semantic associations. The recognizable visualizations tend to contain more human recognizable objects (objects are present in 74% of the top third most recognizable visualizations) compared to the least recognizable visualizations (objects are present in 8% of the bottom third least recognizable visualizations). These human recognizable objects are examples of visual associations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Michelle A. Borkin is with the University of British Columbia, and Harvard University. E-mail: borkin@cs.ubc.ca.  Zoya Bylinskii, Constance Bainbridge, and Aude Oliva are with the Computer Science and Artificial Intelligence Laboratory (CSAIL) at the Massachusetts Institute of Technology (MIT), E-mail: {zoya,oliva}@mit.edu  Nam Wook Kim, Hanspeter Pfister, and Chelsea S. Yeh are with the School of Engineering &amp; Applied Sciences, Harvard University, E-mail: {namwkim,pfister}@seas.harvard.edu  Daniel Borkin is with the University of Michigan. E-mail: drborkin@umich.edu  *Equal contribution. Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication xx xxx 2015; date of current version xx xxx 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication 20 Aug. 2015; date of current version 25 Oct. 2015. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Digital Object Identifier no. 10.1109/TVCG.2015.2467732</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>GENDER EQUALITY IN LABOR FORCE PARTICIPATIONMESSAGE REDUNDANCY GENDER EQUALITY IN LABOR FORCE PARTICIPATION DATA &amp; MESSAGE REDUNDANCY</head><label></label><figDesc>Circle, Diagram, Distribution, Grid &amp; Matrix, Line, Map, Point, Table, Text, Trees &amp; Networks] Outline of where the actual data values are visually encoded (e.g., bars in a bar graph, points in a scatterplot, etc.).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>).</cell></row><row><cell cols="11">Data (type) [Area, Bar, Graphical Element Outline of any visual elements that are not related to the visual representation or description of the data.</cell></row><row><cell cols="3">Legend</cell><cell></cell><cell></cell><cell cols="6">Outline of any legends or keys that explain the data's visual encoding (e.g., color scales, symbol keys, map legends, etc.).</cell></row><row><cell cols="3">Object</cell><cell></cell><cell></cell><cell cols="6">[Photograph, Pictogram] Outline of any human recognizable objects (HRO) in the image. Objects are either realistic in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">representation (photograph) or abstract drawings (pictogram). Descriptions of each object were also recorded.</cell></row><row><cell cols="2">Text</cell><cell></cell><cell></cell><cell></cell><cell cols="6">[Axis Label, Header Row, Label, Paragraph, Source, Title] Outline of any text in the image. Subtypes cover all the common</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">representations from prose to labels.</cell></row><row><cell cols="6">GENDER EQUALITY IN LABOR FORCE PARTICIPATION</cell><cell cols="5">GENDER EQUALITY IN LABOR FORCE PARTICIPATION</cell></row><row><cell>RATIO OF FEMALE TO MALE</cell><cell>100% 25% 50% 75%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MALE RATIO OF FEMALE TO</cell><cell>100% 25% 50% 75%</cell><cell>82%</cell><cell>70%</cell><cell>69%</cell><cell>34%</cell></row><row><cell></cell><cell>0%</cell><cell cols="3">CHINA KOREA JAPAN</cell><cell>INDIA</cell><cell></cell><cell>0%</cell><cell cols="3">CHINA KOREA JAPAN</cell><cell>INDIA</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Source: Gender Statistics 2013, World Bank</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Source: Gender Statistics 2013, World Bank</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ORIGINAL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DATA REDUNDANCY</cell></row><row><cell cols="6">CHINA LEADS IN FEMALE LABOR FORCE PARTICIPATION WHEREAS INDIA LAGS SIGNIFICANTLY BEHIND IN 2013</cell><cell cols="5">CHINA LEADS IN FEMALE LABOR FORCE PARTICIPATION WHEREAS INDIA LAGS SIGNIFICANTLY BEHIND IN 2013.</cell></row><row><cell>RATIO OF FEMALE TO MALE</cell><cell>25% 50% 75% 100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MALE RATIO OF FEMALE TO</cell><cell>100% 25% 50% 75%</cell><cell>82%</cell><cell>70%</cell><cell>69%</cell><cell>34%</cell></row><row><cell></cell><cell>0%</cell><cell>CHINA</cell><cell>KOREA</cell><cell>JAPAN</cell><cell>INDIA</cell><cell></cell><cell>0%</cell><cell>CHINA</cell><cell>KOREA</cell><cell>JAPAN</cell><cell>INDIA</cell></row><row><cell></cell><cell></cell><cell>CHINA</cell><cell>KOREA</cell><cell>JAPAN</cell><cell>INDIA</cell><cell></cell><cell></cell><cell>CHINA</cell><cell>KOREA</cell><cell>JAPAN</cell><cell>INDIA</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Source: Gender Statistics 2013, World Bank</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Source: Gender Statistics 2013, World Bank</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Massachusetts (Massive) Visualization Dataset (MASSVIS) available at http://massvis.mit.edu.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For all the p-values reported in this paper, t-tests were corrected for multiple comparisons.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The ordering of fixations is also available in the data, but was not used for the present study.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We removed visualizations for which participants complained about the text being too small to read.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank Phillip Isola, Alexander Lex, Tamara Munzner, Hendrik Strobelt, and Deqing Sun for their helpful comments on this research and paper. This </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structuring the space: a study on enriching node-link diagrams with visual references</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hollerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;14</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1825" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding data videos: Looking at narrative visualization through the cinematography lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;15</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1459" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The intrinsic memorability of face photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Exp. Psychology: General</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1323</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Useful junk?: the effects of visual embellishment on comprehension and memorability of charts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mandryk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcdine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2573" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Does metaphor increase visual language usability?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Languages</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">State-of-the-art of visualization for eye tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroVis</title>
		<meeting>EuroVis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study on using visual embellishments in visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdul-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Floridi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2759" to="2768" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What makes a visualization memorable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2306" to="2315" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A review of visual memory capacity: Beyond individual items and toward structured representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visual longterm memory has a massive storage capacity for object details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="14325" to="14329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of traditional, orthogonal, and radial tree diagrams by an eye tracking study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Konevtsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoeferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intrinsic and extrinsic effects on image memorability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>In Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graphical perception: Theory, experimentation, and application to the development of graphical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of the Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">387</biblScope>
			<biblScope unit="page" from="531" to="554" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Trees, Maps, and Theorems: Effective Communication for Rational Minds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doumont</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Principiae</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The chartjunk debate: A close examination of recent findings. Visual Business Intelligence Newsletter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Few</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving revisitation in graphs through static spatial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface</title>
		<meeting>Graphics Interface</meeting>
		<imprint>
			<publisher>Canadian Human-Computer Communications Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparing information graphics: A critical look at eye tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Helfman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BELIV&apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Isotype visualization-working memory, performance, and engagement with pictographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;15</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How capacity limits of attention influence information visualization effectiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using eye tracking to investigate graph layout effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APVIS &apos;07</title>
		<imprint>
			<date type="published" when="2007-02" />
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A graph reading behavior: Geodesic-path tendency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PacificVis &apos;09</title>
		<imprint>
			<date type="published" when="2009-04" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Benefitting infovis with visual difficulties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2213" to="2222" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">What Makes a Photograph Memorable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1469" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A process dissociation framework: Separating automatic from intentional uses of memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Jacoby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="513" to="541" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Does an eye tracker tell the truth about visualizations?: Findings while investigating visualizations for decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upatising</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2421" to="2430" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word imageability affects the hippocampus in recognition memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Klaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schaller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hippocampus</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="704" to="712" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conceptual distinctiveness supports detailed visual long-term memory for real-world objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Konkle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Exp. Psychology: General</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="558" to="578" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Eye movements reveal distinct search and reasoning processes in comprehension of complex graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Krner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="893" to="905" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding charts and graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kosslyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="225" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The inferential basis of familiarity and recall: Evidence for a common underlying process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Leboe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W A</forename><surname>Whittlesea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="804" to="829" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Is chart junk useful? an extended examination of visual embellishment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moacdieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting</meeting>
		<imprint>
			<publisher>SAGE Publications</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1516" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Helping users recall their reasoning process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Lipford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stukes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Symposium on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Memorable words are monogamous: An information-theoretic account of word memorability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fedorenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Under Review</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing: The judgment of previous occurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="252" to="271" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Memorability of visual features in network diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Purchase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wybrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2477" to="2485" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mental imagery in associative learning and memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paivio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How deceptive are deceptive visualizations?: An empirical analysis of common distortion techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Satterthwaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A theory of graph comprehension. Artificial intelligence and the future of testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="73" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparing the readability of graph layouts using eyetracking and task-oriented analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Aesthetics in Graphics, Visualization and Imaging</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual analysis of perceptual and cognitive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Agapkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International J. of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual perception of parallel coordinate visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siirtola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heimonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Raiha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intern. Conf. on Information Visualisation</title>
		<imprint>
			<date type="published" when="2009-07" />
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An evaluation of the impact of visual embellishments in bar charts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Skau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroVis</title>
		<meeting>EuroVis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The Visual Display of Quantitative Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tufte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Cheshire (Conn</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Memory and consciousness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tulving</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Psychology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evaluating the effect of style in information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vande Moere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grechenig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2739" to="2748" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Long-term memory for 400 pictures on a common theme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magnussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="298" to="303" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Concrete words are easier to recall than abstract words: Evidence for a semantic contribution to short-term serial recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hulme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Exp. Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
