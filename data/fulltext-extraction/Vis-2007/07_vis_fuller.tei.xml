<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmentation of Three-dimensional Retinal Image Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">R</forename><surname>Fuller</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Zawadzki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacey</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Wiley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Werner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Hamann</surname></persName>
						</author>
						<title level="a" type="main">Segmentation of Three-dimensional Retinal Image Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>support vector machine</term>
					<term>segmentation</term>
					<term>image analysis</term>
					<term>retinal</term>
					<term>optical coherence tomography</term>
					<term>volume visualization</term>
					<term>image processing</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We have combined methods from volume visualization and data analysis to support better diagnosis and treatment of human retinal diseases. Many diseases can be identified by abnormalities in the thicknesses of various retinal layers captured using optical coherence tomography (OCT). We used a support vector machine (SVM) to perform semi-automatic segmentation of retinal layers for subsequent analysis including a comparison of layer thicknesses to known healthy parameters. We have extended and generalized an older SVM approach to support better performance in a clinical setting through performance enhancements and graceful handling of inherent noise in OCT data by considering statistical characteristics at multiple levels of resolution. The addition of the multi-resolution hierarchy extends the SVM to have &quot;global awareness.&quot; A feature, such as a retinal layer, can therefore be modeled within the SVM as a combination of statistical characteristics across all levels; thus capturing high-and low-frequency information. We have compared our semi-automatically generated segmentations to manually segmented layers for verification purposes. Our main goals were to provide a tool that could (i) be used in a clinical setting; (ii) operate on noisy OCT data; and (iii) isolate individual or multiple retinal layers in both healthy and disease cases that contain structural deformities.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Advancements in medical imaging are facilitating the extraction of accurate information from volumetric data making three-dimensional (3D) imaging an increasingly useful tool for clinical diagnosis and medical research. This development makes possible non-invasive examination and analysis of diseases by providing clinicians insight into the morphology of disease within the body and how it changes over time and through treatment. Common non-invasive imaging modalities are magnetic resonance imaging (MRI) and computed tomography (CT). Our efforts focus on the analysis and visualization of volumetric OCT retinal data. OCT, described in <ref type="bibr" target="#b7">[8]</ref>, is an acquisition system based on back-scattering of coherent light producing a stack of images similar to MRI and CT. A light beam is directed into a patient's eye where reflected light is merged with a reference beam eliciting an interference pattern that is used to gauge reflectance at various depths along the beam path. Quickly sweeping the beam across the retinal surface, in a structured pattern, produces the image stack.</p><p>The ophthalmology field historically identified diseases by examining fundus images (captured using an ophthalmoscope showing the retina, macula, and optic disc) and more recently by 2D thickness maps of retinal layers. OCT has drastically improved the type of information available to vision scientists allowing for a more intuitive view as well as analysis of retinal layer information. Recently, 3D OCT imaging has gained popularity by giving practitioners more information for their evaluations due to advancements in OCT technology. As a result, we have built software that turns what is an otherwise qualitative evaluation into a quantitative form.</p><p>An automatic approach that segments, classifies, and analyzes retinal layers from 3D OCT would be ideal. However, the morphology of retinal layers depends on the patient and the disease in question, which has caused problems for existing automatic retinal layer extraction methods <ref type="bibr" target="#b18">[19]</ref>. To address this problem we have developed a semiautomatic segmentation system in which the morphology of retinal structures can be discovered and refined by a clinician. The clinician interactively specifies the location of a retinal layer on a few select slices of the volume. This selection is then extrapolated throughout the entire volume using a SVM classifier in order to create a segmentation. Once segmented, we provide visualizations and measurements of the resulting segmentation to aid in disease diagnosis. The main visualization interface is an interactive 3D volume rendering of the segmented portions of the volume. We also provide more familiar visualizations such as a thickness map <ref type="bibr" target="#b0">[1]</ref>, currently a common diagnosis tool, and a 2D summed-intensity projection of the data resembling a fundus image (feature included for completeness, but we show no images of it in this paper). Additionally, the user can compute volume and thickness from layer segmentations, which have proven useful in retinal disease diagnosis. Speckle noise is a normally distributed noise component introduced by the data acquisition process. Our SVM approach, based on the original work <ref type="bibr" target="#b23">[24]</ref>, considers a voxel's mean value and variance across multiple resolutions in order to gracefully handle this noise and to give the SVM a global perspective over feature shapes. Additionally, this SVM is more tolerant of misclassification by the user, variation between patients and diseases, and adapts well to the data variation constituting retinal layer morphology.</p><p>Our main goals were to provide a tool that could (i) be used in a clinical setting; (ii) operate on noisy OCT data; and (iii) isolate individual or multiple retinal layers. Our main contributions to achieve these goals are (i) integration of a hierarchical data representation into SVM computations to counter noise and to better find retinal layers and (ii) several speedups for improving SVM performance allowing its practical use within a clinical setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>We evaluated a number of methods in order to find one that met our needs for OCT retinal data. One commonly used volumetric data segmentation method is a visualization technique that uses a onedimensional transfer function to map scalar intensity to color and opacity <ref type="bibr" target="#b11">[12]</ref>. This approach effectively segments regions based on scalar intensity so that areas of interest are shown opaquely while other regions are rendered transparently. Problems arise when different biological features share similar scalar values, manifesting as opaque regions that should not be, and is exacerbated when noise is present. Some improved methods address these problems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. However, the user interface for these types of methods is typically too cum- bersome for clinicians to use in everyday practice since they need intimate knowledge of the intensity-to-color mapping in order to properly understand the user interface. Our work is based on the work described in <ref type="bibr" target="#b21">[22]</ref>, which describes how artificial intelligence algorithms can be used to construct N-dimensional transfer functions through an intuitive user interface.</p><p>Machine learning algorithms such as artificial neural networks have been used in medical imaging research somewhat successfully <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. However, SVMs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4]</ref> have yielded more reliable results for feature detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16]</ref>. The method discussed in <ref type="bibr" target="#b22">[23]</ref> compares the use of neural networks and SVMs when constructing an N-dimensional transfer function (mapping). In our case, the ability of SVMs to handle error, both in the form of speckle noise and user misclassification, makes them attractive.</p><p>Typical characteristics used to train machine learning algorithms are scalar value, gradient magnitude, spatial location, orientation, and neighborhood information. However, these characteristics can cause a SVM to be sensitive to noise or objects that are structurally deformed, resulting in poor segmentations. We have found that careful selection of the characteristics defining the SVM input vector is typically better than adding as many as possible. Too many characteristics dilute the input vector by slowing down SVM computations and this fact often leads to unwanted segmentation results. Additionally, SVMs are typically fed local data characteristics establishing a dependence on the base resolution that ignores global feature trends that are apparent when looking at the data macroscopically. The method described in <ref type="bibr" target="#b16">[17]</ref> uses wavelets combined with a SVM classifier to identify texture properties of image data. This concept is useful since the method identifies patterns with distinct texture characteristic on a more global scale. The method described in <ref type="bibr" target="#b19">[20]</ref> employs a multi-resolution SVM kernel to account for macroscopic features and differs from the method discussed in this paper in that we instead compute a multi-resolution representation of the data.</p><p>Good examples of the current state of the art of 3D OCT visualization and analysis are described in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13]</ref>. To our knowledge, no existing system for 3D OCT retinal visualization and analysis is as complete and accurate as that presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">OCT Retinal Data</head><p>Volumetric OCT data are captured by directing a light beam at a patient's retina in a gridded fashion. Back-scattered interference patterns are captured through a complex feedback system, described in <ref type="bibr" target="#b25">[26]</ref>, to produce -scans similar to slices obtained via MRI, CT, or ultrasound. However, neighboring slices are not registered to one another as they are in MR imaging, due to naturally occurring unconscious eye movements. During the scanning process, the clinician monitors a realtime display of the -scans in order to eliminate low-frequency movements. High-frequency vibrations are almost always present. Typically, the clinician collects 80 to 200 -scans having dimensions ranging from 500 × 250 to 1000 × 500 pixels in size (corresponding to a region of about 8mm × 8mm × 300 m in size). <ref type="figure" target="#fig_1">Figure 1</ref> shows a single -scan and also a cross section of several -scans showing the (mis-)alignment along the stack axis. In a preprocessing step, slices are registered using standard registration techniques. Our clinicians use ImageJ <ref type="bibr" target="#b17">[18]</ref>, which computes rigid body transformation (translation and rotation) to minimize the difference between neighboring -scans. <ref type="figure">Figure 2</ref> shows volume data sets of three different retinal diseases compared to a healthy human retina. Often, the disease type is obvious from a single -scan. However, a time-series volumetric data set can show disease progress that is not apparent from a single scan. In addition, a clinician can extract volume information of fluid, in the case of retinal detachment (fluid collects beneath the photoreceptor layer), in order to gauge severity of a disease before surgery and improvement after surgery.</p><p>For a normal healthy retina, the retinal surface and layers are welldefined but obfuscated by noise inherent in OCT data. This is exacerbated by the fact that patient's move their eye while being scanned resulting in "spikes" in -scans (as indicated in the left-side of the top image of <ref type="figure" target="#fig_1">Figure 1</ref>). The clinician cannot always capture blemish- free volumes since the patient must not move their eye for about ten seconds, which is not always possible due to natural involuntary eye movements. In addition, we needed to build a system that could segment and analyze diseased retina. In diseased cases, the retinal layers are not well-defined and can i) be missing entirely, ii) vary in thickness across the retina, iii) be very thin, iv) be very thick, v) be very bumpy, and vi) be erratic in morphology; all of which are handled effectively by a SVM due to its flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SVM SEGMENTATION METHOD</head><p>The two main choices when implementing a SVM are the kernel and input vector used to classify the feature space. We use a radial basis function kernel since we assume it can represent our feature space well and also that speckle noise is normally distributed across the data. Additionally, it allows non-linear separation of the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Space</head><p>The data characteristics we include in our input vector are scalar intensity, gradient, spatial location, mean of the neighbors, and variance. The most obvious inclusion for a voxel i = {i, j, k} is scalar intensity f i and allows for efficient segmentation of regions having relatively low standard deviation (noiseless data) and in our case, even though we have substantial noise, this characteristic still proves useful. The spatial location p i = {x i , y i , z i } of a voxel (considered with the other characteristics) allows the differentiation between features having similar data-distribution characteristics but residing in different locations.</p><p>The method described in <ref type="bibr" target="#b21">[22]</ref> suggests that six neighbor intensity values f i±1, j±1,k±1 should be considered at i to counter noise. They suggest that if a voxel value has been perturbed by noise, inclusion of its neighbors will help determine the "actual value." We found, in our case, that this can lead to disconnected components due to those regions having the appropriate neighbors, but not necessarily the correct local data distribution. Our method instead uses as a parameter the mean of the six neighbors, leading to improved results. We also include the variance (instead of the standard deviation) to include a data distribution characteristic and additionally include gradient magnitude to identify tissue boundaries.</p><p>In summary, for a voxel i at world-space location p i with scalar value f i having neighbors N = {i ± 1, j ± 1, k ± 1} the data characteristics we use for our input vector are:</p><formula xml:id="formula_0">f i , (1) x i ,<label>(2)</label></formula><p>Healthy Age-related macular degeneration Retinal detachment Glaucoma <ref type="figure">Fig. 2</ref>. Comparison of different retinal diseases. The front portion of each volume has been clipped in order to reveal internal structures. Drusen, extracellular deposits on the photoreceptor layer, are indicative of age-related macular degeneration. The retinal detachment data were captured after surgery. All images are of the foveal region (the dip in the middle) except for the glaucoma case, which is of the optic nerve head.</p><formula xml:id="formula_1">y i , (3) z i ,<label>(4)</label></formula><formula xml:id="formula_2">f i = 1 |N| n∈N f n ,<label>(5)</label></formula><formula xml:id="formula_3">2 i = 1 |N| n∈N f n −f i 2 ,<label>(6)</label></formula><formula xml:id="formula_4">f i = 1 |N| + 1 f i + n∈N f n .<label>(7)</label></formula><p>Thus, we include for each voxel i the scalar value f i , location p i , mean valuef i of the neighbors around i, variance 2 i around i, and the gradient f i at i using a local difference operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-resolution Hierarchical SVM</head><p>Even in the presence of noise, a person can seemingly automatically classify features by the distribution of data intensities in both local neighborhoods and globally. When confronted with subtle changes in these distributions, a person is able to perceive material (tissue) boundaries that are not necessarily well-defined by scalar value, but are instead identified by the transition of one noisy distribution to another. To expand the operation of our SVM beyond the local scope of data characteristics, we construct a hierarchy of representations of the volume and sample the mean value, gradient magnitude, and variance at each level of this hierarchy in order to capture the distribution across all levels and accumulate a weighted average of these samples prior to inserting them into the SVM.</p><p>Our algorithm constructs a mipmap-like hierarchy having the full resolution data as level l 0 and successive levels l i , i &gt; 0, having half the resolution (in each dimension) of the previous level l i−1 , see <ref type="figure">Figure</ref> 3. The user specifies "feature" and "background" voxels on l 0 and then specified voxels are mapped to coarser levels l i , i &gt; 0 in order to determine the intensity value, mean value, gradient, and variance at that level. Since each lower-resolution level voxel covers a larger region, we weigh each successive level's result to reasonably diminish its influence on the final value. Since each successive level in our hierarchy is an eighth the size as the previous level, each level l receives a weight w l = 1/8 l . We allow the user to manipulate this value to increase or further diminish the influence of lower resolution levels. Additionally, we normalize the weights so that {l} w l = 1 to maintain the integrity of the values. <ref type="figure" target="#fig_2">Figure 4</ref> shows the effect of different hierarchy-level weights on SVM segmentations. To include the multiresolution hierarchy into our SVM computations, we applied weighted averaging each to scalar intensity, gradient, mean of the neighbors, and variance characteristics. Thus, when obtaining characteristics for each voxel (either to train or classify), we access the hierarchy at all levels to produce a weighted average for each of those characteristics. We did not include spatial location in this process since this characteristic was not affected by multiple levels of detail. Additionally, we found that the maximum number of levels was less than ten in most cases since one of the primary axes would vanish by this point.</p><formula xml:id="formula_5">l 0 l 1 l 2 l 3 Fig. 3.</formula><p>We construct a mipmap-like hierarchy in order to compute varying levels of data distributions to sample as input to our SVM algorithm. Top-left image shows a slice through a level having a resolution of 475 × 150 × 48. Remaining images show levels having a resolution half the one before it. (The change in the "bump" is due to the influence of neighboring slices in front and behind the shown slice.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Specification</head><p>A user provides SVM training data through an intuitive interface in order to create the "segmentation function." Our system allows a user to quickly classify features by using a small number of specification (training) points. We perform this specification on axis-aligned 2D slices of the volume similar to <ref type="bibr" target="#b21">[22]</ref>. The user can slice a plane through the volume to see a 2D intersection image and can "paint" on that plane to mark points as "feature" or "background" as indicated by the green and red marks, respectively, in <ref type="figure" target="#fig_2">Figure 4</ref>. This approach can be learned quickly and has proven to be user-friendly within a clinical setting. The user is required only to draw through regions of interest and to indicate regions not of interest. Unfortunately, as the training data set grows through painting, so does the complexity of the SVM. This additional complexity leads to drastically longer segmentation times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Speed Improvements</head><p>SVMs have a large computational cost that hinders their application to an entire volume with real-time response behavior. We have implemented several techniques that reduce computational cost for both SVM training and classification while having minimal effect on the resulting segmentation quality. These methods are intended to minimize clinician time needed to iterate between specifying training data and viewing results. The methods include (i) training-data reduction;</p><p>(ii) evaluation on a single slice; (iii) checkerboard SVM sampling; (iv) SVM multi-threading; and (v) clipping planes; each is discussed in detail in the following sections.</p><p>Training-data reduction We found that users typically specified many training data points resulting in duplication of the data characteristics fed into the SVM since our painting interface lays down a "block" of specification points whenever the user marks a region. Thus, using every voxel marked by this block introduces some redundancies resulting in substantial SVM computation penalties while As indicated in the bottom image, the presence of an overlying grid becomes noticeable when lower-resolution levels are weighted more. In this case, the entire segmentation region should be connected, but in general, this is not the case, for example, when segmenting fluid pockets caused by retinal detachment.</p><p>not necessarily producing better results. A representative subset R = {r 0 , r 1 , r 2 ,...,r |R| } from the entire training set T = {t 0 , t 1 , t 2 ,...,t |T| } is sufficient to achieve nearly the same results while requiring much less computation time. To obtain the subset R ⊂ T, we implemented a discrete form of best-candidate sampling where a random point r 0 = t rand(|T|) is first added to R. Then, we repeatedly add to R the point from T that is farthest (in Cartesian distance) from all points currently in R. This process continues until (i) the number of points |R| reaches a user specified maximum threshold or (ii) the next point to be added is closer to another point in R than a user-specified minimum distance. Both of these thresholds mitigate the growth of training data by reducing the set of points used in a fashion that covers all training regions well. We found in practice that having no maximum threshold while setting a minimum distance of two voxels between training data points in R has little to no effect on the resulting segmentation while speeding up the SVM computation by nearly 60%.</p><p>Evaluation on a single slice Our software allows a user to test the SVM segmentation on individual 2D slices to evaluate the resulting segmentation for that slice. If needed, the user can modify the painted regions or browse additional slices and subsequently apply the SVM or further mark those slices. The application of the SVM to a single slice requires only a few seconds (even for larger volumes of 1000x × 500 × 200 voxels). Once satisfied with the segmentation on individual slices, the user can apply the SVM to the whole volume. Checkerboard SVM classification After the SVM has been trained, we apply the assumption that retinal features are much larger than an individual voxel by using a checkerboard scheme that first classifies every other voxel by the SVM as either a feature or background. Each unclassified voxel is then determined by finding the majority classification of its six neighbors. If the classification is tied, we then apply the SVM to that voxel to decide. We found the checkerboard scheme consistently reduces SVM classification time by roughly 45% and consequently leads to smoother object boundaries and less "orphaned" voxels (sparse individual misclassified voxels).</p><p>SVM multi-threading Once trained, an SVM inherently classifies each voxel independently of others. We take advantage of this by incorporating multi-threaded SVM classification in order to take advantage of popular multi-processor computers (the primary computer used to run our software has four processor cores). We have found this to do the expected by speeding up the SVM classification step according to the number of physical processors.</p><p>Clipping planes Clipping planes can be specified to restrict the SVM application to a sub-volume of the data. Our software allows a user to specify axis-aligned clipping planes in order to specify a subregion (bounded by the clipping planes). This approach reduces the (i) number of voxels to be processed, (ii) specification time by the user, and (iii) training data size while subsequently reducing the complexity of the SVM resulting in both faster training and classification. For retinal data, clipping planes are useful for isolating broad regions around relatively flat retinal layers of interest. Thus, while a simple concept, this method is quite useful for our application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LAYER THICKNESS ANALYSIS</head><p>Once a retinal layer has been isolated, measurements are computed in order to track patient progress over time or to diagnose diseases such as glaucoma, which can be identified by retinal-layer volumes outside normal parameters. Additionally, the fluid volume beneath the photoreceptor layer caused by a retinal detachment can be measured before and after surgery to insure that the surgery was a success and additionally to monitor disease advancement.</p><p>Typically, a clinician isolates the full retinal thickness as the region between the bottom of the photoreceptor layer and the top surface of the retina in order to examine thickness maps, see <ref type="figure" target="#fig_3">Figure 5</ref>. Thickness maps provide insight into gradually changing thickness and other abnormalities. The clinician also isolates the photoreceptor layer itself in order to reveal thickness abnormalities. More details on thickness map analysis of retinal layers can be found in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>Our software is currently being used by vision scientists at the University of California, Davis Medical Center. We present work by two clinicians to examine our software's practicality in a clinical setting as well as the quality of segmentation results. We have applied our Manual SVM <ref type="figure">Fig. 6</ref>. Extracting the "healthy" thickness map from full retinal thickness segmentations. Left-column indicates manual segmentation while rightcolumn is the SVM. Top-images show the segmentation specification on a slice. Middle-images show the segmentation applied to the whole volume. Bottom-images show the extracted thickness maps. For thickness, one voxel unit is approximately equal to 1um. methods to data from patients having age-related macular degeneration and retinal detachment and compare these to a healthy retina, see <ref type="figure">Figure 2</ref>. We examined how well our method segments healthy retinal layers, diseased retinal layers, and pockets of fluid by comparing our semi-automatically generated segmentations to manual generated segmentations.</p><p>We included a manual segmentation tool within our software allowing a clinician to isolate retinal layers by defining top-and bottombounding polylines on key frames throughout the volume. The specified polylines are linearly interpolated between key frames in order to segment every slice. We use this segmentation as a "gold standard" to gauge the quality of our SVM-based segmentations. We asked one clinician to define three gold standards for our test data and found that this process required 30 minutes on the healthy data and up to 50 minutes on the diseased data to specify. <ref type="figure">Figure 6</ref> shows a manuallyand SVM-generated segmentation as well as the process from (i) slice segmentation to (ii) volume segmentation to (iii) full retinal thickness map of the healthy data.</p><p>Data acquisition In practice, the clinician spends approximately 30 to 45 minutes with a patient in order to capture the 3D OCT data. The patient must fill out medical forms, is educated about equipment being used, their pupils may be dilated, and is then positioned into a chin rest for the scanning. The clinician then goes through a systematic process of instructing where to fixate enabling the capture of several scans of various regions on their retina. The clinician evaluates realtime 2D scan images in order to guarantee data quality. Once scanning is finished, the clinician proceeds with data processing and visualization.</p><p>There is a series of steps involved in order to prepare the data for our  <ref type="table">Table 1</ref>. Improvement influence on full retinal thickness segmentation performance as applied to the healthy data. Improvements tested were checkerboard (CH), multi-resolution hierarchy (MR), and best candidate reduction (RED). (Multi-threading was used in all tests.) All tests began with the same training data. Tests were performed on a laptop having a 2GHz Intel Core 2 Duo processor (two cores total) and 2GB of main memory and also on a desktop computer used by the clinicians having dual 3GHz Intel Xeon processors (four cores total) with 3GB of main memory. The multi-resolution hierarchy used the weights w l = 1/8 l . The best candidate training data reduction included a minimum distance of two voxels between training data.</p><p>software. The majority of time is spent exporting 2D slices from the acquisition software and subsequently registering them to one another. It takes approximately 15 minutes for a clinician to process all of the collected patient data in this fashion.</p><p>Protocol We asked the two participating clinicians (identified as C1 and C2) to isolate the photoreceptor layer (PRL) and the full retinal thickness (encompassing the space from the retinal surface up to and including the PRL) using the painting interface on all three data sets. At first, we allowed them to spend as much time as they thought necessary to obtain adequate results and found that they spent around 20 to 30 minutes per feature segmentation. Then, we asked them to repeat each feature segmentation in under ten minutes. Their results are explained in detail over the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SVM Performance</head><p>We measured the running time for the checkerboard, multi-resolution hierarchy, and best-candidate training-data reduction improvements. We performed the tests on the healthy data using the "unlimited time" SVM training data from clinician C1. We used two computers having different amounts of main memory and processor capabilities, see <ref type="table">Table 1</ref>. Overall, we found that the checkerboard speedup slightly improved segmentation quality while significantly speeding up the entire process. Thus, we left this option on for these tests.</p><p>Overall, we determined that the multi-resolution hierarchy had a negligible effect on SVM training while having a SVM classification penalty of only 3% to 8% in all cases. We had expected the multiresolution hierarchy to impact the SVM classification running time significantly more, but it seems that the SVM computations are far more complex than the additional data accesses needed to implement the hierarchy. The checkerboard improved performance by about 50% in all cases, which is expected since it applies the SVM classification to about half of the data. The best-candidate training data reduction (having a minimum distance of two voxels between training points) improved SVM training performance by about 90% and further reduced SVM classification running time by about 60% while not significantly effecting the resulting segmentations. Quality measurements are detailed in the next section. <ref type="table">Table 2</ref> shows the running time results for all SVM segmentations (as performed on the lab computer). The full thickness segmentations required on average less than two minutes. The SVM classification <ref type="table">Table 3</ref>. Comparison of SVM segmentations to gold-standard manual segmentations. C1 and C2 indicate each clinician. The values in the table indicate the thickness difference in voxel units between the SVM and manual segmentations. The 68% metric states that 68% of the thickness differences are less than the value specified in the table. Layer thicknesses typically range from 30 to 100 voxel units depending upon the disease.</p><p>cians had difficulty isolating the PRL in many patient cases due to the PRL being thin (&lt; 30 voxels) and the local data characteristics used in standard SVM computations were not able to capture the "thin" morphology of the PRL. With the addition of the multi-resolution hierarchy, we are now able to isolate the PRL accurately. This is due to the ability of the hierarchy to examine lower-resolution levels (lowfrequency information) in order to find feature characteristics that define the thin PRL. <ref type="figure" target="#fig_5">Figure 7</ref> compares the manual segmentation with the multi-resolution SVM segmentation of the PRL for the healthy data. The multi-resolution SVM segmentation captures the foveal region well while additionally revealing retinal blood vessels.</p><p>Our SVM method performed well on both healthy and disease data. The ability of the SVM to isolate arbitrary features, such as the pocket of fluid beneath the retina in the case of retinal detachment, furthers the application of our method. <ref type="figure">Figure 8</ref> shows the results of our fluid segmentations for the retinal detachment data. We noticed that all of the SVM segmentations underestimated the volume slightly. This is apparent in the histogram of thickness-map differences also shown in <ref type="figure">Figure 8</ref>. Additionally, we were especially pleased with the results in disease cases having retinal layer deformation. This was primarily evident with the drusen on the PRL of the age-related macular degeneration data. We obtained reasonable reproducibility among clinicians as well as tolerable error, see <ref type="figure">Figure 9</ref>.   <ref type="table">Table 4</ref>. Improvement influence on full retinal thickness segmentation quality as applied to the healthy data set. The values in the table indicate the thickness difference in voxel units between the SVM segmentation computed using the improvement and the manual segmentation. Improvements tested were checkerboard (CH), multi-resolution hierarchy (MR), and best candidate reduction (RED). All tests began with the same training data. The multi-resolution hierarchy used the weights w l = 1/8 l . The best-candidate training data reduction included a minimum distance of two voxels between training points. Improvements tended to increase segmentation quality. The difference in how the manual and SVM segmentations handle blood vessels manifested as voxel difference error in our thickness maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE RESEARCH</head><p>We used a combination of volume visualization and data analysis techniques to better diagnose and subsequently treat retinal diseases. We have found that applying volume visualization techniques to 3D retina image data collected in a clinical setting has achieved success by revealing subtle features that standard diagnosis procedures miss as well as providing accurate quantitative measurements of retinal structures. This tool is currently being used in a clinical environment and is continually providing insight into challenging retinal visualization and analysis problems. We found that handling noise is a difficult task when training and using any type of machine learning algorithm for segmentation. We plan to investigate image filtering techniques to reduce speckle noise prior to SVM processing. We also plan to create training data that can be applied to multiple volumes eliminating the need to retrain the SVM for new patients. Our method shows reproducibility among different clinicians and yields good accuracy. The primary differentiating factor between clinicians is their interpretation as to where a feature begins and ends, which is better controlled through a clinical protocol. Our method currently produces desirable results in about ten minutes. This is a significant improvement over past machine learning applications to volumetric segmentations.</p><p>The most prominent drawback from our method is that the SVM mis-classifies some voxels resulting in scattered noise. This is noticeable in the thickness maps shown in <ref type="figure">Figure 9</ref>. However, the clinicians are aware of this issue and are willing to cope with it (by weighting <ref type="figure">Fig. 8</ref>. Isolation of fluid beneath retina from retinal detachment data. Top-left image shows SVM segmentation from clinician C2 having a ten minute time constraint. Top-right image shows associated thickness map. Bottom-image shows the histogram of thickness map differences between the manual and SVM segmentations for all fluid SVM segmentations. SVM segmentation consistently slightly underestimated the fluid volume as compared to the manually specified segmentation. We noticed that this was because the manually specified polylines were placed on the layer boundary as opposed to just inside of it, which is how the SVM identifies the region.</p><p>the hierarchical levels differently) due to the enormous time savings involved with using the SVM over other methods.</p><p>As a result of this work, vision scientists are using this software to provide quantitative information useful in treatment planning that is not otherwise available. It is certain to contribute to research and eventually may facilitate clinical diagnosis and monitoring. SVM Segmentation -10 min -C1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manual 10 min -C1</head><p>No limit C2 No limit C1 <ref type="figure">Fig. 9</ref>. Isolation of PRL and drusen (bumps on the PRL) for the agerelated macular degeneration data. Top-image shows the 10 min SVM segmentation from clinician C1. Middle-images show the thickness maps obtained for these data. Bottom-image shows the histogram of thickness map differences between the SVM and manual segmentations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Published 14</head><label>14</label><figDesc>September 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Top image shows one OCT -scan. Middle image shows 80 unaligned -scans along the stack axis. Bottom image shows the scans after alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Influence of multi-resolution hierarchy on SVM segmentation. Top-image uses standard local data characteristics while subsequent segmentations accumulate data characteristics across the hierarchy using the weight shown. Each segmentation used the same training data points as indicated by the green (feature) and red (background) marks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Regions isolated by clinicians are the photoreceptor layer (PRL) and the full retinal thickness indicated by the boundary at the bottom of the PRL and ending at the top of the inner limiting membrane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>68%</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison between manual and multi-resolution SVM segmentation of the PRL of the healthy data. Left-image shows gold standard manual segmentation. Right-image shows the multi-resolution SVM segmentation. Note how the SVM segmentation reveals blood vessels and better indicates the location of the fovea (red area in the middle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Fuller and Hamann are with Institute for Data Analysis and Visualization (IDAV) and Department of Computer Science, University of California, Davis, E-mail: arfuller@ucdavis.edu and hamann@cs.ucdavis.edu. • Zawadzki, Choi, and Werner are with Vision Science and Advanced Retinal Imaging Laboratory, Department of Ophthalmology and Vision Science, University of California, Davis, E-mail: {rjzawadzki, sschoi, jswerner}@ucdavis.edu. • Wiley and Hamann are with Stratovan Corporation, E-mail: {wiley, hamann}@stratovan.com.</figDesc><table /><note>Manuscript received 31 March 2007; accepted 1 August 2007; posted online 27 October 2007. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>2GHz Intel Core 2 Duo + 2GB main memory</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="3"># Training SVM Training SVM Class.</cell></row><row><cell>Improvement</cell><cell>Points</cell><cell>Time (sec)</cell><cell>Time (min)</cell></row><row><cell>(laptop) None</cell><cell>2902</cell><cell>8.92</cell><cell>29.74</cell></row><row><cell>CH only</cell><cell>2902</cell><cell>8.73</cell><cell>14.74</cell></row><row><cell>CH+MR</cell><cell>2902</cell><cell>8.75</cell><cell>15.94</cell></row><row><cell>CH+RED</cell><cell>736</cell><cell>1.02</cell><cell>6.36</cell></row><row><cell>CH+MR+RED</cell><cell>732</cell><cell>1.00</cell><cell>6.59</cell></row><row><cell cols="4">(lab machine) Dual 3GHz Intel Xeon + 3GB main memory</cell></row><row><cell>None</cell><cell>2902</cell><cell>5.25</cell><cell>7.54</cell></row><row><cell>CH only</cell><cell>2902</cell><cell>5.05</cell><cell>3.76</cell></row><row><cell>CH+MR</cell><cell>2902</cell><cell>5.13</cell><cell>4.00</cell></row><row><cell>CH+RED</cell><cell>736</cell><cell>0.53</cell><cell>1.60</cell></row><row><cell>CH+MR+RED</cell><cell>732</cell><cell>0.52</cell><cell>1.64</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the National Science Foundation under contracts ACI 9624034 (CAREER Award), NEI grant 014743, through the Large Scientific and Software Data Set Visualization (LSSDSV) program under contract ACI 9982251, and a large Information Technology Research (ITR) grant; and the National Institutes of Health under contract P20 MH60975-06A2, funded by the National Institute of Mental Health and the National Science Foundation.</p><p>Alfred Fuller has been supported by a Student Employee Graduate Research Fellowship (SEGRF) form Lawrence Livermore National Laboratory.</p><p>We thank the members of the Visualization and Computer Graphics Research Group at the Institute for Data Analysis and Visualization (IDAV) at the University of California, Davis.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>time did not seem to directly correlate with the training data size. Additionally, we found that clinicians tended to paint aggressively up front when under a time constraint in order to avoid more iterations later and, when under no time limit, the clinicians tended to paint sparsely and iterate more often. This led to the ten-minute training data being comparable or even larger than the unlimited-time training data.  <ref type="table">Table 2</ref>. Measured running times for full retinal thickness segmentations using multi-threading, clipping planes, checkerboard, multi-resolution hierarchy, and best-candidate training-data reduction. C1 and C2 indicate each clinician. Tests were performed on our "lab computer," a dual processor 3GHz Intel Xeon with 3GB of main memory (four processor cores total). Multi-resolution hierarchy weights were set to w l = 1/8 l and the best-candidate training-data reduction had a minimum distance of two voxels between training points. Measured running time without our improvements ranged between from 30 min to 2 hours in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SVM Segmentation Quality</head><p>We derived a thickness map from each segmentation by considering the distance (thickness) between the top and bottom surfaces of the segmentation. To estimate the quality of our SVM method, we compared thickness maps extracted from the gold-standard manually generated segmentation to that obtained by our clinician's SVM segmentation. We computed the difference, at each point, between the thickness maps as our error metric. We computed the mean and standard deviation of these differences and additionally found a useful metric to be the value that 68% of the differences fell below, see <ref type="table">Table 3</ref>. Furthermore, we investigated the effect our checkerboard, multi-resolution hierarchy, and best-candidate training data reduction schemes had on segmentation quality, see <ref type="table">Table 4</ref>. We found in each of these cases that the resulting SVM segmentation better matched the gold standard.</p><p>Prior to our addition of the multi-resolution hierarchy, our clini- </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Application of rapid scanning retinal thickness analysis in retinal diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Asrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1145" to="1151" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of view-based object recognition algorithms using realistic 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Artificial Neural Networks</title>
		<meeting>Int&apos;l Conf. Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="251" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Ann. Workshop Computational Learning Theory</title>
		<meeting>Fifth Ann. Workshop Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Support vector network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural networks for volumetric mr imaging of the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gelenbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Neural Networks for Identification, Control, Robotics, and Signal/Image Processing</title>
		<meeting>Int&apos;l Workshop Neural Networks for Identification, Control, Robotics, and Signal/Image essing</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="194" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optical coherence tomography measurement of macular and nerve fiber layer thickness in normal and glaucomatous human eyes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Guedes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="189" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of neural network and fuzzy clustering techniques in segmenting magnetic resonance images of the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bensaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Velthuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Silbiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="672" to="682" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puliafito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical coherence tomography. Science</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="page" from="1178" to="1181" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rgvis: Region growing based techniques for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific Graphics &apos;03 Conf</title>
		<meeting>Pacific Graphics &apos;03 Conf</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-automatic generation of transfer functions for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Durkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. &apos;98 IEEE Symp</title>
		<meeting>&apos;98 IEEE Symp</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">Visualization</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive volume rendering using multi-dimensional transfer functions and direct manipulation widgets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization &apos;01 Conf</title>
		<meeting>IEEE Visualization &apos;01 Conf</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optical coherence angiography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yatagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yasuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="7821" to="7840" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Retinal nerve fiber layer thickness map determined from optical coherence tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mujat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cense</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Boer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Express</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="9480" to="9491" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to kernel-based learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training support vector machines: An application to face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. &apos;97 Conf. Computer Vision and Pattern Recognition</title>
		<meeting>&apos;97 Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wavelets and support vector machines for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INMIC 2004. 8th International</title>
		<meeting>INMIC 2004. 8th International</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="328" to="333" />
		</imprint>
	</monogr>
	<note>Multitopic Conference</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Rasband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">U S</forename><surname>Imagej</surname></persName>
		</author>
		<ptr target="http://rsb.info.nih.gov/ij" />
	</analytic>
	<monogr>
		<title level="j">National Institutes of Health</title>
		<imprint>
			<biblScope unit="page" from="1997" to="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Errors in retinal thickness measurements obtained by optical coherence tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Sadda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Richine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Labree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="293" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-resolution support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cherkassky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN apos;99. International Joint Conference on</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1065" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thorsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th European Conf. Machine Learning</title>
		<meeting>10th European Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A novel interface for higherdimensional classification of volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization 2003 Conference</title>
		<meeting>the IEEE Visualization 2003 Conference</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An intelligent system approach to higher-dimensional classification of volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Estimation of Dependences Based on Empirical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Three-dimensional retinal imaging with high-speed ultrahigh-resolution optical coherence tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wojtkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Schuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowalczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1734" to="1746" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d oct imaging in clinical settings: Toward quantitative measurements of retinal structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Zawadzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Bower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Izatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Werner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photonics West -Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">6138</biblScope>
			<biblScope unit="page" from="8" to="18" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
