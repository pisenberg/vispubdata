<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Quality Multimodal Volume Rendering for Preoperative Planning of Neurosurgical Interventions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-10-27">27 October 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Beyer</surname></persName>
							<email>beyer@vrvis.at</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Medical University Vienna</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Medical University Vienna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hadwiger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Medical University Vienna</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Medical University Vienna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wolfsberger</surname></persName>
							<email>stefan.wolfsberger@meduniwien.ac.at.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Medical University Vienna</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Medical University Vienna</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Bühler</surname></persName>
							<email>buehler@vrvis.at</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Medical University Vienna</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Medical University Vienna</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">High-Quality Multimodal Volume Rendering for Preoperative Planning of Neurosurgical Interventions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-10-27">27 October 2007</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2007; accepted 1 August 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimodal Volume Rendering</term>
					<term>Hardware Assisted Raycasting</term>
					<term>Surgery Planning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Surgical approaches tailored to an individual patient&apos;s anatomy and pathology have become standard in neurosurgery. Precise preoperative planning of these procedures, however, is necessary to achieve an optimal therapeutic effect. Therefore, multiple radiological imaging modalities are used prior to surgery to delineate the patient&apos;s anatomy, neurological function, and metabolic processes. Developing a three-dimensional perception of the surgical approach, however, is traditionally still done by mentally fusing multiple modalities. Concurrent 3D visualization of these datasets can, therefore, improve the planning process significantly. In this paper we introduce an application for planning of individual neurosurgical approaches with high-quality interactive multimodal volume rendering. The application consists of three main modules which allow to (1) plan the optimal skin incision and opening of the skull tailored to the underlying pathology; (2) visualize superficial brain anatomy, function and metabolism; and (3) plan the patient-specific approach for surgery of deep-seated lesions. The visualization is based on direct multi-volume raycasting on graphics hardware, where multiple volumes from different modalities can be displayed concurrently at interactive frame rates. Graphics memory limitations are avoided by performing raycasting on bricked volumes. For preprocessing tasks such as registration or segmentation, the visualization modules are integrated into a larger framework, thus supporting the entire workflow of preoperative planning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Minimally invasive neurosurgical procedures are constantly gaining importance with the aim to minimize surgical trauma, shorten recovery times and reduce postoperative complications. For surgery of deep-seated structures, neurosurgical keyhole procedures are becoming standard, where a small opening in the skull is sufficient to gain access to a much larger intracranial region via an endoscope or operating microscope. In contrast, interventions in areas directly below the skull require a larger and individually tailored opening of the cranial bone. For both approaches (i.e., surgery of deep-seated structures and near the brain's surface), orientation is necessary to perform the skin incision and bone cover removal at the optimal location. For deepseated targets, further orientation is crucial to find the structures of interest while additionally preserving the surrounding tissue.</p><p>Preoperative planning enables the surgeon to identify anatomical landmarks and critical structures (e.g., large vessels crossing the path of the operating microscope or critical cranial nerves) and in determining the optimal position of incision prior to surgery. It is during this planning session that the physician decides upon the optimal approach by adapting the general surgical plan to the individual patient's anatomy. The medical doctor uses this knowledge during surgery to determine the current location in the skull and the subsequent optimal course of action. Therefore, the success of a surgery, especially in keyhole approaches, depends largely on accurate preoperative planning.</p><p>Up to now, the standard approach of presurgical planning is performed using stacks of raw images obtained from medical scanners such as CT (Computed Tomography) or MRI (Magnetic Resonance Imaging). In the field of neurosurgery, MR scans are the medium of choice for depicting soft tissue such as the brain, whereas CT scans are superior in picturing bony structures. Functional MR (fMR) images depict neural activity, Positron Emission Tomography (PET) shows metabolical activity and Digital Subtraction Angiography (DSA) depicts vessels in high quality. However, a mental combination of all these datasets and a correct 3D understanding by simple slice-by-slice analysis is very difficult, even for the skilled surgeon.</p><p>3D visualization alleviates this problem by enhancing the spatial perception of the individual anatomy and, therefore, speeding up the planning process. Considering the neurosurgical background, a preoperative planning application has to meet certain requirements: First of all, it should provide a high-quality, interactive and flexible 3D visualization of the volumetric datasets using direct volume rendering (DVR). The main advantage of DVR compared to surface rendering lies in the increased amount of information that can be conveyed in one image, as the entire volumetric dataset is used to create the final rendering. Next, a preoperative planning application should offer multimodal visualization of datasets from different imaging modalities such as CT, MRI, fMRI, PET or DSA. Interactive manipulation of the visualization such as simulated surgical procedures, endoscopic views or virtual cutting planes should be available and, finally, an intuitive workflow is necessary, which is integrated into an application framework and ready for use by surgeons or medical staff.</p><p>This paper describes an application for preoperative planning of tailored neurosurgical procedures. Our application consists of a multivolume rendering framework for the concurrent and fused visualization of multimodal datasets (see <ref type="figure" target="#fig_1">Figure 1</ref> for several examples), including three main tasks:</p><p>• Planning of the surgical approach to access the brain, by simulating the skin incision and removal of the cranial bone without any prior segmentation.</p><p>• Visualization of superficial brain areas, including information from additional volumes such as DSA, fMR or PET to provide further insight into the data.</p><p>• Visualization of deep-seated structures of the brain for (keyhole) surgery, by including segmentation information.</p><p>All visualization modules are integrated into a framework that is designed to support surgeons in the task of preoperative planning, including a preprocessing stage for registration and optional segmentation of the different datasets.</p><p>Rendering performs real-time GPU raycasting with perspective projection, in general using a single raycasting pass for 32-bit floating point computations and blending. We employ efficient empty space skipping, early ray termination, and bricking for memory management of multiple volumes. Raycasting is performed through several volumes at the same time, potentially taking into account multiple volumes at a single sample location.  The technical contributions of this paper are:</p><p>• Unified handling of multi-volume raycasting and bricking with and without segmentation masks. For each sample location, the volume to be sampled is either chosen depending on segmentation information, or multiple volume samples are blended. We circumvent GPU memory constraints by bricking each volume (CT, MR, DSA, PET, fMR), and downloading only active bricks into 3D cache textures (one per modality or unified). Segmentation information is represented as a bricked object ID volume over all modalities, which likewise employs a 3D cache texture.</p><p>• Skull peeling for selectively removing structures obscuring the brain (skin, bone) without segmentation ( <ref type="figure">Figure 4</ref>). In contrast to opacity peeling <ref type="bibr" target="#b17">[18]</ref>, we consider registered CT and MR data at the same time for more dependable results. The impact of clipping is resolved consistently. Layers can also be peeled away selectively by painting 2D clipping areas <ref type="figure" target="#fig_7">(Figure 9</ref>).</p><p>• The result of skull peeling is view-dependent. In order to employ it for powerful view-independent clipping, we first generate a view-dependent depth map, which is then transformed into volume space and resampled into a static segmentation mask.</p><p>• Smooth rendering of segmented object boundaries, taking into account the contributions of multiple volumes. In contrast to our earlier work <ref type="bibr" target="#b8">[9]</ref>, we do not propose a general solution, but an approach that is customized for the needs of our neurosurgery pipeline that achieves better results in this case. During raycasting, the precise transition between two adjacent materials is reclassified depending on user-specified iso-values and searching the object ID and data volumes along the gradient direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Preoperative planning for neurosurgery has been an active research topic for several years <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>, with the main focus often on the integration of imaging, registration, and segmentation into a planning workstation <ref type="bibr" target="#b4">[5]</ref>, but often falling short of a high-quality visualization of multi-volume datasets. The virtual tumor resection planning of Serra et al. <ref type="bibr" target="#b20">[21]</ref> uses volume slicing with basic support for multiple volumes. Even though volume slicing approaches achieve a fast and flexible visualization, they usually do not reach the quality of raycasting methods, especially with respect to close-up perspective views. Other approaches employ iso-surface rendering <ref type="bibr" target="#b15">[16]</ref>, or the extraction of 3D contours that can subsequently be blended into a mono-volume rendering <ref type="bibr" target="#b10">[11]</ref>. This, however, is not optimal for versatile preoperative planning as it does not offer the amount of flexibility needed by surgeons for interactive exploration, e.g., changing the transfer function. For rendering multimodal data, several methods have been proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref>. Their key differences lie in the way how the volumes are combined. Different data intermixing levels (e.g., accumulation level, illumination level, image level) and fusion methods (e.g., one or multiple properties per sample) are used depending on the characteristics of the volumes and the desired results. Manssour et al. <ref type="bibr" target="#b13">[14]</ref> use an MRI volume to define the opacity transfer function, while a PET volume determines the color transfer function. Clusters <ref type="bibr" target="#b27">[28]</ref> and specialized volume rendering hardware <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> have also been used. Recently, Rößler et al. <ref type="bibr" target="#b18">[19]</ref> introduced a slice-based multi-volume rendering method to display a template brain along with patient-specific fMR data, including advanced clipping techniques and render modes. All of the above methods, however, do not address the problem of high-quality rendering of segmented multimodal data. High-quality renderings of the human brain from cranial MR scans usually require segmentation. However, this segmentation process, called skull stripping <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>, is not trivial and automatic methods often have problems with noise or require certain MR sequences or scanners. If the brain is rendered without prior segmentation, it is occluded by surrounding tissue of similar intensity values (e.g., skin). Adjusting the transfer function alone, including multi-dimensional transfer functions <ref type="bibr" target="#b11">[12]</ref>, cannot solve this problem. Methods such as opacity peeling <ref type="bibr" target="#b17">[18]</ref> or confocal volume rendering <ref type="bibr" target="#b14">[15]</ref> peel away outer, less important regions of a volume to visualize the inner structures. These methods, however, are hard to use in clinical applications because the visual results are very sensitive to several user-defined parameters. For high-quality rendering of segmented data, object boundaries must be determined at the subvoxel level <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>, mostly using linear or cubic filtering. Tiede et al. <ref type="bibr" target="#b22">[23]</ref> propose a CPU-based method for threshold-segmented objects. They compare the intensity of each sample to the objects in its 2 <ref type="bibr" target="#b2">3</ref> neighborhood to assign the object ID. If the objects have not been segmented via thresholding, trilinear filtering of object masks is used. They also propose to extend their approach to multimodal data. Two-level volume rendering <ref type="bibr" target="#b8">[9]</ref> is a flexible rendering method for segmented data with trilinear object boundary filtering and perobject transfer functions and rendering modes. We build on previous research in the area of multimodal volume rendering, and especially GPU-based raycasting. While first approaches were based on slicing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref>, GPU raycasting is now a viable and very powerful alternative <ref type="bibr" target="#b12">[13]</ref>. The basis for our volume rendering framework is a GPUbased raycaster <ref type="bibr" target="#b19">[20]</ref> (requiring Shader Model 3.0) that achieves interactive frame rates also for large volumes. However, we have extended this raycaster considerably in order to support multiple volumes, segmentation masks, flexible per-object as well as view-dependent clipping, and rendering modes tailored for neurosurgical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WORKFLOW</head><p>For daily use in clinical environments it is crucial for CASP applications (Computer Aided Surgical Planning) to be integrated directly into the clinical workflow. CASP applications should support the surgeon, who usually has a very tight schedule, by offering an intuitive, easy-to-use interface. Therefore, we have integrated our rendering framework as a plugin into the medical workstation and PACS system Impax 6.0 by Agfa Healthcare <ref type="bibr" target="#b0">1</ref> . The complete workflow of our planning tool can be seen in <ref type="figure" target="#fig_2">Figure 2</ref> and contains the following steps: Data acquisition, registration, segmentation, planning of the skin incision and bone removal area, brain surface visualization, and surgery planning for deep-seated structures, tailored to the individual anatomy. The application consists of a preprocessing stage, which was intentionally  Additionally, binary segmented objects can be displayed in high quality and without staircase artifacts by an on-the-fly subvoxel classification algorithm based on raycasting (Section 5.5). During the entire interactive visualization process, tools for further exploration of the data are available to the surgeon, such as MPR (multiplanar reconstruction) views, clipping geometry, or endoscopic views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PREPROCESSING STAGE</head><p>In the registration module, the different multimodal datasets are aligned and resampled. We use an automatic registration algorithm based on mutual information <ref type="bibr" target="#b3">[4]</ref> and perform rigid registration (i.e., only translation and rotation). The algorithm usually converges after a few seconds. If the result is not satisfactory, the user can improve the registration manually by interacting with three orthogonal slice views. The segmentation module implements manual segmentation, thresholding and watershed segmentation based on markers <ref type="bibr" target="#b5">[6]</ref>. Watershed based on markers is a semi-automatic segmentation algorithm where the user has to draw initial markers for the different objects into the volume data. For each voxel its most probable membership to an object is calculated. The algorithm completes within seconds, however, manual refinement of the initial markers is necessary most of the time. The binary segmented objects are saved in an additional segmentation volume that defines the object membership of each voxel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUALIZATION MODULES</head><p>The three visualization modules illustrated on the right-hand side of <ref type="figure" target="#fig_2">Figure 2</ref> constitute the main part of our application. Section 5.1 provides a technical introduction to our multi-volume rendering system, which is the basis for all visualization modules. Specifics of the modules are then described in Section 5.2 for skull peeling in order to perform view-dependent clipping of "uninteresting" parts of a volume, Section 5.3 for multi-volume blending in order to visualize unsegmented data, and Section 5.4 for segmented multi-volume rendering for visualizing segmented objects from multiple modalities. Rendering smooth boundaries of segmented volumes is described in Section 5.5. General interaction tools which support the surgeon in the exploration and planning process are explained in Section 5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multi-Volume Rendering</head><p>A very important issue in multi-volume rendering is how the contributions of different modalities are combined. Our rendering pipeline offers several options that are part of a single consistent framework. Contributions of multiple volumes are combined on a per-sample basis during raycasting. For a given sampling location, either a single volume is sampled and mapped to optical properties via a transfer function, or the contributions of multiple volume samples taken at the same relative location in each modality are blended after separate transfer functions have been applied.</p><p>A single object ID volume guides these choices for combining multiple volume contributions. <ref type="table" target="#tab_2">Table 1</ref> gives an overview of the most important texture types that are used in our implementation. Each voxel is assigned the ID of the segmented object it belongs to (tex objectID), and each object is assigned the volume it belongs to using a 1D mapping (tex volumeID). For example, "bone voxels" would be assigned to the CT volume, whereas "brain voxels" would be assigned to the MR volume. Each object can use its own transfer function, which is determined using its object ID <ref type="bibr" target="#b8">[9]</ref>. The mapping of object ID to volume ID implicitly solves the problem of using different transfer functions for different volumes, as long as each object uses only a single transfer function. Additionally, special blending object IDs known by the raycasting shader are used to indicate that for this object a given configuration of multiple volumes should be blended at each sampling location, where each volume uses its own transfer function and blending weight. We currently support only a fixed number of useful configurations, such as blending the MR and the DSA volume used in the visualization module described in Section 5.3. However, adding additional configurations is easy, and our system could easily be extended to full generality using additional look-up textures. <ref type="table" target="#tab_3">Table 2</ref> illustrates the basic texture look-ups used by multi-volume rendering. The transfer function TF j in <ref type="table" target="#tab_3">Table 2</ref> is determined by the object ID, which either means j = o(x) for regular object IDs, or a set of predefined j's is used by the shader because o(x) is a blending object ID and thus uses multiple transfer functions. Each object can further have its own set of up to six axial clipping planes, whose positions are obtained in the shader by sampling two 1D look-up textures for minimum and maximum (x, y, z) coordinates, respectively ( <ref type="table" target="#tab_2">Table 1)</ref>. Note that segmentation information is not strictly required. If no object ID volume is available, all parts of our pipeline will implicitly assume that all voxels belong to the same default object. In this case, the only option for combining different volumes is blending them.</p><p>Data and Memory Management: Another important challenge in multi-volume rendering is volume data management and coping with memory consumption. We use a bricked volume rendering scheme that subdivides each volume into equally-sized (e.g., 16 <ref type="bibr" target="#b2">3</ref>    layout textures are used for address translation <ref type="bibr" target="#b9">[10]</ref> between "virtual" volume space and actual cache texture coordinates. <ref type="figure">Figure 3</ref> shows an overview of our system. One cache stores segmentation information (an object ID per voxel), with its corresponding layout texture. Each data volume (e.g, CT, PET) has its own layout texture, and either also uses an individual cache texture, or references bricks in a larger unified cache for multiple modalities. Individual caches are less flexible because their size must be chosen at startup, whereas a unified cache allows to change the memory limit dynamically for each modality. However, in this case the GPU must support the texture dimensions required by the larger unified cache. In general, raycasting is performed in a single rendering pass <ref type="bibr" target="#b19">[20]</ref> through "virtual" volume space. At each sampling location, one or multiple address translations are performed, and the sample from each modality is obtained from the corresponding cache texture. Although address translation is a relatively fast process, it can optionally be reduced by using a global cache layout for all volumes. This, however, leads to a higher number of active bricks and thus requires bigger caches, since in this case culling cannot be performed independently for each modality.</p><formula xml:id="formula_0">s i (x) =sample tex volume i(x) , x transfer function TF j (x) =sample tex tf, (s i (x), j)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Skull Peeling -Surgical Approach to the Brain</head><p>This section describes the skull peeling algorithm for directly displaying the unoccluded brain from MR data without the need for prior segmentation. Keeping the requirements for clinical applications in mind, the intention is to reliably visualize the brain without the need of tedious preprocessing or complex user interaction. Naturally, a manual segmentation of the brain would achieve the best visual results, but would require a much longer preprocessing time we want to avoid.</p><p>Our algorithm is based on the idea of opacity peeling <ref type="bibr" target="#b17">[18]</ref>, a viewdependent method for peeling away layers in DVR guided by accumulated opacity. Although opacity peeling quickly generates meaningful images, a major problem for medical practice is its dependency on the threshold parameters. Minor changes of these settings can cause major changes in the resulting images (such as a shrinking or expanding brain). Thus, it is an important goal to improve reliability.</p><p>Skull Peeling: The skull peeling algorithm simultaneously uses the information of registered CT and MR volumes in order to remove areas along the viewing ray which occlude the brain in the MR volume <ref type="bibr" target="#b1">[2]</ref>. While the brain is depicted well in MR scans, CT scans are superior in depicting bony structures with very high intensity values. We exploit this knowledge to decide automatically if a sample lies within a bony area (i.e., the value of the CT dataset is above 1000 Hounsfield units). During raycasting, both the CT and the MR volume are sampled. When the current ray hits a bone for the first time, the accumulated opacity and color values from the MR volume are reset and the ray is advanced until the bony area is exited. At that point, accumulation starts again in order to reveal the brain. This algorithm needs no user input and works well in standard cases where the brain is surrounded by bone (see <ref type="figure" target="#fig_1">Figure 1a)</ref>.</p><p>However, when the brain is not surrounded by bone (e.g., after surgery, or when clipping planes are enabled during rendering) this algorithm would fail. The ray would hit a bone for the first time after traversing the brain and everything in front of that hitpoint would be skipped. We therefore added the following extensions:</p><p>• The position of the first hitpoint of the skin (i.e., first sample with a density higher than air) is saved. If the ray does not hit a bone within a certain number of steps (defined by threshold T 1 ) we assume that there is no bone in front of the brain and use the radiance accumulated from the first hitpoint. • A second extension to the algorithm was made to improve visualization near the skull base. When looking at the brain from Optional Unified Cache <ref type="figure">Fig. 3</ref>: Bricked memory management for multi-volume rendering. No volume is required to be resident in GPU memory in its entirety. Raycasting is performed in "virtual" volume space, with addresses translated to actual cache texture coordinates using layout look-up textures.</p><p>below, along the spinal chord, many small bone pieces occlude the view of the brain. We introduce a threshold T 2 which specifies the minimum distance two bony areas must have in order to assume the area in-between to be brain. If this distance is not reached, the area between these two bone areas is skipped and not rendered.</p><p>Both thresholds have default values that usually work very well and only need to be adjusted for special cases (e.g., looking at the brain from below). <ref type="figure">Figure 4</ref> outlines the standard case of skull peeling and a case where threshold T 1 is needed. Clipped Skull Peeling: Finding the ideal position for the skin incision and subsequent bone removal is important for minimizing the invasiveness of a surgery. For this purpose, we introduce clipped skull peeling to visualize the simulated surgical approach <ref type="figure" target="#fig_1">(Figure 11a</ref>). The user input consists of the surgeon drawing the areas for the skin incision and subsequent bone removal directly onto the volume-rendered image of the head. After skin removal, the skull is rendered with shaded DVR, as this enhances the 3D perception and helps the surgeon to find anatomical landmark points on the skull, which can be used as orientation aides during surgery. The result of clipped skull peeling is generated in three raycasting passes by using the stencil buffer in order to restrict pixels and thus rays to one of three different cases with one specific rendering mode each: (1) Everything outside the specified clipping areas is rendered using unshaded DVR of the MR volume; (2) Inside the skin incision area the skull is displayed (shaded DVR of the CT data), but accumulation of color and opacity is started only after the threshold for bone has been exceeded; and (3) The bone removal area is skull-peeled. The assignment of these three rendering modes to their corresponding pixels is performed as follows: After clearing the stencil buffer to zero, the polygon that was drawn by the user to simulate the skin incision is rendered, increasing the stencil values of the covered pixels to one. Next, the polygon drawn for bone removal is rendered as well, which increases the stencil values of the area to be skipped potential brain area (might be discarded later) brain area <ref type="figure">Fig. 4</ref>: Skull peeling algorithm. The lower ray displays the standard case where the ray hits the skull prior to the brain. The upper ray depicts the case where the brain is not covered by bone. corresponding pixels to two. However, for rendering the bone removal polygon the stencil function is set such that stencil values are only modified in areas where the stencil buffer is already one. This ensures that the bone is only removed in areas where the skin has already been removed. Then, the three raycasting passes outlined above are performed, restricting rendering to the corresponding pixels by setting the stencil function accordingly. Note that this algorithm could easily be extended to more general view-dependent clipping methods.</p><p>View-Independent Skull Peeling: The skull peeling algorithm is inherently view-dependent. This, however, implies that the peeled volume is static with respect to the image plane instead of the volume, and the part of the volume that is peeled away changes whenever the view is changed. We extend skull peeling to a powerful viewindependent approach for volume clipping. During raycasting, a depth image is generated that stores the depths where rays first hit a part of the volume that is not peeled away. In order to generate a segmentation mask that corresponds to the peeled area, each voxel position is transformed according to the view transform into an (x, y) position and corresponding depth z. Comparing the voxel's transformed depth with the depth stored in the depth image at (x, y) determines whether the voxel is included or excluded from the skull peeling segmentation mask, which is equivalent to a voxelized selection volume <ref type="bibr" target="#b24">[25]</ref>. This process is very similar to shadow mapping <ref type="bibr" target="#b26">[27]</ref>. The generated mask allows to switch back to standard volume rendering with segmented masks, toggling the visibility of the peeled area on demand while it stays constant with respect to the volume, even when the view is changed. Therefore, view-independent skull peeling is the first step for multi-volume brain surface visualization (Section 5.3) as it offers an unobscured view to the brain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multi-Volume Blending -Brain Surface Visualization</head><p>Combining multiple modalities in a single rendering can significantly increase the understanding of the actual clinical situation. Surgery at the brain's surface primarily includes tumor resection, epilepsy surgery, and vessel surgery (arteriovenous malformation, AVM). For these cases, after virtually removing the bone cover, the surgeon wants to see the brain surface with additional information such as DSA or functional data (i.e., PET or fMRI). For this task, we employ a visualization approach that combines different modalities without requiring segmented masks. A major motivation for this is that PET data is very diffuse and cannot be segmented well, fMRI data is almost binary, and DSA data can be visualized clearly with a simple ramp transfer function. Also, avoiding the need for segmentation significantly speeds up the preprocessing phase. Therefore, a method that combines multiple volumes based on property fusion is a very good choice for visualizing the brain surface with information from multiple modalities.</p><p>Our algorithm only needs two or more registered volumes as input. It is flexible with regard to the number of volumes that can be visualized concurrently, since our bricking scheme (Section 5.1) makes the approach scalable. Each volume has its own individual transfer function. The volume contributions are combined during raycasting by applying the transfer functions of all volumes at each sample location and combining the classified values. We have implemented different combination techniques ranging from simple linear blending to more specialized combination modes. For blending DSA data with MR or CT, for example, it is sufficient to only display the DSA data whenever its opacity value lies above a certain level. Otherwise, the other volumes are blended and displayed. For visualizing functional data such as PET along with anatomical data, the PET values can be used for color classification while the anatomical data determines the opacity values. <ref type="figure" target="#fig_3">Figure 5</ref> shows examples of multi-volume rendering by blending.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Segmented Multi-Volume Rendering -Deep Lesions</head><p>For thorough planning of surgery on deep-seated processes, after skull peeling and brain surface visualization certain structures need to be emphasized, e.g., a tumor or the optical nerve. This is achieved by a prior segmentation of the structures of interest, and individually applying multiple optical properties such as transfer functions and rendering modes. In the following, we assume that an object ID volume is available, which is the combination of multiple binary segmented objects. Our rendering algorithm for segmented volumes is conceptually based on two-level volume rendering <ref type="bibr" target="#b8">[9]</ref>, which allows to define a separate rendering mode and transfer function for each individual object. However, we use raycasting instead of slicing, which allows for much more flexibility such as the mask smoothing described in Section 5.5. During rendering, the object ID of a sample determines the corresponding rendering mode and transfer function. All 1D transfer functions are stored in a single 2D texture with one transfer function per row. All 2D transfer functions are stored in a single 3D texture. For multivolume rendering, the user additionally chooses a specific volume for each object ID in order to select the modality that depicts the underlying kind of data best. For example, choosing an MRI as underlying data of a segmented bone is not a good choice, since bone is not depicted well by this modality. Using MRI data for the "brain object," and CT data for the "bone object" surrounding the brain, however, is a very good choice. During rendering, a small 1D look-up texture is used to fetch the corresponding volume ID for each object ID. The shader then simply samples the volume texture corresponding to the volume ID of the sample. <ref type="figure" target="#fig_4">Figure 6</ref> depicts examples of multi-volume rendering for segmented data, which also shows that it is possible to specify per-object clipping planes. The clipping plane equations are obtained from two 1D textures, as outlined in Section 5.1, and the shader simply discards fragments that should be clipped. A combination of multi-volume blending with segmented data is also possible, which is determined by special blending object IDs, as described in Section 5.1. In this case, each object can have as many transfer functions as there are volumes, and the result is blended per sample after all transfer functions have been applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Smooth Rendering of Segmented Multi-Volume Data</head><p>The main visual problem of rendering binary segmented objects are the staircase artifacts that appear at object boundaries <ref type="figure" target="#fig_5">(Figure 7a</ref>).  Especially small objects with narrow diameters such as vessels get a ragged appearance with clearly discernable object boundaries of voxel size. For high-quality visualization, the real boundary of the object is needed with subvoxel accuracy. Approaches such as trilinear filtering of object boundaries improve visual appearance <ref type="figure" target="#fig_5">(Figure 7b</ref>) and work for all kinds of segmented objects <ref type="bibr" target="#b8">[9]</ref>. However, trilinear interpolation does not completely remove all artifacts (especially in close-up views), as it takes into account only the binary segmentation information and not the underlying data values. Our approach for smooth rendering of segmented data <ref type="figure" target="#fig_5">(Figure 7c</ref>) is based on the assumption that the object boundary can be described by an iso-value. Values above the specified iso-value belong to the object whereas values below or equal are outside. This works well for segmented vessels as well as other structures of interest in neurosurgery, such as the bone or implanted electrodes. We take advantage of this existing iso-value boundary to adjust the object ID of each sample on-the-fly during rendering. The algorithm works as follows: First, for each object that should use improved smooth boundaries, the iso-value corresponding to its boundary must be specified by the user. Then, for each sample during raycasting, we take a number of steps (defined by a user-adjustable parameter) in the positive and negative gradient direction to check if there is another object nearby. The gradient direction is used since new structures are most likely to appear in the direction of the greatest change of intensity. If a sample in the gradient direction belongs to a different object than the original sample, the original sample is treated as boundary sample, as below. If the sample is not a boundary sample, standard raycasting for segmented objects is performed as described above. For a boundary sample, the following steps are applied: The intensity of the current boundary sample (I cur ) is compared to the predefined boundary iso-value of the current object (Iso cur ) as well as to that of the adjacent object found (Iso ad j ). If the intensity value corresponds to the iso-value of the adjacent object, the current sample is re-classified by changing its object ID (oID cur ) to the adjacent object's ID (oID ad j ). <ref type="figure" target="#fig_6">Figure 8</ref> depicts the different steps of the algorithm, and Equation 1 summarizes the reclassification step:</p><formula xml:id="formula_1">oID cur =</formula><p>oID ad j if I cur &gt; Iso ad j ∧ Iso ad j &gt; Iso cur ∨ (I cur &lt; Iso cur ) oID cur else.</p><p>Tiede et al. <ref type="bibr" target="#b22">[23]</ref> have presented a similar approach based on thresholdsegmented objects and their corresponding min and max threshold values. Their approach, however, only takes into account the eight surrounding voxels of each sample to reassign object memberships, whereas we search a user-defined length along the gradient direction. This gives us the possibility to adapt the boundary to our needs. We can, for example, increase the boundary iso-value of a segmented vessel on-the-fly to show only the interior of the vessel, or lower the isovalue to display the vessel and its vascular hull. The main advantage of our algorithm is that even inexact segmentation masks (e.g., slightly too small or too large masks) can be rendered correctly and with a smooth appearance because the object boundary is adapted to the actual underlying data. When extending this algorithm to multiple volumes one has to be careful to always use the correct volume for iso-value comparison. When comparing the current sample's intensity value to the adjacent object's iso-value, the adjacent object's ID has to be used to fetch the correct volume for getting the intensity at the current sample. <ref type="figure" target="#fig_5">Figure 7</ref> shows the visual result of our algorithm compared to standard rendering of segmented masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Interaction Aides</head><p>Various features have been implemented to support the surgeon in the task of preoperative planning:</p><p>Transfer Function Specification: Colors and opacities are specified over the intensity range in a standard graphical TF editor, in which it is possible to load a set of predefined transfer functions and adapt them manually to the individual dataset.</p><p>Flythrough Navigation / Microscopic &amp; Endoscopic View: The datasets can be explored by flythrough navigation. To simulate the operating microscope the viewpoint for rendering can be set inside the volume and moved around interactively. An endoscopic lens with an adjustable field of view can be simulated by perspective raycasting.</p><p>Slice Views: Next to the 3D visualization window an MPR (multiplanar reconstruction) can be displayed. The MPR consists of three orthogonal slice views (axis aligned) displaying the raw data as well as the segmented objects.</p><p>Integration into a PACS: The whole framework is integrated into Agfa's Impax 6.0 medical workstation. Impax 6.0 offers a plugin interface which allows to extend the basic functionality of the workstation to meet the individual demands of the users. Therefore, by integrating our visualization framework, all other features of the medical workstation (e.g., additional segmentation possibilities, data access) can be used in combination with our neurosurgical planning application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>We demonstrate the usefulness of our application by presenting two distinct planning cases as they were performed by a neurosurgeon. The cases consist of a tumor resection at the frontal lobe near the brain's surface and a tumor resection near the pituitary gland. The first patient underwent CT, MR, PET and fMRI scans, as the tumor was in the vicinity of the motor language area. First the datasets were registered and resampled to have the same volume dimensions (256 3 ). After initial exploration of the datasets (e.g. via skull peeling) and positioning of the patient's head as done in real surgery, skin incision and bone removal were performed tailored to the individual anatomy. Next, the datasets (MR, PET, fMRI) were visualized by multi-volume blending where the MR data depicts the anatomy, PET shows the metabolic active parts of the tumor and the fMRI data shows the brain areas involved in language function, which must be kept intact during surgery. Screenshots from different stages of the planning process are depicted in <ref type="figure" target="#fig_1">Figure 11</ref>. The virtual anatomic structures such as gyri, sulci and blood vessels were found to correlate well with the intraoperative view <ref type="figure" target="#fig_7">(Figure 9</ref>), thus allowing the surgeon to preoperatively plan the resection borders. Concurrent visualization of fMR data helped in identifying critical "no-touch" areas at the left resection border. PET data revealed a focus of high metabolic activity in the right part of the tumor where consequently a separate specimen was taken and sent to histology during surgery, leading to additional irradiation treatment. The second patient had a deep-seated lesion near the pituitary gland and underwent CT, MR and MR angiography scans (dataset size 512x512x164). In this case, after registration, tumor and vessels were segmented by thresholding. Next, the surgeon used our multi-volume rendering for segmented masks to gain insight into the individual anatomy (i.e., position of critical vessels in relation to the tumor). After he had a clear perception of the location of the tumor and other anatomical landmarks he then used the skull peeling algorithm to plan the optimal position of the surgical approach ( <ref type="figure" target="#fig_1">Figure 10</ref>).</p><p>During development we kept a tight feedback loop with the department of neurosurgery at the General Hospital Vienna to iteratively refine the system. The skull peeling algorithm was received very well as it offered a direct view of the brain's surface instantly, without tedious preprocessing. The main drawback is the need of a registered CT dataset which might not always be available. Multi-volume rendering by blending also convinced because of its instant visualization without requiring segmentation and its ease of use. However, the optimal method to blend/combine the different volumes (e.g., linear blending, taking the first volume to define opacity and a second to define color) depends strongly on the type of datasets that are visualized. Therefore, an automatic setting of the combination method depending on the types of datasets could further improve the usability of the entire system. The multi-volume rendering of segmented masks was again perceived as very helpful by the surgeon. A drawback, however, is the amount of parameters that need to be set for each mask individually (assigning a volume to the mask, choosing the rendermode and transfer function, setting the iso-value for smooth object boundaries). Naturally, these parameters offer a very high flexibility for visualizing the datasets, however they also reduce the ease of use of the application. According to the surgeon, the most tedious part of the workflow consists of collecting and registering all the different datasets prior to visualization, especially fMR, PET and DSA data. All things considered, our surgery planning application was very well perceived and is now used almost daily in clinical practice.</p><p>Additional User Effort: In the routine clinical setting, preprocessing has been shown to take an average of 10 minutes <ref type="table" target="#tab_6">(Table 3)</ref>, and initial case setup for visualization up to 3 minutes. After initial exploration, all parameters of a case can be saved in an archived casefile. Case exploration is interactive and re-loading of archived cases takes less than 1 minute. The 3D cases are routinely prepared by young residents and later demonstrated to and discussed with the performing neurosurgeon: For the resident, this has the advantage of teaching and training neuroanatomy of the oncoming surgical approach, for the   advanced surgeon time expenditure is thus very small. The cases are chosen by the surgeons either on the basis of anatomical difficulty, individual variations in anatomy, or simply for the convenience of a preoperative 3D visualization. Performance: All our visualization algorithms run at interactive frame rates. Naturally, the frame rates vary depending on the transfer functions and rendermodes that are used (e.g., unshaded DVR, shaded DVR). <ref type="table" target="#tab_8">Table 4</ref> gives an overview of the frame rates of both case studies for the different visualization methods. Timings are for a Pentium 4, 3.2 GHz with 3 GB RAM and an ATI Radeon X1800 graphics card.</p><p>Multi-volume rendering by either blending or segmented masks achieves the highest frame rates. After activating our algorithm for rendering smooth object boundaries, however, the frame rate drops significantly due to the complex shader for rendering smooth boundaries. This problem can be alleviated by automatically switching back to normal rendering during user interaction (e.g. while rotating the view). The frame rates of the skull peeling algorithm can be explained by the costly branching-statements that have to be performed in the shader to cover all special cases for peeling the skull correctly. On the whole, the framerates were found adequate by the users, since minor viewpoint changes are usually sufficient during preoperative planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Usage:</head><p>The actual memory footprint of our system depends on the cache sizes chosen, and on whether texture dimensions need to be padded to power-of-two dimensions or not. For example, Case Study 2 <ref type="table" target="#tab_8">(Table 4</ref>) can use caches of size 512 2 x128 each for the CT and MR volumes and of size 512 2 x64 for the MRA volume (all 16-bit voxels), and a 512 2 x256 cache for the object IDs (8-bit voxels). This yields a memory consumption of 224MB, which would allow this configuration to be rendered even on a 256MB graphics card. In comparison, the original volumes sum up to 287MB, but if padding to power-of-two dimensions is required (e.g., Radeon cards) would actually consume 448MB of GPU memory, which requires at least a 512MB graphics card. However, it is important to note that our caching scheme works better with larger volumes (e.g., 512 2 x300 and upward), and especially helps to alleviate power-of-two requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>Our multi-volume rendering system for preoperative planning of neurosurgical interventions was directly inspired by the needs of neuro-   surgeons to visualize multimodal data fast, in high quality, and with as little user interaction as possible. The surgical approach to the brain is simulated by interactively removing surrounding tissue such as skin and bone from MR data by making use of additional information present in a registered CT dataset. Further we developed multi-volume rendering techniques that work either purely on the data or include additional segmentation masks. Rendering of segmented objects was improved by an algorithm for smooth rendering of object boundaries.</p><p>To encourage use in daily clinical practice, we integrated our multivolume visualization system into a medical workstation which offers registration, segmentation and interactive data exploration possibilities. In our future work we want to incorporate DTI (Diffusion Tensor Imaging) into our multimodal visualization. Displaying neuronal pathways could further improve the minimal invasiveness and security of neurosurgical interventions. As 3D visualization has become well accepted among neurosurgeons, the next logical step would be to connect our system to a neuronavigation system for tracking of the intraoperative position of the surgeon's instruments. Visualizing the multimodal datasets in parallel to the real surgery could further help the surgeon in identifying structures of interest which are not visible during surgery (e.g. functional areas, optimal skin incision line).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Published 14</head><label>14</label><figDesc>September 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Visualization of the brain without prior segmentation using our skull peeling algorithm. (b) Multi-volume rendering of segmented data (green: tumor -MR, red: vessels -MRA, brown: skull -CT). (c) Multi-volume blending (black/white: brain -MR, red: metabolic active part of tumor -PET, yellow: brain areas active during speech -fMR). (d) Perspective volume rendering for simulating keyhole surgery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Detailed workflow of our neurosurgical planning application. kept as short as possible, and an interactive stage for rendering. After acquisition of the radiological images, the different datasets are registered and segmented directly in the medical workstation before starting the interactive visualization. Segmentation is not mandatory but helps later on by giving additional information during the visualization of deeper structures. Using skull peeling (Section 5.2), the surgeon can take a look at the brain's surface and optionally define the area on the patient's head for the skin incision and ensuing bone removal. The superficial brain can be displayed via direct multi-volume rendering (Section 5.3), showing the brain (MRI) along with other structures such as vessels (DSA), implanted electrodes (CT), or functional brain areas (fMRI, PET). Next, the surgeon may navigate the viewpoint through the keyhole in the cranial bone to access deeper brain structures, using multi-volume rendering of segmented data (Section 5.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Multi-volume rendering by blending different modalities. (a) Blending MR and DSA. (b) Blending MR and fMR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Multi-volume rendering of segmented data. (a) CT and MR data for visualization of implanted electrodes for epilepsy surgery. (b) CT, MR and MRA data for tumor resection planning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>(a) Unfiltered object boundaries. (b) Trilinear filtered object boundaries. (c) Our smooth boundary rendering algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Smooth object boundary rendering. The density of a sample is compared to the iso-values of neighboring objects in gradient direction. The sample is assigned to the object with the closest iso-value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Comparison of a skull peeled image (a) with the corresponding intraoperative view (b). Arrows show points of correspondence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Planning of a right subfrontal approach for pituitary tumor resection. (a) Skin incision. (b) Operating microscope view. (c) Keyhole approach planning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Workflow for planning a tumor resection near the brain's surface. (a) Planning the surgical approach. (b,c,d) Multi-volume blending for visualization of superficial structures. The visualization includes MR (black/white), PET (red) and fMR (yellow and white) data. The PET transfer function shows an area of high metabolic activity within a low grade glioma. The fMR spots delineate areas activated during speech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>or 32<ref type="bibr" target="#b2">3</ref> ) bricks and maintains 3D brick cache textures. For rendering, active bricks of the volume are held in GPU cache memory, and small 3D</figDesc><table><row><cell>Texture</cell><cell cols="2">Dim.+ Type Function</cell></row><row><cell>tex objectID</cell><cell>3D (I)</cell><cell>map sample position to object ID</cell></row><row><cell>tex volumeID</cell><cell>1D (I)</cell><cell>map object ID to volume ID</cell></row><row><cell>tex volume i</cell><cell>3D (I)</cell><cell>texture cache for volume i</cell></row><row><cell>tex clipmin</cell><cell>1D (RGB)</cell><cell>map object ID to clip planes (min)</cell></row><row><cell>tex clipmax</cell><cell>1D (RGB)</cell><cell>map object ID to clip planes (max)</cell></row><row><cell>tex tf</cell><cell cols="2">2D (RGBA) packed 1D textures TF</cell></row></table><note>j for all j</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>obtain ... at x</cell><cell></cell><cell>as</cell></row><row><cell>object ID</cell><cell>o(x)</cell><cell>=sample tex objectID, x</cell></row><row><cell>volume ID</cell><cell>i(x)</cell><cell>=sample tex volumeID, o(x)</cell></row><row><cell>volume scalar</cell><cell></cell><cell></cell></row></table><note>Basic multi-volume rendering textures. Texture type I is single-channel (intensity), and RGB is three-channel, to store three ax- ial clipping plane positions (min or max for x, y, and z, respectively).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Basic multi-volume rendering quantities and texture look-ups.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>User effort for initial case setup (parameter setting).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Frame rates of the visualization methods (viewport 512x512).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.agfa.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This research project was funded by the Austrian KPlus project and by AGFA Healthcare Vienna. The datasets are courtesy of the Neurosurgery Department at the Medical University of Vienna. We would also like to thank Christof Rezk-Salama.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Difficulties of T1 Brain MRI Segmentation Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE Med. Imaging</title>
		<meeting>of SPIE Med. Imaging</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">4684</biblScope>
			<biblScope unit="page" from="1837" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentierungsfreie Visualisierung des Gehirns für Direktes Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolfsberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bühler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Bildverarb. für die Medizin</title>
		<meeting>of Bildverarb. für die Medizin</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="333" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data Intermixing and Multi-Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eurographics</title>
		<meeting>of Eurographics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust and Fast Medical Registration of 3D-Multi-Modality Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Capek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wegenkittl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Medicon</title>
		<meeting>of Medicon</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="515" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imageguided Neurosurgery at Brigham and Women&apos;s Hospital: The Integration of Imaging, Navigation and Interventional Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Archip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">F</forename><surname>Talos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mcdannold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hynynen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Kacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Golby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Jolesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Engineering in Medicine and Biology Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="67" to="73" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Implementation and Complexity of the Watershed-from-Markers Algorithm Computed as a Minimal Cost Forrest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wegenkittl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruckschwaiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eurographics</title>
		<meeting>of Eurographics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Framework for Fusion Methods and Rendering Techniques of Multimodal Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hardware Assisted Multichannel Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Graphics International</title>
		<meeting>of Computer Graphics International</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-Quality Two-Level Volume Rendering of Segmented Data Sets on Consumer Graphics Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization</title>
		<meeting>of IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-Time Ray-Casting and Advanced Shading of Discrete Isosurfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharsach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Buhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eurographics</title>
		<meeting>of Eurographics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal and Multi-Informational Neuro-Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fleig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Seigneuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Morandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scarabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CARS -Computer Assisted Radiology and Surgery</title>
		<meeting>of CARS -Computer Assisted Radiology and Surgery</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="167" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multidimensional Transfer Functions for Interactive Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="285" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Acceleration Techniques for GPU-based Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization</title>
		<meeting>of IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing Inner Structures in Multimodal Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Manssour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Furuie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Olabarriaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIB-GRAPI</title>
		<meeting>of SIB-GRAPI</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Confocal Volume Rendering: Fast Segmentation-Free Visualization of Internal Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mullick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Butman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE -Int. Symp. on Optical Science and Technology</title>
		<meeting>of SPIE -Int. Symp. on Optical Science and Technology</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">STEPS -An Application for Simulation of Transsphenoidal Endonasal Pituitary Surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neubauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolfsberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wegenkittl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bühler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization</title>
		<meeting>of IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive Volume Rendering on Standard PC Graphics Hardware Using Multi-Textures and Multi-Stage Rasterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Graphics Hardware</title>
		<meeting>of Graphics Hardware</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Opacity Peeling for Direct Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eurographics</title>
		<meeting>of Eurographics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GPU-based Multi-Volume Rendering for the Visualization of Functional Brain Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rößler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tejada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fangmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Knauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SimVis</title>
		<meeting>of SimVis</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="305" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perspective Isosurface and Direct Volume Rendering for Virtual Endoscopy Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharsach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neubauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolfsberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bühler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eurovis &apos;06</title>
		<meeting>of Eurovis &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal Volume-based Tumor Neurosurgery Planning in the Virtual Workbench</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Kockro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Nowinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MICCAI</title>
		<meeting>of MICCAI</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1007" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comparison Study of Clinical 3D MRI Brain Segmentation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mensh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Engineering in Medicine and Biology Society</title>
		<meeting>of IEEE Engineering in Medicine and Biology Society</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1671" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High Quality Rendering of Attributed Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Höhne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization</title>
		<meeting>of IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Smooth Volume Rendering of Labeled Medical Data on Consumer Graphics Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vega Higuera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hastreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Naraghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fahlbusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE Med. Imaging</title>
		<meeting>of SPIE Med. Imaging</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive Clipping Techniques for Texture-Based Volume Visualization and Volume Shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="312" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficiently Using Graphics Hardware in Volume Rendering Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Casting Curved Shadows on Curved Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page" from="270" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive Multi-Volume Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Comp. Science</title>
		<meeting>of Int. Conf. on Comp. Science</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
