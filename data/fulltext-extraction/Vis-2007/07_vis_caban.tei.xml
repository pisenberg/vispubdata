<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Texture-based feature tracking for effective time-varying data visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-09-14">14 September 2007.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>IEEE</roleName><forename type="first">Jesus</forename><forename type="middle">J</forename><surname>Caban</surname></persName>
							<email>caban1@cs.umbc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member</roleName><forename type="first">Alark</forename><surname>Joshi</surname></persName>
							<email>alark1@cs.umbc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">IEEE Student Member, and Penny Rheingans IEEE Member</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Texture-based feature tracking for effective time-varying data visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-09-14">14 September 2007.</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2007; accepted 1 August 2007; posted online</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature tracking</term>
					<term>texture-based analysis</term>
					<term>flow visualization</term>
					<term>time-varying data</term>
					<term>visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Analyzing, visualizing, and illustrating changes within time-varying volumetric data is challenging due to the dynamic changes occurring between timesteps. The changes and variations in computational fluid dynamic volumes and atmospheric 3D datasets do not follow any particular transformation. Features within the data move at different speeds and directions making the tracking and visualization of these features a difficult task. We introduce a texture-based feature tracking technique to overcome some of the current limitations found in the illustration and visualization of dynamic changes within time-varying volumetric data. Our texture-based technique tracks various features individually and then uses the tracked objects to better visualize structural changes. We show the effectiveness of our texture-based tracking technique with both synthetic and real world time-varying data. Furthermore, we highlight the specific visualization, annotation, registration, and feature isolation benefits of our technique. For instance, we show how our texture-based tracking can lead to insightful visualizations of time-varying data. Such visualizations, more than traditional visualization techniques, can assist domain scientists to explore and understand dynamic changes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Effective visualization of time-varying data is extremely difficult to achieve due to the complexity of illustrating multiple timesteps and clearly showing changes and variations over time. In time-varying data, each timestep has specific features which domain scientists are interested in visualizing, analyzing, and comparing. Commonly, key features move independently and at different speeds throughout the volume space without following any particular pattern or linear transformation. Such random transformations create a challenging problem when visualizing relative motion, tracking specific features, and illustration volumetric changes over time.</p><p>Domain experts and scientists need to analyze, track, and visualize specific features within time-varying data to better understand the underlying dynamics and build predictive models. Recently, it has been demonstrated that analysis and quantification of changes occurring within the inner eye-wall of hurricanes will permit forecasters to predict sudden strengthening or weakening <ref type="bibr" target="#b17">[18]</ref>. Such research underscores the necessity of developing new techniques that through accurate tracking and extraction of hurricane features will provide more information-rich visualizations of changes occurring within the timevarying data.</p><p>Currently, forecasters and atmospheric scientists primarily visually inspect animations and simulations to analyze hurricane intensity, detect weather changes, and make predictions. However, such visual examinations are limited in terms of analyzing the hurricane changes due to the motion stabilization that occurs in the viewer's eyes. The human eye inherently focuses on broad patterns such as the general rotation of the hurricane while ignoring minor patterns including specific feature paths and changes. This is problematic since minor patterns are extremely important in the analysis and prediction process. An accurate tracking, such as the eye is not capable of accomplishing, of hurricane attributes-such as vortex tubes, clouds, and wind-would greatly enhance current visualization and prediction techniques.</p><p>A widely used method to examine changes between different timesteps is that of applying a voxel-wise difference operation after aligning the data. Among volumes where features move independently and in different directions, however, such difference operations are impractical given the complexity of volume registration. Establishing correspondence between volumes is an important and key problem that must be solved before feature changes can be accurately analyzed. Without accurate volumetric registration, the differences between consecutive timesteps are imprecise and highly dominated by false positive results. Since most of the key features within dynamic time-varying datasets move independently and are mixed with noise or less important features, we cannot assume any particular rigid transformation between consecutive timesteps. Therefore, existing linear registration techniques are not capable of accurately aligning dynamic time-varying data. An alignment technique that offers a possible solution for registering time-varying data is a free-form deformable registration approach <ref type="bibr" target="#b0">[1]</ref>. However, since features within dynamic timevarying data change in shape, intensity, and orientation, deformable registration techniques usually do not converge or the resulting registration is highly dominated by errors.</p><p>Within time-varying data, an accurate tracking of key features is a challenging task due to the overall restrictions and the few assumptions that should be made. First, features of interest are not easily segmented or thresholded from less important data. Second, a specific local feature may not have a corresponding key feature in the consecutive timestep. Third, a particular feature may not have overlap in the next timestep.</p><p>In this paper, we introduce a texture-based feature tracking technique capable of tracking multiple features over time by analyzing local textural properties and finding correspondent properties in subsequent volumes. Tracked objects are used to illustrate changes over time and to suggest volume segments that can be used as landmarks for a deformable volumetric registration. Our texture-based tracking method has been tested with both synthetic and real world timevarying volumetric data. Results with both synthetic and actual hurricane data show the strength of texture-based techniques and demonstrate how tracked features can be used to better visualize, illustrate, and annotate volumetric data for highlighting specific paths, variations, and changes over time. <ref type="table">Table 1</ref>. Run-length-based, 1 st and 2 nd order statistical texture features used to accurately characterize specific textural patterns. The right image shows a set of subvolumes with characteristic textural patterns.</p><p>identify computer vision feature extraction techniques and to extend them from the 2D image domain to the 3D volume domain.</p><p>Since then a number of researchers have proposed other techniques for tracking features within volumetric time-varying data. Silver and Wang <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> presented a feature tracking technique that extracts features, organizes them into an octree structure, and tracks the thresholded, connected components in subsequent timesteps by assuming that all the features between adjacent timesteps overlap. Reinders et al. <ref type="bibr" target="#b9">[10]</ref> proposed a tracking technique that uses feature attributes, such as position, mass, and size, to solve the feature correspondence problem between frames. Ji et al. <ref type="bibr" target="#b6">[7]</ref> introduced a method to track local features from time-varying data by using higher-dimensional isosurfacing. Tzeng and Ma <ref type="bibr" target="#b15">[16]</ref> proposed a technique to extract and track features in time-varying data by using a machine learning module capable of learning from the transfer function which specific features the user wishes to track and visualize and then applying that knowledge to the visualization pipeline in subsequent volumes.</p><p>There are, however, several limitations to existing feature tracking techniques for time-varying volumetric data. First, previous approaches assume a high and sufficient temporal sampling in which features overlap in subsequent timesteps. Second, most existing techniques assume that important features can be easily segmented. That is, the 3D image can be thresholded to find and highlight the specific features to track. Third, noisy volumetric data has not been considered.</p><p>Texture analysis and pattern classification techniques have been widely used for various tasks in the computer vision, computer graphics, and visualization fields. Belongie et al. <ref type="bibr" target="#b1">[2]</ref> used color-and texturebased image segmentation together with the expectation-maximization (EM) algorithm for retrieving similar images from a large image collection. Recently, Guan et al. <ref type="bibr" target="#b3">[4]</ref> used texture-based techniques for categorizing traditional Chinese painting images.</p><p>In the medical imaging field, texture analysis has recently received much attention. Wang et al. <ref type="bibr" target="#b16">[17]</ref> have used it for identifying anisotropic features to improve mammogram image registration. Christodoulou et al. <ref type="bibr" target="#b2">[3]</ref> used texture-based analysis to extract features and accurately classify plaque images. Xu et al. <ref type="bibr" target="#b18">[19]</ref> proposed a 3D texture feature approach to classify and differentiate lung CT images.</p><p>In designing our texture-based tracking technique, we sought to overcome specific limitations found in current feature tracking techniques and to construct a method capable of registering and delivering more effective visualization, illustration, annotation, and isolation of important features over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Our feature tracking technique is based on our observation that timevarying data presents specific pattern and textural features that are characteristic to particular volume segments. These characteristic patterns represent actual dynamic data movement and thus allow tracking of changes in time-varying data to be better illustrated and visualized.</p><p>Initially our system employs a feature selection process to determine which specific features of interest bear tracking. This advanced texture-based tracking, analysis, and correlation approach assigns each feature of interest a characteristic multi-dimensional vector that defines and contains the specific properties of the target features. Those characteristic multi-dimensional vectors that best match our target features are then located in subsequent timesteps, permitting multiple features to be tracked simultaneously over time. These tracked features allow time-varying volumetric data to be effectively visualized, illustrated, and annotated and thus better capture and illustrate dynamic changes occurring over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Texture Analysis</head><p>In computer vision and image processing, textures are fundamental to identifying, characterizing, and comparing objects and regions with similar properties. Image differentiation can be achieved by characterizing similar properties and regular frequencies that occur in the repeated patterns of specific images.</p><p>Through the detection and tracking of textural changes, especially entering, salient, and special patterns, our system can achieve better illustration and visualization of the variations occurring within timevarying data. Our technique uses several textural metrics to identify and track features within dynamic time-varying data.</p><p>We use a combination of first-order, second-order, and run-length matrices to analyze, characterize, and compare textures. The firstorder statistics measure the likelihood of observing a gray value at a random location in the image. Second-order statistics are defined as the likelihood of observing a pair of voxels v 1 and v 2 separated by a distance vector −→ d xyz . Run-length matrices capture the coarseness of the texture.</p><p>First-order statistics contribute six different textural elements to our multi-dimensional feature vector. A frequency distribution or histogram is generated for each feature of interest, and then texture attributes, such as mean, variance, and standard deviation, are computed for the particular subvolume under consideration. <ref type="table">Table 1</ref> shows the six first-order textural features and operations we compute for each feature being tracked.</p><p>To compute the second-order statistics, we use co-occurrence matrices. Second-order statistics measure the probability of a pair of voxels v 1 and v 2 with intensities i and j occurring at some distance −→ d xyz apart. The co-occurrence matrices measure the frequency that a grayscale value appears in relation to another grayscale value on the image. Cooccurrence matrices are defined by</p><formula xml:id="formula_0">C(i, j) = n ∑ p=1 n ∑ q=1 n ∑ r=1 f (x) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1, if I(p, q, r) = i and I(p + Δx, q + Δy, r + Δz) = j 0, Otherwise (1)</formula><p>where I is the n 3 subvolume under consideration and (Δx, Δy, Δz) is the distance vector between corresponding voxels. Our system averages four co-occurrence matrices in the direction of 0, 45, 90, and 135 degrees to guarantee rotational invariance.</p><p>A particular advantage of co-occurrence matrices is that several textural features can be easily computed for free. Haralick et al. <ref type="bibr" target="#b4">[5]</ref> pre-sented 14 texture features, including contrast, correlation, variance, and entropy, that are automatically obtainable from a co-occurrence matrix. <ref type="table">Table 1</ref> lists nine second-order textural features that our technique uses to accurately characterize and compare textures.</p><p>The third texture analysis technique our system uses is run-length analysis, which captures the coarseness of a texture in a given direction. The general idea is to find strings of consecutive pixels that have the same gray level intensity along a specific linear orientation. For a 3D image I, a run-length matrix p(i, r) is defined as the number of pixels of gray level i and run length r along the −→ d xyz direction. Again, to guarantee rotationally invariant attributes, we average four different matrices in the direction of 0, 45, 90, and 135 degrees. Two main features we extract from run-length matrices are long-and short-run emphasis (LRE, SRE) defined by</p><formula xml:id="formula_1">SRE = ∑ N g i=1 ∑ N r r=1 p(i,r) r 2 ∑ N g i=1 ∑ N r r=1 p(i, r) (2) LRE = ∑ N g i=1 ∑ N r r=1 r 2 p(i, r) ∑ N g i=1 ∑ N r r=1 p(i, r) (3)</formula><p>where N g is the number of different gray levels in the image, N r is the number of different run lengths that occur, and p(i, r) the run-length matrix.</p><p>Within 3D time-varying data, run-length comparisons are important since the dynamic changes and random motions of voxels frequently leave behind long runs characteristic of their paths. <ref type="table">Table 1</ref> lists 11 attributes we compute from run-length matrices. The run-length matrices our system incorporates are primarily those identified by Tang <ref type="bibr" target="#b14">[15]</ref> as they proved to be excellent for preserving texture information and improving image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Selection</head><p>Our system's feature selection process first determines what specific features of interest to tracked. Features can be selected through either an automatic or a user-based technique. Given a timestep T initial , the automatic selection process picks all the features within a specific threshold value (e.g. size, brightness) as those for tracking. However, in the interest of flexibility, we also incorporated the option of manually choosing specific features within the volumetric data as those of interest to be tracked. This option is the only feature of our system that requires user input.</p><p>Our system then assigns a characteristic multi-dimensional vector to each feature of interest within the selected feature set F set . For each feature F i ∈ F set , a subvolume S i is computed. Then, a multidimensional vector that defines and contains the specific textural properties is computed for each subvolume S i and becomes a unique identifier for the feature under consideration. <ref type="table">Table 1</ref> shows the 26 different texture attributes our system computes for each subvolume of interest. The table 1 image shows a set of 3D texture blocks obtained from different volumetric data. An important feature of our texture-based tracking technique is that each 3D subvolume receives an individual vector v ∈ R 26 that defines and characterizes its specific texture properties.</p><p>Finally, we addressed the ultimate goal of our multi-dimensional feature vector -the determination of an accurate descriptor for each feature under consideration. Very little can be assumed about features within time-varying volumetric data. Thus, we could not presume that all 26 textural features had the same relevance or that features of interest had unique properties. For instance, there could be situations where specific textural properties, such as intensity, would be shared between the feature under consideration and neighboring voxels that are not a part of the feature of interest. The necessity of weighting our textural feature vectors according to their overall relevance arose from such problems and uncertainties. To maximize the importance of specific features and minimize spurious metrics not truly indicative of differences between features, rather than using the R 26 feature vector employing a uniform weighting value, our system utilizes mutual information to define a different cost for each element of our feature vector.</p><p>Specifically, minimal-redundancy-maximum-relevance (mRMR) is used to assign a weight to each element of the feature vector <ref type="bibr" target="#b8">[9]</ref>. For a given feature of interest, mutual information shared between voxels within the feature under consideration and its neighbors is used to compute the costs per metric, which allows development of a set of textural features that better differentiates one set of voxels from the other. This is accomplished by computing, from each subvolume of interest S i , a new subvolumeS i = S i ∩ S i where S i is a larger subvolume that includes S i in addition to a predefined offset of voxels per axes. Then a weighting value for each of the 26 textural features is derived by computing a texture-based multi-dimensional feature vector forS i and finding in linear time the weighted permutation that increases vector differences between S i andS i while also maximizing feature relevance and minimizing redundancy. For consistency purposes, our system always includes the 26 textual metrics in the final descriptor for each feature of interest; however, a different cost is assigned to each metric.</p><p>After completing the feature selection and weighting function processes, our system begins tracking multiple features simultaneously over time by locating in subsequent timesteps the characteristic multidimensional vectors that best match our target features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Tracking</head><p>Our feature tracking technique incorporates similarity measurements to find the best texture match in subsequent timesteps. To find the best corresponding subvolume, the system first estimates a tracking window and then applies a distance measurement function that compares different subvolumes to find the feature with the closest textural properties.</p><p>The size and location of the tracking window are chosen based on the dynamics observed between different timesteps within the timevarying data. In time-varying volumetric data with high temporal sampling, that is where features of successive timesteps can be found within a given offset of the original feature's location, which need not be overlapping, a simple tracking window shift and resize allow estimation of the tracking space. An accurate tracking window may also be estimated by using the gradient descent of the image to approximate the local affine motion of the voxels and then estimating the direction and offset that a search window should be shifted between two given timesteps. Just such a technique has been used to track features in video sequences and to estimate inverse motion parameters <ref type="bibr" target="#b0">[1]</ref>. Finally, in time-varying volumetric data where nothing is known about its dynamic movements and the intensity variation causes the gradient descent to return erroneous estimations, a significantly large subvolume-or even the entire volume-can be used as the tracking window. However, this last situation should be avoided as both the overall tracking errors and the time required by the algorithm to converge will significantly increase.</p><p>After a tracking window has been estimated, a distance measurement function is applied to find the "best match". The "best match" function BM is defined as:</p><formula xml:id="formula_2">BM(S t i xyz ) = min max x ∑ p=min x max y ∑ q=min y max z ∑ r=min z ED(S t i xyz , S ti+1 pqr )<label>(4)</label></formula><p>where S t i</p><p>xyz is the subvolume under consideration at timestep i, (min x , min y , min z ) and (max x , max y , max z ) represent the estimated tracking window in which the feature could lie and ED(S t i</p><p>xyz , S ti+1 pqr ) is the Euclidean distance function between the feature under consideration and the current location within the tracking window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Effective Visualization</head><p>An in-depth analysis of changes and variations within time-varying data is difficult to achieve with standard visualization techniques. Traditional visualization techniques present several limitations, especially t=1 t=4 t=7 t=9 t=12 in emphasizing time-varying features that move independently and at different speeds throughout the entire volume space. Current visualization techniques that have been enhanced with feature tracking results make it possible for domain experts to better understand, visualize, and analyze the transformations that specific features undergo over time. We present three different ways for a user to visualize specific features of interest.</p><p>The simplest visualization and illustration technique is to create a path that can be annotated by combining the location of each independent feature of interest over time. This is a convenient method for adding additional information and cues to traditional visualizations.</p><p>The second visualization alternative is to view the feature of interest in isolation as it undergoes transformations. To accomplish this, our system visualizes each feature as an individual cutaway for which it can provide snapshots or animation of that feature's transformation over time.</p><p>The third visualization alternative is volumetric illustration, which has proven to be extremely effective. In particular, two-level volume rendering allows the user to visualize a feature within an entire volume <ref type="bibr" target="#b5">[6]</ref> in two specific ways. First, specific segments of the volume can be highlighted or rendered in a way that draws the user's attention to particular features or regions of interest. Additionally, structures and features that surround features of interest can be illustrated for context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS ON SYNTHETIC DATA</head><p>The strengths and limitations of our texture-based feature tracking technique have been tested with synthetic data. We have generated a time-varying dataset with 15 timesteps and a set of objects that independently move within the volume space without following any specific pattern. Each feature within the volume space has a particular pattern or texture used to characterize and identify it. Our synthetic data testing validated that our method worked even when overlap between features in subsequent timesteps did not exist and the shape and size of features did not remain constant. To test our system and its performance in the worst possible cases, random noise was added to the volumetric data, nothing was assumed about the possible motion of voxels, and the entire volume was used as the tracking window. <ref type="figure">Figure 1</ref> shows a screenshot of five different timesteps within our synthetic data. Observe how the two cylinders/circles move, the center triangle rotates, and the upper cube/square changes in size without any specific transformation occurring.</p><p>Our first experiment with the synthetic data was to automatically track three different objects over time. The three objects under consideration moved randomly and in most of the timesteps any of the features overlapped itself in consecutive timesteps. Knowing the center and location of each object of the synthetic data, we were able to measure the accuracy of our tracking results by computing the distance between the resulting location and the "real" position. This experiment demonstrated a 99.43% accuracy on average, thus confirming the strength of our technique. <ref type="figure">Figure 2a</ref> shows tracking accuracy in different timesteps and the errors that occurred over the 15 time steps through which we tracked each feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Background Noise</head><p>To test the accuracy of our technique with noisy data, we added random background noise to the synthetic volume. <ref type="figure" target="#fig_1">Figure 3</ref> shows the synthetic volume before and after a 20% addition of random background noise. After running the texture-based tracking technique five separate times on three individual objects, we demonstrated that background noise has minimal effect on the accuracy of our technique. On average, this experiment demonstrated a 99.20% accuracy in finding the location and centroid of the feature under consideration. The performance and accuracy of our tracking technique augmented with 20% background noise is illustrated in <ref type="figure">Figure 2b</ref>. In this background noise experiment, we also found that actual noise minimally affected features with the textural properties of each remaining almost constant.</p><p>By tracking a set of individual features in synthetic volume space over time, we demonstrated that it is possible to annotate data and illustrate volume to obtain more accuracy regarding specific motions, changes, and movements. <ref type="figure" target="#fig_2">Figure 4</ref> depicts the paths followed by three Noise = 0% Noise = 20%  individual features over the 15 timesteps of the time-varying dataset used. The changes seen over time in line width allow accurate illustration of the specific path each feature follows over time. A primary advantage of our system is its flexibility in relation to annotation of tracks, paths, and changes within volumetric data. Such flexible annotation provides cues and insights about specific changes that visualization techniques and 2D/3D animations currently in use do not provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Texture Noise and Variation</head><p>We further tested the efficacy of our technique with textural noise and variation by adding random noise to the moving features. By inserting random noise, we found that within the texture properties and patterns of individual features variations and changes did occur. <ref type="figure" target="#fig_3">Figure 5</ref> shows the patterns that resulted after 20% of textural noise was added to three features. By running our texture-based tracking technique with percentages of textural noise that ranged from 0% to 30%, we found that our technique has a 25% tolerance. That is, we found that by limiting the maximum tracking error to only one object where the resulting centroid was at most half of the object size away from the "real" location, acceptable limits exist when textural noise and variation is less or equal than 25%. <ref type="figure">Figure 2c</ref> illustrates how the accuracy of our tracking results decreases when textural noise and variation are increased.</p><p>The computed tolerance of 25% should not be taken, however, as definitively binding to our system. By analyzing and filtering input data and by weighting the multi-dimensional feature vector, it is possible to increase the accuracy of our tracking technique even when it   operates within noisy data. Of note, though, is that the specific filtering and weighting functions necessary to secure the overall accuracy of our system with noisy volumes are data and domain specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Time Analysis</head><p>Even with the worst possible scenario of background noise, textural noise, and complete volume (256x256x36) as the tracking window, on average our system required 20.43 seconds (+-1.21sec) to find the three objects in consecutive volumes. It is important to mention that in this worst case scenario experiment, the same dataset and same object were tracked in a fraction of a second by using the underlying dynamics of the volumetric data as explained in Section 3.3.</p><p>To better understand, analyze, and test the time required by our texture-based tracking technique, we extended the synthetic data so that we could measure the time required to find a specific feature within different track windows. <ref type="table" target="#tab_0">Table 2</ref> shows the time required to track a specific feature within different tracking windows. Note that the tracking of features within small tracking windows is efficient and can be done in a fraction of a second. However, when relatively large tracking windows are used, the time required to converge and return the best match significantly increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION DOMAIN: HURRICANE DATA</head><p>Visualization is a crucial step in the process of understanding and analyzing actual hurricane data. The development of our technique has been much enhanced by the knowledge of a domain expert experienced in the study of dynamic changes and variations found in the time-varying data of hurricanes. This expert's ability to compare relative differences of subsequent hurricane timesteps proved extremely important for analysis of changes within the hurricane.</p><p>The primary challenges in visualizing hurricane data are that the features are moving at varying speeds and in different directions. Elements, such as clouds, vortex towers, and rain bands, move throughout the volume space without following any particular patterns. Accurately tracking hurricane features within time varying-data is critical to annotating and illustrating hurricane changes.</p><p>Our results indicate that our texture-based tracking technique provides a better understanding of the internal dynamic changes happening over time within hurricanes. In this section, we discuss the applicability and efficacy of our technique to track, visualize, and analyze hurricane data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Feature Selection</head><p>Generally, time-varying hurricane data are represented by dense arrangements of voxels in which each particle moves independently and at different speeds. We have extended our feature selection technique presented in Section 3.2 to automatically determine features of interest within hurricane data and to estimate a tracking window. Our feature selection approach consists of three steps.</p><p>During the first step, a given hurricane volume V t1 composed of Z images is divided into K groups. For each group or subvolume S i ∈ Z K , the maximum intensity projection image I i is computed. During the second step, each image I i is thresholded and a new image I ti with a different window level is generated. Finally, the derivative (I di ) of each thresholded image I ti is computed and used to determine features of interest.</p><p>The general concept behind our feature selection process is that each thresholded image I ti is comprised of a summary of the features found within each particular segment of the subvolume under consideration. This is, finding specific features of interest within 3D subvolumes is a difficult task due to the amount of information in each subvolume. Similarly, finding features of interest within a single slice per subvolume does not provide enough information to decide which are important features. By computing the maximum intensity projection (MIP) image of the subvolume, our system produces a 2D image that more accurately summarizes and shows the underlying important features within the subvolume. Furthermore, changing the window level and thresholding the image allows us to highlight the specific features that contain the most information. Then, by translating back into 3D space, we have a 3D feature of interest.</p><p>The important features found within the thresholded MIP image are also used to better guide movement of the tracking window. In the particular situation of hurricanes, our technique increases the accuracy with which each feature is detected because of its ability to analyze all three images (I i , I ti , and I di ) before translating the tracking window. <ref type="figure" target="#fig_5">Figure 6</ref> shows the pipeline we designed to automatically locate and select features of interest and to translate the tracking window in hurricane time-varying data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effective Visualization</head><p>We have tested our feature tracking technique with hurricane data from simulations of two dramatic atmospheric events: Hurricane Bonnie and Hurricane Katrina. This data was generated by researchers at NASA and shared with our collaborators in the UMBC Atmospheric Physics Department. To effectively illustrate feature changes and variations over time, we first computed a set of features of interest following the approach presented in section 5.1. The software, then, automatically tracked each feature independently. During the automatic tracking process, the characteristic properties of each feature were updated to compensate for structural variation and changes occurring over time. Since hurricane features can disappear or merge with other neighboring features, a maximum possible difference threshold value was defined between timesteps. If that maximum tolerance was met, the specific feature under consideration was not longer tracked. <ref type="figure">Figure 7</ref> shows results obtained through tracking eight independent features. From these results it is possible to see that most of the features under consideration followed a clear pattern such as moving counter-clockwise and inwards. Furthermore, when the tracking is limited to a particular number of timesteps, it is possible to see that features closer to the hurricane's center moved faster than features farther away from the hurricane's eye. Both of these findings are extremely important in terms of the analysis and interpretation of hurricane data.  <ref type="figure">Figure 8b</ref> shows the path followed by three independent features over the same period of time. After tracking those three specific features within the first 26 timesteps in hurricane Bonnie, we found that features closer to the hurricane's eye rotated 360 degrees while features farther from the center rotated less than 90 degrees during the same time period of 78 minutes. Such results make it is possible to determine the rotational speed differences around the hurricane and to compute a feature's movement and speed towards the interior of the hurricane.</p><p>To better understand the intensification process of hurricanes, our collaborators propose focusing on the change undergone by the hurricane from one timestep to another. In general, the change undergone by a hurricane is has been calculated by taking a voxel-wise difference operation and then visualizing difference. Due to the fact that a hurricane rotates even as the center of its eye moves over time, such difference operations applications are limited in their use. Before a meaningful difference between two consecutive timesteps can be computed, the hurricanes have to be registered. We tested a number of registration techniques, several of which were capable of applying global transformations. We also found that rigid registration techniques lack accuracy due to the dynamic changes and variations that take place within hurricane features and are reflected in data. Also, we found that deformable registration techniques were unable to deal with registering different timesteps due to the uncertainty found within the data.</p><p>Based on our tracking results we determined that instead of registering the entire volume, we should isolate and segment out specific hurricane features and visualize them over time. We consider each target feature as an independent cutaway feature and thus provide the viewer with snapshots, animations, and illustrations of the transformation of these cutaways over time. The dynamic nature of hurricane data makes hurricane features extremely hard to segment and separate. However, from our accumulated tracking information, we can create a subvolume around a specific feature and then segment out each subvolume. Computing that subvolume over the track timesteps allows us to cut away a specific feature and visualize changes and variations over time.</p><p>Illustrative visualization has proven to be extremely effective in a wide range of medical and scientific domains. Our research indicates the specific, particular structural changes and variations occurring over time within each target feature are less clear when visualized than when illustrated. <ref type="figure" target="#fig_7">Figure 9</ref> shows a hurricane feature being tracked over time. After cutting away the specific feature, we can visualize its changes over the first 15 timesteps of the data. We believe that such specific feature segmentation will provide crucial information to domain experts seeking to better understand the dynamics happening between timesteps.</p><p>Additionally, we have applied the two-level rendering approach as a visualization technique in regards to hurricanes. By illustrating the hurricane and rendering the specific feature under consideration, it is possible to draw the viewer's attention to a specific region of interest while also providing the context of the hurricane. <ref type="figure" target="#fig_8">Figure 10</ref> shows an illustration of the eye of a hurricane. Applying boundary and silhouette enhancement techniques <ref type="bibr" target="#b10">[11]</ref> highlights the overall structure of the hurricanes. Furthermore, when we combined illustration techniques with direct volume rendering, it was possible to show the volumetric data of a target feature while illustrating the rest of the hurricane. In particular this method permits, the feature of interest to be visualized with full detail while the rest of the hurricane is visualized in an illustrative manner that provides context around the targeted feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Accurate tracking, visualization, and illustration of specific features within time-varying data still remains an open research topic. Over the last several years a number of researchers have proposed different techniques for tracking features within volumetric time-varying data. However, assumptions made in previous work limited the specific type of data and applications addressed. For instance, the three key observation and assumptions made by Silver and Wang <ref type="bibr" target="#b12">[13]</ref> about overlapping features and the basic assumption made by Ji et al. <ref type="bibr" target="#b6">[7]</ref> about creating and following an isosurface between timesteps can only be applied to data with high temporal resolution and sampling. Our technique is not based on those assumptions and is capable of working with both low-temporal and high-temporal resolution in time-varying volumes.</p><p>A significant advantage of our technique is that it is an extensible approach that can be used across multiple domains and with different data formats. The multi-dimensional feature vector that characterizes each feature of interest has the potential to be extended for even better tracking of particular objects. For instance, we propose that if specific features can be distinguished by a their particular shape, a 3D shape descriptor added to the multi-dimensional feature vector would permit more accurate differentiation and tracking of features over time.</p><p>A limitation of our technique is the "drifting problem" which exists when small errors are introduced to our texture-based multidimensional feature vector over time. As part of our future work, we would like to explore how a statistical update approach similar to the one proposed by Matthews et al. <ref type="bibr" target="#b7">[8]</ref> can be used to reduce such cumulative errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper we have introduced a texture-based feature tracking technique that overcomes some current limitations that hamper the illustration and visualization of dynamic changes within time-varying volumetric data. Experiments and results with synthetic and real-world hurricane data demonstrate the usefulness and effectiveness of our texture-based tracking technique. Furthermore our results show that our texture-based tracking can lead to insightful visualization, illustration, annotations, and segmentation of specific features over time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Five different timesteps within the synthetic data. Plots showing the accuracy of our tracking technique. (a) Accuracy in volumes without background noise. (b) Accuracy in volumes with 20% background noise. (c) Accuracy decrease as we increase textural noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Original volume (b) Resulting volume after a 20% background noise was added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>From the tracking results it is possible to illustrate the path followed by any independent features and to show the actual direction by growing the line's width over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Patterns before and after a 20% of textural noise and 5% background noise was added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>A thresholded image of a maximum intensity projection image of a subvolume under consideration is used to find features of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Resulting path after tracking a number of independent features over time. (a)Results of tracking a hurricane feature within 30 timesteps. (b) Path followed by three independent features over the same period of time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Visualization and illustration of a specific hurricane feature over time. Our technique provides the flexibility to track, isolate, and illustrate specific feature changes and variations over time. The set of 2D images show the resulting thresholded MIP images used to better estimate the tracking window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Direct volume rendering of the hurricane feature under consideration within the illustration of the overall structure of the hurricane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Size</cell><cell>Tracking Window</cell><cell>Tracking</cell></row><row><cell>(Subvolume)</cell><cell>Size</cell><cell>Time (sec)</cell></row><row><cell>10 3</cell><cell>10 3</cell><cell>0.000173</cell></row><row><cell>10 3</cell><cell>20 3</cell><cell>0.186</cell></row><row><cell>10 3</cell><cell>32 3</cell><cell>1.613</cell></row><row><cell>10 3</cell><cell>48 3</cell><cell>6.657</cell></row><row><cell>10 3</cell><cell>64 3</cell><cell>14.800</cell></row><row><cell>20 3</cell><cell>20 3</cell><cell>0.00107</cell></row><row><cell>20 3</cell><cell>32 3</cell><cell>2.248</cell></row><row><cell>20 3</cell><cell>48 3</cell><cell>22.048</cell></row><row><cell>20 3</cell><cell>64 3</cell><cell>66.351</cell></row></table><note>. Performance of our texture-based feature tracking technique.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank the anonymous reviewers for their comments and suggestions. We would like to thank our collaborators, Lynn Sparling and Sam Trahan of the UMBC Physics Department, for a fruitful partnership, Scott Braun from NASA for providing hurricane data, and Rosemary Klein for her help and support in writing this paper. This work was supported in part by NSF grant 0121288.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Equivalence and efficiency of image alignment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1090" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Color-and texturebased image segmentation using EM and its application to content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Computer Vision</title>
		<meeting>the Sixth International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="675" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Texture-based classification of atherosclerotic carotid plaques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Pattichis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantziaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaides</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-07" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="902" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic categorization of traditional chinese painting images with statistical gabor feature and color feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lectures in Computer Science</title>
		<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="743" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Textural features for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="610" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Two-level volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Bischi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Volume tracking using higher dimensional isosurfacing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;03</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The template update problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2004-06" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="810" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information: criteria of max-dependency, max-relevance, and minredundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attribute-based feature tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J W</forename><surname>Spoelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Visualization &apos;99</title>
		<meeting><address><addrLine>Wien</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Volume illustration: Non-photorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visualizing features and tracking their evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samtaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zabusky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>IEEE Computer Society Press</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Volume tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;96</title>
		<editor>R. Yagel and G. M. Nielson</editor>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tracking scalar features in unstructured datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;98</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Texture information in run-length matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions of Image Processing</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1602" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intelligent feature extraction and tracking for large-scale 4d flow simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Supercomputing Conference</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic registration of mammograms using texture-based anisotropic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2006 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<meeting>2006 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="864" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Atmosphere: Forecasting hurricane intensity and impacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Willoughby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Science</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="page">1232</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MDCTbased 3-D texture classification of emphysema and early smoking related lung pathologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mclennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Hoffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="464" to="475" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
