<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IStar: A Raster Representation for Scalable Image and Volume Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-10-27">27 October 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Kniss</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Hunt</surname></persName>
							<email>whunt@cs.unm.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Potter</surname></persName>
							<email>kpotter@cs.utah.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Sen</surname></persName>
							<email>psen@ece.unm.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IStar: A Raster Representation for Scalable Image and Volume Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-10-27">27 October 2007</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2007; accepted 1 August 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Topology</term>
					<term>Compression</term>
					<term>Image Representation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Topology has been an important tool for analyzing scalar data and flow fields in visualization. In this work, we analyze the topology of multivariate image and volume data sets with discontinuities in order to create an efficient, raster-based representation we call IStar. Specifically, the topology information is used to create a dual structure that contains nodes and connectivity information for every segmentable region in the original data set. This graph structure, along with a sampled representation of the segmented data set, is embedded into a standard raster image which can then be substantially downsampled and compressed. During rendering, the raster image is upsampled and the dual graph is used to reconstruct the original function. Unlike traditional raster approaches, our representation can preserve sharp discontinuities at any level of magnification, much like scalable vector graphics. However, because our representation is raster-based, it is well suited to the real-time rendering pipeline. We demonstrate this by reconstructing our data sets on graphics hardware at real-time rates.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In visualization and computer graphics, we are often interested in rendering multivariate functions parametrized over 2D (e.g. RGB images) or 3D (e.g. multimodal volume data) that contain important discontinuities which represent the boundaries between different materials (e.g. skin and bone, gray matter and white matter, etc.). These discontinuities can be preserved during rendering by representing them with implicit compositions of simpler functions. In 2D, for example, we can do this with vector graphics (e.g. Postscript <ref type="bibr" target="#b24">[25]</ref>) where the function is represented by mathematical primitives such as circles, lines, and polynomial curves. In 3D, this process is performed by a standard 3D renderer, which generates images from an implicit composition of mathematically-defined primitives such as spheres, polygons, etc. These approaches allow for scale-invariant reconstruction of the original function while preserving important discontinuities. Because these representations encode individual primitives, they also provide a mechanism for attaching semantic meaning to the different primitives which is useful for visualization (e.g. the ability to shade a specific kind of object, such as bone, with a new color or to remove it altogether from the rendering process).</p><p>However, these geometry-based formats have several serious drawbacks. For one, they are too slow for rendering complex data sets at real-time rates. The inherent problem is that their rendering complexity grows linearly with the number of primitives and is potentially unbounded. This linear dependence makes it difficult for rendering systems to keep up with the real-time rates, especially when these images are used to texture map surfaces in a scene. In addition, it is not clear how to apply an arbitrary filter kernel to these implicit representations, like when pre-conditioning the signal for antialiasing.</p><p>Another way to represent these functions is with a uniformly sampled representation, such as raster or bitmapped images for 2D data sets and 3D textures for volume data sets. The advantage of raster images is that they require a known, constant time to evaluate the function at any point because it can be done with a fixed number of accesses into an array (enough to support the reconstruction kernel) plus some bounded computation to execute the kernel itself. This explains the popularity of using 2D bitmaps for texture mapping surfaces in inter-active applications. In addition, operations such as convolution with a Gaussian kernel are well defined on raster data sets, which makes antialiasing raster images relatively simple. However, a significant drawback is that raster images must be band-limited to eliminate aliasing because they are, by definition, sampled representations. This means that important semantic information present in our original function, such as the sharp discontinuities between different materials, can be lost during the sampling process.</p><p>In this paper, we propose a new framework for representing functions with discontinuities efficiently in a raster format which we call IStar. This representation shares many of the advantages of both vector and raster formats. First, our approach allows discontinuities to be reconstructed precisely, independent of scale as can be done with vector formats. Second, the rendering algorithm is simple and its constant-time complexity is independent of the number of primitives in the IStar image. This allows us to render IStar images on graphics hardware and use them to texture map scenes at real-time rates. Third, IStar's raster representation can be compressed using conventional techniques, resulting in a data structure that is more space-efficient than traditional bitmaps yet can still reconstruct the sharp discontinuities of the original image. Fourth, because our structure preserves the semantic meaning of regions in the image, we can easily modify their characteristics during rendering. Finally, since our structure encodes information on the topology of the data, we can use it enforce constraints or fix classification errors, which is useful for visualization applications.</p><p>We begin the paper with an overview of the 2D version of our algorithm in Section 2 to give the reader with an intuitive understanding of our approach. In Section 3, we introduce a topology framework that we will use to describe the algorithm more formally in Section 4. In Section 5, we discuss details important for implementation of the technique, and compare it to previous work in Section 6. Section 7 presents our results and we conclude in Section 8 with future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of Algorithm</head><p>The IStar encoding process takes as input an image function with regions of constant color, which we call the primal image. The top row of <ref type="figure" target="#fig_1">Figure 1</ref> shows an example of the encode process with an original 2D image (a) and the primal image with uniquely defined regions (b). This primal image can be used to generate a graph-like structure, called the dual complex, whose nodes represent the unique regions and edges represent the boundaries between regions, as seen in (c). The next step is to embed the dual complex in a color space, (d), and relabel (recolor) the primal image based on the coordinates of the node positions in this new space, (e). Note that disjoint regions with the same color in the primal image are mapped to different nodes in the dual complex and are therefore embedded in different positions of the color space, giving the relabeled image a distinct color for every  unique region. This labeling helps overcome artifacts introduced during the compression process and allows regions to be uniquely identified during decoding. The relabeled image function is then sampled and compressed (e.g. through downsampling), resulting in a miniature bitmap representation, (f). At this point, the colors of the downsampled bitmap can be tweaked to improve reconstruction quality, an optimization which is discussed in detail in Section 5. The node and connectivity information are encoded by storing the coordinates of each node and a list of edges, (g), and because the original color is lost during relabeling, a color palette that associates each node with its color in the original image is generated. The downsampled bitmap, the graph structure and the color palette forms the complete IStar structure, (h). The decode process, <ref type="figure" target="#fig_1">Figure 1</ref> bottom row, takes as input the compressed bitmap, (i), and upsamples it using a reconstruction filter to create a blurry version of the relabeled image, (j). Note, the histogram of the upsampled image follows the structure of the dual complex, as seen in (k) and (m), because of the way the positions of the nodes were used to determine the color of the relabeled image in encoding. Sample values along the boundary of two regions are a linear combination of the colors of the regions and map to the line segment that connects the two nodes of each region, as demonstrated by the three samples in (l) mapping to the line in (n). In the case where three regions share a common boundary at a point, upsampling results in sample values between all three nodes, thereby mapping to points inside the triangle formed by the nodes in histogram space. Using the positions of samples within the dual complex, the upsampled image is "cut" into unique regions, and combined with the original color palette to recolor the image and produce the final, reconstructed image, (o).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Comparison to Previous Work</head><p>Topology is a field of mathematics that deals with abstract manifolds and their characteristics, such as interrelationships, canonical topological forms, algebraic operations, and mathematical definitions of shape. A solid introduction and reference for basic foundations of algebraic topology can be found in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. The application of topology to general scientific problems is surveyed by Dey et al. <ref type="bibr" target="#b1">[2]</ref>. In general, topological analysis focuses on the characteristics of a single manifold in isolation. In contrast, our work analyses the characteristics and em-bedability of multiple intersecting manifolds and their duals. A result of this work is the development of an image function that maps from the primal complex to its embedded dual.</p><p>Digital topology refers to image processing methods that apply the concepts of mathematical topology to the processing and understanding of digital images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> by looking at the shape induced by binary images in the integer domain Z. The adjacency graph captures the topology related to manifold intersections and is often use in vision and object matching applications <ref type="bibr" target="#b16">[17]</ref>. In contrast, our work develops image topology for continuous image and data domains, and rather than using an adjacency graph, uses a full dual CW-complex for adjacency information, which is essential for reparametrizing the image.</p><p>The process of identifying salient topological features in gray-scale (scalar) image data has generally relied on several techniques: skeletonization, Morse-Smale complex identification and simplification, and Reeb graph identification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>. Though the work of <ref type="bibr" target="#b0">[1]</ref> does not explicitly develop a topological framework, it does apply a barycentric interpolation over simplicial complexes to extract geometry from a volume faction data. Topology has also played an important role in the analysis of vector flow fields <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>. The survey on computational topology for shape modeling by Hart <ref type="bibr" target="#b5">[6]</ref> covers many common and useful applications of topology in computer graphics. Our work extends scalar image topology methods by developing a framework for multi-variate image topology and techniques for creating images (encodings) that preserve topological characteristics by construction.</p><p>Most approaches to represent functions with discontinuities for rendering are based on implicit representations that composite primitives with known mathematical properties. However, the representation of discontinuities in raster-based approaches have recently become an active research. Sen et al. <ref type="bibr" target="#b23">[24]</ref> provide an excellent survey of current approaches. The methods of Sen <ref type="bibr" target="#b22">[23]</ref> and Tumblin and Choudhury <ref type="bibr" target="#b26">[27]</ref> are examples of methods that explicitly handle boundary representations as geometric primitives tied to a pixel representation that render in real-time on graphics hardware. However, unlike our approach, they simplify curved features to be piece-wise linear which can produce artifacts when not properly sampled. The methods of Ray et al. <ref type="bibr" target="#b20">[21]</ref> and Loviscach and Bremen <ref type="bibr" target="#b15">[16]</ref> utilize an implicit representation of surface discontinuities, which are effectively level-sets of smooth scalar functions. The work of Ray et al. captures cusp discontinuities as the intersection of multiple boundary functions. Loop and Blinn <ref type="bibr" target="#b14">[15]</ref> describe a framework for evaluating bounded regions represented as Bezier splines, which is well suited for hardware accelerated raster graphics applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topological Framework</head><p>This section covers the mathematical foundations of this work. First we define the CW-Complex, which forms the basis of our representation. Next, we present a new topological description of images based on intersecting regions. We use this definition to analyze and extract the topology of an image's data-space, or histogram domain. This information can then be used to reparametrize the image so that unique pixel values identify unique regions, even when the image is been band-limited. The process of reparametrization and rendering are covered in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The CW-Complex</head><p>A CW-complex is a fundamental construct from topology. The "CW" stands for closure-finite weak topology. CW-complexes are like generalized graphs with nodes (termed 0-cells) and edges (1-cells), however they can also contain surfaces (2-cells), volumes (3-cells) and so on. of χ, or the set of all points in the complex. A subscript will indicate a particular element of the skeleton. CW-complexes are useful because they capture both the concrete geometric and abstract algebraic/combinatoric characteristics of manifolds. For a more complete description of CW-complexes and their properties, see Hatcher <ref type="bibr" target="#b6">[7]</ref> Appendix, page 519.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Primal CW-Complex</head><p>Assume for a moment that objects in the real-world are spatially distinct, i.e. they do not overlap except at infinitesimally thin boundaries. Objects are things that are semantically different like air, skin, softtissue, and bone. A region of vacuum would still be considered an object since it is semantically distinct from the others. Then, for some finite volume of space D ⊂ R d , we say that the N objects A i that occupy this space fill the entire volume:</p><formula xml:id="formula_0">D = N i=1 A i .</formula><p>Formally, we state that the union of objects forms a closed cover of the compact set D.</p><p>From these objects, we can construct a CW-complex χ which we call the primal complex. <ref type="figure" target="#fig_2">Figure 2</ref> (top row) illustrates the primal complex for a 3D image. For a d-dimensional image, the primal complex is defined as χ = {χ q |q = 0,...,d}, where</p><formula xml:id="formula_1">χ q = ⎧ ⎨ ⎩ d+1−q j=1 A i j = ∅ ⎫ ⎬ ⎭ .<label>(1)</label></formula><p>Equation 1 indicates that lower-order q-skeleton elements are formed by object intersections. For 3D volume data, 3-skeleton elements are the objects themselves, 2-skeleton elements are formed by the intersection of two objects, 1-skeleton elements by the intersection of three objects, and 0-skeleton elements by the intersection of four objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Dual CW-Complex</head><p>Given an image of N primal objects, assume we have an algorithm that automatically segments the image and produces N binary images, each of which identifies a unique object with a value of 1 if the object is present at a pixel, 0 otherwise. We can stack these images together to form a single image with an N dimensional vector at each pixel. We call this vector the "indicator vector" since it indicates which object is present at each pixel. We can treat the discrete image as a continuous function, I v : D → R N , by convolving with an interpolation kernel k:</p><formula xml:id="formula_2">I v ( x ∈ D) = D k( x − s) N ∑ i=1 δ ( s − p i )V i d s,<label>(2)</label></formula><p>where the V i is the vector at pixel p i , and δ is Dirac delta function. What structure do the image values have with respect to R N , the data domain? To answer this question, consider what happens to the interior of a segmented object A i . In the data domain, points that are not near the object's boundary map to a single point in R N , i.e. they all have the same value because the samples within the support of the kernel are all equal. Points near the boundary between two objects, however, take on values that are linear combinations of the values of the two objects, due to the blurring effect of convolution-based interpolation. Therefore, these values map somewhere on the line connecting  the values of the objects in the value domain. Points near the region formed by the intersection of three objects will have values that are linear combinations of the three object values, therefore mapping to the triangle with vertices at the values for each object. Similarly, locations near the region formed by the intersection of four objects will have values located within a tetrahedron in the value domain. This structure (points, lines, triangles, tetrahedrons) is the dual complex, χ * , of the primal, and is defined as χ * = {χ * q |q = 0, 1, 2, 3} where χ * q = χ 3−q . In other words, 3D cells in the primal skeleton become points in the dual, 2D faces become lines, 1D lines become 2D faces, and points become 3D cells. <ref type="figure" target="#fig_2">Figure 2</ref> (bottom row) shows χ * embedded in 3D. We can now see how the dual graph structure described in Section 2 is the dual complex of our primal image. Because our structure encodes the dual of an image I, we refer to it as I * or IStar. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the image structre in both the primal and dual spaces. A small portion of the dual complex is shown in each image for reference. Notice that the histogram in <ref type="figure" target="#fig_3">Figure 3</ref>[E] shows the structure of the embedded dual complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Embeddings of χ and χ *</head><p>By definition, the primal complex χ embeds in D, χ → D. A complex embeds in a space if the uniqueness of open sets (elements of the skeleton minus their boundary) is preserved. For example, any point in D is covered by a unique object A i or is shared by multiple A i 's only if it is in the boundary formed by them, as described by Equation 1. χ * embeds in R N when each of the N nodes are set on unique axes of the space. In fact, χ * embeds in an N − 1 dimensional subset of R N .</p><p>While χ * embeds trivially in R N , it can be shown that with a few additional constraints, χ * is embeddable in a compact subset of R d , where d is the dimension of the image <ref type="bibr" target="#b8">[9]</ref>. The constraints are:</p><formula xml:id="formula_3">(i) either m j=1 A i j = ∅ or codim m j=1 A i j = m − 1 (ii) A i</formula><p>is simply connected (iii) A i can intersect itself, but only along its boundary Condition (i) guarantees that the subjects will only intersect in stable, or generic, configurations. For a 2D image, the generic intersection types are 2-way, where two objects meet at a line, and 3-way, where three objects meet at a point. For a 3D image, the generic intersections are 2-way, where two objects meet at a plane, 3-way, where three objects meet at a line, and 4-way, where four objects meet at a point. Generic intersections are those that cannot be destroyed when a subject is moved some infinitesimal amount. Non-generic intersections, greater than N+1-way, can be fixed by "blowing-up" the intersection, which introduces a new subject as seen in <ref type="figure" target="#fig_4">Figure 4</ref>. Condition (ii) requires each subject to be strictly simply connected, i.e. disconnected regions and holes are not allowed. When a subject is composed of multiple disconnected regions, we can make each simply-connected region a separate subject. Condition (iii) allows subjects with holes to be fixed by linking disconnected boundary segments, as seen in <ref type="figure" target="#fig_5">Figure 5</ref>. Combined, (i-iii) are sufficient conditions for defining a topological CW-complex which uniquely represents the subjects and their boundaries and its dual. Later in this paper, we will discuss how constraints (ii) and (iii) may not always be necessary when we embed χ * in a higher dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Algorithm</head><p>In this section we describe the algorithms for embedding and decoding of IStar images. Together, the embedded dual complex and reparametrized values form an encoding of the primal complex that preserves the uniqueness of objects and their boundaries. Discontinuous boundaries are implicitly represented in the encoded image and uniquely identified by their parametrized value. We will use this fact to reconstruct the original image with its discontinuities at any resolution from a raster image. We will also show that the encoded image size can be reduced substantially without significantly impacting the quality of the reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Encoding</head><p>The input to the IStar encoding algorithm is an image function, I v , which can be obtained from a data set, for example an image in 2D or a volume in 3D. For 2D data sets, I v is a high resolution sampled image of at least 2000 × 2000 pixels. While it is possible to use geometric representations as input, it is easier to work with high resolution raster images. Note that this does not compromise generality since vector representations can be always be rasterized. For 3D data sets, we use a geometric representation and simply evaluate the indicator vector at any point in the spatial domain. To ensure dual embedability, disconnected objects in the original image may need to subdivided into individual objects, which can be done by re-tagging the image using a flood fill. For ease of explanation, the discussions in the paper are restricted to 2D and 3D, regularly sampled data sets; however, the algorithm and math are general with respect to dimension, and can be used with irregularly sampled data.</p><p>The following steps are needed to encode: 1) Identify and refine primal complex. Make each connected component of each object an independent object, and create "ghost" 0-cell elements for all n-way intersections greater than d + 1 where d is the dimension of image.</p><p>2) Identify dual complex. Each object becomes a 0-cell, any pair of objects co-located within a differential volume become a 1-cell, and so on.</p><p>3) Embed dual in a compact space. Use a graph layout solver to embed all 0-cell positions in the data space. 4) Initialize encoded image. For each pixel in the encoded image, assign its value to the embedded dual 0-cell position associated with the object at that spatial location in the original image. 5) Optimize. Using variational minimization, make the encoding reproduce the original image with minimal error. The first step in encoding an image is to analyze the high resolution input to determine the primal complex. Objects and intersections are found and classified. When (d+1)-way intersections are found, they are preserved by inserting a "ghost" 0-skeleton element in the dual; however, a new region is not inserted into the primal complex. Rather, the ghost nodes are effectively ignored during decoding and the attribute assignment goes to the next closest 0-skeleton element. This ensures that these non-generic intersections are not lost or degenerate. An example of inserting a ghost element in 2D can be seen in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>The dual complex is found by iterating over locations in the primal image, inspecting neighborhoods around each sample for differences in objects id's. N-way intersections are recorded by introducing the appropriate skeleton element into a simple data structure which maintains lists for object id's, embedded position X i , indices into other skeleton lists, and associated attributes such as color.</p><p>Reparametrizing the data into a lower dimensional value space involves embedding the dual complex in a space R E , where E is the dimension of the target value space greater than or equal to the image dimension d. In particular, we are interested in a straight-line embedding in which the elements of the q-skeleton are parametrized as linear combinations of the elements in the 0-skeleton. For example, the elements of the 1-skeleton are lines (not curves). Although we can show that the dual complex is embeddable in a space of dimension E = d, this embedding may require curved q-skeleton elements. With an additional dimension, however, most skeletons can be straight-line embedded.</p><p>Embedding can be accomplished using a modified, force-directed graph solver which is adapted to handle CW-complex embeddings by introducing charged-particle forces for faces (dual 2-skeleton elements) and volumes (dual 3-skeleton elements). This is done by adding extra nodes at faces and volumes. These nodes are constrained to the center of their respective faces and volumes and are there only to force 0-skeleton nodes ( X i ) away from these regions.</p><p>For 2D images it can be shown with a straightforward proof that a 3D target space (equivalent to the common color image value space of RGB) will always be sufficient for embedding. Ignoring elements in the 2-skeleton, the 0 and 1-skeletons form a planar 2D graph, which is straight-line embeddable in 2D <ref type="bibr" target="#b2">[3]</ref>. The straight-line graph can be mapped to a plane in R 3 . Now, move all 0-skeleton elements that intersect the interior of a 2-skeleton element some distance normal to the embedding plane. Continue moving those who still intersect 2skeleton interiors until all intersections are removed. We know that intersecting sub-complexes must be nested; if they are not, they would have overlapping edges, which have already been resolved by straightline embedding the graph. Therefore, each lifting step will free at least one 2-cell. So, for finite complexes, this process will terminate and no intersections will remain.</p><p>For volumetric data, we default our embedding to a 4D value space (i.e. RGBA). It is easy to detect a false embedding by traversing the embedded elements of the skeleton, decoding their attribute vectors, and identifying any incorrectly assigned attributes. In practice we have never needed more than a 4D embedding for volumetric data, however it is possible to construct pathological cases where each object shares a boundary with every other object in the spatial domain. However, even these situations can be remedied by subdividing the objects.</p><p>Now that we have computed and laid out our dual complex, we are ready to start creating the encoded image. We initialize the encoded image by assigning values from the indicator vector. If we let X i be the embedded position of the 0-skeleton element χ * 0 i in the target space R E and v be a pixel value, then the parametrized value</p><formula xml:id="formula_4">v is v = N ∑ i X i v i ,<label>(3)</label></formula><p>where v i is the i th element of the value vector v. When the indicator vector has a single 1 at position k and all other elements 0, we get v = X k , which basically says that we relabel our encoded image using the embedded position of k th 0-skeleton element χ * 0 k . For 2D images, the initial encoded image is the same size as the original raster image and we initialize every pixel of it with the position of its corresponding dual-embedded 0-skeleton element. The downsample process that follows will produce a smoothed, or band-limited, encoding. In a similar way, we initialize the encoded image for 3D volume data by assigning to each sample the position of the embedded dual 0-skeleton element associated with the object at that location, except that in this case we do not get this information from a raster data set but by directly evaluating the function using the 3D geometry. It is also possible to blur the resulting encoded image to enforce smoothness. When the attribute is smooth (C 0 continuous or better), for example with distance transforms or posterior probabilities, the initial values can be weighted sums of the the embedded dual 0-skeleton elements.</p><p>Once the encoded image has been downsampled, we can choose to tweak the sample values in order to yield a more accurate reconstruction. This process can be described as a more general optimization problem. If we define a "decode" function γ : R E → R N that takes the encoded values and returns them to the discrete colors of the original image, then the encode step described by Equation 3 can be refined using a variational minimization of</p><formula xml:id="formula_5">ε({ v i |i = 1 ...N}) = D |I v ( x) − γ(I e ( x))| 2 d x,<label>(4)</label></formula><p>where I v is the original image function, I e is the encoded image function, and | ...| 2 is the L2 norm. The characteristics of the encoding depend entirely on the choice of the original image function I v , decode function γ, and interpolation kernel k. The goal of this optimization is to reproduce the original image function with minimal error. Note that neither I v nor γ(I e ) are C 0 continuous functions. Therefore, we cannot take derivatives of Equation 4 and solve it as a linear system. However, we can minimize Equation 4 with direct search methods <ref type="bibr" target="#b10">[11]</ref>, which can robustly and efficiently minimize discontinuous functions. When the original image function is represented as a high resolution rasterized image, the integral in Equation 4 becomes a summation,</p><formula xml:id="formula_6">ε({ v i |i = 1 ...N}) = M ∑ j=1 V j − γ(I e (P j )) 2 ,<label>(5)</label></formula><p>where V j is a pixel value (an indicator vector) for object j at pixel location P j in the original image, v i is a pixel value in the encoded image, and M is the number of pixels in the rasterized original image. The direct search optimizer, also known as "pattern search" <ref type="bibr" target="#b10">[11]</ref>, works by modifying the unknowns v i incrementally, accepting changes that lower the error ε in a greedy manner. Since we typically use interpolation kernels with finite support, we only need to compute the error summation for the region of the original image covered by the kernel support for that sample's location in the encoded image. One important consideration for optimization is the precision of the target image. Although solvers typically operate on real-valued types (float or double), quantization to fixed precision types (such as 8-bit char) can affect the quality of the encoding. We have found it valuable to include knowledge of the target precision in the optimization. There are two ways to do this. In the direct search optimizer, one can restrict the increments to the size of the target precision's epsilons. Alternatively, one can periodically quantize the solution to the target precision during the optimization, while shrinking the step size. Orlin et al. provide a more thorough treatment of fixed precision optimization <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Decoding</head><p>Once encoded, the image can be "rendered" or decoded at any resolution. This involves the following steps: 1) Resample encoded image to desired resolution.</p><p>2) Apply decoding function γ for color assignment at each pixel.</p><p>3) Antialias image, an optional step which blends colors near object boundaries. Decoding an encoded image requires knowledge of the embedded dual skeleton, which is encoded in the IStar structure. The process basically consists of resampling the image and evaluating γ for each sample. Throughout this paper, the γ function is a general attribute assignment function, which associates regions of value space to an indicator vector, color, position in value space, etc., depending on the context. When an image is encoded, the γ function captures the boundary discontinuities by partitioning (or cutting) the encoded value space. We discuss specific examples of γ functions in Section 6.</p><p>Once identified, the sample value is replaced by the attribute associated with object i. This attribute is usually a color, but can also be a more complicated function or shader. For a given γ function that returns an N dimensional indicator vector ( v = γ(I e ( x))), color assignment can be expressed as</p><formula xml:id="formula_7">c = N ∑ i=1 v i c i ,<label>(6)</label></formula><p>where c i is the color associated with the i th object. While both the original image I v and "decoded" image γ(I e ) functions are discontinuous, the encoded image function I e is a continuous function, where the degree of continuity depends on the interpolation kernel. We can leverage this fact to perform analytic antialiasing. Given a γ function we can derive a signed distance function g i : R E → R that returns the distance to the partition in the encoded value space for object i.</p><p>The distance d i to the boundary in the spatial domain, is</p><formula xml:id="formula_8">d i ( x) = g i ( x) |∇g i ( x)| 2 .<label>(7)</label></formula><p>Antialiasing can be achieved by blending based on the distance to the boundary, c = w c i + (1 − w) c j where w = clamp(d i ( x)/2 + .5), c i is the color associated with the X i closest to I e ( x), and c j is the color associated with the second closest X j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Decoding Function γ: Plane vs. Voronoi</head><p>One way to partition the value space is with a Voronoi tessellation. While the Voronoi-based γ function is simple to implement, it places a hard constraint on the embedding procedure for the dual skeleton: not only must the dual embed in the target value space, but it must also be arranged so that the Voronoi tessellation produces the correct results. For even moderately complicated images, it may be difficult to satisfy the Voronoi constraint. The use of Voronoi γ functions also has a subtle side-effect in the reconstructed images. Triple intersections in the primal domain tend to be shaped like the intersections in the value domain. Voronoi intersections have a "Y" shape in the value domain, so the intersections in the spatial domain are also slightly "Y" shaped, even when the image is aggressively optimized. This effect is caused by the smoothness of the interpolation kernel. We can address both these issues with a more flexible γ function that defines the tessellation using planes in the value space and allows us to create "T" junctions. In this case, each embedded 0-skeleton element X i is bounded by a number of planes,</p><formula xml:id="formula_9">p i j ( x) = w T i j x + w 0,i j ,<label>(8)</label></formula><p>where p i, j ( x) = 0 is the plane separating X i and X j , w and w 0 are the plane coefficients. The γ function can be expressed as where a i is the attribute such as color associated with object i. The value space distance functions for this γ are</p><formula xml:id="formula_10">γ(I e ( x)) = a i where p i j (I e ( x)) &gt; 0 ∀ j,<label>(9)</label></formula><formula xml:id="formula_11">g i ( x) = min j p i j ( x) | w i j | 2 .<label>(10)</label></formula><p>This function simply returns the distance to the closest boundary plane (p i, j ) in the value space, and is used for antialiasing in Equation 7.</p><p>One advantage of a plane basis for γ is the fact that the planes can be arranged to preserve the structure of object boundaries in the value domain for any valid embedding. The second advantage is the ability to arrange the n-way object intersections in the value domain so that they minimize the decode error. <ref type="figure" target="#fig_6">Figure 6</ref> illustrates the difference between "Y" intersections produced by Voronoi tessellations and "T" intersections produced using planes.</p><p>Antialiasing object boundaries requires us to capture the second closest X j , i.e. the closest X j that shares a 1-skeleton element with the selected X i . This distinction is important since a plane may be necessary to separate two X i 's that happen to embed closely but do not share a boundary in the spatial domain. Since Equation 10 requires a division by the plane coefficients, we pre-normalize the coefficients so that this does not need to be done for each sample. This makes Equation 10 identical to Equation 8. The second closest X j will be the one with the minimum g i j that also shares a 1-skeleton element with X i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Considerations for Volumetric Data</head><p>The dual-parametrization method can also be used for 3D volume and higher dimensional images. The utility of dual-parametrization for volumetric data is less about compressing the image size (extents), and more about compactly representing object attributes. Consider the segmentation of a data set. Many segmentation and classification algorithms are capable of producing sub-pixel accurate results, for instance level-set and random walker methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>. These methods require a scalar image for each segmented object to preserve this sub-pixel accuracy. Therefore, 10 segmented regions would require 10 scalar volumes. For this reason, many discard the continuous representation for a binary tagged data set, which identifies classified features only at data samples. Dual-parametrization can be directly applied, as described in the previous section, where I v produces an indicator vector based on the segmentation's scalar fields. More general classification methods produce posterior probabilities or volume fractions for each object in the data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>. In this case, the dual-parametrization can be used to encode these continuously varying probabilities as seen in <ref type="bibr" target="#b9">[10]</ref>. The only modification necessary to make that method a dualparametrization is the connected-component subdivision of objects (condition (ii) in Section 4). In this case, connected objects and the dual structure can be identified based on maximum a-posteriori class assignments.</p><p>Level-set segmentations can easily be transformed into signed distance functions with respect to the segmentation boundary <ref type="bibr" target="#b27">[28]</ref>. A dual encoding can capture this information as well by adapting the minimization from Equation 4 to</p><formula xml:id="formula_12">ε({ v i |i = 1 ...N}) = N ∑ j=1 D j d l j ( x) − d e j ( x) 2 d x<label>(11)</label></formula><p>where D j is the region for which the level-set distance d l j for object j is positive, and d e j is the distance function from Equation 7. Unlike the discrete object representation discussed earlier, this functional has continuous derivatives. We also implement Equation 11 as a summation,</p><formula xml:id="formula_13">ε({ v i |i = 1 ...N}) = N ∑ j=1 M ∑ k=1 d l j (P k ) − d e j (P k ) 2 h d l j (P k ) + ε ,<label>(12)</label></formula><p>where P k is a pixel location in the image domain, and h is the Heaviside step function, which ensures that we only optimize regions within a distance ε from object i's boundary. We minimize this functional using an iterative Gauss-Seidel solver with successive over-relaxation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Enforcing Stable Singularities</head><p>One of the advantages of IStar images over traditional raster images is that infinitely thin objects can be represented, such as lines and cusps. As noted earlier, a signed boundary distance function can be used to outline subjects. Therefore, to represent lines, we implicitly represent them as a boundary. A line can be forced to exist in a dual encoded image by adding additional objects so that their boundary is the line. These new objects are hidden when the illustration is rendered by making their color identical to the original object that they were cut from, with the exception of their boundary, which is rendered as a partial outline, the line itself. Cusps, or sharp edges, can be captured by 3way object intersection in the dual encoded image, and be made an intrinsic property of a boundary by forcing a triple intersection at the cusp's point. Just as with lines, this is done by adding an extra hidden subject. <ref type="figure">Figure 7</ref> shows the letter A with cusps supported by hidden subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>In this section, we evaluate the IStar framework by first implementing the reconstruction algorithm on graphics hardware in order to texture map surfaces in real-time applications. Next, the quality of the reconstruction is compared against existing techniques for efficiently representing 2-D images, both for real-time and for offline applications. Finally, we demonstrate the preservation of semantic meaning in the IStar representation and give an example of how it can fix classification errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Graphics Hardware Implementation</head><p>The Voronoi decoding of IStar images was implemented as a fragment program on programmable graphics hardware. To do this, the compressed IStar bitmaps are bound to the appropriate surfaces in the scene and its texture samples are bilinearly interpolated on the hardware. The position and color of the nodes on the dual complex are provided as two additional non-interpolated textures. To compute the correct color, we must find the closest node to the upsampled value at each pixel. This can be done by comparing each node in turn, resulting in an algorithm with linear complexity with respect to the number of nodes. Instead, we use a constant-time algorithm that first divides the original image into a uniform grid of m × n cells in the spatial domain during encode. For each cell, we determine which regions (i.e. 0-cell elements in the dual skeleton) it contains, and the maximum number of regions in any single cell is computed. This information is then <ref type="figure">Fig. 9</ref>. Objects textured with IStar images for an interactive walkthrough. The scene is a "museum" of IStar images with a wide view on the left, and a detailed view on the right where the user has walked up to one of the artworks. The standard texture on the wall appears pixelated, the IStar image is not. This scene renders at about 480 fps.</p><p>output to a constant-sized list of nodes for every cell, where the size of the lists is determined by the cell with the most nodes. For cells with less than the maximum node count, the excess nodes are padded out with large values which will never be accessed during the nearestnode determination. During decode, we first determine which cell the point to be shaded is in, and compute the square-distance in histogram space from the upsampled value at that point to each of the nodes in that cell. A series of conditional move statements allows us to determine the node with the smallest square distance, thereby mapping the sample point to its Voronoi cell in the dual complex. This algorithm has constant complexity since it is limited by the maximum number of nodes in each cell. In our experiments, this number was less than 10, even for complex images. The rendering algorithm runs at real-time rates (&gt; 100 fps) at 1024 × 1024 resolution on an NVIDIA 7900GTX with 512MB of video memory. <ref type="figure">Figure 9</ref> demonstrates how we can apply IStar images to general texture mapping by using them to texture map the walls of a walkthrough environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Quality of Reconstruction</head><p>To measure the quality of the IStar algorithm, we must compare the original primal image with the reconstructed result. However, the quality of the reconstruction varies with the amount of downsampling in the encoding process, making this method a form of lossy image compression. In other words, the algorithm is replacing a highresolution bitmap with a smaller representation by trading memory space for algorithmic complexity.</p><p>Therefore, we also compare our algorithm to existing raster-based compression schemes. However, unlike our IStar representation, these do not take discontinuities into account. In particular, we compare against the three most common approaches used in real-time applications: downsampling with (1) nearest-neighbor and (2) linear upsampling, and (3) S3TC image compression. For completeness, we also compare against high-end offline image compression techniques such as jpeg and jpeg2000. We present our results in <ref type="figure" target="#fig_1">Figures 10 and 11</ref>, where the Peak-Signal to Noise Ratio (PSNR) is plotted against compression ratio for the different approaches (higher PSNR means better quality). The size of the entire IStar structure (compressed bitmap, dual skeleton, and color palette) is taken into account when doing our calculations. While the size of the skeleton and color information are constant, we can vary the size of the IStar structure by changing the amount of downsampling for the bitmap. While jpeg and jpeg2000 outperform our approach, these compression schemes are not compatible with real-time applications as their decompression algorithms cannot exploit the regular sampling and locality that traditional raster images can. In addition, jpeg and jpeg2000 compress the image at a fixed resolution, which means that magnification of the image will not preserve the boundary discontinuities. If desired, IStar images can be combined and optimized with jpeg or jpeg2000 and enjoy both aggressive compression and resolution independence. <ref type="figure" target="#fig_1">Figures 11 and 12</ref> compare detail regions of a test image for each compression method.</p><p>The artifacts introduced by our approach at high compression ratios are very different in nature than those of other compression schemes. Rather than the ringing, blurring, or block artifacts of other compression schemes, high compression of IStar images introduces artifacts that are simplifications of the geometry of the dual encoded image, which are less perceptually objectionable.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Modification of Regional Attributes</head><p>The goal of most visualization applications is to produce images that highlight features of interest. IStar images are well suited for such applications because their structure stores distinct regions of the data as nodes in a dual graph, which can be used to add semantic information to each desired region. The assignment of node colors can be changed from that of the original color palette to produce false-color images or create special effects such as transparency. In addition, the antialiasing method presented in Section 5.2 can be altered to generate outlines, which can help highlight objects during visualization. Several of these effects are applied to a 2D test image in <ref type="figure" target="#fig_1">Figure 13</ref>. These approaches can also be used to get around the limitation that our input primal image must have regions of constant color. In order to process an image with arbitrary gradients, we first add a preprocessing step to our pipeline where we segment the image into separate regions. This creates a constant-color image that can be used by the rest of the system. During rendering, the decoded regions can then be used as "region masks" while another source of data (a lowresolution texture, fragment shader, etc.) is used to reconstruct the gradient within those regions. As shown in <ref type="figure" target="#fig_1">Figure 13</ref>[e], this is a simple way to add gradients or high frequency texture detail to IStar images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Fixing Classification Errors with Topological Information</head><p>An interesting application of the IStar structure is using the stored topological information to fix errors in the data set. <ref type="figure" target="#fig_0">Figure 14</ref> (left image), shows a classified MRI of a human head that is known to contain errors, in this case incorrect classification that places white matter tissue in direct contact with both skull and cerebro-spinal fluid (CSF). We can enforce the anatomically-correct configuration in which CSF completely separates the skull and white matter by eliminating the edge between the white matter and skull nodes in the dual skeleton and then reparametrizing so that the white matter-skull interpolated values follow the white matter-CSF-skull boundary path in the value domain. The result of this reparametrization is the corrected data set shown on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future Work and Conclusions</head><p>Our continuing efforts will focus on developing the IStar framework for more general image classes, including arbitrarily varying attributes, such as color gradients and texture. We believe this approach iStar jpeg2k s3tc linear nearest original <ref type="figure" target="#fig_1">Fig. 12</ref>. Comparison of compression artifacts. The image on the left was compressed 1000 : 1 with various algorithms. The insets show the artifacts introduced by the algorithms for the regions specified. "Linear" and "nearest" refer to a bilinear interpolation and nearest-neighbor upsampling of the downsampled data, respectively. Our algorithm does very well against the real-time approaches and even compares favorably against jpeg2000. Note that IStar artifacts are geometric simplification.</p><p>will benefit multi-class classification and segmentation methods as an acceleration mechanism. The compact data representation may be able to eliminate the need for computing independent scalar fields for each class and allow all class probabilities to be solved simultaneously. We are also interested in automating the cusp support algorithm and providing a postscript decode implementation.</p><p>We have presented a new topological description for continuous images in both 2D and 3D. This topological description allows images to be reparametrized so that discontinuities are preserved at arbitrary resolutions. By applying the machinery of topology, lower bounds can be found for the dimension of a value space that is capable of preserving the uniqueness of n-way object intersections. We have demonstrated that this preservation of semantic structure can enhance 3D visualizations and 2D images.  shows connectivity between all three tissues. (Right) rendering from reparametrized data which disallows skull-white matter boundaries. The dual skeleton has been modified and laid out so that values interpolated by the white tissue-skull boundary are mapped to the CSF. Data source: BrainWeb Phantom.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Published 14</head><label>14</label><figDesc>September 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of IStar encoding/decoding described in Section 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Each N-cell in the complex must be completely bounded by (N − 1)cells and homeomorphic to an N-ball (the interior of an N − 1 sphere). For instance, each 2-cell is homeomorphic to a disk and bounded by 1-cells, each 1-cell is bounded by 0-cells, etc. The boundary of a 0-cell is simply 0. A CW-complex χ can be deconstructed as set of N-cell sets, or a set of skeletons, χ = χ 0 , χ 1 , χ 2 ,... , where the superscript indicates the dimension of cells in the set. We call χ 0 the 0-skeleton = = The relationship between the primal complex and its dual for a 3D image with four "objects." (top) primal complex composed of the 3, 2, 1, and 0-skeletons. (bottom) the corresponding dual complex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>A hand dataset comparing image structure in primal and dual spaces. The left column shows the image in the primal domain. The right column shows the image in the dual space. [A] shows the hand with an arbitrary color palette. [B] shows the encoded image. [C] shows the "cut regions" in the dual space with the orignal palette and [D] shows the cut regions with the encoded color palette. [E] shows the histogram of the encoded image, which follows the dual-complex structure. [F] shows the histogram overlayed with the dual-domain cut regions. Source: Grays Anatomy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Blowing up a non-generic boundary in 2D in order to embed it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Making a region simply connected. The center image shows the object intersecting with itself, region A 1 is contracted in grey to show that it is simply connected. Regions can also be made simply connected by dividing them into 2 regions, seen on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>A comparison of "Y" versus "T" intersections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Supporting cusps by introducing hidden support objects. An ant image. Left: encoded primal image. Left-center: cuts in the dual domain. Right-center: regions in dual domain assigned colors from the original palette. Right: decoded primal image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Rate-distortion comparisons of PSNR versus compression rate for compression algorithms operating on the test image ofFigure 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Rate-distortion comparisons of PSNR versus compression rate for compression algorithms operating on the hand image ofFigure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>A comparison of different attribute assignments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Visualizing errors in classification and reparametrizing to enforce a correct anatomical model. (Left) red regions outline incorrect boundaries between skull and white matter. The dual skeleton (inset)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We would like to thank Mariana Ruiz for the ant image of <ref type="figure">Figure 8</ref>. We also thank Ross Whitaker for seeding some of these ideas and Bobby Hanson for invaluable help with the math. This work was supported in part by the DOE High Performance Computing Graduate Fellowship and NSF Grant CCF-0702787.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Material interface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Bonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Duchaineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schikore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Joy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics(TVCG)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="500" to="511" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Computational topology. Advances in Discrete and Computational Geometry (Contemporary mathematics 223)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="page" from="109" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On straight line representation of planar graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Univ. Szeged. Sect. Sci. Math</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="229" to="233" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilabel random walker image segmentation using prior models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="763" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Topology-based simplification for feature extraction from 3d scalar fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gyulassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization 2005</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="275" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computational topology for shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape Modeling International &apos;99</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Algebraic Topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hatcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Representation and display of vector field topology in fluid flow data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Helman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hesselink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="page" from="27" to="36" />
			<date type="published" when="1989-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Images and dual dataspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hanson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Utah</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statictically quantitative volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Uitert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization 2005</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimization by direct search: New perspectives on some classical and modern methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Torczon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="482" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Concepts of digital topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topology and its Applications</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="219" to="262" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Digital topology: Introduction and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Proc</title>
		<imprint>
			<date type="published" when="1989-12" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="357" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Geometric Model Extraction from Magnetic Resonance Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laidlaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resolution independent curve rendering using programmable graphics hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1000" to="1009" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient magnification of bilevel textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loviscach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bremen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Conference Abstracts and Applications</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The color adjacency graph representation of multi-coloured objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<idno>VSSP-TR-1/95</idno>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Surrey</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topology</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ε-optimization schemes and L-bit precision: alternative perspectives in combinatorial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Orlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="565" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Topology Diagram of Scalar Fields in Scientific Visualization, chapter Chapter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topological Data Structures for Surfaces</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vector texture maps on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Levy</surname></persName>
		</author>
		<idno>ALICE-TR-05-003</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scheuermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tricoche</surname></persName>
		</author>
		<title level="m">Visualization Handbook, chapter Topological Methods in Flow Visualization</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="341" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Silhouette maps for improved texture magnification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Hardware</title>
		<meeting>Graphics Hardware</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="65" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shadow silhouette maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cammarano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="526" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Postscript Language Reference Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Systems</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mri tissue classification with neighborhood statistics: A nonparametric, entropy-minimizing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Awate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of (MICCAI)</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="517" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bixels: Picture samples with sharp embedded boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics Symposium on Rendering</title>
		<meeting>the Eurographics Symposium on Rendering</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="186" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A framework for level set segmentation of volume datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Breen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Museth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Soni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of ACM Workshop on</title>
		<imprint>
			<biblScope unit="volume">Graphics</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
