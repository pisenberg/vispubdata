<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextualized Videos: Combining Videos with Environment Models to Support Situational Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-09-14">14 September 2007.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">David</forename><forename type="middle">M</forename><surname>Krum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enylton</forename><forename type="middle">M</forename><surname>Coelho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Doug</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
							<email>bowman@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>David</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robert Bosch Research and Technology Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contextualized Videos: Combining Videos with Environment Models to Support Situational Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-09-14">14 September 2007.</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2007; accepted 1 August 2007; posted online</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>situational awareness</term>
					<term>videos</term>
					<term>virtual environment models</term>
					<term>design space</term>
					<term>testbed design and evaluation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Multiple spatially-related videos are increasingly used in security, communication, and other applications. Since it can be difficult to understand the spatial relationships between multiple videos in complex environments (e.g. to predict a person s path through a building), some visualization techniques, such as video texture projection, have been used to aid spatial understanding. In this paper, we identify and begin to characterize an overall class of visualization techniques that combine video with 3D spatial context. This set of techniques, which we call contextualized videos, forms a design palette which must be well understood so that designers can select and use appropriate techniques that address the requirements of particular spatial video tasks. In this paper, we first identify user tasks in video surveillance that are likely to benefit from contextualized videos and discuss the video, model, and navigation related dimensions of the contextualized video design space. We then describe our contextualized video testbed which allows us to explore this design space and compose various video visualizations for evaluation. Finally, we describe the results of our process to identify promising design patterns through user selection of visualization features from the design space, followed by user interviews.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Video cameras are widely used in many applications: factory monitoring, traffic and security surveillance, telerobotics, telemedicine, and teleconferencing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>. People observe videos to understand the situation, to make decisions, and to communicate with each other. Technological developments have made video cameras more affordable, allowing multiple cameras to be deployed to cover a larger space or to observe the space from multiple viewpoints. As the system scales, understanding the situation recorded by these videos can become very difficult, because observers often need to mentally reconstruct the spatial relationships between multiple cameras.</p><p>Building security surveillance is an illustrative example. In a standard security monitoring system that displays a number of video thumbnails or cameos, the operator must maintain a detailed mental model of the building or site and perform numerous mental mappings in order to understand the activities shown in the videos. Previous research has shown that mental registration of multiple views is a challenging cognitive activity <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. A promising hypothesis is that contextualized videos -that is, a combination of videos with a model of the 3D environment -will allow observers to see the activities in the videos in their proper locations. In this case, spatial relations are presented in the visualization, allowing some cognitive work to be offloaded onto the perceptual system <ref type="bibr" target="#b20">[21]</ref>.</p><p>With the performance increase in graphics hardware, various contextualized video approaches are now technically feasible. For instance, Sawhney et al. <ref type="bibr" target="#b15">[16]</ref> presented a system that projects a video stream onto a 3D model. Other techniques have also been proposed, such as video augmented virtual environments <ref type="bibr" target="#b17">[18]</ref> and temporal video visualization <ref type="bibr" target="#b5">[6]</ref>. Interesting research questions naturally follow: (1) Which tasks can benefit from contextualized videos? <ref type="bibr" target="#b1">(2)</ref> What are the design possibilities to support these tasks? (3) Which specific technique should be used to support a specific type of user task?</p><p>To identify and clarify these research questions, we have been investigating different contextualized video designs, as well as 3D model visualization techniques, in the context of video surveillance of multi-story buildings. We employ a user-based approach to explore and define the design space for contextualized video and categorize previously suggested techniques. In our design space exploration, multi-story buildings are naturally of interest since modern surveillance systems must be able to scale up to support complex facilities. Furthermore, several visualization challenges, e.g. occlusion and display clutter, emerge if multiple floors must be monitored.</p><p>This paper summarizes our work to date on the design possibilities and the benefits of contextualized videos, based on the testbed design and evaluation method (Section 3). We first identify user tasks that are likely to benefit from contextualized videos (Section 4). We then propose a design space for visualizations with contextualized videos (Section 5). Finally, we describe our contextualized video testbed, which allows designers and users to explore a large part of the design space, and identify some promising design patterns through user interviews (Section 6).</p><p>Our research has the following contributions:</p><p>• We propose a data-based task classification that helps us understand when different contextualized video designs are appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We identify the structure of the contextualized video design space and point out research opportunities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We explore an important subset of the design space and identify promising design patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>While the image processing and computer vision communities have developed techniques to track human forms and detect anomalous behaviors from video sequences, these techniques will not soon replace human operators in many application areas, for example, surveillance systems. There is still a need to present the results of these algorithms to human operators. Chen et al. proposed the concept of video visualization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. They treated a video as 3D volume data and adopted a variety of volume and flow visualization techniques to summarize the activities captured by a video. They showed that people can identify the patterns in the visualization with a short period of training. Our major concern differs from their research goal and focuses on how to present multiple videos in such a way that users can offload the difficulty of spatial relationship reconstruction onto the display. Sawhney et al. demonstrated the feasibility of projecting multiple videos onto a 3D environment model in their Video Flashlight work <ref type="bibr" target="#b15">[16]</ref>, while Sebe et al. presented an "Augmented Virtual Environment" system which integrated multiple videos into a 3D context model <ref type="bibr" target="#b17">[18]</ref>. They detected moving objects inside the video and visualized them as textured dynamic rectangles moving around in the 3D model. Both of these papers demonstrated the technical feasibility of a particular video placement technique. Girgensohn et al. <ref type="bibr" target="#b7">[8]</ref> proposed the Spatial Multi-Video (SMV) player, in which the videos are dynamically arranged on a head-up display according to the user's selection of interest and the spatial proximity of the cameras' field of view. SMV improved user performance in a suspect-tracking task. All of these techniques can be understood as points in the contextualized video design space. In this paper, we focus on exploring and defining this design space.</p><p>Several projects in teleconferencing and CSCW have placed live videos into collaborative virtual environments <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>. In <ref type="bibr" target="#b16">[17]</ref>, the real-time videos were used as windows through which users could communicate with their colleagues. A user could freely configure his spatial relationship with others by moving the video that represented him in the virtual environment. Spatial understanding was not reported as an issue, probably because the virtual space was very simple and the number of videos was small.</p><p>Combining videos with environment models shares some characteristics with the use of multiple views for visualization tasks, because a video and the model show different aspects of the same data. Baldanodo et al. summarized eight guidelines for the design of multiple views for visualization <ref type="bibr" target="#b0">[1]</ref>. These design rules could be used to analytically evaluate different contextualized video designs. For example, the rule of consistency between multiple views implies that we should try to render the landmarks inside the model in the same way as they appear in the video so the user can quickly match the landmarks. Tory et al. investigated how to combine 2D and 3D views for volume visualization and spatial relationship tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, they found that the users may use pattern matching instead of mental rotation to link two views. This may also be true when people try to link a video and a model. Following Tory et al.'s definition for "3D views" <ref type="bibr" target="#b22">[23]</ref>, both a video frame and a perspective view of the model are 3D views. However, they come from different sources: the video is captured while the model is pre-computed. Combining several 3D views from multiple data sources for visualization is not a well explored problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESEARCH METHOD</head><p>To help readers understand our work in a broader context, we give an overview of our research method in this section. The purpose of our research is not only to find individual effective visualization designs that prove effective, but also to develop general guidelines or theory that can help visualization designers to understand contextualized videos. For the latter purpose, exploring the structure of the design space is an important step, after which we will be able to identify the primary choices for testing in the follow-up controlled experiments. This is consistent with House et al.'s argument: "controlled experiments are quite limited in their ability to uncover interrelationships among visualization parameters, and thus may not be the most useful way to develop rules-of-thumb or theory to guide the production of high-quality visualizations" <ref type="bibr" target="#b9">[10]</ref>. We mainly followed Bowman et al.'s testbed evaluation method <ref type="bibr" target="#b1">[2]</ref>, which is targeted at inventing and evaluating generic designs instead of application-specific techniques. As a user-based research method, we tried to involve users in every step. However, since we targeted general research questions instead of specific applications, we did not constrain our users to be domain experts. Sometimes it is better to involve non-expert users to eliminate the effect of prior experience. The differences between testbed design and evaluation and user-centered design are discussed at length in <ref type="bibr" target="#b2">[3]</ref>. We illustrate the major differences in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Specifically, our research contains the following steps: • Task Extraction: Our first step is to extract domain tasks from the applications. We performed a field study of the building security guards' work, exacted low-level tasks and identified some tasks that are likely to benefit from contextualized videos (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Design Space Characterization: We continuously refined our understanding of the contextualized video design space throughout the whole research cycle. The goals are to identify the primary design dimensions and create a framework to generate new designs. We summarize the design space in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Testbed based Prototyping: We started to explore the design space by prototyping a spectrum of video placement designs in a common testbed. We noticed that the occlusion between the model and the videos often caused serious problems for embedded videos (contextualized videos where the videos are placed directly into the 3D model); we then investigated how to adopt scientific and engineering visualization techniques to manage occlusion. This led to a selected set of prototypes along another design dimension: model processing. The testbed allows us to combine the various techniques in many ways. The prototypes are described as examples of the design possibilities in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Formative Evaluation: Using the testbed, we performed a preliminary exploration of the design space with users in order to investigate the users' usage patterns (also known as usage model <ref type="bibr" target="#b12">[13]</ref>) for two interacting design dimensions: the video placement dimension and the occlusion management dimension. We also surveyed users' preference over different video placement designs. Although the users' preference could not lead to conclusions, they provided reasonable indications. We describe the evaluation process and early findings in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Summative Evaluation: Based on hypotheses generated by the formative evaluation, we are currently planning a controlled experiment to summatively evaluate the choices along the primary design dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TASK EXTRACTION</head><p>In order to extract the real world tasks to motivate our contextualized video designs, we performed a field study, followed by a task categorization, which highlighted the need to combine videos with their environment model. However, we did not attempt to perform a complete survey of the video surveillance domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Field Study</head><p>We visited a mixed-use retail, office and parking building with five floors. The whole building was monitored by one security guard through about 50 CCTV cameras. The videos were arranged as 4 4 arrays on three 23 inch monitors. Each video could be selected by a hardware switch and enlarged on a single monitor. We arranged the field study in two sessions: a preliminary factgathering interview and an activity analysis session. The first session lasted for about one hour. During that time, we asked the security guard to describe his general working process and how he used the monitoring system. The second session lasted for two hours. In that session, we joined the security guard both while he was patrolling the building and monitoring the console in the office. He also provided us with further details such as how to figure out the blind areas of the cameras and how to decide whether a person is suspicious or not.</p><p>We found that the security guards mainly perform two activities in the control room:</p><p>• Monitoring activity: The main responsibilities of security guards are to observe. They frequently scan the videos and try to discover suspicious persons or dangerous situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Tracking activity: Security guards track persons as they move within the building and from video camera to video camera. This allows security guards to determine if people are acting suspiciously, for example, accessing restricted areas or moving equipment. This also allows suspicious individuals to be intercepted and questioned. The security guard reported that he took several weeks to become familiar with all the cameras to a degree that he could identify the blind areas that were not covered by any camera. This process might be even longer for novice users and low spatial ability users <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task Classification</head><p>We extracted low-level tasks from the activities and classified the tasks according to users' information requirements: the video content, the environment context (i.e. the model), or the relationship between the videos and the environment. Type 1 -Video intensive tasks (requiring information presented in videos):</p><p>• Overview monitoring -glance at videos without focusing on any specific one, as in the monitoring activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Close observation -observe a person or activity in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Content-based search -the user knows part of the content, e.g. a landmark in the video, and wants to find its context, so she will scan the videos for the landmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Content-based travel -shift from one video to another, without relating the two videos in a global reference frame. Type 2 -Model intensive tasks (requiring contextual information largely available from the 3D model):</p><p>• Travel -travel from one place to another in the global reference frame • Route Planning -look for a route from one place to the other • Location based Search -the user knows the location in the model and wants to find the corresponding video Type 3 -Integrative tasks (involving information from both the model and the videos):</p><p>• Orientation-based prediction -predict where the person in the video will go by mentally registering the video's orientation into the 3D model's reference frame. Appears in tracking activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Landmark-based prediction -predict the future location of a person outside the video camera's range, based purely on landmarks. Appears in tracking activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Multi-video registration -judge the spatial relationship between objects in two or more videos. From the classification, we can see that Type 3 tasks are rooted in the relationship of the videos to the spatial context. When the required information is not present in the users' working memory, they either recall it from their long-term memory or recognize it from external displays. Because our visual system has a very high information bandwidth, recognition may sometimes be more efficient than recall <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Novice users may prefer to use external displays. For instance, they may need to look at the 3D environment model in order to make an orientation-based prediction, while an expert user can do the same task on a 2D layout of videos. This is because after an extended period of time at the same site, a security guard can develop a mental model that establishes a correlation of the videos generated by the surveillance cameras and the physical location where the cameras are located. However, as the complexity of the environment scales, some technical assistance would be helpful in supporting the development of such a mental model as well as alleviating the cognitive load of maintaining and referring to such a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN SPACE</head><p>Contextualized video visualizations combine video and model data to help people understand complex situations. Naturally, layout is a key design dimension. Nonetheless, several other issues are also relevant and need to be addressed. The functional module diagram <ref type="figure">(Fig. 3)</ref> shows the major design dimensions. From the designer's point of view, the contextualized video design space contains the following primary dimensions:</p><p>• Video Processing Method: how video data is processed before combining with the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Model Processing Method: how the environment is modeled and rendered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Video-Model Layout Design: how to lay out videos and models together in one display, from which the observer can infer some relationship between the videos and the model, as well as between multiple videos. • Navigation Design: how to navigate between different views of a video, between multiple videos, between a video and a view of the model, and between different views of a model. In the rest of this section, we mainly discuss two dimensions, video-model layout and model processing, which were explored using our testbed evaluation method. For the other two dimensions, we only mention some possible directions for further research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Video-Model Layout Design</head><p>The video-model layout problem is a special feature of contextualized video visualization. In principle, either the model or the video can be in the center of the display. We focus on how to organize videos around models in this paper. The video-model layout design module shown in <ref type="figure">Fig. 4</ref> can be characterized primarily as a layout matrix M layout that defines how the post processed video data V post are transformed and projected to form the combined visualization</p><formula xml:id="formula_0">V aug . V aug = M layout (V post ) (1)</formula><p>M layou can be decomposed into multiple simple matrices, which determine V post 's location, orientation, size and projection distortion respectively. For example, the location transformation matrix M layout can be defined to follow the viewer's location, the physical video camera's location, or the location of a moving object segmented from the video. Furthermore, different simple matrices can be defined to follow different objects. <ref type="figure" target="#fig_3">Fig. 5</ref> illustrates some typical layout matrices and the resulting layout designs, which will be described later in this section. While it is not possible to describe all the layout designs in this paper, we analyze the ones that were prototyped in our testbed. We believe there are other promising designs not discovered in this design space.</p><p>According to the spatial relationship of the video and the 3D model, we can classify video placement methods into two categories: associated videos and embedded videos.   Associated Videos -As in traditional video surveillance systems, the videos are displayed as an array of thumbnails in the viewer's viewport. Its M layout is defined to follow the viewer's location and orientation. Hence this layout provides excellent visibility of the video content. A major issue in associated video is how to help user relate videos to their corresponding locations. Some visual cues such as callout lines (as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>) or color coding can be used. But scalability is a major limitation. For example, as the number of callout lines increases, it gets harder for users to follow the links.</p><p>Embedded video designs put the video content in the object space of the environment model. Its M layout is defined to follow the physical cameras' location. Hence, embedded videos give an approximate location cue of the video. Associated video and embedded video can be used together to compensate each other. Depending on how we define the orientation and projection matrix, there are a variety of designs for embedded videos.</p><p>Video Billboards -This type of embedded video maps the video onto a rectangle that always orients itself to face the user <ref type="figure" target="#fig_4">(Fig. 6)</ref>. The billboard's location approximates the location of the video content. Since the orientation depends on the observer's view point, camera orientation is not apparent and video content location can not be precisely determined. Compared with video projection and video on fixed planes, videos are easier to perceive in video billboards. The billboard can either rotate about a point in space (3D billboard), or about an axis (2D billboard).</p><p>Video on Fixed Planes -This embedded video design maps the video onto a fixed rectangle <ref type="figure" target="#fig_5">(Fig. 7)</ref>. The rectangle is oriented to align with the camera's axis of projection, so it approximates the location of the content and reflects the orientation of the video camera. This technique avoids or minimizes the video distortions possible with video projection; however, it can be difficult to perceive the video information from vantage points that are far off the projection axis. Video Projection -This embedded video design projects videos onto the 3D model in the same way as a projector would <ref type="figure" target="#fig_6">(Fig. 8)</ref>. Video projection manifests the camera coverage area and camera direction on the model. If the video is texture projected onto the model from the actual camera location with the correct camera parameters, the walls and floors in the video can seamlessly match the model. However some objects can appear to be distorted if they are captured by the video but not modeled as 3D objects. <ref type="figure" target="#fig_7">Fig. 9</ref> shows such a case: the human figure is distorted because the video is projected onto the wall and the floor instead of a corresponding 3D human model in the 3D space. When the projected model area contains broken walls, e.g. open doors, the projected video may be even harder to perceive and interpret because the video image is broken into multiple parts. Sawhney et al. showed how to implement video projection in <ref type="bibr" target="#b15">[16]</ref>.</p><p>Dynamic imagery -This embedded design maps the video or the extracted moving objects from the video onto a polygon whose movement follows the detected dynamic object's movement in 3D space. In this design, the location and the height of the moving object would be shown precisely. However, if the whole video is mapped onto this polygon, the background of the video will be distorted. Dynamic imagery will be hard to perceive, because it often moves around when the user is observing it. Sebe et al. demonstrated an implementation of dynamic imagery in <ref type="bibr" target="#b17">[18]</ref>.</p><p>It is interesting to note that video on fixed planes and video projections were created by the same layout matrix M layout , even though they don't look similar in appearance. They differ only in terms of what projection surface is used. Video on a fixed plane is projected onto a plane facing the camera, while video projection is projected onto the environment model. <ref type="figure" target="#fig_7">Fig. 9</ref> illustrates the difference between video projection, video on fixed planes and dynamic imagery.</p><p>From the user's point of view, the various video placement methods can be thought of as a continuum, balancing between ease of video perception and ease of video-model spatial alignment. On one end there are associated videos, which are very easy to examine but need the most effort to align with the model. On the other end, there are dynamic imagery and video projections, which are harder to examine but easier to spatially register with the model. Video on fixed planes and video billboards lie between video projections and associated videos. Video on fixed planes eliminates the broken image and projection distortion problem of video projection, at the cost of more difficulty in matching the features between the video and those of the model. Video billboards further eliminate the vantage point distortion problem at the cost of more difficult orientation alignment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Processing</head><p>In our work, an environment model describes the 3D spatial context of the videos. Complex 3D scenes present several known problems, e.g. occlusion and display clutter, which are particularly severe for embedded videos. To explore the usefulness of embedded videos, it is important to address these problems using some model processing techniques.</p><p>Various techniques to deal with occlusion and clutter have been investigated in areas such as scientific visualization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> and engineering illustration <ref type="bibr" target="#b10">[11]</ref>. These techniques can be generally categorized into three strategies:</p><p>Explosion and deformation -This rendering style separates subassemblies and components from the main object so that details can be seen. We briefly describe three explosion techniques that were prototyped in our testbed. Our basic implementation expands the floors vertically. We also implemented two variations on this basic implementation: drawers ( <ref type="figure" target="#fig_0">Fig. 10)</ref> and rotate-and-shear ( <ref type="figure" target="#fig_0">Fig.  11)</ref>. With drawers, the building can be thought of as a bureau or dresser, where each floor acts as a drawer. The user can draw out a floor by selecting it. With rotate-and-shear, each floor can rotate along its own axis or shear out from its neighbors like a pile of cards. The drawers variant was effective for exploring a single floor, while rotate and shear view reduced occlusion between multiple floors in one operation.</p><p>Cutaway -In a cutaway, part of the 3D model is removed to show significant interior features. We implemented a simple cutting plane in our prototype; however, more complex geometric shapes such as spheres, ellipsoids, or arbitrary curved surfaces could be used to define the cutting boundary. We provided 4DOF (four degree of freedom) control of the cutting plane, shifting vertically or rotating along the three axis, in order to provide vertical cutaway views that can be used to reveal inter-floor features like stairways and elevators.</p><p>Ghosting -Ghosting reveals the internal components by fading out less significant regions of the 3D model, such as occluding sections of the exterior skin. The distinction between cutaway and ghosting is that ghosting fades out, but does not entirely remove, the occluding parts. We implemented three ghosting techniques: landmark <ref type="figure" target="#fig_0">(Fig. 12</ref>), wireframe and semitransparency <ref type="figure" target="#fig_0">(Fig. 13)</ref>. Each technique can be applied on a floor-by-floor basis or applied in combination on a single floor. The goal of the landmark view is to eliminate unimportant components while keeping the structure as a context. In the semitransparent view, all the components of the object are rendered in a translucent fashion, but additional depth cues, such as color, are employed to help the user perceive the 3D structure of the object. In the wireframe technique, only edges and vertices are displayed for the 3D model. Among these ghosting techniques, landmark view not only reduces occlusion, but also reduces display clutter; hence the structure of one floor can be easily perceived. However, videos underneath the top floor may still be hidden. Wireframe and semitransparency are able to reveal more videos; hence the user can see an overview of all the videos in a single view. But wireframe and semitransparency may also lead to misjudgment of the video position, because they often fail to provide enough depth cues.</p><p>The above methods are mainly used to visualize the physical environment. We can visualize the cameras as well. For example, we visualized the camera's location and orientation using a very simple 3D camera model in our testbed. We could further visualize the camera's 3D coverage space using a semitransparent pyramid at some expense in clutter and occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Video Processing</head><p>Video processing and computer vision are both well-developed research areas with numerous research results, many of which can be adopted to create innovative contextualized video designs. Sebe et al.'s "Augmented Virtual Environment" system <ref type="bibr" target="#b17">[18]</ref> is such an example. They detected moving objects inside the video and visualized them as textured dynamic rectangles moving around in the 3D model. The simplest case is no video processing as in our current implementation. The next possibility is to do video content analysis on the video streams and highlight the changes and recognize objects like humans inside the 3D model. For instance, visualizing the video signatures <ref type="bibr" target="#b4">[5]</ref> inside the 3D model may be a promising idea. Furthermore, when the models do not provide enough details, we can derive additional 3D details from the videos to refine the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Navigation Design</head><p>Interaction, particularly navigation, is a primary component of contextualized video interfaces. Navigation allows the user to select the appropriate viewpoints for examining multiple videos in a single view, minimize image distortion, gain an understanding of the building structure, and find uncluttered, unobstructed views. Depending on how much user intervention is needed, navigation techniques can be generally categorized into passive (automatic), active (user-controlled) and hybrid <ref type="bibr" target="#b6">[7]</ref>. The proper navigation technique for contextualized videos is likely to depend on the possible views that the user will choose when working with the visualization. We identified six different types of navigations for contextualized video visualization:</p><p>• Navigation within one video (zoom and pan on video) • Navigation from one video to another (shift) • Navigation between two representations of the same video, e.g. the video in the associated view and the same video in the embedded view (as in <ref type="bibr" target="#b0">[1]</ref></p><formula xml:id="formula_1">) •</formula><p>Navigation between a video and a larger view of its nearby context in the model (focus and contextualize) • Navigation between an detailed view of the model and an overview (zoom on the model) • Navigation between different parts of the model (travel) Some navigation modes, e.g. passive navigation between two representations of the same video, have not been fully explored. In our user-based exploration (Section 6), we observed which viewpoints users chose. We plan to further explore this direction in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SUBSPACE EXPLORATION</head><p>Since we are in the first research cycle as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, instead of trying to cover the whole design space, we focused on an important subspace which is composed of two major design dimensions: the video-model layout dimension and the model processing dimension. Since there are many possible visualizations that can be created by combining designs from the two dimensions, we asked users to look for potentially useful visualizations while considering realistic tracking tasks. To allow users to freely select viewpoints in the testbed, we employed a trackball-like navigation technique (similar to <ref type="bibr" target="#b19">[20]</ref>) allowing users to rotate the model and zoom in or out. With proper model processing and proper viewpoint, the occlusion effect can be reduced and the advantages of different video-model layout methods can be demonstrated.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Testbed based Design and Exploration</head><p>We used a testbed method to explore this design space. A testbed allows rapid composition of solutions for different design dimensions into specific configurations that can be tested and compared. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an overview of the testbed. The testbed is mainly implemented using OpenSceneGraph <ref type="bibr" target="#b13">[14]</ref>. The user can enable and disable each technique by menu selection and hot keys. Some techniques, e.g. landmark view and drawers, are applied on a floor by floor basis via mouse selection. The rest like explosion and rotate-and-shear are applied on the whole model. The prototyped techniques are described and analyzed in Section 5. Besides these techniques, we also implemented several visualization features that the user may choose to utilize, e.g. the 3D camera models that represent the cameras' location and orientation in the 3D building model.</p><p>Since the design space is huge and the interaction between multiple design concerns is complex, it would be very complex to run a fully-controlled experiment to compare all of these designs initially. Rather, we chose to run an informal formative evaluation, allowing users to explore the design space and make comments on various combinations of techniques, with the goal of identifying specific hypotheses that we could later test more formally.</p><p>Eleven users completed the study. We sampled one task from each task category described in Section 4. For each task, the users created a variety of interesting and reasonable visualizations.</p><p>Analyzing these visualizations, we discovered some commonly used usage patterns, some of which involved two or more design dimensions, indicating that in some cases a video-model layout method needs proper model processing support to show its advantages. These usage patterns helped us identify a limited number of promising designs to evaluate in a future experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.2</head><p>Promising Usage Patterns</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Video Monitoring Task</head><p>This task is a video intensive overview task. To support this task, users would create a visualization that put all the videos in one display so that they can monitor the whole situation of the building.</p><p>The following patterns were found: Pattern 1: Associated videos only Pattern 2: 2D Billboard + semitransparency or landmark Not surprisingly, associated videos received higher preference than embedded videos among most users. However, two users preferred embedded videos to associated videos ( <ref type="figure" target="#fig_0">Fig. 14 (a)</ref> and (b)). Both users used 2D billboard. The common reason they gave were that the associated videos were arranged in a vertical line and the users had to move their eyes up and down frequently to scan all the videos. By manipulating the models, the users could arrange the videos in a smaller screen space while keeping similar resolution as associated videos, even though the videos were not neatly aligned.</p><p>No users selected fixed plane video or video projection for this task because the videos' orientation was fixed in object space and the users could not find a single view to see all the videos clearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Tracking Task</head><p>This task requires the user to match the video and its nearby environment in the model. In the designed scenario, the users were asked to tell us the suspicious person's location and orientation. For this task the users would create a visualization that showed the details of a particular video, as well as the environment near this video. It is interesting to see the diverse strategies people used to figure out the orientation of the suspicious person in the model: Pattern 3: Associated Video + Fixed Plane Video + semitransparency or landmark Pattern 4: Dynamically switching between Billboard video and Fixed Plane Video + semitransparency or landmark In Pattern 3 and 4 people used fixed plane videos to judge the suspicious person's position and orientation in the model. Some of these people turned to associated videos to closely observe the suspicious person <ref type="figure" target="#fig_0">(Fig. 14 (c)</ref>) and others dynamically switched between billboard video and fixed plane video.</p><p>Pattern 5: Billboard videos + navigate to look behind the camera + 3D walls on (no landmark or wireframe)</p><p>Pattern 5 was used because the video content's orientation matches the model's view when looking behind the camera ( <ref type="figure" target="#fig_0">Fig. 14  (d)</ref>). Users preferred to see the 3D walls, which were used to do feature matching between the video and the model. One user turned on video projection to judge the camera orientation and used billboard video to view the video details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Route Planning Task</head><p>This task is a model intensive task. It requires the user to have an overview of the model and a remote view of a particular video. The resolution of the video was less important. In the designed scenario, the users would plan a route starting from the 1st floor to catch the suspicious person in a particular video on the 2nd floor. The following usage patterns were found: Pattern 6: Video on Fixed Plane or Video Projection + 3D walls with higher view angle Pattern 7: Video on Fixed Plane + landmark Video on fixed plane and video projection were selected because the approximate orientation information could be easily observed from some distance. Half of the users felt more comfortable to see the 3D floors with walls on ( <ref type="figure" target="#fig_0">Fig. 14 (e)</ref>); while others would rather do the tasks with a landmark view. This difference might be related to the user's mental model of the environment. Higher pitch angles were often selected when the walls were shown; because the users wanted to reduce the walls' occlusion in order to quickly see the route.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Summarizing the users' designs and rationales, we found that:</p><p>• Embedded video was preferred for video-model relation tasks while associated video was preferred for suspect detection tasks. Even for video intensive tasks, if occlusion can be effectively reduced, embedded video can still be a reasonable choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>All five video-model layout methods were employed by some users. This fact indicates that each method has its advantages and disadvantages, confirming our analysis in section 5.1. While video projection <ref type="bibr" target="#b15">[16]</ref> and dynamic imagery <ref type="bibr" target="#b17">[18]</ref> were useful for some tasks, they may not be an ideal solution for all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Many people used the strategy that either combined multiple video-model layout methods or dynamically switched between them. This highlighted the requirement for effective interaction support. Based on our experience, an often-effective design is to combine associated videos with embedded videos. In this way, the user can choose to use the proper representation when performing different tasks. When monitoring the whole building, the user relies more on the associated videos. When she detects a suspicious person, the user can track the suspicious person using the embedded videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>This paper reports on our exploration of the design space of contextualized video visualization using a testbed design and evaluation approach. By identifying and describing the set of design possibilities, the set of all useful techniques can be identified and characterized. This set of techniques forms a design palette from which the application designer can knowledgably select techniques to match the needs of a particular application and its users.</p><p>We have proposed a data-based task classification and identified the tasks that are likely to benefit from contextualized videos. We proposed a design space for visualizations with contextualized videos. We then analyzed the video-model layout problem and the model processing problem in detail. Based on the testbed, we identified some promising design patterns through user interviews.</p><p>Our research suggests that, despite some occlusion problems, embedded video is especially helpful for tasks where users need to consider a larger spatial context around the videos. Based on the usage patterns we identified, the following hypotheses will be tested in our follow-on summative evaluation:</p><p>• Compared with associated video, embedded video can improve task performance for video-model relation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Combining embedded and associated video will result in a balanced design that performs well for all types of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Billboard videos, together with good 3D visualization support and an appropriate viewpoint, can achieve similar performance as associated videos for video intensive tasks. We have explored two of the four major design dimensions, but many research opportunities remain. We plan to prototype several designs from the video processing dimension and the navigation dimension as well. We expect to find more interesting results as the design complexity increases. For example, if we extract moving objects from videos, what are the possible designs to embed the extracted imagery into the environment model?</p><p>Navigation is also of great importance for contextualized video design. Without usable interaction, the visualization cannot support the users' entire working process. A promising direction is to investigate automatic navigation between multiple views. For example, if the user clicks a video in the associated view, an effective embedded view of the video could be shown automatically.</p><p>Another area of study would be the scalability characteristics of different contextualized video designs, especially in terms of number of videos, display size, or model complexity. Our design space creates a framework for future study of these design issues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An overview of the contextualized video testbed. Billboard video, video projection and associated video are shown together with different visualizations of the building model. The menus in the lowerright corner allow users to choose different video placement and visualization techniques on-line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of Testbed Evaluation and User-Centered Design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>The contextualized video visualization design framework Mapping between transform functions and the resulting videomodel layout designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Associated Video. Callout lines are used to show association.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>2D billboard Video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Video on Fixed-planes. The video is hard to observe in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Video Projection: (a) original video, (b) viewpoint approximately follows the video camera, (c) viewpoint far away from video camera, severe distortion and image fragmentary due to the missing door of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Video projection, video on fixed planes and dynamic imagery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Drawer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Rotate-and-Shear.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Landmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Semitransparency and wireframe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>Usage Patterns: (a) Pattern 2, 2D billboard + landmark view + explosion. (b) Pattern 2, Billboard + semi transparency. (c) Pattern 3, associated video + fixed plane video + landmark. (d) A view behind the camera used in Pattern 5. (e) Pattern 6, video projection + walls.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by a grant from Bosch.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Guidelines for Using Multiple Views in Information Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Q W</forename><surname>Baldonado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuchinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Working Conference on Advanced Visual Interfaces</title>
		<meeting>Working Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Testbed Evaluation of Virtual Environment Interaction Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hodges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoperators and Virtual Environments</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="95" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Survey of Usability Evaluation in Virtual Environments: Classification and Comparison of Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoperators and Virtual Environments</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="404" to="424" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Readings in Information Visualization: Using Vision to Think</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual Signatures in Video Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Botchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Thornton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1093" to="1100" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Taxonomy of 3D Occlusion Management Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Virtual Reality</title>
		<meeting>IEEE Virtual Reality</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effects of presenting geographic context on tracking activity between cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girgensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI</title>
		<meeting>SIGCHI</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1167" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CU-SeeMe VR immersive desktop teleconferencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM International Conference on Multimedia</title>
		<meeting>4th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the Optimization of Visualizations of Complex Phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">High Tech Illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using deformations for browsing volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcguffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tancau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Design of Everyday Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Norman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<pubPlace>New York, Doubleday</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Open Scene Graph</surname></persName>
		</author>
		<ptr target="http://www.openscenegraph.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cognitive Load and Mental Rotation: Structuring Orthographic Projection for Learning and Problem Solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Pillay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Instructional Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="91" to="113" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video Flashlights: Real Time Rendering of Multiple Videos for Immersive Model Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arpa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samarasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Eurographics workshop on Rendering</title>
		<meeting>13th Eurographics workshop on Rendering</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="157" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Moving Office: Inhabiting a Dynamic Building</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schnädelbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Steadman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Koleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rodden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conference on Computer Supported Cooperative Work</title>
		<meeting>ACM Conference on Computer Supported Cooperative Work</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D Video Surveillance with Augmented Virtual Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">First ACM SIGMM International Workshop on Video Surveillance</title>
		<imprint>
			<biblScope unit="page" from="107" to="112" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mental Rotation of Three-Dimensional Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="701" to="703" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ARCBALL: a user interface for specifying threedimensional orientation using a mouse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shoemake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Graphics Interface</title>
		<meeting>Graphics Interface</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Illuminating the Path: The Research and Development Agenda for Visual Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mental Registration of 2D and 3D Visualizations (an Empirical Study)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualization Task Performance with 2D, 3D, and Combination Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding Visualization through Spatial Ability Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tremaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
