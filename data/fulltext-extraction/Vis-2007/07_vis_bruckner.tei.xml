<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Depth-Perception with Flexible Volumetric Halos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bruckner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">M</forename><forename type="middle">Eduard</forename><surname>Gröller</surname></persName>
						</author>
						<title level="a" type="main">Enhancing Depth-Perception with Flexible Volumetric Halos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Volume rendering</term>
					<term>illustrative visualization</term>
					<term>halos</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Volumetric data commonly has high depth complexity which makes it difficult to judge spatial relationships accurately. There are many different ways to enhance depth perception, such as shading, contours, and shadows. Artists and illustrators frequently employ halos for this purpose. In this technique, regions surrounding the edges of certain structures are darkened or brightened which makes it easier to judge occlusion. Based on this concept, we present a flexible method for enhancing and highlighting structures of interest using GPU-based direct volume rendering. Our approach uses an interactively defined halo transfer function to classify structures of interest based on data value, direction, and position. A feature-preserving spreading algorithm is applied to distribute seed values to neighboring locations, generating a controllably smooth field of halo intensities. These halo intensities are then mapped to colors and opacities using a halo profile function. Our method can be used to annotate features at interactive frame rates.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The area of illustrative visualization is concerned with developing methods to enhance the depiction of scientific data based on principles founded in traditional illustration. The illustration community has century-long experience in adapting their techniques to human perceptual needs in order to generate an effective depiction which conveys the desired message. Thus, their methods can provide us with important insights into visualization problems.</p><p>Resolving the spatial arrangement of complex three-dimensional structures in an image can be a difficult task. Particularly renderings of volume data acquired by imaging modalities such as CT or MRI often suffer from this problem. As such data frequently contain many fine, overlapping structures, the resulting images often look confusing and are difficult to interpret without additional cues. For this reason artists and illustrators have long exploited the fact that the human visual system is especially sensitive to local variations in contrast by drawing halos around objects. Near the boundary of objects that are located in front of other structures the tone is locally altered: the background or partly occluded objects are slightly darkened to create the impression of depth. Similarly, bright halos are frequently placed around objects to visually detach them from the background. In the most simple case, a halo is just a gap around the edges of an object. In photography and film halos, achieved by careful placement of lights and camera, are commonly used to accentuate objects. While this technique is motivated by natural lighting phenomena such as shadows and atmospheric effects, halos in illustrations are typically overemphasized and localized to enhance occlusion cues. Halos are used in a wide variety of different styles as illustrated in <ref type="figure" target="#fig_2">Figure 1</ref>. Manifestations of the effect range from thick opaque outlines to soft darkening of the background almost undistinguishable from realistic shadows under diffuse illumination. Frequently, halos are used as a subtle way to put emphasis on certain objects. Our goal in this work is to enable the same kind of flexibility for computer-generated halos in direct volume rendering. As it is often dependent on the context of the visualization which kind of halo provides the most effective cues, we present an approach which allows interactive adjustment of their appearance.</p><p>The paper is structured as follows: Related work is discussed in Section 2. We present our approach for generating volumetric halos in in Section 5. In Section 6 we discuss the implications of our method. The paper is concluded in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>One way to add depth cues to volume rendered images is to use a more realistic illumination model. Yagel et al. <ref type="bibr" target="#b28">[29]</ref> employ recursive ray-tracing which allows for effects such as specular reflection and shadows. Behrens and Ratering <ref type="bibr" target="#b1">[2]</ref> add shadows to texture-based volume rendering. The model presented by Kniss et al. <ref type="bibr" target="#b11">[12]</ref> captures volumetric light attenuation effects including volumetric shadows, phase functions, forward scattering, and chromatic attenuation. Max <ref type="bibr" target="#b18">[19]</ref> gives a comprehensive overview of different optical models for volume rendering. The problem of increasing the physical realism is, however, that these models often lack control over the specific appearance of certain structures of interest. As they are based on actual physical laws, it is difficult to control individual visualization properties separately. Some approaches therefore use inconsistent illumination. Stewart <ref type="bibr" target="#b24">[25]</ref> introduces vicinity shading, a view-independent model to enhance perception of volume data based on occlusions in the local vicinity of a sample point resulting in shadows in depressions and crevices. Lee at al. <ref type="bibr" target="#b13">[14]</ref> present a system for automatically generating inconsistent lighting based on the object geometry. Kersten et al. <ref type="bibr" target="#b8">[9]</ref> study the effect of different depth cues on the perception of translucent volumes. Many approaches have been developed which deliberately use nonphotorealistic techniques as a mechanism to improve the visualization of regions of interest. Levoy <ref type="bibr" target="#b14">[15]</ref> was the first to propose modulation of opacity using the magnitude of the local gradient to enhance surfaces in volume rendering. Csébfalvi et al. <ref type="bibr" target="#b3">[4]</ref> visualize object contours based on the magnitude of local gradients as well as on the angle between viewing direction and gradient vector using depth-shaded maximum intensity projection. Hauser et al. <ref type="bibr" target="#b5">[6]</ref> propose two-level volume rendering for focus+context visualization of volume data by combining maximum intensity projection and direct volume rendering. Nagy et al. <ref type="bibr" target="#b19">[20]</ref> combine line drawings and direct volume rendering techniques. Yuan and Chen <ref type="bibr" target="#b29">[30]</ref> enhance surfaces in volume rendered images with silhouettes, ridge and valleys lines, and hatching strokes. Zhou et al. <ref type="bibr" target="#b30">[31]</ref> propose the use of distance to emphasize and de-emphasize different regions. Viola et al. <ref type="bibr" target="#b26">[27]</ref>, inspired by cutaway views which are commonly used in technical illustrations, apply different compositing strategies to prevent an object from being occluded by less important structures. Krüger et al. <ref type="bibr" target="#b12">[13]</ref> use interactive magic lenses based on traditional illustration techniques for focus+context visualization of iso-surfaces. Multi-dimensional transfer functions have been proposed to extend the classification space which allows better selection of features. Kniss et al. <ref type="bibr" target="#b10">[11]</ref> use an intuitive direct-manipulation interface for multi-dimensional transfer functions. Lum and Ma <ref type="bibr" target="#b17">[18]</ref> use lighting transfer functions to enable  data-dependent illumination. Kindlmann et al. <ref type="bibr" target="#b9">[10]</ref> employ curvature information to achieve illustrative effects, such as ridge and valley enhancement.</p><p>Halos and similar techniques have been used by numerous researchers to enhance depth perception. As an early example, Appel et al. <ref type="bibr" target="#b0">[1]</ref> proposed an algorithm for generating haloed lines in 1979. Interrante and Grosch <ref type="bibr" target="#b7">[8]</ref> employ halos to improve the visualization of 3D flow. Their approach uses line integral convolution of a texture of slightly enlarged noise spots to compute a halo volume which is then used during ray casting. Wenger et al. <ref type="bibr" target="#b27">[28]</ref> use similar techniques for volume rendering of thin thread structures. Rheingans and Ebert <ref type="bibr" target="#b21">[22]</ref> present feature halos for scalar volume visualization. Their approach computes an additional halo volume based on properties of the original data values. Svakhine and Ebert <ref type="bibr" target="#b25">[26]</ref> extend this method for GPU-based volume rendering by computing the halo volume on the graphics hardware. Loviscach <ref type="bibr" target="#b15">[16]</ref> presents a GPU-based implementation of halos for polygonal models. Ritter et al. <ref type="bibr" target="#b22">[23]</ref> encode spatial distance in halo-like non-photorealistic shadows for the visualization of vascular structures. The approach of Luft et al. <ref type="bibr" target="#b16">[17]</ref> is capable of enhancing surface-based images using halos by performing an unsharp masking operation on the depth buffer. Their work is a major inspiration for the approach we present in this paper, although the techniques significantly differ due to the fact that in direct volume rendering halo generation can not be performed as a post-processing step.</p><p>In this paper we contribute a new technique for generating a wide variety of halo effects using GPU-based volume rendering. Our approach classifies, generates, and maps volumetric halos on-the-fly and therefore allows flexible control over their appearance. No precomputation is required and all parameters can be modified interactively. We demonstrate that this technique is effective in enhancing depth perception in volumetric data sets without obscuring features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENERATING VOLUMETRIC HALOS</head><p>Previous halo-generation approaches for volume rendering have frequently relied on a pre-processing step which generates a volume of halo contributions. This halo volume is then used during the rendering process to identify halo regions. The problem of this approach is that it does not allow for easy modifications of many parameters. In order to remedy this, our approach determines halo contributions during volume rendering. The algorithm operates on view-aligned slices through the volume in front-to-back order. In addition to regular sampling, classification, shading, and compositing, a halo generation pipeline is executed for every slice to process its halo contributions. The pipeline consists of three basic stages and an additional compositing step for blending the halo with the regular volume rendering. <ref type="figure" target="#fig_3">Figure 2</ref> illustrates this process. First, regions to emit a halo are identified. We will refer to this step as halo seeding (see Section 3.1). Next, a field of halo intensity values is generated from the seeds by applying a filtering process (see Section 3.2). Finally, the halo intensities are mapped to the actual color and opacity contributions of the halo and combined with the regular volume rendering (see Section 3.3). For simplicity, the following description is only concerned with one halo. Our approach allows multiple halos, each with its own set of parameters, to be defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Halo Seeding</head><p>We assume a continuous scalar-valued volumetric function f (P). A sample of this function at point P is denoted by f P , the gradient vector at P is denoted by ∇ f P . For generating volumetric halos we need to classify which structures should emit halos -we call this process halo seeding. During halo seeding, a seed intensity value is generated for all samples on a view-aligned slice through f . Every point with nonzero halo seed intensity is a seed point. These seed intensity values are used in the subsequent step to derive the halo intensity values for other locations.</p><p>As halos are only drawn around the contours of objects, we need to limit our seeds to these regions. In volume rendering, contours can be characterized by the angle between the view vector v and the gradient vector ∇ f P . If these vectors are nearly orthogonal, the sample point is on a contour. Furthermore, the magnitude of the gradient vector |∇ f P | can be used for preventing noise in nearly homogeneous regions to produce erroneous halo seeds. Using these two attributes, we can generate effective halo seeds for a given volumetric data set <ref type="bibr" target="#b21">[22]</ref>.</p><p>However, since we also want to generate localized halos which are only emitted by certain structures, we introduce a halo transfer function h(P). The halo transfer function consists of several separable scalar-valued functions in the range [0..1]. Our approach currently supports three different components, but this could be easily extended to include, for instance, segmentation information, if available:</p><p>Value influence function h v (P). This function is based on the data value at the sample point. It is useful, for example, for generating localized halos by limiting their influence to a certain value range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Directional influence function h d (P)</head><p>. This function is based on the direction of the eye-space normal, i.e., the angle between the projected gradient vector and the positive vertical axis of the image plane. It allows for directionally varying halos.</p><p>Positional influence function h p (P). This function is based on the distance of the sample point to a user-defined focus point to allow easy generation of localized halos for regions which cannot be identified solely using the data value.</p><p>The halo transfer function is then simply defined as the product of these components <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_0">h(P) = h v (P) h d (P) h p (P)</formula><p>The halo transfer function defines a basic seed intensity at a sample position P. This value is then combined with the gradient magnitude and the dot product between view vector and the normalized gradient vector to form the final seed intensity s(P):</p><formula xml:id="formula_1">s(P) = h(P) |∇ f P | α (1 − ∇ f P • v) β</formula><p>where α and β are used to control the influence of the gradient magnitude and the dot product, respectively. For halos these values This definition can lead to uneven halo seeds as contours identified by dot product between normalized gradient and view vector can vary in thickness. To avoid this, we could additionally employ a modulation based on the normal curvature along the viewing direction as proposed by Kindlmann et al <ref type="bibr" target="#b9">[10]</ref>. However, as our halo generation approach (see Section 3.2) takes special care to equalize halo contributions, we found that this is generally not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Halo Generation</head><p>Per definition halos are located outside of objects, while the halo seeds lie within other structures. Therefore, during halo generation the seed intensities are spread out to form a halo field image H. Each point within the halo field is defined by having nonzero halo intensity.</p><p>One important aspect of halo generation is the fact that halo contributions from smaller structures should not be lost during the spreading process. A naive approach would be a simple convolution of the halo seeds with a low-pass filter. This method, however, results in a reduction of halo contributions from smaller regions. The halo seeds of small structures are essentially blurred out of existence if the filter kernel is too large, although exactly those features could particularly benefit from the emphasis provided by a halo. If the kernel size is too small, on the other hand, the seed intensities are not distributed enough to generate a visible halo. This effect is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. In <ref type="figure" target="#fig_0">Figure 3</ref> (a) an artificial halo seed image featuring regions of multiple scale is shown. When a low-pass filter is applied to it, as shown in <ref type="figure" target="#fig_0">Figure 3</ref> (b), the seed values are spread out, but contributions from smaller areas are lost. Other approaches such as the unsharp masking technique used by Luft et al. <ref type="bibr" target="#b16">[17]</ref> also suffer from this problem. Thus, while acceptable from an aesthetic point of view, these methods are not suitable for our purpose.</p><p>A different approach to generating the halo field would be to perform a distance transform on the seed image. As this is computationally expensive and not well-defined on non-binary images, we realize a spreading approach which preserves the halos of small features while still generating a smooth halo field. The algorithm executes in N passes. During each pass i ∈ [1..N] two input images are used: H 0 , the initial image, and H i−1 , the result of the previous pass. For the first pass, the two input images are identical. For each output pixel (x, y), the algorithm first performs a convolution with a low-pass filter over the pixels of H i−1 . Then it combines the result of this operation with the corresponding pixel from H 0 :</p><formula xml:id="formula_2">H i (x, y) = δ F i (x, y) + (1 − δ F i (x, y)) H 0 (x, y) with F i (x, y) = ∑ u,v w(u, v)H i−1 (x + ku, y + kv)</formula><p>where w is the weight function of the filter, k = 2 N−i , and δ is a user-specified parameter in the range [0..1] which controls the amount of spreading. If δ is zero the unfiltered seed image will be passed through. Increasing δ causes an increased contribution from previous passes and therefore results in a smoother, more spread-out halo while still preserving higher frequency components. Small features are preserved due to the fact that spread out values are filled up in every pass.</p><p>This algorithm is effective in distributing the halo seed values to neighboring pixels without removing high frequencies. This is visible by comparing <ref type="figure" target="#fig_0">Figure 3 (b)</ref>, which uses normal low-pass filtering to <ref type="figure" target="#fig_0">Figure 3 (c)</ref>, which depicts the result obtained with our approach.</p><p>However, there is still a problem as larger regions now generate a significantly larger halo. In order to remedy this, we apply the spreading algorithm to the gradient of the halo seed image instead of the original which equalizes the contributions, as illustrated in the right column of <ref type="figure" target="#fig_0">Figure 3</ref>. <ref type="figure" target="#fig_0">Figure 3 (d)</ref> depicts the gradient of the seed image. <ref type="figure" target="#fig_0">Figure 3</ref> (e) shows that only applying a low-pass filter to the gradient image is not effective. The presented process applied to the gradient of the halo seed image, however, results in a smooth halo field with equalized contributions from structures of multiple scale, as depicted in <ref type="figure" target="#fig_0">Figure 3 (f)</ref>.</p><p>If some reduction of high frequencies is desired, a median filtering could be applied to the seed image before the spreading process. However, in our experiments we found that this is not necessary in general. We use a filter kernel size of 3 × 3 with Gaussian weights. The number of iterations N determines the maximum amount of spreading that can occur -for all our purposes a value of N = 4 has shown to be sufficient. This algorithm is conceptually similar to the jump flooding paradigm for parallel computing <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_5">Figure 4</ref> shows results of the spreading processes for different values of δ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Halo Mapping and Compositing</head><p>After generating the halo field image it has to be mapped to visual contributions in the image. For this purpose we employ a halo profile function: this function maps all nonzero halo intensity values to colors and opacities. Halo intensities of zero are always transparent. While the spreading parameter δ only controls the distribution of intensities in the halo field, the profile function allows further adjustment of the halo appearance.</p><p>In the simplest case, the halo profile function just maps halo intensities directly to opacities using a constant color. Other possibilities include, for instance, a halo profile with constant opacity which results in a sharp border. <ref type="figure" target="#fig_6">Figure 5</ref> shows a few examples. In the last row of this figure, the use of directionally varying halos is also demonstrated. Finally, the mapped halo has to be combined with the volume's contribution. Based on how this combination is performed, we can distinguish between two different kinds of halos: From the point of view of compositing, the halo behaves as if it were part of the volume. Thus, for emissive halos the halo intensity value is first mapped using the halo profile function and then blended after the actual volume contributions using the front-to-back formulation of the over-operator. The halo therefore (partially) occludes everything located behind it including the background.</p><p>Occlusive halos. In addition to emissive halos, illustrators sometimes employ another kind of halo: the halo only contributes to the image if it occludes other structures -the halo by itself has no emissive contribution. Although similar to a shadow, this type of halo is usually drawn in the same style irrespective of the distance between the two objects and not necessarily consistent with the overall lighting conditions to strengthen the occlusion cues. This type of halo can be useful as it might be less intrusive and only highlights occlusions. For generating occlusive halos, contributions of the halo field image H need to be accumulated to be able to influence samples located behind them -this is similar to a shadow buffer <ref type="bibr" target="#b11">[12]</ref>. For this purpose, we introduce an additional halo occlusion image O. The current halo field image H is combined with O in every pass using maximum blending. Halo mapping is then performed based on the halo occlusion image. The resulting mapped halo color is mixed with the volume sample color using the mapped halo contribution's opacity as an interpolation weight. The opacity of the volume sample remains unchanged. Thus, if no sample is occluded by the halo, it has no contribution to the image.</p><p>Both halo types are useful for different purposes. While emissive halos can be used to emphasize particular features by giving them an outline or a glow, occlusive halos provide a means for accentuating oc-   <ref type="figure" target="#fig_7">Figure 6</ref> (c) a one-sided halo was generated using the directional component of the halo transfer function. <ref type="figure" target="#fig_7">Figure 6 (d)</ref> shows an omnidirectional halo for comparison. The advantage of this approach over realistic shadows lies in the fact that it can be easily adjusted without influencing the global appearance while providing emphasis in the targeted areas. Illustrators generally do not draw physically correct shadows, but they exploit the fact that the human visual system interprets this cue as an indicator for occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>Our GPU-based volume renderer with halo support was implemented in C++ using OpenGL/GLSL. As already outlined in Section 3 our approach for integrating halos with direct volume rendering is based on an interleaving of the halo generation pipeline with conventional view-aligned slicing of the volume. In this section, we discuss further details of our implementation.</p><p>Initially, six off-screen buffers are generated: I 0 and I 1 are used for compositing in a ping-pong approach. In our description, we use I p to denote the accumulated image from the previous iteration, and I c for the image written in the current iteration -they are swapped after each iteration. The buffer S stores the halo seed image, and H 0 , H 1 , and H 2 are used for halo generation. We use H to denote the buffer which contains the current halo field image, which can be either H 1 or H 2 . For occlusive halos, an additional halo occlusion image O is required. We use the OpenGL ARB framebuffer object extension to render to and read from these buffers. The renderer slices the volume in viewing direction and has two basic phases:</p><p>Rendering. Although described separately in Section 3, as they are conceptually different stages, it is advantageous to combine halo seeding and halo mapping with regular sampling, classification, shading, and compositing in a single rendering pass. Since all these steps require the data value and gradient at the same sample location, this avoids redundant texture reads and computations and eliminates the need for an explicit mapped halo image. We take advantage of OpenGL's capabilities to write to multiple render targets in one rendering pass. The two images written to are I c , the current accumulated image, and S, the halo seed image. First, conventional sampling, classification and shading is performed. Next, halo mapping is performed on the halo intensities read from the current halo field image H for emissive halos. For occlusive halos, the halo occlusion image O is used instead. For emissive halos, the shaded color of the volume sample is first composited with the previously accumulated color read from I p and then written into I c . Then the mapped halo contribution is composited. In the case of an occlusive halo the shaded sample color is mixed with the mapped halo contribution and then composited with the previously accumulated color. Finally, halo seeding is performed and the result is written to S.</p><p>Generation. This phase performs halo generation as described in Section 3.2. First, the gradient magnitude of the halo seed image is computed and written into H 0 . Next, several iterations  Halo generation is the most expensive part of the rendering procedure. However, it is not necessary to perform this step for every iteration. We can use OpenGL's blending functionality to accumulate the halo seeds of several slices and then perform halo generation on this image only every M-th slice. As the seed contributions are accumulated, no features will be missed. Only if M is chosen too high, some depth accuracy is sacrificed. In practice, a value of M = 4 has proven to result in no visible artifacts -all figures in this paper were generated using this setting.</p><p>As the GPU operates on vectors rather than scalars we can support four distinct halos, each with its own set of parameters, without additional costs. Each halo is assigned a color channel and all operations, including halo generation, are simply performed on four halo channels instead of one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Halos provide a simple additional option for the generation of volumetric illustrations. They do not require any pre-processing and can be easily integrated into existing volume visualization tools. Similar to layer effects in image editing software such as Adobe Photoshop, they can be applied to enhance or highlight specific regions with great stylistic flexibility ranging from opaque contour-like lines to smooth object shadows. These techniques are ubiquitous in traditional illustrations and so it makes sense that volume visualization can benefit from them. In our experiments, we found that volumetric halos can be an effective and versatile tool for enhancing the visualization of volume data with little additional effort. A wide variety of data sets can benefit from halos. In this section, we present a small selection of visualization results and compare them with un-enhanced depictions.</p><p>Halos are effective for the visualization of transparent objects. Illustrators frequently employ this technique in line drawings: The background object is rendered in full detail and disappears as it approaches the boundaries of the overlapping translucent structure <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure" target="#fig_9">Figure 7</ref> demonstrates this approach: an opaque halo is used to enhance the contours of the transparent skin. Additionally, a smooth dark halo is assigned to the bone to add depth to the image. <ref type="figure" target="#fig_11">Figure 8</ref> demonstrates the use of the positional component of the halo transfer function. A focus point is placed in the pelvic region and a bright halo is used to indicate occlusions. Through these simple means the viewer's attention is directed to the aorta. <ref type="figure" target="#fig_12">Figure 9</ref> shows the combination of different halos for the same data value range. A smooth dark halo and a thin bright halo are assigned to the outer part of the engine block. The darkening of the white contour halo helps in indicating occlusion relationships. The inner structures additionally have a slight cyan glow. Halos may also serve as a complete replacement for normal gradient-based illumination similar to the approach described by Desgranges et al. <ref type="bibr" target="#b4">[5]</ref>. Depending on the style of the halo, this results in an additional degree of stylization which is useful to depict contextual objects. In <ref type="figure" target="#fig_2">Figure 10</ref> we demonstrate this effect. To evaluate the performance of our implementation we compared a standard volume renderer with our algorithm. We used the UNC head test dataset (dimensions: 256 × 256 × 256) and an object sample distance of 0.5. The viewport size was 512 × 512. No high-level optimizations such as empty-space skipping were used. Our test system was an AMD Athlon 64 X2 Dual 4600+ CPU equipped with an NVidia GeForce 8800 GTX GPU. We performed a 360 degree rotation along each axis and averaged the frame rates. The reference renderer without halos achieved 29.34 frames/second in this benchmark. The frame rates of our halo renderer for different values of M are shown in <ref type="table" target="#tab_0">Table 1</ref>. Compared to the reference renderer, our approach achieves approximately one third of the frame rate for the typical setting of M = 4. Halo seeding and mapping alone, due to the increased number of texture fetches and operations per sample, result in a slowdown of about 50 percent. Although halo processing incurs an overhead, interactive frame rates are still achieved. Moreover, as our implementation is not heavily optimized, there is potential room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND FUTURE WORK</head><p>As demonstrated in this paper, halos can be used to generate easily controllable inconsistent lighting, which is not physically plausible but aesthetically pleasing. Localized shadow-like halos help to emphasize spatial relationships in an non-intrusive way as they do not change the global lighting situation. As shown by Ostrovsky et al. <ref type="bibr" target="#b20">[21]</ref>, humans are largely insensitive to such inconsistencies. Artists and illustrators therefore use inconsistent lighting to guide the viewer's attention and to enhance comprehensibility. Cavanagh <ref type="bibr" target="#b2">[3]</ref> has suggested that our brain perceives the shape-from-shading cues only locally. This might be the reason why local cues which are inconsistent with global lighting are so effective. With volumetric halos we can generate this kind of illumination in an interactive result-oriented manner. In general, halos perform best for volumetric data which contains clearly defined features such as tomographic scans. For rather amorphous data, their benefit is limited due to the absence of distinct boundaries.</p><p>Currently, we provide a simple interactive user-interface for changing all halo parameters. The value influence function of the halo transfer function is specified using a conventional transfer function widget. The user has the option to link this function with the normal transfer function, i.e., that halos are connected to the corresponding visible structures. For the directional influence function, we provide a simple angular brushing widget which allows the specification of a range of directions on a radial layout. The positional influence function is defined by clicking on a point in the image. A viewing ray is cast from this position and records the first intersection with visible structures. The focus point is set to this position. By dragging the mouse, the inner radius and transition of the spherical focus region can be modified. The halo profile function is defined using a gradient widget. Other parameters, such as the spreading parameter δ and an additional opacity scale of the profile function can also be edited using a click-and-drag interface. While these tools are very basic, they have proven to be surprisingly effective in quickly improving the appearance of volume renderings. However, there are some enhancements that could be easily accomplished. Extending the focus point to a user-drawn polyline, for example, would be a useful extension. The resulting geometry could then be sliced in parallel to the volume and serve as an input to halo seeding. This approach would allow for even better localization of halos without the need for explicit segmentation. Other such sketch-based interaction metaphors open an interesting direction for further research.</p><p>While our interactive approach allows for quick adjustment of halo parameters, an unsolved question is which settings perform best for which types of data. A study of perceptual performance could lead to valuable insights and the resulting data could be used to derive halo templates for different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we presented volumetric halos as an effective and flexible method for enhancing depth perception in volume renderings. The technique was motivated by the wide-spread use of similar concepts in art and illustration. Our approach does not require pre-processing, as halo seeding, generation, and mapping are performed on-the-fly. During halo seeding, a halo transfer function is used to identify structures which emit a halo. A feature-preserving spreading procedure was discussed, which distributes halo seed intensities to neighboring sample positions. A halo profile function is used to map the resulting halo field to colors and opacities. We also introduced the notion of emissive and occlusive halos. Furthermore, we presented a GPU-based implemen- tation of this algorithm which achieves interactive frame rates. Using several examples, we demonstrated the effectiveness of halos as a simple technique to enhance depictions of volumetric data. Based on the promising results of the presented method and due to the fact that it is easy to integrate into existing volume rendering approaches, volumetric halos can be a useful addition to the illustrative visualization toolbox.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Section 3 .</head><label>3</label><figDesc>Section 4 details our implementation. Results are presented • The authors are with the Institute of Computer Graphics and Algorithms, Vienna University of Technology, Austria, E-mail: {bruckner|groeller}@cg.tuwien.ac.at. Manuscript received 31 March 2007; accepted 1 August 2007; posted online 27 October 2007.For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Published 14</head><label>14</label><figDesc>September 2007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Examples for the different uses of halos in medical illustration for emphasis and accentuation. The images are taken from the Medical Illustration Source Book (http://www.medillsb.com).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the halo pipeline. The volume is processed in view-aligned slices. Halo seeding classifies halo-emitting structures, halo generation distributes the seed intensities, and halo mapping assigns colors and opacities to these seed intensities. Compositing combines the mapped halo intensities with the actual volume rendering. are usually fixed and do not require adjustment. We use values of α = 32 and β = 0.125 for all the images in this paper. The values of s are clamped to the range [0..1]. The result of applying s to all pixels of the view-aligned slice is the halo seed image S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of halo generation strategies on a test image. Left column: (a) Original halo seed image. (b) Low-pass filtered halo seed image. (c) Spreading process applied to halo seed image. Right column: (d) Gradient of halo seed image. (e) Low-pass filtered gradient image. (f) Spreading process applied to gradient image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Results of the halo spreading algorithm using N = 4 iterations with (a) δ = 0.70, (b) δ = 0.80, (c) δ = 0.90, (d) δ = 0.95. Emissive halos. Similar to scattering of light by small particles such as fog, this type of halo causes a visible contribution by itself.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Different halo profile functions applied to a simple data set. The corresponding halo profile function is shown for each image. clusions by enhancing the contrast in areas where one object crosses another. For instance, occlusive halos are frequently used in depictions of vascular structures. Figure 6 shows a comparison of these two styles. Figure 6 (a) depicts a volume rendering without halos. In Figure 6 (b) an emissive halo is used for bones and vessels while Figure 6 (c) employs an occlusive halo. The emissive halo also generates a dark border around objects which do not occlude other structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 (</head><label>6</label><figDesc>c) and (d) also demonstrate the use of the directional component of the halo transfer function. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Emissive and occlusive halos. (a) Volume rendering without halos. (b) A directional emissive halo is emitted by bones and vessels. (c) A directional occlusive halo is emitted by bones and vessels -it is only visible where it occludes other structures. (d) The occlusive halo emitted by bones and vessels is omnidirectional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Improving visualization of transparent structures using halos. (a) Volume rendering without halos. (b) Depth is added to the bone by using a dark smooth halo, contours of the the transparent skin are enhanced using a white opaque halo. (c) A different halo profile function is used for the skin (fade from white to yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 10(b) and Figure 10 (c) depict different combinations of halos using shading, while Figure 10 (d) uses no additional shading. The stylized cartoon-like effect in Figure 10 (d) is achieved by a smooth background-colored halo in combination with a black contour halo.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Using halos to add occlusion cues. (a) Volume rendering without halos. (b) A white opaque halo is specified in the pelvic region using a focus point in order to accentuate the aorta.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Combining multiple halo effects. (a) Volume rendering without halos. (b) The semi-transparent part of the engine block uses a dark smooth halo and a white opaque halo. The inner structures have an additional cyan glow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 .</head><label>10</label><figDesc>Combining global and regional halos. (a) Volume rendering without halos. (b) A smooth shadow-like halo is used to improve depth perception. (c) An additional bright semi-transparent halo is placed around the colon. (d) Rendering without shading -a smooth white halo and a black contour halo are used to indicate the three-dimensional structure resulting in a stylized cartoon-like image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of halo rendering measured on an AMD Athlon 64 X2 Dual 4600+ CPU equipped with an NVidia GeForce 8800 GTX GPU. The volume size was 256 × 256 × 256 with a viewport size of 512 × 512 and an object sample distance of 0.5. The reference renderer without halos achieved 29.34 frames/second in this benchmark.of the spreading algorithm are executed on H 0 by ping-ponging between buffers H 1 and H 2 . The final halo field image is then located in buffer H 1 or H 2 , depending on the number of iterations. The current halo field image H is set to this buffer. If occlusive halos are used, this image is additionally blended with the halo occlusion image O.</figDesc><table><row><cell cols="3">M frames/second % of reference</cell></row><row><cell>1</cell><cell>4.65</cell><cell>15.85</cell></row><row><cell>2</cell><cell>7.72</cell><cell>26.31</cell></row><row><cell>4</cell><cell>10.26</cell><cell>34.97</cell></row><row><cell>8</cell><cell>12.59</cell><cell>42.91</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the anonymous reviewers for their valuable comments. The work presented in this publication was carried out as part of the exvisation project (http://www.cg.tuwien.ac.at/research/vis/exvisation) supported by the Austrian Science Fund (FWF) grant no. P18322.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The haloed line effect for hidden line elimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Rohlf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1979</title>
		<meeting>ACM SIGGRAPH 1979</meeting>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adding shadows to a texture-based volume renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Volume Visualization</title>
		<meeting>IEEE Symposium on Volume Visualization</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pictorial art and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The MIT Encyclopedia of the Cognitive Sciences</title>
		<editor>R. A. Wilson and F. C. Keil</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="644" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast visualization of object contours by non-photorealistic volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Csébfalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="452" to="460" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradient-free shading: A new method for realistic interactive volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Desgranges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paladini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vision, Modeling, and Visualization</title>
		<meeting>Vision, Modeling, and Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-level volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-I</forename><surname>Bischi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Guild Handbook of Scientific Illustration</title>
		<editor>E. R. S. Hodges</editor>
		<imprint>
			<date type="published" when="2003" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>2 nd edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Strategies for effectively visualizing 3D flow with volume LIC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Interrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="421" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing depth perception in translucent volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1117" to="1124" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Curvature-based transfer functions for direct volume rendering: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multidimensional transfer functions for interactive volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="285" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A model for volume lighting and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Premoze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcpherson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ClearView: An interactive context preserving hotspot visualization technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="941" to="948" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometry-dependent lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stylized haloed outlines on the GPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loviscach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image enhancement by unsharp masking the depth buffer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Colditz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 2006</title>
		<meeting>ACM SIGGRAPH 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1206" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lighting transfer functions using gradient aligned sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive volume illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vision, Modeling, and Visualization</title>
		<meeting>Vision, Modeling, and Visualization</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceiving illumination inconsistencies in scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cavanagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1301" to="1314" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Volume illustration: Nonphotorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Peitgen. Real-time illustration of vascular structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dicken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="877" to="884" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jump flooding in GPU with applications to voronoi diagram and distance transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Symposium on Interactive 3D Graphics and Games</title>
		<meeting>ACM Symposium on Interactive 3D Graphics and Games</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vicinity shading for enhanced perception of volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interactive volume illustration and feature halos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Svakhine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific Conference on Computer Graphics and Applications</title>
		<meeting>the Pacific Conference on Computer Graphics and Applications</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Importance-driven feature enhancement in volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="408" to="418" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive volume rendering of thin thread structures within multivalued scientific data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="664" to="672" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Realistic volume imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Illustrating surfaces in volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint IEEE/EG Symposium on Visualization</title>
		<meeting>Joint IEEE/EG Symposium on Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distance based enhancement for focal region based volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Döring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Tönnies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Bildverarbeitung für die Medizin</title>
		<meeting>Bildverarbeitung für die Medizin</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="199" to="203" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
