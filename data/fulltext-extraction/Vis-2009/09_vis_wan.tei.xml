<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Interactive Visualization Tool for Multi-channel Confocal Microscopy Data in Neurobiology Research</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-10-11">11 October 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wan</surname></persName>
							<email>wanyong@cs.utah.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideo</forename><surname>Otsuna</surname></persName>
							<email>ostuna@neuro.utah.edu</email>
							<affiliation key="aff0">
								<orgName type="department">is with Scientific and Imaging Institute</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Bin</forename><surname>Chien</surname></persName>
							<email>chi-bin.chien@neuro.utah.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Hansen</surname></persName>
							<email>hansen@cs.utah.edu</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Charles Hansen is with Scientific and Imaging Institute at</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Interactive Visualization Tool for Multi-channel Confocal Microscopy Data in Neurobiology Research</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-10-11">11 October 2009</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2009; accepted 27 July 2009; posted online</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visualization</term>
					<term>neurobiology</term>
					<term>confocal microscopy</term>
					<term>qualitative analysis</term>
					<term>volume rendering</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Confocal microscopy is widely used in neurobiology for studying the three-dimensional structure of the nervous system. Confocal image data are often multi-channel, with each channel resulting from a different fluorescent dye or fluorescent protein; one channel may have dense data, while another has sparse; and there are often structures at several spatial scales: subneuronal domains, neurons, and large groups of neurons (brain regions). Even qualitative analysis can therefore require visualization using techniques and parameters fine-tuned to a particular dataset. Despite the plethora of volume rendering techniques that have been available for many years, the techniques standardly used in neurobiological research are somewhat rudimentary, such as looking at image slices or maximal intensity projections. Thus there is a real demand from neurobiologists, and biologists in general, for a flexible visualization tool that allows interactive visualization of multi-channel confocal data, with rapid fine-tuning of parameters to reveal the three-dimensional relationships of structures of interest. Together with neurobiologists, we have designed such a tool, choosing visualization methods to suit the characteristics of confocal data and a typical biologist&apos;s workflow. We use interactive volume rendering with intuitive settings for multidimensional transfer functions, multiple render modes and multi-views for multi-channel volume data, and embedding of polygon data into volume data for rendering and editing. As an example, we apply this tool to visualize confocal microscopy datasets of the developing zebrafish visual system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>There has been a tremendous explosion in the popularity of confocal microscopy <ref type="bibr" target="#b5">[6]</ref> in recent years, due to its ability to produce highquality 3D images, scan fluorescent specimens that have a thickness of hundreds of microns, and generate time sequence images of living cells and tissues as 4D data. The discovery of fluorescent proteins <ref type="bibr" target="#b16">[17]</ref> provides an invaluable approach for marking biological targets. When fluorescent proteins or dyes of different emission wave lengths are used for marking different cell/tissue types in confocal scannings, the resulting image datasets are multi-channel.</p><p>In neurobiology, confocal technology is widely used for studying the three-dimensional structure of the nervous system; <ref type="figure">Figure 1</ref> shows the typical workflow. Visualization tools are required for qualitative analysis, which gives an overall evaluation of the experiment results, and higher quality and interactivity of these tools can help researchers decide which quantitative measurements to make, and extract biologically significant conclusions.</p><p>However, most neurobiologists' tools for qualitative analysis are rudimentary, such as looking at image slices or maximal intensity projections. There are several academic and commercial visualization packages available, but these have various significant feature limitations when applied to multi-channel confocal data. Despite the plethora of volume rendering techniques that have been available for many years, there is a real demand from neurobiologists, and biologists in general, for a flexible visualization tool that allows interactive visualization of multi-channel confocal data, with rapid fine-tuning of parameters to reveal the three-dimensional relationships of structures of interest.</p><p>Confocal microscopy data have their own characteristics, which differ from other biomedical data, such as CT or MRI, which must be taken into consideration as we design such a tool for confocal microscopy visualization:</p><p>Multi-channel data: As mentioned above, labeling with different fluorescent proteins and fluorescent dyes yields multi-channel data, with each channel representing a different cell or tissue type. Usually the data in different channels are spatially interwoven, with data from one channel having the highest interest, such as the channel containing labeled neuron fibers.</p><p>Subtle boundaries: Clearly visualized boundaries of brain regions are often essential for analysis, as when analyzing connectivity of neuron fibers between regions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref>. However, biologically meaningful boundaries may be only subtly presented in the confocal data, and may be present in only one channel of the multi-channel data. Thus, boundary segmentation must often be done manually.</p><p>Finely detailed structures: Biomedical techniques such as antibody staining and gene transfer allow delivery of fluorescent dyes to specific cell or tissue types, which can result in very finely detailed structures, such as neuronal fibers or synapses.</p><p>Visual occluders and noise: Structures irrelevant to the analysis may also be labeled through the fluorescent staining process, resulting in visual occluders that obscure the structures to be visualized. Fine detailed structures can also be obscured by noisy data, due to statistical noise or electronic noise from the scanning device <ref type="bibr" target="#b6">[7]</ref>.</p><p>Working together with neurobiologists, we have designed an interactive visualization tool, which suits a typical biologist's workflow and meets the challenges listed above. The contributions of our work and this application paper to visualizing confocal microscopy data are:</p><p>Interactive settings of volume rendering properties to maximize rendering quality: For better rendering quality and depth perception, we add shading and depth cueing to volume rendering. For detail enhancement and noise suppression, a 2D transfer function can be set through intuitive parameters. All the volume rendering parameters take effect interactively.</p><p>Multi-modes and multi-views for multi-channel data visualization: The multi-channel dataset can be combined in a single render view with different render modes, with each mode showing a different aspect of the data. With multi-view, different render modes can be displayed at the same time, or several datasets can be compared.</p><p>Embedding polygon data into volume data for region definition and volume editing: Biological boundaries are usually manually extracted as polygon data with segmentation tools. These polygon data can be rendered together with volume data, which is a clear and efficient way to show the regions of interest. Furthermore, polygon data can be used to trim the volume data, and volume data within different regions defined by polygon data can have different property settings to aid visualization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We have drawn our techniques from previous work on 3D visualization, including volume rendering, transfer function settings, and polygon rendering. Cai and Sakas <ref type="bibr" target="#b3">[4]</ref> proposed three levels of data intermixing and rendering pipelines in direct multi-volume rendering, which include image level intensity intermixing, accumulation level opacity intermixing, and illumination model level parameter intermixing. They applied their method to radiotherapy treatment planning, and compared the features of each method. Rossler et al. <ref type="bibr" target="#b19">[20]</ref> described a framework for GPU-based multi-volume rendering, which was used for the visualization of functional brain images. In their framework, they provide a correct overlaying of an arbitrary number of volumes, with the visual output for each volume independently controlled. In his thesis work, Grimm <ref type="bibr" target="#b7">[8]</ref> presented a full-blown high-quality raycasting system, which can efficiently process and visualize multiple large medical volume datasets.</p><p>Kniss et al. <ref type="bibr" target="#b10">[11]</ref> proposed using multidimensional transfer functions for interactive volume rendering, and used a set of direct manipulation widgets for transfer function settings. Seg3D [22] uses the widgets described in Kniss's paper to set 2D transfer functions for volume rendering. Rezk-Salama et al. <ref type="bibr" target="#b18">[19]</ref> presented a framework for implementing semantic models for transfer function assignment in volume rendering applications and demonstrated that semantic models can effectively be used to hide the complexity of visual parameter assignment from the non-expert user for a specific examination purpose. Everitt <ref type="bibr" target="#b4">[5]</ref> described an algorithm for interactively rendering orderindependent transparent polygon objects, also known as depth peeling, with graphics hardware. The depth peeling algorithm is widely used for correctly blending transparent polygon meshes. Kreeger and Kaufman <ref type="bibr" target="#b11">[12]</ref> presented an algorithm that embeds opaque and/or translucent polygons within volumetric data, by rendering thin slabs of the translucent polygons between volume slices using slice-order volume rendering. They demonstrated their algorithm with examples of medical applications and flight simulators. Nagy and Klein <ref type="bibr" target="#b13">[14]</ref> presented the concept of volumetric depth-peeling, and they separated the volume data into interior and exterior based on a fixed iso-value. Weiskopf et al. <ref type="bibr" target="#b24">[26]</ref> proposed clipping methods that are capable of using complex geometries for volume clipping, which enable selecting and exploring subregions of the dataset.</p><p>There is excellent previous work on visualization and segmentation of data from optical microscopes. Janoos et al. <ref type="bibr" target="#b9">[10]</ref> presented a method to reconstruct dendrites and spines from optical microscope data by using a surface representation, and the dendrites and spines are visualized in a manner that displays the spines' types and the inherent uncertainty in identification and classification. Mosaliganti et al. <ref type="bibr" target="#b12">[13]</ref> described methods to reconstruct cellular biological structures from optical microscopy data, and they applied their methods to light, confocal and phase-contrast microscopy data.</p><p>There are some commercially available software packages that can be used for visualizing confocal data. Amira <ref type="bibr" target="#b23">[25]</ref> can render volume datasets from confocal microscopes, and visualize them together with polygon data, which are usually generated by its segmentation tool automatically or manually. Imaris <ref type="bibr" target="#b1">[2]</ref> incorporates multiple volume rendering algorithms for visualizing microscopy data interactively, and it can also generate polygon data for rendering or volume editing. Volocity <ref type="bibr" target="#b8">[9]</ref> can load multi-channel confocal data, and it provides both interactive and non-interactive volume renderers for visualizing them. The neurobiologist users often feel there are still problems with these tools: many don't provide adequate parameter settings for fine-tuning volume rendering results; some are not interactive when adjusting parameters; and it is always laborious to analyze repetitive experiments. <ref type="figure">Figure 1</ref> shows a detailed workflow of qualitative analysis, which in neurobiological research, answers questions such as whether certain types of cells are present in a region, how neuron fibers connect different regions, and if there is a difference between samples. In this workflow, pre-processing often consists of basic image processing techniques such as noise reduction and contrast enhancement; median filters are usually used for noise reduction <ref type="bibr" target="#b15">[16]</ref>. Segmentation and visualization steps are sometimes iterative, involving generation of polygon data, combining the rendering of polygon data and volume data, and regenerating polygon data. In the visualization steps, details of the datasets are examined, which requires fine-tuned rendering quality with great interactivity. In the comparison step, datasets from different samples are often compared, such as datasets of a mutant and a wildtype sample of zebrafish, or datasets from replicate experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VISUALIZATION OF CONFOCAL MICROSCOPY DATA FOR QUALITATIVE ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Workflow of Qualitative Analysis of Confocal Microscopy Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interactive Volume Rendering and Rendering Quality Enhancement</head><p>We use GPU slice-based volume rendering for real-time display and user interaction. Optical properties, color information, and opacities are assigned and blended <ref type="bibr" target="#b22">[24]</ref>. Compared to looking at image slices and maximum intensity projections, our tool can provide better perception of the spatial structures. Compared to volume rendering methods previously used in neurobiology, our tool has the advantage of providing a strong visual cue for orientation and depth, and high interactivity. It is useful not only for single dataset visualization but for comparing several different samples, especially when the datasets are scanned with samples oriented differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Shading and Depth Cueing</head><p>Our tool provides better perception for 3D spatial structures by adding shading and depth cueing to the volume rendering <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b2">3]</ref>. User adjustable settings allow fine-tuning of these effects. Shading is calculated according to the Phong model <ref type="bibr" target="#b17">[18]</ref>, where normals are approximated from gradients and stored in the 3D textures. <ref type="figure" target="#fig_0">Figure 2A</ref> shows the default shading effect, and <ref type="figure" target="#fig_0">Figure 2B</ref> shows the result after changing the ambient intensity, and shapes of local features, such as individual cells, are better perceived. Our tool lets user set the voxel aspect ratio for loaded volume datasets, as confocal volume usually has lower Z resolution and thus the voxels are anisotropic. To avoid lighting artifacts caused by changing voxel aspect ratio, the pre-calculated normals are rescaled in the shader programs according to user settings.</p><p>Depth cueing is applied by attenuating the intensity values according to the relative depths of voxels, and the attenuated intensity value is calculated with the following equation:</p><formula xml:id="formula_0">V attn = f ×V bg + (1 − f ) ×V data , f = d data −d f ront d back −d f ront ×V scale Where V</formula><p>attn is the attenuated intensity, V bg is the background intensity, V data is the voxel intensity, d data , d f ront and d back are the distances of the voxel, front and back of data from the view point, and V scale is a parameter controlling how much attenuation is applied, which can be adjusted by the user. <ref type="figure" target="#fig_0">Figure 2C</ref> shows the results after depth cueing is applied, where the overall shape of the 3D structure is more apparent than with only shading ( <ref type="figure" target="#fig_0">Figure 2B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Intuitive and Efficient Transfer Function Settings</head><p>2D transfer functions <ref type="bibr" target="#b10">[11]</ref> are used for setting rendering properties of volume data, as their boundary extracting capability can render fine structures from confocal data. We found, however, that neurobiolo-gists prefer intuitiveness and efficiency to complicated transfer function widgets and settings. With this in mind, we chose a family of the 2D transfer functions that best suits confocal data structure extraction, while the parameters for fine-tuning the shapes of the transfer function are chosen and named for better operability. The shape of the 2D transfer function, as well as the parameters, are illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.  Boundary extraction: Controls the cut-off value of gradient magnitude. Setting a higher value can isolate better-defined boundaries in the volume data. <ref type="figure" target="#fig_0">Figure 2F</ref> shows that spreading of nuclei is seen in a combined rendering with other channels. By increasing the boundary extraction value, only the voxels defining nucleus boundaries are rendered. Combined with transparency adjustment, both the underlying channels and the spreading of nuclei are seen, which is not possible by adjusting transparency solely ( <ref type="figure" target="#fig_0">Figure 2E</ref>).</p><p>Offset: Sets the intensity peak in the 2D transfer function, so that voxels with the corresponding intensity value are accentuated. <ref type="figure" target="#fig_0">Figure 2H</ref> and I show that the continuity of neuron fibers is recovered after adjusting intensity offset.</p><p>Low and high thresholds: Set the low and high cut-off values of scalar intensity. These values are useful for noise suppression. <ref type="figure" target="#fig_0">Figure 2J</ref> and K show an example before and after the threshold values are adjusted; noisy data are eliminated after adjusting the low threshold value.</p><p>Gamma: Controls how values off the intensity peak are attenuated by adjusting the exponent of the intensity values. Gamma is adjusted to get a better contrast of the output renderings.</p><p>For multi-channel volume dataset, transfer function for each channel can be adjusted independently. Our tool lets users interact with a limited set of parameters, with each parameter adjusted by either linked slider or numerical entry. The corresponding parameter settings in the user interface are listed in <ref type="figure" target="#fig_7">Figure 9</ref>. By avoiding complicated widgets or the jargon of transfer function settings, the provided interface is more intuitive for neurobiologists to use and can quickly obtain the desired visualization results. Users can also save the settings of previous work, and apply them to similar datasets, or use them as a starting point for later fine-tuning, which further accelerates the visualization workflow. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-modes and Multi-views for Multi-channel Volume Data</head><p>For multi-channel confocal microscopy data, qualitative analysis usually requires visualizing the spatial relationship between data from different channels. When combined together, however, data from different channels often interfere with each other, and details of interest from one channel can be occluded. Our tool provides three render modes suggested by our collaborating neurobiologists for multi-channel volume data. When used jointly, both the spatial relationships and details can be visualized clearly. <ref type="figure" target="#fig_3">Figure 4</ref> compares the results of same threechannel dataset with different modes, and the three render modes are:</p><p>Layered mode: Similar to layers in 2D painting software, the volume data are layered on top of one another, rendered in the order of channels specified by the user. In this mode, the top layer data cover the lower ones. This does not respect the relative depth relationships within the data, especially during user interaction. Visualization experts did not expect this mode to be effective. Surprisingly neurobiologists often prefer this mode since it can better show fine inner structures, such as neuron fibers, when placed in the top layer ( <ref type="figure" target="#fig_3">Figure 4A</ref>).</p><p>Depth sorted mode: The multi-channel volume data are blended first for each polygon slice and then the slices are blended together. This is the correct way to show the spatial relationships between channels, and most visualization tools that support multi-channel datasets use this mode. But sometimes the fine structures from one channel are covered by voxels from other channels with lower depth values. Lowering the transparency of the obstructing data can reveal the deeper structures, but usually the details of the obstructing data are lost ( <ref type="figure" target="#fig_3">Figure 4B</ref>).</p><p>Composite mode: This is the image level intermixing described by Cai and Sakas <ref type="bibr" target="#b3">[4]</ref>. Each dataset of the multi-channel volume data is first rendered into a texture, and the textures are composed into the final rendering with color component addition. As shown in <ref type="figure" target="#fig_3">Figure 4C</ref>, information from all channels can be seen at the same time, as long as distinguishable colors are used. As it is not necessary to increase the transparency of the occluding channels, the renderings of all channels are bright and full of details. As most datasets in confocal research have three channels or less, it is most effective to set colors as pure red, green, and blue. Shading effect calculation is clamped to single color components if data channels are set to pure red, green, and blue. So the original data channel information can still be extracted from the exported images of this mode, by separating the color channels. And this is important for further processing and publishing.</p><p>Neurobiologists may find features they need in each mode, with each mode best suiting certain applications. Joint views of different render modes can allow even better data comprehension. We provide an interface to allow the neurobiologists to switch between the render modes quickly, and multiple viewports can be set for different render modes, which can be operated separately, or synchronized to the same viewing direction.</p><p>Multi-views are indispensable when comparing different datasets in the qualitative analysis workflow. Datasets from replicate samples or from mutants and wildtypes are visualized and compared in different views. Like the transfer function settings, users can set the views quickly and accurately, or let the tool remember the view settings for later comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Embedding Polygon Data for Region Definition and</head><p>Volume Editing As mentioned above, incorporation of biologically meaningful boundaries can greatly aid interpretation of confocal data. However, boundaries often cannot be reconstructed simply by setting transfer functions or through automatic segmentation, so that polygon data resulting from manual segmentation of the volume data are necessary to visualize the boundaries. For some applications such as crude region definition or volume culling, simple polygon geometries can be generated on the fly, and translated, rotated, or scaled to specific positions. This is less time-consuming than manual segmentation, but is still sufficient for many cases in qualitative analysis where precision is not a major concern.</p><p>We use the depth peeling <ref type="bibr" target="#b4">[5]</ref> algorithm to solve the ordering problem when multiple transparent objects as well as volume data are rendered. The user can set its accuracy by adjusting the number of peeling layers.</p><p>In many applications of qualitative analysis, one peeling layer can achieve a satisfactory result while maintaining high interactivity. With a higher peeling layer setting, better accuracy can be achieved, allowing better understanding of how the volume data and the polygondefined regions are spatially related. For most complex geometries resulting from confocal data segmentation, we found four layers enough for sufficient accuracy. <ref type="figure">Figure 5</ref> illustrates the algorithm and <ref type="figure">Figure 6</ref> compares the difference between the depth peeling settings. The examples show how the positions of neurons relative to the eye and central brain can be better perceived. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Volume Editing with Polygon Data</head><p>Some visual occluders are clustered and large, and therefore hard to eliminate by using transfer functions. Polygon data can be used to cull these data. We generate voxelized objects <ref type="bibr" target="#b24">[26]</ref> from polygon data, which are 3D textures containing information whether a voxel is inside or outside the enclosure defined by polygon data. The mask volume separates data volume into interior and exterior, either of which can be culled. Different channels can share one mask volume, or in most cases, they are culled with different mask volumes. <ref type="figure" target="#fig_5">Figure 7</ref> shows how volume culling is applied to a three-channel dataset, where eye muscles and neurons are clearly visualized, and their spatial relationships better revealed, after culling the visual occluders.</p><p>Furthermore, different transfer functions can be applied to volume data within different regions defined by polygon data. <ref type="figure" target="#fig_6">Figure 8</ref> shows how this is applied to the data from the visual system, where interconnected neurons are color-coded according to the regions where they are situated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation and Application</head><p>In collaboration with neurobiologists, we have realized our design goals and chosen techniques as a working tool, which can aid neurobiologists for qualitative analysis of confocal data. Our implementation uses an OpenGL-based volume rendering library we developed. The input formats of our tool are tiff and nrrd, which are commonly used in medical researches and can be easily converted from manufacturer specific raw formats of confocal microscopes. The tool reads in the volume datasets, pre-calculates gradient fields and 2D histograms. The in-memory datasets are broken into blocks that each can fit into the graphics memory, and the data blocks are sent to the graphics card for rendering as OpenGL texture objects. Shading and depth cueing effects, 2D transfer function lookup, the render modes, and depth peeling are all coded with OpenGL Shading Language (GLSL). A screenshot of the tool is shown in <ref type="figure" target="#fig_7">Figure 9</ref>, and its functions and operations are demonstrated in the supplementary video.</p><p>As an example, our collaborating neurobiologists applied our tool to visualize Tg(brn3a-hsp70:GFP) transgenic zebrafish embryos <ref type="figure" target="#fig_8">(Figure 10)</ref>, recently described in the neurobiological literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>, and compared the result with that of using maximum intensity projections. These transgenics express GFP in retinal axons as well as tectal neurons. Our tool's visualizations ( <ref type="figure" target="#fig_8">Figure 10A</ref>-C) illuminated several features that were previously obscured in maximum intensity projections ( <ref type="figure" target="#fig_8">Figure 10D-F)</ref>. First, there was a clear boundary between the optic tectum, where the retinal axons terminate, and the cell bodies of the tectal neurons ( <ref type="figure" target="#fig_8">Figure 10B)</ref>; this boundary is obscured in the maximum intensity projection ( <ref type="figure" target="#fig_8">Figure 10E</ref>). Second, 3D relationships that are hidden in the maximum intensity projection ( <ref type="figure" target="#fig_8">Figure 10E</ref>) become clear when volume-rendered: the eye and the tectobulbar tract are located deeper than the optic tectum ( <ref type="figure" target="#fig_8">Figure 10B</ref>). Third, volume rendering reveals surface texture ( <ref type="figure" target="#fig_8">Figure 10C</ref>) obscured by pixel saturation in maximum intensity projections ( <ref type="figure" target="#fig_8">Figure 10F)</ref>; showing for instance the presence of an arborization field contacted by the retinal axons just before they reach the optic tectum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance and Rendering Quality Comparison</head><p>The tool has been tested and compared to other available packages by neurobiologists, and they found it better suits their research needs in terms of interactivity, rendering quality, and efficiency. <ref type="figure" target="#fig_0">Figures 2, 4, 6, 7, 8, 10</ref>, and 11 were all generated by neurobiologists in their studies of zebrafish. <ref type="table" target="#tab_1">Table 1</ref> shows the rendering speed of our tool for the two datasets we used in the paper (on Windows PC with Core 2 Quad Q9550 2.83GHz, GeForce GTX 280, 4GB RAM, and 1600x1200 display). <ref type="table" target="#tab_2">Table 2</ref> compares the operating time of our tool and other commonly used commercial packages. The timings were calculated by studying the video captures of the operations by a fluent user in neurobiology confocal research. The dataset used for comparing is the zebrafish head dataset. In <ref type="table" target="#tab_2">Table 2</ref>, data loading is the time from opening the datasets to when they are displayed in the viewport; parameter adjusting is the time used to adjust rendering properties to get satisfying results. Please notice that the user we studied here had worked with the datasets and the tools for quite long time, and was very familiar with the processes. So the timings only reflect the fastest operations possible. Even our tool has more parameters, neurobiologist users find them necessary and easy to work with. <ref type="figure" target="#fig_9">Figure 11</ref> compares the rendering results of same dataset with different tools. The results were generated by a neurobiologist working</p><formula xml:id="formula_1">A (before) B (after) C 1 C 2 C 3 C 4 D1 D2 D3 D4</formula><p>RGCs eye muscles with confocal data for eleven years and familiar with all the tools compared. The rendering parameters of each tool were adjusted with the aim of showing details of fine structures as well as overall surface shapes of the sample studied. By rendering the same dataset, we can see each tool's strength and shortcomings in rendering quality. For example, Volocity is good at rendering details of the fibers and cells but doesn't interpret the surface shape as good as Imaris. Our tool can render both local details and global shapes clearly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature Discussion</head><p>Our collaborating neurobiologists compared the tool to other tools that use different rendering techniques, and they concluded that our tool has the best interaction speed, without apparent rendering quality loss. The advantage of a GPU-based volume renderer is most obvious when there are many finely detailed structures in a dataset -usually the user wants to explore the dataset quickly, and often wants to keep high rendering quality during viewport interaction, so that he or she can keep track of certain structural details. Our collaborating neurobiolo-   gists also appreciate the instant visual feedback of rendering parameter changes that a GPU-based volume renderer can provide, where they can quickly fine-tune the rendering properties without waiting for the changes to take effect. Our collaborating neurobiologists mentioned many times that the available volume rendering packages are not efficient for confocal data. This is because many volume rendering packages try to provide comprehensive settings for volume properties such as transfer function editors, which are sometimes confusing and laborious to work with. In contrast, we analyzed the specific features of confocal data, and designed parameter settings accordingly. The neurobiologists found the set of parameters we provided for transfer function manipulation are more intuitive for confocal data; they can often get the desired results within minutes. This is especially important when there are multiple datasets to process as in high-throughput microscopy.</p><p>As mentioned in the introductory section, most confocal datasets are multi-channel. Our collaborating neurobiologists felt that many available tools either neglected this important feature completely, or did not pay much attention as to how to present different channels together, yet render clearly both individual channels and the relationships between them. They found the multiple render modes of our tool a good way to handle multi-channel data. They often start with layered mode, and setting the channel with the finest detail as the top layer. For instance, motor neuron labeling ( <ref type="figure" target="#fig_3">Figure 4A</ref>) has many fibers and is otherwise easily occluded by other channels. They then switch to other modes ( <ref type="figure" target="#fig_3">Figure 4B</ref> and C) to better perceive spatial relationships. The synchronized multi-views are often used for displaying the different modes at the same time, as their advantages complement each other.</p><p>As mentioned in section 3.4, boundary extraction typically needs to be done manually. Thus, our collaborating neurobiologists appreciate the flexibility our tool provides, of loading either manually or automatically segmented polygon data, and of creating and manipulating simple polygon geometries, for volume editing. We also found that using polygon data is probably the easiest method for volume editing, as the process resembles that in a polygon-based 3D modeling tool. By cutting volume data and setting properties for different subregions, our collaborating neurobiologists found they could make more elegant and effective visualizations of their confocal data.</p><p>Through our development process, the feature most emphasized on by our collaborating neurobiologists, was not rendering quality, but the user interface. They found that many similar tools are frustrating to use because of their user interface. We studied the workflow and operative behaviors of a typical neurobiologist carrying out his research on confocal data, and accordingly fine-tuned the user interface of our tool. Some small features, such as providing multiple methods for viewport interaction including mouse dragging, slider adjusting and numerical entering, saving frequently used parameters as a user's default, synchronizing the multi-view for comparison, or even laying out the user interface elements at handy positions, are surprisingly highly valued by our collaborating neurobiologists. They found that these features accelerate the workflow greatly, especially for repetitive analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have presented an interactive visualization tool for multi-channel confocal microscopy data in neurobiology research. We followed the typical workflow of a neurobiologist, and discussed how visualization techniques, such as interactive volume rendering, shading and depth cueing for volume data, transfer function settings, and embedding polygon data into volume data for region definition and editing, are applied to the qualitative analysis of the datasets. We also explored how to make the workflow easier and more efficient. Available commercial tools were deemed lacking by neurobiologists, leading to the design goals stated in the introductory section, and our neurobiologist coauthors found the new tool allows them to better perform analysis and high-throughput neurobiology studies. For future work, we would like to work on larger datasets and temporal data sequences, as well as the integration of focus-plus-context techniques. Neurobiologists would like to find methods to visualize the temporal development of the volume confocal data in real-time, such as growth of the zebrafish embryo. It would also be of interest to add the most recent research results in volume data segmentation and apply them for both easier segmentation and segmentation of temporal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Amira 5 Imaris 6</head><p>Volocity 4</p><p>Our Tool </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Rendering effects. A: Default shading effect; B: Lowering the ambient intensity increases the contrast for local features; C: Depth cueing can better show the overall shape; D: Cyan colored channel (all cell nuclei) obstructs other channels; E: Increasing the transparency may be helpful, but it makes the rendering obscure, and underlying channels are still partially occluded; F: Increasing the boundary extraction value can better show the spreading of the cells and underlying channels; G: The motor neurons (green) projecting to the eye muscles appear artifactually disconnected (arrowhead); H: Adjusting the offset value reveals that motor neuron fibers are in fact connected; I: Shading helps better define the shape; J: Noise is superimposed on the data of interest in the red channel (eye muscles); K: Increasing the low threshold suppresses the noise; L: A map of the regions analyzed. (Dataset: Zebrafish head)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>te n s it y V a lu e G r a d ie n t M a g n it u d eFig. 3. 2D Transfer function and its parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>shows a joint histogram of the volume data, its axes being intensity value and gradient magnitude. The 2D transfer function occupies a rectangular region of the histogram and has a tent-like shape. The meanings of the parameters are:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Render modes. A: Layered; B: Depth sorted; C: Composite. In layered mode (A), the rendering order of the channels, from top to bottom, is: neurons (green), muscles (red), and all cell nuclei (blue). The fibers of motor neurons can be observed without any obstruction, even when user rotates the view. In depth sorted mode (B), almost all the information from other channels is covered by that of all cell nuclei (blue). Increasing the transparency of the obstructing channel makes it obscure and less detailed, as seen inFigure 2E. With composite mode (C), all the channels can be seen at the same time, as well as the fine details. (Dataset: Zebrafish head)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 5 .</head><label>65</label><figDesc>Depth peeling results. A: Ventral view of the volume data showing retinal ganglion cells connecting between the eye and the brain; B: Polygon data added, separating volume data into eye (magenta) and brain (cyan); depth peeling layers set to one; C: Same data, depth peeling layers set to four. Arrowheads point to two branches of visual neuron fibers. With more depth peeling layers (C), it is clear that the lower branch is located deeper behind the eye region, which is not apparent in either A or B. (Dataset: Zebrafish head) Depth peeling algorithm. A: Only one depth peeling layer; B: n depth peeling layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Volume culling with polygon data. A: Original volume data; B: Volume data after culling; C: The process of culling the occluding data in the yellow channel, showing retinal ganglion cells (RGCs; C1: Original volume data; C2: Polygon data enclosing the layer of photoreceptor cells are loaded; C3: Volume data inside of the region are culled; C4: The volume data after culling); D: Culling the visual occluders in the green channel, showing motor neurons (D1: Original volume data; D2: Polygon data enclosing neuron clusters of the brain are loaded; D3: The volume data inside of the polygon data are culled; D4: The output data show only motor neurons.). (Dataset: Zebrafish head)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Different transfer function settings of volume data in different regions. The process shows how cells in different biologically meaningful regions are marked out, where the color-coded volume data represent cells in eye (green) and tectum (yellow) regions respectively. A: The loaded volume data show the eye and tectum; B: Two polygon datasets are loaded, defining the regions of the eye and tectum; C: Connecting neuron fibers between the eye and tectum can be culled; D: Different colors can be set for different regions. (Dataset: Zebrafish head)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>A screenshot of our tool. A: Toolbar, and the buttons are: Open Volume, Open Project, Save Project, Open Mesh, New View, Make Movie, and Info. B: Data View, loaded datasets are listed. C: Scene View, manages the viewports and associated datasets. D: Render View, displays the datasets. E: Viewport Settings, top: render modes, screen capture button, background color setting, depth peeling layers; left: depth attenuation setting; right: zoom factor; bottom: rotation angles. F: Volume Property Settings, items are listed in figure, and items for transfer function settings correspond to those inFigure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>The zebrafish visual system, rendered with our tool (A-C) and compared to previously used maximum intensity projections (D-F). A, D: Dorsal views of neurons expressing Tg(brn3a-hsp70:GFP) (green), and all nuclei (magenta). B, E: Dorsal views of Tg(brn3a-hsp70:GFP)expressing neurons only. C, F: Medial view of Tg(brn3a-hsp70:GFP)expressing neurons. Red, cell bodies colocalized with nuclear staining; green, neural fibers. Arrowhead indicates pretectal arborization field. TeO, optic tectum; TTB, tectobulbar tract; RA, retinal axon. A, anterior; P, posterior; D, dorsal; V, ventral; M, medial; L, lateral. (Dataset: Zebrafish visual system)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>The rendering result comparison between commercial packages and our tool. Left column: renderings of three channels (red: muscles, green: neurons, blue: all cell nuclei). The dashed rectangular regions (A1-D1) show the fine details of the neurons inside the eye, which can be better seen in our tool. Right column: renderings of single channel (all cell nuclei). The dashed rectangular regions (A2-D2) show a ruptured region of the tissues. The indentation of the damaged tissues can be better observed in our tool. (Dataset: Zebrafish head)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Rendering Speed of Our Tool</figDesc><table><row><cell></cell><cell cols="2">Zebrafish Head Zebrafish Visual</cell></row><row><cell></cell><cell>Dataset</cell><cell>System Dataset</cell></row><row><cell>X Res.</cell><cell>512</cell><cell>512</cell></row><row><cell>Y Res.</cell><cell>512</cell><cell>512</cell></row><row><cell>Z Res.</cell><cell>160</cell><cell>136</cell></row><row><cell>Data Length</cell><cell>8 bit</cell><cell>8 bit</cell></row><row><cell>Samples/Voxel</cell><cell>3.2</cell><cell>3.2</cell></row><row><cell>Channels</cell><cell>3</cell><cell>2</cell></row><row><cell>FPS</cell><cell>12.3</cell><cell>24.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Operating Time Comparison</figDesc><table><row><cell></cell><cell cols="3">Data Loading Parameter Adjusting Parameters per</cell></row><row><cell></cell><cell>(sec)</cell><cell>(sec)</cell><cell>Channel</cell></row><row><cell>Amira 5</cell><cell>80</cell><cell>130</cell><cell>3</cell></row><row><cell>Imaris 6</cell><cell>70</cell><cell>180</cell><cell>5</cell></row><row><cell>Volocity 4</cell><cell>80</cell><cell>70</cell><cell>3</cell></row><row><cell>Our Tool</cell><cell>20</cell><cell>75</cell><cell>11</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We wish to acknowledge the following funding: NIH R01-EY12873, Dana Foundation, NSF: CNS-0615194, CNS-0551724, CCF-0541113, IIS-0513212, DOE VACET SciDAC, KAUST GPR KUS-C1-016-04.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laterotopic representation of left-right information onto the dorso-ventral axis of a zebrafish midbrain target nucleus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hamaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uemura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Concha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Okamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="238" to="243" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Bitplane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imaris</surname></persName>
		</author>
		<ptr target="http://www.bitplane.com/go/products/imaris" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interaction of different modules in depth perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Blthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Mallot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/IAPR First Intl. Conf on Computer Vision</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data intermixing and multi-volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Interactive Order-Independent Transparency. White paper, Nvidia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Everitt</surname></persName>
		</author>
		<ptr target="http://developer.nvidia.com/object/Interactive_Order_Transparency.html" />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to Confocal Microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Davidson</surname></persName>
		</author>
		<ptr target="http://www.olympusconfocal.com/theory/confocalintro.html" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">CCD Signal-To-Noise Ratio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Davidson</surname></persName>
		</author>
		<ptr target="http://www.microscopyu.com/tutorials/java/digitalimaging/signaltonoise/index.html" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-Time Mono-and Multi-Volume Rendering of Large Medical Datasets on Standard PC Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gaullachergasse</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2005-02" />
		</imprint>
		<respStmt>
			<orgName>Vienna University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">High performance 3D imaging software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Improvision</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volocity</surname></persName>
		</author>
		<ptr target="http://www.improvision.com/products/volocity/visualization/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification and uncertainty visualization of dendritic spines from optical microscopy imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Janoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nouansengsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="879" to="886" />
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multidimensional transfer functions for interactive volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="285" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mixing translucent polygons with volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kreeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reconstruction of cellular biological structures from optical microscopy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mosaliganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="863" to="876" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth-peeling for texture-based volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Pacific Conference on Computer Graphics and Applications</title>
		<meeting>the 11th Pacific Conference on Computer Graphics and Applications</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="429" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Systematic analysis of the visual projection neurons of drosophila melanogaster. i. lobula-specific pathways</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Otsuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Neurology</title>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="928" to="958" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pawley</surname></persName>
		</author>
		<title level="m">Handbook of Biological Confocal Microscopy</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Introduction to Fluorescent Proteins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Piston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lippincott-Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Claxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Davidson</surname></persName>
		</author>
		<ptr target="http://www.microscopyu.com/articles/livecellimaging/fpintro.html" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive volume rendering on standard pc graphics hardware using multitextures and multi-stage rasterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIG-GRAPH/EUROGRAPHICS workshop on Graphics hardware</title>
		<meeting>the ACM SIG-GRAPH/EUROGRAPHICS workshop on Graphics hardware<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-level user interfaces for transfer function design with semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1021" to="1028" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gpu-based multi-volume rendering for the visualization of functional brain images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rossler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tejada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fangmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Knauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SimVis</title>
		<meeting>SimVis</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="305" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Genetic single-cell mosaic analysis implicates ephrinb2 reverse signaling in projections from the posterior tectum to the hindbrain in zebrafish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hamaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hosoya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Okamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5271" to="5279" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perception of surface curvature and direction of illumination from patterns of shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="595" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Direct volume rendering with shading via three-dimensional textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vangelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1996 Volume Visualization Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<ptr target="http://www.amiravis.com/overview.html" />
		<title level="m">Visage Imaging. Amira</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive clipping techniques for texture-based volume visualization and volume shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="312" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
