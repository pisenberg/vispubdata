<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Markerless View-Independent Registration of Multiple Distorted Projectors on Extruded Surfaces Using an Uncalibrated Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Sajadi</surname></persName>
						</author>
						<title level="a" type="main">Markerless View-Independent Registration of Multiple Distorted Projectors on Extruded Surfaces Using an Uncalibrated Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Registration</term>
					<term>Calibration</term>
					<term>Multi-Projector Displays</term>
					<term>Tiled Displays</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. Left: A 2 × 4 array of eight projectors on a cylindrical display surface showing a weather map visualization; Right: A 2 × 3 array of six projectors on a more general extruded surface showing a medical visualization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Tiled multi-projector displays on curved screens (e.g. cylindrical or spherical screens) are becoming more popular for visualization, education, entertainment, training and simulation applications. Their appeal lies in the greater sense of immersion and presence they can create, and at times, the superior aesthetics they provide. Displays are tools used by these application users who are not expected to be experts in setting them up or maintaining them. Unfortunately, most registration algorithms designed for curved displays expect them to be one. Registering multiple projectors on such a display has been a challenge, primarily due to the fact that recovering the 3D shape of the display quickly almost always requires attaching fiducials (physical markers) on the display screen for providing robust correspondence between the screen and the camera, which is especially obtrusive. Using structured light patterns to achieve the same results is a time consuming process. Finally, both these methods are complex requiring a complex camera calibration, all of which are too difficult for a layman user to execute in a successful manner.</p><p>We seek a simple procedure to register multiple projectors on a curved display that can be used even by a layman user like a doctor in a medical facility, teacher in a school or a worker in a theme park. We observe that most of the time, geometrically simple surfaces, like partial cylinders (e.g. pillars or surround immersive environments), are used as the display screen. So we impose two simple priors on the screen. First, the screen is a vertically extruded surface -a surface made by sweeping a 2D curve, called the profile curve, along a direction perpendicular to it. This covers a large number of shapes that can be built by soft folding of a rectangular sheet in one direction ( <ref type="figure" target="#fig_0">Figure  2</ref>) -a cylinder is an important special case. Second, we assume the aspect ratio of the planar rectangle formed by the four corners of the extruded surface is known. Such a measurement is easy to provide, even for a layman user. Having these priors allows us to prevent the use of any markers on the display screen and still recover the shape of the display using a single image from an uncalibrated camera. This allows easy set-up and maintenance of such multi-projector displays by the user, even in the face of changes in the display surface or projector configurations and severe non-linearities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Main Contributions</head><p>In this paper we present a new efficient algorithm to register images from multiple projectors on a vertically extruded surface. Using the priors of an extruded shape and the known aspect ratio, we use a single image of the display surface from an uncalibrated camera to recover both the camera parameters and the 3D shape of the surface. The display surface is then arc length parameterized in both dimensions. Then we capture a few images of patterns from the projectors to relate the projector coordinates with the display surface points, and represent this relationship using a rational Bezier patch. This relationship is then used to segment the appropriate parts of the image for each projector to register them and create a seamlessly wall-papered projection on the display screen.</p><p>To the best of our knowledge, this is the first work that can achieve the following many desirable qualities of geometric registration on these non-planar surfaces. All prior work can only address one or a few of these desired qualities, however our work addresses all simultaneously for the first time.</p><p>1. Markerless: Using some simple priors on the display surface, we can register images from multiple projectors on a vertically extruded screen without using any correspondence between the 3D display and the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Uncalibrated Camera:</head><p>We show that with some simplifying assumptions on the intrinsic parameters of the camera, we can achieve this registration using an uncalibrated camera.</p><p>3. View Independent: Unlike registering with respect to a camera where the registration is correct from a single sweet spot, we paste the image like a wallpaper on the display. A wallpapered image does not look perspectively correct from any single viewpoint. Yet, human observers can easily correct for the existing distortions irrespective of their viewpoint since we are used to seeing wallpapered images commonly. Hence, by wallpapering we assure that multiple viewers can observe our display at the same time, making our method view independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Allowing Low-Resolution Sensor:</head><p>Since we use a rational Bezier patch to relate the projector to the display parameters, we can achieve a good fit even if we sample the function sparsely. As a result, we can use a relatively low-resolution camera (e.g. VGA camera) to register a much higher resolution display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Allowing Non-Linearities:</head><p>Further, since our registration depends on a 2D parametrization of the display generated from the recovered 3D surface rather than auto-calibrating projectors on the 3D surface itself, we can handle severe non-linearities in the projectors (like radial distortion). Thus, we can allow a compact set-up with inexpensive short-throw lenses mounted on the projectors that usually have non-linear distortions. Non-linear distortions have been addressed when using planar displays <ref type="bibr" target="#b3">[4]</ref>. When using non-planar displays, non-linear lens distortion has been addressed in a limited manner by using a high-resolution camera to sample the function relating the projector to the display parameters densely <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7]</ref>. However, we can correct such distortions even with a low-resolution camera using a sparse sampling of the function relating the projector and display parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Allowing Display Imprecision:</head><p>The 2D parametrization additionally assures that a moderate deviation of the screen from being a perfectly extruded surface will not affect the accuracy of the geometric registration. Thus, we can handle manufacturing imprecision in the vertically extruded display surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>. Accuracy: Our method assures subpixel accuracy even in the presence of projector non-linearities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Efficiency:</head><p>Finally, our method can be run in real-time on the GPU making it ideal for interactive video applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>Camera-based geometric registration of multi-projector displays can be either view-dependent or view-independent. View-dependent registration yields an image on the display that is correct from only one sweet view-point, usually the view of the camera. Deviation of the viewer from this location shows view-dependent distortions. Hence, view-dependent registration is usually appropriate for static singleuser applications. On the other hand, view-independent registration pastes or wall-papers the images on the display surface. Since wallpapering is a common way to accommodate multiple viewers, such registration can cater to more than one viewer easily. Such a registration not only requires registering the projectors in a common camera frame but also the (conformal) parameterization of the shape of the display surface. There has been a large amount of work on registering images on planar multi-projector displays in a view-independent fashion using linear homographies enabled by the planar screen <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3]</ref>. Such registration can be achieved in the presence of projector non-linearities using rational Bezier patches <ref type="bibr" target="#b3">[4]</ref>.</p><p>View-dependent registration on a non-planar display has been achieved by using special 3D fiducials and a large number of structured light patterns for a complete device (camera and projector) calibration and 3D reconstruction of the display surfaces, which are then used to achieve the registration <ref type="bibr" target="#b12">[13]</ref>. Aliaga et al. in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> use a similar 3D reconstruction method to achieve a similar registration on complex 3D shapes, but without using any physical fiducials. To constrain the system sufficiently, this method uses completely superimposed projectors and validates results from photometric and geometric stereo, resulting in a self-calibrating system. Raskar et al. in <ref type="bibr" target="#b11">[12]</ref> use a stereo camera pair to reconstruct special non-planar surfaces called quadric surfaces (spheres, cylinders, ellipsoids and paraboloids) and propose conformal mapping and quadric transfer to minimize pixel stretching of the projected images in a view-dependent registration.</p><p>More recently, there has been work on view-independent registration for the special case of a cylindrical surface rather than a general non-planar surface <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. Using the fact that cylindrical surfaces are developable, they have achieved a 'wall-paper' registration on such surfaces. However, these methods do not recover the shape of the surface in 3D, but attempt to find its 2D parametrization in the camera space. Hence, they need precise correspondences between the physical display and the observing camera. To achieve this, a precisely calibrated physical pattern is pasted along the top and bottom curves of the cylinder. Using these correspondences, a piecewise linear 2D parametrization of the display is computed and linked to a piecewise linear representation of the projector coordinates via the camera that observes both. This allows segmenting the appropriate parts of the image for each projector using linear/non-linear interpolations to create a wall-papered display. However, to avoid fiducials at a high spatial density, the physical pattern only samples the rims of the display. This insufficient sampling results in distortions or stretching, especially towards the middle of the display surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Comparison of Our Method</head><p>Unlike earlier methods for view-independent registration of cylindrical displays that assume a piecewise linear representation of the surface to parametrize it in the 2D camera space <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>, we recover the 3D geometry of the display. Hence, we can parametrize the display directly in 3D rather than in the camera image space, resulting in a geometric registration of the projected imagery without any stretching or distortions. Use of a perspective projection invariant function, e.g. a rational Bezier function, for interpolation instead of a simple linear interpolation allows us to maintain registration in the presence of severe projector distortions and considerable imprecision in manufacturing of the extruded surface. Further, as shown in <ref type="bibr" target="#b3">[4]</ref>, unlike a piecewise linear function, a rational Bezier function can be interpolated accurately even from a sparse set of samples. This allows our method to use a  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y=1 (-</head><formula xml:id="formula_0">a/2,1,0) (a/2,1,0) (a/2,0,0) B t Z=0 Y=0 B b X Y Z Y=0 (-a/2,0,0) Image Plane I t Z B</formula><p>Camera <ref type="figure">Fig. 4</ref>. This illustrates the world coordinate systems and the display surface and camera set-up with respect to it. The sampled points on the 2D top curve in the camera (blue) is reprojected in 3D to estimate the 3D top curve (black), and translated down to estimate of the 3D bottom curve (purple), and finally projected back on the camera (red). The distance between these points and the orange curve on the camera image plane, B, is minimized in the extrusion based optimization step. low resolution camera while registering a much higher resolution display. Unlike earlier methods for non-planar displays that recover the 3D shape using complex stereo or structured light based procedures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref>, we simplify the process using a single image from a single camera position by imposing the prior that the surface is vertically extruded. Finally, we avoid calibrating the camera to recover the 3D shape by using some simplifying assumptions on the intrinsic parameters of the camera and the aspect ratio of the display surface that is provided by the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM</head><p>Let the display surface, the image planes of the camera and the projector be parametrized by (s,t), <ref type="bibr">(u, v)</ref> and (x, y) respectively. We denote the 3D coordinates of the point at (s,t) in the display by</p><formula xml:id="formula_1">(X(s,t),Y (s,t), Z(s,t)).</formula><p>Since the display is a vertically extruded surface, the four corners of the display lie on a planar rectangle, whose aspect ratio a is known. We define the world 3D coordinate with Z axis perpendicular to this plane and X and Y defined as the two orthogonal basis of this planar rectangle. We also consider this planar rectangle to be at Z = 0. Considering these 3D coordinates, the top and bottom curves of the surface lie respectively on Y = 1 and Y = 0 planes in 3D. Hence, Y (s, 0) = 0 and Y (s, 1) = 1. Further, these two curves are identical except for a translation in the Y direction. Therefore, ∀s,</p><formula xml:id="formula_2">(X(s, 0), Z(s, 0)) = (X(s, 1), Z(s, 1))</formula><p>. This is illustrated in <ref type="figure">Figure  4</ref>. We assume that our camera is a linear device without any radial distortion. Note that a distorted camera will still provide good registration but the wallpapering will be imperfect. Limitations that will be imposed by this assumption are discussed in further detail in Section 5.3. However, our projectors need not be linear devices.</p><p>A view-independent geometric registration essentially requires us to define a function from (x, y) projector coordinates to the (s,t) display coordinates. Our method follows three steps to achieve this <ref type="figure" target="#fig_1">(Figure 3)</ref>. First we use a single image of the display from the uncalibrated camera and the known aspect ratio of the display to recover the camera properties (intrinsic and extrinsic parameters) using a non-linear optimization. Using the estimated camera parameters, we next recover the 3D shape of the display. Then, we use the profile curves of the vertically extruded surface to define 2D parametrization of the display surface based on the arc length of the profile curves flanking the display. After calibrating the camera and reconstructing the display, in the next phase, we capture an image of a blob-based pattern from each projector and use these to find samples of the mapping from the projector (x, y) to the display (s,t). Then we approximate this mapping from these samples by fitting a rational Bezier patch to the correspondences. Assuming that an image pasted on the display results in the image coordinates being identical to the display coordinates (s,t), this automatically achieves the geometric registration by defining the part of the image to be projected by each projector so that the resulting display is seamlessly wallpapered. Each of the above four steps are described in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recovering Camera Properties</head><p>In this step, we use a single image of the display surface ( <ref type="figure" target="#fig_2">Figure 5</ref>) to recover the intrinsic and extrinsic parameters of the observing uncalibrated camera using a non-linear optimization. A large number of image formats like jpg or tiff store EXIF tags for images which provide some of the camera parameters used during the capture. One of these is the focal length of the camera, the critical component for the intrinsic parameter matrix of the camera. As in <ref type="bibr" target="#b15">[16]</ref>, we use this focal length to initialize the intrinsic parameter matrix in our non-linear optimization. To convert the focal length to the unit of pixels, we divide resolution of the camera by the CCD sensor size and multiply it with the focal length specified in the EXIF tags. The sensor size of the camera is available in its specifications.</p><p>In most cameras today, it is common to have the principal center at the center of the image, no skew between the image axes and square pixels. Using these assumptions, we express the intrinsic parameter matrix of a camera, K c , as</p><formula xml:id="formula_3">K c = ⎛ ⎝ f 0 0 0 f 0 0 0 1 ⎞ ⎠<label>(1)</label></formula><p>The camera calibration matrix that relates the 3D coordinates with the 2D camera image coordinates (u, v) is given by M = K c [R|RT ] where R and T are the rotation and translation of the camera with respect to the world coordinate system. In this step, we use the initial estimate of f and the aspect ratio a as input and use a non-linear optimization to estimate seven parameters of the camera calibration matrix -these include the focal length f , the three rotations that comprise R and the three coordinates of the center of projection of the camera T . Our non-linear optimization has two phases. In the first phase, plane based optimization (Section 3.1.1), the seven camera parameters are estimated using just the projection of the corners of the display surface on the camera image. These estimates are used to initialize the extrusion based optimization (Section 3.1.2) with a more expensive error function to refine the camera parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Plane Based Optimization</head><p>We estimate the seven parameters in this step based on the image of the plane formed by the four corners of the screen whose 3D coordinates are given by:</p><formula xml:id="formula_4">(− a 2 , 1, 0), ( a 2 , 1, 0), ( a 2 , 0, 0) and (− a 2 , 0, 0). Consequently, the (u, v) coordinates in the camera of any 3D point (X(s,t),Y (s,t), Z(s,t)) on the display are given by (uw, vw, w) T = M(X(s,t),Y (s,t), Z(s,t), 1) T<label>(2)</label></formula><p>where (uw, vw, w) T are the 3D homogeneous coordinates corresponding to the camera coordinate (u, v) and M is the 3 × 4 camera calibration matrix defined by the seven camera parameters. We estimate the seven camera parameters in this step by using a non-linear optimization method that minimizes the reprojection error E r , (i.e. the sum of the distances of the projection of these 3D corners on the camera image plane from the detected corners). We initialize the angle of rotations about the X, Y and Z axes that comprise R to be zero and T to be roughly at the center of the planar rectangle formed by the four corners of the display at a depth of a similar order of magnitude as the size of the display i.e. (0, 0, a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Extrusion Based Optimization</head><p>The seven estimated camera parameters in the plane based optimization are used to initialize the extrusion based optimization that attempts to refine these parameters further. This also uses a non-linear optimization method that minimizes the error E = w r E r +w c E c , where E r is the error function from the plane based optimization step, and E c is an error function based on the reprojection error in the similarity of the flanking curves of the display as described next, and w r and w c are the weights to combine them. The vertically extruded display surface is constrained by the fact that the points on the top curve of the vertically extruded surface when translated by Y = −1 should lie on the bottom curve. We use the deviation from this constraint to define E c . Let the image of the top and bottom boundaries of the vertically extruded display in the camera be I t and I b respectively. We first use image processing to segment the image and sample the curves I t and I b . We fit a parametric curve to the samples on I b . Let us denote it with B. We use the current estimate of M to reproject I t in 3D. This is achieved by ray casting through the sampled points on I t and intersecting it with Y = 1 plane. The 3D curve thus obtained is B t . Then we translate the samples on B t along Y direction by 1 to get the samples on the 3D bottom curve, B b . Then we project these samples back on to the camera using M, denoted by M(B b ). Sum of the square of the distances of these samples from the curve B provides the reprojection error of the estimated bottom curve from the detected bottom curve. In case of perfect estimation, this error should be zero. Hence, we seek to minimize E c in addition to E r .</p><p>To solve both the plane and extrusion based optimizations, we use standard gradient descent methods. To assure faster convergence we (a) apply a pre-conditioning to the variables so that the range of values that can be assigned to them is normalized; and (b) use decaying step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recovering 3D Display Parameters</head><p>After convergence of the optimization process, we use the estimated M to reproject samples on I t and I b in 3D and intersect them with Y = 1 and Y = 0 planes to find B t and B b respectively. Due to accumulated errors, B t and B b may not be identical. So, we translate both the curves on Y = 0 plane and find their average to define B b . This is then translated to Y = 1 to define B t . This assures that both B t and B b are identical except for a translation along Y. We use a polynomial curve fitting to find a parametric representation of B t and B b .</p><p>Next, we seek a 2D parametrization of the display D with (s,t). The profile curve B b on the XZ plane is arc length parametrized using the parameter s. Considering the 3D point (X,Y, Z) on the display surface, X = X(s,t) = X(s) and Z = Z(s,t) = Z(s). Since extrusion is along the Y direction, Y = Y (s,t) = t. Using the vertical extrusion assumption we can conclude that X and Z are independent of t and Y is independent of s. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Geometric Registration</head><p>Geometric registration entails defining, for each projector, a function that maps the projector coordinates (x, y) to the display coordinates (s,t) via the camera coordinates (u, v). Mathematically,</p><formula xml:id="formula_5">(s,t) = M D←C (M C←P (x, y))<label>(3)</label></formula><p>where M C←P maps the (x, y) to (u, v) and M D←C maps (u, v) to (s,t).</p><p>As in <ref type="bibr" target="#b3">[4]</ref>, we use a rational Bezier patch to define M C←P . To find M C←P we project a number of blobs and use the camera to capture them ( <ref type="figure" target="#fig_3">Figure 6</ref>). The center of these blobs are known in the projector coordinate space (x, y). When these centers are detected in the camera space (u, v), they provide direct correspondences between (x, y) and (u, v). We fit a rational Bezier patch using a non-linear least squares fitting solved efficiently by the Levenberg-Marquardt gradient descent optimization technique. To compute M D←C , we do the following. For every mapped (u, v) coordinate in the camera, we cast a ray through this point and find the point of intersection with the recovered 3D display. Then we find the 2D parameter corresponding to this 3D point. Using a rational Bezier for representing M C←P provides two important capabilities to our algorithm, as in <ref type="bibr" target="#b3">[4]</ref>. First, we can achieve accurate registration in the face of severe non-linear distortions like lens distortion (barrel, pin-cushion, tangetial and so on). Such distortions are common when using inexpensive short throw lenses on projectors to allow a compact setup. The rational Bezier in this case can represent the non-linearities both due to the curved nature of the display and projector non-linearities. Second, unlike previous method <ref type="bibr" target="#b17">[18]</ref> that uses a piecewise linear function to represent M C←P and hence requires a dense sampling of the correspondences to estimate it, the rational Bezier can be estimated accurately even from a sparse sampling of the correspondences. This allows the use of the low resolution camera to calibrate a much higher resolution display. For example, we can achieve calibration on a 3000 × 1500 display using a VGA camera (640 × 480). Though these two capabilities were demonstrated for planar displays in <ref type="bibr" target="#b3">[4]</ref>, we demonstrate them for the first time for a class of non-planar displays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>We have implemented our method in MATLAB for two types of displays. First, we have used a large rigid cylindrical display -an extruded surface with a radius of about 14 feet and an angle of 90 degrees. Since a cylinder is an extruded surface, our method is applicable. We tiled eight projectors in a casually aligned 2 × 4 array to create the display. Second, in order to demonstrate the success of our method on a large number of vertically extruded shapes, we made a flexible display using a rectangular sheet of flexible white styrene. This was supported by five poles to which the styrene sheet was attached <ref type="figure">(Figures 9 and  10)</ref>. The shape of the profile curve of this extruded display can be changed by simply changing the position of the poles. Thus, we can create a large number of extruded shapes. We use six projectors on this display in a casually aligned 2 × 3 array to create the tiled display. For all the setups, we use Epson 1825p projectors ($600). We show results by using two types of sensors: (a) a high-end high-resolution (13 Megapixel) Canon Rebel Xsi SLR camera ($800); and (b) a low-end low-resolution (0.25 Megapixel) Unibrain camera ($200). We achieve color seamlessness by using the constrained gamut morphing method presented in <ref type="bibr" target="#b14">[15]</ref>. <ref type="figure" target="#fig_2">Figure 5</ref> shows the single image used to recover the camera and display properties. To find the projector to camera correspondences, we display a rectangular grid of Gaussian blobs whose projector coordinates are known. These are then captured by the camera. We use a 2D stepping procedure where the user identifies the top-left blob and its immediate right and bottom neighbors in camera space. Following this, the method (a) estimates the rough position of the next blob in scan-line order, and (b) searches for the correct blob position using the nearest windowed center-of-mass technique <ref type="bibr" target="#b5">[6]</ref>. If this is not possible for extreme projector/screen distortions, one can binary-encode the blobs and project them in a time sequential manner to recover the exact ids of the detected blobs and find the correspondences <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> ( <ref type="figure" target="#fig_3">Figure 6</ref>). Our projectors have relatively large throw-ratios and hence do not reveal major lens distortions. To demonstrate the capability of our method to handle non-linearities, we chose to simulate the distortion digitally by distorting the input images to the projectors. Such distortions will be common when mounting inexpensive short throw lens on the projector to create a compact setup.</p><p>Real time image correction using GPU: The registration is done offline and takes about five minutes. This generates the rational Bezier patches, (u, v) = B(x, y), for each projector, which are then used for image correction. We have implemented a real-time image correction algorithm using modern GPUs through Chromium -an open-source distributed rendering engine for PC clusters <ref type="bibr" target="#b7">[8]</ref>. A module for Chromium is written that first precomputes the coordinate-mappings of all pixels using the rational Bezier parameters. This per-pixel projector to screen lookup table is used by a fragment shader to map pixels from the projector coordinate space to the screen coordinate space during rendering. <ref type="figure" target="#fig_4">Figures 1, 7 and 9</ref> show the results of our method on different extruded surfaces including the most common case of a cylindrical surface. We demonstrate our results on particularly challenging contents like text, especially common for visualization applications, and show accurate geometric registration. <ref type="figure">Figure 10</ref> demonstrates that our method can handle severe projector non-linearities enabling mounting inexpensive short throw lens for compact set-up. <ref type="figure" target="#fig_5">Figure 8</ref> shows the two distortions we used in our experiments. Our supplementary video demonstrates the interactive rates which we achieve in all these renderings using our GPU implementation.</p><p>The degree of the rational Bezier used to achieve geometric registration depends on the amount of non-linearity present due to the curved screen and the distortions in the projectors. In our set-ups, we used a bicubic rational Bezier representation for the cylindrical surface. For our flexible display, we use a rational Bezier of degree 5 and 3 in horizontal and vertical directions respectively. With large projector distortions and larger curvature of the display, higher order rational Beziers will be more appropriate.</p><p>In <ref type="figure" target="#fig_7">Figure 11</ref> we compare our method with three different methods. Since our work is the only work that can achieve a markerless and view-independent registration, probably the only fair comparison is with using a homography-based registration that assumes a piecewise planar display surface and uses a homography tree to register all the projectors <ref type="bibr" target="#b4">[5]</ref>. However, in <ref type="figure" target="#fig_7">Figure 11</ref> we also show comparisons with the view-dependent method presented in <ref type="bibr" target="#b17">[18]</ref>. View-dependent registration defines a mapping from the projector coordinates (x, y) to the camera coordinates (u, v), as opposed to the display coordinates (s,t) and Equation 3 becomes</p><formula xml:id="formula_6">(u, v) = (M C←P (x, y)).<label>(4)</label></formula><p>Hence, the distortions of the camera (like the perspective projection) embeds itself in the registered display. Further, this method uses a piecewise linear mapping for M C←P (x, y) that requires a much denser sampling of projector-camera correspondences compared to our method. Hence, in the face of severe distortion, even with an order of magnitude higher number of samples, it cannot achieve the accuracy of registration achieved by our method. Finally, the ability to reconstruct the rational Bezier patches from a sparse sampling of the function allows us to use a very lower resolution camera (e.g. 640x480 VGA camera) to accurately calibrate a much higher resolution display (e.g. 3500x1200). <ref type="figure" target="#fig_0">Figure 12</ref> compares the geometric registration achieved using a high-resolution vs a low resolution camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSIONS</head><p>In this section, we discuss the dependency of our method on various parameters like the camera position, precision in the display surface, and the degree of the rational Bezier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Camera Placement</head><p>Our method achieves markerless registration on extruded surfaces using an uncalibrated camera. Even in the presence of the priors on the display surface, there is a set of camera positions that will lead to degeneracy for one or both phases of our non-linear optimization. Consider the plane based optimization stage where the goal is to find the focal length f and the extrinsic parameters. Let us assume the camera calibration matrix C to be</p><formula xml:id="formula_7">C = ⎛ ⎝ f 0 0 0 f 0 0 0 1 ⎞ ⎠ ⎛ ⎝ r 1 r 2 r 3 t x r 4 r 5 r 6 t y r 7 r 8 r 9 t z ⎞ ⎠ .<label>(5)</label></formula><p>Note that in the plane based optimization we are using four points that have Z = 0 and Y = 0 or Y = 1. Now, consider the case where r 7 = r 8 = 0 and r 9 = 1. This is equivalent to placing the camera on the Z-axis with the normal to the image plane being parallel to the Z-axis. In this case, the homogeneous coordinates of the images of the four corners of the plane are given by ( f × (ar 1 + t x ), f × (ar 5 + t y ),t z ) T when Y = 0 and ( f × (ar 1 + r 2 +t x ), f × (ar 5 + r 6 +t y ),t z ) T when Y = 1. Note that these points have a scale factor ambiguity, i.e. multiplying t z and f with the same scale factor would result in the same image coordinates. Intuitively, if the camera is placed with the image plane parallel to the planar rectangle defined by the extruded surface on the Z-axis, moving the camera on the Z-axis can create the similar change as scaling its focal length and we cannot find a unique solution to the camera parameters. Hence, this camera placement should be avoided. Second, let us consider the two 3D curves B t and B b , where B t = B b + (0, 1, 0). If the camera placement is such that the images of these two curves, I t and I b respectively, are related by I t = I b +(0, k) where k is a translation in the vertical image direction, then the extrusion based optimization will be redundant. This camera placement occurs when the normal to the camera image plane lies on a plane parallel to the XZ plane i.e. is perpendicular to the Y-axis. Hence, this camera placement should also be avoided. Note that the former placement that resulted in the scale factor ambiguity is contained in this latter condition since Zaxis is on the XZ plane. Hence, as long as a camera placement where the normal to the image plane is parallel to the XZ plane is avoided, our optimization will yield an accurate solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Accuracy and Sensitivity</head><p>Our system estimates the camera and display parameters and makes assumptions on the type of the display surface. Hence, it is important to answer two questions: (a) How accurate are the estimated camera and display parameters in the non-linear optimization stage?; and (b) how sensitive is the geometric registration to the inaccuracies of these  (a) Using simple homography; (b) Using view dependent piecewise linear method with a sparse 4 × 5 = 20 projector to camera correspondences -note the perspective projection of the camera embedded in the display shown by the more stretching on the left side than on the right; (c) Using view dependent piecewise linear method with a dense 10 × 12 = 120 projector to camera correspondences; (d) Our method using a sparse 4 × 5 = 20 projector to camera correspondences -note the perfect wall papering and the equal or higher quality of registration than the piecewise linear method despite the sparse sampling, especially in the presence of projector non-linearities. Please zoom in to check quality of results.</p><p>estimates or the priors imposed on the display surface? It is difficult to analyze all the above issues in real systems, hence we have conducted extensive analysis in simulation and real systems (whenever possible) to answer these questions.</p><p>First, we study the accuracy of the estimated camera extrinsic parameters following our non-linear optimization process. Our simulation of many different camera and display parameters shows that when an accurate intrinsic parameter matrix is given, our estimated extrinsic parameter matrix is very accurate. The error analysis of the deviation of the estimated parameters from the actual parameters is provided in <ref type="table" target="#tab_2">Table 1</ref>. For the orientation of the camera, we provide the deviation in degrees from the actual orientation. For translation, we provide the ratio of the error in estimation with the distance from the screen. We also study the accuracy of the estimated 3D profile curves of the display in this situation. To compare the estimated curves with the actual ones, we first sample the estimated curves densely. Then, for each sample, we find the minimum distance to the original curve. The ratio of the maximum of these distances to the length of the original curve is considered to be the measure of the accuracy of the display geometric reconstruction and is reported in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>We analyzed the validity of our simplifying assumption for the camera intrinsic matrix by running some experiments. For each of our camera set-up, we used standard algorithms and toolboxes to accurately estimate the camera's intrinsic matrix <ref type="bibr" target="#b19">[20]</ref>. The skew estimated by Zhang's method was always zero and the principal center deviated from the center of the image by a percentage error that is within the error tolerance of Zhang's method. These two confirm the validity of our use of a simpler intrinsic matrix. Further, we compared the estimated focal length from this method to the focal length estimated by our nonlinear optimization to analyze the accuracy of the estimated intrinsic camera parameters. We found that when provided with a good initial estimate as is available from the EXIF tags, the focal length estimated by our method is very close to that recovered by Zhang's method as indicated in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>We analyze the sensitivity of our registration to imprecision in the display surface or errors in the estimation of the display shape, both of which would result from a deviation of the real surface from a perfect extruded surface. However, our rational Bezier function provides a particularly robust framework for handling deviation from extruded surfaces. This is due to the fact that a small deviation from extrusion will lead to an erroneous 2D parametrization of the display surface, but the overlapping pixels from the multiple projectors will still map to the same (s,t). Hence, an imprecision in the extrusion can create small image distortions but will not lead to any misregistration. This is one of the strengths of our algorithm and is well-demonstrated by our flexible display which shows considerable imprecision due to its make-shift flexible prototype nature, but almost no misregistration of the projected images is visible even on this display. We quantitatively evaluate the effect of deviation of a surface from an extruded surface on the accuracy of the estimated camera parameters in <ref type="figure" target="#fig_1">Figure  13</ref>. Deviation from extrusion is measured by the maximum difference of the top and bottom curves with respect to the curve length. This plot shows even in presence of large deviation of the screen from being an extruded surface our method can achieve a reasonable estimation of camera pose and focal length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Camera Non-Linearity and Resolution</head><p>We assume the camera to be a linear device devoid of any non-linear distortion in Section 3. However, even if this is not true when using commodity cameras, our method will not result in any pixel misregistration since the camera non-linearity will be accounted for by the fitted rational Bezier patches. However, the camera non-linearity will affect the accuracy of the reconstruction of the 3D shape of the screen and hence, the final result may not be perfectly wall papered. Fortu- nately, human visual system can tolerate such minor deviation from wall papering. For verification, we performed our registration using an uncalibrated Unibrain Fire-i webcam with 640x480 resolution (one tenth of our display resolution) which had significant non-linear lens distortion (with quadratic coefficient of 0.01 and quartic coefficient of -0.009). We compare the achieved result with the ones achieved by our high resolution camera in <ref type="figure" target="#fig_0">Figure 12</ref>. Note that the deviation from wall-papering is hardly detectable and the registration is comparable. In case of more severe camera non-linearities one can use standard camera calibration techniques to undistort the captured images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">User Assistance</head><p>Our method needs to detect the four corners and the top and bottom curves of the extruded surface. Since the screen is usually the most distinct white object in the environment, segmenting it is relatively easy if the background is of reasonable contrast. Further, more often than not, a display environment is designed to have relatively diffused illumination, which does not affect the segmentation adversely. Even in the worst case of a low contrast between the screen and the background color, one can always use user interaction to improve the segmentation. All other steps of our method are completely automated as long as the projection area of the projectors are entirely within the screen and the screen is entirely within the camera's field-of-view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In summary, we have presented the first work for markerless viewindependent registration of tiled projection-based displays on extruded surfaces using an uncalibrated camera. We have shown that by imposing practical priors on the display surface, the registration technique can be simplified to be easily used by layman users. Our method provides a very user-friendly and cost-effective way to sustain such displays in large establishments like visualization centers, museums, theme-parks. Further, our method also offers the ability of recalibration and reconfiguration at a very short notice. These can be especially useful for applications like digital signage and aesthetic projections in malls, airports and other public places.</p><p>In the future, we would like to explore the similar concept of practical priors leading to easier registration for a different kind of widelyused non-planar surfaces, the domes. In the recent years, the number of domes have surpassed the number of IMAX theater installations <ref type="figure" target="#fig_10">(Figure 14)</ref>. However, there still does not exist an easy way to calibrate these displays. Our goal is to extend our fundamental concept in this direction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Some examples of vertically extruded surfaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The flowchart of our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>The image used for calibrating a cylindrical display with 2 × 4 array of eight projectors. This image of the screen, with no projectors turned on, used for recovering the camera and the display properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The images used for calibrating a cylindrical display with 2 × 4 array of eight projectors. Four different images of blobs from non-overlapping projectors used to find the projector to display correspondences. Please zoom in to the image to see blobs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Our geometric registration on a cylindrical display using 2 × 4 array of 8 projectors. Please zoom in to see registration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Top: A barrel distortion of degree 2; Bottom: A combination of a pincushioning of degree two and a first order tangential distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Our geometric registration on two different extruded surfaces (top and bottom) created using our flexible display set-up when using 2 × 3 array of 6 projectorsleft column shows the casually aligned set-up and the right column shows the wall papering of images after our registration is used. Please zoom in to see the quality of results. Our geometric registration handling severe non-linear distortion on an general extruded surface when using 2 × 3 array of 6 projectors -left column shows the casually aligned set-up and the right column shows the wall papering of images after our registration is used. Top: Using severe barrel distortion; Bottom: Using severe pin-cushioning and tangential distortion. Please zoom in to see the quality of results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Comparison of our method with other methods -Without projector non-linearities on the left and with severe radial distortion on the right. From top to bottom:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Geometric registration, Top: with our low resolution webcam (0.25 Megapixels), Bottom: with our high-end SLR camera (13 Megapixels).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Accuracy of camera parameter estimation in presence of deviation of the screen from being an extruded surface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Domes vs IMAX theaters opened in recent years.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Percentage Errors of the estimated camera and display parameters over a large number of simulations with different configuration of the devices and the 3D display.</figDesc><table><row><cell>Parameter</cell><cell>Max</cell><cell cols="2">Mean Std</cell></row><row><cell>Camera Orientation (deg)</cell><cell cols="3">0.494 0.192 0.167</cell></row><row><cell>Camera Position (%)</cell><cell cols="3">0.432 0.186 0.150</cell></row><row><cell>Focal Length (%)</cell><cell>3.82</cell><cell>2.26</cell><cell>0.98</cell></row><row><cell cols="4">Top and bottom curves (%) 0.547 0.217 0.153</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Maxim Lazarov for helping in making the video for this paper and building the flexible display set-up used in this work. We would like to thank Epson and Canon for their generous donations of projectors and cameras used in this project. This research is funded by NSF SGER 0743117 and NSF CAREER IIS-0846144.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Digital inspection: An interactive stage for viewing surface details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aliaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symp. on I3D</title>
		<meeting>ACM Symp. on I3D</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photogeometric structured light: A self-calibrating and multi-viewpoint framework for accurate 3d modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aliaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A flexible projector-camera system for multi-planar displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashdown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Registration techniques for using imperfect and partially calibrated devices in planar multi-projector displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bhasker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable alignment of large-format multi-projector displays using camera homography trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Vis</title>
		<meeting>of IEEE Vis</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Practical methods for geometric and photometric correction of tiled projector displays on curved surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Culbertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzhugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanguay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PROCAMS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chromium : A stream processing framework for interactive rendering on clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klosowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pixelflex 2: A comprehensive automatic casually aligned multi-projector display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PROCAMS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Auto-calibration of multi-projector display walls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Polleyfeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICPR</title>
		<meeting>of ICPR</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Immersive planar displays using roughly aligned projectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE VR</title>
		<meeting>of IEEE VR</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quadric transfer function for immersive curved screen displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Baar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Willwacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi projector displays using camera based registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Vis</title>
		<meeting>of IEEE Vis</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Forlines. ilamps: Geometrically aware and self-configuring projectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Willwacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transaction on Graphics</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Color seamlessness in multi-projector displays using constrained gamut morphing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sajadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lazarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gopi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo tourism: Exploring photo collections in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Calibrating multi-projector cylindrically curved displays for &quot;wallpaper&quot; projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Culbertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Workshop on PROCAMS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pixelflex: A reconfigurable multi-projector display system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hensley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Vis</title>
		<meeting>of IEEE Vis</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Camera based calibration techniques for seamless multi-projector displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flexible camera calibration by viewing a plane from unknown orientations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
