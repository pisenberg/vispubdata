<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hue-Preserving Color Blending</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Hue-Preserving Color Blending</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image compositing</term>
					<term>perceptual transparency</term>
					<term>color blending</term>
					<term>volume rendering</term>
					<term>illustrative visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. Volume rendering of a tomato data set using traditional (left) and hue-preserving (middle) color blending. The data histogram, transfer function, and color legend are shown on the right.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Color and transparency play crucial, fundamental roles in visualization. For example, color mapping is frequently and effectively used to display quantitative and qualitative data. Color is particularly effective for visual grouping, which can be utilized for labeling regions of a data set, visualizing nominal data. Transparency is almost indispensable when displaying 3D structures because it is one way of alleviating occlusion problems. For example, direct volume visualization depends heavily on transparency to show the complete 3D structure of a volumetric scalar field.</p><p>This paper focuses on the combination of color and transparency for the visualization of nominal data. An important application example is volume visualization with a transfer function that classifies different materials and shows them by clearly distinct colors. For example, 3D medical images like CT or MRI are often classified according to their material components that are then labeled by different colors. Since we propose a modification of the basic Porter and Duff compositing operators <ref type="bibr" target="#b26">[27]</ref>, our approach is generic and may be applied to any transparent image overlay for visualization or illustrative application.</p><p>Per se, color and transparency have been extensively investigated in both visualization and perception literature. However, the interaction between color and transparency in terms of perception has played a very limited role in visualization before. The most significant and directly related prior visualization work is by Wang et al. <ref type="bibr" target="#b30">[31]</ref>, who propose that opposite colors should be used for two semi-transparent layers to avoid hue shift after alpha blending. In addition, saturation of the input colors may be modified to reduce hue effects. We follow their rationale that hue shift should be avoided to prevent problems from false colors and respective mislabeling of nominal data and extend their color-design guidelines to a complete and robust computational model. We contribute a parameter-free model for generic image compositing that handles hue, saturation, and brightness separately in order to have independent control over achromatic and chromatic compositing (Section 4). Blending of brightness resembles existing and established blending schemes, whereas we keep the hue that has the dominant input color (the color which has the strongest impact on the final image). Color discontinuities that might be introduced by naive hue preservation are avoided by smoothly adjusting saturation. An important practical benefit is that our approach only affects the additive aspect of color compositing and, thus, any kind of Porter and Duff compositing operator can be extended to observe hue preservation. In particular, the over operator for the discretization of volume rendering can be made hue-preserving. Several examples of hue-preserving volume rendering are shown and discussed in Section 5. Hue-preserving blending is founded on results from research in perceptual psychology and psychophysics, which we review in Section 2. Those results guided the development of our blending model, as documented in the form of respective design criteria and requirements (Section 3). <ref type="figure">Figure 1</ref> compares traditional blending with hue-preserving blending for a typical example of direct volume visualization. The rendered data set shows the scan of a tomato. The transfer function was chosen with variable opacity, but only three different, discrete colors: red, orange, and yellow. Traditional blending <ref type="figure">(Figure 1</ref> (left)) leads to a mix of those input colors so that different materials tend to be hard to distinguish. In particular, the outer peel (red) and next layer (orange) are indistinguishable. In contrast, hue-preserving blending <ref type="figure">(Figure 1</ref> (middle)) separates all regions clearly-even the outer peel from the rest of the tomato.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Semi-transparency has been playing a relevant role in previous research on human visual perception, visualization, and computer graphics alike. We review related work from those research areas.</p><p>In computer graphics and visualization, the focal point is the algorithmic aspect and efficiency of semi-transparent rendering, either for rendering semi-transparent surface geometry or participating volumetric media (see Engel et al. <ref type="bibr" target="#b11">[12]</ref>). The conceptual basis for both surface and volume rendering with transparency is usually related to alpha blending-also known as the over operator in image compositing according to Porter and Duff <ref type="bibr" target="#b26">[27]</ref>. Alpha blending can be directly applied to semi-transparent surface overlays and indirectly to volume rendering, where the volume rendering integral can be discretized by iterative application of alpha blending in back-to-front order. While global illumination effects for translucent materials are highly relevant for photorealistic rendering (e.g., subsurface scattering <ref type="bibr" target="#b31">[32]</ref> or translucent volume visualization <ref type="bibr" target="#b21">[22]</ref>), volume rendering with single scattering is the de-facto standard for direct volume visualization due to its efficiency and ease of interpretation by the user. This paper also relies on the algorithmic basis of semi-transparent rendering, adopting the fundamental idea of a compositing operator that (subsequently) blends two images. Our main goal is volume visualization, but we also consider generic image overlay for visualization or illustrative purposes.</p><p>This paper focuses on a modification of the Porter and Duff compositing operator, guided by human visual perception. We consider previous work in the perception literature because perceptual transparency is not identical with physical transparency, which is typically the starting point in computer graphics. Perceptual transparency relates to the perception of two objects, where one object is recognized as being in front of the other background object <ref type="bibr" target="#b22">[23]</ref>. A key observation is that perceived transparency is not based on inverse physical optics, but influenced by low-level, mid-level, and high-level components of visual perception. In fact, physical transparency is neither sufficient nor necessary for perceptual transparency. Transparency perception is affected by many different aspects, including luminance, chromaticity, apparent motion, stereo depth, subjective contours, and figural organization. In particular, figural organization and geometric aspects play an important role, such as x-junctions <ref type="bibr" target="#b4">[5]</ref>, part boundaries <ref type="bibr" target="#b29">[30]</ref>, or Gestalt aspects <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>We restrict ourselves to per-pixel compositing of images and, therefore, focus on the low-level color blending aspect of transparency. Other conditions for transparency are complementary to, and can be combined with, our approach; typically, these conditions are related to parameters beyond image compositing, such as scene configuration, camera parameters, or lighting conditions. There is strong and ample empirical evidence that image luminance has the most impact on transparency perception. In fact, most studies have focused on investigating achromatic configurations; see, for example <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>. Based on empirical results, different variants of psychophysical models of transparency were developed. These models can be broadly classified as additive or subtractive, referring to their underlying compositing of colors. The role model of additive transparency is the episcotister model by Metelli <ref type="bibr" target="#b24">[25]</ref>: conceptually, a disc with an open sector rotates in front of the background object; the perceptual effect of the overlay of the weighted foreground (disc) and background colors is achieved by fusion. This model is identical to alpha blending by the over operator <ref type="bibr" target="#b26">[27]</ref>. The Metelli model can be generalized in a couple of ways, for example to the model of linear atmospheres that modifies the luminance shining through them <ref type="bibr" target="#b1">[2]</ref> or to generic addition (translation) and mix of colors <ref type="bibr" target="#b10">[11]</ref>. Alternatively, subtractive models rely on the idea of a light-transmitting material such as a colored screen (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>). There are conflicting empirical results favoring either additive or subtractive models. However, for the achromatic case, both types of models are hard to distinguish and lead to comparable results <ref type="bibr" target="#b5">[6]</ref>. As we adopt the luminance computation from the literature (and modify only the chromatic computation), we may as well reuse any of the previous luminance models. In accordance to traditional blending in computer graphics, we follow the approach of (additive) alpha blending.</p><p>While the crucial role of the achromatic channel is undisputed for perceptual transparency, a large portion of the perception literature indicates that chromatic information has very limited influence on transparency perception. For example, Nakayama et al. <ref type="bibr" target="#b25">[26]</ref> report that transparency perception is robust under a wide range of color configurations, both for the occluder and the occludee. Similarly, Anderson <ref type="bibr" target="#b2">[3]</ref> identifies achromatic contrast as the primary determinant of scission. An extreme view would remove chromatic information completely from a transparency model. Such a view is quite accepted for the perception of motion, where chromatic contrast apparently plays (almost) no role; see, for example <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref>. However, there is also some evidence that special configurations of chromatic contrast alone can trigger transparency perception <ref type="bibr" target="#b10">[11]</ref>. For example, the color of the overlay image should share hue properties with the images underneath <ref type="bibr" target="#b8">[9]</ref>. As a consequence of the unclear role of chromatic information, we favor a conservative approach to perceptual transparency by focusing on well accepted models of luminance composition and by reducing the impact of the chromatic channels. In particular, we "synchronize" the hue characteristics in an extreme way: by favoring complete preservation of hue.</p><p>There is much previous work on perceptual transparency in psychology and psychophysics, but only little related work in computer graphics and visualization. Most relevant for our work is the recent publication by Wang et al. <ref type="bibr" target="#b30">[31]</ref>. They investigate and provide guidelines and rules for color design for illustrative visualization. In particular, they describe the appropriate choice of colors for semi-transparent layers: colors should have opposite hue in order to avoid hue shift after blending. In the case of more than two semi-transparent layers, they propose further constraints on the input colors. One of their guideline variants is to assign two colors with opposite hues for the two most important image elements and a more neutral color for the less important element(s). An alternative guideline is to change the input colors locally: they recommend reducing the saturation of the background element in overlap regions. The (geometric) overlap is detected by depth peeling. We adopt the very idea that hue shift should be avoided, but guarantee hue preservation by a generic blending model that allows for arbitrary number and configuration of input colors. In particular, we provide a complete computational and parameter-free model that may be applied to any kind of compositing problem and without constraints on the color maps.</p><p>Another recent example of transparency research is the perceptual evaluation of volume rendering techniques by Boucheny et al. <ref type="bibr" target="#b7">[8]</ref>, who report that motion parallax and perspective projections are well suited to improve depth perception. Their work does not consider the impact of color, but their findings can be used to improve volume visualization in general and, thus, can be immediately combined with our approach to volume visualization. Fleming and Bülthoff <ref type="bibr" target="#b14">[15]</ref> investigate lowlevel image cues for the perception of translucency, as produced by subsurface scattering. They particularly focus on achromatic aspects and the image blurring introduced by subsurface scattering, as compared with traditional image blending. They only briefly touch color, where they report that saturation is neither necessary nor sufficient to generate the impression of translucency. Finally, Bair et al. <ref type="bibr" target="#b3">[4]</ref> present guidelines for perceptually optimal visualization of layered surfaces, focusing on suitable texture patterns, but not on image compositing.</p><p>We aim at using color for visual grouping and labeling, which is most effective by means of chromatic information, not luminance information <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>. In general, the design of appropriate color maps for visualization has been studied extensively in the literature. There are useful guidelines for generating effective color maps (e.g. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>). For this paper, we assume that an effective color palette is provided for the visualization of nominal data, i.e., for clearly separable elements or regions in the visualization. Typically, a small number of distinct colors are easily discriminated and, therefore, can be used for visual labeling. For example, up to roughly seven different colors may be used effectively <ref type="bibr" target="#b17">[18]</ref>. Similarly, basic color names, which are known across cultures, could be used for color labels <ref type="bibr" target="#b6">[7]</ref>. Distinguishable colors can also be used to design color palettes that lead to reduced display energy <ref type="bibr" target="#b9">[10]</ref>. On a technical level, this paper makes use of computations in color space to guarantee hue preservation during the construction of visual overlays. A description of color spaces and tristimulus theory can be found in related books like the ones by Fairchild <ref type="bibr" target="#b12">[13]</ref> or Wyszecki and Stiles <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN OF HUE-PRESERVING BLENDING</head><p>We discuss the perceptual motivation and the design considerations for the development of hue-preserving blending before we present the respective computational model in Section 4. Since we target perceptual transparency, our compositing approach is not subject to any physical constraints, but can be formulated as an algebraic model. The discussion is initially restricted to compositing two overlaid images, and it will be later extended to compositing several images and even to continuous compositing in volume rendering. The primary goal of the new compositing model is to support easy perception of distinct colors for labeling, in combination with a good perception of transparent overlays.</p><p>Summarizing previous work in perceptual psychology and psychophysics (see Section 2), the following observations can be made:</p><p>[O1] Perceptual research indicates that luminance is most important for the perception of transparency.</p><p>[O2] Shape perception by shape-from-shading is based on luminance information.</p><p>[O3] The chromatic channels play a major role in visual grouping; hue is particularly well suited for visual labeling, e.g., of nominal data.</p><p>[O4] Chromatic information and especially saturation play a minor-at least unclear-role for transparency perception.</p><p>From these observations, we arrive at the following design criteria:</p><p>[D1] Any new compositing model has to exhibit the same behavior for the luminance channel as established compositing models. According to [O1], luminance is critical for transparency perception, and there exist models with demonstrated effectiveness. In addition, the achromatic channel may carry important information, such as shape-from-shading information [O2], that should not be interfered with.</p><p>[D2] The same, constant hue should be used for each nominal data entry to facilitate visual grouping [O3].</p><p>[D3] Artificial color discontinuities should be avoided for continuously varying input colors, so that artificial perceptual contours are avoided.</p><p>These design criteria guide the construction of a generalized compositing operator. According to Porter and Duff <ref type="bibr" target="#b26">[27]</ref>, a wide range of compositing strategies can be formulated as the weighted sum of two colors. In particular, their approach includes alpha blending (the over operator) typically used for computing transparent overlays according to the Metelli model. We adopt the compositing idea by Porter and Duff and add just a little modification: instead of a direct, componentwise sum of two colors C 1 and C 2 , a new "add" operator is proposed that meets the above design criteria. We denote traditional addition of colors by the symbol "+" and the new operator by "⊕". In this notation, the hue-preserving sum of colors is:</p><formula xml:id="formula_0">C new = C 1 ⊕C 2<label>(1)</label></formula><p>From the above design criteria, we impose the following requirements that hue-preserving color addition has to meet:</p><p>[R1] The same luminance behavior as in traditional summation for the achromatic case should be achieved: the luminance of (C 1 ⊕C 2 ) should be identical to the sum of the luminances of C 1 and C 2 .</p><p>[R2] The hue of C new is either equal to the hue of C 1 or C 2 :</p><formula xml:id="formula_1">Hue(C new ) ∈ {Hue(C 1 ), Hue(C 2 )}.</formula><p>The hue of C new is chosen as the dominant hue of the two colors C 1 and C 2 . The dominant color is the one whose hue would be closest to the blended color in traditional color summation.</p><p>[R3] Saturation variations are used to avoid color discontinuities. When the dominant color, and thus the final hue, is to change, C new should go through the gray point with vanishing saturation, so that even an abrupt change of hue does not imply a discontinuity in chromaticity. The semantics and mathematical structure of the new ⊕ operator is designed to resemble the traditional + operator as much as possible, so that it can be used in any existing blending algorithms, especially in compositing schemes for volume rendering. The ⊕ operator is binary: it takes two input colors. The extension to compositing several image layers or to many samples along viewing rays in volume rendering is possible by applying ⊕ several times along the image compositing stack. The mechanics and mathematical definition of the ⊕ operator are presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MECHANICS OF HUE-PRESERVING BLENDING</head><p>This section presents the computational model of hue-preserving blending that follows the requirements [R1]- <ref type="bibr">[R3]</ref>. We aim at a generic compositing model, modifying the Porter and Duff image compositing approach. In its original form, any Porter and Duff operator can be written as a weighted sum of two input colors C A and C B <ref type="bibr" target="#b26">[27]</ref>:</p><formula xml:id="formula_2">(α A F A )C A + (α B F B )C B<label>(2)</label></formula><p>where α A and α B are the alpha values associated with the two colors and F A and F B are respective fractional components. The scalar values (α A F A ) and (α B F B ) can be interpreted as combined weights for the two input colors. The original version of those compositing operators assumes colors in RGB color space. However, any other color space related to RGB by linear transformation may be employed, e.g. CIE XYZ. The basis of color computation is the tristimulus theory, which interprets color as elements in a 3D vector space. Equation (2) contains two relevant arithmetic operations: the multiplication of a scalar weight with a 3D color, and the sum for 3D colors. With hue-preserving blending, multiplication with a scalar weight remains unchanged. The only difference is that the traditional component-wise addition by the + operator is replaced by the new operator ⊕ from Eq. (1).</p><p>The hue-preserving ⊕ operator is based on computations in a set of appropriate color representations: in hue, saturation, and brightness components that are modified separately. We start from linear RGB as basis for our color computations and apply transformations to separate the hue, saturation, and luminance aspects. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates and compares traditional blending with huepreserving blending. <ref type="figure" target="#fig_0">Figure 2</ref>(a) sketches the geometry of blending in the hue-saturation plane-with hue as angle and saturation as radial distance from the center. The two exemplary input colors, teal and orange, are marked by small white circles. Depending on the relative weights assigned to the two colors, the result of traditional blending yields a color on the long dashed line crossing several color hues. The possible resulting hues are also shown in the color bar in <ref type="figure" target="#fig_0">Figure 2(b)</ref>. Our aim is to modify the traditional blending + operator so that when Note the presence or absence of the yellow hue in both color profiles. <ref type="figure">Fig. 3</ref>. Blending opposite (i.e., complementary) colors in the traditional color blending model leads to a more neutral color and preserves either original hue. We follow the same idea in our hue-preserving color blending model. Given two arbitrary colors (circled in white) that are not necessarily opposite to each other, we modify only the hue component of the non-dominant color to be the opposite hue of the dominant color (circled in red), then they are added as before.</p><p>two colors are blended, the resulting color only has the same hue as either of the original ones, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c). The basic idea is to blend two colors through the middle gray point (or the central axis, where color saturation equals zero), as illustrated by the red dotted line in <ref type="figure" target="#fig_0">Figure 2(a)</ref>. In other words, hue-preserving blending can be essentially split in two pieces: blending from one input color C 1 towards the gray axis (which keeps the hue of C 1 ), or blending from the other input color C 2 towards the gray axis (which keeps the hue of C 2 ). We decide which of the two pieces is used by examining the relative "strengths" of the two input colors; the hue of the dominant color determines the hue of the blended color. The actual compositing step has to ensure that the dominant hue does not change. This is achieved by modifying the nondominant color in a way that it becomes the opposite of the dominant color; the saturation and luminance of the non-dominant color stay the same. By adding opposite colors, the color moves towards the gray point, and we guarantee that the original hue does not change. <ref type="figure">Figure 3</ref> illustrates this idea.</p><p>Algorithm 1 describes our hue-preserving blending model. In particular, we do not require that hue values are explicitly available, but we just need a mechanism that provides the notion of equal hue (i.e., colors that are on the same straight line from the gray axis in 3D color space), opposite hue (i.e., colors that are on a straight line on the opposite sides of the gray axis), and isoluminance. Here, the gray axis denotes the line that goes from black through the white point; it cor-responds to the completely desaturated center point in <ref type="figure" target="#fig_0">Figure 2</ref>(a) and <ref type="figure">Figure 3</ref>. Luminance is explicitly available in CIE XYZ or indirectly in CIELAB and it can be easily computed in RGB by a weighted sum of the RGB components.</p><p>In particular, the following abstracted functions are required. The function equal hue(C 1 ,C 2 ) yields the Boolean value "true" if the two input colors C 1 and C 2 have the same hue. This can be implemented by checking whether the color difference vector between C 1 and the gray axis (at the same luminance level as C 1 ) is a positive multiple of the difference vector between C 2 and the gray axis (at the same luminance level as C 2 ). Alternatively, hue values of C 1 and C 2 may be directly computed and compared, provided that a color system with explicit notion of hue is used.</p><p>The other required function is opposite color(C 1 ,C 2 ): it computes a color that is opposite to C 1 (opposite with respect to the gray axis) and that has the same luminance and saturation as C 2 . Opposite color is achieved by negating the difference vector between C 1 and the gray axis (at the same luminance level as C 1 ). Isoluminance is crucial to implement [R1] of Section 3. The same luminance and saturation as C 2 is achieved by scaling the negated difference vector so that isoluminance is guaranteed and equal distance to the gray axis (for isosaturation). Alternatively, in color systems with an explicit notion of hue and saturation, the hue angle can be rotated by 180 degrees to obtain the opposite color.</p><p>The code in Algorithm 1 first checks if the two input colors have the same hue, and if so, then the result is identical to traditional blending. Otherwise, hue preservation has to be explicitly ensured by modification of color addition. Here, we first assume that C 1 is the dominant color, leading to a tentatively assigned mixing color C new . If C new has different hue than C 1 (in fact, opposite hue), the assumption that C 1 is dominant is wrong. In this case, C new is computed by using C 2 as dominant color. Put differently, the dominant color is indirectly determined by testing the two potential alternatives for hue-preserving mixing of colors.</p><p>The example images of this paper are produced by using functions equal hue and opposite color computed in linear RGB space, along with calculations of luminance values from CIELAB. For the final display, linear RGB colors are transformed to sRGB.</p><p>Algorithm 1 Calculating C new = C 1 ⊕C 2 Require: C 1 and C 2 are valid colors in tristimulus space Ensure: C new is a valid color in tristimulus space if equal hue(C 1 ,C 2 ) then</p><formula xml:id="formula_3">C new = C 1 +C 2 else {!equal hue(C 1 ,C 2 )} C ′ 2 = opposite color(C 1 ,C 2 ) C new = C 1 +C ′ 2 if ! equal hue(C 1 ,C new ) then C ′ 1 = opposite color(C 2 ,C 1 ) C new = C ′ 1 +C 2 end if end if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We illustrate the effects of hue-preserving blending for several different examples of image compositing, and compare them to traditional blending. First, we start with the simple case of alpha blending two colors in a hue-preserving way. Two colors C 1 and C 2 are alphablended: <ref type="figure" target="#fig_1">Figure 4</ref> compares pairs of alpha-blended color profiles using traditional and hue-preserving blending. The two input colors are at opposite ends of each color profile, and alpha ranges from 0 to 1. It is easy to see that the hue-preserving blending produces no extra hues other than the original ones. A nice property of our method is that blending opposite colors or blending same-hue colors yields the same result as traditional blending, as shown in <ref type="figure" target="#fig_1">Figure 4</ref>(c)- <ref type="figure" target="#fig_1">Figure 4</ref>(e). show the typical cases where hue-preserving blending employs color transitions through gray to avoid extraneous hues. Images (c) and <ref type="bibr">(d)</ref> show that for blending opposite colors our method gives the same result as traditional blending. Image (e) demonstrates blending two colors of the same hue, which also yields the same result as traditional blending.  <ref type="figure" target="#fig_2">Figure 5</ref> demonstrates the additive mixing of three distinct color lights: red, green, and blue. Traditionally, the red, green, and blue lights combine to form yellow, magenta, and cyan. In our huepreserving approach, only the original colors are present. Perceptual transparency is still perceived because our method maintains the original luminance.</p><formula xml:id="formula_4">C new = (1 − α)C 1 ⊕ αC 2<label>(3)</label></formula><p>Next, we examine the more complex example of blending several colors normally encountered in volume rendering. In volume rendering applications, it is typical for users to choose a few distinct colors for visual labeling of classified materials during data exploration (usually 1-7 material colors). However, as the number of colors exceeds 2, the colors that can result from traditional blending cover a large and continuous range of different hues. <ref type="figure">Figure 6</ref> compares the possible colors that can result from blending up to 4 colors in both the traditional and hue-preserving methods. The colors are displayed in their respective coordinates in the HSL double-cone, viewed from above (i.e., looking down the HSL double cone from where L = 1.0). The traditional approach covers a large portion of the HSL color space as many mixed colors are introduced, whereas the hue-preserving approach is limited to its distinct input hues.</p><p>We are now ready to apply our blending technique to actual volume visualization of 3D scalar data sets. Images are rendered by frontto-back raycasting with optical properties such as color and opacity assigned to data values via a 1D transfer function. <ref type="figure">Figure 7</ref> shows a tooth model using traditional blending and hue-preserving blending.</p><p>In the traditional tooth model, the 3 input colors yellow, red, and blue mix to produce tints of orange and purple. The presence of these offhue colors is quantitatively documented in the color hue histogram in <ref type="figure">Figure 7</ref>(b). Using the new blending method, only the original 3 colors are present, as confirmed by the hue histogram in <ref type="figure">Figure 7(c)</ref>. In this way, color labeling is improved at no loss of feature identification. <ref type="figure">Figure 8</ref> shows the volume rendering of a human chest data set. In <ref type="figure">Figure 8</ref>(a), we use opposite colors blue and yellow, and show that our <ref type="figure">Fig. 6</ref>. In volume rendering, many colors of various hues may be mixed. At the top, we show the colors used in the blending. The next (middle) row shows all possible colors that can result using traditional color blending. The last (bottom) row shows the colors that can result from hue-preserving blending. Note that the possible colors are viewed in the HSL color cone from above (showing hue by angle and saturation by radius), so that the lower-lightness colors are occluded. We surround the colors with isoluminant HSL color circles to aid readers in identifying color hues.</p><p>approach produces the same result as traditional blending. However, when the blue flesh color moves its hue towards cyan, the traditional blending produces an undesirable tint of green, whereas our approach does not. This example was designed to resemble the color choice by Wang et al. <ref type="bibr" target="#b30">[31]</ref> in their <ref type="figure">Figure 8</ref>. If opposite colors are chosen according to their guidelines, hue-preserving blending is identical to traditional blending. However, we have essentially given the user the freedom to select arbitrary colors without having to worry about generating extraneous hues and false, mixed colors.</p><p>The smooth transition of colors through gray, as required by [R3], is demonstrated in <ref type="figure">Figure 9</ref>. Here, the opacity of the brain is gradually increased (from left to right). With increasing opacity, that inner part of the volume data set is becoming more and more pronounced and the respective color (green) is increasingly more dominant. The transition from dominant exterior color (red) and dominant interior color goes through gray (instead of yellow, as in traditional blending) with smooth variations of saturation. Additional comparisons of traditional and hue-preserving blending are shown and documented in <ref type="figure">Figures 10  through 13</ref>. <ref type="figure" target="#fig_1">Figure 14</ref> shows the tooth data set using energy aware colors <ref type="bibr" target="#b9">[10]</ref>. The hue-preserving method fixes a problem of the volume-rendering application in the original paper on energy aware colors, where colors shifted dramatically due to blending. The original design goal was to specify a palette of discrete, distinguishable color, which was achieved completely only for 2D maps. With hue-preserving blending, we can now maintain constant hue even in volume rendering. There is an additional side benefit of hue-preserving blending-it tends to lower energy consumption further because desaturated colors tend to be more energy-efficient.</p><p>There are some drawbacks of using hue-preserving color blending in practice. Gray colors in hue-preserving blended images can be confusing, as gray can come from blending various hues (see <ref type="figure" target="#fig_2">Figure 15)</ref>. One possible solution for reducing the amount of gray is to incorporate a bias function during the blending so that colors tend to be at the saturated end of the color vector (rather than in the less saturated, gray regions). Another drawback is that hue-preserving blending is orderdependent. When blending more than two colors, our method can produce different results depending on the blend order. Consider blending different colors one after another, and any pair of those colors blends separately to gray, then there are 3 possible results depending on the blend order.</p><p>Hue-preserving color blending in other color spaces produces similar results, as documented on our web page <ref type="bibr" target="#b0">[1]</ref>. We also encourage the reader to view our supplementary videos because motion parallax improves depth perception in any variant of volume rendering <ref type="bibr" target="#b7">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have presented hue-preserving blending as a modification of general Porter and Duff image compositing. The goal of hue preservation is to avoid false colors and improve visual labeling by color, even in transparent rendering. Our model is based on results from previous perception research indicating that perceptual transparency may be treated separately for achromatic and chromatic information. Accordingly, we have reused existing blending models for the achromatic channel and just modified chromatic compositing. Here, the main idea is to identify the dominant color whose hue survives blending; continuous color transition is achieved by gradually changing saturation, instead of hue. A practical benefit of our approach is that it may be readily included in any visualization system using Porter and Duff compositing because only minimal algorithmic changes are required.</p><p>We have targeted direct volume visualization as the main application, but any kind of non-photorealistic image overlay may benefit, too.</p><p>Hue-preserving blending and the color guidelines by Wang et al. <ref type="bibr" target="#b30">[31]</ref> share the same basic motivation of avoiding hue shifts. A fundamental difference is that our approach provides a generic blending model with parameter-free mathematical expressions, whereas Wang et al. focus on guidelines for color design, not on mathematical models. A related difference is that we target a replacement of arbitrary Porter and Duff image compositing, while Wang et al. rely on the specific geometric computation of overlap of surface geometry by means of depth peeling. Therefore, hue-preserving blending can be included in any transparency computation, including volume rendering. On the perceptual level, our model achieves a brightness behavior analogous to the Metelli model for the achromatic case, by separate compositing of brightness values. In contrast, the approach by Wang et al. does not target separate control of brightness. Furthermore, we have proposed the concept of dominant color to identify which color should shine through. In contrast, the local blending approach by Wang et al. always chooses the hue of the foremost color. That choice is not robust under small changes of input colors or scene geometry because even little tints of close-by colors could completely change the final image, which is particularly problematic for volume rendering. Therefore, hue-preserving blending can be considered the extension of the guidelines by Wang et al. to a robust, generic, versatile, and parameter-free computational model. Hue-preserving blending is motivated by previous perception research that provides a reliable basis for our approach to handling achromatic information and visual labeling. However, it should be pointed out that the exact role of the chromatic channels for perceptual transparency and corresponding computational models are still under investigation in perceptual psychology. According to some indication in previous work, we have ignored the chromatic channels for perceptual transparency. Yet, there are other, conflicting publications that indicate that color may play a role, too. Therefore, further perceptual studies in that area are necessary, subject to future work. It may turn out that transparency perception might be improved by loosening the hard restriction to complete hue preservation and by tuning further blending parameters. Such research might have to dive deeply into complex research questions of perceptual psychology. Since our main goal is to improve visual color labeling in combination with transparency rendering, the subtle factors for optimizing transparency perception have been outside the scope of this paper.</p><p>Another area of future research could implement hue-preserving blending based on color systems different than RGB. In particular, a strict computational separation of different perceptual channels could be achieved by more sophisticated color systems. One advantage of our approach is that we do not rely on measures of perceptual difference between different hue values, but only on a mechanism that provides identical hue or opposite hue. Therefore, perceptual uniformity is not really needed. However, care must be taken such that hue does not change along a straight line from the gray axis. Finally, applications outside direct volume visualization could be investigated. <ref type="figure">Fig. 10</ref>. Traditional (left) and hue-preserving (right) rendering of a fish data set. On the right, both red and orange are more distinguishable from each other, and the fish bone structure is more pronounced. <ref type="figure">Fig. 11</ref>. Traditional (left) and hue-preserving (right) rendering of a piggy bank data set. On the right, the blue color of the coins is much more distinguishable from the purple color of the piggy bank. <ref type="figure" target="#fig_0">Fig. 12</ref>. Traditional (left) and hue-preserving (right) rendering of a foot data set. Traditional blending produces shades of purple, whereas huepreserving blending does not. <ref type="figure">Fig. 13</ref>. Traditional (left) and hue-preserving (right) rendering of a segmented frog data set, with five features identified and color-coded. Input colors light green, yellow, and orange blend in the traditional approach to produce various shades of orange across the entire image. On the other hand, in the hue-preserving approach, we see that only the frog eyes are orange. This comparison shows that hue-preserving blending can work with a larger number of input hues. <ref type="figure" target="#fig_1">Fig. 14</ref>. Traditional (left) and hue-preserving (right) rendering of a tooth data set using energy aware colors. The inset shows a zoomed-in view, where traditional blending exhibits a substantial hue shift, producing tints of blue and green. In contrast, hue-preserving blending shows constant teal. <ref type="figure" target="#fig_2">Fig. 15</ref>. Traditional (left) and hue-preserving (right) rendering of an engine data set. Although colors are much more distinguishable in the hue-preserving case, gray colors can be confusing as they can come from blending various colors. One possible solution to reduce gray is discussed at the end of Section 5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Traditional blending of two colors yields various color hues (indicated by white dotted line). In contrast, hue-preserving color blending mixes the two colors so that they go through the gray point (red dotted line), avoiding any extraneous hues. (b) Traditional alpha blending of teal and orange. (c) Hue-preserving alpha blending of teal and orange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>In each pair, traditional (left) and hue-preserving (right) alpha blending for two colors are compared side by side. Images (a) and (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Traditional (left) and hue-preserving (right) color compositing of red, green, and blue. Since luminance is preserved, color transparency remains perceivable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .Fig. 8 .Fig. 9 .</head><label>789</label><figDesc>(a) Traditional (left) and hue-preserving (right) rendering of a tooth data set. In the traditional rendering, orange colors can be seen where red and yellow mix. There are also purple hues where red and blue mix. These extraneous hues completely disappear in the hue-preserving rendering. The color hue histograms for both renderings are shown in (b) and (c). Note the three vertical lines in the hue-preserving histogram, representing the original color hues. (a) Traditional (left) and hue-preserving (right) rendering of a chest data set, using opposite colors blue and yellow. Since the original colors are already opposite to each other, the traditional method does not suffer from extraneous hues, and in fact looks just like the hue-preserving rendering. (b) The blue hue of the flesh is offset towards cyan, and we immediately see that traditional blending produces tints of green. This, however, does not pose a problem for hue-preserving blending, which still maintains only cyan and yellow. (a) Volume rendering of a segmented frog data set with only the flesh and brain shown. (b) We illustrate the effect of increasing the brain opacity (left to right) in both the traditional (top) and hue-preserving (bottom) methods. The gray colors in the hue-preserving approach indicate the smooth transitions between the two colors. The yellow hue from traditional blending is eliminated in our approach.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is made possible through the support of the Natural Science and Engineering Research Council of Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Additional results of hue-preserving color blending on the web</title>
		<ptr target="http://www.cs.sfu.ca/gruvi/Projects/HuePreservingColorBlending/" />
		<imprint>
			<date type="published" when="2009-07" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lightness perception and lightness illusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The New Cognitive Neurosciences</title>
		<editor>M. Gazzaniga</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="339" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of illusory lightness and transparency in monocular and binocular images: The role of contour junctions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="452" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Texturing of layered surfaces for optimal viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1125" to="1132" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the role of figural organization in perception of transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ivry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="585" to="594" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The perception of transparency with achromatic colors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Prazdny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ivry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="407" to="422" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Basic Color Terms: Their Universality and Evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>University of California Press</publisher>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A perceptive evaluation of volume rendering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boucheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droulez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thibault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Test of a convergence model for color transparency perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>D'zmura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Energy aware color sets. Computer Graphics Forum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Color transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>D'zmura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Colantoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knoblauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Laget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="492" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Real-Time Volume Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Peters, Natick, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Color Appearance Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Psychophysical model of chromatic perceptual transparency based on substractive color mixture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ekroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1084" to="1095" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-level image cues in the perception of translucent materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="382" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transparent layer constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gerbino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Stultiens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Troost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>De Weert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology. Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ColorBrewer.org: an online tool for selecting color schemes for maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Harrower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="37" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Choosing effective colours for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visualization</title>
		<meeting>the IEEE Conference on Visualization</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename></persName>
		</author>
		<title level="m">Essentials of Neural Science and Behavior. Appleton &amp; Lange</title>
		<meeting><address><addrLine>Norwalk</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Organization in Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kanizsa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>Praeger</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Precision, accuracy, and range of perceived achromatic transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kasrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A A</forename><surname>Kingdom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A model for volume lighting and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Premoze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcpherson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Principles of Gestalt Psychology. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koffka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perceptual motion standstill from rapidly moving chromatic displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lesmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of National Academy of Science</title>
		<meeting>National Academy of Science</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="15374" to="15379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The perception of transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="91" to="98" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transparency: Relation to depth, subjective contours, luminance, and neon color spreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimajo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="513" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compositing digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (ACM SIGGRAPH 1994 Conference)</title>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="253" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Does colour provide an input to human motion perception?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Gregory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="55" to="57" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tool for dynamic explorations of color mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tebbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symp. on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="145" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Part boundaries alter the perception of transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="370" to="378" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Color design for illustrative visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Giesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zolliker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1739" to="1754" />
			<date type="published" when="2008" />
			<publisher>IEEE Visualization</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A practical model for subsurface light transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Marschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Color sequences for univariate maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="41" to="49" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Information Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Color Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wyszecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Stiles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
