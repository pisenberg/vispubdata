<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perception-Based Transparency Optimization for Direct Volume Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-10-11">11 October 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Ming-Yuen</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcai</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Ho</forename><surname>Mak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<email>chenwei@cad.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Perception-Based Transparency Optimization for Direct Volume Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-10-11">11 October 2009</date>
						</imprint>
					</monogr>
					<note type="submission">received 31 March 2009; accepted 27 July 2009; posted online</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Direct volume rendering</term>
					<term>image enhancement</term>
					<term>layer perception</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The semi-transparent nature of direct volume rendered images is useful to depict layered structures in a volume. However, obtaining a semi-transparent result with the layers clearly revealed is difficult and may involve tedious adjustment on opacity and other rendering parameters. Furthermore, the visual quality of layers also depends on various perceptual factors. In this paper, we propose an auto-correction method for enhancing the perceived quality of the semi-transparent layers in direct volume rendered images. We introduce a suite of new measures based on psychological principles to evaluate the perceptual quality of transparent structures in the rendered images. By optimizing rendering parameters within an adaptive and intuitive user interaction process, the quality of the images is enhanced such that specific user requirements can be met. Experimental results on various datasets demonstrate the effectiveness and robustness of our method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Volume visualization is a useful means to discover meaningful structures in volumes. It relies on proper transfer function specification to deliver the expected results according to user requirements. In typical scientific volumes, structures to be visualized may be layered or partially occluded by others in the rendered images. Instead of completely removing the occluding structures or exterior layers and assigning an opaque property to the target structures, the structures are often rendered in a semi-transparent manner to preserve their appearances and spatial information in the image, which is an advantageous characteristic of volume rendering. Despite its attractiveness, producing satisfying direct volume rendered images (DVRIs) is still a challenging research issue, as witnessed by the large amount of literature on transfer function design and volume rendering.</p><p>There are several problems in obtaining satisfactory rendered images of volumes with semi-transparent structures. Firstly, all the constituent structures should obtain a balanced and sufficiently high opacity in order to be visible in the image. As the opacity of structures affects the visibility of other layered structures, such mutual effects are difficult to resolve when the structural complexity of the image is high. A well-balanced opacity specification or adaptive adjustment becomes a non-trivial problem. Actually, visibility is a necessary criteria but not sufficient for expressive visualization. The structure and transparency perceptions play a more important role in viewers' understanding of the volume. Meanwhile, other visual properties like color and lighting are some of the crucial factors influencing our visual perception of the structures. These factors lead to a high dimensional parameter space, which is complicated and tedious to explore or manipulate. Therefore, an automatic or interactive adjustment method is necessary to maintain the quality of the rendered image.</p><p>Enhancing the perception of semi-transparent structures has been studied for decades. Non-photorealistic lighting <ref type="bibr" target="#b9">[10]</ref> and visual cues <ref type="bibr" target="#b0">[1]</ref> are often integrated in typical approaches. Psychological studies <ref type="bibr" target="#b24">[25]</ref> have identified that luminance and contrast are two major factors in transparency perception, while the contextual information is useful to provide visual hints. Other factors including lighting, shadow, reflection, and contours are also evaluated in the psychological studies. It is thus reasonable to exploit these psychological principles to facilitate the enhancement of the visualization process.</p><p>The goal of this paper is to develop a unified framework for automatic specification of rendering parameters and interactive enhancement for DVRIs to reveal layered structures in a semi-transparent manner. Based on the perception principles, transfer functions for illustrating layered volumetric structures can be obtained by means of novel image quality measures on visibility, shape, and transparency in conjunction with a parameter optimization procedure. The result is an image quality improvement that preserves the meaningful structures while revealing the context and spatial relation of these structures.</p><p>The contributions of this paper are as follows:</p><p>• A suite of image quality measures for assessing the effectiveness of the rendered image in revealing the layered semi-transparent structures in the volume • An adoption of psychological principles to derive rules to estimate the perceived transparency of structures in an image • A novel optimization framework for enhancing the rendering parameters and consequently the perceived quality of semitransparent structures in the image • An adaptive and interactive refinement solution to obtain specific refinements on transparent structures</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>In this section, we will first review some recent methods for transfer function design. The typical techniques for layer and surface visualization will then be briefly surveyed. Related work on transparency perception in psychology and perception fields will also be discussed.</p><p>Transfer Function Design Transfer functions <ref type="bibr" target="#b19">[20]</ref> can be categorized as data-centric or image-centric. The former determines the visual properties based on the volume data values and their derived attributes. Multi-dimensional transfer functions <ref type="bibr" target="#b13">[14]</ref> can be defined on the local properties of the volume to reveal the target structures. Properties like curvature <ref type="bibr" target="#b12">[13]</ref> and size <ref type="bibr" target="#b2">[3]</ref> have also been used. Alternatively, the image-centric transfer function is designed based on the rendered images. For example, transfer function can be searched based on the specific features <ref type="bibr" target="#b28">[29]</ref> or visibility <ref type="bibr" target="#b3">[4]</ref> of structures in the rendered image. Image processing operations have also been incorporated into the transfer function design <ref type="bibr" target="#b5">[6]</ref>. To facilitate the transfer function specification, many intelligent approaches have been proposed, including semi-automatic generation <ref type="bibr" target="#b4">[5]</ref> and semantics layers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>. Our approach is both data-and image-centric, and focuses on the optimization of the perception of transparent structures.</p><p>Surface/Layer Visualization Effective surface visualization not only concerns making a surface visible in an image but also maintaining the properties such as curvature, orientation, and texture of the shape. Shape-from-shadow <ref type="bibr" target="#b10">[11]</ref> is a common approach. To enhance the shape perception, Gooch et al. <ref type="bibr" target="#b9">[10]</ref> proposed a non-photorealistic lighting model for technical illustration. Light Collages <ref type="bibr" target="#b14">[15]</ref> use multiple lights and local illumination to adaptively enhance the shape of different parts of the structures. Lighting methods can emphasize shape perception through the features such as shadows, highlights, and silhouettes in an image. Specular reflection <ref type="bibr" target="#b7">[8]</ref> has also been proven to be an effective channel for shape perception. Another common class of techniques is shape-from-motion <ref type="bibr" target="#b26">[27]</ref>. Through the spatial and temporal changes in a sequence of images, shape details can be reconstructed by viewers. Kinetic visualization was proposed by Lum et al. <ref type="bibr" target="#b15">[16]</ref> to add motion cues to static objects using a particle system. For layered surfaces, texturing has also been extensively explored. Different textures were tested and a guideline on texture synthesis <ref type="bibr" target="#b0">[1]</ref> was developed for effective layered surface visualization. Interrante et al. <ref type="bibr" target="#b11">[12]</ref> also used textures to enhance the relative depth and features of layered surfaces. Other visual cues like boundary or silhouette contours, stereo, and occlusion can also be used to encode the shape information of surfaces. Volume illustration approaches <ref type="bibr" target="#b22">[23]</ref> have also been proven to be an effective way to convey the structural details in volumes. In this paper, we do not work on non-photorealistic rendering or visual cues for shape but focus on enhancement of direct volume rendered images. We believe a well-balanced rendered image is necessary before any visual effects or techniques can be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transparency Perception</head><p>The perceived transparency of structures depends on subjective human perception. This topic has been studied in psychology for decades. Various factors <ref type="bibr" target="#b6">[7]</ref> like shadow, lighting, contrast, color have been considered as the critical visual cues to reveal transparency. Metelli et al. <ref type="bibr" target="#b17">[18]</ref> used a simple physical model to rationalize visual perception on transparency. Luminance <ref type="bibr" target="#b8">[9]</ref> is considered as an important channel in conveying transparency information. Based on this theory, Singh and Anderson <ref type="bibr" target="#b24">[25]</ref> formulated an extension using contrast and proposed the transmittance anchoring principle (TAP) to evaluate the transparency of layers in images. This principle was tested in various conditions <ref type="bibr" target="#b25">[26]</ref> and has been widely used in the perception field. Commonly used visual cues for transparency actually emphasize the luminance profile of the image to enhance the transparency. For example, lighting and color <ref type="bibr" target="#b27">[28]</ref> can be used to give distinct luminance and contrast to transparent structures. The effects on the image can be explained and evaluated by these perception rules. Our work is closely inspired by these principles, which lead to a new transparency measure and guide the optimization of appropriate transparency configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ENHANCEMENT FRAMEWORK</head><p>The proposed enhancement framework consists of several image quality measures and an optimization process. Given a volumetric dataset, we assume that the structures in the volume are defined and assigned with color and importance values (or opacity). Our objective is to automatically adjust the rendering parameters to reveal the structures in a semi-transparent manner. This semi-transparent appearance of structures can help preserve the context and spatial relation information among layered or hierarchical structures in the volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structures in Volume</head><p>To evaluate the perception of semitransparent layers in a rendered image, the structures in the volume have to be implicitly or explicitly defined in the volume or other feature space. In this paper, we implicitly segment the data based on intensity and the segmented regions are treated as structures in the following discussion. We focus on the conceivable quality enhancement and regard the volume segmentation as an input. The discussion of volume segmentation or volume classification via transfer function designs is beyond our scope. Likewise, we define the boundaries of structures as the structural layers that can be computed with previous methods like opacity peeling <ref type="bibr" target="#b21">[22]</ref> and volume catcher <ref type="bibr" target="#b18">[19]</ref>.</p><p>Structural Layers in DVRI Each structure in a volume can be treated as a structural layer projected on the rendered image with certain transparency. A layer can reveal the shape and appearance of the corresponding structure in the volume. However, the layers may not be perceived effectively due to various factors like poor lighting and rendering parameter settings. Image quality may also deteriorate with overlapping or adjacent layers. Our objective is to enhance the visual perception of layers in conveying the underlying structural information. Users can specify the expected visual properties (i.e., color and opacity) of each class of structures using a transfer function interface. The opacity is considered as the importance value and the transparency of the layers is optimized with respect to it in the final image.</p><p>An overview of the framework is shown in <ref type="figure">Fig. 1</ref>. Given the volume and structural information, some invariant volume and image metrics are first pre-computed. In the optimization process, the quality of the rendered images is assessed based on three aspects of layered visual perception, namely, visibility, shape, and transparency. Quantitative measures are proposed to evaluate these perceptions. The fundamental idea is to ensure that the layer and shape information of the structures can be faithfully conveyed in the rendered image. The deviation between the volume content and perceived image is minimized. We formulate the perceptual deviation as a set of energy terms for a least square optimization, yielding an optimal rendering setting. An enhanced DVRI is produced using the optimized rendering parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERCEPTUAL QUALITY MEASURES</head><p>Visibility is the first necessary condition for a structure to be clearly shown in an image. To reveal the layered structures in an image, each layer should acquire a significantly high opacity and meanwhile the visibility of the structures should be balanced. Provided a good visibility, the shape and transparency should also be faithfully presented in the image for depicting the details of structures and ensuring the distinguishable appearance and correct layer perception. Several perception rules were derived from these factors in previous psychological studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> and were supported with extensive experimental evidences. Based on these investigations, we formulate three measures to assess the quality of rendered images with layered structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visibility</head><p>While a structure may be assigned a significantly high opacity, its visibility in the rendered image may still be low. In the ray casting process, each ray may pass through various structures and has different ray compositions. Consider that a layer at the back may be severely occluded if the accumulated opacity is high. The ideal situation is to have all the constituent structures contribute to the ray composite value in proportion to their structural composition. Our solution is to equalize the opacity of structures with respect to the portion of constituent structures (layers) in the image and the originally assigned opacity (importance). The optimal opacity setting should ensure the visibility of structures to be proportional to their constituent portion in every ray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structural Visibility Histogram</head><p>A B  The visibility measure is proposed to evaluate the overall deviation of the structural constituent and visibility profiles of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Structural Opacity and Visibility Histograms</head><p>Given a volume with defined structures and associated importance (opacity), volume analysis is performed to evaluate the composition of structures in the rendered image by considering the content of the cast rays during rendering. The opacity in the image is derived by the compositing equation given by</p><formula xml:id="formula_0">α accum = α o (1 − α accum ) + α accum (1)</formula><p>where α o is the opacity value. Each sample point contributes to the final image in different degrees and its contribution or visibility α v is defined by α(1 − α accum ). For each ray, we compute the voxel opacity α o (importance) of every sample point on the ray and its visible opacity α v . The voxel opacity profile along the ray indicates the voxels to be visible in the ideal case without occlusion. By comparing these two ray opacity profiles, we can compute the possible deviation between actual and perceived ray constituents. The visibility of each class structure should be kept proportional to its contribution to the ray opacity. For each ray r, the opacity α o and visibility α v of a structure s ∈ S are defined as the normalized sum of assigned opacity and visible opacity of all sampled voxels belonging to that structure along the ray. The distribution of opacity and visibility of constituent structures can be represented as histograms <ref type="bibr" target="#b3">[4]</ref>  <ref type="figure" target="#fig_0">(Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Deviation Measure and Equalization</head><p>The difference between the structural opacity and perceived visibility is derived to indicate the visibility deviation. As occlusion is inevitable in ray composition, the visibility of voxels is always lower than its initially assigned opacity. However, we can still strike for a low variance and average of deviation in the optimization process. Similar to the histogram equalization in image processing, we equalize the visibility of structures by minimizing the visibility deviation of rays. The visibility deviation δ v of a ray is defined as |α o − α v | and the overall deviation of the image is given by</p><formula xml:id="formula_1">E v = 1 |R| ∑ r∈R ∑ s∈S δ v (s, r) = 1 |R| ∑ r∈R ∑ s∈S |α o (s, r) − α v (s, r)| (2)</formula><p>Minimizing this deviation achieves a balanced visibility distribution and yields the first criteria for transparency enhancement. We describe the other two measures below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shape</head><p>Shape is another important perception factor that influences the perception of layered transparency. The shape can be interpreted as the variation of surface orientation. Geometric or topological details of structures provide useful information and must be faithfully depicted in the image. In fact, shape visualization is a typical perceptual problem and is generally attributed to various psychological factors <ref type="bibr" target="#b0">[1]</ref>. The structural shape should have a strong correlation with visual variations on an image. For example, crests and valleys on a structure should result in a gradual change in the image intensity. Any visual deviation on this correlation can affect the discrimination of the shape. To evaluate the expressiveness of the image in revealing the shapes, a measure on the overall visual shape deviation of an image is needed.</p><p>We define the structural shape v s and image variation v i as the curvature of structures and the gradient of the image respectively. We follow the methodology of curvature measurement <ref type="bibr" target="#b12">[13]</ref> for volume data. In particular, we use the mean curvature H = 1 2 (κ 1 + κ 2 ) for the structural shape and use the Sobel operator to estimate the image gradient G. The scalar magnitudes of the two measures (i.e., |H| and |G|) are normalized to [0..1]. The value of v s of a pixel in an image is given by the weighted sum of v s of all voxels on the boundary of structures along the ray. The deviation δ s between these measures at each pixel is derived by a composite function (Eq. 3) and the overall shape deviation E s of an image I is defined as their average, as shown in Eq. 4.</p><formula xml:id="formula_2">δ v = 1 − exp(− (v s − v i ) 2 s )<label>(3)</label></formula><formula xml:id="formula_3">E s = 1 |I| ∑ i∈I δ v (i)<label>(4)</label></formula><p>where s controls the steepness of the exponential function. The response is high if the shape value v s is high, while the image variation v i is low or vice versa. It indicates the unclear shape of structures or fault shape cues in the image. For example, a focused spot light with high spot exponent and low cut-off angle gives an illusion of strong shape variation on a plane. In fact, lighting <ref type="bibr" target="#b14">[15]</ref> and shading are the determinant shape perception factors in such images and are adjusted in the optimization process. Moreover, we agree that shading in combination with visual cues provides powerful emphasis on shapes. Based on our evaluation results, textures <ref type="bibr" target="#b0">[1]</ref> or shape cues can be adaptively applied on the poorly perceived structures in the image. By minimizing the E s , image variations are reinforced at the regions of structures in the image to reveal the underlying shape information or fine structural details. This yields a boosting of the shape perception to ensure the correct discrimination of shape variations through the rendered image and maximize the expressiveness power of the image in conveying the shape information.</p><p>An illustration using a sinc function plane is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Random noise is added to the center peak to generate high frequency shape variations, which are indicated in the bright regions in the volume variation image. The composite image is generated by convolving the volume variation with the image gradient. The bright region indicates the unclear shape of the structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transparency 4.3.1 Perception Theory</head><p>Equalized visibility is not always sufficient to obtain a good perception of transparent layers. Given the same opacity, the perceived transparency of a layer can still vary dramatically. For example, Wang et al. <ref type="bibr" target="#b27">[28]</ref> showed that the color design of the layers is crucial and transparency can change with the color saturation of layers. Furthermore, lighting on the layer and context can provide hints to the recognition of the layers. Other image cues <ref type="bibr" target="#b6">[7]</ref> like highlight, contrast, and blur also play an important role in human visual perception.</p><p>Metelli's episcotister model <ref type="bibr" target="#b17">[18]</ref> is a widely adopted transparency perception theory. Given a transparent layer on a bipartite background of region A and B <ref type="figure" target="#fig_2">(Fig. 4(a)</ref>), the transmittance α met of the layer, which indicates the fraction of light passing through the layer, is derived from the physical equations (Talbot's Law) as follows:</p><formula xml:id="formula_4">p = α t a + (1 − α t )t q = α t b + (1 − α t )t α met = (p − q)/(a − b)<label>(5)</label></formula><p>where p, q, a, and b are the reflectances of regions P, Q, A, and B respectively. The reflectance can be replaced by luminance, which has been proven to be an intuitive channel to human visual system and is more effective in rationalizing the contrast perception <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure" target="#fig_2">Fig. 4(c)</ref> shows an example of two spheres overlapping in an image. Both spheres have identical opacity but different colors. The perceived transmittance of yellow and purple layers in the overlapping region P is derived using the Metelli's equation and the values are 0.49 and 0.36 respectively. It indicates that less light is allowed to pass through the purple layer, thus resulting in a higher opacity. This complies with our visual perception that P is more purple than yellow. This model can explain the transparency perception of layers with uniform luminance. For textured or complex layers, Singh et al. <ref type="bibr" target="#b24">[25]</ref> extended the original theory as a generative model to tackle the inhomogeneity in transmittance and reflectance. Inferring the transparency from luminance distribution involves scaling and anchoring problems. They determine how the luminance ratio is mapped to the ratio of perceived transparency and how to anchor this relative scale to the absolute one. Based on the psychological observation, scaling can be implied in contour contrast, which changes linearly with the transparency. Moreover, according to the transmittance anchoring principle (TAP), the highest contrast segment along a contour can serve as an anchor for determining the absolute scale of lower contrast regions. This model was validated in the experiments on various inhomogeneous surfaces and media. The transmittance is defined in terms of the contrast (range of luminance I) of background A and transparent regions P <ref type="figure" target="#fig_2">(Fig. 4(b)</ref>) and is given by</p><formula xml:id="formula_5">α tap = I max (P) − I min (P) I max (A) − I min (A) = I range (P) I range (A)<label>(6)</label></formula><p>This implies that the contrast of the underlying contour is reduced by the overlay transparent layer. Singh et al. <ref type="bibr" target="#b25">[26]</ref> further improved the model by replacing the luminance range with the Michelson contrast defined as follows:</p><formula xml:id="formula_6">C = I range = I max − I min I max + I min<label>(7)</label></formula><p>To avoid the luminance ranges being affected by noise, the I min and I max are defined as max(0, I µ−2σ ) and min(I MAX , I µ+2σ ), given I is in [0..I MAX ] and µ, σ are the mean and variance of luminance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Transparency Measure</head><p>In our paper, we adopt a hybrid approach of TAP and Metelli's model. In homogenous regions with low contrast, Metelli's model can per- fectly estimate the transparency, while the TAP (Eq. 6) becomes unstable with small ranges of luminance. Therefore, a modulation is performed on the transparency, given by</p><formula xml:id="formula_7">L 0 L 4 Image Sub-regions L 1 L 3 L 2 R 0 R 1 R 2 R 5 R 6 R 4 R 3 (a) Image Sub-regions L 0 L 1 L 3 L 2 R 0 R 1 R 2 R 5 R 6 R 4 R 3 R 7 R 8 (b)</formula><formula xml:id="formula_8">α t = α met h + α tap (1 − h)<label>(8)</label></formula><p>where h = (I MAX − I range )/I MAX . α t tends to α met if the regions become homogeneous. The perceived opacity is interpreted as 1 − α t .</p><p>With this transparency perception model, we can compute the transparency of layers as well as the perception deviation in different regions of the image. In the preprocessing procedures, the structural layer composition at different parts in the image is computed and the image is decomposed into sub-regions based on their composition. The regions are then classified into different types (empty, plain view, or overlay) and the perceived transparency of each constituent layer is computed, as shown in <ref type="figure" target="#fig_3">Fig. 5(a)</ref>. An empty region does not consist of any layer, while a plain view region only contains one layer. An overlay region consists of more than one layer and is either an overlapping or enclosed regions of structural layers. A structural layer may be decomposed into several sub-regions and the relations between the structural layers are defined as separate, touch, overlap, or enclose using the region connection calculus <ref type="bibr" target="#b1">[2]</ref>. Based on this information, we can determine the relation property of the constituent layers in each region. This information can be used to determine the perceived transparency as well as the transparency perception deviation δ t of the region. An illustration of perceived transparency computation is shown in <ref type="figure" target="#fig_3">Fig. 5(a)</ref> and the rules are summarized as follows:</p><p>Case 1: For an empty region (R o ) or a plain view region (R 3 ) with a separate or touch layer, the deviation is zero because no layered structure is present.</p><p>Case 2: For a plain view region (R 1 , R 4 , R 6 ) with an enclosing or overlap layer, the transparency of the layer can be derived by Eq. 6 for the enclosing layer (L 1 ) or Eq. 8 for the overlapping layer (L 3 , L 4 ).</p><p>Case 3: For an overlay region (R 2 , R 5 ), the transparency of each layer is determined by its relation property. If a layer belongs to an enclosing layer (L 1 in R 2 ), the transparency is the same as that in the enclosing region (L 1 in R 1 ). If the layer belongs to an overlapping layer (L 3 and L 4 in R 5 ), the transparency can be derived by Eq. 8. </p><formula xml:id="formula_9">α L i = I {L} − I {L i } I {L−L i } − I { / 0}<label>(9)</label></formula><p>where I { / 0} is the empty view excluding any enclosing layer. For example, the transparency of L 2 in R 5 is computed as (</p><formula xml:id="formula_10">I R 5 −I R 3 )/(I R 6 −I R 1 ).</formula><p>Given the perceived transparency α t of the non-empty regions, the transparency deviation δ t of a region is derived by the sum of square differences between the perceived opacity 1 − α t and the structural opacity α o of all the constituent layers. The overall transparency deviation E t measure is given by</p><formula xml:id="formula_11">δ t = ∑ i∈L (1 − α t (i) − α o (i)) 2 (10) E t = 1 |R| ∑ r∈R ω r δ t (r)<label>(11)</label></formula><p>where R is the set of all non-empty regions in the image and ω r is the weight of region r, which is given by its portion of area in the image. By minimizing the transparency deviation, the perceived transparency of the structures will be closer to the expected transparency or composition of the structures in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OPTIMIZATION</head><p>Recall that the perception of a semi-transparent structure in a DVRI is driven by the visibility, shape, and transparency, which are governed by the rendering parameters including opacity, lighting, and color. To faithfully depict the structures in the image, we have to solve for an optimal parameter setting for rendering. The three measures are used to drive the optimization of the rendering parameters for an optimal rendered result. More specifically, the rendering parameters (transfer functions) of the structures are formulated as an objective function f and is optimized as a least square problem <ref type="bibr" target="#b16">[17]</ref>. Our objective is to minimize the perception deviation (or measures) of the overall image by fitting an optimal parameter configuration for the volume at a specific viewpoint. For each ray r, we derive the energy as</p><formula xml:id="formula_12">E = ω v E v + ω s E s + ω t E t<label>(12)</label></formula><p>where ω v , ω s , ω t are the weights of the measures. We setup an overdetermined system of all the ray quality equations and compute the sum of residue</p><formula xml:id="formula_13">S = ∑ r∈R E( f , r) 2<label>(13)</label></formula><p>The optimal solution with the minimum residue is derived by finding an f for the given DVRI, such that</p><formula xml:id="formula_14">argmin f {E( f )}<label>(14)</label></formula><p>To solve the non-linear least square problem, the parameters are refined by an iterative solver <ref type="bibr" target="#b16">[17]</ref>. We adopt the conjugate gradient method, which is a widely used direct search method with good convergence performance. Because it is difficult to explicitly compute the analytical expressions for the partial derivatives of the measure equations, we perform an empirical approximation by sampling the image with different f . Based on the steepest descent direction −∇ f E( f ) and the derived conjugate direction Λ f , the transfer function is updated as</p><formula xml:id="formula_15">f n+1 = f n + α n Λ f n (15)</formula><p>where α n is given by argminE( f n + α n Λ f n ).</p><p>Optimizing all the rendering parameters simultaneously is inefficient; thus, a hierarchical approach is adopted. The visibility of structures is first optimized by adjusting the opacity. Given that every structure becomes basically visible in the image, the shape of the structures is then preserved by proper lighting. The transparency perception of the structures is finally enhanced with proper color. Optimization can be done sequentially and each step only involves a subset of the parameters. The importance and color of structures provided by users as well as the default light parameters are the initial guess for the optimization. To avoid local optima, the simulated annealing technique is applied on the parameter optimization. For example, a high transition probability is assigned to opacity if the current visibility deviation is high or does not show a significant improvement from its initial value. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ADAPTIVE REFINEMENT</head><p>A globally optimized solution may not be applicable to all parts of the image. The optimized configuration may be biased towards the dominating structures in the image and leaves the less significant structures unenhanced. Our system allows users to selectively enhance a specific part of the image or structures. To guide the interactive enhancement process, an image quality map interface is provided to show the deviation measure values at different parts of the image. Users can use a lens-like tool to specify the regions with poor quality with reference to the quality map. The regions are then enhanced individually. Users can also specify an expected visibility or transparency to the structures of interest within the region to ensure that they are clearly shown and enhanced in the refined image. An example is shown in <ref type="figure" target="#fig_5">Fig. 6(a)</ref>.</p><p>Besides, as the perceived transparency may slightly vary between viewers and may not always change linearly with the luminance mean and contrast, a calibration tool is provided to estimate the transparency and contrast relation ( <ref type="figure" target="#fig_5">Fig. 6(b)</ref>). According to the perception theory, the expected luminance values of different transparencies are represented by the straight line between the background luminance (L 0 , R 0 ) and the origin. From the psychological experiment <ref type="bibr" target="#b24">[25]</ref>, we can observe that the user perception falls within the region shown in the chart. The exact perception can slightly deviate from the straight red line in <ref type="figure" target="#fig_5">Fig. 6(c)</ref>. To calibrate the curve, a test on sample images is performed to record the user perception on layers with different transparencies. After the test, a calibrated curve representing the user perception is computed. The perceived transparency can be located on the calibrated curve. The transmittance value derived from the contrast ratio (Eq. 6) will be adjusted according to this curve in the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>We conducted experiments on several datasets to demonstrate the quality measures and the optimization of rendered images. Our system was run on a Dell machine (Pentium Core2Duo 6400, 2G RAM) equipped with an NVIDIA GeForce 7600GTS graphics card. The volumes were pre-segmented. Results on different measures will be first discussed and two comprehensive results will be provided afterwards.</p><p>We first used a carp datatset (256 × 256 × 512) as shown in <ref type="figure" target="#fig_7">Fig. 7</ref> to show the result of visibility equalization (opacity optimization). The skin and bones of the carp were assigned with importance values of 0.2 and 0.8 and our objective was to balance the overall visibility of each structure based on this weighting. The equalization on the opacity was performed by minimizing the visibility deviation from the importance weighting (i.e., visibility measure). The measure image indicating the deviation of the original DVRI showed that the bones were occluded. The opacity of the structures was optimized and the result showed that the overall deviation of the image was lowered and the bone structures were revealed according to the importance ratio. Some optimized results with different ratios were also shown in the figures. By observing the resulting quality image and the refined DVRI, we can see that the carp was rendered in a semi-transparent manner with balanced visibility after the optimization based on our measure.</p><p>To demonstrate the shape enhancement result, we conducted an experiment on a CT head dataset (128 × 128 × 231) as shown in <ref type="figure">Fig. 8</ref>.  The shape measure image indicates the shape variations and perception deviation in the DVRI. To obtain a better shape perception of the face and skull, the lighting parameters for the structures have to be optimized for emphasizing the shape variations on both layers. In the optimization process, the parameters obtained by the structures could be different. For example, a large specular highlight (small specular reflection exponent) is exerted on a large and smooth surface (e.g., skin). By optimizing the lighting parameters of the structures, the shapes as well as features on different layers of the head were enhanced. This can be reflected in the reduced overall value of the resulting shape measure image. From the experiment, we can see that illumination is important to shape enhancement and our measure effectively drives the optimization on illumination to achieve better shape-revealing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equalized Result Original</head><p>To demonstrate the transparency measure, an experiment was conducted on a protein (Neghip) molecule dataset (64 × 64 × 64) obtained by simulation. To show the effect of color on transparency perception, DVRIs of the molecule were generated with different color saturation values <ref type="figure">(Fig. 9)</ref>. In the DVRIs, the layers of structures overlapped and the color saturation change resulted in different transparencies of the outer layer. The perceived transparency of the outer layer decreased with the saturation; thus, the overlapped inner structures became less visible. Based on the TAP, we can derive the perceived transparency of the structures in the DVRIs. These results show that the transparency perception does not only depend on visibility but also the color or appearance of the layer. To faithfully present the structures in the image, we should optimize the transparency of structures in the image in addition to the opacity of each constituent structure. From the experiment, we can see that the TAP can effectively estimate the perceived transparency of layers in the volume and the result follows the previous psychological findings. Various transparency effects can be achieved by optimizing on the color using our measure.</p><p>Comprehensive experiments were conducted to illustrate the complete pipeline of the optimization process. We first demonstrated how to generate the semi-transparent style DVRI of a CT engine dataset (256 × 256 × 128) with layered structures, as shown in <ref type="figure">Fig. 10</ref>. The opacity equalization was first performed to balance the visibility of the interior and exterior structures. Semi-transparent layers of structures were generated as a result. Afterwards, the lighting parameters were optimized to enhance the overall shape perception of the transparent layers of structures. The results show that the features of the structures were better preserved in the image. To ensure that the perceived transparency complies with the composition (structural opacity) of the layers, the color of the structures were optimized with respect to the TAPbased transparency measure and the expected transparency. Another experiments was conducted on a CT chest dataset (384 × 384 × 240) as shown in <ref type="figure">Fig. 11</ref>. The result shows that the structures became more distinguishable and the details were better preserved after the optimization on visibility, shape, and transparency. From these experiments, we can see that our optimization method allows high quality semi-transparent style DVRIs to be generated with optimal visibility, shape, and transparency perception.</p><p>The performance of the system benefits from the hierarchical optimization, which only involves a subset of the parameters in each step. The results usually converge within 10 iterations in each step. In our experiment, we found that the expected opacity (importance values), color of structures, and default lighting parameters could already provide a good initial guess for the optimization and enhancement could be done very efficiently based on these settings. As the partial derivatives are empirically estimated on the rendered image, the process can be speeded up by sampling on the image. The performance depends on the computation in ray and structural analysis, which increases with the size and complexity of the volume as well as the image resolution. For the CT chest, each iteration takes about 0.2s and the whole process completes in 10s with an image resolution of 512 × 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EVALUATION AND DISCUSSIONS</head><p>To validate our method, we invited 20 graduate students to participate in our user study. Before the test, they were given sample images illustrating different degrees of transparency as reference for quantitative judgment of perceived transparency. While the effect of opacity and lighting on visibility and shape perception has been studied <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, we specifically studied the correlations between transparency and color by conducting controlled experiments. The subjects were first asked to rank a set of DVRIs, which were generated by adjusting the color of different structures, based on the perceived transparency. The results were compared with the measured transparency (Section 4.3). The results showed that 85% of the subjects got the expected rankings coincident with those of the measured values.</p><p>To more quantitatively study the transparency perception, the subjects were then asked to evaluate the degrees of perceived transparency of the structures in 10 DVRIs of the 5 datasets used in our experiments (Section 7). The reported values were compared with the measured transparency of the structures. The results showed that most subjects could perceive the correct degree of transparency of the structures. While visual perception varied among viewers, the means of the perceived transparency of images were close to the measured transparency and the relative differences were between 3.7% and 8.2%. The relative standard deviations of the results were between 9% and 16%. A single-sample t-test was also conducted for the results of each image with the measured transparency value as the hypothetical mean. The pvalues were between 0.02 and 0.21, while 2 results generated from the CT head dataset fell below the significance level of 0.05. The minor errors could be attributed to the users' varied experience with transparent structures and deviation due to the existing image cues <ref type="bibr" target="#b6">[7]</ref>. In general, there was no significant difference between the perceived and measured values. The user study demonstrated that the transparency measure can correctly estimate the perceived transparency.</p><p>Finally, the subjects were asked to rate the improvement and quality of the images throughout the optimization process. The feedback from the subjects indicated that the layered transparent structures (e.g., ribs in the chest image) might not be distinguishable even after the visibility equalization but an improvement was observed after the shape and transparency adjustment. It indicated that a good perception of transparent structures relies not only on visibility (opacity) but also the color and lighting. All the subjects agreed that color and opacity are important visual factors and 70% also thought that lighting can improve the visual quality of transparent layers. 90% subjects rated the improvement in visual quality as good or significant. This study demonstrated that all the three measures introduced in the paper have their values and can improve the perception of transparent layers.</p><p>Our method is an improvement over the conventional methods. Compared with the manual specification of transfer function, our approach does not require any user involvement. Manual manipulation highly depends on user expertise. For many end-users such as doctors and scientists, they do not have the expertise to directly manipulate transfer functions and lighting parameters. Thus, it is unlikely that they can obtain results with similar quality by manual manipulation. By using the proposed measures, optimal and objective results based on human perception can be automatically obtained. More- over, the specific and adaptive refinements on each layer cannot be achieved manually using typical intensity or class-based transfer function interfaces. Compared with the semi-automatic approach <ref type="bibr" target="#b4">[5]</ref> which generates the opacity transfer function based on the histogram volume structure (i.e., boundaries), our solution is a data-and imagecentric approach and can provide comprehensive optimization on more rendering parameters including color and lighting. While the semiautomatic approach provides a high-level interface for opacity specification, our method can automatically generate semi-transparent layers with proper opacity, color, and lighting as well. Recall that visibility (opacity) is only one of the factors to our transparency perception while color also play an important role <ref type="bibr" target="#b27">[28]</ref>. Our method can comprehensively optimize different rendering parameters, such that the perception of each layered structure in the image is better reinforced. Different from typical transfer function design approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref> in which specific data features are incorporated into the multidimensional transfer function <ref type="bibr" target="#b13">[14]</ref>, we focus on the effectiveness of the resulting images in conveying the layered features in a semi-transparent manner and ensure that the specific features are not only enhanced and rendered properly but also faithfully perceived in the images. Our solution provides an additional perception-based quality enhancement on the image, which has not been addressed in the previous approaches. There are several limitations in applying the measures on rendered images. Recall that layers of structures have to be implicitly or explicitly defined for the perception measurement. An intuitive segmentation or feature specification tool is necessary for the purpose. Furthermore, it is basically an ill-posed problem to determine the transparency of a single layer in plain background. In fact, our perception measures use the available visual hints in the image to estimate the quality of the image based on the user perception. Such hints should be available and they are actually required by humans to make correct visual judgment. Moreover, the improvement may be limited if there are many layered structures coupling in the image. Usually, human vision can only handle a limited number of layered structures and the perceived quality of each layer deteriorates in complex data. Thus, visual cues should be added on the poorly perceived regions indicated by our measures in addition to optimizing the rendering parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this paper, we proposed a DVRI enhancement solution based on the perception principles. Three measures were designed to evaluate the perception of the semi-transparent layer from the visibility, shape, and transparency aspects. Rendering parameters were optimized based on these measures to deliver results complying with viewers' perception. High quality semi-transparent style DVRIs with structures faithfully revealed can be automatically generated using our method. Although our work focuses on the optimization for direct volume rendering, the measures can provide good indications of structural perception so that additional visual cues like textures and shape cues can be adaptively applied to enhance the expressiveness of the image.</p><p>While opacity is usually considered as the determinant factor for the transparency of structures, in the experiment we showed that color and contrast also affect our visual perception of transparency. Our optimization method puts these factors into account to deliver results that can faithfully reveal the layered structures in a semi-transparent manner and ensure a correct perception. Our method also eases the finetuning of the parameters for transparent results. In the future, we will extend the current static viewpoint solution to an efficient image refinement of dynamic views for the purpose of interactive exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The opacity (importance) and visibility of structures are sampled in the ray casting process and the values from all the rays are grouped into different classes of structures and represented as histograms. The overall visibility deviation of the image is defined as the difference between the two measures and is minimized in the equalization process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Shape deviation measure: volume and image variations are computed based on the volume curvature and image gradient. The composite measure estimates the deviation in the strength of variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Example of Metelli's theory: transparent layer on (a) bipartite and (b) inhomogeneous regions; (c) illustration using overlapping spheres.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Structural layers and sub-regions in image: (a) shows different kinds of relations between the structural layers. The image is divided into sub-regions based on the layer composition. Another example of overlapping and enclosed layers is shown in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 (</head><label>5</label><figDesc>b) shows a more complicated example of regions with three or more layers. Region R o , R 1 , R 2 , R 3 , R 8 can be evaluated by the above rules. Region R 4 , R 7 , R 6 consist of three layers. The enclosing layer L 3 is the same as that in R 1 , which is derived by case 2 where R 1 is the background and all regions enclosed by L 3 are treated as a transparent layer. For the overlapping layers in R 4 , R 7 , R 6 , they can be degenerated to the simple case by ignoring the enclosing layer L 3 . Region R 5 contains three overlapping layers L = {L 0 , L 1 , L 2 }. The transparency of a layer L i can be resolved by treating other overlapping layers L − L i as a single layer. The Metelli's equation can be generalized to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>(a) Our user interface for selective enhancement of DVRIs; (b) Luminance chart showing the relation between luminance and transparency; (c) Calibrated chart based on user perception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Experiment on the visibility equalization using a carp dataset. The skin and bone structures are given the importance values of 0.2 and 0.8. Results of other importance ratios are shown at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 . 2 Fig. 9 .Fig. 10 .Fig. 11 .</head><label>8291011</label><figDesc>Experiment on the shape enhancement using a CT head dataset. (a) The original DVRI; (b) An image that indicates the unclear or missing details (deviation) on the DVRI and measures the shape; (c)-(d) Optimizing the lighting parameters for each structure in the data; (e) Final shape-enhanced result. Experiment on the effect of color on transparency perception using Neghip dataset. The outer layer structures in pink are assigned with different saturation values. From left to right: 100%, 75%, 50%, and 25%. The results show different transparency effects. Experiment on a CT engine dataset. (a) The original DVRI; (b) Equalizing visibility through an opacity optimization; (c) Optimizing the lighting parameters for shape enhancement; (d) Adjusting the color for correct transparency perception. Experiment on a CT chest dataset. (a) The original DVRI; (b) The bones are visible after an opacity optimization; (c) The visible structures are further enhanced after a shape enhancement; (d) The transparent structures are more distinguishable after a transparency adjustment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Optimization of Rendering Parameters Structural Opacity Histogram Structural Composition in DVRI Preprocessing of Volume and Image Metrics Transparency Adjustment on Color Visibility Equalization on Opacity Original DVRI Enhanced DVRI Dataset + Segments Shape Enhancement on Lighting Structural Variation in DVRI</head><label></label><figDesc></figDesc><table><row><cell>Fig. 1. Flow chart showing the optimization pipeline.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Visibility Equalization A B Ray 3D View 2D View Ray Ray Structural Visibility Deviation Histogram</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Structural Opacity (Importance) Histogram</cell><cell>Structural Visibility A B Deviation Histogram</cell></row><row><cell cols="2">Ray Profile</cell><cell>Opacity Visibility</cell></row><row><cell>A</cell><cell>B</cell><cell></cell><cell>A B</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The research is partially supported by grant HK RGC CERG 618706 and 973 program of China (2010CB732504) and NSF China (No. 60873123). We thank the reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Texturing of layered surfaces for optimal viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1125" to="1132" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relationaware volume exploration pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1683" to="1690" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Size-based transfer functions: A new volume exploration technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1380" to="1387" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visibility-driven transfer functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific Visualization Symposium</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-automatic generation of transfer functions for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Durkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Volume Visualization and Graphics</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image-based transfer function design for data exploration in volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Biddlecome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tuceryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-level image cues in the perception of translucent materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="382" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Specular reflections and the perception of shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="798" to="820" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transparent layer constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gerbino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stultiens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Troost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Weert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A non-photorealistic lighting model for automatic technical illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="447" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The derivation of 3-d surface shape from shadows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hatzitheodorou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Image Understanding Workshop</title>
		<meeting>Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="1012" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conveying the 3d shape of smoothly curving transparent surfaces via texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Interrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Curvaturebased transfer functions for direct volume rendering: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multidimensional transfer functions for interactive volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="285" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Light collages: Lighting design for effective visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kinetic visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stompel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Methods for non-linear least squares problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tingleff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Balanced and unbalanced, complete and partial transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Pos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavedon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception and Psychophysics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="354" to="366" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Owada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<title level="m">Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The transfer function bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="22" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic layers for illustrative volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rautek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1336" to="1343" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Opacity peeling for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="597" to="606" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Volume illustration: nonphotorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-level user interfaces for transfer function design with semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1021" to="1028" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards a perceptual theory of transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="519" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting perceived transparency in textured displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="277" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human perception of structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Treue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Color design for illustrative visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Giesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zolliker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1739" to="1754" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interactive transfer function design based on editing direct volume rendered images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1027" to="1040" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
