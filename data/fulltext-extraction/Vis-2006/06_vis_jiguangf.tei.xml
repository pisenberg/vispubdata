<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic View Selection for Time-Varying Volumes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangfeng</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Wei</forename><surname>Shen</surname></persName>
						</author>
						<title level="a" type="main">Dynamic View Selection for Time-Varying Volumes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Static view selection</term>
					<term>image based method</term>
					<term>dynamic view selection</term>
					<term>information entropy</term>
					<term>optimization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Animation is an effective way to show how time-varying phenomena evolve over time. A key issue of generating a good animation is to select ideal views through which the user can perceive the maximum amount of information from the time-varying dataset. In this paper, we first propose an improved view selection method for static data. The method measures the quality of a static view by analyzing the opacity, color and curvature distributions of the corresponding volume rendering images from the given view. Our view selection metric prefers an even opacity distribution with a larger projection area, a larger area of salient features&apos; colors with an even distribution among the salient features, and more perceived curvatures. We use this static view selection method and a dynamic programming approach to select time-varying views. The time-varying view selection maximizes the information perceived from the time-varying dataset based on the constraints that the time-varying view should show smooth changes of direction and near-constant speed. We also introduce a method that allows the user to generate a smooth transition between any two views in a given time step, with the perceived information maximized as well. By combining the static and dynamic view selection methods, the users are able to generate a time-varying view that shows the maximum amount of information from a time-varying data set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visualization of time-varying data has been a challenging problem due to the large size and the time varying nature of the underlying datasets. Previously, researchers have proposed various techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18</ref>] to allow for a better understanding of the time-dependent features and their evolutions through high dimensional projection, feature tracking, and illustration. However, the most general and commonly used method for visualizing time-varying data is still animation, which is created by rendering each static volume data in the time sequence. One problem for producing animations for timevarying data is that the features of interest often evolve over time, with their shapes, positions, and orientations changing continuously. To provide the user with the best visualization of those features in an animation, it is very important to select dynamic views that can follow those features so that a maximum amount of information throughout the time sequence can be perceived. As a time-varying dataset is usually large in size and time-consuming to render, selecting views by hand can be a daunting task if it is simply done by trial-and-error. To ensure that the large scale time-varying dataset can be explored in an efficient and effective way, the process of view selection should be done automatically as much as possible.</p><p>In the context of data visualization, researchers have considered ways to automate the process of view selection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. However, their focuses had not been on time-varying data, which requires special treatments in order to maximize the amount of information embedded in the whole time sequence. In addition, certain important factors when selecting a good view for static data such as the perceived colors, curvatures, and opacities in the final image were not considered in their algorithms. In this paper, we first present an improved static view selection technique to address some issues that were not previously considered, and then use the new static view selection method and a dynamic-programming optimization approach to find the best time-varying view. The goal of identifying the optimal time-varying view is to maximize the amount of information the user can perceive from the rendering sequence, with constraints on movement of the views to ensure a smooth viewing path. Our static view method measures the quality of a view based on the opacity, color and curvature</p><p>• Guangfeng Ji is with The Ohio State University, E-mail: ji. <ref type="bibr" target="#b14">15</ref> images generated by a volume rendering technique. The contribution of the paper is as follows:</p><p>• An optimization approach that finds the best time-varying view in a polynomial time within a search space of exponential size. The approach also takes into account the constraints of the movement of the views.</p><p>• We properly design the probability function for the opacity distribution and incorporate it into the opacity entropy evaluation. Our opacity entropy prefers an image with a large projection area with an even opacity distribution. This technique avoids some problems that can be encountered in <ref type="bibr" target="#b3">[4]</ref>.</p><p>• The color transfer function conveys important information for volume rendering. We explicitly take into account the color information by properly designing a probability function and incorporating it into the color entropy evaluation.</p><p>• The curvature of the dataset contains essential geometric information about the dataset. We explicitly take the curvature into account during the static view selection.</p><p>In this paper, we assume that all the view points are located on the surface of a viewing sphere. At each view point the user looks at the center of the sphere, where the volume is located. During view selection, the view moves on the sphere, which means the distance between the view and the volume center is fixed. We also assume the viewing and projection parameters are appropriately set up so that the projection of the volume from any view will not fall outside the window.</p><p>The organization of the paper is as follows. In section 2, we discuss the related work. In section 3, we introduce our static view selection method, which includes the evaluation of opacity entropy, color entropy and information from curvatures. We also discuss how to incorporate all the three factors into a utility function. In section 4, we introduce our optimization method to perform time-varying view selection in a polynomial time from an exponential-size search space. We also give a method to select a dynamic path between any two views which also maximizes the perceived information. In section 5, we present results to prove the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The study of view point evaluation can be dated back to 1976, when Koenderink and van Doorn <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> introduced the idea of aspect graph to partition the viewing regions surrounding an object. The node of the aspect graph is a stable view, around which the topology of the object projection does not change within a small region. The edge of the aspect graph represents a transition from a stable view to an adjacent one. The aspect graph defines the minimum number of views required to represent all the topologically different projections of the object. After its introduction, aspect graph has been studied intensively in computer vision, where many researchers used aspect graph for object recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>In computer graphics, several methods have been proposed to locate the optimal views for polygonal meshes. Kamada and Kawai <ref type="bibr" target="#b11">[12]</ref> defined a view to be optimal if it minimizes the number of degenerated faces under orthogonal projection. <ref type="bibr">Barral et al. [3]</ref> extended the idea to cope with perspective projection. In <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, <ref type="bibr">Vazquez et al.utilized</ref> the concept of information entropy from Information Theory <ref type="bibr" target="#b18">[19]</ref> to evaluate the quality of a viewpoint. The relative visibility of each face is defined as its probability, and the optimal view is found by maximizing the probability distribution using the entropy function. Vazquez et al. <ref type="bibr" target="#b23">[24]</ref> also introduced techniques to accelerate the viewpoint entropy calculation for molecular models based on different OpenGL features. Recently, Takahashi et al. <ref type="bibr" target="#b21">[22]</ref> discussed view selection in the context of volume visualization. They decompose the volume into a set of feature interval volume components, and use the surface-based view point selection method suggested in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> to find the optimal view for each of the components. Then they calculate the globally optimal view by a compromise between the locally optimal views of all the feature components. In <ref type="bibr" target="#b3">[4]</ref>, Bordoloi and Shen took a volume rendering approach and proposed that in a good view point, the visibility of a voxel should be proportional to the noteworthiness value of the voxel. The noteworthiness value, or the weight of the voxel can be determined by factors such as the opacity and color of the voxel. They also discussed view similarity and how to partition the view space.</p><p>There is a rich literature in computer graphics and animation about dynamic view selection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref>. Applicable techniques range from direct orientation interpolation <ref type="bibr" target="#b19">[20]</ref> to complex view planning for complicated 3D scenes. Andujar et al. <ref type="bibr" target="#b0">[1]</ref> proposed a camera path planning method for walkthrough of complex scene models. Their method is based on identifying the free-space structure of the scene and an entropy-based measurement of the relevance of a viewpoint. Wernert and Hanson <ref type="bibr" target="#b28">[29]</ref> discussed the camera path planning based on a personal "guide" that keeps the user oriented in the navigation space which also points to interesting subject area. Barral et al. <ref type="bibr" target="#b2">[3]</ref> presented a method for automatic exploration of static scenes. In their method, the quality of a view is computed by defining a new importance function that depends on the visible pixels of each polygon. Hong et al. <ref type="bibr" target="#b8">[9]</ref> studied how to select camera path to navigate in the human colon. van Wijk and Nuij <ref type="bibr" target="#b22">[23]</ref> introduced an elegant method to generate a smooth animation from one view to the other by zooming and panning. There are two major differences between our work and the previous work. First we deal with the problem of view selection for time-varying data where the underlying phenomena are changing over time. Second our problem involves a different scene setting from the previous work, where our views move on a viewing sphere and look at the center of the sphere. Our goal is to maximize the information perceived from the time-varying data while following the view movement constraints. In <ref type="bibr" target="#b3">[4]</ref>, Bordoloi and Shen considered the problem of finding a good viewpoint for time-varying dataset. However, their method is to find a static view point throughout the animation so that the user can perceive the maximum summation of conditional entropy from the time series. The conditional entropy is the relative entropy of a datastep based on its previous step. Compared with the method, our method tries to find a dynamic viewing path.</p><p>It is also worth mentioning that information entropy has been utilized in lighting design and shape analysis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17]</ref>. Gumhold <ref type="bibr" target="#b7">[8]</ref> designed lighting for static scenes and placed light sources at locations where the illumination information is maximized. The illumination information is measured by the entropy function, which is calculated based on the pixel brightness values. Based on the user perception study, Gumhold further refined the illumination entropy definition by perceptually binning the brightness values and incorporating an importance weight based on surface curvature measured in the image.</p><p>Vazquez et al. <ref type="bibr" target="#b26">[27]</ref> improved the method of Gumhold by defining the information entropy over regions with similar colors measured in the CIELUV color space, rather than only based on the brightness values. In <ref type="bibr" target="#b16">[17]</ref>, Page et al. measured the shape complexity for 2D image contours and 3D triangle meshes by a shape information metric, and they proposed an algorithm to compute the metric based on curvature estimates for both discrete curves and surfaces. The essential problem any view selection technique tries to solve is to find a good view point through which the users are able to perceive the maximum amount of information from the underlying scene. In the context of volume visualization, Takahashi et al. <ref type="bibr" target="#b21">[22]</ref> proposed a surface-based view point optimization algorithm where the geometric properties of interval volumes faces are considered. Their method produces good static views for data that can be decomposed into different interval volumes. Bordoloi and Shen <ref type="bibr" target="#b3">[4]</ref> took a direct volume rendering approach without the need of intermediate geometry. Their method generates good views in general with the exception of some cases. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, there are two voxels in the scene, one with a weight of 0.7 and the other 0.3. Since their method prefers views from which the visibility of the voxel is proportional to its weight, the voxel with weight 0.7 has to occlude the other voxel to some degree in order to achieve a higher score for their entropy formula. However, these two voxels are readily visible through some views such as V 1 . From this example, we can see that if the visibility of a voxel can be maximized, it does not have to be proportional to its weight. To remedy this problem and consider additional important properties of the data, we propose an image-based view selection method. Our method measures the quality of a static view not only based on its opacity and projection size (which is the primary criterion of some of the previous algorithms), but also explicitly considers the color and curvature distribution of the rendered images. Our motivation comes from the fact that color and curvature convey very important information about the underlying phenomenon in many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STATIC VIEW SELECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Measurement of Opacity Distribution and Projection Size</head><p>Imagine a user is visualizing a volumetric dataset using a volume rendering technique. Some voxels in the volume have higher opacity values, meaning these voxels are more important. Less important voxels are assigned with smaller opacities. Initially the user may choose a view through which many opaque voxels are aligned in the viewing direction and hence more occlusion occurs. In this case, some pixels in the final image will have very high opacity values, while the opacity values at other pixels are low. The user realizes that this is not a good view, so s/he changes to a view where less occlusion occurs in the volume, so that the user can see many voxels more clearly. In this case, the opacity value in the image will be more evenly distributed. Besides this, the user may also generally prefer a rendering image with a larger projection area. From this example, it can be seen that an important factor that contributes to the selection of good views is the distribution of opacity values and the size of the projection area in the resulting image. An image with an even opacity distribution and a large projection area should be more favorable than one with an uneven opacity distribution and/or a small projection area. A function is desired to reflect the property. The Shannon entropy function <ref type="bibr" target="#b18">[19]</ref> can be utilized to perform the measurement. In Information Theory, the Shannon entropy function is used to measure the amount of information contained in a random sequence of symbols. Suppose the symbols occur in the set {a 0 , a 1 , ..., a n−1 } with the occurrence probability {p 0 , p 1 , ..., p n−1 }, the average information of the sequence, called entropy, is defined as</p><formula xml:id="formula_0">H(x) = − n−1 ∑ i=0 p i • log 2 (p i )<label>(1)</label></formula><p>One nice property of the entropy function is that it is a concave function. It only has one local maximum value, which is also the global maximum value. It reaches this maximum value log 2 n when p 0 = p 1 = ... = p n−1 = 1/n, that is, the distribution of the probability is perfectly even among all the symbols. As the probability moves away from the perfectly even distribution along a straight line in any direction, the probability becomes less and less evenly distributed, and the value of the entropy function will also decrease.</p><p>The Shannon entropy function can be utilized to measure the information contained in an opacity image. We now explain how the probability is designed so that the entropy function gives a higher value when the opacity value is more evenly distributed and the projection area is larger, while it gives lower values otherwise. Given an opacity image which contains n pixels with opacity value {α 0 , α 1 , ..., α n−1 }, we define the probability p i of the ith pixel as</p><formula xml:id="formula_1">p i = α i ∑ n−1 j=0 α j (2)</formula><p>The image entropy is calculated by equation 1. Although the entropy is evaluated over all the image pixels, the background pixels actually do not contribute to the entropy. The reason is that the opacity value of any background pixel is 0, so it will not affect the probability and entropy contribution of any foreground pixel. Furthermore, since 0 • log 2 0 is defined as 0, background pixels will not contribute to the final entropy value of the whole image. Therefore, we can define the image entropy just over the foreground area. The image entropy gets the maximum value when all the foreground pixels occur in the same probability, that is, all the foreground pixels have the same opacity values.</p><p>The entropy function also takes into account the size of the projection area, which is the foreground of the image. The reason is that the maximum entropy value of an image is log 2 f , where f is the size of the foreground. Therefore, the entropy of an image with a large foreground area and even distribution gets a higher value than one with smaller foreground areas. In summary, our opacity entropy function prefers an image with a large projection area with an even opacity distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Measurement of Color Distribution</head><p>Opacity is just one factor that influences the selection of good views. Another important factor that determines the quality of a view is color. In volume rendering, colors are often assigned to voxels by using a color transfer function. A well-designed color transfer function should highlight salient features by using perceptually attentive colors, and map unimportant voxels to some less attentive colors. The measurement of a view's quality should keep the fidelity of the color transfer function. This means that in the color-mapped volume, even though some colors (the less attentive colors assigned to unimportant voxels, for example) may occur more frequently than some other colors (attentive colors assigned to salient features, for example), the less frequently salient feature colors actually carry more information. Therefore a good volume rendering image should contain more of these colors and thus more information about the salient features. Furthermore, we always want to highlight as many salient features as possible in the limited screen area. If the volume contains multiple salient features, these features should be mapped to the final images equally, i.e, the projected areas for different colors should be as even as possible among all the salient features. Based on the analysis, it can be seen that a good view should maximize the area of the salient colors while maintaining an even distribution among these colors.</p><p>To measure the color distribution of the volume rendering image, we also utilize the Shannon entropy function. The entropy function and the probability evaluation should be designed so that the entropy function gives a higher value for an image with more evenly distributed and larger areas of salient colors, while giving lower values for images with less evenly distributed and/or smaller areas of salient colors. Suppose there are n colors {C 0 ,C 1 , ...C n−1 }, where C 1 ,C 2 , ...C n−1 occurs in the color transfer function and C 0 is the background color (actually C 0 can be a spectrum of colors, which includes every pixel of the image which is not perceptually similar to any of C 1 ,C 2 , ...C n−1 ). Given any pixel in the rendered image, we can determine which feature it belongs to by measuring the perceptual color distance between the pixel color and the feature color. If it does not belong to any feature (either the feature it should belong to is highly occluded, or it comes from unimportant voxels), it will be assigned to C 0 . Please note that a perception-based color space should be used during the process. We choose the CIELUV color model <ref type="bibr" target="#b5">[6]</ref> since it provides a perceptually equal color space, i.e., the distance in CIELUV space reflects the perceptual color difference. Suppose the total window area is T and the color areas of</p><formula xml:id="formula_2">C 1 ,C 2 , ...C n−1 are A 1 , A 2 , ...A n−1 respectively. The area for C 0 is then A 0 = T − ∑ n−1 i=1 (A i ).</formula><p>The probability is defined as</p><formula xml:id="formula_3">p i = A i T (3)</formula><p>It is a probability definition since T = ∑ n−1 i=0 (A i ). The color entropy function is defined as in equation 1. We can see that the entropy reaches its maximum value when A 0 = A 1 = ... = A n−1 , that is, all the color areas are even. Due to the inclusion of A 0 , large background area will incur small total salient color area, and thus uneven probability distribution and small entropy value accordingly. Therefore, the entropy function and our probability definition prefer larger total salient color area and more even distribution among all salient colors. It should be noted that the probability definition can lead to a small undesired effect. This happens when we see each of the salient colors and the background with the same area, which reaches the maximum of the entropy. The entropy will get smaller if the area of salient colors is enlarged, and this is undesired. However, this is less likely to happen in practice since the background area for any given view is usually large enough so that the volume rendering images from all the views can be projected into the window. We can also intentionally increase the window size to avoid the problem. Furthermore, even if the error occurs, it can be as large as log(n) − log(n − 1), which is a negligible number for a relatively large n. A similar approach has been used in <ref type="bibr" target="#b24">[25]</ref> to deal with the background issue.</p><p>It is also noteworthy to mention that we choose a lighting model which involves only ambient and diffuse lighting calculation. Specular lighting is not included since it can alter the color of pixel by the color of the light. The color entropy evaluation works well for a well-designed color transfer function where colors are used to highlight different features (for example, colors are used to depict different components in a segmented volume). If a color transfer function just simply assigns gray-scale or rainbow colors according to different values, the color entropy may not reflect the feature information contained in the view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Measurement of Curvature Information</head><p>Opacity and color are two important factors that measure the quality of a view. In addition to opacity and color, there are other properties that also contribute to the information provided in a volume rendering image. One of such properties is the curvature. Previously, Lee et al. <ref type="bibr" target="#b15">[16]</ref> utilized curvature to identify the mesh importance information. They introduced the idea of mesh saliency as a measure of regional importance for graphics meshes, and the mesh saliency is defined in a scale-dependent manner using a center-surround operator on Gaussian-weighted mean curvatures. In our method, we notice that low curvatures imply flat areas and high curvatures mean highly irregular surfaces, which often contain more information (If the volume is noisy, a smoothing operation should be performed beforehand). Therefore, it is important to take the curvature information into account during the selection of good views.</p><p>One problem of considering curvature information in view selection is how to present the curvatures in a volume rendering image. We achieve this with two steps. First we calculate the curvature at each voxel position of the volume, using the method proposed by Kindlmann et al. <ref type="bibr" target="#b12">[13]</ref>. When the volume is rendered, the color of a voxel is determined by its curvature. Voxels with high curvature are assigned with high intensity colors, while voxels below a certain low-curvature threshold are assigned with the color <ref type="figure">(0, 0, 0)</ref>. The opacity of the voxel is determined independently, which can be based on its original data value, or some other properties such as the gradient. After the rendering is performed and the image is generated, the intensity of the image reflects the amount of curvature perceived from the visible part of the volume, that is, an image with high intensity means that the user can see many high-curvature voxels from that view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Final Utility Function</head><p>Opacity, color and curvature all contribute to the information perceived from a rendering of the volume. We need a function to incorporate all the factors. This utility function <ref type="bibr" target="#b27">[28]</ref> u from a view v should have the following basic form:</p><formula xml:id="formula_4">u(v) = α • opacity(v) + β • color(v) + γ • curvature(v)<label>(4)</label></formula><p>where α + β + γ = 1. One problem with the utility function is that the opacity, color and curvature contributions are not normalized. We should normalize each of the factors into [0, 1] before the summation. The maximum value of the entropy function of an image with a projection size of n is log 2 n. So if we find the maximum projection size M of the images among all the views, each of the entropies can be normalized by dividing over log 2 M. The maximum value of the color entropy is log 2 n, where n is the number of colors (see section 3.2). Therefore, the color entropy can be easily normalized by a division over log 2 n. The normalization of the curvature contribution can also be easily done by a division over the maximum projection size M, since the maximum intensity of each pixel is 1. If we possess any prior knowledge of the volume, it is often desirable to give different weights to different factors. One scenario is that people often design very sophisticated opacity transfer function, but use a simple gray-scale or rainbow color transfer function. In this case, it is desirable to put more weight into opacity(v) than color(v), since opacity conveys more information. However, in another case where different colors are used to highlight different features in a segmented volume, it is desirable to put large weight to color(v). In practice, we can choose proper weight for every factor based on the characteristic of the data and transfer function and the nature of the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DYNAMIC VIEW SELECTION</head><p>In this section, a dynamic view selection algorithm is presented. The goal of dynamic view selection is to allow the user to find a viewing path which shows the maximum amount of information from the time-varying dataset, and the path should show near-constant angular velocity (all the views lie on the surface of a viewing sphere). We formulate this into the following three principles that a good dynamic viewing path should follow:</p><p>• The view should move at a near-constant speed.</p><p>• The view should not change its direction abruptly.</p><p>• The information perceived from the time-varying data should be maximized among all the viewing paths.</p><p>In the following subsections, we first discuss the issue of how to select time-varying views that follow the three principles. Then we present a method that allows the user to find a path between any two views in a given timestep that maximizes the perceived information while obeying the other two principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Time-Varying View Selection</head><p>The problem of time-varying view selection is that given a view at t = 0, among all the possible paths along which the view can move smoothly to the final timestep at a near-constant angular velocity, find the path that gives the maximum perceived information. If in average a view can move to one of n possible views at the next timestep, and there are total t timesteps, the complexity of the problem can be n t . This search space is exponentially large. It is impractical to try all these paths and find the optimal one.</p><p>To solve the problem more efficiently, we can employ the dynamic programming approach. Let's first consider selecting time-varying views with the first and third principles in mind, that is, we want to find a time-varying view that moves at a near-constant speed, and the information perceived from that path is maximized out of all possible paths. Suppose the camera is moving with speed V , with V min ≤ V ≤ V max . V min and V max are used to bound the speed of the view so that when V min is close to and V max , the view moves at a near-constant speed. We use P i, j to denote the position of the jth view at t = i, and MaxIn f o(P i, j ) is the maximum amount of information perceived from P i, j to some view at the final timestep. The following recursive function holds:</p><formula xml:id="formula_5">MaxIn f o(P i, j ) = max Numo fViews−1 k=0 {u(P i, j ) −Cost(P i, j , P i+1,k ) +MaxIn f o(P i+1,k )}</formula><p>where u(P i, j ) measures the information perceived at the view P i, j . Cost(P i, j , P i+1,k ) measures the cost to move from P i, j to P i+1,k . If the jth view point and the kth view are within [V min ,V max ], the cost is 0, otherwise the cost is +∞. The equation basically says the maximum amount of information perceived from P i, j to some view point at the final timestep will be equal to the sum of the information perceived at P i, j , and the maximum information perceived from P i+1,k to some view at the final time step. P i+1,k represents a view point at t = i + 1 that can be reached within [V min ,V max ] distance from P i, j . We will consider all the views P i+1,k at timestep i + 1. The following C-style code performs the calculation of all the MaxIn f o(P i, j ). </p><formula xml:id="formula_6">) = u(P n−1,i ) for i ∈ [0..Numo fViews − 1]</formula><p>. The dynamic programming process calculates all the MaxIn f o{P i, j } backwards in time, according to the recursive function. NextNodeIndex{P i, j } records the view index at the next timestep that gives the maximum information from P i, j to some view at the final timestep, and it can be used to recover the time-varying path. The dynamic programming process finishes all the computation in O(n • v 2 ) time, where n is the number of total timesteps, and v is the number of total views. This process only takes a polynomial time complexity.</p><p>The above dynamic programming calculates an optimal path based on the restriction that the view should move with the speed within [</p><formula xml:id="formula_7">V min ,V max ].</formula><p>But it does not prohibit the view from making sharp turns, which is undesirable when viewing the animation. It is also impossible to use the information stored at NextViewIndex to find the optimal path that does not make sharp turns, since NextViewIndex only records the optimal paths that move at a near-constant speed.</p><p>To address this problem, at each view point on the viewing sphere, we partition its local tangent plane into many different regions, and restrict the allowed turns. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates a partition of eight regions and a matrix that encodes the allowed turns. We use MaxIn f o(P i, j,r ) to denote the maximum amount of information perceived from P i, j to some view point at the final timestep, and P i, j was entered from region r from its previous view. Then the following recursive function holds:</p><formula xml:id="formula_8">MaxIn f o(P i, j,r ) = max t=0..Numo f Regions−1,k∈Regiont {u(P i, j ) −Cost(P i, j , P i+1,k ) + MaxIn f o(P i+1,k,t )}</formula><p>The </p><formula xml:id="formula_9">[i, j]-Cost(j, k) +MaxInfo(i+1, k, t); if (Info&gt;MaxInfo[i, j, r]) { MaxInfo[i, j, r]=Info; NextViewIndex[i, j, r]=k; NextRegionIndex[i, j, r]=t; } } }</formula><p>where o is the region number leaving the jth view, and o can be easily determined based on the the projection to local tangent plane at the jth view. NextViewIndex and NextRegionIndex record the view and region index at the next timestep that offers the maximum information to some view at the final timestep. These two data structures can be used to recover the path. The dynamic programming process finishes all the computation in O(n • r • v 2 ) time, where n is the number of timesteps, v is the number of views, and r is the number of regions. This process only takes a polynomial time complexity. After the dynamic programming is done, given the initial view at t = 0, the results stored at MaxIn f o, NextViewIndex and NextRegionIndex can be used to find the maximum perceived information and the optimal time-varying view associated with the initial view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Viewing Path Between any Two Views in a Given</head><p>Timestep Another case of dynamic view selection is to find a viewing path between any two viewpoints in a given timestep. This viewing path should also follow the three principles, i.e., moves between these two viewpoints smoothly with a near-constant angular velocity, and maximizes the perceived data information at the same time. This technique can be very useful to showcase a static dataset. When generating an animation, keyframes are usually specified by the user, and intermediate frames are generated by interpolation. If different viewpoints are assigned in the different keyframes, spherical linear interpolation (SLERP) is a common technique to interpolate the intermediate view positions. SLERP does give a viewing path with constant angular velocity, but it does not take the perceived information into consideration. Next we will explain how we maximize the perceived information and take all three principles into consideration. Given any two views on a viewing sphere, there are an infinite number of paths that connect these two views. One factor in our design of the dynamic path is that it should follow the general direction of the SLERP path, since the SLERP path is the shortest path that connects the two points with constant angular velocity. Therefore, we only allow the view to move at the neighboring views of the SLERP path (as shown in <ref type="figure" target="#fig_2">Figure 3</ref>). We also need to put restriction on the direction of the allowed movement so that the view will not go back and forth in a circular manner. We achieve this by parameterizing all the neighbors relative to the SLERP path, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. A movement is allowed only if the u parameter is increasing and the v parameter difference is within a threshold. We call these paths monotonic paths. We can also enforce the direction change by adopting the local coordinates and the admissible turn matrix in <ref type="figure" target="#fig_1">Figure 2</ref>. When evaluating the quality of different paths, the summation of information should not be used, since some paths can go through more view points than others. One good criterion can be the average information. The pseudo code below illustrates how to use the propagation method similar to the single-source shortest path algorithm to find the optimal path. Find the one with the maximum average information and it will be the optimal path. } Notice the above process only runs on the neighborhood of the SLERP path. If the neighborhood vertices and edges among the vertices are stored in an adjacency matrix, the algorithm takes O(V 2 ) time. If the vertices and edges are stored in an adjacency list, the algorithm takes O(E +V logV ) time, where V is the number of vertices, and E is the number of edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head><p>We have implemented and tested both the static and dynamic view selection algorithms on a Pentium IV 1.4GHz machine with an nVidia GeForce 6800 graphics card. Our view selection algorithms take as input the opacity, color and curvature images rendered from the dataset, which can be generated by any volume rendering technique. In our implementation, we choose a hardware-based volume slicing technique with 3D texture mapping to generate those images. 256 sample views were used for each dataset, and these views are evenly distributed on the viewing sphere.</p><p>The test result for the 512 × 64 × 64 shockwave dataset is shown in <ref type="figure">Figure 4</ref>. The opacity entropy value is used during the test to show its effectiveness in determining view quality. <ref type="figure">Figure 4 (a)</ref> shows the worst view which has the smallest opacity entropy, and <ref type="figure">Figure 4 (b)</ref> shows the best view with the highest opacity entropy. <ref type="figure">Figures 4 (c)</ref> and (d) illustrate the opacity images of the worst and best views respectively. It took 6.92 seconds to compute the opacity entropy values for the 256 views and find the best and worst views, and the size of all the images is 256 × 256. By using entropy and the proposed probability function, our opacity entropy evaluation takes both the opacity distribution and the projection area into consideration, and the opacity entropy prefers an image with an even opacity distribution and a larger projection area. To illustrate how the opacity entropy varies according to different viewing angles, the view is rotated along the Y axis in a complete circle. <ref type="figure">Figure 4</ref> (e) plots the change of opacity entropy with respect to different views.</p><p>We also used the 128 × 128 × 80 tooth data to test the view selection algorithm based on the opacity entropy, and the result is shown in <ref type="figure">Figure 5</ref>. <ref type="figure">Figure 5 (a)</ref> shows the worst view with the smallest opacity entropy, and <ref type="figure">Figure 5 (b)</ref> shows the best view with the largest opacity entropy. <ref type="figure">Figures 5 (c)</ref> and (d) are their opacity images. It took 7.18 seconds to compute the opacity entropy values for the 256 views and find the best and worst views, and the size of all the images is 256 × 256. The variation of opacity entropy with respect to different views is also plotted in the <ref type="figure">Figure 5</ref> (e), where the viewing angle is rotated incrementally around the X axis.</p><p>We used the 128 3 vortex dataset to show the effectiveness of the color entropy function. The data set contains many components and we use the color transfer function to highlight components which may go through topological changes in future timesteps. Other components are assigned a gray-scale color. <ref type="figure">Figure 6 (a)</ref> shows the worst view with the smallest color entropy, and <ref type="figure">Figure 6 (b)</ref> shows the best view. It can been easily seen that <ref type="figure">Figure 6</ref> (b) conveys more information about the five topologically important features than <ref type="figure">Figure 6</ref> (a). In <ref type="figure">Figure 6</ref> (a), the total projection area of the five highlighted features is small, and the projection area ratio among the highlighted features is very uneven. This leads to a very small color entropy value. In contrast, in <ref type="figure">Figure 6</ref> (b), the five highlighted features have a large projection area and an even projection area distribution, and therefore a large value for the color entropy. It took 16.3 seconds to compute the color entropy values for the 256 views and find the best and worst views, and the size of the color images is 256 × 256. <ref type="figure">Figure 6</ref> (c) plots the change of color entropy with respect to different views where the viewing angle is rotated incrementally around the Y axis. <ref type="figure">Fig. 8</ref>. The figure shows two paths which move from one view point to the other. The right path is generated by SLERP interpolation with an average information of 0.51. The left path is generated by our method. The path is smooth and gives an average information of 0.56. <ref type="figure">Figure 7</ref> gives the view-selection result for the Terascale Supernova Initiative (TSI) dataset. The dataset modelled the core collapse of supernovae and was generated by collaboration among Oak Ridge National Lab and eight universities. In the paper, we visualize the entropy scalar component of the dataset, which is derived from pressure and density scalar values. When exploring the dataset, we used the rainbow color transfer function. Therefore, in our view selection test, color information is not considered. Two factors, curvature and opacity, are considered in the calculation of view information. We want to design a utility function which puts more weight for views that show more jagged area. In our design, we set the coefficients for curvature and opacity to 0.8 and 0.2 respectively. <ref type="figure">Figure 7 (a)</ref> shows the worst view, and <ref type="figure">Figure 7</ref> (b) is the best view. It is obvious that <ref type="figure">Figure 7</ref> (b) shows more detailed information about the jagged area than <ref type="figure">Figure 7 (a)</ref>. It took 18.7 seconds to evaluate the curvature information and opacity entropy for all the 256 views and find the best and worst views, and the size of the images is 256 × 256. To show how the view utility function varies, <ref type="figure">Figure 7</ref> (c) plots the change of utility value with respect to different views, where the view is rotated incrementally around the vertical (Y) axis.</p><p>We also used the TSI dataset to test our dynamic view selection algorithm. The supernova is a very dynamic phenomenon where the features are morphing and rotating rapidly in space. Our previous static view selection algorithm shows that at a given timestep, very little information about the phenomenon can be perceived if the volume is viewed from some bad views. If the view for an animation is fixed, much of the phenomenon would be occluded for many timesteps (see <ref type="figure" target="#fig_5">Figure 9</ref> (f)-(i)). Recall that the goal of our algorithm is to find a viewing path with the maximum amount of information, which also follows the constraint that the camera moves at a near-constant angular velocity. We used our static view selection to calculate the view information of every view point at every timestep and used our dynamic programming algorithm to find the best path. All the timesteps use the same view point set on the sphere. <ref type="figure" target="#fig_5">Figure 9</ref> (a) shows the best path in which viewpoint P 0,0 moves in time with the speed within (0.9, 1.2) (The radius of the viewing sphere is 1). Although the supernova phenomenon is morphing rapidly, we still perceive a maximum amount of information following our dynamic viewing path. It took 4.31 seconds for the dynamic programming process to find the optimal path. <ref type="figure" target="#fig_5">Figure 9</ref> (a) shows part of the path, which demonstrates near-constant angular velocity (the distance in <ref type="figure" target="#fig_5">Figure 9</ref> (a) is distorted). Furthermore, following the path, the overall information perceived from the timevarying data is maximized. <ref type="figure" target="#fig_5">Figures 9 (b)</ref>-(e) show four snapshots of the time-varying dataset captured by the time-varying view path, and Figures 9 (f)-(i) show the images seen from the original view at the timesteps corresponding to (b)-(e) respectively. The user can apparently see more turbulent side of the phenomenon all the time from the time-varying views.</p><p>We also used the TSI dataset to show a viewing path selected from any two views in a given timestep. The TSI dataset at t = 0 is used, and <ref type="figure">Figure 8</ref> shows both the SLERP and the optimized paths. It took 0.08 seconds to find the optimized path. The average information perceived by the SLERP path is 0.51, while the optimized path gives 0.56.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we present methods for both static and dynamic view selection. Our static view selection algorithm analyzes opacity, color and curvature images generated from different view points. We properly design the probability functions and use entropy to evaluate opacity and color distributions. Our algorithm also prefers a view which shows high curvature information. Depending on the characteristic of the data set and the opacity and color transfer function, and the nature of the application, we can design different utility functions to assign different weights to the three factors. Based on our static view selection and dynamic programming, our dynamic view selection method maximizes the information perceived from the time-varying dataset following a near-constant angular velocity path. The optimization is achieved in a polynomial time. Our results show the effectiveness of the static and dynamic view selection.</p><p>In addition to dynamic view point planning, another important parameter for animation would be lighting design. Gumhold <ref type="bibr" target="#b7">[8]</ref> discussed light source placement for static polygonal meshes. We would like to conduct the research for lighting design for time-varying polygonal and volumetric data in our future work. Ridge National Laboratory Contract 400045529. The TSI data set was provided by John M. Blondin (NCSU), Anthony Mezzacappa (ORNL), and Ross J. Toedte (ORNL). Thanks go to Kwan-Liu Ma for providing the the vortex data set made available through the NSF ITR project.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The figure to illustrate that the result from<ref type="bibr" target="#b3">[4]</ref> should be improved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An example of a partition of a view point's local tangent plane and one of the possible allowed turns encoded in matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The solid curve is the SLERP path. Our algorithm will consider all the neighbors of the SLERP path that lie within the dotted area. All the neighbors are parameterized by u and v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>, PathLength]=u(S); Initialize all the other PathInfos to a minimum value; NextActiveSet=empty; while(ActiveSet is not empty) { PathLength++; for each view V in ActiveSet for each neighbor N of V The figure shows the static view selection results based on opacity entropy for the shockwave dataset. (a) shows the worst view, (b) is the best view, and (c) and (d) are the opacity images for (a) and (b) respectively. (e) plots the change of opacity entropy with respect to different viewing angles where the shockwave is rotated around the Y axis in a full circle. The figure shows the static view selection results based on opacity entropy for the tooth dataset. (a) shows the worst view, (b) is the best view, and (c) and (d) are the opacity images for (a) and (b) respectively. (e) plots the change of opacity entropy with respect to the viewing angle when the tooth is rotated around the X axis in a full circle. if (the movement from V to N is monotonic) { PathInfo[N, PathLength]=max( PathInfo[N, PathLength], u(V)+PathInfo[V, PathLength-1]); Put N in NextActiveSet; } ActiveSet=NextActiveSet; } For all the PathInfo[D, n] where D is the destination {</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>The figure shows the static view selection results based on color entropy for the vortex dataset. (a) shows the worst view, (b) is the best view, and (c) plots the change of color entropy with respect to different viewing angles when the vortex is rotated around the Y axis in a full circle. The figure shows the dynamic view selection results for the TSI dataset. (a) shows the worst view, (b) is the best view, and (c) plots the change of the final information with respect to the viewing angle when the TSI dataset is rotated around the Y axis in a full circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>The figure shows the dynamic view selection result for the TSI dataset. (a) shows the path of the time-varying view, which exhibits constant angular velocity. (b)-(e) show four snapshots captured by our time-varying view. (h)-(i) show the image from the original static view at the timestep corresponding to (b)-(e) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>@osu.edu.</figDesc><table /><note>• Han-Wei Shen is with The Ohio State University, E-mail: hwshen@cse.ohio-state.edu. Manuscript received 31 March 2006; accepted 1 August 2006; posted online 6 November 2006. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by NSF ITR Grant ACI-0325934, NSF RI Grant CNS-0403342, DOE Early Career Principal Investigator Award DE-FG02-03ER25572, NSF Career Award CCF-0346883, and Oak</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Way-finder: Guided tours through complex walkthrough models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andujar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fairen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="488" to="508" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Viewpoint selection by navigation through entropy maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of International Conference on Computer Vision</title>
		<meeting>eeding of International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="248" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scene understanding techniques using a virtual camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dorme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Plemenos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Eurographics</title>
		<meeting>eeding of Eurographics</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">View selection for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">D</forename><surname>Bordoloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference 2005</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d object recognition using shape similarity-based aspect graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kimia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of International Conference on Computer Vision</title>
		<meeting>eeding of International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Color Appearance Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Planning multiple observation for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gremban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="137" to="172" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum entropy light source placement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Virtual voyage: Interactive navigation in the human colon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Volume tracking using higher dimensional isocontouring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Illustration-inspired techniques for visualizing time-varying data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference 2005</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="86" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple method for computing general position in displaying three-dimensional objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kawai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of International Conference on Computer Vision</title>
		<meeting>eeding of International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="248" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Curvature-based transfer functions for direct volume rendering: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The sigularities of the visual mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The internal representation of solid shape with respect to vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="211" to="216" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mesh saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shape analysis algorithm based on information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koschan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Image Processing</title>
		<meeting>eeding of the International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="229" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing features and tracking their evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samtaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zabusky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Animation with quaternion curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheomake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="245" to="254" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Volume tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference 1996</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A feature-driven approach to locating optimal viewpoints for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujishiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takeshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nishita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="495" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smooth and efficient zooming and panning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Nuij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realtime automatic selection of good molecular views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vaquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Llobet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="98" to="110" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Viewpoint selection using viewpoint entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision Modeling and Visualization Conference</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic view selection using viewpoint entropy and its application to image-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="689" to="700" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perception-based illumination information measurement and light source placement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ICCSA</title>
		<meeting>eeding of ICCSA</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Theory of Games and Economic Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Von</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morgenstern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1944" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A framework for assisted exploration with collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wernert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High dimensional direct rendering of time-varying voulmes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
