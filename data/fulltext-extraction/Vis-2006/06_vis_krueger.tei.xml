<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ClearView: An Interactive Context Preserving Hotspot Visualization Technique</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Krüger</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Schneider</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rüdiger</forename><surname>Westermann</surname></persName>
						</author>
						<title level="a" type="main">ClearView: An Interactive Context Preserving Hotspot Visualization Technique</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Volume rendered imagery often includes a barrage of 3D information like shape, appearance and topology of complex structures, and it thus quickly overwhelms the user. In particular, when focusing on a specific region a user cannot observe the relationship between various structures unless he has a mental picture of the entire data. In this paper we present ClearView , a GPU-based, interactive framework for texture-based volume ray-casting that allows users which do not have the visualization skills for this mental exercise to quickly obtain a picture of the data in a very intuitive and user-friendly way. ClearView is designed to enable the user to focus on particular areas in the data while preserving context information without visual clutter. ClearView does not require additional feature volumes as it derives any features in the data from image information only. A simple point-and-click interface enables the user to interactively highlight structures in the data. ClearView provides an easy to use interface to complex volumetric data as it only uses transparency in combination with a few specific shaders to convey focus and context information.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The necessity to provide additional context information when communicating the exact shape and position of inner organs was already discovered centuries ago by artists aiming for intuitive anatomic sketches. At that time anatomic drawings were entirely new. Without additional visual cues people would not have recognized what they were looking at. Over the centuries the paradigm of presenting both a detailed region and a surrounding context has not changed much. We still gain most information about unknown data not by seeing all information at once, but by looking at a carefully filtered fraction of the data set, usually referred to as the focus, embedded into some other aspect of the data, mostly conveying positional cues, called the context. Among the plethora of examples demonstrating the usefulness of man-made focus+context approaches are the anatomic sketches of DaVinci (see <ref type="figure" target="#fig_0">Figure 1</ref>) as well as the technical illustrations of our time ( <ref type="figure" target="#fig_2">Figure 3)</ref>.</p><p>The major insight behind all these examples has always been the same: Abstraction is the key to condense the information in the data set to a level that allows quick and intuitive understanding. Paradoxically, it turns out that the condensed result often reveals more information than a whole view of the data.</p><p>Today we face similar challenges as artists 500 years ago. Still we are trying to understand complex shapes and to visually communicate relevant features to help viewers relating these features to the entire data. This problem is further aggravated by the fact that the amount of information available today has sheerly exploded during the last decades. At the same time it has become possible to interactively visualize large and highly detailed volumetric data sets. However, volume rendered imagery often includes a barrage of 3D information like shape and appearance of complex structures, and it thus quickly overwhelms the viewer. In particular, when focusing on a specific region a viewer cannot understand the relationship between various structures, unless he has a mental picture of the entire data set by navigating around and observing the component parts. Many users of interactive volume rendering software do not have the required visualization skills for this mental exercise, and many others are not</p><p>• Jens Krüger, E-mail: jens.krueger@in.tum.de.</p><p>• Jens Schneider, E-mail: schneidj@in.tum.de.</p><p>• Rüdiger Westermann , E-mail: westermann@in.tum. <ref type="bibr">de</ref> For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. willing to invest the time and effort. The result is that a large portion of users have difficulties in understanding non-trivial data sets and in finding what they are looking for in those data sets. Consequently there is a dire need for novel techniques that can support users in this task which are intuitive and simple enough to be accepted in practice.</p><p>In this paper we present ClearView, an interactive and intuitive volume visualization tool that provides the user with a simple exploration metaphor. Following traditional technical illustrations, several 3D layers of volumetric data sets are extracted using texture-based raycasting. They are composed to produce high-quality images at high frame rates. The user guides the exploration process by moving the hotspot, a lens-like yet distortion free region, in which additional layers of the data set are augmented to convey relevant features. The proposed GPU system exploits feature-based techniques to improve the understanding of complex 3D data sets, and it utilizes image-based deferred shading to maximize performance. ClearView does not require any additional feature volumes, making the method suitable for the interactive rendering of high-resolution data sets on recent GPUs. Several shaders to intuitively convey material and shape properties are integrated into the system. To keep the user interface slim, only few parameters abstract from the technical realization. The user simply positions the hotspot on the data set and selects an amount of transparency to continuously blend between focus and context information. The ClearView system can be used to visually explore complex data sets at interactive frame rates by using a focus+context metaphor. Even the leftmost image of the 512 3 visible human data set with multiple iso-surfaces was generated at about 15 fps on a 800x600 viewport.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Psychological studies indicate that humans process information perceived as a single visual event intuitively, while the cognitive understanding of distinct events, such as multiple images, requires much greater effort <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. The focus+context paradigm seeks thus to combine multiple aspects of the data into a single visual event by assigning portions of one image to different aspects of the data. The general idea to emphasize certain aspects of the data in an intuitive way is also the basis of many non-photorealistic rendering (NPR) techniques <ref type="bibr" target="#b12">[13]</ref>. Consequently, NPR has been applied to both volume rendering <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref> and focus+context techniques in visualization <ref type="bibr" target="#b31">[32]</ref>. In the following we will shortly summarize previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distortion Lenses</head><p>Early work on automated focus+context originated from the need to visualize vast amounts of information using the limited space of 2D screens. Depending on the type of data to be displayed, different methods have been proposed. Shaw et al. <ref type="bibr" target="#b29">[30]</ref> used a lens metaphor to filter and visualize scattered, high-dimensional information. For multiple 2D Layers, Bier et al. <ref type="bibr" target="#b0">[1]</ref> suggested the Toolglass interface also supporting lenses. For single-layered 2D data, several techniques seeking to assign more screen space to important regions were suggested <ref type="bibr" target="#b21">[22]</ref>. The generalization to handle 3D data requires some effort in order to ensure an unobstructed line of sight to the focus <ref type="bibr" target="#b3">[4]</ref>. For volume rendering however, even if the focus is clearly visible, non-linear distortions in combination with fuzzy, semi-transparent structures are potentially counter-intuitive. Nevertheless, distortion lenses have been applied to 3D volume rendering successfully <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref>. Some unresolved issues remain, though. For instance, it is not yet clear if a transition region <ref type="bibr" target="#b33">[34]</ref> around the lens helps the user to understand the data set, since it deforms the context <ref type="bibr" target="#b14">[15]</ref>. In medical applications the distortions associated with lenses might only be acceptable in selected cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cutaway Illustrations</head><p>In cutaway illustrations a selective view on important details in the interior of an object is provided by omitting extraneous details. Conceptually these illustrations cut away parts of the outer hull using simple geometries such as clip planes or simple, convex objects. Similar to 3D lenses, special care has to be taken in order to provide an unobstructed view through the cut onto the inner structures, making image based methods appealing. For polygonal models, the GPU-based methods by Diepstraten et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> are promising, but were not generalized to volume data. Weiskopf et al. <ref type="bibr" target="#b35">[36]</ref> performed GPU-accelerated clipping for volumes with arbitrary clip geometry. McGruffin et al. <ref type="bibr" target="#b26">[27]</ref> proposed an interactive system to browse pre-classified iso-surfaces in volume data by deforming them according to simple and intuitive metaphors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multiresolution focus+context</head><p>Naturally the user is most interested in the focus region, while the context is needed only to provide positional cures. Consequently, various multi-resolution techniques have been proposed not only gain a speedup by reducing the resolution in the context region, but also to point out a particular feature in still images. Levoy et al. <ref type="bibr" target="#b22">[23]</ref> coupled an early eye tracking device with a volume renderer to select the focus. Cignoni et al. <ref type="bibr" target="#b4">[5]</ref> used a 3D MagicSphere to define the focus for triangle models and to guide appropriate remeshing. For the fo-cus+context reconstruction of iso-surface, a similar concept was later used by Westermann et al. <ref type="bibr" target="#b36">[37]</ref>. Lee et al. proposed to find an optimum view based on saliency <ref type="bibr" target="#b20">[21]</ref>. Weiler et al. <ref type="bibr" target="#b34">[35]</ref> implicitly used a focus+context technique by gradually decreasing the resolution of volumetric data with increasing distance to the camera. Lately, Ropinski et al. <ref type="bibr" target="#b28">[29]</ref> applied focus+context techniques to select shaders for seismic data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Context-Preserving Volume Rendering</head><p>Recently, several authors have recognized the need for volume visualization to perform abstaction beyond the established clip-plane rendering. Interrante et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref> suggested to augment semitransparent iso-surfaces using curvature-directed strokes and 3D Line Integral Convolution, which provides the user with intuitive cues about the shape of these surfaces. Viola et al. <ref type="bibr" target="#b32">[33]</ref> suggested importance driven volume rendering to highlight interesting structures in volume data. Starting with a pre-segmented volume, a semantic importance value is applied to the segments, which affects the final image. Bruckner et al. <ref type="bibr" target="#b2">[3]</ref> suggested a whole toolbox of automatic illustration methods to efficiently provide the user with insights about volumetric data. Bruckner et al. <ref type="bibr" target="#b1">[2]</ref> also propose context preserving volume rendering, a fully automated illustration technique trying to detect interesting structures in volume data using a sophisticated, high dimensional, data-dependent transfer function. Most of these approaches provide the user only with indirect control of the screen-space location of the focus by global and/or data-dependent parameters.</p><p>The remainder of this paper is organized as follows. In Section 3 we provide an overview of our approach. Then, in Section 3.1 we describe the procedure of extracting 3D focus and context layers, including positional and normal information. The compositing pass described in Section 3.2 blends these layers together based on user-defined parameters. This section also adresses the various rendering modes and shaders available in ClearView. Section 4 presents results and timings to demonstrate the interactivity of our approach. Finally, we conclude and discuss some directions for future research in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLEARVIEW</head><p>ClearView is designed with respect to the user's needs by providing an interactive and intuitive means for focus+context-based exploration of large volume data sets. One of the key requirements is to instantly enable a less experienced user to explore complex volumetric data sets without extensive training and preliminary knowledge of the data. Therefore we have based our visualization modes on established techniques, often used in man-made illustrations (see <ref type="figure" target="#fig_2">Figure 3</ref>). Semantically such illustrations are composed of two segments: the focus region -in case of the camera in <ref type="figure" target="#fig_2">Figure 3</ref> this is the digital circuitand the context region -the camera body in this image. In the images shown, the context region is faded out using transparency to reveal the inner focus region. At the same time characteristic parts of the context are visualized to accommodate better understanding of spatial relationships between structures in the data. From these examples the following building blocks for generating technical focus+context illustrations can be deduced: • Context: the images contain one or more semi-transparent context layers.</p><p>• Focus: the images contain one or more opaque focus layers.</p><p>• Shading: important features in the context layers are emphasized to indicate the spatial context into which the focus region is embedded.</p><p>• Compositing: the transparency of non-important regions in the context layers is increased to reveal the opaque focus layers.</p><p>In the following section we explain the techniques developed in ClearView to generate focus+context-based volume rendered imagery taking into account the aforementioned paradigms. We assume that a segmented volume data set, or the data set and a set of iso-values is given. Each context layer either consists of an iso-surface corresponding to a user-defined iso-value or a user-selected segment, or it consists of all structures in front of the focus layer that is closest to the viewer. In this way not only surfaces, but also volumetric structures exhibiting a certain thickness can be displayed to provide context. Two examples demonstrating the different types of context layers supported by ClearView are shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>The information in every layer is generated using GPU-based volume ray-casting <ref type="bibr" target="#b18">[19]</ref>, where the user selects the material properties of each layer. Since we are only interested in rendering iso-surfaces (or a relatively thin layer of structures in front of an iso-surface), ray-casting can benefit extremely from early-ray termination implemented via the early-z test on recent GPUs.</p><p>The entire image is generated in two stages. In the first stage the context and focus layers are rendered, while in the second stage these layers are composed into the final image. During the compositing stage image-based, deferred shaders are applied to each layer to enhance features or to suppress non-relevant structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context and focus extraction</head><p>To render a context layer we distinguish between surface and volume layers. If the user selects an iso-value greater than zero the iso-surface corresponding to this value is rendered. Only the object space position of the first ray-surface intersection with this iso-surface is kept and stored in a floating point render target, i.e. in a geometry image. If the iso-value selected is equal to zero, all matter up to the first context or focus surface is accumulated using alpha-compositing according to the user-defined transfer function. Instead of point coordinates, accumulated color values are now stored in the render target.</p><p>After all context layers have been rendered, as many textures as layers have been generated and stored on the GPU. In the upcoming rendering pass ClearView generates a normal for every texel in a surface layer. This is done by using the coordinates stored in the respective texture map as texture coordinates into the volume texture. At this position the volumetric gradient is approximated by central differences along the object coordinate axes. Note that this pass -although eight fetches are performed -is still very efficient as it can effectively take advantage of texture caches on the GPU. In a final pass, for every texel in a surface layer ClearView computes a curvature measure similar to the umbrella operator on discrete triangular meshes <ref type="bibr" target="#b17">[18]</ref>. At each texel the summed distances between neighboring surface normals are computed. This results in values close to zero in regions of low normal variation and large values otherwise (see <ref type="figure" target="#fig_4">Figure 5</ref>). Since this operation is performed in image space, at surface silhouettes pixels containing no normal information can be considered in the curvature estimate. However, as all render targets are initially set to zero a very high curvature value is computed. This feature will later be used to highlight object silhouettes in the context layers.</p><p>The rendering pipeline for context surface layers is illustrated in <ref type="figure">Figure 6</ref>. If more than one context surface is selected by the user the algorithm is invoked multiple times. In each pass the positions stored in the current texture are used as starting points for the ray-caster. It is the users responsibility to ensure that structures in the i-th context layer are behind the structures in the layers 1 to i−1 with respect to the viewing direction. If structures contained in different layers intersect each other the correct blending of these structures is not guaranteed. <ref type="figure" target="#fig_5">Figure 7</ref> shows the different textures generated during surface layer extraction for a particular example. Texels of a volume layer are treated similarly by applying an image-based high-pass filter to enhance regions exhibiting high color gradients thus highlighting heterogeneous material. The rendering pipeline to generate a focus layer is exactly the same as the one used to generate a context surface layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compositing</head><p>The compositor blends all the different textures generated during context and focus extraction. It takes into account a user-defined focus point, a focus region, curvature-based importance measures, and different rendering shaders. The focus point corresponds to the screenspace position of the mouse cursor projected onto the closest focus layer. This is simply the coordinate of the respective texel in the corresponding position texture. The user can also change the size of the spherical region around this 3D position in which the context information fades out from one to zero (focus point). The importance measure controls the visibility of the context layers within the focus regions via transparency modulation. Finally the user can select from a number of pre-defined shaders the one that yields the most appropriate results. Actually we have implemented four different shaders, which will be explained in Sections 3.2.5 to 3.2.7, but additional shaders can be added with ease. In the following we will discuss the particular importance measures we have integrated into ClearView. Technically the importance measures control the transparency of structures in the data. Here the challenge is to automatically and interactively find out what exactly is "important" and what can be neglected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Curvature-based importance</head><p>As can be seen in <ref type="figure" target="#fig_2">Figure 3</ref> artists often use the shape and in particular sharp features of the context layer as an importance measure. Especially areas exhibiting high curvature are necessary to convey the global shape of the object <ref type="bibr" target="#b16">[17]</ref>. Therefore our first importance shader uses the curvature directly to modify transparency. Given the focus point C, the size of the focus region s, and the surface position P within the data set, the transparency is computed as</p><formula xml:id="formula_0">trans = 1 − saturate max |C − P| s , curvature(P)</formula><p>where saturate clamps its parameter to [0 . . . 1]. Especially when used for technical and anatomical visualizations this importance measure gives excellent results, and it was thus chosen to be the default measure in ClearView. In particular compared to a clip region this approach gives significantly better results as it reveals the focus at the same time indicating the spatial context surrounding the structures in focus. Such a comparison is shown in <ref type="figure" target="#fig_6">Figure 8</ref>. Skin and bone are selected as context and the dental enamel is in focus. It is obviously clear that by using a clip geometry, as shown in the right, a similar context preserving visualization of the bones can hardly be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Distance-based importance</head><p>Instead of using the curvature-based importance measure it is often of interest to visualize how close two structures are to each other. The distance-based importance measure computes the distance from the context surface to the focus surface in the direction of the context normal (see left of <ref type="figure" target="#fig_7">Figure 9</ref>). This render mode is motivated by the observation that context surfaces running in parallel and in close proximity to the focus surface often convey no vital information whereas context surfaces with a diverging normal often relate to important features. Finally, context structures further away from any focus element -regardless of their normal -have a lowered probability of occluding important focus structures and are thus being drawn opaque. By using the identifiers from above the transparency is computed as</p><formula xml:id="formula_1">trans = 1 − saturate max |C − P| s , normalDistance(P)</formula><p>This importance measurement is often useful for technical and medical analysis as well as for pre-operative planning or structure optimization, since proximities are effectively emphasized. The integration of such a distance-based measure into the ray-casting approach is straight forward. It only requires one additional rendering pass that starts the rays at the surface points stored in the position texture and traverses these rays into the direction of the respective normal in the normal texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">View-distance-based importance</head><p>The view-distance-based importance measure computes the transparency of a structure in a very similar way as the distance-based measure. In contrast, instead of computing the distance from the context surface to the focus surface into the direction of the normal the distance into the view-direction is computed (see right of <ref type="figure" target="#fig_7">Figure 9</ref>). This seemingly minor change has two important implications. On the one hand it simplifies the computation. As the intersection points between the rays of sight and the context and focus surfaces are already known for a given view, the computation of the distance into the view direction does not require an additional rendering pass. More important to the user is the visual effect of this render mode. As can be nicely seen in <ref type="figure" target="#fig_0">Figure 10</ref>, this particular importance measure seems to hide the bone structure in a "fog of context" providing the user with an intuitive depth cue. It is particularly useful if the user wants to focus on close-to-surface structures. In the example shown transparency is computed as</p><formula xml:id="formula_2">trans = 1 − saturate max |C − P| s , |sur f F(P) − sur fC(P)|</formula><p>where surfF and surfC denote the extracted focus and context surfaces, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Focus Border</head><p>To enhance the border of the focus region, and thus to guide the view of the user, ClearView allows to draw a border aligned to the context surface. To achieve this effect the compositor uses the distance d of each surface point in the position texture to the focus point C. It decreases the luminance of those pixels in the focus layer for which holds:</p><formula xml:id="formula_3">d ∈ [0.95 • s, s],</formula><p>where s denotes the size of the focus region. This idea is essentially equivalent to intersecting a sphere around the focus center with the surface. As can be seen in <ref type="figure" target="#fig_0">Figure 11</ref> this extension is particularly useful in still images, as it attracts the user's attention to the focus region even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Diffuse Illumination</head><p>The simplest shader in our system evaluates just deferred diffuse lighting. Normals are evaluated as described before and are used to evaluate the diffuse dot product between normal and light direction. An ambient term is added based on the user's preferences. By default, ClearView positions the light at the camera. The reason is that this in general yields best contrast. Another beneficial effect of co-inciding light and view direction is that enhancement of silhouettes comes for free, since zero-crossings of the dot product between view and normal are usually identified as silhouettes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Cool to Warm Shading</head><p>Based on the observation that in nature objects facing the sun are colored in a warm tone, while shadows have a bluish hue due to scattering effects, Gooch et al. <ref type="bibr" target="#b11">[12]</ref> mandated the use of cool to warm shading for technical illustrations. This method keeps relative luminance constant, resolving the problem of low contrast in very dark and very bright areas. In ClearView such shading is accomplished by storing the cool to warm shades in a 1D texture. A lookup using the diffuse lighting contribution as a texture coordinate is then performed to yield the final color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.7">Skin And Bone</head><p>Though diffuse illumination or cool to warm shading yield optimum contrast and resolve the object's shape well, the resulting images may still not be fully intuitive. The reason is that the human brain not only interprets an object's shape, but also texture and material appearance. Skin and bone are readily recognized and qualities like hard or soft,warm are immediately associated. However, the scattering processes that eventually lead to the final appearance of natural materials are extremely involved and complex. Luckily, very simple approximations are sufficient to provide the brain with enough evidence to recognize the depicted material as skin or bone, resulting in intuitive images. We provide two simple and fast empirical shaders for skin and bone. Both are based on a single 1D lookup table (LUT) that maps lighting contributions to colors. For performing the lookup, several weights A, B,C, D are computed. In the following L denotes the light direction, N the normal and V the view direction at a certain pixel. All vectors are normalized to unit length.</p><formula xml:id="formula_4">A bone = 0.5 • L • N + 0.5 B bone = 0.7 − 0.3 • L •V C bone = B bone • (0.5 − 0.5 •V • N)</formula><p>A bone reflects the amount of diffuse light received, B bone is an intermediate estimate of how much light is coming from behind the object, and C bone combines B bone with a silhouette weight. A weighted average of A bone and C bone can then be used to fetch the final color from the LUT. In both shaders, a brightening of silhouette edges serves as a very rough approximation to subsurface scattering of light behind the object, improving contrast and plausibility. For bone, a single lookup at the position saturate(0.6A bone + 0.5C bone ) is performed, no additional specular or ambient component is evaluated.</p><p>For the skin shader, one lookup into the same texture is performed for each of the diffuse, specular, and silhouette components. Additional weights are computed, where re f lect(a, b) denotes the vector a reflected about b. <ref type="figure" target="#fig_0">Fig. 12</ref>. From left to right: Diffuse lighting, cool to warm shading, bone, and skin shader, along with the LUT used. Bone corresponds to a different iso-value.</p><formula xml:id="formula_5">A skin = 0.5 • L • N + 0.45 B skin = 0.6 − 0.4 • L •V B ′ skin = saturate (re f lect(V, N) • L) 4</formula><formula xml:id="formula_6">C skin = 0.5 • B skin • B ′ skin + 0.5 D skin = 0.9 • saturate(1.0 − N •V ) 3</formula><p>A skin thru C skin have the same interpretation as for the bone shader, but now C skin is based on a more specular term. The new parameter D skin is used to model an additional highlight at the silhouettes. Then, the three fetches into the LUT are performed.</p><formula xml:id="formula_7">Color di f f = LUT (A skin ) Color side = (1.0 − N •V ) • LUT (C skin ) Color hi = LUT (D skin )</formula><p>To accommodate for the red-shift in shadows that can be perceived in real skin due to subsurface scattering through blood (see also <ref type="figure" target="#fig_0">Figure  12</ref>, rightmost image), we further modulate the diffuse color component Color di f f towards red if A skin ≤ 0.5. The final color is a weighted average of the above three contributions, using the intermediate B ′ skin expanded to rgb as a specular term.</p><formula xml:id="formula_8">Color f inal = 0.8 •C di f f + 0.25 •C side + 0.15 •C hi + 0.1 • B ′ skin</formula><p>In all of these techniques, shadows can help to make the results even more believable, especially for the purpose of still images. The shaders described here are illustrated in <ref type="figure" target="#fig_0">Figure 12</ref> 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS AND DISCUSSION</head><p>In the following we present some results of our algorithm, and we give timings for different parts of it. All test were run on a single processor Pentium 4 2.8 GHz equipped with an ATI X1800 XT GPU with 512 MB local video memory. One advantage of ClearView is that it is not restricted to volumetric data sets, since it consists of two stages: the extraction stage and the image-based shading and compositing stage. Hence any renderable representation, such as point-based structures, or triangle meshes can be used. <ref type="figure" target="#fig_0">Figures 13, 15, and 16</ref> show show a selection of different data-sets being explored with ClearView.</p><p>To demonstrate the effectiveness of ClearView we have used the 512 3 thorax-abdomen-segment data set from the visible human male CT scan (see <ref type="figure" target="#fig_0">Figure 14)</ref>. Two different surface layers are extracted and visualized to reveal focus and context information.</p><p>When changing parameters that only affect the appearance of the structures being extracted, such as as material properties, shaders, light position and color, or focus position and size, already extracted layers can be re-used. In this case only the image-based shading and compositing stage is executed. This provides the user with the rapid visual feedback needed for interactive data exploration, while at the same time offering full control over the focus region. For a 800x600 viewport, the aforementioned shading and compositing stage is carried out at roughly 1700 fps, while still maintaining highly interactive 300 fps when increasing the resolution of the viewport to 1600x1200 pixels. Compositing three instead of two layers reduces the performance by approximately 15%. It is interesting to note that this is independent of whether an iso-surface or a volume layer is extracted. It is clear that these timings are independent on the data-set being visualized, and only depend on the image resolution as well as the number of layers to be combined.</p><p>Layer extraction is only executed when the user changes the camera position or layer-specific properties. The extraction of one single layer runs at about 30 fps on the low and 7 fps on the high resolution viewport. While the performance of the compositing stage is independent of the structure of the data set, these timings can vary due to emptyspace skipping or early ray-termination. For a thorough discussion of these performance impacts we refer the reader to <ref type="bibr" target="#b18">[19]</ref>, where the underlying GPU ray-casting algorithm is discussed in detail. As multiple layers can be extracted in one single volume rendering pass by writing the layer information into multiple render targets, the time to generate images is about 21 fps for the 800x600 viewport and about 5 fps on the 1600x1200 viewport.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>We presented ClearView, an intuitive and interactive focus+context visualization method. In ClearView, the user controls the appearance of the final visualization using only a few, clearly defined parameters such as the size and location of the focus, a weight for the context, and color/material properties for the regions. Consequently, no extensive training is required for users to successfully use the system. Because ClearView allows for interaction with the focus+context parameters at several hundred fps even for large viewports, users are further supported by rapid visual feedback.</p><p>Since we believe that ClearView is a very useful tool for the rapid generation of high-quality visualizations, we would like to extend the system further in various ways. In this work, we discussed the visualization of volumetric, optionally pre-segmented data. In the future we would like to evaluate how multi-modal data fits into the sys-tem. We would expect that using different modalities such as CT and MRI scans for different layers will convey important structures that cannot be found from visualizing either one of the modalities alone. As ClearView only depends on renderable primitives, which can be points, volumes, polyhedral surfaces etc., the restriction of combining only volumetric data is void. Furthermore, since the required features are extracted on-the-fly, we do not need additional feature data sets, allowing to investigate more compact, renderable data representations to cope with today's and tomorrow's gigantic data sets. To demonstrate the potential that lies within this flexible input data interface, we already rendered triangular meshes (see <ref type="figure" target="#fig_0">Figure 13)</ref> using ClearView, and we would like to further extend the palette of accepted input formats. Parallelizing the system also seems to be very promising to render the gigantic, potentially time-resolved data sets emerging lately as a result of improved numerical simulation capabilities. A trivial parallelization would just extract focus and context layers on separate GPUs. A more promising approach would be to further parallelize the extraction of each layer. Either image-space or object-space partitioning of the data is possible, since layers are extracted with full positional information and can be composed taking occlusion into account. For time-resolved data, additional research is needed as it is not obvious how the focus+context paradigm is best generalized to data sequences. <ref type="figure" target="#fig_0">Fig. 15</ref>. This image shows the Engine data set rendered with one focus and one context iso-surface, and the stag beetle data set with a volume rendered focus. <ref type="figure" target="#fig_0">Fig. 16</ref>. In this image of the backpack data set the focus region covers the the entire image. To the right, the focus on the knee data set was set to highlight the joint between tibia and femur.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Already Leonardo DaVinci<ref type="bibr" target="#b6">[7]</ref> followed the focus+context paradigm to convey relative positions of inner organs and muscles in the human body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The ClearView system can be used to visually explore complex data sets at interactive frame rates by using a focus+context metaphor. Even the leftmost image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Technical Illustration of Acura NSX and a Kodak digital Camera, Images courtesy of Kevin Hulsey Illustration, Inc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>This image compares the two different types of context layers surface only (left) and surface plus volume (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Curvature estimation: The left image illustrates the normal at a surface pixel and at its four neighbors. To estimate the curvature (right) the sum of distances from the center normal to all adjacent normals is computed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>These images shows the position, normal, and curvature textures, as well as a final image including a focus layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>The left image shows ClearView's curvature-based importance shader.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>This image shows the difference between the distance-based (left) and the view-distance-based (right) importance measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>In this image the view-distance-based measure (left) is used to focus only on the structures close to skin. The right image shows the curvature-based measure applied to the same data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>The tooth and C 60 molecule data sets are shown with and without the focus border.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>This image shows ClearView's application to a triangle mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Thorax/abdomen segment of the visible human male CT scan.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Kevin Hulsey Illustration, Inc. for <ref type="figure">Figure  3</ref>, The Visible Human Project for the Visible Human male CT scan, the Computer Graphics Group Erlangen for the Piggy Bank data set, the Institute of Computer Graphics and Algorithms of Vienna University for the stag beetle and www.volvis.org for the backpack data set. Some data sets were segmented by Thomas Schiwietz using the tool described in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toolglass and magic lenses: The see-through interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Bier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Illustrative contextpreserving volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroVis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Volumeshop: An interactive system for direct volume illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="671" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extending distortion viewing from 2d to 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cowperthwaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Fracchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Magicsphere: An insight tool for 3d data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cignoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Montani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scopigno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focus and context for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brodlie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory and Practice of Computer Graphics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dell&apos;anatomia fogli A et B, Quaderni d&apos;anatomia I-IV. Collection Windsor Castle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davinci</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1478" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transparency in interactive technical illustrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diepstraten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="317" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive cutaway illustrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diepstraten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Volume illustration: Non-photorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The cognitive architecture of bimodal event perception: A commentary and addendum to Radeau. Cahiers de Psychologie Cognitive/Current Psychology of Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-02" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="92" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A non-photorealistic lighting model for automatic technical illustrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="447" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Non-Photorealistic Rendering. AK Peters Ltd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walks for interactive organ segmentation in two and three dimensions: Implementation and validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schiwietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A focus and context interface for interactive volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ikits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<ptr target="http://www.cs.utah.edu/ikits" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Illustrating surface shape in volume data via principal direction-driven 3D line integral convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Interrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Illustrating transparent surfaces with curvature-directed strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Interrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive multiresolution modeling on arbitrary meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Campagna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vorsatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Acceleration techniques for GPU-based volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A magnification lens for interactive volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Joy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Graphics</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mesh saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A review and taxonomy of distortionoriented presentation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Apperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="126" to="160" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gaze-directed volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Utah Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonphotorealistic volume rendering using stippling techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hardware-accelerated parallel non-photorealistic volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Non-photorealistic Rendering and Animation (NPAR)</title>
		<imprint>
			<date type="published" when="2002-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention and perception: An information integration perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Massaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica, Special Issue: Action, attention and automaticity</title>
		<imprint>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="211" to="243" />
			<date type="published" when="1985-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using deformations for browsing volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgruffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tancau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonphotorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual exploration of seismic volume datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Steinicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of WSCG</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="73" to="80" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interactive lens visualization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="155" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature-enhanced visualization of multidimensional, multivariate volume data using non-photorealistic rendering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stompel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Graphics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Illustrative visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stredney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE</publisher>
			<pubPlace>Vis</pubPlace>
		</imprint>
	</monogr>
	<note>Tutorial #4</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Importance-driven volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="139" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The magic volume lens: An interactive focus+context technique for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Level-of-detail volume rendering via 3D textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE VolVis</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Volume clipping via per-fragment operations in texture-based volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time exploration of regular volume data by adaptive reconstruction of iso-surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Visual Computer</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
