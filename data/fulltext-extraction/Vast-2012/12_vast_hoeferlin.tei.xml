<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inter-Active Learning of Ad-Hoc Classifiers for Video Visual Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Höferlin</surname></persName>
							<email>benjamin.hoeferlin@uni-osnabrueck.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Cognitive Science</orgName>
								<orgName type="institution">University of Osnabrück</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Netzel</surname></persName>
							<email>rudolf.netzel@visus.uni-stuttgart.de</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visualization Research Center (VISUS)</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Höferlin</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Visualization Research Center (VISUS)</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weiskopf</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Visualization Research Center (VISUS)</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunther</forename><surname>Heidemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Cognitive Science</orgName>
								<orgName type="institution">University of Osnabrück</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Inter-Active Learning of Ad-Hoc Classifiers for Video Visual Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H</term>
					<term>3</term>
					<term>3 [Information Systems]: Information Storage and Retrieval-Information Search and Retrieval; I</term>
					<term>2</term>
					<term>6 [Computing Methodologies]: Artificial Intelligence-Learning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Learning of classifiers to be used as filters within the analytical reasoning process leads to new and aggravates existing challenges. Such classifiers are typically trained ad-hoc, with tight time constraints that affect the amount and the quality of annotation data and, thus, also the users&apos; trust in the classifier trained. We approach the challenges of ad-hoc training by interactive learning, which extends active learning by integrating human experts&apos; background knowledge to greater extent. In contrast to active learning, not only does interactive learning include the users&apos; expertise by posing queries of data instances for labeling, but it also supports the users in comprehending the classifier model by visualization. Besides the annotation of manually or automatically selected data instances, users are empowered to directly adjust complex classifier models. Therefore, our model visualization facilitates the detection and correction of inconsistencies between the classifier model trained by examples and the user&apos;s mental model of the class definition. Visual feedback of the training process helps the users assess the performance of the classifier and, thus, build up trust in the filter created. We demonstrate the capabilities of interactive learning in the domain of video visual analytics and compare its performance with the results of random sampling and uncertainty sampling of training sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Reduction of data to its relevant parts is a central and recurrent step of the visual analytics process, as outlined in the visual analytics mantra <ref type="bibr" target="#b16">[17]</ref>: "Analyse First -Show the Important -Zoom, Filter and Analyse Further -Details on Demand". Such reduction is critical for data scalability and is performed by automatic methods or user-defined filters. While automatic methods reduce the amount of data by exploiting some structure or by calculating predefined features and statistics, filters serve as their equivalents in human visual information seeking. Filters are involved in both exploratory interaction to reduce the amount of data displayed and to focus on the details (e.g., by dynamic queries <ref type="bibr" target="#b28">[29]</ref>) and in confirmatory interaction to confirm or refute hypotheses about the data (e.g., in video visual analytics <ref type="bibr" target="#b14">[15]</ref>). Typically, users define filters by providing model parameters or examples of data instances they want to be included in, or excluded from, their query.</p><p>We focus on the question how filters can be efficiently defined. This question arises especially when analyzing complex and highdimensional data spaces, where appropriate model parameters are unknown and filter definition by a single example is too weak. In such cases-we will use the examples of data and tasks from video visual analytics throughout the paper-query by multiple examples can be useful, which is identical to the training of a complex classifier. In this way, users can specify what they seek by integrating machine learning techniques into information visualization, as commonly recommended (e.g., by Shneiderman <ref type="bibr" target="#b29">[30]</ref> or Chen <ref type="bibr" target="#b5">[6]</ref>). However, in contrast to pre-trained classifiers as they are widely used in video analytics (e.g., person or car detectors, included in many video management systems), classifiers used to define filters within the visual analytics process have to be trained ad-hoc.</p><p>Such ad-hoc trained classifiers for filtering are required within the sense-making loop of analysts <ref type="bibr" target="#b34">[35]</ref>, when they build a case or search for support or evidence for a hypothesis within the data. Let us consider the example of video surveillance operators who assume, after some initial analysis of video sequences, that a cyclist might have been involved in the case of a traffic incident they deal with. Hence, they want to extract cyclists from video data to reduce the amount of video and to focus on promising parts for hypothesis verification. This example illustrates the need for training of new and arbitrary classifiers that can also be highly complex and specialized (e.g., hand-waving bicyclists with red helmets may be important in our example scenario). Since pre-trained instances of such classifiers are generally not available, the analysts have to define the filter by themselves. However, feature selection and model parameter definition for objects such as a bicyclist are too complex to be manually defined, even for domain experts with support by interactive visualization <ref type="bibr" target="#b42">[43]</ref>. Hence, filter definition via query by examples promises to be the only viable solution.</p><p>In contrast to traditional supervised training of a classifier, adhoc training involves new challenges: Annotation Costs: Data annotation is a very costly task because a large amount of annotated data is required for proper training of a classifier. Furthermore, the data has often to be annotated by domain experts in a time-consuming process. These facts question the benefit of ad-hoc training of filters within the analytical reasoning process. In addition, the issue of decreasing analysis performance arises if disruption that comes with high time consumption for training influences the analysis process. Finally, to find appropriate examples that can be labeled tends to be a difficult task that becomes worse with the rareness of the data instances queried (imagine the search for hand-waiving cyclists with red helmets). Annotation Quality: A typical phenomenon of positive example selection in ad-hoc training scenarios is that the provided data instances are not sampled as independent and identically distributed random variables from the query distribution, as required by the learner. In fact, users tend to provide samples drawn from a rather small region in data space. This effect obviously increases with the rareness of suitable examples in the data: if a hand-waiving cyclist was found, he will be annotated in all frames of the video. Although this approach provides multiple training examples, the training set itself is very specialized and may not generalize to all instances of the intended query (e.g., all hand-waiving cyclists). Furthermore, the quality of annotated examples is affected by the often vague idea of the query the users have in mind when defining the filter. The question of the exact range of the data instances of interest may arise (e.g., is a person on a trike also of interest?  A training set (data + large amount of labels) is provided to the learner, either as a monolithic set (batch learning) or in many smaller pieces (online learning). (b) In active learning, a bootstrapped learner iteratively refines itself by posing queries from a pool of unlabeled data U to a (human) oracle that provides labels for the data L. Inter-active learning is an extension (red arrows) of active learning that further allows the human experts to directly integrate their background knowledge into the model.</p><p>predefined. For example, we encounter the question how a data instance should be defined in object or event detection for video analysis: is the object silhouette the right way to crop the example from the video frame or can we use a bounding rectangle to define the image region, and if so, does the precision of the rectangle affect the filter performance? Such questions will not arise in domains where precisely defined documents are available, as it is the case in text document retrieval or content-based image retrieval. These issues especially arise in the ad-hoc training context. We will refer to them by the term annotation noise (see <ref type="figure" target="#fig_2">Fig. 2</ref>). Classifier Quality Assessment: A general question in machine learning is to detect the appropriate moment when to stop training of a classifier. The classifier should well adapt to the training data, but be general enough to correctly classify unseen data instances, too. This overfitting issue is traditionally tackled by cross validation. However, this approach requires either much more annotated examples for a validation set or much more time, since multiple classifiers have to be trained on different subsets of the training set. Furthermore, the generalization assessment of the classifier by cross validation is limited by the sampling bias induced by ad-hoc annotation (cf. annotation quality). Thus, cross validation is hardly feasible and often undesired in the ad-hoc training context. A question related to the stopping criteria in training is the question of stopping criteria in annotation of examples. This issue is of practical relevance because annotation involves costs. However, often no appropriate measure of training progress is available that facilitates the definition of such stopping criteria <ref type="bibr" target="#b35">[36]</ref>. Finally, quality assessment of the classifier becomes important to ad-hoc training because the users have to develop trust in their defined and applied filters. Hence, it is important to them to judge their classifier's performance when it is faced with unseen data or noisy data.</p><p>Training of a classifier under the constraint of high annotation cost is tackled by the field of active learning. In contrast to traditional supervised machine learning (see <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>), an active learner is allowed to choose the data from which it wants to learn. This way, typically greater accuracy can be achieved with fewer training labels. As depicted in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, active learning is an iterative process of refinement in which the learner may pose queries of unlabeled data to an oracle (e.g., a human) that provides the labels for this data. The active learner typically queries labels for the data instances with the highest informativeness or those that promise to reduce uncertainty most. Settles <ref type="bibr" target="#b26">[27]</ref> and Olsson <ref type="bibr" target="#b20">[21]</ref> provide an introduction and comprehensive overview of the field of active learning. For a survey of the application of active learning for multimedia annotation, we refer to Wang and Hua <ref type="bibr" target="#b40">[41]</ref>.</p><p>Theoretical analysis has shown that an active learner (however, a computationally complex query-by-committee approach) can re-duce the complexity of required labels in exponential order <ref type="bibr" target="#b9">[10]</ref>. Empirical analysis reveals that in the majority of applications, active learning is able to reduce the amount of labels <ref type="bibr" target="#b26">[27]</ref>, too. A survey of the usage of active learning for text annotation exhibits that the expectations of most practitioners on the performance of active learning are either fully (36.3%) or partially (54.4%) met <ref type="bibr" target="#b35">[36]</ref>. The reduction of examples to label in practical situations, however, may lie far behind exponential decrease and, dependent on the dataset, sometimes does not show any improvement at all <ref type="bibr" target="#b23">[24]</ref>. Further, some authors report negative results <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> or show that the performance depends on the expertise of the annotator <ref type="bibr" target="#b1">[2]</ref>.</p><p>Due to these results and the further challenges that we face in the context of ad-hoc training, we question whether active learning alone is suitable for ad-hoc training and capable of meeting the tight time constraints existing in this application. Furthermore, we question the idea of the human experts as mere annotators, but believe that their expertise should be utilized in a more direct way.</p><p>In this paper, we introduce a novel method called inter-active learning, an extension to conventional active learning that directly involves human experts in the ad-hoc training process using the visual analytics methodology. The additional interaction introduced by inter-active learning is depicted by red arrows in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>: the goal of this process is to efficiently create filters leveraging the complementary strengths of human and machine, as outlined by Bertini and Lalanne <ref type="bibr" target="#b3">[4]</ref> in the context of the knowledge discovery process.</p><p>In detail, inter-active learning efficiently approaches the goal of a well-trained classifier by iterating over the three basic steps: i) assessment of the performance of the classifier, ii) annotation of data instances and/or manipulation of the classifier model, and iii) retraining of the classifier. In Section 3, we will break down these basic steps into different tasks the users perform in each cycle to incorporate new background knowledge into the trained model. This paper contributes to the current state of research by presenting a way how the problem of ad-hoc training of classifiers for interactive filter definition can be tackled by visual analytics. Besides introducing a general methodology, which can be considered as an extension of active learning methods toward increasing leverage of the users' expertise, we apply inter-active learning to the domain of video analysis for validation. We present an integrated visual analytics system that covers the three steps of inter-active learning and provides an adaption to online learning of the cascade classifier model we use, as well as visualization and interaction models that can cope with the complexity, high dimensionality, and huge amount of data of the video domain. A usage scenario illustrates the different tasks the users carry out within the three steps and further provides validation of our method by comparison to classifier training with random sampling and uncertainty sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our approach is related to techniques from visual analytics, knowledge discovery, data mining, information visualization, and machine learning. Closely related is previous work by Seifert and Granitzer <ref type="bibr" target="#b24">[25]</ref>, Seifert et al. <ref type="bibr" target="#b25">[26]</ref>, May and Kohlhammer <ref type="bibr" target="#b17">[18]</ref>, and Heimerl et al. <ref type="bibr" target="#b13">[14]</ref>. All four methods aim at tight integration of the user into the labeling process. The first work <ref type="bibr" target="#b24">[25]</ref> presents a user-based and visually supported active learning method that was successfully validated on different multi-class datasets of various data domains. Seifert et al. <ref type="bibr" target="#b25">[26]</ref> focus on visual classifier performance assessment, while the learner can be adapted by labeling data instances in an information landscape visualization. Validation is provided by means of a multi-class, multi-label text classification scenario. May and Kohlhammer <ref type="bibr" target="#b17">[18]</ref> also allow the user to refine a classifier model by selection of training examples. They focus on performance feedback of the classifier model using a visualization that facilitates pre-attentive pattern identification. Recently, Heimerl et al. <ref type="bibr" target="#b13">[14]</ref> have introduced a system to interactively train a support vector machine model for text document retrieval based on visual analytics and active learning. Further, they provided a thorough evaluation of their approach. However, all four methods do not go beyond interactive definition of the training set and user selection of the data instances to label. More direct integration of human experts' background knowledge is not considered.</p><p>There are several approaches of interactive definition of classifiers (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>), that do not consider support of (semi-) automatic methods for classifier refinement, such as active learning. Although these methods were successfully applied to train or combine classifiers, purely user-based definition of classifiers seems only to be viable for problems with low complexity <ref type="bibr" target="#b42">[43]</ref>; otherwise, they annoy the users by recurring tasks <ref type="bibr" target="#b31">[32]</ref>. Furthermore, interactive definition often constrains the complexity of the classifier model; hence, decision trees typically appear in these works.</p><p>Another approach incorporating the users' background knowledge into the training process is followed by the active learning community. In the domain of text classification, where features often coincide with words, promising methods were developed that allow the learner to pose queries to the oracle to label features, instead of just data instances <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>. This means that the oracle can tell the learner if a particular feature describes a particular class well. However, feature labeling can only be applied in areas where features are tangible to human users (e.g., word features in text classification). Thus, such methods are not applicable in complex and abstract problem environments in which features do not exhibit any concrete symbolic meaning to humans. Besides the methods that incorporate human decisions in machine learning, visualizations of classifier models for performance assessment and model understanding, such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref>, are naturally related to our approach. In contrast to the existing approaches, we advance the fields of active learning and visual classifier definition by combining them to a visual analytics process called inter-active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INTER-ACTIVE LEARNING</head><p>In this section, we outline the theoretical considerations of interactive learning and specify its requirements for learning models, visualization, and interaction.</p><p>Inter-active learning re-formulates the problem of supervised machine learning as a visual analytics problem. Supervised machine learning for classification, as depicted in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, seeks to find model parameters m that minimize the class confusion error E (according to the distance function f error ) of the classifier function h m : S → T , which maps the data distribution S to a set of target classes</p><formula xml:id="formula_0">T = {0, 1, • • • , n}: E = ∑ i∈I f error (h m (s i ), l i )</formula><p>Here, a training vector of data/label pairs (s i , l i ) is provided, where data samples s i are drawn as independent and identically distributed random variables from S and data labels l i = L (s i ) are given by the labeling function L ; I denotes the set of sample indicies. We do not consider any further regularization terms, such as smoothness of the function.</p><p>In contrast to passive supervised learning, active learning additionally minimizes the costs C = ∑ i∈I C (L (s i )) that arise by acquiring a finite set of labels l i . Active learners are allowed to pose query of data instances to be labeled to the users. The decision which data has to be labeled for the next training cycle depends on the current model m, the available data instances s i ∈ U, and the labeling costs. Active learners typically assume a uniform cost function C (i.e., C = |I|) and thus query labels for the most informative data instances from the users. This process is illustrated for a pool-based active learner in <ref type="figure" target="#fig_0">Fig. 1</ref> (b) (ignoring the red arrows).</p><p>Inter-active learning, as an extension to active learning, pursues the same objective: a well-trained classifier trained with minimal labeling costs. In contrast to active learning, inter-active learning assumes that the users-based on their expertise of the domain and their knowledge about m-are able to select a more effective set of data instances to be labeled. Further, the training process can benefit from direct modifications of the learner model m. In highdimensional data domains with complex dependencies, however, direct definition of model parameters can be difficult <ref type="bibr" target="#b42">[43]</ref>. Hence, we focus on permitting the users to detect and solve contradictions between their domain knowledge and the actually trained model m.  Due to the tight connection between learner and user, visualization and human-computer interaction are, besides automatic methods, the central aspects of inter-active learning. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates this connection between the three major elements: learner model, user, and data. Furthermore, <ref type="figure" target="#fig_2">Fig. 2</ref> depicts the flow of information between the three elements that can be assigned to one of the three category of methods: visualization, interaction, or automatic method. The iterative interaction of these three components results in a visual analytics process that aims to refine the classifier model and its comprehension by the users.</p><p>Each cycle of this iterative process consists of three steps mentioned before: i) assessment of the model performance, ii) refinement of the classifier model, and iii) retraining of the classifier. The first two steps, which include user interaction, can further be divided into tasks the users may consider to process each cycle. For the first step, these tasks include assessing the success of the last training cycle (training feedback) and determining if a stopping criterion was reached (e.g., the model has already reached an appropriate level of quality or training does not improve the model anymore). Furthermore, by assessing the model's performance, users can build trust in their trained model and learn to know its strengths and weaknesses. Hence, they can incorporate the performance and uncertainty of "their" filters into their decisions within the analytical reasoning process. Finally, quality assessment also guides the users in refining the model. Users may detect overfitting of the model, low robustness to noise, or lack of generalization. These issues are tackled in the second step of a cycle. After the classifier model was analyzed in the first step, two types of refinement are available to the users in the second step: data annotation and model manipulation. While both data annotation and model manipulation can be used to broaden the classifier model to accept a wider variety of data instances or to narrow the acceptance range, we recommend using model manipulation mainly for generalization purposes; in contrast, data annotation is suitable for both tasks. This recommendation accounts for the complex dependencies of high-dimensional data distributions. In such cases, it is often easier to tell the system what is wrong (e.g., overfitting of the model) than to define what is right. For data labeling, the users can choose which data regions they intend to annotate for model refinement. In this way, the users can efficiently integrate novel domain knowledge into the system. Labeling of data in regions near the decision boundaries helps increase the classifiers confidence, whereas labeling of data in regions far away from the decision boundary helps explore new regions of the data space and might reduce extensive class confusion. However, users can also rely on the classifier model to provide the most beneficial data instances for labeling utilizing active learning.</p><p>In the next sections, we introduce the main components of a visual analytics system for inter-active learning and explain their application for the tasks mentioned above. These components include, besides different (coordinated) views on the data and model, also the definition and implementation of an appropriate classifier model. Furthermore, we will address issues of scalability and input noise that affects the trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CLASSIFIER REQUIREMENTS</head><p>In this section, we describe the requirements for a learner for adhoc training in general and the classifier model we use for video visual analytics in particular.</p><p>Filters generated by ad-hoc training have to be of low time complexity because such filters are often used to facilitate scalability with increasing data size. In this paper, we use a cascade of classifiers ( <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>) to predict class assignments of sliding windows in each video frame. In combination with a set of basic, yet fast to compute, rectangle features, this method has become popular with the work of Viola and Jones <ref type="bibr" target="#b38">[39]</ref> in the context of face detection. Rectangle features ( <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>) operate on gray-value images and are computed by subtracting the sum of pixel values of the black part from the sum of pixels of the white part.</p><p>Similar to Viola and Jones, we also use a committee of thresholded rectangle features as weak classifiers within each node of the cascade. The features for each node are typically selected and weighted by some boosting algorithm, such as AdaBoost <ref type="bibr" target="#b8">[9]</ref>. The high efficiency of the cascade during evaluation of sliding windows (various rectangular cropped parts of the video frame) results from the low complexity of the first nodes. With only few computations, the first node already rejects about half of all sliding windows; the complexity and number of features typically increase with each node. Sliding windows that pass all nodes in the cascade are considered detections <ref type="figure" target="#fig_3">(Fig 3 (b)</ref>), such as the sliding window in <ref type="figure" target="#fig_3">Fig 3 (a)</ref>, which contains a person onto whom the rectangle feature selected is superimposed and aligned by boosting. This often ap- plied approach only distinguishes between sliding windows that are "detected" by the cascade or not, hence it defines a binary classification problem. If not stated otherwise, the parameters we use for our cascade of classifier are derived from the original work of Viola and Jones <ref type="bibr" target="#b38">[39]</ref>.</p><p>As reported by Tomanek and Olsson <ref type="bibr" target="#b35">[36]</ref>, it is critical for interactive refinement of classifiers that retraining can be performed very fast to keep the idle time of users at an acceptable level. Fails and Olsen <ref type="bibr" target="#b7">[8]</ref> even claim that, in order to be effective, the classifier must be generated from the training examples in under five seconds. These requirements also apply to inter-active learning. Hence, most training algorithms are not suitable for inter-active learning because they tend to need several hours of time for learning the model based on a huge set of labeled data. However, by retraining the classifier with only the samples the users labeled in the current cycle, we can meet the efficiency requirements. Methods that can iteratively update the model according to newly provided data/label pairs are called online training methods.</p><p>In this paper, we use a modified version of online AdaBoost, introduced by Oza <ref type="bibr" target="#b22">[23]</ref>. Since in online training, the learner is faced with only one example a time, the basic idea of online boosting is to maintain statistics for each feature that capture its performance history for all the samples seen so far. This, however, implies that greedy selection of features cannot consider the complete set of examples when choosing a particular feature, since future examples are not available. Hence, the performance of online AdaBoost approaches the performance of batch-mode training in the limit. Based on the introduced statistics, an estimated error e i can be calculated for each weak classifier. A weak classifier h weak i is built from a rectangle feature i that is thresholded after evaluation to obtain a binary decision (i.e., h weak i ∈ {−1, 1}); hence, we use the terms feature and weak classifier interchangeably.</p><p>A subset of all available weak classifiers is selected by the greedy boosting algorithm according to the lowest error to form a strong classifier h strong . The binary decision about the class assignment of each sliding window s is made in each node of the cascade by its respective strong classifier h strong . Therefore, the weighted sum of binary responses of all n boosted weak classifiers that belong to the strong classifier and threshold t are used:</p><formula xml:id="formula_1">h strong (s) = sign(conf(s) − t) conf(s) = n ∑ i=1 a i h weak i (s)<label>(1)</label></formula><formula xml:id="formula_2">a i = log 1 − e i e i</formula><p>AdaBoost lives on rating the importance of each example for training (a correctly classified example is of lower importance than an incorrectly classified one). Online AdaBoost estimates the importance of an example for each feature based on the binary decisions of the preceding weak classifiers. According to the estimated importance, the history maintained by each feature is influenced.</p><p>Training of a node means to adjust the thresholds, weights, and history of all features. The approach of Oza only uses a fixed number of features. We improve on this by integrating the idea of Grabner and Bischof <ref type="bibr" target="#b11">[12]</ref>. They use selectors to dynamically choose the committee of weak classifiers with the best performance out of a number of features that are constantly trained. In this way, changing number of features also becomes possible.</p><p>Each strong classifier of the cascade is trained with a modified version of this online AdaBoost algorithm. The main improvements we make to the method of Grabner and Bischof affect the statistics maintained for each strong classifier. By introducing two histograms (one for the positive and one for the negative examples) of the conf measure (see Equation <ref type="formula" target="#formula_1">1</ref>), we enable a cascade construction that is similar to the original algorithm by Viola and Jones. Using the histogram of confidences of positive training examples, the true positive rate can be adjusted to match the classification goals of a node by decreasing the threshold t to an appropriate level. The false positive rate is then accessible by the confidence histogram of negative examples, by summing up the bins between one and the currently chosen threshold. After the cascade node was trained by an example, either the former or the latter histogram, is updated (depending on the label of the data example) by increasing the particular bin by α. Finally, the histogram is normalized to one. The constant α controls the decay rate that is necessary because the stored confidence values become outdated by training. In our examples, we use an experimentally derived value of α = 2/η, with η being the number of samples seen so far.</p><p>To increase robustness for imbalanced numbers of positive and negative training examples, we modify the boosting algorithm to optimize the distance between the receiver operating characteristic (ROC) of the node's classification history and the point of optimal classification (i.e., TPR = 1, FPR = 0), involving calculation of true positive rate (TPR) and false positive rate (FPR) of each weak classifier. New nodes are added to the cascade until the maximum error rate FPR overall &gt; FPR overall = ∏ m k (FPR k ) is met. For our experiments, we choose an maximum error rate consistent with Viola and Jones <ref type="bibr" target="#b38">[39]</ref>: FPR overall = 10 −5 . Finally, we enable the users to modify the cascade, by adding new features (user-defined features) to a node or changing the position or shape of features that were selected by the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ACTIVE LEARNING</head><p>When users lack the knowledge which data instances are most efficient to label, they can use active learning. Pressing a button, they automatically obtain a selection of data instances for which the active learner is most uncertain. The selection is shaped by a poolbased uncertainty sampling approach. For the cascade of classifiers we use, uncertainty u about the true label of a sliding window s corresponds to a sum of confidence (Equation (1)) values of all m nodes involved in making the class decision:</p><formula xml:id="formula_3">u(s) = 1 ∑ m k=1 conf k (s)<label>(2)</label></formula><p>This is equivalent to the definition of confidence by Visentini et al. <ref type="bibr" target="#b39">[40]</ref> and Graber and Bischof <ref type="bibr" target="#b11">[12]</ref>. The most uncertain data instances will be selected to be further analyzed by the users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUALIZATION AND INTERACTION</head><p>In this section, we introduce the different views and interaction technique of our inter-active learning framework. The screenshot of the workspace depicted in <ref type="figure" target="#fig_5">Fig. 4</ref> shows the three main areas of the graphical user interface (GUI). Left, we see the trained classifier model <ref type="figure" target="#fig_5">(Fig. 4 (b)</ref>) and the cascaded scatterplot ( <ref type="figure" target="#fig_5">Fig. 4 (a)</ref>) showing the evaluation results of the cascade for performance assessment. On the right, three different views on the model and data are available for annotation and model modification, which are used in the second step of each training cycle. In-between both GUI areas, the current selection of data instances, cascade nodes, and features is shown <ref type="figure" target="#fig_5">(Fig. 4 (c)</ref>). The selections connect the performance assessment step with the refinement step and, in this way, provide a natural arrangement of tasks. For interpretability, a distinct color is assigned to each type of selection. This color is used to highlight the selection in each of the coordinated views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Cascaded Scatterplot</head><p>To present the feedback on the quality of the current classifier, we introduce a novel visualization of the class distribution of data instances (sliding windows of the video) in each stage of the classifier. This visualization, which we call cascaded scatterplot, integrates multiple dependent scatterplots that are horizontally aligned to match with the cascade information <ref type="figure" target="#fig_5">(Figs. 4 (a)</ref> and (b)).</p><p>In the cascaded scatterplot, the abscissa is divided into m parts, with m being the number of nodes in the cascade. In contrast to conventional scatterplots, cascaded scatterplots represent each data point up to m times. Coordinates of a data point in the cascaded scatterplot depend on the classification quality of the sliding window s by the respective strong classifier (cascade node). The xvalue of each instance of a data point is made up of an integer value that determines its assignment to a cascade node as well as of a fractional part that represents the confidence of each classifier's decision on the data point. Hence, data points of two classifiers cannot overlap in their x-value. For each data point and classifier, we plot the normalized confidences (distance to the decision boundary on the x-axis) c k (s) ∈ [−1; 1] against the feature robustness r k (s) ∈ [0; 1] (y-axis) for each node N k .</p><p>The robustness r k (s) of a feature decision is influenced by two components: the robustness of the feature weights r weight of each node with n k features, and the distance between the feature response of the signal and the threshold of the feature (its decision boundary) r margin . Together, the feature robustness r k (s) = r utilizes the normalized entropy of the feature weights a i to penalize the skew of weight distribution to a small amount of huge weights because in this case, the decision of a node may be changed only due to the flipping a couple of its features' decisions:</p><formula xml:id="formula_4">r weight k = − ∑ i∈N k a i log(a i ) log(n k )</formula><p>Data instances with a small distance between their feature response and the feature's threshold are likely to flip the weak classifier's decision in the presence of noise. Therefore, r margin k includes the average distance (normalized to the maximum margin of the features) between the features' decision boundaries t i and the signal's response to each feature f i (s):</p><formula xml:id="formula_5">r margin k (s) = 1 n k ∑ i∈N k |t i − f i (s)| maxmargin i</formula><p>The feature robustness is shown along the y-axis of the scatterplot and helps the users judge the classifiers sensitivity to data noise that influences the general quality of the model. On the x-axis, the uncertainty of the class assignment is shown by the distance of the data points to the decision boundary of each strong classifier in the cascade. The confidence measure c k (s) is normalized to the range </p><formula xml:id="formula_6">c k (s) = conf k (s) ∑ i∈N k a i h weak i (s)</formula><p>Besides both boundaries of maximum confidence, we illustrate the decision boundary (dashed lines in <ref type="figure" target="#fig_5">Fig. 4 (a)</ref>) at c k (s) = 0 for orientation, too. By selecting one or more data points in the cascaded scatterplot representation of a node, the data points will be highlighted in all other nodes in which they appear. In this way, the users can easily assess the class assignment and the quality of the decisions of the selected data throughout the whole cascade. The assignment of each data instance to either the positive class (e.g., person) or the negative class (i.e., background region) is indicated by the respective color (green or red) of the data point in the cascaded scatterplot for each node. For convenience, the users can choose to display only positive or negative classifications, or both combined in one plot.</p><p>In combination with other views, the cascaded scatterplot is mainly used to assess the performance of the classifier and to provide feedback on the training progress. In this way, users gain trust in the trained classifier and may decide to stop the refinement process, either because no further progress in training is experienced or the quality of the classifier reached a sufficient level. Recognition of the right point to stop training is of practical relevance, as the survey by Tomanek and Olsson <ref type="bibr" target="#b35">[36]</ref> points out: most of the participants had a stopping criteria that would fit the context of adhoc training. Furthermore, the cascaded scatterplot is used to select data instances for labeling and thus, to avoid overfitting of the classifier, reduce uncertainty of the classification decision, or improve generalization by exploring new data regions.</p><p>To handle the tremendous amount of data we face in the context of video analysis, the amount of data displayed can be adjusted by the user. In this way, we maintain the responsiveness of the interactive visualization and reduce overdrawing in the cascaded scatterplot. The users can define the frame interval and a number of frames to be evaluated. These will be randomly chosen from the interval. Further, we constrain the number of evaluated data instances, by considering only sliding windows that have an aspect ratio similar to the average aspect ratio of the positive training examples.</p><p>The displacements dx and dy (measured in pixels (px)) and scaling factor s f of the sliding windows are further restricted. Applying our default values (dx = 5 px, dy = 5 px, and s f = 1.5) results in almost two million data instances to be evaluated for 50 frames of a video with 720 × 576 px resolution. This shows that evaluating an entire video would exceed the capabilities of human and machine very quickly. Hence, we provide further constraints to keep the amount of data at a manageable level. Besides random sampling of a smaller amount of data instances from the video, we use two additional methods to reduce the amount of data displayed. The first method focuses on relevant regions of the data distribution by interactive definition of robustness and uncertainty intervals in the cascaded scatterplot. The second method automatically selects a fixed number of the most uncertain detections by using active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Annotation</head><p>After data instances have been selected, either by the users or automatically by the active learning subsystem, the data points are visualized in the annotation view <ref type="figure" target="#fig_5">(Fig. 4 (f)</ref>). This helps the user to comprehend the quality of the classifier and discover areas of data instances that have the wrong class assignment (class confusion).</p><p>To facilitate overview and efficient annotation of the data instances, we project the data points onto a two-dimensional map according to their similarity. For this purpose, we use the dimension reduction algorithm developed by Van der Maaten and Hinton <ref type="bibr" target="#b37">[38]</ref>, called t-Distributed Stochastic Neighbor Embedding (t-SNE). Van der Maaten and Hinton showed that this method provides good visualizations of high-dimensional data lying on related low-dimensional manifolds, such as images of multiple objects captured from different viewpoints.</p><p>We visualize the two-dimensional map provided by t-SNE by displaying thumbnails of the sliding windows. Class assignment of the classifier as well as data selection is shown by the border color of the thumbnail (red, green, or orange). Additionally, the thumbnails are augmented by the label information if a data instance has already been annotated (small red or green boxes in the right corner of the thumbnails). To reduce visual clutter and to increase annotation speed, overlapping thumbnails are depicted as clusters. Therefore, only the medoid of a cluster is depicted as representative data instance. The clusters are determined by kernel density estimation in screen-space with a fixed bandwidth that is adapted to the size of the thumbnails. The medoids are iteratively selected according to their density. This way, any overlap of thumbnails is avoided. A cue of the cluster size is shown by the size of the shadow dropped by the medoid's thumbnail. Users can navigate through the data by zooming and panning the map. Zooming and panning enables the users to explore particular clusters that will be unfolded step by step and thus a detailed view on their elements is shown. A minimap supports navigation in the data projection.</p><p>Similar to Möhrmann et al. <ref type="bibr" target="#b19">[20]</ref>, we allow for fast labeling of multiple data instances by selecting and annotating whole clusters of data instances. This supports the users in one of their main tasks while iteratively refining the classifier. The thumbnails augmented with the classifier decision and the label information therefore enable the users to make quick decisions which data instances should be labeled next. Selecting one or multiple data instances in the annotation view will also select these instances in the other views. Therefore, the annotation view helps select similar data instances for further inspection in the other views.</p><p>For annotation, the linked video context view ( <ref type="figure" target="#fig_5">Fig. 4(d)</ref>) is particularly useful because it displays the selected sliding windows in their video frames and, thus, complements the thumbnails of the annotation view by the video context. Furthermore, selection highlighting in the cascaded scatterplot helps the users determine the robustness and confidence of the classifier's decision. By this process, the users can quickly select and label the data that is most beneficial for further training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Representation and Manipulation</head><p>Understanding the classifier model can be crucial to assess its quality and to determine further actions to increase the classifier's performance. Model understanding becomes beneficial, especially in cases where only a small amount of labeled data is available and, thus, cross validation is not applicable.</p><p>A famous example of classifier failure due to a restrictive dataset is the often-cited story of an artificial neural net trained by the army to detect tanks <ref type="bibr" target="#b6">[7]</ref>. After first success on training and validation sets, the system totally failed on a newly captured validation set. After investigating the issue, it turned out that the system learned to distinguish between cloudy and sunny skies rather than between tanks and bushes, due to a bias in the data sets. Recent discussion exhibited that bias is common to most datasets used to train computer vision systems <ref type="bibr" target="#b36">[37]</ref>, even though the datasets strive for representing the whole visual world. Unfortunately, the bias of datasets in ad-hoc training is presumably disproportionately stronger than in such carefully assembled datasets. Due to such bias, it is important to understand the trained model for verification and validation, e.g., by using visualization <ref type="bibr" target="#b32">[33]</ref>. We therefore visualize the classifier model and let the users investigate the trained model and its behavior, as often suggested <ref type="bibr" target="#b15">[16]</ref>.</p><p>One or multiple data instances, selected in the annotation view or in the cascaded scatterplot, are depicted in the model visualization ( <ref type="figure" target="#fig_5">Fig. 4 (e)</ref>). If multiple data instances are selected, the mean image of their sliding windows is displayed. This outlines their commonalities that become recognizable as patterns of patterns in the mean image <ref type="bibr" target="#b41">[42]</ref>. This perspective, however, is similar to the view the classifier has on the data. Next to the mean image, the corresponding feature response map is depicted. This map is constructed by superimposing the color-encoded feature response and feature weight. A legend of the bimodal red-blue color mapping of feature response and the luminance mapping of the feature weight is depicted in <ref type="figure" target="#fig_5">Fig. 4 (e)</ref>. In combination, these two maps help understand the relevance of different parts of the model to the classifiers and, possibly, the reason for that importance.</p><p>To further facilitate model understanding, users may select a single node or multiple nodes of the cascade to be evaluated and depicted in the model view. Further control of the visualization is provided by selecting arbitrary features by drag &amp; drop from the cascade information view <ref type="figure" target="#fig_5">(Fig. 4 (b)</ref>, yellow selections). The selected rectangle features will be displayed in a list with their according weight and response to the selected data instances. These values are also marked in the legend. Additionally to that, outlines of the features-with respect to their position and scale in their classifier node-augment the aggregated feature response map and the mean image of data instances. This supports the users in determining the influence of each feature and the structures it detects. Based on the model visualization, the users may experience discrepancy between their idea of the intended filter and the trained classifier model. In this case, the users are provided by several tools to alleviate the discovered discrepancy by integrating more background knowledge into the classifier. The users may change the properties of one or more features, such as their type, scale, location, and threshold. Furthermore, the users may add or remove feature to or from arbitrary nodes. Finally, by brushing on the mean image, the users can define areas in which the classifier is not allowed to place any features in. After modification of the classifier, the new model will be automatically refined in the next cycle.</p><p>Modification of the model based on the users' expertise can be used to boost the learner, especially in early training cycles, by including fresh domain knowledge. Overfitting of the learner as well as model errors due to lack of generalizing training data (i.e., the samples are not independent and identically distributed) can be detected and corrected (e.g., by removing specialized features, which is similar to pruning a decision tree). By locking particular regions of the sliding windows from being evaluated by features, insignificant areas, such as background or potentially occluded regions can be excluded from being regarded in training. In this way, training has to focus on more relevant parts of the training data (e.g., tanks instead of skies).</p><p>Please note that we do not suggest defining the classifier model from scratch because this is often not possible in complex data domains. Nevertheless, direct model manipulation can be beneficial to integrate background knowledge that is not available in the form of labeled data instances, in cases where discrepancy between trained classifier model and the intended filter was experienced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">USAGE SCENARIO</head><p>In this section, we provide an exemplary usage scenario of our interactive learning system to demonstrate its capabilities and potentials <ref type="bibr" target="#b0">1</ref> . We keep the example simple by learning a classifier model in only four training cycles; this classifier is capable of detecting persons in video sequences. This classifier represents an example of a filter that may be included in an analytical reasoning process and, thus, being defined under severe time constraints. For simplicity reasons, we only depict the most relevant views and elements of the GUI. We use two video sequences gathered from the i-Lids multi camera tracking dataset 2 , one for training and one for validation of the classifier. To verify our method, we compare its performance with training the initial classifier (bootstrapped in Cycle 1) using random sampling of annotated ground truth data instances. Further, we provide the performance of active uncertainty sampling, according to the measure introduced in Equation <ref type="formula" target="#formula_3">2</ref>. For both uncertainty sampling and random sampling, we choose a balanced set of positive and negative examples. Cycle 1: A first training set is populated, used to bootstrap the adhoc classifier. We select just 25 positive examples by drawing rectangles on the video frames, as illustrated in <ref type="figure" target="#fig_6">Fig. 5</ref>. Additionally, 50 negative examples are also selected, either by manually defining the regions or by drawing random samples from the video, in which only small overlap with positive examples is allowed. Both positive and negative examples are depicted in the video player and in separate lists for visual inspection by the user. Based on these examples, the initial classifier cascade is automatically trained. Cycle 2: A first glance at the cascaded scatterplot ( <ref type="figure" target="#fig_7">Fig. 6(a)</ref>) exhibits a high false positive rate by the large number of positively (green dots) classified data instances in the last node. By browsing the video in the context view, this assumption is confirmed: the green rectangles in <ref type="figure" target="#fig_7">Fig. 6(c)</ref> represent sliding windows classified as persons. We choose to label some misclassified background patches using the annotation view. First, we select the set of positive classified data instances in the last cascade node, since we are only interested in the sliding windows that are (false-) positively classified by the whole cascade of classifiers. However, the labeling of these instances affects all nodes, such that also earlier nodes adapt to important new patterns. We choose to select some data instances near the decision boundary and some samples that are far away from the decision boundary for annotation ( <ref type="figure" target="#fig_7">Fig. 6 (b)</ref>). The first selection includes the data instances the model is most uncertain about their classification. This is similar to uncertainty sampling in active learning. Selection of data instances far away the decision bound- <ref type="bibr" target="#b0">1</ref> The supplementary material includes a video of the usage scenario. http://www.vis.uni-stuttgart.de/index.php?id=vva 2 http://www.homeoffice.gov.uk/science-research/ hosdb/i-lids/ c ary includes the data instances the model is most certain about, but this does not mean that these instances are assigned to the right class. Due to the very low number of initially labeled examples it is very likely that we find patches containing no person in the region far away from the decision boundary. By labeling data instances of those both regions, we expect to improve the classifier model most. Within the active learning domain, such query of data instances far from the decision boundary is sometimes termed exploration <ref type="bibr" target="#b21">[22]</ref>. For labeling, we navigate the annotation view (by panning and zooming) to a region of clusters that contains various instances of misclassified samples (see <ref type="figure" target="#fig_7">Fig. 6 (d)</ref>). Further zooming reveals the individual members of the clusters. By selecting individual instances or whole clusters, the video context view jumps at the position of the video from which the selected data instance or the medoid of the cluster were sampled and represents its sliding window by a rectangle in the selection color (orange). This way, the spatio-temporal context of the data instances becomes accessible to the users. Next, we add 50 instances to the set of negative training examples by selecting clusters or single instances. The thumbnail images in the annotation view are augmented by red rectangles to indicate which data instances have been labeled. Finally, we retrain the classifier and proceed with the next refinement cycle. Cycle 3: The cascaded scatterplot and the model view of the cascade provide visual feedback about the last training cycle. We spot decrease of the number of positive classifications. However, we experience by browsing the classification results that the classifier is not satisfactorily trained, since still many background regions are considered as persons. Hence, we decide to label some false positive examples again.</p><p>During the annotation of 50 false positive data instances (and one false negative), we inspect the model representation for common reasons of the wrong classification of background data instances. The mean image of several false positive samples quickly exhibits a pattern of patterns, an explicit vertical edge at its left border (cf. <ref type="figure" target="#fig_8">Fig. 7 (a)</ref>). Superimposing the mean image with rectangle features selected by the boosting algorithm reveals the importance of the vertical edge pattern for recognition. This importance is further confirmed by the strong alignment of some of the rectangle features with the edge and the feature response map in <ref type="figure" target="#fig_8">Fig. 7 (b)</ref>. After we identified the adaption of the classifier to this pattern, which presumably stems from different wall colors and the black border surrounding the video, we decide to alleviate its effect. Therefore, we manipulate the model by defining an area in which the placement of rectangle features by the training algorithm is prohibited. This area is illustrated by the red region in <ref type="figure" target="#fig_8">Fig. 7 (c)</ref>. Cycle 4: After retraining the classifier, we investigate the training state of the classifier (cf. <ref type="figure">Fig. 8</ref>). It turns out that there is a decay of the number of negative examples rejected by the final cascade nodes, whereas the false positive rate determined by the seen training samples increases. Since this can be seen as stopping criterion, we terminate the training process at this point. <ref type="figure">Figure 8</ref>: Since the number of rejected negatives decrease, while the false positive rate increases in the final cascade nodes, we deem the training process to be finished.</p><p>Discussion: Comparison of the performance of inter-active learning with random sampling and uncertainty sampling in <ref type="figure">Fig. 9</ref> reveals the benefit of direct knowledge integration by inter-active learning in the context of ad-hoc classifier training. By only labeling about 100 additional data instances, we were able to achieve within 4 cycles a classification performance that is comparable to the results of the other learning methods that settle down with the number of data instances exceeding 500-600 samples. After a first cycle of initial training, <ref type="figure">Fig. 9</ref> shows that the annotation of 50 false positive samples in Cycle 2 successfully decreases the false positive rate. However, also the number of true positive detections is reduced, since just negative samples were considered. Model manipulation and additional data annotation in the third cycle finally provide us with a performance comparable to the best achievable results for this complex problem. This means for better results, a more powerful model has to been chosen. Further, <ref type="figure">Fig. 9</ref> reveals a possible problem of uncertainty sampling in ad-hoc training situations. Its true positive rate suddenly drops when querying labels for the most uncertain (i.e., near the decision boundary) data instances. Since this behavior is only observable in the context of uncertainty sampling, we presume this effect to originate from the query func-  <ref type="figure">Figure 9</ref>: Comparison of the performance of inter-active learning, random sampling, and uncertainty sampling with respect to the number of annotated data instances. Inter-active learning requires only about 100 additional labels to achieve a similar performance limit as the other methods achieve, when using 500-600 labels.</p><p>tion that introduces a bias to the distribution of data instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have introduced inter-active learning, a method that extends active learning to a visual analytics process in order to define filters by ad-hoc training classifiers. We have outlined the major challenges of ad-hoc training and presented a way to master them by tightly coupling human expertise with machine learning. The main aspects of our approach are: the quality assessment and model understanding by explorative visualization, the integration of experts' background knowledge by data annotation and model manipulation, and the use of automatic methods to support the users in refining the classifier model.</p><p>We have been able to demonstrate the power of our method by a usage scenario in which we have compared inter-active learning with active learning and passive learning (random sampling of training data). The usage scenario has exhibited the advantages and possibilities inter-active learning provides.</p><p>Although we have been able to show the benefits of inter-active learning, more research is required to investigate the extent of practical applications and to judge its efficiency in realistic scenarios. A question that arises in this context is the required proficiency of the human experts. Is domain knowledge sufficient to utilize interactive learning, or to which extent are skills in machine learning necessary? Also, further development of new, and improvement of existing, components for inter-active learning is required, especially for scalable visualizations, classifier models, and integration of automatic methods and techniques for semi-supervised learning. We have demonstrated inter-active learning in the context of video visual analytics; it remains an open question how successfully it can be applied to other domains. This would also require a more comprehensive evaluation of inter-active learning.</p><p>As we see, various exciting directions are open for future research, including the inspection of the bias introduced by interactive learning into the classifier model, such as annotation bias by using cluster-based labeling tools, as well as the integration of fuzzy model modification and data annotation that account for the uncertainty of the human expert.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Data flow of passive supervised machine learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>o t a t io n m o d if ic a t io n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Major components and information flow involved in the inter-active learning process. Solid lines depict information conveyed to the users by visualization, dashed lines represent user interaction, and dotted lines illustrate the flow of information triggered by automatic methods. Furthermore, the contribution of noise-affected data and labels to the model's uncertainty is depicted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Six prototypical rectangle features and a cropped part of a sliding window containing a detected person onto whom one of the rectangle features superimposed. (b) Each node of the cascade of classifiers makes a binary decision on the sliding window: whether it is dropped or processed by the next classifier node. Sliding windows that process the whole cascade are considered to be detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>indicates the reciprocal of the influence of signal noise on the decision of the strong classifier N k . The robustness of the feature weights r weight k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Typical workspace of our visual analytics system for inter-active learning after learning with a couple of training examples: (a) cascaded scatterplot, (b) cascade information, (c) selection interface, (d) video context view, (e) visualization of classifier model, (f) annotation view. Details of the components can be found in Section 6. of [−1, 1], where c k (s) = −1 and c k (s) = 1 represent confident decisions, either negative or positive according the class membership of the data instance:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Selection of the initial training set for bootstrapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>(a) Visual feedback of the classifier's performance after bootstrapping with 25 positive and 50 negative examples. The high number of false positive detections is indicated by the large amount of positively classified data points (green dots) in the last cascade node. (b) Selection of data points that are close and far from the decision boundary. (c) The distribution of sliding windows classified as people (green rectangles). (d) Labeling of false positive data instances in the annotation view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Model inspection and manipulation during the 3rd cycle. (a) The mean image of false positive examples exhibits a black edge pattern of patterns at the left border that is superimposed by rectangle features that response to this pattern. (b) The response map confirms the impact of the left boundary region to person classification. (c) Model manipulation prevents features to be aligned with the edge pattern, since it is deemed to be not relevant for people detection (integration of background knowledge).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). This issue is further intensified in data domains where no clear data instances are</figDesc><table><row><cell></cell><cell cols="2">Annotation</cell><cell>Labels</cell><cell>Retrain</cell><cell>Examples Bootstrap</cell></row><row><cell>Data</cell><cell></cell><cell></cell><cell>L</cell><cell></cell></row><row><cell>Labels</cell><cell>Learner Model</cell><cell cols="2">Understand Manipulate Select</cell><cell>Learner Model</cell><cell>Data</cell></row><row><cell></cell><cell></cell><cell></cell><cell>U</cell><cell>Select</cell></row><row><cell cols="2">IEEE Conference on Visual Analytics Science and Technology 2012</cell><cell></cell><cell></cell><cell></cell></row><row><cell>October 14 -19, Seattle, WA, USA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>978-1-4673-4753-2/12/$31.00 ©2012 IEEE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was funded by German Research Foundation (DFG) by the Priority Program "Scalable Visual Analytics" (SPP 1335).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards an effective cooperation of the user and the computer for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">editors, Information visualization in data mining and knowledge discovery, chapter 18</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sommerfield</surname></persName>
		</author>
		<editor>U. Fayyad, G. G. Grinstein, and A. Wierse,</editor>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="237" to="249" />
		</imprint>
	</monogr>
	<note>Visualizing the simple bayesian classifier</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surveying the complementary role of automatic data analysis and visualization in knowledge discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaining insights into support vector machine pattern classifiers using projection-based tour methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="251" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Top 10 unsolved information visualization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">What artificial experts can and cannot do</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dreyfus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>AI &amp; Society</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fails</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olsen</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent User Interfaces</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computational Learning Theory</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selective sampling using the query by committee algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="133" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active learning for anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gasperin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT Workshop on Active Learning for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On-line boosting and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative batch mode active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual classifier training for text document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics (Proceedings Visual Analytics Science and Technology 2012)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uncertainty-aware video visual analytics of tracked moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Spatial Information Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="87" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mastering The Information Age-Solving Problems with Visual Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mansmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Challenges in visual data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mansmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneidewind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ziegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Visualization (IV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards closing the analysis gap: Visual generation of decision supporting schemes from raw data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="911" to="918" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual exploration of classification models for risk assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Migut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the usability of interfaces for the interactive semiautomatic labeling of large image data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Möhrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Computer Interaction. Design and Development Approaches</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="618" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A literature survey of active machine learning in the context of natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Olsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Swedish Institute of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Balancing exploration and exploitation: A new algorithm for active machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Osugi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online bagging and boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Oza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2340" to="2345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Active sampling for class probability estimation and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saar-Tsechansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="153" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">User-based active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining Workshops</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Classifier hypothesis generation using visual analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sabol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Networked Digital Technologies</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="98" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<idno>1648</idno>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Computer Sciences Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1467" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic queries for visual information seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="70" to="77" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inventing discovery tools: Combining information visualization with data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="12" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Uncertainty sampling and transductive experimental design for active dual supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="953" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">EnsembleMatrix: Interactive visualization to support machine learning with multiple classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1283" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Verification and validation of neural networks: A sampling of research in progress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Darrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moats</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics (SPIE)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5103</biblScope>
			<biblScope unit="page" from="8" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PaintingClass: Interactive construction, visualization and exploration of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="667" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Illuminating the Path: The Research and Development Agenda for Visual Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A web survey on the use of active learning to support annotation of text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT Workshop on Active Learning for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On-line boosted cascade for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Visentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Snidaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Active learning in multimedia annotation and retrieval: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<idno>10:1-10:21</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Visual Thinking for Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interactive machine learning: Letting users build classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
