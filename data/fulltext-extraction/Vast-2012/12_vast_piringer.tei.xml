<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AlVis: Situation Awareness in the Surveillance of Road Tunnels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Piringer</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Benedik</surname></persName>
							<email>rudolf.benedik@kapsch.net</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapsch</forename><surname>Trafficcom</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">VRVis Research Center Matthias Buchetics † VRVis Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AlVis: Situation Awareness in the Surveillance of Road Tunnels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>K</term>
					<term>6</term>
					<term>1 [Management of Computing and Information Systems]: Project and People Management-Life Cycle</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In the surveillance of road tunnels, video data plays an important role for a detailed inspection and as an input to systems for an automated detection of incidents. In disaster scenarios like major accidents, however, the increased amount of detected incidents may lead to situations where human operators lose a sense of the overall meaning of that data, a problem commonly known as a lack of situation awareness. The primary contribution of this paper is a design study of AlVis, a system designed to increase situation awareness in the surveillance of road tunnels. The design of AlVis is based on a simplified tunnel model which enables an overview of the spatiotemporal development of scenarios in real-time. The visualization explicitly represents the present state, the history, and predictions of potential future developments. Concepts for situation-sensitive prioritization of information ensure scalability from normal operation to major disaster scenarios. The visualization enables an intuitive access to live and historic video for any point in time and space. We illustrate AlVis by means of a scenario and report qualitative feedback by tunnel experts and operators. This feedback suggests that AlVis is suitable to save time in recognizing dangerous situations and helps to maintain an overview in complex disaster scenarios.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In many types of large-scale control systems, the increased amount of sensed information may lead to situations where human operators lose a sense of the information's overall meaning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>. This problem is known as a lack of situation awareness, which is agreed as an important requirement for a timely and correct resolution and a pro-active prevention of critical situations <ref type="bibr" target="#b8">[9]</ref>. This paper addresses the application domain of road tunnel surveillance. Several aspects characterize tunnels as a critical type of infrastructure: The confined space limits the possibilities of rescue in case of emergencies and accidents while it drastically exacerbates the consequences of fire and smoke. Moreover, tunnels often represent the only option for travel and an obstruction severly restricts the reachability of entire regions. An appropriate surveillance of road tunnels is thus an important aspect of road safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Task Analysis</head><p>The approach decribed in this paper is the result of a tight collaboration with a company providing equipment for a video-based surveillance of road tunnels. For two years, we have been in permanent contact with experts in building and operating tunnels. As a first step, we identified the subsequent list of tasks of tunnel operators:</p><p>• Video-based traffic surveillance.</p><p>• Close observation of particular vehicles (e.g., trucks transporting hazardous materials).</p><p>• Support in case of accidents and emergencies in the tunnel as well as alerting fire and rescue services. • Traffic control (e.g., closing the tunnel or limiting the maximal speed). • Operation and maintenance of technical equipment inside and outside the tunnel. • Retrospective analysis of incidents and compilation of reports and archiving material thereof. • Training for disaster scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Current Challenges</head><p>The surveillance and operation of a tunnel is based on a variety of sensors of different kinds which permanently acquire data about the traffic situation, environmental aspects (e.g., wind speed), and the state of technical equipment. Amongst those sensors, video cameras play an important role as a source of live video and as an input for image processing systems that analyze the video to automatically detect potentially dangerous situations like smoke, wrongway drivers, and obstacles on the road. However, especially in disaster scenarios like major accidents, these systems tend to generate numerous events within a short time.</p><p>Currently, this information is presented to operators using the limited visualization capabilities of Supervisory Control And Data Acquisition (SCADA) systems <ref type="bibr" target="#b22">[23]</ref>, as also widely used in the automation industry. Visual representations are typically restricted to several red/green state indicators which are distributed in a simplified sketch of the tunnel, as well as an ordered list of events. Tunnel operators reported that such interfaces do not scale to extreme situations when avalanches of incoming messages and alarms often hide the actual reasons of the problem -with potentially fatal consequences. In particular, current SCADA visualizations in tunnel surveillance suffer from the subsequent shortcomings limiting situation awareness:</p><p>• Little support is provided to prioritize information in a situation-sensitive way. For example, the detection of a pedestrian might be critical information during normal operation, while it might be of minor importance during a major disaster scenario. Important information may thus be missed. • It is difficult to get a sense of the temporal development of a scenario which makes it hard to infer causalities and to make predictions. • Efficient access to video is restricted to live observations while it is cumbersome to retrieve video material of a few minutes ago -a relevant task, if the live video is useless due to the expansion of smoke, for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Design Goals</head><p>Based on these observations, we defined the subsequent list of design goals for an interactive visual approach for a situation-aware surveillance of road tunnels:</p><p>• G1 Overview of the spatio-temporal development of scenarios and to support predictions. • G2 Situation-sensitive prioritization of displayed information in order to scale from a normal operation up to major disaster scenarios. • G3 Instantaneous access to live and historic video material.</p><p>• G4 Simplicity to ensure usability also in stress situations.</p><p>• G5 Applicability to tunnels of different size and scenarios of different temporal scales. • G6 Extensibility to additional incident types and intelligent means of pre-processing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Contributions</head><p>The primary contribution of this paper is a design study of AlVis (which stands for "Alert Visualization"), a system designed to enable a situation-aware surveillance of road tunnels. To the best of our knowledge, this application area has not been explored before in the context of Visual Analytics. We describe a use case of AlVis by means of a disaster scenario and we report initial qualitative feedback by tunnel experts and operators. Secondary contributions include novel concepts for situation-sensitive event prioritization and concepts for video-retrieval from a spatio-temporal domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>A widely accepted definition of situation awareness (SA) by Endsley is "the perception of elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future" <ref type="bibr" target="#b8">[9]</ref>. Lacking or inadequate SA has been identified as a primary factor in accidents attributed to human error <ref type="bibr" target="#b29">[30]</ref>. SA has thus been studied intensively in fields like aviation <ref type="bibr" target="#b14">[15]</ref>, military command and control operations <ref type="bibr" target="#b12">[13]</ref>, and power plant management <ref type="bibr" target="#b9">[10]</ref>. As an example which is also related to tunnel surveillance, Baumgartner et al. <ref type="bibr" target="#b6">[7]</ref> address SA using an ontology-driven information system. Greitzer et al. <ref type="bibr" target="#b13">[14]</ref> argue that a sensemaking approach to SA should focus on the process of constructing a plausible story of what is going on.</p><p>In the context of Visual Analytics, the emphasis on designing systems for first responders and cognitively-loaded users makes SA an important topic <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b21">22]</ref>. As one example, Aragon et al. <ref type="bibr" target="#b5">[6]</ref> presented a system to maintain SA in astrophysics. Tesone and Goodall <ref type="bibr" target="#b36">[37]</ref> described a data management strategy to achieve SA even for massive data sets. Recently, MacEachren et al. <ref type="bibr" target="#b27">[28]</ref> presented a geovisual analytics approach to SA for crisis management by leveraging geographic information contained in Twitter feeds. Their visual interface methods are designed to understand the spatio-temporal and topical components of evolving situations.</p><p>In general, making sense of spatio-temporal data is a key aspect of SA. There is an abundant literature on spatio-temporal data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> as well as surveys focussing on specific types of spatio-temporal data like movements <ref type="bibr" target="#b1">[2]</ref> and geo-visual analytics <ref type="bibr" target="#b32">[33]</ref>. A key distinction of most visualization approaches of spatio-temporal event data is the encoding of time. Gatalsky et al. <ref type="bibr" target="#b10">[11]</ref> employ a third dimension to transform a 2D map to a space-time cube. The advantage of representing space and time in a continuous and compact way comes at the cost of incurring known problems of 3D visualizations regarding perception and interaction <ref type="bibr" target="#b33">[34]</ref>. In a similar approach, Kapler and Wright <ref type="bibr" target="#b19">[20]</ref> address this issue by linked time charts. Livnat et al. <ref type="bibr" target="#b26">[27]</ref> encode the temporal dimension as the radius of a circle surrounding a representation of space. Their design focusses on the premise that events have where, what, and when attributes in the context of SA. However, relying on links, encoding space and time for a large number of events is likely to suffer from significant clutter. Other approaches employ controls to restrict the visible time span <ref type="bibr" target="#b4">[5]</ref> and multiple coordinated views to link overviews of time and space <ref type="bibr" target="#b17">[18]</ref>. However, all of these approaches consider at least two spatial dimensions as equally important. In contrast, the spatial dimensions have very different semantics for tunnels, as will be discussed in Sec. <ref type="bibr" target="#b3">4</ref>.</p><p>As another aspect of our work, the integration of video data is important in Visual Analytics especially in the context of surveillance tasks. There has been much work on video indexing and queries for automatic retrieval in general <ref type="bibr" target="#b25">[26]</ref> and in the context of surveillance videos <ref type="bibr" target="#b24">[25]</ref>. A specific problem that has attracted much attention is the visualization and tracking of activity. The challenge is to deal with a large number of video sources covering different areas in space. In their system for analyzing collections of audio and video recordings, Kubat et al. <ref type="bibr" target="#b23">[24]</ref> display one video channel at full resolution and up to ten video channels at reduced resolution. Höferlin et al. <ref type="bibr" target="#b16">[17]</ref> superimpose video sequences by extracted trajectory data which may also be filtered by their relevance. The same authors also support SA of CCTV operators by complementing the video display by an auditory display <ref type="bibr" target="#b15">[16]</ref>. <ref type="bibr">Romero et al.</ref> [32] present a system for activity analysis based on analyzing and visualizing overhead videos. More closely related to our challenges, Ivanov et al. <ref type="bibr" target="#b17">[18]</ref> combine a simultaneous presentation of time and space with access to video material for surveillance of an office building. Analysts may issue spatio-temporal queries on motion sensor data to track persons. While we also take a space-centric approach, we distinguish between multiple types of events located in a spatial domain that is inherently different from office buildings. Moreover, we focus on SA during live surveillance rather than on the mere analysis of historic data.</p><p>Only little research has addressed the combination of video and spatial representations to support surveillance tasks. As two exceptions, Girgensohn et al. <ref type="bibr" target="#b11">[12]</ref> studied ways to present a geographic context when tracking activity between cameras on a 2D map. Wang et al. <ref type="bibr" target="#b38">[39]</ref> analyzed the design space of contextualized video with a focus on integration in 3D models. Both works indicate a significant potential for a tight integration of video and spatial context, but also show that respective approaches depend heavily on the representation of space. There is no work addressing a spatio-temporal selection of both live and historic video in emergency situations in general and no approaches dealing with the special properties of tunnel-like structures in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SURVEILLANCE INFRASTRUCTURE AND DATA</head><p>This paper focusses on visualization and interaction concepts of AlVis, a novel system designed for the surveillance of road tunnels. Before discussing design decisions in Sec. 4, this section first describes the hardware and software infrastructure for tunnel surveillance. In particular, we focus on the data acquired from this infrastructure. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates key components of the architecture and the data flow. The primary data source is an array of video cameras surveilling the situation in the tunnel. Cameras are typically installed each 80 to 100 meters at the top of the tunnel and there are additional cameras at the tunnel portal and at critical spots like emergency exits. The surveillance of long tunnels may thus require several dozens or even more than a hundred of cameras. Most cameras for traffic surveillance cover all lanes, typically watching the traffic from behind in one-way tunnels.</p><p>The material acquired from the cameras has three purposes: First, each camera enables operators a live view of a certain part of the tunnel. Second, video material is stored for a certain amount of time for documentation and an ex post analysis of incidents. Video without incidents is typically stored for 72h while material covering incidents is automatically archived without temporal limitation. Third, the live video is permanently analyzed to detect incidents in real-time. The algorithms and details of the Incident Detection System (IDS) are the know-how of our company partner Kapsch TrafficCom AG and beyond the scope of this paper. However, we provide some basic information about the events themselves originating from IDS (called "IDS events") as necessary for understanding the design of AlVis.</p><p>One can distinguish between stateful and stateless types of IDS events. Stateful events indicate state changes and are mostly binary, i.e., the state is either on or off. Example of stateful IDS events include the detection of smoke, traffic congestion, lost cargo, pedes- trians on the road, wrong-way drivers, occupied breakdown bays, and the loss of a video signal. Stateless events indicate a single occurence of an incident. The most important stateless IDS event is indicating a vehicle passing a particular camera. Such events are called "speed events" and account for the vast majority of all events -basically every vehicle moving through the tunnel is generating as many speed events as the number of cameras installed for traffic surveillance. In contrast to most other types of IDS events, speed events are thus rarely relevant as such but only under certain conditions (e.g., for exceptionally fast or slow moving vehicles). They also constitute a basis for computing traffic statistics.</p><p>All types of IDS events share some properties which are relevant for subsequent processing and visualization. All IDS events have a certain timestamp and refer to a particular camera. The position of the camera is the only information about the spatial position of IDS events -the current version of IDS does not provide a more detailed spatial resolution. Some types of IDS events have additional information like the lane or an estimation of the speed of a passing vehicle. Moreover, the detection operates independently for each camera, i.e., IDS events do not have relationships to other events. Extracting higher level information like vehicle trajectories or the expansion of smoke thus requires additional logic on top of the IDS.</p><p>Providing this logic is the main task of a component referred to as AlVis server. In addition to IDS events, this server also receives data from a SCADA system. Examples include values from additional sensors (e.g., wind speed) and actions of the operators like changes of the tunnel configuration (e.g., a partial tunnel closure). Conceptually, the server receives IDS and SCADA events (summarized as raw events) in real-time, performs different processing steps, and makes the result as well as the raw events available to one or more AlVis clients, as described in Sec. 4. The processing of raw events by the AlVis server can be categorized as follows:</p><p>• Enrichment describes the augmentation of raw events by derived information. Examples include classifications (e.g., classify speed events as fast or slow vehicles), relationships (e.g., relate multiple events indicating a wrong-way driver to build a trajectory), and plausibilities (e.g., depending on the relative frequency of occurrence).</p><p>• Aggregation refers to the introduction of new events that summarize multiple raw events. For example, while each camera detects the presence of smoke separately, an aggregated smoke event describes the entire area covered by smoke.</p><p>• Prediction creates events whose occurence is in the future. Examples include an expected expansion of smoke and an estimation when the tunnel will be empty based on the current traffic. Predictions can be updated as time elapses. Algorithmic details for event processing are beyond the scope of this paper. In general, however, the respective computations should be possible (almost) in real-time which limits their potential complexity. During live surveillance, the server sends updated data to its clients one time per second. Decoupling data processing from further steps (e.g., visualization) enables to make the processed data available to multiple clients. In our context, these clients are the SCADA system and one or more AlVis clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUAL TUNNEL SURVEILLANCE</head><p>This section describes our visualization approach and discusses design decisions in the specific application context of tunnel surveillance. The visualization approach is implemented as a software component referred to as AlVis client. <ref type="figure" target="#fig_1">Fig. 2</ref> provides an overview and a brief summary of its main parts which are described in detail in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spatio-Temporal View</head><p>The Spatio-Temporal View (STV) is the core of the AlVis client with respect to an efficient access to video (see Sec. 4.3) and to visualizing where, what, and when attributes <ref type="bibr" target="#b26">[27]</ref> of event data as streamed from the AlVis server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Basic Layout</head><p>The key idea of the layout is to simplify a tunnel to a straight tube. As a justification, an exact representation of detailed shape information like curves and slopes is neither relevant for surveillance nor provided by visual representations in current SCADA systems. This simplification is thus accepted by operators.</p><p>Any 2D point of the tunnel can thus be described in terms of two orthogonal dimensions: The distance from one portal along the center of the road and the distance from the center across the road. Viewing the tunnel as a respectively parameterized shape is already important during tunnel construction and the dimensions "along" and "across" are thus defined and well-known. The benefit of employing this space instead of the Euclidean space for positioning is to exploit the different extents and the different degrees of importance of the dimensions. The position along a tunnel is essential for selection of cameras covering a certain area and also for locating incidents. The position across the road is generally of minor importance. As one reason, this dimension has no relevance for camera selection since most cameras for traffic surveillance cover all lanes. If at all, the lane of an incident is relevant as a categorical information rather than the exact position across the road.</p><p>Building on this notion of space, the layout of the STV consists of three parts dedicated to the present state, the past, and the future, respectively. The parts share a common X axis representing the position along the tunnel. In general, the X axis is proportionate to reality. However, we also support spatial focus + context by a piecewise linear magnification of a certain spatial interval based on the concept of bifocal displays <ref type="bibr" target="#b35">[36]</ref>. Clicking on a movable interval enables or disables the distortion by magnifying the interval by a pre-defined constant factor. This distortion was considered helpful especially for long tunnels where focussing on a certain area is necessary while the entire tunnel must always stay visible. In addition to the screen space representing actual locations along the tunnel, some horizontal space is reserved for information without an explicit spatial reference, e.g., state changes like tunnel closure. The part dedicated to the present state displays a simplified map of the tunnel and is subsequently referred to as the Present View. This part thus also visualizes the space across the road. Within the Present View, the scaling of the Y axis is typically not proportionate to reality and different from the X axis. Key elements of the Present View include areas like driving lanes, breakdown bays, and emergency exits. If required for a particular tunnel, the Present View may also show additional information like the driving direction per lane or the position of traffic signs.</p><p>The parts dedicated to the past and the future display a spatiotemporal continuum with time being encoded as the Y axis. These parts are subsequently referred to as History View <ref type="figure" target="#fig_1">(Fig. 2b</ref>) and Future View <ref type="figure" target="#fig_1">(Fig. 2c</ref>), respectively. Per default, the History View covers three minutes and the Future View covers one minute, but these temporal ranges can be adjusted as described in Sec. 4.4. Discussion among our target users has indicated a preference for time moving from top to bottom, i.e., a point in time is entering the display at the top and moves downwards to leave it at the bottom (see Sec. 7). Consequently, the Future View is located above the Present View while the History View is located below. Based on initial interviews of target users, it was an informed design decision not to represent positions across the tunnel (i.e., the second spatial dimension) in the History View and the Future View. The benefit is to cover space and time within a simple 2D layout that avoids problems of other spatio-temporal visualizations (see Sec. 2) and enables an intuitive navigation for video selection (see Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Visual Encoding of Incidents</head><p>The layout defines the mapping from points in time and space to coordinates in screen space. This section describes, how different types of incidents are represented visually. In our context the term "incident" comprises a coherent happening within the tunnel of a specific type. Incidents may or may not have a duration and are thus defined more broadly than "events" which always refers to a single point in time without duration.</p><p>We first introduce a classification which abstracts from specific semantics of incidents. A key advantage of this classification is the ease at which new types of incidents can be integrated in the visualization by classifying them as a particular type of information primitive. Building on the characterization by Aigner et al. <ref type="bibr" target="#b0">[1]</ref>, we discriminate between points in time and intervals in time. With regards to space, we discriminate between information without spatial reference, points in space, and regions in space. For intervals in time, spatial points and regions can be further distinguished as constant or moving. It should be noted that the distinction between points in space and regions in space is based on the question whether an incident may span multiple adjacent cameras (e.g., smoke) rather than on the actual extents. The subsequent list summarizes combinations that occur in our application context and provides examples:</p><p>• Point in time and point in space: a single detection of a passing vehicle (i.e., a speed event). Areas indicate regions in space and intervals in time. In the Present View, the incident type also specifies whether an area covers a single lane (e.g., traffic congestion) or stretches across the tunnel width (e.g., smoke). In the History View and Future View, areas become bars for constant regions in space and may have curved boundaries for moving regions in space.</p><p>Icons have two purposes: First, icons indicate points in time and points in space. Second, icons describe the incident type by displaying a unique symbol for each type. To this extent, icons are displayed at the beginning and at the end of intervals in time, i.e., strokes and areas. For points in time without spatial reference, a horizontal line crossing the entire X axis emphasizes the global semantics in the History View and the Future View. In the Present View, icons indicate the presence of incidents. For incidents without duration (i.e., events), icons are displayed for three seconds. For vertical placement in the Present View, icons are displayed at the center of the respective part of the tunnel, e.g., the lane, the breakdown bay, the camera, or the vertical center of the tunnel. Vertical shifting avoids occlusion in the case when multiple icons would be placed at the same position.</p><p>The visual appearance of strokes, areas, and icons is determined by the priority of the respective incident type. The assignment of incident types to priority levels is not fixed but depends on the current situation and may be changed interactively by the operator (see Sec. 4.2). In collaboration with domain experts, we identified a four-level prioritization scheme which is defined as follows:</p><p>1. Critical incidents potentially require immediate action by the operator and must be easily perceptible. 2. Relevant incidents are considered important for a holistic assessment of the current situation but might not require immediate action. 3. Contextual incidents refer to happenings which are considered relevant during normal operation but are of minor importance in the current situation, e.g., in disaster scenarios. 4. Irrelevant incidents are considered uninteresting for the current situation and should not be displayed at all. In our design, the priority level affects visual attributes as well as the drawing order of information primitives. Basically, each incident type could interpret the prioritization scheme individually for its visual appearance (compare to Matkovic et al. <ref type="bibr" target="#b28">[29]</ref> for process visualization). In order to increase consistency, however, the visualization of the information primitives adheres to certain guidelines.</p><p>For icons and strokes, color and size are used to discriminate critical from relevant information. We use red to indicate critical incidents and a dark gray for relevant incidents. Icons of critical incidents are larger in size while the shown symbols are identical for both levels. Icons of contextual incidents are collapsed to points which just show the presence of the respective information but not the incident type. Lines of contextual incidents are partly transparent. Areas are handled differently: The opacity reflects different priority levels while the hue of an area uniquely identifies the incident type independent of the priority level. As a justification, only very few incident types are encoded as area in practice -currently only smoke, traffic congestion, and camera signal loss -and for them, a distinction by hue was considered most intuitive. With respect to the drawing order, icons are drawn on top of strokes, which are drawn on top of areas. Within one class, visual primitives of higher priority are drawn on top of those with a lower priority.</p><p>In addition to a visual encoding of information, some situations require an access to precise details. Clicking on visual representations of incidents displays details on demand in a separate view (see <ref type="figure" target="#fig_1">Fig. 2h</ref>). Depending on the incident type, these details may include attributes which are not visualized by graphical primitives, e.g., the speed as measured by the Incident Detection System. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Situation-Aware Prioritization</head><p>The importance of particular incidents is highly situation-sensitive. AlVis supports manual, automatic, and interative prioritization of incidents according to the four-level scheme as introduced in Sec. 4.1.2. For manual prioritization, the operator can select a particular pre-defined configuration within a combo-box any time. Such configurations define the priority levels for all incident types and are thus an efficient way of prioritization.</p><p>In stress situations, however, no interaction should be required at all. Selecting "auto" as configuration enables an automatic prioritization which is based on rules. For each incident type, an ordered set of rules can be defined. Each rule represents a query of the current situation and assigns a particular priority level. The evaluation is triggered at the occurence of new events and processes rule by rule until a rule is applicable -the remaining rules are ignored. The last rule is often applicable in all situations and thus defines a default priority in cases when no other rule applies. As an example, a simple set of two rules for a pedestrian could look as follows:</p><p>1. If traffic congestion then priority = relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Priority = critical.</head><p>An appropriate set of rules is typically defined iteratively during a setup phase based on feedback from the operator. In order to enable a flexible prioritization also during operation, we provide an interactive way of prioritizing information.</p><p>To this extent, we designed a widget called Prioritization Legend (see <ref type="figure" target="#fig_3">Fig. 3</ref>). The Prioritization Legend is structured in rows where each row corresponds to one priority level and displays an icon for each incident type which is currently assigned the respective priority. Only incident types are shown which occur at least once in the time covered by the Temporal Overview (see Sec. 4.4). Icons of new incident types are added at the right-most side and gaps are filled by shifting icons leftwards. We call the widget a legend, as one of its benefits is to provide a compact overview of all incident types involved in the current situation. As another benefit, simple drag-and-drop of an icon enables an efficient re-prioritization of the corresponding incident type. However, it was a requirement that certain incident types must always appear as "critical" (e.g., wrong-way drivers). Respective icons are indicated by a red triangle and can not be dragged.</p><p>The Prioritization Legend also supports to filter information. While the priority level 'irrelevant' is invisible by definition, small arrows at the left border of the Prioritization Legend also enable to set the threshold for visible information to exclude 'contextual' or even 'relevant' incidents. A dashed pattern indicates priority levels as hidden. A single click thus temporarily reduces the visualization to show only the most essential incidents. As another option to focus on a certain type of information, hovering over a particular icon highlights all occurances of the respective incident type in the STV by orange circles. This type of focus facilitates a spatio-temporal localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Context-Sensitive Video Retrieval</head><p>The visualization of incidents in the STV conveys an overview of the past, present, and future situation in the tunnel. However, a direct access to video material is still necessary for a detailed inspection and a human-oriented assessment of the correctness of incident detections. An efficient access to both live and historic video material was thus an important requirement (design goal G3). Our design supports video access on two levels: A context-sensitive video cursor, and tightly coupled video players.</p><p>The purpose of the video cursor is to provide a quick preview of available video material for a particular point in time and space as represented in the STV. Our goal was to keep the focus of attention on the navigation of time and space rather than requiring the operator to repeatedly shift the attention back and forth between the STV and a separate view for video playback. The video cursor thus appears as one or more small video screens which are located next to the mouse cursor. Hovering the mouse over the Present View, the video cursor displays live video of all active cameras covering the respective location. If a spot is covered by multiple cameras, the screens are aligned horizontally. The area covered by each camera is part of the tunnel configuration.</p><p>In the History View, treating space as one-dimensional enables an intuitive navigation of time and space simulataneously by moving across the 2D view. For convenience, we also support snapping to points in time and space as indicated by icons. In general, movements along the X axis efficiently provide all video material for a particular point in time. Movements along the Y axis enable a navigation of the video history for a particular place in the tunnel. Clicking temporarily fixes the video cursor to its current spot to enable a selection of further options. As one option, the temporal context can be inspected as a film-strip metaphor (see <ref type="figure" target="#fig_1">Fig. 2e</ref>). The additional screens provide a quick overview of the temporal development in steps of five and ten seconds around the current point. For inspection using a larger resolution, each screen can be assigned to a video player using one click or drag-and-drop.</p><p>Video players are another user interface element for access to video material. Unlike the small preview provided by the video cur-sor, video players display the video in full resolution. Video players also provide common features like pausing, resuming, and defining the direction and the speed of video playback. Additionally, video players support tracing of movements as a common task in tunnel surveillance. We discriminate between manual and automatic tracing. For manual tracing, arrows at the left and the right border of the player enable to switch to the previous or next camera along the tunnel, respectively. When enabling automatic tracing, it refers to the incident type that was detected most recently at the particular camera (e.g., a wrong-way driver or a transport of hazardous material). At the occurance of the same incident type at surrounding cameras, the video player switches automatically. As another way to take advantage of the linear structure of tunnels, video players can horizontally be enlarged to also display the video for the next and the previous camera as spatial context information (see <ref type="figure" target="#fig_4">Fig. 4</ref>). This provides an efficient way to define a coherent region for video surveillance.</p><p>Video players are typically shown on one or more separate physical screens than the one which is used to display the STV. There is usually a pre-defined number of player windows and their layout is pre-configured with some flexibility, e.g., for horizontal enlargement by dragging a horizontal pane between two adjacent player windows. While most players are free for manual assignment of live or historic video as decribed above, certain players can be reserved for an automated assignment in case of the detection of incidents which are prioritized as critical.</p><p>In any case, it is an important issue to ensure a visual correspondance between each player and the respective position in time and space as shown in the STV. Our approach for supporting visual correspondance is based on color and connecting lines. Concerning color, the title bar of each video player and a circle surrounding the representation of the corresponding camera share the same unique color (see <ref type="figure" target="#fig_1">Fig. 2</ref>). There are typically less than ten video players which suggests color as an appropriate choice for discrimination <ref type="bibr" target="#b39">[40]</ref>.</p><p>As a stronger way of emphasizing the correspondance, bold lines optionally connect a player to the representation of its camera. Connection lines appear in the color of the respective video player and they are shown on top of all other information and across window boundaries (see <ref type="figure" target="#fig_4">Fig. 4</ref>), as inspired by VisLinks <ref type="bibr" target="#b7">[8]</ref>. Connection lines are either depicted on demand by hovering over a video player, or automatically for three seconds on automatic camera assignments and switches. Besides providing correspondance information, the automatic display of connection lines also draws the attention of the operator to the occurance of critical incidents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Temporal Overview + Detail</head><p>The STV typically displays only a few minutes of the past. The main reason is to ensure a sufficient temporal resolution for displaying information and for video selection. As a justification, many critical situations evolve rapidly and the past few minutes are typically most relevant for an assessment. Some situations, however, require an overview of a significantly longer period of time. As a first design option, we suggested to apply a non-linear scaling for time. However, the interviewed tunnel experts were sceptical that a non-linear scaling could be mis-interpreted in stress situations.</p><p>As an alternative, the AlVis client applies the concept of overview + detail <ref type="bibr" target="#b18">[19]</ref> with respect to time: A view called Temporal Overview displays incidents of a user-defined period of time ranging from 30 minutes up to one day (see <ref type="figure" target="#fig_5">Fig. 5</ref>). The goal of the Temporal Overview is to support a quick assessment, when particular incident types have been active. The Temporal Overview therefore discriminates different incident types by spatial encoding. A separate column is dedicated to each incident type that actually occurs in the time interval covered by the Temporal Overview, i.e., the number of columns may change over time. The spatial location of incidents is not represented in the Temporal Overview. We thus only distinguish between points in time and intervals in time as information primitives. Analogously to the STV, intervals in time are represented as vertical lines. Icons at the beginning and at the end of a line indicate the respective incident type. Unlike in the STV, these icons do not necessarily refer to the same place, i.e., the same camera. For a wrong-way driver moving through the tunnel, for example, the icons might indicate the time of entering the tunnel at one portal and leaving it at the other portal. Temporally overlapping occurances of incidents with identical types are merged for visualization.</p><p>Single points in time are encoded as icons. However, we employ color intensity to encode the density of events for some incident types which may potentially have many occurances within a short time, e.g., detections of slowly moving vehicles in case of dense traffic. Similar to a histogram, we perform binning in time. The time interval of each bin depends on the period of time covered by the entire Temporal Overview, e.g., one bin corresponds to five minutes in case of an overall period of six hours. What is considered 'maximal density', i.e., a bin drawn in black, also depends on the period covered by each bin. In general values for 'maximal density' are pre-defined for a particular tunnel in order to take tunnels of different size into account.</p><p>As a key interaction within the Temporal Overview, the operator may define the time interval to be displayed in the STV. This time interval is indicated by the background color and a line indicates the present time (see <ref type="figure" target="#fig_1">Fig. 2</ref>). The user may drag the bottom border to extend or contract the time interval of the History View. Similarly, dragging the present line affects the time shown in the Future View.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the infrastructure for video surveillance consists of multiple components which are communicating over a network. The system AlVis itself only refers to the AlVis server and the AlVis client. The remaining components are existing products of Kapsch TrafficCom AG (i.e., the systems for video storage and incident detection) or of third parties (i.e., the SCADA system). The AlVis server currently receives raw events via the HTTP protocol. The communication between the AlVis server and client is based on the messaging library YAMI4 <ref type="bibr" target="#b34">[35]</ref>. For the transmission of video, a proprietary protocol is implemented on top of UDP.</p><p>As an important requirement, both the AlVis client and server are highly configurable. All aspects which are specific to a particular tunnel are defined in XML files, e.g., the location of lanes, breakdown areas, emergency exits, and cameras. All locations are registered to match the visual representation of the tunnel as shown by the Present View. This representation is a bitmap graphic that may display any information which may be helpful, e.g., the location of traffic signs. Further configuration examples include incident types and rules for automated prioritization. In the AlVis server, the event management is separated from the logic for processing events. Interfaces enable a modular extension of new event types and logic for prediction or aggregation.</p><p>Both the AlVis server and the AlVis client are written in C#. The display of information in the AlVis client is based on the Windows Presentation Foundation (WPF) framework. Multi-threading is used for the retrieval and the play-back of video to avoid any interruption in the display of event data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SURVEILLANCE SCENARIO</head><p>This section illustrates a use case of AlVis by means of a fictional disaster scenario (see <ref type="figure" target="#fig_6">Fig. 6</ref>). As surveillance data from real disaster scenarios is highly sensitive information, this scenario has been generated in collaboration with experts in tunnel surveillance by manual editing and playing back log files in combination with appropriate historic video material. The tunnel of the scenario is 2000 meters long and consists of two lanes heading in the same direction.</p><p>Initially, regular vehicle movements are prioritized as irrelevant and are thus hidden during normal operation. One vehicle is indicated as fast-driving but eventually leaves the tunnel without causing problems. One minute later, lost cargo is suddenly detected approximately 400 meters before the tunnel exit (see <ref type="figure" target="#fig_6">Fig. 6a</ref>). This critical incident automatically assigns the live video to a video player which shows a bag on the road just after a cross-cut. Inspection of historic video using the video cursor reveals that a heavily packed truck lost one of its bags a few seconds before. The operator sets the spatial focus to the respective part of the tunnel.</p><p>While the operator is about to make a call in order to request a removal of the bag, a non-moving vehicle is detected at the same spot. The video shows that a car has crashed into the bag and has suffered minor damage. The operator closes the tunnel by pressing an alert button in the SCADA system. Tunnel closure turns all traffic lights to red. In the AlVis client, tunnel closure is indicated by crossed lanes in the Present View and a global state change in the History View. Tunnel closure also automatically re-prioritizes regular vehicle movements as relevant (see <ref type="figure" target="#fig_6">Fig. 6b</ref>). This shows that the driver of the accidental car has closed in on another car right before the cargo and was too late to switch to the other lane. More importantly, it also reveals two more vehicles within the tunnel.</p><p>Both vehicles do not notice the traffic lights and move on. The first one, a tanker, recognizes the standing vehicle too late and crashes into it despite braking sharply. While the operator informs the fire and rescue service, the driver of the tanker exits the vehicle and is represented in AlVis as a pedestrian. Video surveillance shows that the tanker has caught fire (see <ref type="figure" target="#fig_6">Fig. 6c</ref>). The expansion of smoke is also detected by the Incident Detection System and displayed in AlVis. The Future View shows that smoke is predicted to expand rapidly towards the tunnel exit. An expansion towards the opposite tunnel portal is indicated as unlikely apart from an effect called back-layering, i.e., a typically harmless layer of smoke next to the tunnel roof which is indicated by a light gray in AlVis. The operator begins to open the ventilation dampers of the tunnel via the SCADA system as a countermeasure.</p><p>The second vehicle comes to a halt before the accident and the driver exits the car too. However, as the expansion of smoke intensifies, the presumably scared driver enters the car again and becomes a wrong-way driver by heading back where he came from. Unfortunately, automated detection shows another vehicle entering the tunnel and driving fast. The video cursor immediately shows that this car belongs to the fire and rescue service. The Future View predicts a crash in less than 20 seconds (see <ref type="figure" target="#fig_6">Fig. 6d</ref>). The location and the time of the potential crash is approximated by intersecting extrapolations of the current movement of both vehicles. Projecting intervals around the measured speeds rather than single values accounts for precision-related uncertainty. Having a direct connection to the fire and rescue service, the operator is able to warn the rescue driver just in time of the oncoming wrong-way driver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FEEDBACK</head><p>Feedback about AlVis has been collected in two stages. As the first stage, a test installation of AlVis playing back historic data has been demonstrated to eleven experts in tunnel surveillance, six of them actively working as tunnel operators in different countries (e.g., Austria, Italy, the Netherlands). On average, a demonstration session took two hours including time for questions, feedback, and discussion. In this case, the system was operated by an experienced user but the experts were free to try using the system themselves after an initial demonstration. As the second stage, AlVis was deployed in the traffic control center of Munich / Germany in April 2012 for a period of two months in order to evaluate its application for surveillance of a highly frequented city tunnel on a daily basis.</p><p>The overall feedback of both stages was very positive. All experts and users agreed that information overload is a key problem in disaster scenarios and all of them considered AlVis as an effective approach to increase situation awareness and to save time in recognizing potentially dangerous situations. The most appreciated aspects of the visual design included the access to live and historic video which was commented as very intuitive, and the ability to convey the history and future development of a situation at one glance. Citing the head of the traffic control center of Munich (translated from German), "We carefully tested AlVis for two months. The system is very powerful and comprehensive. It offers a variety of useful information which is well represented and adjusts to the needs of the users. In particular the possibilities of an intuitive access to video material are a significant improvement as compared to other solutions."</p><p>In both stages, several experts considered the visualization appropriate for documentation and presentation purposes. In general, the feedback indicated that potential use cases of AlVis include presentation, analysis, and training in addition to live surveillance.</p><p>Major design decisions of AlVis were approved by the experts in both stages, e.g., the decision to simplify a tunnel as a straight tube and to treat space as one-dimensional in the History View and the Future View. In the second stage, operators commented that a representation of precise geographic positions is not relevant to them. The experts considered the prioritization scheme by means of four discrete levels as intuitive and simple enough to be helpful in stress situations.</p><p>There were very few critical comments in the first stage. Discussions concerned the direction of time, including arguments like "information should flow from top to bottom like water" versus "new information should appear at the bottom as when reading a document". It was also stressed that some incident types should always be considered critical.</p><p>The second stage was more effective in generating feedback concerning specific features. First, the user requested extensions of the logic for filtering events. Traffic jams, for example, occur on a daily basis in many city tunnels and typically should not be considered critical whereas particular occurrences of traffic jam (e.g., originating in the center of the tunnel) are of high relevance. Second, the users requested support for facilitating an export of such video material that is relevant for a particular incident. Third, it was suggested enabling a synchronized play back of multiple video players. Finally, the operators are in charge of 7 tunnels now and this number will increase to 20 or more tunnels by 2018. Therefore, an extension of AlVis to enable a simultaneous surveillance of multiple tunnels was considered highly relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND FUTURE WORK</head><p>Summarizing key aspects of AlVis explains how our design corresponds to the goals stated in Sec. 1.3: The Spatio-Temporal View (STV) provides an overview of the present situation in the tunnel, the history, and predictions of potential future developments (G1). Manual, automated, and interactive prioritization of incident types enables to adjust the amount of visual information to the current situation (G2). The video cursor and tightly coupled video players provide instantaneous access to video material for any point in time and space (G3). All user interactions which are potentially relevant in stress situations are based on hovering (e.g., the video cursor), single clicks (e.g., filtering priority levels), and dragging (e.g., modifying the displayed time period). Moreover, automated prioritization and video assignment reduce or even eliminate the need for interaction during stress situations (G4). Spatial distortion enables a focussed surveillance also for long tunnels while temporal overview + detail conveys short-termed as well as long-termed developments (G5). Classifying incidents as information primitives facilitates an integration of additional incident types. Decoupling data processing from visualization as fostered by the server-client architecture enables to modify or extend algorithms for data processing without affecting the client (G6).</p><p>Concerning scalability with respect to the number of incoming raw events, we successfully tested AlVis on common surveillance infrastructure with up to 1000 raw events per second by playing back historical data of dense traffic by a speed-up factor of ten. In practice, the vast majority of raw events are detections of passing vehicles (i.e., 'speed events'). Handling arrays of speed events rather than a single speed event for data processing and transmission solves most technical scalability limits. Concerning visual scalability, AlVis employs aggregation of raw events (e.g., for smoke and trajectories), prioritization of incident types, and focus + context approaches. However, a current limitation is the lack of event localization by the Incident Detection System -all events detected by one camera are assigned the same position. More precise position information would make better use of the horizontal visual space. Respective extensions of the Incident Detection System are planned for the future.</p><p>We consider video-based traffic surveillance an application domain which is directly in scope of Visual Analytics. The current version of AlVis addresses important respective challenges: We combine automated image processing, automated event processing, and interactive visualization in a distributed system architecture. Moreover, combining an event-based visualization with interactive access to video material is an example of handling multi-modal data that is streamed in real-time. Feedback by experts indicates that the current version of AlVis already has clear benefits as compared to former approaches for tunnel surveillance. However, there is a variety of additional issues to be addressed by future work:</p><p>• Some road tunnels have a complex topology like multiple joining access roads. As our next step, we thus plan to extend the tunnel model of AlVis to support a represention as multiple connected segments. We envision to enable multiple instances of the Spatio-Temporal View, each of them providing a detailed surveillance of one segment. An additonal visualization will provide a less detailed overview of the entire tunnel complex. Related issues include a surveillance of multiple separated tunnels and an application to traffic surveillance outside tunnels. • Uncertainty of information plays a major role in tunnel surveillance and originates from acquisition as well as prediction <ref type="bibr" target="#b30">[31]</ref>. Acquisition-related sources include incomplete video coverage, imperfect video images, wrong detection of incidents (including false positives and false negatives), and imprecise measurements (e.g., of vehicle speed). The representation of a predicted crash (see Sec. 6 and <ref type="figure" target="#fig_6">Fig. 6d</ref>) is an example of a scenario-specific visualization of uncertainty. However, a general framework for handling uncertainty in our context is an important topic of future work. • Retrospective analysis and reporting are important tasks of operators which are not yet sufficiently addressed, because the previous focus has been on tunnel surveillance in real-time. • The initial goal of AlVis was to complement the limited capabilities of current SCADA systems regarding visualization and video access and not to replace them. However, initial feedback by operators suggests that the AlVis client could also be an intuitive user interface for tunnel control. • While AlVis is designed for tunnel operators, it seems reasonable to make the visualization of the tunnel situation available to additional stakeholders, e.g., emergency units. Respective challenges include an adaptation for thin clients <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this paper we described a design study of AlVis, a system aimed for increasing situation awareness in the video-based surveillance of road tunnels. Our design is directly motivated by Endsley's three-part process of situation awareness <ref type="bibr" target="#b8">[9]</ref>: 1) A spatio-temporal view enables a perception of incidents within time and space. 2) Data processing, prioritization, and efficient access to live and historic video facilitates a comprehension of the meaning of these incidents. 3) An explicit representation of predictions provides a projection of the situation in the near future. A use case illustrated how AlVis supports decision making in tunnel surveillance in different stages of an increasingly critical scenario. Initial qualitative feedback from tunnel experts and operators suggest that AlVis saves time in recognizing potentially dangerous situations. We thus believe that AlVis can be an important step to ultimately increase the traffic safety in road tunnels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The infrastructure of AlVis for video-based tunnel surveillance. Arrows indicate the direction of data flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The AlVis client for visual tunnel surveillance: The Spatio-Temporal View consists of (a) a sketch of the present state, (b) a history of the past few minutes, and (c) predictions of future developments -in this example the expansion of smoke. (d) Situation-aware prioritization affects the visual representation of various types of incidents in the Spatio-Temporal View. (e) A video cursor enables an immediate access to live and historic video for any point in time and space within the tunnel. In this example, it also displays a temporal context as a filmstrip metaphor. (f) Additional windows for video playback employ color to visually refer to the source camera. (g) A Temporal Overview represents up to several hours and supports a navigation in time. (h) Details are provided on demand for selected incidents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Point in time without spatial reference: a change of the tunnel configuration. • Interval in time and constant point in space: the detection of lost cargo or an occupied breakdown bay. • Interval in time and moving point in space: the trajectory of a vehicle or of a pedestrian. • Interval in time and constant region in space: a region without surveillance due to the loss of a video signal. • Interval in time and moving region in space: smoke or a traffic congestion. The visual encoding of information primitives is based on strokes, areas, and icons. Strokes occur only in the History View and the Future View. They generally indicate intervals in time and points in space. Strokes become straight vertical lines for constant points in space and curves for moving points in space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The Prioritization Legend provides an overview of all incident types involved in the current situation and their assignment to the four priority levels. Interactions include re-prioritization, filtering, and selection of a particular incident type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Video surveillance of a wrong-way driver using a video player. Simultaneous access to adjacent cameras provides spatial context information. A connection line emphasizes the correspondance between the video image and the position of the respective camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The Temporal Overview focusses on the occurence of incident types over time. Vertical lines indicate intervals in time while the density of events is indicated by color intensity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Four stages of a disaster scenario: (a) The detection of lost cargo due to a truck losing one of its bags. (b) When a car has suffered minor damage from crashing into the bag, the operator closes the tunnel. (c) Smoke begins to expand when a tanker does not notice the tunnel closure and crashes into the car. (d) A successive car becomes a wrong-way driver and is heading towards an oncoming emergency unit.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work has been supported by the Austrian Funding Agency (FFG) within the scope of the COMET K1 program. Thanks go to all project participants of Kapsch TrafficCom, and to E. Gröller and J. Kehrer for valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual Methods for Analyzing Time-Oriented Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Conceptual Framework and Taxonomy of Techniques for Analyzing Movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kisilevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="232" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tominski. Space, Time and Visual Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dransch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dykes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fabrikant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1577" to="1600" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploratory Analysis of Spatial and Temporal Data: A Systematic Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer-Verlag New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards Exploratory Visualization of Spatio-Temporal Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gatalsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd AGILE Conference on Geographic Information Science</title>
		<meeting>of the 3rd AGILE Conference on Geographic Information Science</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using Visual Analytics to Maintain Situation Awareness in Astrophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Aragon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Aldering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quimby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd IEEE Symposium on Visual Analytics Science and Technology (VAST 2008)</title>
		<meeting>of the 3rd IEEE Symposium on Visual Analytics Science and Technology (VAST 2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BeAware! -Situation Awareness, the Ontology-Driven Way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gottesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Retschitzegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schwinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1181" to="1193" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">VisLink: Revealing Relationships Amongst Visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1192" to="1199" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward a Theory of Situation Awareness in Dynamic Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Endsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="32" to="64" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving Teamwork in Organization: Applications of Resource Management Training, chapter Applying Crew Resource Management in Offshore Oil Platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">E</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bowers</surname></persName>
		</author>
		<editor>&amp; E. Edens</editor>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Erlbaum</publisher>
			<biblScope unit="page" from="217" to="233" />
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive analysis of event data using space-time cube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gatalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th International Conference on Information Visualisation (IV &apos;04)</title>
		<meeting>of the 8th International Conference on Information Visualisation (IV &apos;04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effects of Presenting Geographic Context on Tracking Activity Between Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girgensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;07)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1167" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring team situation awareness in decentralized command and control environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Winner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ergonomics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1312" to="1325" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A sensemaking perspective on situation awareness in power grid operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Greitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guttromson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Power and Energy Society General Meeting -Conversion and Delivery of Electrical Energy in the 21st Century</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Defining aircrew coordination: Searching mishaps for meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hartel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at the 6th International Symposium on Aviation Psychology</title>
		<meeting><address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive Auditory Display to Support Situational Awareness in Video Surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Itnl. Conf. on Auditory Display (ICAD)</title>
		<meeting>of the Itnl. Conf. on Auditory Display (ICAD)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uncertainty-Aware Video Visual Analytics of Tracked Moving Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Spatial Information Science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing the history of living spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The information mural: A technique for displaying and navigating large information spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Jerding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="257" to="271" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kapler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wright</surname></persName>
		</author>
		<title level="m">Geo Time Information Visualization. Information Visualization</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="136" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual Analytics on Mobile Devices for Emergency Response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mellema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Collinss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd IEEE Symposium on Visual Analytics Science and Technology (VAST 2007)</title>
		<meeting>of the 2nd IEEE Symposium on Visual Analytics Science and Technology (VAST 2007)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<title level="m">Visual Analytics for the Strategic Decision Making Process. GeoSpatial Visual Analytics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="299" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Securing SCADA Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Krutz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley Publishing Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TotalRecall: Visualization and Semi-Automatic Annotation of Very Large Audio-Visual Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kubat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Decamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th International Conference on Multimodal Interfaces (ICMI &apos;07)</title>
		<meeting>of the 9th International Conference on Multimodal Interfaces (ICMI &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Surveillance Video Indexing and Retrieval Using Object Features and Semantic Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1439" to="1476" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Content-Based Multimedia Information Retrieval: State of the Art and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Communications and Applications</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual Correlation for Situational Awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Livnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization 2005</title>
		<meeting>IEEE Symposium on Information Visualization 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SensePlace2: GeoTwitter Analytics Support for Situational Awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Maceachren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pezanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savelyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blanford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Visual Analytics Science and Technology (VAST 2011)</title>
		<meeting>of the IEEE Conference on Visual Analytics Science and Technology (VAST 2011)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Process visualization with levels of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sainitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization</title>
		<meeting>IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Making sense out of teamwork errors in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bergondy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cuevas-Mesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Annual Industrial/Organizational Behavior Conference</title>
		<meeting><address><addrLine>Roanoke, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Approaches to Uncertainty Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Wittenbrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lodha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="370" to="390" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Viz-A-Vis: Toward Visualizing Video through Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Summet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1261" to="1268" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Analytical, visual, and interactive concepts for geo-visual analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Information availability in 2d and 3d displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Smallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Oonk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Cowen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Compututer Graphics &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Programming Distributed Systems with YAMI4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sobczak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Maciej Sobczak</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data base navigation: An office environment for the professional</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Spence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Apperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour and Information Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="54" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Balancing interactive data management of massive data with situational awareness through smart aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Tesone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Goodall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2007 IEEE Symposium on Visual Analytics Science and Technology (VAST &apos;07)</title>
		<meeting>of the 2007 IEEE Symposium on Visual Analytics Science and Technology (VAST &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Illuminating the Path: The Research and Development Agenda for Visual Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Contextualized Videos: Combining Videos with Environment Models to Support Situational Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Krum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1568" to="1575" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Information Visualization: Perception for Design, Second Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
