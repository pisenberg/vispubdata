<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Designing for Mobile and Immersive Visual Analytics in the Field</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Whitlock</surname></persName>
							<email>matthew.whitlock@colorado.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keke</forename><surname>Wu</surname></persName>
							<email>keke.wu@colorado.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Albers Szafir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">Albers</forename><surname>Szafir</surname></persName>
							<email>danielle.szafir@colorado.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Boulder</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Colorado Boulder</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Colorado Boulder</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Designing for Mobile and Immersive Visual Analytics in the Field</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934282</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Immersive Analytics</term>
					<term>Augmented Reality</term>
					<term>Mobile Visualization</term>
					<term>Outdoor Visualization</term>
					<term>Emergency Response</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. We explore how visualization can enhance data use in the field for domains such as earth science and emergency response. Working with field analysts, we elicit recommendations for how mobile and immersive technologies might bridge spatial and temporal gaps in data collection and analysis. We use these recommendations to develop FieldView, an extensible prototype data collection and visualization system that uses mobile overviews and situated AR visualizations to communicate data about ongoing operations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Data-oriented decision making is transforming practices in a broad variety of domains. Applications in earth science <ref type="bibr" target="#b17">[18]</ref>, geology <ref type="bibr" target="#b48">[49]</ref>, and emergency response <ref type="bibr" target="#b45">[46]</ref> all leverage data collected in the field to describe the state of complex environments. Field analysts collect data to model the changing state of field sites and share information across teams to increase situational awareness and deepen scientific and operational understanding. However, our interviews with field workers across multiple domains reveal that current practices for working with data in the field rely heavily on decoupled solutions that separate data analysis from the spatial and temporal contexts that data describes. Data is collected in the field but analyzed in operations centers or remote laboratories. Analysts report that this separation limits their abilities to use this data in on-going operations: remote analysts lack the context and local situational awareness of people in the field; people in the field lack tools to access, analyze, and act on data while in the field. This work explores challenges and opportunities for using visualization to support data analysis in the field.</p><p>The lack of support for analytics in the field means that decision making typically occurs in remote operations centers either during or after operations. Current practices for fieldwork require analysts to first preplan their operations based on data from previous collection efforts and archival data streams. Analysts then collect new data either on mobile devices or in field notebooks, physically transport data to a central location to synchronize with other sources, and replan subsequent collection efforts and operational practices based on the newly revised data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref>. This workflow decontextualizes collected data from the environment, prevents analysts from reacting to new data, and obscures possible errors in data collection by creating spatial and temporal gaps between data collection and analysis <ref type="figure" target="#fig_0">(Fig. 2)</ref>. Spatial gaps in understanding arise when remote analysts lack the physical context surrounding field data, resulting in a lack of visual grounding for remote analysis and limited shared context during real-time operations. Temporal gaps stem from an inability to analyze data during an operation, resulting in stale data being used in dynamic environments and an inability to make timely decisions. Temporal gaps could jeopardize a day's work or even the entire operation due to data quality issues and reduced global awareness of the state of the broader operation. While domain experts noted that these gaps are acceptable for some complex analyses (e.g., those requiring heavy computation or in-depth exploration), improving access to field data could transform many aspects of field practices by expanding field analysts' situational awareness.</p><p>We explore how mobile and immersive analytics tools can begin to bridge spatial and temporal gaps between data collection and analysis in fieldwork. Specifically, we examine how coupling these technologies might increase contextual and situational awareness in field practices to improve data collection, sharing, analysis, and decision making by better connecting field analysts with their data. Mobile technologies offer portable connectivity to access and synchronize data in the field  and can offer at-a-glance overviews that help summarize an operation's current state <ref type="bibr" target="#b4">[5]</ref>. However, mobile technologies suffer from several limitations in the field. For example, interacting with mobile devices is challenging while wearing gloves and forces analysts to divide their attention between the visualization and environment. Immersive visualization techniques can resolve these issues by visually embedding data in the physical environment <ref type="bibr" target="#b60">[61]</ref>; however, we have limited knowledge of how to design effective AR visualizations. Our goal in this paper is to elicit insight into how visualization tools can be designed to effectively support the needs of field data analysis to provide preliminary steps towards a deeper integration of data into field operations.</p><p>This paper presents an exploratory investigation into how visualization tools could support increased situated awareness and optimize data-driven decision making in the field. Building on preliminary conversations with field workers in domains ranging from earth science to emergency response, we created an interactive design probe architected to support an integrated data collection and visual analysis workflow combining mobile and immersive tools. We use this probe to elicit formal recommendations for visual analytics tools for field work and develop a preliminary prototype based on these recommendations. The prototype addresses three target scenarios identified during the interviews where a lack of access to field data causes significant operational challenges: team coordination, data quality validation, and integrating autonomous sensors. Our results suggest new opportunities for leveraging visualization to empower field operations through data.</p><p>Contributions: Our primary contribution is a formative qualitative study used to establish preliminary design considerations, constraints, and scenarios for integrated data collection and analysis tools in the field. We worked with scientists and public safety officials to identify challenges in current field data practices and develop a design probe demonstrating how mobile and immersive technologies might overcome these limitations. This probe scaffolded interviews with 10 domain experts about the potential of complimentary mobile and immersive visualizations to transform field operations. After synthesizing these interviews, we refined the workflow introduced in the design probe to develop FieldView, an open-source prototype system to address three critical use scenarios identified by analysts (teaming, data quality validation, and data fusion; <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Bridging the gap between data collection and analysis for field work requires understanding how contextual awareness can inform analysis and the technologies and approaches that might enable such solutions. We focus on developing methods that provide field analysts ready access to important information to improve the quality of their data collection practices, interpretation, and operational decision making. We build on work in situational awareness and contextual computing in HCI as well as mobile and immersive analytics to support these goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contextual Awareness</head><p>Current practices for analyzing field data limit contextual and situational awareness (SA). Technologies may consider many forms of SA. For example, Endesley et al. models three hierarchical phases of SA in dynamic systems <ref type="bibr" target="#b20">[21]</ref>: <ref type="bibr" target="#b0">(1)</ref> perceiving elements in the environment, (2) comprehending the active situation based on disjoint elements, and (3) projecting the future status of a situation based on the status and dynamics of the current situation. Systems can increase SA across these phases by designing for change detection, preparedness for interruption, goal reorientation, and detection of missed changes <ref type="bibr" target="#b33">[34]</ref>.</p><p>Increasing SA is especially critical in technologies for highly dynamic situations where new data frequently changes operational strategies, such as search-and-rescue and emergency response. For example, Cao et al. found that people trusted their own situated understanding over the advice of agencies in predicting wildfire spread <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr">Kim et al.</ref> increase field responder SA by visualizing location-based data on mobile devices, allowing users to "see through the fog" <ref type="bibr" target="#b35">[36]</ref>. While these solutions increase awareness of areas invisible to users, they require divided attention and map complex data to a limited visual space. We address these limitations by exploring how AR may supplement existing mobile data collection and visualization approaches, bypassing the need for device-to-environment context shifts by situating data directly within the physical environment to increase contextual awareness.</p><p>We ground our exploration of SA in visualization systems. These systems allow analysts to bring domain expertise and contextual awareness to data exploration and decision making by allowing analysts rather than algorithms to synthesize insights from available data. For example, Chan et al. uses multiple spatially aware displays to increase SA in a command center <ref type="bibr" target="#b8">[9]</ref>. Visual analytics systems may support field work by reducing reliance on verbal/radio communication, integrating information from multiple sources, and combining both streaming and manual data entry <ref type="bibr" target="#b25">[26]</ref> as well as improving collaborative teaming <ref type="bibr" target="#b36">[37]</ref>. These technologies aim to provide domain experts with sufficient knowledge about the global state of the operational environment to make data-informed decisions in dynamic environments. However, these solutions generally target a remote operator with a global view of the task at hand but limited understanding of the context of field data. We instead aim to put these visualizations in the field, considering how mobile technologies and immersive visualization can support field analysis needs while mitigating limiting factors of current mobile approaches like divided attention and limited screen size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mobile Data Collection and Visualization</head><p>Mobile devices support portable data collection and sensing across a variety of applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>. Using mobile devices for data entry can enhance data quality over traditional physical map-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b52">53]</ref>. For example, Pascoe et al. discuss user interfaces for field data collection that minimize the attention needed to use a device while maximizing benefits of contextual awareness <ref type="bibr" target="#b47">[48]</ref>. Tomlinson et al. show how mobile devices can support distributed data collection efforts <ref type="bibr" target="#b57">[58]</ref>. ESCAPE provides a middle-ware for exchanging information between mobile devices in emergency response <ref type="bibr" target="#b58">[59]</ref>. These studies show that simplicity and contextual awareness are critical components of successful field data collection.</p><p>However, analyzing data on mobile devices can be difficult due to the limited display, storage, and computation capabilities of these technologies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45]</ref>. These challenges are exacerbated in field scenarios and extended to include other challenges such as decision time and privacy concerns <ref type="bibr" target="#b21">[22]</ref>. Solutions like Siren leverage context-aware paradigms to support peer-to-peer messaging based on environmental triggers, choosing to reduce cognitive load through automated analysis based on contextual data <ref type="bibr" target="#b31">[32]</ref>. Though mobile technologies have limited display size and resolution, they readily support location-based visual analysis, including mobile tourist guides <ref type="bibr" target="#b5">[6]</ref>, overview-detail map visualizations <ref type="bibr" target="#b34">[35]</ref> and mobile device-mediated navigation tools <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref>. These visualizations generally focus on a few critical data attributes to increase analysts' abilities to get important information at a glance <ref type="bibr" target="#b59">[60]</ref>. While these systems provide useful at-a-glance overviews for general analysis or detail views for well-defined tasks, they use a small form factor that divides both data and analyst attention between the screen and the operational environment, limiting contextual integration into analysis and decision making. We build on these approaches to support location-based data collection and overview analysis and extend these approaches with immersive AR visualizations to enable deeper engagement with data. These AR visualizations blend data and context to more closely couple data exploration with the operational environment for applications ranging from forest ecology to public safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Immersive Visualization for Fieldwork</head><p>Immersive Analytics (IA) uses embodied data analysis for data understanding and decision making, generally through 3D virtual or augmented reality. IA offers opportunities that standard 2D displays do not, including benefits associated with situated analytics, embodied data exploration, collaboration, and increased engagement <ref type="bibr" target="#b42">[43]</ref>. Prior work in IA focuses on the benefits of situated analytics, especially how the use of augmented reality (AR) can enable contextually-aware data analysis by outfitting the physical environment with virtual data (see Schmalsteig &amp; Höllerer for a survey <ref type="bibr" target="#b54">[55]</ref>). These techniques can leverage data from disparate environments for situated analysis <ref type="bibr" target="#b19">[20]</ref>. Such techniques include mid-air displays for map navigation <ref type="bibr" target="#b14">[15]</ref> and cross-device displays for collaborative visualization <ref type="bibr" target="#b1">[2]</ref>. Systems have used AR to directly annotate the environment with data for applications in construction and architectural oversight <ref type="bibr" target="#b29">[30]</ref>, energy aware smart homes <ref type="bibr" target="#b30">[31]</ref>, manufacturing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref>, and situated learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b53">54]</ref>. Other systems use AR to highlight critical information along 3D surfaces including rock folds <ref type="bibr" target="#b24">[25]</ref>, mammograms <ref type="bibr" target="#b16">[17]</ref>, and physical bodies <ref type="bibr" target="#b27">[28]</ref>. In our target domain, visualization overlays on proximal referents can help highlight previously searched paths in search-and-rescue <ref type="bibr" target="#b60">[61]</ref>. These techniques offer interactive methods for engaging with spatial data that may outperform other mobile technologies such as tablets <ref type="bibr" target="#b0">[1]</ref>.</p><p>In fieldwork, these technologies may help resolve the "field map shuffle" where researchers constantly reference several potentially outof-date sources for geographical data <ref type="bibr" target="#b48">[49]</ref>. For example, McCaffrey et al. theorize how portable stereo rendering might enable field-based visualization for complex geospatial models <ref type="bibr" target="#b43">[44]</ref>. Pavlis et al. extend this vision to suggest how immersive visualization may improve map synthesis <ref type="bibr" target="#b48">[49]</ref>. However, these efforts offer purely theoretical insight into such designs. More recent works have begun to explore preliminary designs for targeted analysis problems in field research. For example, Gazcón et al. allow field researchers to augment an existing view of mountains with lines highlighting geological folding patterns <ref type="bibr" target="#b24">[25]</ref>. Ramakrishnaraja et al. visualize sensor data in AR to increase SA for oil and gas workers <ref type="bibr" target="#b50">[51]</ref>. However, these systems focus on a narrow set of well-defined analysis tasks and do not consider how field analysts can integrate incoming information. We extend these ideas to enable holistic situated analysis for field domains. We do so by first systematically understanding how immersive situated analysis in tandem with mobile devices may better support data-oriented fieldwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY REQUIREMENTS ANALYSIS</head><p>We aim to understand how visual analytics can improve field operations by bridging spatial and temporal gaps in field data usage. To characterize the problem space, we enumerate the anticipated requirements of field VA systems and identify limitations in existing approaches in preliminary unstructured interviews with four field analysts: one in public safety, two in wildland fire, and one in geological science. We synthesized challenges and requirements noted by all participants to derive four preliminary design considerations. Our design probe embodies these design considerations to elicit insight into how visual analytics can enhance field practices. While we found no studies specific to field data needs, several points raised in the interviews exemplify general data and technological challenges in emergency response.</p><p>In these interviews, analysts universally identified four primary limitations that prevent them from adequately leveraging data in the field: divided attention, small form-factors, touchscreen input, and reliance on operations centers. The first two factors echo concerns for mobile devices in GIS work <ref type="bibr" target="#b6">[7]</ref>. Analysts currently preplan daily goals based on archival data from prior operations before entering the field, using mobile devices and paper to collect new data in the field. Due to limited support for data collection in the field, experts' current data consists of digital images collected during pre-op aerial fly-overs (wildland fire and public safety), archival data printed onto paper maps (e.g., topography, structural locations, and prior survey data; all domains), simple quantitative measurements from portable sensors streamed to a portable drive (e.g., canopy density, soil acidity; geological science and wildland fire), and hand-written qualitative observations (geological science). Analysts reported only having preplanning data and handwritten observations while on a mission and had no methods for engaging in any analysis tasks in the field. They instead synchronize and analyze new data after returning to the operations center or at a remote site after leaving the field, forcing operations centers to resolve data collection inconsistencies <ref type="bibr" target="#b8">[9]</ref>. Current mobile-only solutions offer insights into how data could inform field practices, but require analysts to use notes and memories to integrate environmental context back into their analyses. Mobile visualizations offer a potential alternative, but divide attention between the display and the operational environment, introducing hazards similar to texting and driving <ref type="bibr" target="#b37">[38]</ref>.</p><p>These limitations lead to heavier reliance on external analysis at an operations center or field site, but limited connectivity can cause information loss and substantial delays in data exchange. Operations centers also lack the contextual awareness of field analysts as noted in Flentge et. al. <ref type="bibr" target="#b21">[22]</ref>, while field analysts lack the awareness of multiple fieldsites granted to remote operators as noted in Kim et. al. <ref type="bibr" target="#b35">[36]</ref>. As a result, field analysts enumerated three options for using data in current practices: operate agnostic of field collected data, use stale data from daily preplanning, or rely on strictly verbal communications from the operations center. All analysts felt that visual analytics tools could transform these options by increasing situational awareness in the field. The discussions made clear that field practices offer a rich design space that differs from conventional visual analytics tools. However, analysts had a limited sense of the capabilities of current devices for situated analysis and had difficulty reasoning about how these technologies might best augment their workflows. Our interviews elicited four design considerations for field analysis agreed upon by all participants:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R1-Offline &amp; Distributed Data Collection &amp; Analysis:</head><p>Field analysts often work in remote locations with limited to no connectivity, inhibiting fully cloud-based solutions <ref type="bibr" target="#b21">[22]</ref>. Most field analysts collect and log data in the field, store that the data on portable thumbdrives, and fuse that data with data from other efforts back at the remote operations center or lab. Analysts noted that variations in data collection practices used by different teams can lead to format inconsistencies that further inhibit data sharing between teams. Aggregating and visualizing new data in real time would improve decision making for team operations when connected and provide timely perspectives on their current situations when offline. R2-Merge Environmental Context &amp; Analysis: Data collection in fieldwork attempts to capture important aspects of a fieldsite to build a broader understanding of the changing environments, functions, and scenarios present in the world <ref type="bibr" target="#b13">[14]</ref>. However, not all aspects of an environment can be quantified and captured in a database. Therefore, building effective tools for situated field analysis requires support for analysts to assess incoming data in tandem with its environmental context to increase contextual awareness and enrich decision making. R3-Mitigate Information Overload: Analysts often multitask in the field. They therefore need to get a quick sense of how their data fits into the current context without having to spend significant amounts of time immersed in the data. Traditional desktop analysis systems are too complex for use in the field <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>. As a result, situated field analysis systems should balance specificity with flexibility: analysts need to rapidly generate important insights without incurring substantial cognitive overhead to build these insights. R4-Use in Outdoor Environments: Operations in outdoor environments require analysts to carry any necessary equipment. Field analytics systems must be sufficiently portable and, when possible, robust to bulky equipment like gloves <ref type="bibr" target="#b6">[7]</ref>.</p><p>While we intended to use an iterative, user-centered design process, the lack of shared understanding about capabilities of IA made it difficult to elicit meaningful information about specific tasks and potential designs. To this end, we used our results to build a design probe to scaffold in-depth interviews about visual analytics for fieldwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN PROBE IMPLEMENTATION</head><p>We used our preliminary interviews to construct a design probe to ground interviews about visualization design for fieldwork ( <ref type="figure" target="#fig_1">Fig. 3</ref>). The probe allows analysts to input data on mobile devices, synchronizing to a local database and then to a remote data store when connected (R1). Analysts can visualize cached data on their mobile devices using standard overview visualizations (R3) or in immersive interactive visualizations (R2) using either a portable see-through display (e.g., <ref type="figure" target="#fig_5">Fig.  9</ref>) or the mobile device's camera (e.g., <ref type="figure">Fig. 5</ref>). Our initial probe used an Android device and Samsung Gear VR headset to provide portable and integrated data collection and analysis (R4; <ref type="figure" target="#fig_1">Fig. 3</ref>). We architected our probe using a geospatial dataset of altitude measures at various sites (seeded with the set of mountains in Colorado with elevation greater than 14,000 feet) and image data collected using the phone's camera, corresponding to data used by public safety and wildland fire experts. Target Workflow: We illustrate our design probe's workflow using a scenario synthesized from our preliminary interviews where an analyst is in the field collecting geotagged data. Analysts sync their data during preplanning, and can analyze the new data collected during an operation using an overview+detail paradigm distributed across devices: mobile visualizations provide rapid overviews of data collected by all teams involved in an operation while immersive detail visualizations show data related to the local environment. As the analyst and other team members collect data, these visualizations update in real-time to remedy temporal analysis gaps. By situating data in the same physical space the data represents, analysts can overcome spatial analysis gaps by analyzing data in the environment. Analysts can continue this collectand-analyze workflow over the course of the operation and refine their practices based on observations developed in real-time. We support this workflow by integrating mobile, cloud, and immersive technologies, using an Android application for data collection and mobile visualization and immersive visualizations built in Unity. Analysts first synchronize a remote target database with the mobile application, which extracts and displays a list of existing data and creates an input form based on the preplanning schema. Analysts navigate the target environment, adding datapoints to a local database that syncs to the remote database when possible. They can then explore this data at a glance through three mobile overview visualizations-a bar chart, scatterplot, and heatmap-to get a quick understanding of the full data corpus. These visualizations increase global situational awareness by providing an overview of data from the fieldsite.</p><p>From within the mobile application, analysts can transition to an immersive detailed view of this data. The analyst sees datapoints projected into the physical environment using geolocation tags from the mobile device. These visualizations enable analysts to more fully explore the data <ref type="figure">(Fig. 5</ref>). Analysts can pull up details on demand by tapping the D-pad on the side of the headset while looking at a point of interest, and can pan the visualizations by sliding a finger across the D-pad. Our implementation supports the requirements enumerated in initial interviews ( §3) as follows: R1-Offline &amp; Distributed Data Collection: The disparate storage media used in the field make it difficult to combine data across teams and with archival sources, requiring substantial curation prior to analysis. Mobile data collection platforms can substantially improve distributed data collection <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b57">58]</ref>; however, limited connectivity in the field can hinder these approaches. We build on prior mobile data collection efforts for distributed teams and for limited connectivity. We leverage a two-phase approach similar to Truong et al. <ref type="bibr" target="#b58">[59]</ref> to allow analysts to locally cache new data offline and to integrate that data with active cloud datastores when connected. The application uses the remote database schema to populate fields in a field notebook interface allowing numeric, text, time, GPS, and image data collection, standardizing data entry across distributed teams <ref type="figure" target="#fig_2">(Fig. 4a</ref>). As analysts add data, new datapoints immediately sync to the local datastore. To support distributed collection efforts, new entries are pushed to the remote cloud database when the device is online, and any new entries in the remote database are pulled to the local datastore. When offline, the application caches the new data and flags the data as unsynchronized to provide insight into the freshness of the available data. To enable ready extensibility, we implement our cloud datastore using a MySQL database hosted on a standard web server. Our dataflow uses standard HTTP message passing where analysts could easily integrate custom server-side functionality tailored to particular domains or operations. R2-Merge Environmental Context &amp; Analysis: Recent advances in mobile and immersive analytics (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b60">61]</ref>) allow analysts to explore data in real time to increase situational awareness and to ground those analyses within the environmental context. However, these techniques are currently tailored to specific applications and do not integrate data collection and analytics as is critical for field operations. Our probe allows analysts to launch either mobile or immersive interactive visualizations directly from the Android application. Mobile visualizations provide rapid overviews of the data, allowing for increased situational awareness over large geographic regions. Analysts can use these visualizations to identify coverage gaps in current data collection efforts. However, mobile visualizations suffer from several potential limitations (see §3) including decontextualization and limited screen space.</p><p>We overcome these limitations by allowing analysts to use immersive visualizations for more detailed exploration within the local environment. These visualizations use AR to provide greater detail about data describing the immediate local environment. To understand how analysts might use these visualizations, our design probe uses three immersive visualization strategies: a low-coupling billboarding approach  <ref type="figure">Fig. 5</ref>. Our design probe used three immersive visualizations to increase contextual awareness: (a) a billboarded map, (b) a 3D bar chart, and (c) an embedded bar chart. We used simple visualizations in the probe based on R3 and to minimize bias from design in our qualitative study. <ref type="figure">(Fig. 5a</ref>) that provides a flat interactive representation of locations as scatterplot points similar to a conventional heads-up display, a lowcoupling 3D representation of the data <ref type="figure">(Fig. 5b)</ref>, and a high-coupling approach that employs spatial mapping to allow the user to walk around the visualizations and engage with data that keeps its physical placement as the analyst moves <ref type="figure">(Fig. 5c</ref>). This approach of overlaying data on top of the physical environment provides analysts immediate understanding of the data's real world context by collocating data with the physical environment it describes. True high-coupling in some scenarios goes beyond spatial location to incorporate object position and structure. This mapping would require advanced computer vision techniques outside of the scope of our design probe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R3-Mitigate Information Overload:</head><p>We couple mobile and immersive AR visualization using an overview+detail paradigm to mitigate information overload <ref type="bibr" target="#b18">[19]</ref>. Mobile visualizations provide an overview of the data from the entire operational area while the immersive visualizations allow exploration of detailed subsets relevant to the active area of exploration. Within our mobile visualizations, analysts can quickly visualize important data dimensions using a map, bar chart, or scatterplot <ref type="figure" target="#fig_2">(Fig. 4)</ref>. These simple designs adhere to analysts expressed needs to quickly "sanity check" analyses and are tailored to the limitations of mobile devices <ref type="bibr" target="#b59">[60]</ref>, while providing simple, familiar data representations to ground our discussion of the design space using this probe. To support rapid insight into local and remote data, our visualizations encode remote data values as blue marks, cached data as green, and unsynchronized data in red <ref type="figure">(Fig. 5</ref>). R4-Use in Outdoor Environments: using the rear-facing camera of the phone for pass-through AR. We use AR rather than VR due to the potential for AR to bind data and context <ref type="bibr" target="#b60">[61]</ref> and to mitigate potential occlusion issues that would inhibit active operation in an outdoor environment. We also implement our probe on the Microsoft HoloLens to showcase the benefits of using an AR headset with spatial mapping, notably embodied navigation and freehand gestural input. Though gestural interaction allows for hands-free data exploration, it does not resolve issues associated with data entry on a mobile device while wearing gloves. Manual data can be input by multiple team members and the probe's architecture allows sensor data to stream to the remote database. Future iterations could explore integration of sensor-specific visualizations and verbal dictation using the phone or HMD microphone to automatically log data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">QUALITATIVE STUDY: DESIGNING FOR FIELD ANALYTICS</head><p>We used our design probe to ground semi-structured interviews with 10 field analysts to understand how visualization systems can bridge spatial and temporal gaps in field data practices. Interviews ranged from 1-2 hours and were conducted in five unique sessions. Six interview subjects were in emergency response domains, two were earth scientists in forest ecology, one was a computer scientist using field data for climate modeling, and one specialized in robotic sensing with drones.</p><p>We first conducted a step-by-step interactive demonstration of the design probe's workflow with participants, starting with an overview of functionality. Then, we walked participants through example usage in field operations-assessing terrain using the geospatial altitude dataset described in §4-interacting with the mobile application and immersive visualizations throughout this walk-through. We collected transcripts of comments during the participants' initial interactions with the probe and, following the demonstration, asked a series of interview questions. Our interview questions (provided in the supplemental materials) focused on analysts' backgrounds; current use of data in the field including collection, storage, and analysis; responses to the design probe; and envisioned uses for field analysis based on the probe.</p><p>We analyzed the transcribed interviews using thematic analysis to identify common practices in field analytics across different domains that visualization systems can support. We found that, though our participants ranged greatly in technical expertise and domain focus, they identified several common themes in potential uses for field visualization. In this section, we discuss these themes (summarized in <ref type="figure">Fig. 6</ref>), how analysts envision using the design probe to support needs expressed in each theme, and how our approach could be extended to better support field analysis. Though we interviewed only 10 domain experts, we found common themes across groups that spoke to critical opportunities for visualization to enhance field practices. We elicit design recommendations and critical use cases for technology-mediated field analysis from these interviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reliance on Location-Based Data</head><p>All analysts responded positively to the design probe's emphasis on geospatial data. Though experts in each domain leveraged different types of data, analysts generally stressed the importance of spatial data for field decision making. For example, seven analysts used image data labeled with geospatial locations to increase global awareness of the field site. The two earth scientists employ stratified sampling to collect data at regular intervals in a geographic area of interest by subdividing a fieldsite into regular grids and "taking x number of plots in different categories" in each gridcell to ensure sufficient sampling (P6). Aerial firefighters use land ownership data defined by property boundaries to guide their operations. The two earth scientists, computer scientist, and roboticist leverage sensor-based or manually-entered environmental data such as temperature, humidity, glacial velocity, and tree diameter to reconstruct an environment and model spatiotemporal patterns.</p><p>The focus on maps and their fusion with the data in the immersive visualizations allows analysts to visualize data about the full space of operations. The six participants in field-oriented domains (fire fighters, search-and-rescue, and earth scientists) remarked that the simple visualizations enabled appropriate levels of analysis while providing opportunities to dig deeper into the data through interaction. The appropriate level varied depending on the target task and context. For example, firefighters wanted access to terrain maps, ownership labels, and other archival data in preplanning, but only simple, minimally obtrustive shapes that indicated decision-relevant information (e.g., simple temperature alerts or points indicating team member or payload locations) during operations. However, participants wanted to see how data could be more tightly interwoven into the physical space. All participants emphasized that the best way to improve field practices through data would be to provide a more informed picture of the operational environment: "the more situational awareness, the better" (P8). This coupling could improve the integration of contextual awareness into data analysis and better support operational decision making. Design Recommendation: Geospatial data projected onto its location in physical space could enhance data sensemaking and at-a-glance decision making. Key Tasks: Analyze stratefied samples of local data and overhead imagery, navigate relevant property boundaries and physical structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Teaming under Limited Connectivity</head><p>All participants conduct field operations in teams. Those teams can be distributed over a wide space (e.g., around the perimeter of a wildfire) or collocated (e.g., collecting tree cover measurements in a specific fieldsite). Currently, all noted data sharing between teams is done through direct communication in collocated teams or by radio commands from an operations center. None had solutions for sharing and collaboratively analyzing updated field data across teams in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support for uniform sampling Close coupling of data with physical context</head><p>High-range, portable router for collaboration in low connectivity Label data based on the person/team entering the data Allow identification of easily resolvable data inconsistencies Highlighting data quality issues via embedded visualizations Archival data, streaming data from teams and drones Multiple perspectives allows integration of new sensor data <ref type="figure">Fig. 6</ref>. We synthesized design recommendations for how data collection and visualization might enhance data use in field operations. These insights fall into four primary categories that can guide designers to scenarios and design considerations for field analysis tools.</p><p>All but the roboticist (9/10 partipants) were excited about the potential of analytics tools to support teaming in both collocated and distributed environments and support consistency checks across team members. For example, the modeling expert noted an incident where two teams at a fieldsite calibrated their sensing equipment differently, leaving their data unusable. Shared data between these team members would allow for at-a-glance identification of these potential anomalies. For distributed teams, aerial firefighters noted that recent advances have allowed them to share basic geocoordinates alongside radio transmissions to help improve situational awareness, but that these techniques are extremely limited. They anticipated labeling data values based on team entry in the map visualizations would significantly expand their awareness of the state and progression of a fire and let them more readily adapt operations through communication between teams.</p><p>Participants in emergency response and climate modeling saw the probe's two-phase caching scheme as beneficial in some instances, but insufficient for scenarios where the entire fieldsite lacks connectivity. In these sites, even collocated team members may struggle to share data. Our search-and-rescue expert observed that "[rescue workers] all have cell phones and we can get so much data until we don't have cell service and then you can't do anything" (P8). The climate modeler typically uploads data and receives an automated email with summary statistics to confirm the data has been stored. When mobile on a fieldsite, this confirmation pipeline fails making "a validation of field data...the first thing to try to support" (P9). Aerial firefighters noted a willingness to carry a high-range portable router to collate data from collocated teams, similar to the peer-to-peer infrastructure in Siren <ref type="bibr" target="#b31">[32]</ref>. Field analysis systems may be able to use this approach for visualization to improve data sharing among local teams. Design Recommendation: Integrating intermediate shared data caching can support collocated teaming in environments with no connectivity through the visualization of both cached and cloud data. Key Tasks: Calibrate between sensors and teams, increase communication and global awareness between teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Data Quality Validation</head><p>Participants advised against attempting all operation center functionality in AR. For example, one earth scientist stated "when I am engaging in the data in a complex way... I am at home at my computer. The things I want to know in the field are pretty simple" (P6). Interviewees emphasized the need to avoid overwhelming analysts with too much data while in the field and to focus on operational tasks that analysts can assess and address in context such as validating the quality and consistency of incoming data.</p><p>Nine analysts noted that data collection errors force them to return to the field and repeat data collection if errors are found while at the fieldsite or to discard large amounts of data if errors are undetected until later analysis. For example, our modeling expert remarked that mistakes in data collection for glacial research were unresolvable as they required flying equipment back to remote locations in Greenland or Antarctica. Earth scientists remarked that measurements of tree circumferencetypically taken at chest height-could introduce substantial variation depending on the scientist's height. While these inconsistencies are easy to resolve, they are currently impossible to detect in the field. Four analysts specifically mentioned validating data by "looking at the size of the file and saying "that looks about right" (P10).</p><p>Analysts saw the design probe as a way to immediately evaluate collected data for errors in quality, with the four participants in the sciences noting that these visualizations would save substantial time and money in field operations. They further remarked that the simplicity of the probe's visualizations would allow analysts to quickly locate errors using familiar representations and to verify consistency by exploring high-level patterns in existing data. Quick glances at the mobile visualizations provide a sense of data coverage, while the immersive visualizations allow analysts to dig deeper into anomalies and compare those variations side-by-side with the conditions of the environment to remedy collection issues.</p><p>Four emergency responders and the climate modeler noted that systems may further aid in data quality validation by automatically highlighting potential quality issues, a challenge that could be addressed by coupling automated and visual analysis. Analysts recommended field analytics systems more deeply consider how embedded visualizations might aid in quickly identifying problematic areas in data collection to increase data quality and efficiency. Such technologies could allow analysts to actively update their goals and operational strategies as a function of the quality and completeness of incoming data. Design Recommendation: Analysis systems should explicitly support detecting, contextualizing, and remedying low quality data. Key Tasks: Identify missing data, detect anomalies, monitor incoming data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Data Fusion Across Perspectives</head><p>All ten field analysts rely on archival data, such as environmental surveys, topological maps, or data from prior efforts, to plan each day's operations. They noted that the design probe's visualization systems could improve efficiency by bringing these sources into the field. For example, aerial firefighters wanted to overlay land ownership markers on the operational site. Public safety officials wanted to see key geological features, such as altitude shifts. Earth scientists wanted to couple visualizations of tree health from prior field surveys to study changes over time. Updating visualizations during data collection may allow analysts to inform operations and change data collection practices on the fly. Nine of the ten participants felt that extending the immersive visualization suite to integrate other teams' locations could further improve operational efficiency, allowing analysts to better coordinate operations across teams collecting data over a single site.</p><p>While the design probe relies heavily on archival and manually collected data, analysts wanted to also fuse this data with data collected by robotic drones and other physical sensors. All participants saw the distributed notebook interface as an improved method for merging data across teams in many scenarios; however, the emergency responders and earth scientists felt the notebook collection interface was potentially problematic when gloves were required for safety or due to environmental conditions (e.g., working in active burn zones or tropical bogs).</p><p>Integration with automated sensors helps overcome these limitations and expands both the data variety and field coverage used in analysis: sensors can stream more data faster and from more locations. In wildland firefighting, sensor readings of temperature, wind, and humidity significantly aid analysts in estimating wildfire spread, but "the more data [analysts] get, the harder it is to process and analyze and make decisions off of" (P2). Experts noted that the design probe's approach could significantly improve the integration of such sensing devices by allowing analysts to fuse archival data with incoming data streams <ref type="figure">Fig. 7</ref>. We iterated on our design probe using expert feedback to create the FieldView prototype. FieldView shows how visualization can support three use cases from our interviews: localized team coordination, data quality validation, and data fusion from heterogeneous sources.</p><p>currently only accessible to operations centers. For example, image data collected by drones can provide multiple perspectives that extend the area an analyst can see. In earth science, this integration would allow analysts to simultaneously study canopies from above and below to estimate forest health. All analysts noted how practices could be improved by visualizing sensor data in real-time, and that visualizations of this data would bring new kinds of data into the field. Our search and rescue expert said, "there's only so much you can learn from a picture or video... When someone provides me with a cool sensor and I can get real-time data on gas concentrations on a hazmat incident and I can have it broadcast in, I'll be stoked" (P8). Analysts recommended that fused views should also allow for separating data from different sources, giving field analysts the ability to distinguish preplanning data from data collected by team members and autonomous sensing technologies. Design Recommendation: Visualizations should incorporate data from other sources, including autonomous drones, to increase awareness of both local and global operations. Key Tasks: Couple data and images from multiple sources, stream real-time data from autonomous sensors into the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FIELDVIEW</head><p>Our interviews revealed design recommendations and key scenarios for how visualization can enhance field practices. To provide a proof-ofconcept for how visualization systems might use these recommendations, we developed FieldView, an open-source extensible prototype, on top of our design probe's base implementation. FieldView extends the probe's data collection functionality through an intermediate cache on a portable server to support collocated teaming under limited connectivity (see §5.2) and basic sensor data integration (see §5.4). It includes situated geospatial AR visualizations designed for three operational tasks noted in our interviews: collocated teaming, data quality validation and refinement, and autonomous sensor integration. As there is little empirical guidance for immersive visualization design (see Marriot et al. for a survey <ref type="bibr" target="#b41">[42]</ref>), these use cases demonstrate how situated analysis may enhance field operations and decision making and illustrate novel capabilities of our system to support field analysis <ref type="figure">(Fig. 7)</ref>.</p><p>Case 1 and Case 2 simulate multiple analysts collecting scorch rate data, drawing on a use case from our interviews with ecologists ("sampling what percentage of needles are scorched"(P6)). As we were unable to revisit the original fieldsite, we retargeted the geocoordinates of the scorch rate data to a local fieldsite. Case 3 simulates a desired "synthetic vision" (P1) provided by seeing the physical environment with temperature data and drone-collected imagery provided by our earth science collaborators and again retargeted to a new fieldsite. We published these use cases as part of an opensource system designed for use with these cases and readily extensible to new data and devices based on the users' operational needs (https://cmci.colorado.edu/visualab/fieldview).</p><p>The Gear VR lacked robust tracking, limiting our ability to integrate data into the environment. While these capabilities can be gained through additional sensing devices, many recent headsets have these capabilities readily available. Our case studies use the Microsoft Hololens with freehand gestures for interaction as P1 reiterated challenges with gloves while in the field inhibiting touch interaction. However, as with the design probe, FieldView does not address the challenges of entering data on a mobile device while wearing gloves. Our modular architecture ( <ref type="figure" target="#fig_1">Fig. 3</ref>) allows analysts to extend FieldView to specialized use cases and technologies-such as other AR headsets or more sophisticated fusion and analytics algorithms-by changing the data source and schema and integrating custom analysis processes and visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Case 1: Team Coordination</head><p>Interviewees discussed the potential for the data collection and immersive visualization components of the design probe to support team coordination by aggregating data collected by multiple team members ( §5.2) and enabling a shared situational awareness across sites. While remote fieldsites may complicate use of mobile devices and cloud infrastructures, our public safety and aerial firefighting experts were willing to carry lightweight portable servers that could establish a local connection between collocated teams and mentioned using ATAK 1 servers for basic data exchange. In this use case, field analysts can enter and synchronize data with local teams while offline <ref type="figure" target="#fig_4">(Fig. 8)</ref>.</p><p>A local Flask server is hosted on a Raspberry Pi (23 grams) and connects analysts to a local network. Team members use FieldView's data collection interface to add new scorch rate samples alongside image references drawn from archival data. All analysts connected to the server can add new data to a local database that synchronizes with the server's intermediate database while within range, allowing the entire team to view data collected within range of the server even when no internet connection is available ( §5.2). When a team member enters a new scorch measure, the visualization updates and all team members connected to the server have access to the new data. Team members engaged with the mobile platform can quickly view coverage of a surveyed area on the mobile application's map view, while analysts engaged in immersive analysis can see incoming data overlaid on the field <ref type="figure" target="#fig_4">(Fig. 8</ref>). Our overlay visualization uses GPS data from the analysts' mobile device and the HoloLens' internal depth sensors to localize data over an explicit collection grid, supporting stratified sampling ( §5.1). Outdoor and large-scale tracking issues with current AR headsets and mobile GPS granularity can limit the utility of this approach, but analysts mentioned they generally use more precise GPS devices that help mitigate these limitations.</p><p>Analysts first explore the mobile visualization to get an overview of data coverage with a simple heatmap of the sampled area. They can move to a relevant field location and launch the immersive visualization to see where data was collected within that site and to map teams in the field to operational goals. Within the immersive view, analysts can visualize scorch rates using an embedded scatterplot applied to the field grid with points color mapped to scorch intensity values <ref type="figure" target="#fig_4">(Fig. 8</ref>). This approach minimizes the amount of space consumed by the visualization while contextualizing data in the environment and supporting stratified sampling practices. Analysts can select collected data points via freehand gestures to display archival image data collected at that location and the team member who uploaded the data on demand. As new data is uploaded, older data from the same grid location forms a smaller point underneath the most recent measure to track changes over time.</p><p>This prototype extends current field analysis capabilities to address three design considerations identified in our interviews. First, the dynamically-updated gridded scatterplot paired with archival data increases the local team coordinator's awareness of the team's operations across the field site ( §5.1). With an updated data model and contextually relevant data situated in the physical environment, field analysts can visualize coverage and synchronize efforts across limited human resources in real time. Second, new data is combined with preplanning data in real time, allowing analysts to update and extend data in existing databases based on mobile inputs ( §5.4). Finally, the simple, embedded scatterplot provides at-a-glance insight into collected data to allow the team to adjust and adapt their operations in response to incoming data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case 2: Data Quality Validation</head><p>Incomplete and poorly collected field data costs analysts time and money, but is difficult to detect while in the field despite analysts being well-positioned to correct the errors. Automated methods for detecting these errors are limited, and current mobile visualizations do not provide sufficient resolution to effectively detect most errors in the field. Further, understanding the context of collected data can enable rapid identification and correction of data collection errors. For example, detecting anomalies while in the field can call analysts' attention to sensors that may be occluded due to weather or dirt or that may be improperly configured. By visualizing data in the context of the operational area, field analysts can identify and rectify these errors as they occur, leading to substantial savings in both time and budget.</p><p>Analysts can validate field data in FieldView at a global scale using a mobile map overview visualization <ref type="figure" target="#fig_4">(Fig. 8</ref>) to identify regions with missing or unusual measures. They can then move to that region and locate missing data highlighted on a AR detail visualization <ref type="figure" target="#fig_5">(Fig. 9)</ref>. FieldView highlights gridcells without collected data in blue <ref type="bibr" target="#b56">[57]</ref>, while existing data are encoded in the gridded scatterplot from §6.1. When the analyst enters data for the missing region in the mobile app, they see the blue highlight replaced with an appropriately color-coded datapoint in real-time. When coupled with the offline teaming infrastructure from (a) Overhead view of distant imagery with a wedge object as for geospatial reference (b) Overhead view of close range imagery with a dotted dropdown object for geospatial reference <ref type="figure">Fig. 10</ref>. Use Case 3: Visualizing geotagged sensor data (temperature encoded by color) and imagery allows analysts to leverage autonomous sensing data in real time. We complement situated visualizations with a heads-up display (HUD) in the left and right peripheries to help find datapoints beyond the line of sight. §6.1, this functionality allows analysts to coordinate data validation efforts with team members performing distributed data collection.</p><p>Analysts can also use FieldView to explore patterns across an environment to look for anomalies that might suggest stale data. For example, analysts can survey scorch rate data across the fieldsite and compare the distribution of sphere colors against the expected data distribution of the visible environment to find inconsistencies. Once those inconsistencies are located, analysts can walk directly to any problematic areas, assess the source of the inconsistency (e.g., by using imagery data from Case 1), and update data.</p><p>This use case addresses data quality assessment tasks mentioned by all participants: data completeness and correctness ( §5.3). Visualizing data within the environment allows analysts to readily bring expertise and contextual information to bear on on-going data collection to identify and remedy simple anomalies. Future extensions of these approaches could integrate automated or comparative solutions into quality analysis. For example, our modeling expert envisioned integrating these approaches with automated processing algorithms used by glacial researchers to call attention to salient irregularities in their data. Forest ecologists envisioned direct visualizations of statistical power metrics overlaid on the physical environment to assess anomalies based on data density and other measures. Our approach enables analysts to rectify errors by providing the means to identify the presence of anomalies in mobile overviews and detailed investigation using IA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Case 3: Data Fusion Across Perspectives</head><p>Autonomous data collection is increasingly central to field data and can help mitigate data entry challenges due to bulky equipment such as gloves. Aerial firefighters want to extend their own vision with imagery captured by drones and supplement that imagery with additional measures collected using on-board sensors; earth scientists wish to examine a forest canopy from both the air and the ground ( §5.4). Analysts can use FieldView to explore autonomous data both within the field of view and from nearby devices. We demonstrate these visualizations using a dataset of overhead imagery from drones used by our earth science collaborators during a recent field operation coupled with synthetic temperature data. Field analysts can use these visualizations to increase their situational awareness using data from autonomous devices and to coordinate action between human and robot teams.</p><p>Analysts can visualize distant imagery through an adapted Wedge technique <ref type="bibr" target="#b26">[27]</ref>. Within the immersive visualization, analysts see drone data collected within their field of view as scatterplot points positioned at the points of collection, with color of the image border and wedge object encoding temperature using a red-blue color scale interpolated in CIELAB. They can then select a point via freehand gesture to see the source image, which connects the image to its physical referent using a color-coded wedge <ref type="bibr" target="#b1">2</ref>  <ref type="figure">(Fig. 10a)</ref>. To avoid information overload and keep the center of the analyst's view unoccluded, images render on either the left or right side of the field of view.</p><p>To provide more comprehensive awareness of local conditions, analysts can also view data from devices outside their immediate field of view via a heads-up display (HUD) along left and right periphery. To keep these visualizations simple ( §5.3) and to further minimize any unnecessary occlusion ( §3), data in the HUD is rendered using a variant of SidebARs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">56]</ref>: data from each collection site is visualized as a semi-transparent point, with points projected along a vector from the measurement point to the viewer. The color of each point corresponds to the current temperature reading. This design mirrors the needs of wildland firefighters who note that the intensity and spread of a wildland fire can change quickly and having data from remote sensors will enable them to react to changes in the environment.</p><p>Our interviews found that data fusion with both human-collected and autonomous sensors providing multiple perspectives on a fieldsite is key for increasing situational awareness beyond current capabilities. Our design choices aim to balance simplicity and situational awareness for field analysts engaging with automated sensors. While our approach builds on techniques for visualizing data beyond the immediate field of view, future techniques could more deeply consider how visualizations could enable collaboration between analysts and autonomous systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION &amp; FUTURE WORK</head><p>This work provides formative insight into how visualization systems can enhance field work, focusing on solutions that integrate data collection and analysis through combined mobile and immersive visualizations. We developed a design probe to ground interviews with ten domain experts and identified four key design themes and several analysis tasks relevant to field visualizations. We embodied these themes in our FieldView prototype to provide a proof-of-concept of how visualization might enhance field work in three scenarios: teaming, data quality validation, and visualizing autonomous sensor data. Our results provide a formative basis for designing visualization tools for field analytics and raise new questions for future tools, including:</p><p>What kinds of analysis and decision making processes should field analytics tools support? How can we optimize visualization designs distributed across mobile and immersive technologies? How can we design integrated solutions for effective collection and analysis workflows in the field? How can visualizations better consider hardware constraints? What kinds of analysis and decision making processes should field analytics tools support? Our interviews revealed common tasks that field analysis tools can support, such as evaluating data quality, supporting distributed teams, and integrating data from drones and other automated sensors. These interviews also yielded critical design recommendations for visualizing this data. For example, visualizations should incorporate archival data with collected data, display this data in its physical context, mitigate information overload, and consider constraints of outdoor environments. FieldView provides a proof-ofconcept for how systems might reconcile the spatial gap by visualizing diverse data in the physical environment it was collected. This approach also helps alleviate temporal gaps by allowing analysts to make timely decisions while out in the field. Our interviews uncovered scenarios, datasets, and components of visualization designs that can support field analysis, but we still lack empirical insight into which tasks require field support and which tasks are best left to the command center. Future investigations into specific domains may provide frameworks for optimizing such distributed sensemaking.</p><p>How can we optimize visualization designs distributed across mobile and immersive technologies? Evolving AR, robotic, and collaborative technologies offer new challenges for effective data visualization, especially in scenarios where data comes from heterogeneous sources. Immersive AR allows analysts to utilize their implicit sense of the environment (e.g., the trajectory of wildfire smoke or the physical bridge being inspected) alongside data. Mobile solutions enable field analysts to engage with complex visualizations in the field, but all experts emphasized the need for understanding situational data at a glance. Analysts needed both quick overviews of the data and the ability to make sense of geospatial data in more detail supported by physical context. Positive feedback from the design probe suggests significant potential for integrated mobile and immersive visualizations; however we have little understanding of how to optimize visualizations to leverage these differing form factors simultaneously. Future research should further explore combined mobile overview and detailed immersive visualizations, similar to the David &amp; Goliath approach to complimentary smartwatch and large display visualizations <ref type="bibr" target="#b28">[29]</ref>.</p><p>How can we design integrated solutions for effective collection and analysis workflows in the field? With our design probe and FieldView, we built example workflows that could empower field analysts with visualizations for data-oriented decision making. Despite significant excitement about how field visualization tools could transform existing practices, we were limited in our ability to evaluate FieldView in the wild due to scientists' reluctance to take new technology into the field, stemming from the high time and financial costs of field operations and legal restrictions around head-mounted displays in public safety. We anticipate that these barriers will change in the near future. Technological solutions for emergency response are receiving considerable political attention, including U.S. Congressional support through a 2019 Act to "promote the use of the best available technology to enhance the effective and cost-efficient response to wildfires" <ref type="bibr" target="#b15">[16]</ref>. Our interviews also exposed several key design challenges for using situated visualizations in the field, such as balancing complexity and simplicity and coupling data with context. However, there is little empirical or grounded guidance for crafting immersive visualizations. Our interviews suggest a need to balance visualization simplicity with support for a broad set of tasks and data sources. Future work should explore ways of providing necessary information in immersive visualizations while retaining a relatively unobscured view of the physical environment.</p><p>How can visualizations better consider hardware constraints? Our work also demonstrated the importance of new hardware platforms for field analysis and visualization. In our interviews, we learned of technologies already being incorporated in the field that would further bolster the proposed workflow and system architecture. For example, earth scientists generally find mobile phone GPS insufficiently precise for many measurements; however, they have workarounds that could inform more precise geospatial data integration. The localized connectivity provided by the high range portable router described by aerial firefighters would enable better teaming when using an integrated system such as FieldView. Continued collaboration with field researchers about the technology already in use could yield field-ready VA systems. Visualization tools should consider how to best integrate these technologies to support richer and more robust analyses. Further, though FieldView employs the Microsoft HoloLens, excessive weight, reduced contrast in natural light, and challenges with spatially mapping large outdoor environments make existing HMDs unreliable for real-world deployments. While we have begun exploring pass-through solutions, fieldwork offers novel design challenges for AR hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Visualizations have the power to transform data-oriented practices in field work by bringing data into the field. This integration can bridge spatial and temporal gaps by integrating data collection and analysis. We explore how mobile and immersive visualization could transform field analysis practices by bridging these gaps through integrated data collection and visualization platforms. We develop preliminary insight into how visualizations might support this domain in interviews with 10 experts from 5 different domains. Our results allowed us to elicit design guidelines and open questions for data analysis systems for fieldwork. We instantiate this feedback in FieldView, an open-source prototype platform for the mobile and situated visualization of field-collected data. This research provides preliminary steps towards characterizing the novel design space of visualization for field analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Current field practices separate data collection and analysis, leading to temporal (separating data collection and analysis by time) and spatial (decontextualizing data from the environment) gaps that hinder effective analysis and decision making with field data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>An overview of our design probe architecture-data is collected and merged with cached data on a mobile device and synchronized with a cloud datastore when the device is connected. Cached data can be viewed on the mobile device or explored in a portable immersive headset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The design probe's mobile application provides (a) a field notebook-style data collection interface, (b) a corresponding list of local data points collected in the field, and (c &amp; d) interactive mobile overview visualizations based on remote server data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Billboarded map in 2D on Gear VR (b) 3D bar chart with altitude data on Gear VR (c) Embedded bar chart on MS HoloLens</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Use Case 1: Team coordinators can assess coarse-grain coverage at a glance on the mobile device (left). They can survey team responsibilities and validate collected data (redder data points indicate greater scorch percentage) using a contextualized scatterplot (top right) and compare new data to archival data (bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Use Case 2: Scorch data is visualized in a scatterplot embedded within the field sampling grid with one point per grid to encode scorch rates, mitigating occlusion of the surrounding environment. Blue regions within the grid allow analysts to rapidly identify gaps in data coverage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2019; accepted 1 Aug. 2019. Date of publication 16 Aug. 2019; date of current version 20 Oct. 2019. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2019.2934282</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://syzygyintegration.com/atak-android-tactical-assault-kit/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Stereo viewing in AR-HMDs allow these points to appear at the point of collection, but positional depth cues are lost when rendered to a 2D figure.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank the expert participants whose feedback drove this work. This work was supported by NSF Award #1764092 and an IGP grant from the University of Colorado Boulder.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The hologram in my hand: How effective is interactive exploration of 3d visualizations in immersive tangible augmented reality?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordeil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="467" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Visfer: Camera-based visual data transfer for cross-device visualization. Information Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1473871617725907</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An annotated situation-awareness aid for augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Höllerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual ACM symposium on User Interface Software and Technology</title>
		<meeting>the 15th Annual ACM symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="213" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards efficient visual guidance in limited field-of-view head-mounted displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnelzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2983" to="2992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visualizing ranges over time on mobile phones: a task-based crowdsourced evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="619" to="629" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Location-aware visualization of vrml models in gps-based mobile guides</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burigat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chittaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on 3D Web Technology</title>
		<meeting>the Tenth International Conference on 3D Web Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geographical data visualization on mobile devices for user&apos;s navigation and decision support activites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burigat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chittaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spatial Data on the Web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="261" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The smoke is rising but where is the fire? exploring effective online map design for wildfire warnings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Boruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Mcneill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Hazards</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1473" to="1501" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Envisioning the Emergency Operations Centre of the Future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anslow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Seyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maurer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-45853-315</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="349" to="372" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marvist: Authoring glyph-based visualization in mobile augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing information on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chittaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="40" to="45" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sleeptight: low-burden, self-monitoring technology for capturing and reflecting on sleep behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Kientz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Playing with the artworks: engaging with art through an augmented reality game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pucihar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kljun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Coulton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gi science, disasters, and emergency management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Cutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions in GIS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="439" to="446" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Map navigation using a wearable mid-air display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dancu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fourgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Obaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fjeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services</title>
		<meeting>the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Conservation, management, and recreation act. 116th U</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Dingell</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>.S. Congress, S.47</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Augmented reality: Advances in diagnostic imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Wilke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Boone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wintermark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimodal Technologies and Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Use of lidar-derived images for mapping old landslides under forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V D</forename><surname>Eeckhaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moeyersons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Beek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandekerckhove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Earth Surface Processes and Landforms</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="754" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A taxonomy of clutter reduction for information visualisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1216" to="1223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ubiquitous analytics: Interacting with big data anywhere, anytime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="86" to="89" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward a theory of situation awareness in dynamic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Endsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Factors</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="64" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Designing contextaware hci for collaborative emergency management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flentge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ziegert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Workshop on HCI for Emergencies in conjunction with CHI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2008</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Handheld computers: A feasible alternative to paper forms for field data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Toomey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Wagenaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="178" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mobile crowdsensing: current state and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Immersive analytics for geology: Field sketch-like visualization to assist geological structure analysis during fieldwork</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Gazcón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M T</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Larregui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Ganuza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Bjerg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Castro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modelling information flow and situational awareness in wild fire response operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goubran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Whitehead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Human Interface and the Management of Information</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wedge: clutterfree visualization of off-screen locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Augmented studio: Projection mapping on moving body for physiotherapy education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reinoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Joukhadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vetere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1419" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When david meets goliath: Combining smartwatches with a large vertical display for visual data exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Infospot: A mobile augmented reality method for accessing building information through a situation awareness approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Irizarry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheisari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automation in Construction</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11" to="23" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The energy aware smart home</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jentsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Prause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pramudianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Akkad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reiners</surname></persName>
		</author>
		<idno type="DOI">10.1109/FUTURETECH.2010.5482712</idno>
	</analytic>
	<monogr>
		<title level="m">2010 5th International Conference on Future Information Technology</title>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Siren: Context-aware computing for firefighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pervasive Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="87" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Touchpivot: blending wimp &amp; postwimp interfaces for data exploration on tablet devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2660" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Staying up to speed: Four design principles for maintaining and recovering situation awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Smallman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Engineering and Decision Making</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="139" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karstens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<title level="m">Presenting large and complex information sets on mobile handhelds. E-commerce and Mcommerce Technologies</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="32" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mobile analytics for emergency response and training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ostmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="88" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Situational awareness support to enhance teamwork in collaborative environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kulyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Veer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Dijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Conference on Cognitive Ergonomics: the Ergonomics of Cool Interaction</title>
		<meeting>the 15th European Conference on Cognitive Ergonomics: the Ergonomics of Cool Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cognitive load and detection thresholds in car following situations: safety implications for using mobile (cellular) telephones while driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lamble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kauranen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laakso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Summala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accident Analysis &amp; Prevention</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="617" to="623" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of mobile phone sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miluzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Vis tiles: Coordinating and combining co-located mobile devices for visual data exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="626" to="636" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Providing information on the spot: Using augmented reality for situational awareness in the security domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lukosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lukosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cidota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Supported Cooperative Work (CSCW)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="613" to="664" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Immersive analytics: Time to reconsider the value of 3d for information visualisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hlawatsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nacenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Immersive Analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="25" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Immersive Analytics</title>
		<imprint>
			<biblScope unit="volume">11190</biblScope>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unlocking the spatial dimension: digital technologies and the future of geoscience fieldwork</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccaffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Holdsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Imber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trinks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Geological Society</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="927" to="938" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hdr video on small screen devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Dynamic Range Video</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Crisis informatics-new data for extraordinary times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Palen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6296</biblScope>
			<biblScope unit="page" from="224" to="225" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mobile phones and paper documents: Evaluating a new approach for capturing microfinance data in rural india</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Javid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<idno type="DOI">10.1145/1124772.1124857</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;06</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Using while moving: Hci issues in fieldwork environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction (TOCHI)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="437" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Computer-based data acquisition and visualization systems in field geology: Results from 12 years of experimentation and future potential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Pavlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Serpa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geosphere</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="294" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A 3d city info for mobile users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rakkolainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vainio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="619" to="625" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Don&apos;t be caught unaware! enhancing situational awareness in oil &amp; gas industry through augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Ramakrishnaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cellular census: Explorations in urban data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sevtsuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ratti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Enhanced reality fieldwork: the context aware archaeological assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bar International Series</title>
		<imprint>
			<biblScope unit="volume">750</biblScope>
			<biblScope unit="page" from="269" to="274" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Augmented reality as multimedia: the case for situated vocabulary learning. Research and Practice in Technology Enhanced Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I W</forename><surname>Lübke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Taketomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M T</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sandor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41039-016-0028-2</idno>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Augmented reality: principles and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hollerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Addison-Wesley Professional</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sidebars: improving awareness of off-screen elements in mobile augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Herskovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Chilean Conference on Human-Computer Interaction</title>
		<meeting>the 2013 Chilean Conference on Human-Computer Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Where&apos;s my data? evaluating visualizations with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864914</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="914" to="924" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The use of mobile phones as a data collection tool: a report from a household survey in south africa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ijumba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Escape-an adaptive framework for managing and providing context information in emergency situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-L</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Juszczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dustdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Smart Sensing and Context</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="207" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Drivesense: Contextual handling of large-scale route map data for the automobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wiehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Embedded data representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="461" to="470" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
