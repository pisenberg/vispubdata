<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Jardine</surname></persName>
							<email>njardine@cookcountyassessor.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ondov</surname></persName>
							<email>ondovb@umd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Franconeri</surname></persName>
							<email>franconeri@northwestern.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Elmqvist</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Institutes of Health in Bethesda</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">and University of Maryland in College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Maryland in College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934786</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graphical perception</term>
					<term>visual perception</term>
					<term>visual comparison</term>
					<term>crowdsourced evaluation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., &quot;biggest delta&quot;, &quot;biggest correlation&quot;) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the &quot;biggest mean&quot; and &quot;biggest range&quot; between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a &quot;Mean length&quot; proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a &quot;Hull Area&quot; proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Perceptual Proxies of Visual Comparison</head><p>Nicole Jardine, Brian D. Ondov, Niklas Elmqvist, Senior Member, IEEE, Steven Franconeri <ref type="figure">Fig. 1</ref>: What's visual in visual comparisons, such as finding the larger mean value? We identify mark arrangements that allow for better performance across comparison tasks. Combining previous results with the results of two new tasks fails to produce a clean ranking of arrangement effectiveness across tasks. We argue that to explain these complex patterns of performance, we first need a perceptual explanation of how visual comparison actually unfolds. Viewers likely perform these mathematical comparison operations with perceptual proxies. We propose and evaluate a candidate set of proxies for two visual comparison tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual comparison is a core perceptual task in data visualizations <ref type="bibr" target="#b10">[10]</ref>. An epidemiologist might use two bar charts to assess whether, across all age groups, there is a larger overall population of women than men. An education researcher might use two bar charts to assess whether one group of students' test scores has a larger range than another. Nei-ther of these comparison tasks need rely on the averages of each of these sets, or identification of individual values. They simply require a judgment of which set's mean or spread is larger than the other. Although certain visual channels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">18]</ref> are known to convey higherprecision information (position) than other marks (hue), existing evaluations of visual comparison performance suggest that there is no single mark or spatial arrangement that optimizes visual comparison.</p><p>In prior work, Ondov et al. <ref type="bibr" target="#b20">[20]</ref> evaluated the perceptual precision of visual comparisons of the "biggest delta between items" and "biggest correlation between sets" for different visualization marks (bar, line) and spatial arrangements (stacked, mirrored, adjacent, superposed, and animated). Precision was not optimized by a single mark type or spatial arrangement. Instead, the precision of visual comparison depended on an interaction of mark, arrangement, and task ( <ref type="figure">Figure 2</ref>). The best static chart for a precise delta comparison, for example, was one that was spatially superposed, rather than juxtaposed in a stacked format <ref type="figure">(Figure 1</ref>), validating an intuitively- <ref type="figure">Fig. 2</ref>: Visual comparison depends not on a single dimension of mark, arrangement, or task, but of the interactions between them. These interactions can be represented as a cube. Our present goal is not to examine the full space of the cube, but rather to understand how a viewer uses visual features to serve analytic task goals depending on the marks and arrangements they see. motivated guideline from Gleicher et al. <ref type="bibr" target="#b10">[10]</ref>. Not predicted by current guidelines was the discovery that, to support biggest delta comparisons in data, animation provided the most precise performance. Animation, however, did not perform as well for comparisons of correlations.</p><p>We first extend this work to empirically evaluate performance across these arrangements for two new tasks-"biggest mean" and "biggest range"-and again find that performance is strongly impacted by spatial arrangement. Comparison was most precise when these two datasets were vertically stacked, and least precise when the datasets were superposed. This pattern of which arrangements were best was strikingly different than for the previous pattern for "biggest delta between items" and "biggest correlation between sets."</p><p>Why is there not a single clean emerging answer, where a given arrangement is best across various tasks? This empirical evidence for the more complex nature of visual comparison is consistent with the idea that it requires a series of visual actions at a variety of scales from one object, to multiple objects, to whole sets of objects <ref type="bibr" target="#b9">[9]</ref>. Taxonomies of visual comparison describe multiple stages of perceptual and cognitive steps <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">9]</ref>, and vary in describing one or many types of visual comparisons, but the visual mechanisms supporting these stages are unclear. We argue that an empirical description of the precision of visual comparison across each combination of mark × arrangement × task would be valuable, but unlikely to scale to have predictive beyond its status as a lookup table. A different approach is required. We propose that instead of continuing to fill out the entries of the cube in <ref type="figure">Figure 2</ref>, it may be more productive to study perceptual proxies of a visualization are actually used to reason about a visual comparison task. The goal of this approach is to begin to identify the proxies for visual comparison, as opposed to merely gathering additional empirical data.</p><p>We propose several candidate visual proxies and implement them as simulations. This allows us to evaluate each proxy's objective performance in performing the same task given to the human participants. But by comparing the performance of these proxies to human performance on the same questions, we can rank proxies by which most closely mirrors human performance.</p><p>Our evaluation of perceptual proxies suggests that although these two comparison tasks show similar arrangement-driven patterns of results, these patterns are consistent with different proxies. To compare means, the visual features that best predict human performance are the ensembles of lengths and the centroids of the bars. To compare ranges, the best-predictive visual features were those that compared deltas be-tween all items within a set or only between neighboring items.</p><p>The complex dependency of visual comparison performance on combinations of marks, arrangements, and tasks might soon be predicted by a model that accounts for such a set of perceptual proxies. We speculate that these proxies follow two broad categories of global or set-based visual comparisons and focal or item-based visual comparisons, drawing from perceptual psychology research on that division in types of visual processing.</p><p>This work contributes (1) results on how visualization design arrangement affects two new comparison tasks with two new crowdsourced graphical perception experiments for visual comparisons of "biggest mean" and "biggest range", (2) a framework of perceptual proxies for visual comparison, (3) implementations of these proxies for empirical evaluation, and (4) data generation procedures designed to estimate the amount of signal needed in the data to support a given visual comparison between sets of items. Our findings present a first step toward a model of human perception during visual comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Here we review empirical research on visual comparison and suggest that visual comparisons can often be classified as being made between isolated parts (i.e., a bar distinct from other bars in its set) or whole sets (i.e., all the bars). Frequently, these correspond to analytic tasks for which a goal is identification or comparison of items, or of sets, in data. We propose that these analytic task goals correspond to proxies that determine the visual features that a viewer uses for visual comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Comparison</head><p>Frameworks of visual comparison are often driven by the analytic goals of the viewer; for example, Amar et al. <ref type="bibr" target="#b1">[2]</ref> names comparison as a high-level "compound task" central to many specific analysis tasks. Gleicher et al. <ref type="bibr" target="#b9">[9]</ref> conducted a review of the taxonomies of visual comparison with the goal of a top-down approach examining what people do when they do visual comparison. They propose that, broadly, to "compare" in a visualization involves at minimum three components: targets (which multiple "items" are being compared), the relationship between these items, and an action (a mechanism operating on the relationship between these targets). These targets may correspond to items or sets of data. Yi et al. <ref type="bibr" target="#b28">[28]</ref> discussed adding a comparison task to their seven-task taxonomy, but ultimately decided against it because "compare is a higher-level user goal or objective." In contrast, our work here and in past work <ref type="bibr" target="#b20">[20]</ref> frames comparison as a relatively low-level perceptual task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Comparison: Parts vs. Wholes</head><p>Visual comparison across multiple series can be comparisons of itemsto-items (focal) or sets-to-sets (global). We refer to these as different visual spans of comparison. One study tested visual comparisons between smaller regions of time series charts, or between larger areas of time series charts <ref type="bibr" target="#b14">[14]</ref>. The data were consistent with the idea that these are distinct visual actions: viewers conducting focal visual comparisons of small regions benefited from shared-space charts that overlapped in space, whereas viewers conducting visual comparisons over the entire sets were better served by separated-space charts.</p><p>Another study <ref type="bibr" target="#b20">[20]</ref> investigated the perceptual precision of two visual tasks, and tested a series of chart arrangements to see which arrangements best supported each visual comparison. In one task, people saw two sets of 7 items and compared them to identify which of the 7 items changed the most between the first and second chart. In this item-to-item comparison, visual comparison was most precise when the charts were spatially superposed within a single chart space, or had an animated transition between them (temporally superposed). These arrangements created visually salient features that mapped to these item-to-item changes, either as overhang in superposed charts or as a salient motion cue that captured attention in the animated charts. Comparison was less precise when the sets were separated spatially such that the two chart axes were arranged horizontally, vertically, or mirroring each other. In another task, people saw two pairs of bar charts that were correlated with each other to some degree. The viewer's task was to pick the pair with the strongest correlation. Here, performance was best when each pair of correlated charts had axes that mirrored each other. Speculatively, viewers could rely on he rapid perception of symmetry between chart items, which in this task happened to indicate correlation between charts.</p><p>These focal vs. global modes have analogues in perceptual psychology research as different perceptual modes. A viewer can opt between these modes (with a mandatory dominance for global mode for a first glance at a new image) to flexibly meet the demands of focal tasks (identify items within sets of larger items) or global tasks. In other words, when people are presented with a visual stimulus, they can flexibly choose whether to attend to the "forest" or the "trees" of that stimulus at varying spatial scales. These different attentional modes switch which scope of a visual stimulus is used for a task goal <ref type="bibr" target="#b19">[19]</ref>.</p><p>In sum, we predict that item-to-item comparisons are facilitated by animated and superposed charts that place these items in close proximity to each other, allowing a focused mode to subserve this more focused task. Set-to-set comparisons may be facilitated by arrangements that spatially separate these sets, allowing the visual system treat each set globally as its own unit, because preserving the values of specific items is not necessary. Different arrangements support different comparisons because, we propose, these arrangements offer visual proxies that people actually use to conduct visual comparisons. Biggest delta benefits from animation because a salient visual cue (motion speed of an item changing the most) naturally maps to, or is compatible with, the apprehension that the data value is also changing the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interim Summary</head><p>Empirical evaluation of how human observers perceive values or relationships in a visualization suggests that people can rely on a variety of proxies, operating over the marks and arrangements and other visual properties of a visualization, to meet the demands of an analytic task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HYPOTHESES</head><p>Our original hypothesis was that the two new tasks MaxMean and MaxRange would show similar patterns of performance to the Max-Correlation and MaxDelta tasks from previous work <ref type="bibr" target="#b20">[20]</ref>: that comparisons of means would, like comparison of correlation, benefit from mirrored charts. We were surprised to see an almost opposite pattern of performance, leading us to turn out attention toward the path of seeking perceptual proxies that might provide more explanatory power for why these strikingly different patterns emerged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODS</head><p>Our first goal is to quantify the perceptual precision with which human observers can perform visual comparison between two charts of horizontal bars, and to measure whether that precision differs based on the spatial arrangement of those charts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks</head><p>We chose two tasks to build on previous work: MAXMEAN: Of two sets, which had the largest average (mean) value? Difficulty is increased by reducing the delta between the mean values, so that the difference between sets is less distinguishable. Displays were controlled so that the largest single-item in a chart was not predictive of that chart having the largest mean and so that charts in a trial were of approximately equal variance. Within-chart variance ranged from .04 to .09. Harder discriminations (smaller mean deltas) spanned the low to high variance range, whereas easier discriminations tended to be lower variance. The two extreme values that bounded data generation were directly included in a randomly selected chart, ensuring that the highest or lowest individual value did not correlate with the correct answer, and allowing us to examine whether participants used this as a proxy. MAXRANGE: Of two sets, which had the widest range between its min and max values? Difficulty is adjusted by varying the delta value between the range widths of the two charts. Since range may be a less widely-understood concept, we gave our participants a detailed description with a simple example, both at the start of the trials and each time they were incorrect in training trials. See past studies <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b22">22]</ref> for similar tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Titer Staircase Method</head><p>Our goal was to quantify the magnitude of the difference of means for the MAXMEAN task, and the magnitude of the difference of range widths for the MAXRANGE task, for each arrangement. We dynamically titrated stimulus difficulty using a staircase method <ref type="bibr" target="#b20">[20]</ref>. Briefly, this method scales task difficulty on a trial-by-trial difference. The end result quantifies a titer: a value between 0 and 1 that scales the magnitude of the difference between stimuli to determine the threshold at which a participant can barely perform a discrimination task (expected performance of 75%). <ref type="figure" target="#fig_0">Fig. 3</ref> illustrates this.</p><p>In the MAXMEAN task, the titer controlled whether there was a large difference between the two chart means (large titer: easy task) or a smaller difference between the two means (smaller titer: harder task). For MAXRANGE, the titer controlled whether there was a large difference between the range widths of the two chart means or a smaller difference between the range widths of the two chart means ( <ref type="figure">Fig. 1</ref>, left; orange), making this pair stand out more from the baseline pair.</p><p>Titers and stimulus datasets changed trial-by-trial depending on participant performance for the previous trial. Briefly, the initial titer value that scaled the difference-of-means or the difference-of-ranges was 0.5. Depending on whether the participant's discrimination in that trial was correct or incorrect, the next trial adjusted the titer by -0.01 or +0.03. The goal of this staircase procedure is that by the end of the trials, the titer reflects a stable magnitude of signal that allows the participant to perform with 75% accuracy for that arrangement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Arrangements</head><p>As in previous work <ref type="bibr" target="#b20">[20]</ref>, datasets were presented in blocks of 5 different arrangements ( <ref type="figure">Fig. 1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>):</head><p>Stacked: Vertically juxtaposed small multiples (i.e. one chart is placed above the other).</p><p>Adjacent: A more commonly used instance of small multiples, in which data series are horizontally juxtaposed.</p><p>Mirrored: This horizontally "mirrored" variation of adjacent opposes the direction of the x-axis in each chart. The Gestalt nature of bilateral symmetry suggests this arrangement prompts "set" proxies rather than "item" proxies in viewers.</p><p>Superposed: A combined chart depicting both data series within the same space. Past work has claimed that overlaying values, or superposition, minimizes eye movements and memory load, and may lead to efficient comparison <ref type="bibr" target="#b10">[10]</ref>.</p><p>Animated: In this "arrangement," a single chart is transitioned, or morphed, from one data series to another over time, using cubic interpolation to ease the transitions <ref type="bibr" target="#b6">[7]</ref>.</p><p>In trials, the order of these 5 blocks was rotated, and each rotation reversed, for a total of 10 possible orderings, each of which was presented to 5 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Task and Procedure</head><p>Before each trial began, the screen contained a centrally placed fixation dot and outlines of where the charts would appear. Participants clicked a button to start the trial. After a countdown, the visualization appeared for a short, fixed time. Static and animated charts were shown for 1.5 seconds. In contrast to previous work, at the end of the impression, both sets of data were removed from the display. Participants then clicked on the orange or blue button corresponding to the orange or blue set of bars to provide a response. For the MAXMEAN task they were instructed to "Click on the chart that had the biggest mean values"; for MAXRANGE to "Click on the chart that had the widest range between min and max values." Participants were informed if they were correct and, if incorrect, what the correct answer was. This feedback was provided to make the task more engaging and to reinforce the goal. Between trials, the titer was adjusted based on the response. To seek 75% accuracy during trials, the titer was increased three times as much for an incorrect answer as it was decreased for a correct answer (see <ref type="figure" target="#fig_0">Figure 3</ref>). Dynamic data generation according to the titer value is described in Supplemental Materials A. Each experiment included all five arrangements. There were twenty trials for each arrangement, and arrangements were blocked. The order of the arrangement blocks was changed between participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training</head><p>Before training, participants were shown examples of stimuli and the task. Before each arrangement block, participants were given a timeunconstrained version of the task, which they were required to answer correctly before proceeding (once for the MAXMEAN task, three times for MAXRANGE). Additionally, the first non-animated arrangement given to a participant followed untimed training with three timed training trials, which were identical to the real trials except that they always had the easiest (largest) titer. Data were regenerated on incorrectly answered training answers to minimize answering by elimination. Video demos are in Supplemental Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Participant Recruitment</head><p>Based on previous work, we predicted N = 50 would provide sufficient statistical power to reliably detect the presence or absence of an effect of arrangement. We also expected that more participants would struggle to understand the MaxRange task, so we collected data from 50 MTurk workers for the MaxMean task and 54 MTurk workers for MaxRange. Participants were asked to self-select out of the study if they had color vision deficiencies. Each participant completed the staircase procedure for all 5 arrangements of one of the tasks (MAXMEAN or MAXRANGE). Worker IDs were used to ensure uniqueness of participants across all such combinations. All workers recruited for participation were adults in the United States.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We evaluated the magnitude of deltas required in the data for nonexpert participants to reliably identify the set with the largest mean (Experiment 1) or the largest range (Experiment 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Exclusion Criterion</head><p>The dependent measure in these experiments is the average titer value from the final 10 trials of each task. We excluded datasets from participants whose average titers (averaged over spatial arrangement) was more than 2 standard deviations from the mean. This procedure eliminated 1 observer each from the MAXMEAN and MAXRANGE tasks.</p><p>We also adopted a second criterion. In a staircase procedure, the goal is to find a converged titer value for which a participant is 75% accurate. The procedure fails if a participant repeatedly reaches ceiling performance (a minimum titer value of 0.01) or floor performance (the maximum titer value of 1.0) because at this point the stimuli cannot titrate difficulty beyond these floors and ceilings.</p><p>Because viewers performed tasks for 5 arrangements, we excluded participants for whom there were at least 5 trials of floor or ceiling titer values. These criteria excluded 0 from the MAXMEAN task, but for MAXRANGE there was 1 trial in which a participant reached ceiling performance and 109 trials who repeatedly reached the floor titer (largest delta). We excluded 7 participants for whom there were at least 5 (up to 22) trials of floor titer values (one of whom was also the participant excluded with the standard deviation procedure), leaving N = 49 for the MAXMEAN task and N = 47 for MAXRANGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Titer Analysis</head><p>We computed each observer's mean titer values from the final 10 trials for each arrangement. We used the final 10 trials because visual evaluation of trial-by-trial data suggested that this was approximately when the staircase procedure stabilized around a narrow range of titers, for most participants. Thus we analyze the final 10 titer values achieved for each of the five arrangements, for each subject. <ref type="figure" target="#fig_1">Figure 4</ref> (far left) displays the mean final 10 delta values for the MAXMEAN task, and (second from the left) displays the mean values for the MAX RANGE task. These titer values correspond to the differences between the charts being compared. Means could be discriminated when they differed by approximately 5-8% of the chart axis, and range widths when they differed by approximately 14-17%. For both tasks, the precision of visual comparison was affected by arrangement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Exp. 1 and 2: MaxMean and MaxRange</head><p>Titer values for the present experiment were analyzed with a mixed ANOVA to test for experiment-level and arrangement-level effects.</p><p>Titer values varied between experiments, F(1, 94) = 9.06, p = .003, η 2 p = 0.09, but this is likely because the titer values scale to different stimulus changes between the two experiments. As such we avoid a meaningful comparison between differing titer values.</p><p>More meaningful is that there was a significant effect of arrangement on precision, F(3.09, 290.7) = 8.17, p &lt; .0001, η 2 p = 0.08, without evidence for an interaction between arrangement and experiment, F(3.09, 290.7) = .34, p = .85, η 2 p = 0.004, both Greenhouse-Geisser corrected. This suggests that arrangement produces largely similar effects on the precision of visual comparisons of means and of ranges.</p><p>We conducted pairwise comparisons within each experiment. Stacked charts were the most precise arrangement overall for both tasks. Precision was better in stacked relative to adjacent charts for the MAXRANGE task, t(49) = 2.73, p = .009, although not significantly better in the MAXRANGE task, t(47) = 1.70, p = .09. Superposed charts resulted in the lowest precision for both tasks. Note that these patterns are strikingly different compared to prior evaluation of visual comparisons of items, which were best supported by animated and superposed charts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Accuracy</head><p>The goal of a staircase procedure is to titrate the task's difficulty so difficulty might change across arrangements, but that accuracy is equiv- alent between arrangements. Mean accuracy in the MAXMEAN task for each arrangement ranged from 76.4% (stacked) to 79.9% (mirror), with no evidence that accuracy was different between arrangements. This suggests the staircase procedure reliably converged for this task.</p><p>Mean accuracy in the MAXRANGE task ranged from 75.2% (superposed) to 84.7% (stacked), and a repeated measures ANOVA found that accuracy consistently differed between arrangements, F(4, 184) = 4.34, p = .002. The staircase procedure did not reliably converge for all arrangements in the MAXRANGE task due to large effects of arrangements on people's ability to perceive range widths. Stacked charts allowed for higher accuracy and high precision than other arrangements. In a pilot version of this experiment with fewer participants, we tested a larger initial titer value so that participants unfamiliar with statistical ranges could use a very large signal in this task, but found the same pattern: superposed charts simply outperform stacked charts regardless of initial task difficulty. Because of these strong effects of chart arrangement, future work using this method might better titrate task difficulty by relying on a greater number trials, or a different kind of adaptive titration procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>The precision of visual comparison of MAXMEAN and MAXRANGE tasks were best supported by vertically stacked charts, and least supported by superimposed charts. This is in contrast to previous findings for item-item comparisons, which were best supported by animated and superimposed charts. These differential findings are consistent with the principle that item-item and set-set comparisons of data are supported by arrangements that enable focal and global visual feature comparison, respectively. That vertically stacked charts were best may be unsurprising because marks are horizontally extending bars. Viewers can simply slice downward to extract lengths between charts.</p><p>We hypothesize that the reason these arrangements best supported these tasks is that visual system can, somewhat flexibly, adopt a series of focal or global "perceptual proxies" that operate on a visualization. To understand visual comparison we must understand not only what visual features exist in a given visualization, but when they are used for which tasks. But these are post-hoc explanations. Are people actually using that proxy for both tasks? Or are people using different visual features to support MAXMEAN and MAXRANGE? This question cannot be answered by evaluating the precision of visual comparisons. Next we study whether the same patterns of precision arising from the same arrangements for different tasks arise because of the different visual features that people use for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">PERCEPTUAL PROXIES</head><p>Instead of mathematical operations, people more likely rely on heuristic perceptual proxies to extract data values and patterns from data visualizations. Heuristics are shortcuts that rely on a simplified metrica proxy metric-to convey the desired information. Perceptual heuristics are easily computable features that (at least) correlate with the right answer, allowing viewers of visualizations to use visual features as accurate or inaccurate proxies for the data those features represent.</p><p>One example is the perception of correlation in scatterplots. The perceptual process does not appear to calculate the true mathematical correlation, and there are instead proposals for multiple proxies that might underlie correlation perception <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b27">27]</ref>, including the aspect ratio of the bounding box surrounding the points <ref type="bibr" target="#b27">[27]</ref>. This proxy can be efficient because it relies on a rapid perceptual process of inspecting a shape boundary around the points. This proxy is also fairly accurate <ref type="bibr" target="#b27">[27]</ref>: the width of the bounding box of the visualization corresponds strongly to the correlation in the data.</p><p>Scatterplots are commonly used to show correlation data, but not all links between visualization and analytic task are so strong. Furthermore, an analyst might not always know what kind of visualization they will see. Finally, the selection of a proxy will be strongly affected by the visual features that are available in a visualization.</p><p>Different proxies may afford not only different data patterns, but different conceptual associations of what those values might mean. The same two data points graphed as two bars or as two endpoints of a line chart can evoke different visual actions taken on visual features of the visualization. Zacks and Tversky <ref type="bibr" target="#b29">[29]</ref> presented simple line or bar charts to participants for open description. Participants' descriptions of bar charts overwhelmingly tended to involve discretizing words, such as "Y is higher than Z," and descriptions of line charts entirely used continuous relations, such as "as X increases, Y decreases." This bar-line message correspondence seems to occur because the type of mark is associated with metaphors of bars being containers or groups, in contrast to lines, which are continuous entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Candidate Proxies</head><p>A visualization contains any number of visual features potentially available as a proxy for a given task, such as the lengths of the top most items of each set, or the perceived symmetry of each set. Different visual features might be better proxies than others for different visual comparisons. Here we explore which visual features appear to be most similar to participant performance (making the same decision), when used as a proxy for MAXMEAN or MAXRANGE. We developed two broad categories of candidate features, informed by research in both visualization and perceptual psychology. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Global Features</head><p>Global-level features describe properties aggregated over a visual set of items, rather than comparing two focal items. Viewers can rapidly compute global statistics such as the mean of a collection of items <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25]</ref>, though from present work it is unclear if this ability is mediated by a proxy. One high-precision proxy is that the lengths of bars in a set are veridically averaged together and the chart with the largest ensemble length is chosen as the answer for the task.</p><p>The mean length feature tests this veridical averaging. Viewers might also perceptually organize the bars into a coherent object, such that what they perceive is the convex hull of the bounded object that includes the heights of the bars and the white space between bars, and then compare the centroids or areas of these two hulls. These object boundary proxies might be subject to perceptual biases, such as overweighting outer edges in contour judgments <ref type="bibr" target="#b5">[6]</ref>. Empirical research on human attention suggests that the allocation of attention through-out visual displays is preceded by the organization of the scene into objects and groups <ref type="bibr" target="#b4">[5]</ref>, and that the center-of-area of those objects can be rapidly computed <ref type="bibr" target="#b17">[17]</ref>. The hull area and hull centroid proxies test whether this visual feature is consistent with participant responses and consistent with differences in the data. Note that for superposed charts, the two hulls are overlapping, such that this particular visual feature may be harder for people to see because it involves filtering using color rather than space (as with the stacked, mirrored, and vertical arrangements). Finally, people are highly sensitive to symmetry in displays <ref type="bibr" target="#b24">[24]</ref> and are biased to select symmetric over asymmetric information <ref type="bibr" target="#b15">[15]</ref>. One possible heuristic is that people use symmetry as a proxy for range, such that any chart that is less symmetric is selected as the one having the bigger range.</p><p>We suspected that visual features describing global, rather than focal, characteristics of the visualization would be better predictors of human decisions in the MAXMEAN task, because this task involves set-level comparisons. Conversely, because precision for a MAXRANGE task requires the isolation of individual item lengths for comparison between sets, we predicted visual features describing focal characteristics would be better predictors of human decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Focal Features</head><p>Focal features describe pairwise differences between two items. People can discriminate small differences in line segment lengths <ref type="bibr" target="#b16">[16]</ref>. Chart viewers might be sensitive to the deltas, either between charts (Biggest Mover Pair) or within a chart (Neighbor Delta). In addition, focal attention can be biased to attend to the topmost item in a collection <ref type="bibr" target="#b26">[26]</ref>, so one possible proxy is that people compare only the lengths of the topmost items of the two sets (Biggest First Item).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Implementations</head><p>We implemented these global and focal perceptual proxies for all charts ( <ref type="figure" target="#fig_2">Figure 5</ref> and pseudocode in Appendix B in the supplemental materials). Some visual features may be salient <ref type="bibr" target="#b13">[13]</ref> to human observers, but not useful for an analytic task (uncorrelated with the answer). For example, the delta between adjacent bars (i.e., the amount of overhang) might be a salient and useful indicator for an analytic task involving comparing items, but if the viewer's goal is to compare means, relying on this feature should impair task performance.</p><p>To evaluate these proxies, we simulated what would happen if each proxy was tested on every data series combination that each observer actually saw in the two experiments. Each proxy was used to make a decision about a visual comparison (e.g., Hull Area generated a convex hull around each of the two charts, calculated their areas, and evaluated the pixel difference in their areas), and provided an "answer" to the task (i.e., larger area is used as a proxy for mean or for range).</p><p>Note that this procedure necessarily shows the proxies different stimuli depending on arrangement: because the stimuli have been titrated to respond to viewer accuracy, the charts "shown" for stacked stimuli will have different properties than the charts "shown" for superposed stimuli. Because the data in the charts "shown" to the proxies is arrangement-specific, proxies were implemented to be arrangementinvariant. The proxies were calculated using raw data values, the length of each mark, and the relative location of each mark (e.g., the first datum in a chart was at the "top" location), not as visual features extracted from an image-based representation. Future work should also test proxy performance using image-based implementations.</p><p>We computed two outputs for each of these proxies: which chart would the proxy have chosen, and was this choice correct? Although we excluded some participants from the titer analysis for low accuracy, we included their data in the simulation to allow for the future possibility of testing whether their poorer task performance is consistent with using different perceptual proxies than other viewers with higher-precision visual comparison.</p><p>Files that contain trial-by-trial data for properties of the stimuli, human responses, the pixel information used by each perceptual proxy to inform a heuristic about a chart decision, and each proxy's decision, for all combinations of arrangement and task, are posted at https://osf.io/uenzd/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Perceptual Proxy Results</head><p>The goal of this proxy approach is to evaluate which visual features are consistent with human performance, and which are actually useful for the task. As such we evaluate the "decisions" of each proxy against two baselines. On what proportion of trials did the proxy agree with the participant's response? And on what proportion of trials did the proxy agree with the true answer of the stimulus? We treat all of the following results as initial speculations, and make no claims of their statistical reliability. These values are depicted in <ref type="figure" target="#fig_3">Fig. 6</ref>. A visual feature can be considered useful if a decision using the differences in that visual feature is consistent with the task-dependent differences in the data. The dots in <ref type="figure" target="#fig_3">Fig. 6</ref> to the right of 50% show features that give above-chance performance at the task. We highlight a few patterns.</p><p>First, the most useful visual feature depends on comparison task. Broadly speaking, global visual features such as centroids are better candidates for the MAXMEAN comparisons, and focal visual features such as Neighbor Delta are better candidates for MAXRANGE comparisons. For the MAXMEAN task, visual features of the Mean lengths (global), Bar Centroids (global), and Biggest Mover Pair (focal) were the most predictive of the difference in the means. It was unexpected that the Biggest Mover Pair, which computes pairwise differences between chart items, predicted the difference of means at above-chance levels. It suggests that in the data, the largest between-item change (neighbor delta in superposed charts, motion in animated charts) was predictive of the chart means, moreso than other global features. For MAXRANGE, the Range proxy (which computed all pairwise distances between items) was most useful, closely followed by pairwise differences only between neighboring items (Neighbor Delta).</p><p>Second, people tend to make decisions consistent with using the most useful visual features: the bars that show agreement between proxy responses and human responses tend to follow the dots that shows the most task-relevant useful features in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><p>Third, we note the absence of a symmetry bias. The Symmetry proxy, which uses stimulus symmetry as a proxy on which to make MAXMEAN and MAXRANGE decisions, was predictive neither of actual differences in means or range, nor of human responses.</p><p>Fourth, there is weak evidence of a bias for people to perform the MAXRANGE task with the global proxies of Hull Centroid and/or Area Trapezoid Centroid, to a higher degree than is actually useful in the task: note where in <ref type="figure" target="#fig_3">Figure 6</ref> the human behavior bars are to the right of the proxy dots.</p><p>We speculate that these findings are broadly consistent with the idea that global visual features are useful for set-level visual comparisons, and local visual features are useful for item-level visual comparisons. MAXMEAN and MAXRANGE tasks benefit from the same chart arrangements, but use different emergent visual features in these chart arrangements for visual comparison. Visual comparison is afforded by more than precision of marks and their arrangements. The "visual" component of visual comparison may rely on a flexible suite of visual proxies that viewers can rely on to accomplish a given task, depending on what visual features are present. The slight bias to erroneously use global features for the MAXRANGE task raises the speculative possibility that, in some tasks and arrangements, viewers use global shapebased proxies even when these proxies are not useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Caveats, Limitations, and Future Directions</head><p>We enumerate several important caveats below and how they suggest possible future avenues of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Whither the cube?</head><p>We began this work with the intent of filling out more cells of the mark × arrangement × task "cube" in <ref type="figure">Figure 2</ref>. While we quickly found that this model does not seem to scale, it is possible that more data (varying marks, arrangements, and tasks) might show that it can scale with some modifications. For example, dividing the tasks axis into "local" and "global" might allow a useful level of predictive validity, even if the underlying perceptual explanations are less satisfying.</p><p>2. More data on the cells of the cube: Existing work has only now empirically evaluated four visual comparison tasks, and . The true answer dots indicates that some features are more useful than others for a given visual comparison.</p><p>there are many more to test. The present experiment focused only on bar charts, due to their ubiquity in real visualizations, and combination of position and length encodings. It will clearly be important to fill out the "cube" by testing other marks, including lines, orientations, saturations, etc., both to test the robustness of the cube model and to provide more data for the enterprise of searching for candidate perceptual proxies for visualization tasks. The bar charts in the present work and that of Ondov et al. <ref type="bibr" target="#b20">[20]</ref> were horizontally extended, an increasingly common design <ref type="bibr" target="#b8">[8]</ref>. Other variants even of bar charts might reveal the use of different proxies for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Different dependent measures:</head><p>The current experiments titrate the size of the compared difference, instead of other potential difficulty manipulations, such as the time allowed to make the judgment, or the number of objects in each set. But some visual comparisons need not be precise, and future work should test whether the same patterns of results hold for these alternative dependant measures. We would not be surprised by substantial differences in those results, as the perceptual proxies that help make precise judgments could differ substantially from those that allow coarser judgments, or judgments over larger sets of values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Artificial artificial artificial intelligence:</head><p>We measured the performance of workers on Mechanical Turk (whose slogan is "artificial artificial intelligence," because human workers take on tasks that are often automated) on making visual comparisons, and then matched their trial-by-trial performance to the predictions of each candidate proxy. Another way to test the match of the proxies is to create "bots" that perform the same experiments, simulating Mechanical Turk workers that exclusively use only one proxy. Large numbers of these simulated participants could run through the real experiment, with difficulty titrated according to their performance across trials, in a way that produces mean titer levels for each "proxy bot," allowing another type of comparison of the proxy to human performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Proxies merely fit performance: This human-proxy agreement metric can only reveal features that are consistent with human performance. They cannot confidently reveal what features people actually use. As in science more broadly, we can rarely be sure of an answer, but we can be sure which of many generated potential answers is most consistent with the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Proxy overlap:</head><p>The tested visual features are highly intercorrelated. As such, we cautiously refrain from strong conclusions about which best predict performance. As above, future work should use more sophisticated modeling that accounts for this shared variance. It could also rely on datasets that are intentionally designed to maximally differentiate among the predictions of the candidate proxies. For example, our data generation for the MAXMEAN task was specifically designed so that the largest item was not predictive of the largest set mean. As such a largest-item proxy could not be useful for the current MAXMEAN data sets, but could be for other data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head><p>A proxy for proxies: The proxies implemented here did not use a computer visual system to "look at" pixels of a chart's visual features and parse those pixels into values. We used the actual data values to generate models of these perceptual proxies. The value of this approach is that if we can determine the properties of the data and arrangements that lend themselves to particular proxies for comparison, then a potential application of this approach is that an automated visualization system would only need know the data values and the designer's desired comparison to construct the mark and arrangement to support that comparison. In other words, these proxies do not directly take into account limitations in perceptual visual acuity, or the capacity limitations of attention and memory.</p><p>8. How can one generate new candidate proxies? Future work should generate more, and more sophisticated, proxies (including combinations of proxies, and eventually, predictions for who will use which, and when). We generated proxies with a combination of intuition and consultation with the perceptual psychology literature, including a strong influence of the literature on focal vs. global processing modes in vision. Our list is by no means exhaustive, and identifying new candidates will be a creative process that, like hypothesis generation across the rest of science, relies on engaging a diverse group of people with different types of background knowledge across both the perception and data visualization communities. A brute force approach would be to generate the full space of mathematically possible pairwise and set-wise proxies. Another route could be based on interviews with viewers engaged in a particular task, to see which aspects of their proxies might be consciously verbalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Implications for Visualization</head><p>The study of the visual features used for visual comparison points to fruitful paths forward for both perceptual psychology as well as for inspiring guidelines for effective visual comparison. For practitioners, because our results indicate that people use different visual features for different tasks, a visualization designer could use these rules to optimize their visualizations and arrangements depending on the perceived task. For example, a visualization where the key task is perceiving a range, such as trend over time in, e.g., a stock market visualization, should clearly optimize for focal visual comparisons. In visualizations where understanding the biggest mean is central, value arrangements should favor global visual features. Our study here was confined to bar charts, but it is clear from the richness of our results that this limitation did not restrict the complexity of the performance results. Bar charts are clearly flexible visual representations in that they support both global and focal visual comparison. Nevertheless, another clear next step is to expand this work to other visual mark types.</p><p>In our empirical study of perceptual proxies, the features that yielded the most similar responses suggests that some proxies are more likely than others to explain human behavior. This is not to say such a correspondence would prove that people use these extremely simplified proxies exactly, or alone, but instead point a path forward to possible mechanisms that can be empirically evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">How Might Viewers Choose Proxies?</head><p>The proxy approach has been a fruitful one for the study of value estimation and comparison of correlation in scatterplots <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b27">27]</ref>. Scatterplots are an ideal "Petri dish" with which to test perceptual proxies for visualization for a number of reasons. Scatterplots are also used often to communicate a single statistic (correlation) of a set for which precision is important. A viewer seeing a scatterplot will likely develop the analytic goal of perceiving correlation, which should be more likely to trigger analysis of the proxies available in the scatterplot visualization to calculate correlation <ref type="bibr" target="#b21">[21]</ref>. Designers are likely to be implicitly aware of some of these perceptual proxies for global and focal tasks. It is possible that the ubiquity of bar chart histograms, or multi-item bar charts in general, is because they allow for both global and focal comparisons.</p><p>One possible avenue of future work is a survey of the literature of what kinds of visualizations are used for different kinds of visual comparisons, to test the validity of the global-versus-focal distinction. A possible eventual extension of this line of research is the potential automation of constructing marks and arrangements, based on characteristics of the data and on the designer's desired comparison they wish to enable in the viewer. This line of work would invert the approach used here. In the current work, we assessed performance of proxies and of observers with a known task. The experimental task may have guided the proxy that observers used. But what if a viewer does not have a specific task before seeing a visualization? One possibility is that the proxy that yields the most salient comparison compels the viewer to perform a task guided with that proxy. For example, in a visualization with two charts with one large item outlier, the single item outlier might capture attention provoke a focal comparison. Future work could manipulate the salience of different proxies in a stimulus and evaluate which task observers perform based on the salience of those proxies, and verify this approach with interviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we found evidence that visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. We proposed a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by comparing their performance to human performance for these marks, arrangements, and tasks. We used this process to highlight candidates of perceptual proxies that might scale more broadly to explain performance in visual comparison.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>In the staircase procedure, a correct response produces a smaller difference in the subsequent trial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Means of averaged final titer values across participants performing the MAXMEAN and MAXRANGE tasks. Smaller titers correspond to more precise differences between means (range widths). The precision of both the MAXMEAN and MAXRANGE tasks was affected by chart arrangements. Also presented are titer values from previously published empirical evaluations of the precision of other comparison tasks. Note that different chart arrangements support different visual comparisons. Gray bars represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>A set of candidate perceptual proxies that might be used in visual comparison of means and ranges (and possibly other tasks). The proxies are arranged by their correspondence with hypothesized distinctions between global and local visual scopes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Results of the two analyses of visual feature performance, split by task: MAXMEAN and MAXRANGE. The x-axis is the percentage of trials for which the visual proxy was predictive, for human behavior (vertical bars), and for true answer for the comparison (colored dots). The small dots show individual subjects, and light gray around the black lines shows 95% confidence interval. True answer dots are color-coded to show whether we informally coded them as a global proxy feature (blue) or focal proxy feature (orange)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Nicole Jardine was supported by the U.S. National Science Foundation grant number DRL-1661264 while affiliated with Northwestern University. Brian Ondov was supported by the Intramural Research Program of the National Human Genome Research Institute, a part of the U.S. National Institutes of Health. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the respective funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representing multiple objects as an ensemble enhances visual cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2011.01.003</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFOVIS.2005.24</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Information Visualization</title>
		<meeting>the IEEE Symposium on Information Visualization<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semiology of Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bertin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>University of Wisconsin Press</publisher>
			<pubPlace>Madison, Wisconsin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matches, mismatches, and methods: Multiple-view workflows for energy portfolio analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2466971</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="449" to="458" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Object-based attention: A tutorial review. Attention, Perception, &amp; Psychophysics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-012-0322-z</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="784" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Average orientation is more accessible through object boundaries than surface features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Levinthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">585</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal distortion for animated transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bezerianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<biblScope unit="page" from="2009" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/1978942.1979233</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Now You See It: Simple Visualization Techniques for Quantitative Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Few</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Analytics Press</publisher>
			<pubPlace>Oakland, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Considerations for visualizing comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744199</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="413" to="423" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual comparison for information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jusufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
		<idno>doi: 10.1177/ 1473871611416549</idno>
	</analytic>
	<monogr>
		<title level="m">formation Visualization</title>
		<imprint>
			<date type="published" when="2011-10" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="289" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ensemble perception: Summarizing the scene and broadening the limits of visual processing. From Perception to Consciousness: Searching with Anne Treisman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitney</surname></persName>
		</author>
		<idno type="DOI">10.1093/acprof:osobl/9780199734337.003.0030</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ranking visualizations of correlation using Weber&apos;s law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno>doi: 10.1109/ TVCG.2014.2346979</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1943" to="1952" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graphical perception of multiple time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcdonnel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2010.162</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="927" to="934" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape constancy and a perceptual bias towards symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tangney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03204219</idno>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="136" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vernier acuity, crowding and amblyopia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1016/0042-6989(85)90208-1</idno>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="979" to="991" />
			<pubPlace>Vision Research</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Shapes, surfaces and saccades. Vision Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Melcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kowler</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0042-6989(99)00029-2</idno>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2929" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Visualization Analysis and Design. A.K. Peters Visualization Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Forest before trees: The precedence of global features in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Navon</surname></persName>
		</author>
		<idno>doi: 10.1016/ 0010-0285(77</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90012" to="90015" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face to face: Evaluating visual comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ondov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jardine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864884</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="861" to="871" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Artificial Intelligence and the Future of Testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pinker</surname></persName>
		</author>
		<editor>R. Freedle</editor>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Lawrence Erlbaum Associates, Inc</publisher>
			<biblScope unit="page" from="73" to="126" />
			<pubPlace>Hillsdale, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note>A theory of graph comprehension</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The perception of correlation in scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldridge</surname></persName>
		</author>
		<idno>doi: 10. 1111/j.1467-8659.2009.01694.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1203" to="1210" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Four types of ensemble coding in data visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1167/16.5.11</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="11" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Characteristics and models of human symmetry detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagemans</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1364-6613</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1105" to="1109" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ensemble perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Yamanashi</forename><surname>Leib</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-010416-044232</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="105" to="129" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Capacity for visual features in mental rotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<idno>doi: 10.1177/ 0956797615585002</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1241" to="1251" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Correlation judgment and visualization features: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2810918</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1474" to="1488" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward a deeper understanding of the role of interaction in information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Jacko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70515</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1224" to="1231" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bars and lines: A study of graphic communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zacks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
		<idno>doi: 10.3758/ BF03201236</idno>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1073" to="1079" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
