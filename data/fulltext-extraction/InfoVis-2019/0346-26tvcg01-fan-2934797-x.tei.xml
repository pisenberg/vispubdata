<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Fan</surname></persName>
							<email>mfan@cs.toronto.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
							<email>jianzhao@uwaterloo.ca.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winter</forename><surname>Wei</surname></persName>
							<email>winter.wei@mail.utoronto.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khai</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fxpal</forename><forename type="middle">Mingming</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto and Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934797</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Think-aloud</term>
					<term>visual analytics</term>
					<term>machine intelligence</term>
					<term>user study</term>
					<term>usability problems</term>
					<term>session review behavior</term>
					<term>UX practices</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1: VisTA: a visual analytics tool that allows UX practitioners to analyze recorded think-aloud sessions with the help of machine intelligence to detect usability problems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Think-aloud protocols were initially developed in psychology to study people's thought processes when solving problems <ref type="bibr" target="#b12">[13]</ref> and were later introduced into the human-computer interaction (HCI) field to identify usability problems with interface design <ref type="bibr" target="#b26">[27]</ref>. It is considered as the single most useful usability testing method <ref type="bibr" target="#b34">[35]</ref> and often used by the majority of user experience (UX) practitioners in usability testing <ref type="bibr" target="#b30">[31]</ref>.</p><p>Although it is beneficial to conduct many rounds of usability testing in the early stage of a project <ref type="bibr" target="#b27">[28]</ref>, analyzing testing sessions can be time-consuming and UX practitioners often work under time pressure to deliver results in time <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. Recent research has shown that when users encounter problems in think-aloud sessions, their verbalizations tend to include more observations, negative sentiments, questions, abnormal pitches, and speech rates <ref type="bibr" target="#b13">[14]</ref>. Thus, there is an opportunity to leverage the patterns to increase the efficiency of UX practitioners in analyzing large amounts of think-aloud sessions. At the same time, however, we face many open questions.</p><p>First, with the advancement in natural language processing (NLP) and machine learning (ML), it is interesting to explore whether ML models can be designed to detect where in a recorded think-aloud session the user likely encounters usability problems. Second, for better utilization of the prediction, visualizations can be designed to present the usability problem encounters (referred as problems hereafter) detected by ML and enable effective exploration. Would the visualization improve UX practitioners' performance or offer them a different perspective to scrutinize usability problems? Third, since recent research shows that many UX practitioners have little experience working with an artificial intelligence (AI) agent or understand the capabilities and limitations of ML <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, how would they perceive and manage their relationship with ML during their analysis?</p><p>In this research, we take the first step toward answering these questions by designing and evaluating a visual analytics tool powered by ML to assist UX practitioners with interactively investigating video-recorded think-aloud sessions.</p><p>We first conducted and recorded think-aloud sessions in which eight participants used both digital and physical products. We transcribed the sessions, labeled user-encountered problems as the ground truth, classified verbalizations into categories <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>, and automatically extracted textual and acoustic features. We used these information to train a range of ML models, including random forest (RF), support vector machine (SVM), convolutional neural network (CNN), and recurrent neural network (RNN), to detect usability problems that users encountered, and evaluated the model performances.</p><p>Following an iterative user-centered design process, we developed VisTA, a visual analytics tool that allows UX practitioners to interactively explore and analyze recorded think-aloud sessions with machine intelligence <ref type="figure">(Fig. 1)</ref>. The tool presents the ML-inferred problems along a timeline, as well as the verbalization and speech features that the ML model takes as input, allowing for a better understanding of the model. In addition, VisTA provides a video player for browsing recorded sessions, and offers the capabilities of annotating and tagging identified problems.</p><p>To deeply understand how UX practitioners perceive and utilize ML-inferred problems with this visual analytics approach, we conducted a between-subjects controlled study with 30 UX practitioners. In addition to VisTA, we included a Baseline condition, in which UX practitioners did not have the assistance from ML, to learn about the impact of ML. We also included a VisTASimple condition, in which UX practitioners were only able to see the ML predictions (without showing input features), to evaluate the effect of having access to these verbalization and speech features in their usage of ML.</p><p>In sum, our contributions in this paper are in two-fold: A novel visual analytics tool, VisTA, to assist UX practitioners with analyzing recorded think-aloud sessions, which integrates ML for usability problem detection with interactive visualization of ML-inferred usability problems and the ML's input features as well as video review and problem annotation functions; Results of a controlled user study that quantitatively and qualitatively characterize how UX practitioners used the tool and provide in-depth insights into how they leveraged and perceived ML in their analysis and how they reviewed think-aloud videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK 2.1 Think-aloud Verbalizations and Usability Problems</head><p>Think aloud protocol was developed in psychology to study human thought processes <ref type="bibr" target="#b12">[13]</ref>. Later it was introduced into the HCI field to understand usability problems <ref type="bibr" target="#b26">[27]</ref>. McDonald et al. surveyed the use of the think aloud protocol and found that 90% of the UX practitioners in their study often use it <ref type="bibr" target="#b30">[31]</ref> and it was considered as the single most valuable usability engineering method <ref type="bibr" target="#b33">[34]</ref>. When using think aloud, participants verbalize their thought processes while carrying out a task. Participants' verbalizations provide data to understand their thought processes. To safeguard the validity of the verbalizations, Ericcson and Simon and a later meta analysis suggest three guidelines: use a neutral instruction to think aloud that does not request specific types of verbalizations; use a think-aloud practice session to allow participants to become familiar with verbalizing their thoughts; use a neutral "keep talkin" token to remind participants to think aloud if they fall into silence <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>. In practice, usability professionals may not adhere to the three guidelines <ref type="bibr" target="#b4">[5]</ref>. For instance, usability evaluators may probe participants, which can cause changes in their behavior <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. Thus, we followed the three guidelines to conduct think-aloud sessions in this research.</p><p>Researchers systematically decomposed users' verbalizations into more manageable segments and categorized them into four categories: Reading (R)-read words, phrases, or sentences directly from the device or instructions; Procedure (P)-describe his/her current/future actions; Observation (O)-make remarks about the product, instructions or themselves; and Explanation (E)-explain motivations for their behavior <ref type="bibr" target="#b7">[8]</ref>. This categorization was further validated by Elling et al. <ref type="bibr" target="#b11">[12]</ref> and was often cited by later work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref>. Recently, Fan et al. studied the verbalization categories and speech features (e.g., pitch, speech rate) of segments in which users encountered problems and found that when users experienced problems, their verbalizations tend to include the Observation category, negative sentiment, negations, questions, and abnormal pitches and speech rates <ref type="bibr" target="#b13">[14]</ref>. Inspired by this finding, we extend this line of research by examining if it is possible to automatically detect problems based on these verbalization and speech features that tend to occur when users encountered problems; and how to integrate such machine intelligence into visual analytics to assist UX practitioners to better analyze think-aloud sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine Learning for User Experience (UX)</head><p>As ML is increasingly integrated into products, it is inevitable that UX practitioners would encounter ML in their workflow. However, a recent survey revealed that many UX practitioners struggled to understand the capabilities and limitations of ML and they often tend to join projects after functional decisions have been made <ref type="bibr" target="#b9">[10]</ref>. Even if UX practitioners can join projects early, they often fail to see places where ML could improve UX <ref type="bibr" target="#b41">[42]</ref>. Consequently, many UX practitioners are unprepared to effectively leverage ML capabilities <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> where it may be able to enhance user experience.</p><p>To address this problem, some developed education materials that aim to teach UX practitioners technical concepts of ML <ref type="bibr" target="#b18">[19]</ref>. Some organized workshops to bring designers and technologists together to explore how ML might function as a creative material <ref type="bibr" target="#b16">[17]</ref>. These work implies that designers should gain technical knowledge of ML. However, a recent interview study with UX practitioners, who had years of experience designing ML-enhanced products, found that they knew little about how ML works but yet they still could use their "designerly abstraction" to work with ML <ref type="bibr" target="#b40">[41]</ref>. This finding supports that it is possible for designers to treat ML as "design material" when improving UX with it <ref type="bibr" target="#b39">[40]</ref>. Inspired by this idea, we would like to understand how UX practitioners would use, perceive, and react to ML by creating an opportunity for them to work with ML when analyze think-aloud sessions and learn from their experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Analytics to Facilitate Qualitative Research</head><p>Purely applying machine learning to solve qualitative research tasks can be challenging. That is because machine learning models are often used to classify or cluster data into categories, but qualitative researchers might need more than automatically generated labels. Further, the results generated by machine learning models may be inaccurate. Researchers attempt to integrate human knowledge and machine intelligence via interactive visualization. For example, Drouhard et al. designed a tool called Aeonium to facilitate collaborative qualitative coding process <ref type="bibr" target="#b10">[11]</ref>. Aeonium highlights ambiguity in qualitative coding and facilitates the evolution of code definitions.</p><p>Moreover, several visualization systems have been proposed to analyze interaction logs, which helps qualitative researchers recover users' intentions and reasoning processes behind. The interaction logs can include low-level inputs (e.g., mouse clicks and eye-tracking data) and high-level actions (e.g., zooming and panning). Heer et al. discussed the design space of interaction histories and proposed a thumbnail-based approach for showing the graph structures of user interactions <ref type="bibr" target="#b19">[20]</ref>. The HARVEST system aims to capture the provenance of users' insights based on their low-level inputs <ref type="bibr" target="#b17">[18]</ref>. Interaction logs and think-aloud data are used together to help users recall their strategies in visual analytics systems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>. In addition, various visualization techniques have been proposed for displaying eye-tracking data, including point-based, AOI-based, and those integrating both. Blascheck et al. provided a comprehensive survey on this topic <ref type="bibr" target="#b2">[3]</ref>. By combining think-aloud, interaction, and eye movement data together, VA 2 facilitates the analysis of multiple concurrent evaluation results via coordinated views <ref type="bibr" target="#b1">[2]</ref>. Also, researchers have investigated user-generated annotations and developed visual interfaces to assist with the discovery of higher-level patterns in users' sense-making processes <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>While several previous tools utilize think-aloud data for analyzing users' behaviors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>, unlike VisTA, verbalization and speech patterns have not been explored to build ML that detects problems and assists UX practitioners with analyzing think-aloud sessions. Further, it is still an open question how UX practitioners would perceive and work with ML during their analysis. In this paper, we strive to address this by conducting user studies to compare different conditions of integrating ML into the visual analytics system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESEARCH QUESTIONS</head><p>In this work, we explore the following research questions <ref type="figure">(</ref> We iteratively developed a visual analytics tool, VisTA, that integrates machine intelligence and used it as a vehicle to answer the RQs. We designed a controlled study to expose UX practitioners to ML at different levels, and recorded a rich set of quantitative and qualitative data about their interactions with the tool, their analysis behaviors (e.g., pauses and rewinds), and their perceived relationship with ML.</p><p>In the rest of paper, we first describe how we curated a dataset of think-aloud sessions and trained ML models to detect problems (Sec. 4) and how we designed the visual analytics tool (Sec. 5). Next, we explain our study design and analysis methods (Sec. 6). Finally, we present the results and discuss our findings (Sec. 7 &amp; 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THINK-ALOUD DATASET AND PROBLEM DETECTION 4.1 Data Collection</head><p>To curate a think-aloud dataset, we recruited 8 native English speakers (4 females and 4 males, aged <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref> to participate in our think-aloud study. Participants had diverse education backgrounds including humanity, engineering, and sciecnes.</p><p>In our think-aloud study, we collected data of participants using three different interfaces, including one digital product (i.e., a science and technology museum website) and two physical products (i.e., a universal remote control and a multi-function coffee machine).</p><p>During the study, the moderator first played a short video tutorial <ref type="bibr" target="#b35">[36]</ref> to demonstrate how to think aloud, and then asked each participant to practice thinking aloud when setting an alarm on an alarm clock. Next, each participant used each of the three products (in a counter-balanced order) to complete a task while thinking aloud. The tasks were related to major functions of the products and were as follows: 1) search for a photo of the instructions for an early telescope, 2) program the coffee machine to make two cups of strong-flavored drip coffee at 7:30 in the morning, and 3) program a remote control to operate a DVD player. For physical products, participants were also given a hard-copy of their instruction manuals.</p><p>All think-aloud sessions were video recorded with audio stream. The average session duration was 222 seconds (σ = 131) for the website, 619 seconds (σ = 195) for the universal remote control, and 854 seconds (σ = 251) for the coffee machine. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Labeling and Feature Extraction</head><p>The think-aloud sessions were manually transcribed into text. Then, two coders divided each think-aloud session recording into small segments, similar to the approach in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. The beginning and end of a segment was determined by pauses between verbalizations and the verbalization content <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. Each segment corresponded to a verbalization unit, which could include single words, but also clauses, phrases and sentences. For each segment, two coders first labeled independently whether the think-aloud user encountered a problem (e.g., being frustrated, confused or experiencing a difficulty) and later discussed to consolidate their labels. We used the binary problem labels as the ground truth for training ML models. In total, there were 3080 segments, of which users encountered problems in 370 segments. For each segment, two coders assigned it with one of the four verbalization categories (i.e., reading, procedure, observation, and explanation) <ref type="bibr" target="#b7">[8]</ref>. Recent research found when users encounter problems in think-aloud sessions, their verbalizations tend to include the Observation category, negative sentiment, negations, questions, and abnormal pitches and speech rates <ref type="bibr" target="#b13">[14]</ref>. Inspired by this finding, in addition to labeling the category information for each segment, we computed its sentiment based on the transcript using the VADER library <ref type="bibr" target="#b24">[25]</ref>. Moreover, we designed a keyword matching algorithm to determine whether users verbalized negations (e.g., no, not, never) in a segment. Similarly, we designed a keyword matching algorithm to determine whether users asked a question in a segment by searching for keywords (e.g., what, when, where) that were located at the beginning of a sentence. Lastly, for each segment, we computed user's pitch (HZ) using the speech process toolkit Praat <ref type="bibr" target="#b3">[4]</ref> and the speech rate by dividing the number of words spoken in a segment by its duration. To determine whether the user verbalized with abnormally high or low pitches and speech rates, we computed the mean and the standard deviation (STD) of the pitch and the speech rate of the entire think-aloud session and automatically labeled a segment as having abnormally high or low pitch or speech rate) if any value in the segment was two standard deviations higher or lower than the mean pitch or speech rate.</p><p>In sum, six verbalization features were generated for each segment: category, sentiment, negations, questions, abnormal pitches, and abnormal speech rates. In addition, for each segment, we also computed the TF-IDF (i.e., term frequency-inverse document frequency) using scikit-learn library <ref type="bibr" target="#b36">[37]</ref> and trained word embeddings on our dataset using Tensorflow <ref type="bibr" target="#b0">[1]</ref>. In the end, eight features were used as the input for training a range of machine learning models to determine whether the user encountered a problem in each segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training and Evaluation</head><p>We employed four machine learning methods: random forest (RF), support vector machine (SVM), convolutional neural network (CNN), and recurrent neural network (RNN), which have been shown effective in text-based classification tasks.</p><p>We extracted the TF-IDF features for each segment in the dataset and used them to train SVM and RF models using the scikit-learn. We used the word-embedding features to train CNN and RNN models. The CNN had an embedding layer followed by a convolution layer, a ReLu layer, a max pooling layer, and then a softmax layer. The RNN had an embedding layer followed by an LSTM with GRU (Gated Recurrent Unit) as the RNN cell and softmax as the activation function. To evaluate the models, We performed a 10-fold cross validation on our data set and used the performance of these models as the baseline. In addition, we appended the verbalization and speech features (i.e., category, sentiment, negations, questions, abnormal pitches, and abnormal speech rates) to the end of the TF-IDF or word embedding vector for each segment as the input to train the same four ML models. Similarly, we performed 10-fold cross validation.</p><p>The results in <ref type="table" target="#tab_2">Table 1</ref> show that verbalization and speech features helped improve all ML models' performance. The SVM models performed the best. CNN and RNN did not outperform SVM or RF probably because our dataset was relatively small for CNN or RNN to learn optimal hyper parameters . Thus, we decided to use the best performed SVM models to predict the problem labels (i.e., whether the user encountered a problem or not) for all segments of the think-aloud sessions. After this process, all segments in each think-aloud session had a binary ML-inferred problem label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISTA: VisUAL ANALYTICS FOR T HINK-ALOUD</head><p>Following a typical user-centered iterative design process, we developed VisTA that interactively presents the verbalization features and the problems detected by ML as described earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design Principles</head><p>Our initial design of VisTA presents the verbalization and speech features of the entire think-aloud session to UX practitioners as a series of synchronized timelines <ref type="figure" target="#fig_0">(Fig. 2)</ref>, in addition to the functions that allow evaluators to play the recorded sessions and add problem descriptions. To get a sense of the effectiveness of this design, we recruited 12 UX practitioners (8 females and 4 males, aged 22-31) as usability evaluators and asked them to use the tool to analyze the recorded think-aloud sessions. Afterwards, we interviewed them to understand their usage and preferences of the tool functionalities. Each study session lasted about 1.5 hours and each evaluator was compensated with $30. Based on the results, we derived two principles to improve the design of VisTA.</p><p>Be Simple and Informative. Evaluators wanted to have a simple interface that offers concise information that they could consult to if need, while allowing them to focus on watching or listening to the recorded sessions. Although evaluators felt that each of the feature can be informative, showing all of them at once was overwhelming, as one evaluator pointed out that "because lines are so busy, it is hard to pick up significant areas while reviewing the session." Instead of viewing all the raw features and trying to figure out important information, they would prefer just having one condensed type of information while still being able to access the raw features if needed. Be Interactive and Responsive. Evaluators felt that the function of clicking anywhere on any timeline to move the session recording to that timestamp was helpful. In addition, they wanted to interact with the input features, such as filtering particular features, to better understand and leverage the features. Evaluators also wanted to tag their identified problems with short annotations to facilitate their analysis. We adopted these two principles in the redesign of VisTA. Specifically, we integrated the machine intelligence into the analysis flow among other capabilities <ref type="figure">(Fig. 1)</ref>. The refined VisTA interface provides a typical video player, a problem timeline that visualizes the ML-inferred problems, and a feature timeline that visualizes ML's input features on the left, as well as a panel on the right for logging and tagging identified problems and filtering input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Session Reviewing and Problem Logging</head><p>A UX evaluator can play and pause the think-aloud video <ref type="figure">(Fig. 1a</ref>) by pressing the ESC key, or fast-forward or backward by pressing the right or left arrow keys. While the video is playing, a red vertical line ( <ref type="figure">Fig. 1e)</ref> moves to indicate the current timestamp, which is also automatically updated <ref type="figure">(Fig. 1f)</ref>. Evaluators can write a problem description <ref type="figure">(Fig. 1g)</ref>, add short and reusable problem tags <ref type="figure">(Fig. 1h)</ref>, and finally log the problem by pressing the "Add" button.</p><p>All problems that the evaluator identified are visualized in the problem table <ref type="figure">(Fig. 1i)</ref>. Clicking a problem entry in the table navigates the video to the timestamp on the timeline where the problem was added so that the evaluator can replay the video segment if needed. Moreover, the tag area ( <ref type="figure">Fig. 1h</ref>) allows the evaluator to create multiple tags and attach them to a problem description. VisTA stores all the created tags in a dropdown list so that the evaluator can reuse them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualization of Problems and Features</head><p>VisTA visualizes ML-inferred problems on the problem timeline ( <ref type="figure">Fig. 1c)</ref>, following the idea of showing "condensed" information. This design hides the complexity of the raw verbalization and speech features that might be hard for the evaluator to understand in their analysis. As this is the primary augmented information to a think-aloud session video, it is placed directly under the video player to facilitate quick scanning. The long red vertical line on the problem timeline ( <ref type="figure">Fig. 1e)</ref> indicates the current time of the video. Further, to allow the evaluator to access the raw features without being overwhelmed, VisTA only reveals the ML's main input features in a short time window (e.g., 10 seconds) around the current time in the video, instead of the entire video as in our initial design, on the feature timeline as shown in <ref type="figure">Fig. 1d</ref> and <ref type="figure" target="#fig_1">Fig. 3</ref>. The start and end of the window are marked with two short red vertical lines around the current time on the problem timeline ( <ref type="figure">Fig. 1e)</ref>. When the evaluator plays the video, the feature values on the feature timeline are dynamically updated. As this is a less demanded feature per pilot users' feedback, it is placed under the problem timeline.</p><p>When the evaluator pauses the video, VisTA shows a snapshot of the input features at the current time on the top of the problem timeline <ref type="figure">(Fig. 4b</ref>). It also highlights parts of the video that have the same features. For example, <ref type="figure">Fig. 4c,d</ref>,e contain the same features as the current time as shown in <ref type="figure">Fig.4b</ref>. To help the evaluator better assess how ML-inferred problems align with the highlight areas, we color-code <ref type="figure">Fig. 4</ref>: The revised problem timeline. VisTA highlights all the segments that have the same set of features as the currently paused timestamp to help UX evaluators spot where else in the session the same features appear and how these areas align with ML-inferred problems. the areas where the ML detects problems in blue ( <ref type="figure">Fig. 4c,d</ref>) and those where the ML detects no problems in pink <ref type="figure">(Fig. 4e)</ref>. When the video is playing again, the highlight and the feature snapshot will disappear to avoid distraction. When the evaluator adds a problem, VisTA adds a feature snapshot on the top of the problem timeline ( <ref type="figure">Fig. 4a</ref>) to help the evaluator remember the location of the problem and what the features look like at that time. Meanwhile, VisTA also adds an entry into the problem table <ref type="figure">(Fig. 1i)</ref>. When the evaluator clicks on the snapshot, VisTA highlights all areas that have the same set of features.</p><p>Furthermore, as shown in <ref type="figure">Fig. 1j</ref>, VisTA also provides a filter function that allows evaluators to manually select a combination of features, which automatically highlights the areas on the problem timeline that have the same set of features. We posit that the highlighting areas would allow evaluators to better assess how the features of their choice align with the ML-inferred problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USER STUDY 6.1 Design</head><p>To investigate how UX practitioners would use the problem timeline and the feature timeline in their analysis, we conducted a controlled laboratory study to compare different versions of VisTA. More specifically, we developed VisTASimple that only shows the problem timeline without the input features <ref type="figure" target="#fig_2">(Fig. 5)</ref>, in order to better understand the effect of the feature timeline on a UX practitioner's analysis. This also helps us to investigate how it can affect the user interactions on the problem timeline. Moreover, to study the effect of the whole ML in the analysis process, we included a Baseline condition that shares the same user interface as VisTASimple except not having the problem timeline. Thus, UX practitioners do not have any access to ML.</p><p>Because there are potentially learning effects between conditions, we adopted a between-subjects design for the study. For example, after a participant used VisTA, she would know the ML's input features, which might prime her to consider these features in the other two conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Participants</head><p>We recruited 30 UX practitioners from local UX communities at a large metropolitan area by posting advertisements on social media platforms. They participated in the study as usability evaluators. We randomly assigned them to the three conditions, thus each having 10 evaluators. They self-reported having 1-9 years of experience. The averages for the Baseline, VisTASimple, and VisTA conditions were the same: 3 years (σ = 2, 3, 2 respectively). Mann Whitney U test found no significant difference in the years of experience between conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Procedure</head><p>We conducted the studies in a quiet office room with a 27-inch monitor connected to a laptop computer. After getting the evaluators' informed consent, the moderator explained that their task was to review three recorded think-aloud sessions to identify when users were confused, frustrated, or experienced problems. The three videos were about users operating on three different products (i.e, one website, one universal remote, and one coffee machine), and were randomly chosen from the dataset described in Sec. 4, and the same set of videos was used for all participants in the whole study.</p><p>In the beginning of the study, evaluators were demonstrated how to use the tool (Baseline, VisTASimple, or VisTA) by loading a trial think-aloud session, and the moderator answered any questions that they had. In each session, before evaluators analyzing the video, the moderator introduced the product and the task that the user worked on in the recorded video. The study lasted about 1.5 hours. We set the time for reviewing each video to be no more than three times of its playback length to ensure all the tasks would be completed within the study time. After each session, the moderator conducted a brief interview by asking how they analyzed the video. In the end of the whole study, evaluators filled in a questionnaire to rate their experience in using ML (for VisTA and VisTASimple) and their confidence in the problems that they identified on a 7-point Likert scale. Then, the moderator interviewed evaluators to further understand their confidence in the analysis results and their usages to the problem and the feature timelines (where appropriate). All interviews were audio-recorded. Each evaluator was compensated with $30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Data Capture and Analysis</head><p>The software tool in all three conditions (i.e., Baseline, VisTASimple, and VisTA) recorded evaluators' interactions during the study. Specifically, it saved all the problem descriptions and their corresponding timestamps. We analyzed the reported problems to understand how evaluators performed in each condition. The tool also continuously recorded pairs of timestamps per second, (SessionTime, VideoTime), when evaluators were analyzing the sessions. This reflects the relationship between the timestamps in the analyzed video and in the study session. We analyzed this information to understand how evaluators reviewed the sessions (i.e., play, pause, rewind). We analyzed the evaluators' answers to the questions in the questionnaire to understand their usage of the tool. In addition, all interviews with the evaluators were recorded and transcribed. Two researchers coded the transcripts independently and then discussed to consolidate their codes. They then performed affinity diagramming to group the codes and identify the core themes emerged from the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>We present quantitative and qualitative results based on RQs in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">RQ1: How Would UX Practitioners Leverage ML in</head><p>Their Analysis?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Number of reported problems</head><p>We counted the number of problems reported in each condition for each session (see <ref type="table" target="#tab_3">Table 2</ref>). Evaluators found the highest number of problems in each session when using VisTA, followed by VisTASimple and then Baseline. One-way ANOVA found no significance in the number of problems identified between conditions for the first (F 2,27 = 2.70, p = .09, n 2 p = .17), and the second session (F 2,27 = 1.33, p = .28, n 2 p = .09), but found a significant difference for the last session (F 2,27 = 4.13, p = .03, n 2 p = .23). Post-hoc Bonferroni-Dunn test found significant difference between Baseline and VisTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">How did evaluators use the problem timeline?</head><p>The interview data revealed four main ways of using the problem timeline. First, they used it as an overview to get a sense of the amount of potential problems and their distribution over the session even before playing the session. This overview information was useful for evaluators to get mentally prepared: "Before the video starts, I looked at the chart to give me a heads up."-P39. In the case of the third video where ML identified 17 problems, evaluators used this information to look out for "big, overarching issues, instead of small little things."-P24.</p><p>Second, evaluators used the problem timeline as guides, reminders, and anticipations. It was common that they may zone out while watching or listening to a long recorded test session, especially when hearing a long period of verbalizations of procedures that do not reveal any problem. In contrast, with the problem timeline, the "spikes" acted as reminders to pull them back and alert them to get ready. "I'm using the spikes as anticipation...of when I should pay more attention."-P12. "I'll be like..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.a problem's coming up and then I'd pay attention and I will be waiting for the problem to pop up."-P26.</head><p>Third, evaluators used the "spikes" on the problem timeline as anchors to facilitate their re-visitation. "Then in the second <ref type="bibr">[pass]</ref>, I wanted to see all the ones that the machine learning highlighted <ref type="bibr">[to]</ref> find things that I didn't notice on my first pass...I just would click where it starts going up, and then go through each one."-P21. They also used it for grabbing representative quotes from users "If I need to grab a quote, I will fast-forward to that part [the "spikes"]."-P12.</p><p>Fourth, evaluators used the "spikes" to help them allocate attention. Some reported that they paid attention to all areas of the sessions but paid extra attention to the "spikes". Alternatively, because the "spikes" were visually salient, some paid more attention to the non-spike areas in their first pass of reviewing the sessions to catch any problems that ML might have missed. "I should pay attention...when there's a long flat line...maybe they didn't pick up something. So I was listening to that part as well."-P20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">How did evaluators use the feature timeline?</head><p>Evaluators in the VisTA condition had access to the feature timeline that visualizes the ML's main input features. But they usually allocated less attention to the feature timeline than the problem timeline. Given the short study time and the amount of videos to review, the filtering and highlighting functions (Section 5.3) were hardly used as evaluators mainly focused on leveraging the problem timeline and the feature timeline while reviewing the videos and writing problem descriptions. Evaluators mentioned that there was a learning curve to digest and leverage all the features and thus typically only considered the feature timeline in the second or third video session, when they became relatively familiar with the interface.</p><p>Evaluators felt that knowing the input features was helpful because this information allowed them to know what features were omitted by ML. Also, it allowed for them to better understand where ML could have missed problems, if the cues for a problem were primarily from the features that ML did not consider, such as visual cues. As a result, they could pay more attention to these features, which in turn allowed for better leverage of ML in their analysis.</p><p>In addition to employing the feature timeline to help better understand ML, some evaluators used the features directly in their own analysis. Among the features, categories were used more frequently as some observed that "observation...could be a potential problem,"-P13 but "reading [is] probably not so much of an issue."-P17. On the contrary, evaluators had different opinions about pitch. Some thought it was helpful; for example, high pitch could reflect that the user was confused and raising a question. But others thought it was not a reliable signal without understanding the user's normal speaking behaviour.  For example, some people tend to raise their tones toward the end of a sentence even if it is not a question.</p><p>In contrast, evaluators in the VisTASimple condition, who did not have access to the feature timeline, were asked if they had developed some understanding of the features that ML might have picked up. While many did not have any idea, some pointed out that ML might have used keywords or visual cues (e.g., how much movement the user had). These guesses were either only partially correct or not correct at all, which could prevent them from using the strategies that evaluators in the VisTA condition used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">RQ2: How Would ML Influence UX Practitioners' Session Review Strategies? 7.2.1 Numbers of pauses and rewinds</head><p>We counted the number of times that evaluators paused and rewound the video in each session under each condition (see <ref type="table" target="#tab_4">Table 3</ref>). Evaluators paused the most when using VisTA, followed by VisTASimple and then Baseline. One-way ANOVA showed that the difference was not significant for the first sessions (F 2,27 = 1.9, p = .17, n 2 p = .12), but was significant for the second (F 2,27 = 4.6, p = .02, n 2 p = 0.25) and the third session (F 2,27 = 6.4, p = .006, n 2 p = .32). Further, evaluators rewound the most when using VisTASimple, followed by VisTA, and then Baseline. One-way ANOVA indicated that there was a significant difference for the first (F 2,27 = 7.7, p = .002, n 2 p = .36), the second (F 2,27 = 3.4, p = .049, n 2 p = 0.20), and the third session (F 2,27 = 5.0, p = .014, n 2 p = .27). The differences between VisTASimple and Baseline were significant, but the differences in all other condition pairs were not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Session review strategies</head><p>We analyzed the pairs of timestamps (SessionTime, VideoTime) to further understand their session reviewing behaviour. We categorized typical behaviours by both the number of passes on a video and the playback behaviours when going through a pass <ref type="figure" target="#fig_3">(Fig. 6)</ref>. In general, evaluators adopted one of the one-pass and two-pass approaches.</p><p>For the one-pass approach, there were three typical behaviours, namely No-Pause-Write, Pause-Write, and Micro-Playback-Write. No-Pause-Write means that evaluators kept the video playing while entering the problems identified <ref type="figure" target="#fig_3">(Fig. 6a)</ref>. This behaviour was more common in the third video potentially due to the video length and the number of problems presented. For Pause-Write, evaluators paused the video while they enter the problems identified <ref type="figure" target="#fig_3">(Fig. 6b)</ref>. With Micro-Playback-Write, evaluators repeatedly rewound and played a small section of the video while entering the problems identified <ref type="figure" target="#fig_3">(Fig. 6c</ref>). Evaluators who used VisTA or VisTASimple tended to adopt the Micro-Playback-Write strategy more than the Baseline. In particular, this strategy was adopted 6 times in Baseline, 18 times in VisTASimple, and 11 times in VisTA across all the sessions. It suggests that seeing the problem timeline had made them more cautious in their analysis. In addition, the Micro-Playback-Write strategy was adopted more in VisTASimple than VisTA, suggesting that knowing the input features of ML allowed them to trust ML more and thus needed to rewind less frequently. When evaluators adopted the two-pass approach, some used the first pass to gain an understanding of the context and to get a heads up of where the problems might be, i.e., Overview-then-Write. They sometimes played through the video without pausing or rewinding in the first pass if gaining context was the goal <ref type="figure" target="#fig_3">(Fig. 6d)</ref>. On the other hand, some evaluators identified problems during the first pass and used the second pass as a chance to pick up the problems they might have missed or re-assessed issues they were not sure of, i.e., Write-then-Check <ref type="figure" target="#fig_3">(Fig. 6e)</ref>. <ref type="table" target="#tab_5">Table 4</ref> shows the number of evaluators who used one-pass or two-pass approach. In all conditions, evaluators adopted one-pass and two-pass approaches and the proportions of the two were similar between conditions. The two-pass behaviour was more common in the first session than the last two, which may be due to the length of the videos, as the last video was the longest among all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">RQ3: How Would UX Practitioners Perceive and</head><p>Manage the Relationship with ML? 7.3.1 Questionnaire responses First, evaluators strongly agreed that they compared the ML-inferred problems in their analysis when using VisTASimple (Mo = 7, Md = 7) and VisTA (Mo = 7, Md = 6.5). They felt positively that they knew how to make use of the problem timeline when using VisTASimple (Mo = 5, Md = 6) and VisTA (Mo = 6, Md = 6). In general, evaluators agreed that ML helped them notice parts of the videos that they might have otherwise skipped if analyzing the videos without it when using VisTASimple (Mo = 5, Md = 5) and VisTA (Mo = 5, Md = 5). Also, evaluators would like to use VisTASimple (Mo = 6, Md = 6) and VisTA (Mo = 5, Md = 6) in future analysis.</p><p>Second, evaluators agreed more on the problems that the ML inferred (Mo = 5, Md = 5) than the problem-free areas that the ML inferred (Mo = 3, Md = 3) when using VisTA, and the difference was significance (z = −1.98, p = .047). In contrast, the difference was not different in VisTASimple.</p><p>Third, evaluators were confident that others would agree on the problems they identified: Baseline (Mo = 6, Md = 5.5), VisTASimple (Mo = 6, Md = 6), and VisTA (Mo = 5, Md = 5). Kruskal-Wallis test found no significant difference (H = 1.79, p = 0.40). They were also confident about the areas that they identified as problem-free: Baseline (Mo = 4, Md = 5), VisTASimple (Mo = 5, Md = 5), and VisTA (Mo = 6, Md = 6). No significant difference was found between conditions (H = 2.85, p = 0.24).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">What were evaluators' attitudes toward ML?</head><p>Evaluators developed different perspectives on ML from their experiences. Four evaluators considered ML as a colleague or coworker, who could provide a second perspective on the identified problems. "It might be picking up something that I had not been thinking about in a different sense...Could it be revealing something else I'm not picking up? Because I have my own confirmation bias."-P33.</p><p>Two evaluators treated ML as a backup when ML agreed with them, which increased their confidence in the problems they identified. "ML will back up my judgment, helped me confirm that there is a problem."-P17. Three evaluators saw ML as an aid that helped them identify problems faster, not necessarily providing a different perspective that prompted them to reassess their disagreements. "Use it for anticipation. When there is a prediction, I picked out problem faster. I don't consider it a different perspective."-P13.</p><p>Additionally, four evaluators considered that there was a competition between them and ML. Evaluators had this feeling that they wanted to prove that they can do a better job and they had skills that ML may not necessarily possess. "I didn't feel like it was smarter than me."-P36, "I want to feel I have skills too."-P17.</p><p>On the other hand, three evaluators expressed concerns that using ML might cause them to be overly relying on it and get lazy in their analysis. "If you don't care about your job you will just follow the chart...Someone still has to watch it (the video)."-P24.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Amount of agreement and disagreement</head><p>Two researchers went through each problem that evaluators reported and compared its description and timestamps with the ML-inferred problems to determine if evaluators and the ML referred to the same problem. The agreement and disagreement of the reported problems between evaluators and the ML are shown in <ref type="table">Table 5</ref>. One-way ANOVA found no significant difference in the number of problems that evaluators and the ML agreed for the first session (F 2,27 = 1.6, p = .22, n 2 p = .1) and the second session (F 2,27 = .9, p = .42, n 2 p = .06), but found significant difference for the third session (F 2,27 = 5.8, p = .008, n 2 p = .30). Post-hoc Bonferroni-Dunn test found significant difference between Baseline and VisTA. In contrast, there were no significant difference in the number of problems that were reported only by evaluators for the first (F 2,27 = .07, p = .93, n 2 p = .005), the second (F 2,27 = .006, p = .99, n 2 p = .0005), or the last session (F 2,27 = 2.2, p = .13, n 2 p = .14). Similarly, there were no significant difference in the number of problems that were reported only by the ML for the first (F 2,27 = 2.3, p = .12, n 2 p = .15), the second (F 2,27 = .66, p = .52, n 2 p = .05), or the last session (F 2,27 = 1.6, p = .22, n 2 p = .11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4">How did evaluators deal with (dis)agreement with ML?</head><p>Evaluators felt that the agreement with the ML acted as confirmation and reassured them that they were correct with their reported problems. "If I find a problem and the model also finds it, I feel more confident."-P26. Evaluators also felt that seeing the agreement would make them "pick up the problems faster."-P13.</p><p>Evaluators generally understood that it was possible that ML is imperfect, "Computer is not perfect...I don't expect it to be,"-P17, and that ML can pick up different problems than they would. When it came to the disagreement, they considered false positives and false negatives of ML differently. When ML suggested a problem, they generally gave it a second thought even if it might be a false positive. "I often wonder if I missed any problems, so it is safe to assume there is one [if the <ref type="table">Table 5</ref>: The agreement and disagreement of reported problems between evaluators and the ML. : problems that both evaluators and the ML reported; : problems that only evaluators reported; : problems that only the ML reported. Results are shown as μ(σ ). In contrast, if they thought that the think-aloud user encountered a problem but ML did not point it out, they generally considered that ML missed the problem and would more likely choose to trust themselves. "By the third session, I started to really believe that the machine was just purely picking up more of the audio than the visual. So I think that's why...I gained a little bit more confidence."-P26. In addition, they generally valued recall over precision, which is consistent with the findings of recent research (e.g, <ref type="bibr" target="#b25">[26]</ref>). This can be explained by the fact that the goal for evaluators is to find potential problems, therefore it is safer to be overly-inclusive, which would introduce false positives, than missing potential problems, which would introduce false negatives. It is also worth noting that evaluators in VisTASimple generally put less weight on the ML's predictions than those in VisTA when disagreements happened, which is probably because the ML in VisTASimple was more likely to be perceived as a "black-box." "I don't know much about what it is based on and how developed the machine learning is, so I don't know how much I can trust it."-P18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.5">What were the perceived limitations of the ML?</head><p>Evaluators pointed out a number of limitations based on their usage of ML. First, they noted that ML was often able to detect the moments when the user exhibited symptoms of a problem but did not pinpoint the start and end of the problem. However, observing the problem build-up process was important to fully understand it. "There was...what I call...a lagging factor. I would have liked to see some of those issues highlighted earlier than some of these spikes on the timeline."-P14.</p><p>Second, evaluators mentioned that ML did not understand the nuances in a user's personality. For example, some users may prefer to say negative words even when they did not experience too much of a problem. "I don't think computer will pick up nuanced behaviours and personalities."-P17.</p><p>Third, they felt that ML did not fully understand the context of users' actions. For example, ML had difficulty picking up repetitions in actions: when users did something repetitively, it could be a problem even though all the steps were performed correctly. Additionally, they felt that ML did not consider the number of steps taken to complete a task as a factor when detecting problems. For example, taking more steps than needed could mean a problem although the user completed the task successfully. Lastly, they felt ML did not fully comprehend the structures of the tasks (e.g., what (sub)tasks did users struggle with?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION 8.1 The Effect of ML on Evaluators' Analysis</head><p>Evaluators identified more problems when using VisTA than Baseline in all three sessions. The implication is that evaluators had more instances to understand potential problems when using VisTA. Such difference was significant for the third session, but not for the first two sessions (Sec. 7.1.1). One possible reason could be that as this was the first time evaluators had access to ML, they needed time to learn and understand how to leverage the ML-inferred problems in their analysis over the sessions. Evaluators mentioned that they either did not have much time to carefully consider the problem timeline or were still testing it in the first session. But over time, they were able to develop four general strategies (Sec. 7.1.2) to use the problem timeline. These strategies encouraged evaluators to be more cautious about their analysis, which was evident by the fact that evaluators using VisTA paused the videos significantly more than those using Baseline (Sec. 7.2.2). Another possible reason for non-significance in the first two sessions could be that these sessions were shorter and contained much fewer problems than the last one and thus the potential variations between conditions would also be smaller. Further research is needed to confirm whether the record think-aloud session's length influences ML's effectiveness.</p><p>Intuitively, an evaluator pointed out, "Without ML, it is much easier to ignore and let go some issues."-P21. When evaluators were watching a session to understand the development of a problem, a new problem might come up, which could take their attention away. If they did not rewind or pause the video in time, they could have missed the locations where they would otherwise want to follow up later. In contrast, the problem timeline acted as an overview, guides, anchors or anticipations, which facilitated evaluators with pinpointing the areas that they wanted to rewind and pause. This was more effective than using the Baseline to check the points that they might have missed. It is worth pointing out that the way in which evaluators used the problem and feature timelines is inherently tied to their session reviewing behaviour (e.g., pausing and rewinding), and is eventually tied to the number of reported problems. The significant difference in the numbers of problems and in the amounts of pausing and rewinding suggest that a ML-enhanced visualization is capable of helping evaluators become more cautious of their analysis and notice problems that they might have missed.</p><p>Despite the evaluators reported more problems when using VisTA than Baseline, they did not rewind the videos significantly more often. One potential explanation is that the evaluators' reported problems were also visualized at the corresponding timestamps on top of the problem timeline in VisTA <ref type="figure">(Fig. 4a)</ref>. The visualization of the reported problems might also have acted as anchors, in addition to the MLinferred problems on the problem timeline, that allowed the evaluators to better determine where they should rewind the video.</p><p>Although evaluators found more problems using VisTASimple than Baseline for all three sessions, One-way ANOVA did not find a significant difference. This could potentially suggest that having access to the feature timeline might play a role in encouraging evaluators to identify more problems. One reason could be that since evaluators in the VisTA condition knew what features were considered by ML, they could better infer when ML would make a mistake and focus on those features, such as visual cues, that ML did not consider. Another reason could be that evaluators leveraged the feature timeline as additional information in their analysis instead of merely using it to understand ML. However, individual differences between Baseline and VisTASimple could also come into play, as we used the between-subjects design and tested each condition with only 10 evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Attitudes toward ML</head><p>"Evaluator effect" refers to the fact that different evaluators might identify different sets of problems when analyzing the same session <ref type="bibr" target="#b22">[23]</ref>. Although it is recommended to have more than one UX evaluator analyze a usability test session to reduce potential evaluator effect, fewer than 30% UX practitioners actually had an opportunity to work with others to analyze the same usability test session <ref type="bibr" target="#b14">[15]</ref>. Our study reveals that a common attitude toward ML was to treat it as a "colleague" or a "coworker", who can provide a second perspective on their analysis or back up their identified problems. This finding points out an opportunity to leverage ML to help reduce the "evaluator effect" for UX practitioners, who often operate under resource and time constraints <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. Toward this goal, we have identified three factors to consider when designing a user interface that leverages ML to offer a second perspective to UX practitioners.</p><p>First, evaluators felt that knowing the severity of the problems that ML identified can help them to prioritize their analysis especially when they are under time pressure to analyze a large amount of test sessions. Second, evaluators also felt that knowing the confidence level of ML in its inferences can be helpful. For example, they could filter out the low-confident inferred problems and focus more on the high-confident ones, especially when the session is long and has many inferred problems. Third, evaluators felt that ML would be more like a "colleague" if it could provide explanations for the detected problems. But what explanations are appropriate and how to generate them? Recent research suggested that the taxonomy for explaining ML to designers is likely "to be radically different from ones used by data scientists" <ref type="bibr" target="#b38">[39]</ref>. In fact, evaluators who used VisTA felt that the current terms used for input features, such as category and sentiment, were too system-oriented and hard to interpret. They preferred the features to be expressed using layman terms, such as the level of surprise, excitement, or frustration. In addition, it might be beneficial to consider multiple explanations instead of seeking for the best one <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Reliance on ML</head><p>Three evaluators expressed the concern that this may make UX practitioners rely on ML too much, thus less diligent in their jobs. However, we did not find any evidence to support this. First, in all three conditions, evaluators identified problems that the ML did not identify, and there was no significant difference between conditions. Similarly, in all the conditions, evaluators disagreed on some of the ML-inferred problems and there was no significant difference between conditions either. These results suggest that evaluators did not just focus on the ML-inferred problems or took the words from the ML without scrutinizing them in the VisTA and VisTASimple conditions. Additionally, some evaluators even felt that there was a competition between them and the ML, making them subconsciously eager to prove that they could find more problems than the ML. It is, however, worth noting that as our study duration was short, no baseline trust with the ML had been established. Consequently, it is hard to determine whether evaluators would become over-reliance on ML or develop sustainable cooperative strategies in the long run.</p><p>Although none of the evaluators solely relied on the ML without putting in their own thought during analysis, we identified two ways in which evaluators wanted the ML to be presented. One way is to allow evaluators to analyze a test session by themselves in the first pass and then revealing the problem timeline to them in the second pass. In this way, the problem timeline would mainly help them confirm their judgment or double check if they might have missed any problem. The other way is to show the problem timeline all the time. The rationale for this design is that the two-pass reviewing process might not be practical especially when the session is long. This was evident that there were fewer evaluators who adopted the two-pass strategy for the third video, which was the longest among all (see <ref type="table" target="#tab_5">Table 4</ref>). Although offering evaluators an option to turn on and off the problem timeline seems to be a compromised approach, it remains an open question how and when to best present ML to evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Trust in ML</head><p>We did not explicitly measure evaluators' trust in ML, however, we identified two factors from the interviews that could have affected their trust, including the sophistication and the amount of disagreement. The sophistication of ML is determined by the number of features that it considers (e.g., audio and visual features) and whether it understands the context of the task (e.g., the number of steps required to complete a task; meaningless repetitive user actions) or the personality of the user (e.g., the speaking behavior). Evaluators in all three conditions were fairly confident in their reported problems no matter how many problems they missed. This could suggest that UX practitioners might suffer from "confirmation bias" <ref type="bibr" target="#b32">[33]</ref>. Confirmation bias can be mitigated by revealing the prior probability of events or input attributions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>. For example, it might be helpful to show evaluators the prior probability of catching all the problems from a test session (e.g., 70%) so that they might be more willing to reconsider their decisions when the ML disagrees with them. The goal of having ML's support is to encourage evaluators to scrutinize their analysis with the input of a different perspective from ML. It is, however, not to overly convince evaluators to agree with ML as it is still an open question whether increasingly agreeing with ML is beneficial for UX analysis. Another way could be to redesign the user interface to prompt evaluators to enter the features that they have considered and then ML could point out the features that they might have neglected. However, how to best design such systems that both deliver ML results and facilitate trust remains to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Future Work</head><p>We have identified four directions to explore in the future. First, evaluators felt that it would be informative to know the level of confusion or frustration (i.e., the severity of problems) and the confidence of ML for each identified problem, which could allow them to better prioritize their attention when time and resource is constrained. Future research is needed to design methods for detecting such information. Second, challenges need to be addressed for designing a visual analytics tool that effectively present all the information without overwhelming evaluators. One approach is to allow for turning on and off different functions. Our study results suggested some design considerations. For example, while some evaluators preferred to analyze think-aloud sessions without ML assistance in the first pass and only see ML-enhanced information in the second pass, others preferred to see ML-enhanced information in one pass to reduce time cost. Third, it is promising to explore ways of describing or explaining ML-inferred problems using a language familiar to UX practitioners. Future research should examine how UX practitioners communicate usability problems with their colleagues. Fourth, we used supervised learning to detect problems. Mixed-initiative interaction design, on the other hand, would allow a UX evaluator to annotate the ML's errors from which the ML can learn and evolve. Although promising, one potential caveat of learning from a UX evaluator is that the ML might behave more and more like the evaluator as the evaluator "corrects" the ML. Because one critical benefit of the ML is to offer a different perspective on the analysis, such an overly personalized ML, in the context of UX evaluation, would likely enhance the evaluator's confirmation bias instead of helping her spot potentially neglected areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>We took the first step to explore how UX practitioners use, perceive and react to machine intelligence when analyzing recorded think-aloud sessions. We designed a visual analytics tool, VisTA, that presents ML-inferred problems and input features with timeline visualizations among other functions to facilitate UX practitioners with their analysis. Our three-session between-subjects controlled study compared VisTA, VisTASimple, and Baseline. Results showed that UX evaluators identified significantly more problems without needing to rewind the video more often when using VisTA than Baseline by the last session. Evaluators used the problem timeline as an overview, reminders, anticipations, and anchors to help them allocate their attention, spot areas that they might have otherwise neglected, and better revisit the videos. They used the feature timeline to understand what features were used and omitted by the ML and also used the features directly as an additional source of information in their analysis. Evaluators treated ML as a "colleague" who can offer a different perspective, as an aid that can make the analysis more efficient, or even as a "competitor" who encouraged them to spot more problems to "beat" it. In addition, evaluators both agreed and disagreed with ML-inferred problem encounters in all test conditions and did not seem to be overly reliant on ML. However, long-term deployment study is needed to validate this conjecture. Furthermore, they perceived the cost of false negatives of ML higher than that of false negatives and valued recall over precision. In addition, evaluators adopted three types of one-pass and two types of two-pass session reviewing strategies in each of the three conditions. Lastly, advanced features in VisTA, such as filtering and highlighting functions, were underused by evaluators. Therefore, in addition to detecting a richer set of information about problems (e.g., severity, explanation), it is also important to explore ways to deliver such information so that UX practitioners can better exploit it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>An early version of VisTA that visualizes the verbalization and speech features of the entire think-aloud session to UX evaluators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The feature timeline shows ML's main input features in a short time window around the current time and updates as the video plays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>VisTASimple has the same functions as VisTA except that it does not show the feature timeline or provide the filter function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Typical playback behaviours (where x-axis indicates the study session time and y-axis the reviewed video time): (a) One-pass: No-Pause-Write; (b) One-pass: Pause-Write; (c) One-pass: Micro-Playback-Write; (d) Two-pass: Overview-then-Write; (e) Two-pass: Write-then-Check.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2019; accepted 1 Aug. 2019. Date of publication 16 Aug. 2019; date of current version 20 Oct. 2019. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2019.2934797</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The performance of the ML models trained with input features.</figDesc><table><row><cell></cell><cell cols="3">TF-IDF/ Word embedding</cell><cell></cell><cell>All features</cell><cell></cell></row><row><cell></cell><cell cols="3">Precision Recall F-score</cell><cell cols="3">Precision Recall F-score</cell></row><row><cell>RF</cell><cell>0.79</cell><cell>0.53</cell><cell>0.64</cell><cell>0.80</cell><cell>0.64</cell><cell>0.71</cell></row><row><cell>SVM</cell><cell>0.59</cell><cell>0.73</cell><cell>0.65</cell><cell>0.76</cell><cell>0.70</cell><cell>0.73</cell></row><row><cell>CNN</cell><cell>0.81</cell><cell>0.41</cell><cell>0.54</cell><cell>0.79</cell><cell>0.48</cell><cell>0.60</cell></row><row><cell>RNN</cell><cell>0.60</cell><cell>0.43</cell><cell>0.50</cell><cell>0.76</cell><cell>0.54</cell><cell>0.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The number of problems reported by evaluators (in μ(σ )).</figDesc><table><row><cell></cell><cell cols="2">Session 1 Session 2</cell><cell>Session 3</cell></row><row><cell>Baseline</cell><cell>3.9 (2.0)</cell><cell>6.2 (2.7)</cell><cell>13.8 (4.8)</cell></row><row><cell>VisTASimple</cell><cell>5.9 (2.8)</cell><cell>7.1 (2.4)</cell><cell>18.2 (6.6)</cell></row><row><cell>VisTA</cell><cell>6.7 (3.3)</cell><cell>8.4 (3.8)</cell><cell>21.2 (5.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The number of times for pauses and rewinds (in μ(σ )).</figDesc><table><row><cell></cell><cell></cell><cell>Session 1</cell><cell>Session 2</cell><cell>Session 3</cell></row><row><cell></cell><cell>Baseline</cell><cell>5.0 (3.3)</cell><cell>5.4 (3.5)</cell><cell>7.6 (6.9)</cell></row><row><cell>Pauses</cell><cell>VisTASimple</cell><cell>8.4 (5.5)</cell><cell>8.0 (4.7)</cell><cell>17.1 (6.4)</cell></row><row><cell></cell><cell>VisTA</cell><cell>8.7 (7.3)</cell><cell>12.3 (7.6)</cell><cell>17.5 (7.5)</cell></row><row><cell></cell><cell>Baseline</cell><cell>4.5 (3.8)</cell><cell>4.3 (3.2)</cell><cell>7.6 (5.8)</cell></row><row><cell>Rewinds</cell><cell>VisTASimple</cell><cell>20 (15.3)</cell><cell cols="2">15.3 (12.3) 18.0 (9.4)</cell></row><row><cell></cell><cell>VisTA</cell><cell>5.2 (6.2)</cell><cell>9.0 (6.7)</cell><cell>9.8 (6.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The evaluators' session review strategies based on passes.</figDesc><table><row><cell></cell><cell cols="2">Session 1</cell><cell cols="2">Session 2</cell><cell cols="2">Session 3</cell></row><row><cell></cell><cell cols="2">1-Pass 2-Pass</cell><cell cols="2">1-Pass 2-Pass</cell><cell cols="2">1-Pass 2-Pass</cell></row><row><cell>Baseline</cell><cell>6</cell><cell>4</cell><cell>8</cell><cell>2</cell><cell>10</cell><cell>0</cell></row><row><cell>VisTASimple</cell><cell>4</cell><cell>6</cell><cell>6</cell><cell>4</cell><cell>8</cell><cell>2</cell></row><row><cell>VisTA</cell><cell>7</cell><cell>3</cell><cell>9</cell><cell>1</cell><cell>9</cell><cell>1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Operating Systems Design and Implementation</title>
		<meeting>the Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Va2: A visual analytics approach for evaluating visual analytics applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467871</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visualization of eye tracking data: A taxonomy and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13079</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="260" to="284" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<ptr target="http://www.praat.org/" />
		<title level="m">Praat: doing phonetics by computer</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Thinking aloud: Reconciling theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on professional communication</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="278" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The role of explanations on trust and reliance in clinical decision support systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bussone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>O'sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Healthcare Informatics</title>
		<meeting>the International Conference on Healthcare Informatics</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="160" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding usability practices in complex domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Chilana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Assessing concurrent think-aloud protocol as a usability test method: A technical communication approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cooke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Professional Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="202" to="215" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recovering reasoning processes from user interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stukes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Lipford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2009.49</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ux design innovation: Challenges for working with machine learning as a design material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Halskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="278" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aeonium: Visual analytics to support collaborative qualitative coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drouhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kocielnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pea-Araya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Aragon</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACIFICVIS.2017.8031598</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Pacific Visualization Symposium (PacificVis)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining concurrent think-aloud protocols and eye-tracking observations: An analysis of verbalizations and silences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. De</forename><surname>Jong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Professional Communication</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="206" to="220" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Protocol analysis: Verbal reports as data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Concurrent think-aloud verbalizations and usability problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3325281</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput.-Hum. Interact</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis in practical usability evaluation: a survey study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Følstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Do procedures for verbal reporting of thinking have to be reactive? a meta-analysis and recommendations for best reporting methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Best</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">316</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human-centred machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gillies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fiebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heloir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nunnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Characterizing users visual analytic activity for insight provenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST.2008.4677365</idno>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Symposium on Visual Analytics Science and Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Machine Learning for Designer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hebron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graphical histories for visualization: Supporting analysis, communication, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2008.137</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1189" to="1196" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What do thinkingaloud participants say? a comparison of moderated and unmoderated usability sessions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hertzum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Borlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Kristoffersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="557" to="570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scrutinising usability evaluation: does thinking aloud affect behaviour and mental workload?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hertzum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The evaluator effect: A chilling fact about usability evaluation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hertzum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="421" to="443" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Investigation of indirect oral operation method for think aloud usability testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Human Centered Design</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vader: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth international AAAI conference on weblogs and social media</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Will you accept an imperfect ai?: Exploring designs for adjusting end-user expectations of ai systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kocielnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<idno>pp. 411:1-411:14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Using the&apos;thinking-aloud&apos;method in cognitive interface design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lewis</surname></persName>
		</author>
		<idno>RC9265</idno>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
		<respStmt>
			<orgName>IBM TJ Watson Research Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Usability: lessons learned and yet to be learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="663" to="684" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Design of an intelligible mobile context-aware application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on human computer interaction with mobile devices and services</title>
		<meeting>the 13th international conference on human computer interaction with mobile devices and services</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Helping users recall their reasoning process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Lipford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stukes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST.2010.5653598</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring think-alouds in usability testing: An international survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Professional Communication</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="19" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dual verbal elicitation: the complementary use of concurrent and retrospective reporting within a usability test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="647" to="660" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Confirmation bias: A ubiquitous phenomenon in many guises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Nickerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of general psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="220" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Usability engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Thinking aloud: The# 1 usability tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
		<respStmt>
			<orgName>Nielsen Norman Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Demonstrate Thinking Aloud by Showing Users a Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nielson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Designing theory-driven user-centric explainable ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<idno>pp. 601:1-601:15</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Machine learning as a ux design material: How can we imagine beyond automation, recommenders, and reminders?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mapping machine learning advances from hci research to reveal starting places for design innovation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Banovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">130</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Investigating how experienced ux designers effectively work with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scuito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steinfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Designing Interactive Systems Conference</title>
		<meeting>the 2018 Designing Interactive Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="585" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Planning adaptive mobile experiences when wireframing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Conference on Designing Interactive Systems</title>
		<meeting>the 2016 ACM Conference on Designing Interactive Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Annotation graphs: A graph-based visualization for meta-analysis of data based on user-authored annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glueck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breslav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598543</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Supporting handoff in asynchronous collaborative sensemaking using knowledge-transfer graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glueck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2745279</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="340" to="350" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The impact of two different think-aloud instructions in a usability test: a case of just following orders?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="183" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
