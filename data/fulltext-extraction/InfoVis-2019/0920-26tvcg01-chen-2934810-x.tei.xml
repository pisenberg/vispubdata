<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20022006">2002 2006 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhutian</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianwen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huamin</forename><surname>Qu</surname></persName>
						</author>
						<title level="a" type="main">Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="20022006">2002 2006 2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934810</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automated Infographic Design</term>
					<term>Deep Learning-based Approach</term>
					<term>Timeline Infographics</term>
					<term>Multi-task Model</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The first year of my Ph.D. Everything is wonderful! My first submission to VIS has been acceptedâ€¦ My second submission to VIS has been accepted Again! Fig. 1: An automated approach to extract an extensible timeline template from a bitmap image. a) Original bitmap image; b) Content understanding including global and local information of the timeline; c) Extensible template contains editable elements and their semantic roles; d) New timeline (with mock-up colors) automatically generated with updated data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graphic designers have been producing infographics in a variety of fields, such as advertisement, business presentation, and journalism, because of their effectiveness in spreading information <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b51">52]</ref>. To inform data context and engage audiences, infographics are often embellished with icons, shapes, and images in various styles <ref type="bibr" target="#b25">[26]</ref>. However, creating infographics is demanding. Designers should consider not only perceptual effectiveness but also aesthetics, memorability, and engagement <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b17">19]</ref>. Researchers have introduced design tools <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref> to alleviate the burden of infographics creation by automating some processes (e.g., visual encoding). However, these tools require users to manually initialize most of the design (e.g., drawing graphical elements). The process remains difficult and time-consuming, especially for laymen, leading to the demand for automated infographic design.</p><p>Using templates is an effective approach to enable automated info- graphic design, which has been widely used in commercial software, such as Microsoft PowerPoint and Adobe Illustrator. These systems can automatically generate infographics by plugging in data to a design template. Although easy to use, these systems typically only provide limited types of templates with default styles, which leads to a lack of diversity. By contrast, many infographics "in the wild" with diverse styles can only be accessed as images in a bitmap format. If users want to follow the styles of these bitmap infographics, they have to manually create their own infographics, which is difficult and tedious. In this work, we investigate the methods of automatically extracting an extensible template from a bitmap infographic. Compared to editable templates, extensible templates contain not only the editable elements but also the semantic roles of these elements, which enable the automatic extension with updated data. Previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> attempt to extract visual encodings and color mappings from chart images based on rules and machine learning (ML) methods, by utilizing the legends, axes, plot areas, and common layouts. However, the content of infographics can be unstructured and manifold. This makes it challenging to analyze infographic images and extract extensible templates from them. As a first step towards automated infographic design, we focus on the timeline infographics, which have been widely used for centuries and whose design space has been extensively studied <ref type="bibr" target="#b8">[10]</ref>.</p><p>Automatically extracting an extensible template from a bitmap timeline infographic is non-trivial. Particularly, two obstacles stand in the way. First, it is challenging to interpret a bitmap timeline infographic au- tomatically. Understanding the content of the infographic is necessary for automating the extraction. However, the elements in an infographic can be distributed in any place with any style (e.g., shapes, colors, and sizes, etc.) It is difficult for a machine to interpret the infographic that can only be accessed in pixels. Second, it is intricate to convert a bitmap infographic to be extensible automatically. An understanding of a timeline infographic is not enough for using it as a template. Even if the machine has already obtained structural information of the timeline (e.g., type, orientation, and categories and locations of its elements), how to convert the timeline to an extensible template remains unclear, not to mention the information could be incorrect.</p><p>To address these challenges, we propose a novel end-to-end approach for automatically extracting an extensible template from a bitmap timeline infographic. Our approach adopts a deconstruction and reconstruction paradigm. We address the first challenge at the deconstruction stage. We propose a multi-task deep neural network (DNN) that simultaneously parses two kinds of information from a timeline image: global and local information. Global information includes the representation, scale, layout, and orientation of the timeline. Local information includes the location, category, and pixels of each visual element on the timeline. These two kinds of information provide a panorama of the timeline. We tackle the second challenge at the reconstruction stage. By utilizing the deconstruction results, we propose a pipeline with three techniques, i.e., Non-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an extensible template from the infographic. The output can be used to generate new timelines with updated data.</p><p>To evaluate our approach, we synthesize a timeline dataset with 4296 labeled images and collect a real-world timeline dataset from the Internet. We report quantitative evaluations of the two stages over the two datasets. We then present examples of automatically extracted templates with various visual styles and timelines automatically generated based on these templates to qualitatively demonstrate the eperformance. The results confirm that our approach can effectively extract extensible templates from real-world timeline infographics. Finally, we discuss lessons learned and future opportunities.</p><p>Our primary contribution is an automated approach to extracting extensible templates from bitmap infographic timelines. The approach consists of 1) a multi-task DNN that automatically deconstructs bitmap timeline infographics and 2) a pipeline that automatically reconstructs extensible templates. We evaluate our approach with quantitative evaluations and qualitatively demonstrate its effectiveness with examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>This section introduces prior studies that are most relevant to our work, including automated visualization design, computational interpretation of visualization, and deep learning-based object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Automated Visualization Design</head><p>Automated visualization design systems aim at producing visual encodings for given input data based on both the criteria summarized by experts (e.g., Bertin <ref type="bibr" target="#b6">[8]</ref>, Cleveland and McGill <ref type="bibr" target="#b11">[13]</ref>) and constraints defined by users <ref type="bibr" target="#b36">[37]</ref>. Prior work on automated visualization design can be classified into two general categories based on how the criteria are derived: rule-based and learning-based approaches.</p><p>Mackinlay's APT system <ref type="bibr" target="#b33">[34]</ref> is a pioneering example that enumerates, filters, and ranks visualizations using expressiveness and perceptual effectiveness criteria. It was extended by SAGE <ref type="bibr" target="#b41">[42]</ref>, BOZ <ref type="bibr" target="#b49">[50]</ref>, and ShowMe <ref type="bibr" target="#b34">[35]</ref> with additional considerations of data properties, low-level perceptual tasks, and candidate groupings. Recent systems like Voyager and Voyager 2 <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> have further recommended data transformation (e.g., normalization) in addition to visual encodings.</p><p>Foregoing explicit rules, researchers have recently designed learningbased systems that directly learn visualization designs from visualization corpora. DeepEye <ref type="bibr" target="#b32">[33]</ref> applies ML models and design rules to determine whether a visualization is "good" or "bad" and recommends the "good" candidates. Data2Vis <ref type="bibr" target="#b12">[14]</ref> uses a Recurrent Neural Network to automatically translate JSON-encoded datasets to Vega-lite <ref type="bibr" target="#b44">[45]</ref> specifications. Draco <ref type="bibr" target="#b36">[37]</ref> learns weights between hard and soft constraints that represent users' requirements and design guidelines. VizML <ref type="bibr" target="#b19">[21]</ref> trains a fully-connected neural network to predict design choices based on input data. Although we also aim for automated design, these systems, however, cannot be adapted to infographics. They focus mainly on recommending visual encodings for the input data (e.g., how to encode data using visual channels). By contrast, designing infographics requires additional attention to visual styles (e.g., how to embellish the visualization with shapes and icons), which are omitted in these systems. In this regard, our work is inherently different from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computational Interpretation of Visualization</head><p>Computational interpretation of visualization seeks to enable machines to understand the content of visualization images (e.g., data, styles, and visual encodings). According to the targets, prior methods can be divided into two categories: for charts and for infographics.</p><p>A general pipeline when interpreting a chart is first to identify the type of the chart via classifications, then detect elements (e.g., marks or text) in the chart, and finally extract the underlying information (e.g., data or visual encodings). As a pioneer, Savva et al. introduced ReVision <ref type="bibr" target="#b45">[46]</ref>, in which the graphical and textual features are fed into a support vector machine (SVM) model for a chart type classification. ReVision then localizes the marks and extracts data from pie and bar charts by using a carefully designed multi-steps method based on image processing and heuristics. Siegel et al. <ref type="bibr" target="#b47">[48]</ref> extended the method of ReVision to handle line charts. They developed a convolutional neural network (CNN) for the chart classification and designed a heuristic approach to use legend information for data extraction. Recently Kafle et al. <ref type="bibr" target="#b21">[23]</ref> have used a deep dual-network model to directly parse the data from bar charts without heuristic rules. Instead of extracting the data of charts, Poco and Heer <ref type="bibr" target="#b37">[38]</ref> aimed to recover the visual encodings. To complete the task successfully, they proposed a state-ofthe-art approach to interpreting the text in a multi-stage pipeline, which combines ML and heuristics methods. Building on this, Poco et al. <ref type="bibr" target="#b38">[39]</ref> further explored the color mapping extraction of visualization images.</p><p>Apart from charts, researchers have explored the computational interpretation of infographics. Bylinskii et al. <ref type="bibr" target="#b10">[12]</ref> used fully convolutional networks (FCNs) to predict the visual saliency of an infographic. Bylinskii et al. <ref type="bibr" target="#b9">[11]</ref> also applied DNNs to select representative textual and visual elements from an infographic automatically. On the basis of several deep learning models, Kembhavi et al. <ref type="bibr" target="#b23">[25]</ref> designed a multistage approach to parse the relationships among elements in diagrams in science textbooks. More recent research investigated using DNNs to detect UI components in mobile apps <ref type="bibr" target="#b30">[31]</ref> and icons in infographics <ref type="bibr" target="#b35">[36]</ref>. Although these methods enable computational understanding of an infographic from certain perspectives, the information they interpret cannot be used to reconstruct an extensible template (e.g., how to change or extend the content of an infographic is unknown). We take a first step towards the interpretation of infographics for an automated design purpose. Unlike using multiple models and handcrafted features, our approach uses one end-to-end DNN to complete the interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning-based Object Detection</head><p>To extracting an extensible template, we need to understand each object on it. We achieve this goal with deep learning-based object detection. Object detection is a computer vision (CV) task whose goal is to localize each object using a bounding box (i.e., where) and classify its category (i.e., what). Deep learning-based object detection methods can either be one-stage <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref> or multi-stage <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b40">41]</ref>. One-stage models directly predict objects' bounding box and category without involving intermediate tasks. YOLO <ref type="bibr" target="#b39">[40]</ref> is a representative one-stage model that divides the image into small cells and predicts bounding boxes for each cell. One-stage models have the advantage of fast detection in real time, which affects accuracy. By contrast, multi-stage models can predict accurately, but are often less time efficient. Multi-stage models, such as RCNN <ref type="bibr" target="#b15">[17]</ref>, usually first propose a manageable number of candidate regions (region proposals) that may contain objects. If an object exists within, then they will predict its bounding box and category. Time consumption is not our first priority, so we base our work on a multistage model. Mask R-CNN <ref type="bibr" target="#b22">[24]</ref> is a leading multi-stage model in several benchmarks. It can further predict the pixels of an object within its bounding box (i.e., Instance Segmentation). We extend Mask R-CNN to interpret not only the information of objects (i.e., local) but also that of the entire timeline infographic (i.e., global). To the best of our knowledge, we are the first to adopt this kind of instance segmentation networks to deal with the infographics interpretation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM STATEMENT</head><p>This section introduces the background of timeline infographics and the problem, overview of the proposed approach, and the datasets. Timeline infographics have been recently investigated by Brehmer et al. <ref type="bibr" target="#b8">[10]</ref>. We briefly describe the insights from them as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>Timeline Data. A timeline presents interval event data (i.e., a sequence of events), which is different from continuous quantitative time-series data. A timeline infographic for storytelling usually has a small underlying dataset because the storyteller is assumed to have already distilled the narrative points from the raw dataset. Timeline Design. A timeline can be described as a combination of three dimensions, namely, representation, scale, and layout. The combination can be used as the type of timelines. No more than five options are available for each dimension <ref type="table" target="#tab_2">(Table 1)</ref>. Besides, only 20 out of 100 combinations of these options are viable. These dimensions indicate how the events are organized in a timeline. For example, events are placed along a straight line in a linear representation, which is the most common way to represent a timeline. Typically, an event is visually encoded by a graphical mark, such as the rectangles in <ref type="figure" target="#fig_0">Fig. 2</ref>. The position of this mark is used to encode the occurred time of the event. Extra annotations (e.g., text or icons) are added, commonly adjacent to the event mark, to depicts the details of an event. In practice, infographic timelines are widely spread in the form of bitmap images. However, they are not easy to reproduce. Given a bitmap timeline, we aim to extract its extensible template <ref type="figure" target="#fig_0">(Fig. 2</ref>) automatically. To this end, two requirements should be fulfilled:</p><p>Parse the content. The machine should first parse the content of the image. A computational understanding of an image can be represented as a structural information, which is necessary for an automation process. However, the infographic image can only be accessed in pixels, which is a byte array with the shape of width Ã— height Ã— RGB.</p><p>A process is required to take the bitmap image as input and output its structural information. Construct the template. With the structural information of the image as a basis, the machine should be able to construct an extensible template out of it automatically. The template should contain detail information (e.g., position, color, font, and shape) of the elements to be reused and the elements to be updated. Given the image and its structural information, another process should be involved to extract such types of detail information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approach Overview</head><p>To fulfill the two requirements above, we design a two-step approach, starting from defining the input and output of each step.</p><p>Deconstruction. The goal of the first step ( <ref type="figure">Fig. 1a</ref> and <ref type="figure">Fig. 1b</ref>) is to parse structural information from the input, a bitmap timeline infographic I. For the output, we define two kinds of information, namely, the global one G and the local one L. The global information is about the entire timeline, including its three dimensions mentioned in <ref type="table" target="#tab_2">Table 1</ref> and its orientation. The local information is about each individual element, including its category (what), location (where), and the pixel-wise mask (which pixels). Therefore, the ideal process of the first step can be formulated as a mapping function f :</p><formula xml:id="formula_0">f : I â†’ (G, L)<label>(1)</label></formula><p>We propose to approximate f using a DNN model h â‰ˆ f with a set of parameters Î˜. This set of parameters Î˜ can be learned from a corpus C = {(</p><formula xml:id="formula_1">I i : (G i , L i ))} n i=1 , where each entry (I i : (G i , L i ))</formula><p>is a bitmap image associated with its global and local information. Hence, we can obtain the output via (G, L) = h(I|Î˜).</p><p>Reconstruction. To reconstruct the extensible template, a function g should take the bitmap infographics I and its global and local information G, L as the input, and return the detail information about elements to be reused E r (e.g., the rectangle and circle marks in <ref type="figure" target="#fig_0">Fig. 2a</ref>) and elements to be updated E u (e.g., the text and icons in <ref type="figure" target="#fig_0">Fig. 2a</ref>), i.e., g :</p><formula xml:id="formula_2">(I, G, L) â†’ (E r , E u )<label>(2)</label></formula><p>E is a set of elements, each of which is represented as a set of attributes, i.e., E = {e</p><formula xml:id="formula_3">i := (a 1 , a 2 , ..., a m )} n i=1 .</formula><p>According to G and L, we can infer attributes of elements in E r and E u , such as size, shape, color, position, and offset to others. We highlight the necessary attributes for enabling extensible templates. For E r , the essential attribute is the graphical marks to be reused (e.g., the rectangle marks in <ref type="figure" target="#fig_0">Fig. 2a</ref>). Hence, we need to segment the pixels of E r from the original image.</p><p>As for E u , the attributes related to the font (e.g., font family, size, color, etc.) must be identified to maintain the styles of the updated content. In addition, we note that the outputs from h may not be perfect, reducing the quality of the outputs of g. Thus, g should be smart enough to correct errors in G and L as much as possible.</p><p>Considering these issues, we design a heuristic-based pipeline, with three novel techniques, as g to automatically output E r and E u . <ref type="figure">Fig. 3</ref>: Categories of elements in a timeline infographic. The event mark, annotation mark, and main body can be reused, while others need to be updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Datasets</head><p>We use two datasets to train the model h and evaluate our approach. The first one (referred to as D 1 ) is a synthetic dataset. We extended TimelineStoryteller (TS) <ref type="bibr" target="#b3">[5]</ref>, a timeline authoring tool, to generate D 1 , covering all types of timelines. The second dataset (referred to as D 2 ) consists of real-world timelines, collected from Google Image [4], Pinterest <ref type="bibr" target="#b4">[6]</ref>, and FreePicker <ref type="bibr" target="#b2">[3]</ref> by using the search keywords timeline infographic and infographic timeline. D 2 has more diverse styles, especially for marks, and it covers the most common types of timelines. To scope this work, we focus on timelines that have less than 20 events and whose events have the same number and types of annotations (e.g., text and icon). We also exclude the titles, footnotes, and legends. Collection. For D 1 , TS allows us to generate timeline images with various visual encodings and styles. We generated timeline images using nine embedded datasets of TS to cover the design space of timelines. To increase diversity, we randomly modified the timeline orientation, the style of graphical marks (including color, size, and shape), texts (font, size, color, offset to others), and the background (color) in a curated range that guarantee the viability of the timeline. We created 9592 timelines in this process.</p><p>For D 2 , we implemented crawlers to download the search results. The crawling process was manually monitored and stopped when 10 consecutive return results are not timelines. We collected 1138 timelines in this process. Following, four of the coauthors separately reviewed all the timelines to remove the repeated and problematic instances, such as images with heavy watermarks or with low resolutions (i.e., smaller than 512 Ã— 512), and timelines that out of the scope of this work. They obtained 412 remaining timelines. The scale of D 2 is consistent with manually collected visualization datasets in similar research <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Among the five representations in <ref type="table" target="#tab_2">Table 1</ref>, radial, grid, or spiral representations appear only 19/412 (4.6%) timelines, whereas the rest 393 timelines are with linear or arbitrary representations. This ratio is consistent with <ref type="bibr" target="#b8">[10]</ref> (23/263, 8.7%). Considering the scarce number of the radial, grid, or spiral representations, we excluded them in D 1 and D 2 and focused on the more common linear and arbitrary representations.</p><p>Labeling. To identify the categories of elements in a timeline, four of the coauthors independently reviewed all the timelines in D 1 and D 2 . Each of them iteratively summarized a set of mutually exclusive categories that can be used to depict elements in a timeline infographic. Gathering the reviews resulted in six categories <ref type="figure">(Fig. 3)</ref>. We explain the details of these categories in the supplemental material.</p><p>Each timeline in D 1 was then converted from SVG to bitmap format and annotated with its representation, scale, layout, and orientation. We also analyzed the SVG and the bitmap to generate the annotations for each element in a timeline, including its category (from the label sets in <ref type="figure">Fig. 3</ref>), bounding box (referred to as bbox), and pixel-wise mask (referred to as mask). For each timeline in D 2 , we manually annotated its representation, scale, layout, and orientation, as well as the category, bbox, and mask of each element, by using our annotation tool that is built on Microsoft PowerPoint. Finally, D 1 contains 4296 timelines, whereas D 2 contains 393. <ref type="figure" target="#fig_1">Figure 4</ref> and <ref type="table" target="#tab_3">Table 2</ref> present samples and statistics of these timelines, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DECONSTRUCTION</head><p>Parsing bitmap timeline infographics to extract structural information is difficult due to the absence of fixed rules for the styles and layouts of timeline elements. We achieve this goal from two perspectives, global and local. In contrast with prior studies <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48]</ref> that extracted structural information from charts using different methods in multiple steps, we use a DNN to extract structural information in one shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parsing Global Information</head><p>Our dataset comprises 10 types of timelines. The type, i.e., the combination of the three dimensions in <ref type="table" target="#tab_2">Table 1</ref>, is necessary for constructing an extensible template. In addition to the type of timeline, the orientation, which could be horizontal, vertical, and others, is equally indispensable for the template. As type and orientation only involve a few discrete choices, we can identify them through classification.</p><p>Taking into account that CNN models have shown excellent capability in chart classification <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>, we propose a CNN-based classifier to recognize the type and orientation of a timeline. Many CNN architectures have been proposed (e.g., AlexNet <ref type="bibr" target="#b26">[27]</ref>, GoogLeNet <ref type="bibr" target="#b50">[51]</ref>). We use ResNeXt <ref type="bibr" target="#b55">[56]</ref>  <ref type="figure" target="#fig_2">(Fig. 5a</ref>) to extract the features of a timeline infographic, since ResNeXt achieves state-of-the-art performance in many CV tasks. It takes a 3-channel image (i.e., RGB) as input and output a feature map with 2048 channels. We then use two siblings fully connected (FC) layers <ref type="figure" target="#fig_2">(Fig. 5b)</ref> as Class heads to predict the timeline's type and orientation based on the feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parsing Local Information</head><p>After parsing the global information, the machine should further extract the local information of the timeline. We have defined six categories of elements <ref type="figure">(Fig. 3</ref>) in a timeline. We need to detect each element in the timeline (where and what) and segment it from others (which pixels).</p><p>To tackle these tasks, a possible solution is to solve them one by one using well-established methods. For example, we can use sliding windows <ref type="bibr" target="#b27">[28]</ref> to localize elements, then employ SVM to determine the category of the element within, and lastly segment the element from the image. This multi-step solution can be effective and has been used in previous works <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>. However, given the ad-hoc nature, extending this solution to other scenarios is challenging. Therefore, we prefer to adopt a unified method to complete all tasks.</p><p>Considering that we have already extracted the feature maps of the infographic in Sect. 4.1, we propose to reuse these feature maps, which contain rich information of the image. Specifically, we extend the classification model in <ref type="figure" target="#fig_2">Fig. 5</ref> by adding components for object detection to parse the local information. We achieve this extension using Mask R-CNN <ref type="bibr" target="#b22">[24]</ref>, a leading architecture that can detect objects and predict their pixel masks. By this means, our model can simultaneously finish all five tasks (i.e., two global and three local) in one shot. The complete architecture is depicted in <ref type="figure">Fig. 6</ref>. We successfully train this multi-task learning model and achieve a good performance. <ref type="figure">Fig. 6</ref>: Complete architecture to parse both global and local information simultaneously. Apart from the components in <ref type="figure" target="#fig_2">Fig. 5</ref>, we add more components (in green color) to parse local information.</p><p>In the architecture <ref type="figure">(Fig. 6</ref>), we first extend ResNeXt with Feature Pyramid Network <ref type="bibr" target="#b28">[29]</ref> (FPN). FPN is a top-down architecture that can build semantically strong feature maps at multiple scales using the feature maps from ResNeXt. FPN makes our model scale-invariant and able to handle images of vastly different resolution. We then feed the feature maps from the ResNeXt-FPN into a Region Proposal Network <ref type="bibr" target="#b40">[41]</ref> (RPN) to localize elements in a timeline. RPN is an FCN that simultaneously predicts element locations (i.e., by bbox) and objectness scores (i.e., whether there is an object within the bbox) in an image. These element location hypotheses are then be used to extract regions of interest (RoIs) from the feature maps. Each RoI is normalized to a fixed size using a RoIAlign layer and then passed to two heads, namely, a Box head and a Mask head. The Box head uses two sibling FC layers to classify the category and regress the bbox of the element. The Mask head uses an FCN for predicting the pixels of the element within the bbox. Additional details on the architecture and training process are presented in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Validation</head><p>Our model is implemented using Pytorch <ref type="bibr" target="#b5">[7]</ref> with two types of CNN backbone, namely, ResNeXt-50 (R50) and ResNeXt-101 (R101), following the standard configurations <ref type="bibr" target="#b55">[56]</ref>. R50 has 50 layers, which is more lightweight and easier to train, while R101 has 101 layers, which performs better in CV tasks at the cost of efficiency and is more difficult to train. We trained these two implementations of our model using D 1 and D 2 together. We randomly split the images in D 1 and D 2 into 9 : 1 such that no testing sample is in the training set. To increase the diversity of the training data, we conduct several data augmentation strategies, including random horizontal or vertical flip, random 90-degree rotations (the labels are updated accordingly), and random color channels swap. Finally, the number of training samples for one epoch is 33760. We evaluated models trained with 10 epochs on the two datasets separately. We first report the performance of parsing global information and then report the average precision (AP) on parsing local information. Reported numbers are averaged over 10 independent runs.</p><p>Parsing Global Information. To access the performance of our model on the two classification tasks (i.e., 10 classes of timeline type and 3 classes of orientation), we calculate the precision, recall and F1-score. <ref type="table" target="#tab_4">Table 3</ref> presents the results. Both implementations achieve good performance on D 1 and D 2 . As expected, R101 has a better performance on D 1 and D 2 than R50. The classification of type on D 2 performs worse than that on D 1 , which is largely due to the more diversity and small size of D 2 . Nevertheless, F1-score is still higher than 90% when using R101. Parsing Local Information. To evaluate the performance of parsing local information, we use the metrics in COCO challenge <ref type="bibr" target="#b0">[1]</ref>. COCO is a large-scale object detection and segmentation dataset that contains more than 330K images with high-quality annotations. It uses AP metrics <ref type="bibr" target="#b13">[15]</ref> to access the three tasks (i.e., what, where, and which pixels) together. Basically, AP is a measure of precision-recall tradeoff calculated using all possible confidence level that is represented by the classification score associated with each predicted bbox. Intuitively, AP is the area under the precision-recall curve <ref type="figure" target="#fig_3">(Fig. 7)</ref>. To calculate the precision and recall at a confidence level, we first need to calculate the intersection over union (IoU) between each predicted bbox B p and its corresponding ground truth B g t by IoU = area(B p âˆ©B gt ) area(B p âˆªB gt ) . If the IoU exceeds a threshold (e.g., 0.5), the prediction is considered as a true positive; otherwise a false positive. We can then further calculate the precision and recall over all confidence level to draw the curve.  <ref type="bibr" target="#b22">[24]</ref>. <ref type="table" target="#tab_5">Table 4</ref> presents the AP of our model on the two datasets. The higher the AP, the better it is. We provide state-of-the-art performance on COCO reported by He et al. <ref type="bibr" target="#b22">[24]</ref> as a background, due to the lack of benchmarks. AP 50:95 is the average AP over different IoU, from 0.5 to 95 with step 0.05. AP 75 and AP 50 is the AP calculated at IoU = 0.75 and IoU = 0.5, respectively. The larger the IoU, the stricter the metric will be. As indicated in <ref type="table" target="#tab_5">Table 4</ref>, our model achieves high AP on bbox detection and pixel segmentation on D 1 . This result is because the overall diversity of D 1 is limited, the size of D 1 is big enough in terms of its diversity, and the auto-generated annotations of D 1 are perfect for enabling effective learning. By contrast, D 2 has more diversity, a smaller size, and imperfect annotations in comparison with that of D 1 , leading to a decrease in performance. Nevertheless, our model still achieves an acceptable performance on D 2 , considering the stateof-the-art performance on COCO. We further discuss the annotation perfectness of D 2 in Sect. 5.5. <ref type="figure">Fig. 8</ref>: The reconstruction pipeline: a) uses NMM and RR to eliminate repeated and fix failed detections, respectively; b) uses DL GrabCut and text recognition to collect the elements to be reused and updated, respectively; c) the final outputs can be depicted by a specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RECONSTRUCTION</head><p>After interpreting a timeline infographic, the next problem is how to automatically extract an extensible template from it. We achieve this by a reconstruction pipeline <ref type="figure">(Fig. 8</ref>) that exploits the outputs from the previous step. Our pipeline first eliminates the repeated bboxes using Non-Maximum Merging (NMM), and then infers the missing elements using Redundancy Recovery (RR). Next, DL GrabCut is employed to extract high-quality graphical marks for reuse. Finally, the font of event text and annotation text is identified using a publicly available API. A quantitative validation confirms the effectiveness of our pipeline. <ref type="figure">Fig. 9</ref>: Eliminate repeated bboxes: a) NMS keeps the bbox with the highest confidence and removes the others; b) NMM merges bboxes to the one with the highest confidence and the largest area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Eliminate Repeated BBoxes: Non-Maximum Merging</head><p>Multiple predicted bboxes may exist on one object, such as the two bboxes in <ref type="figure">Fig. 9a</ref>. A commonly used method to remove repeated bboxes for natural images is Non-Maximum Suppression (NMS) <ref type="bibr" target="#b15">[17]</ref>. NMS iteratively eliminates bboxes whose confidence score (i.e., the classification score) are less than a predefined threshold. For instance, in <ref type="figure">Fig. 9a</ref>, with a threshold of 0.8, NMS will eliminate the pink bbox with 0.58 and output the red one. However, for infographics, a part of an object may still be a "complete" object, which hinders the effectiveness of NMS. For example, in <ref type="figure">Fig. 9b</ref>, the mark in the steel blue bbox and the part of it in the deep blue bbox are both valid annotation marks. In such case, each of the two bboxes will be assigned a high confidence score (e.g., 1.0). Therefore NMS cannot eliminate the repeated box.</p><p>Therefore, we design NMM to eliminate repeated bboxes. Specifically, for bboxes with the same category, we rank them using the confidence score plus the area (normalize to [0, 1]). For the top 1 bbox, we merge other bboxes that overlap with it and exceed a IoU threshold to form a union bbox. This process is repeated until all overlapping boxes are merged. <ref type="figure">Fig. 9b</ref> shows the boxes before and after NMM. In practice, we apply both NMS and NMM separately and then check the consistency of the shapes between the resulted bboxes and other non-repeated bboxes. The most consistent results are kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fix Failed Detections: Redundancy Recovery</head><p>Our model may miss elements (i.e., false negative) or detect elements with wrong categories (i.e., false positive). To fix these failed detections, we leverage the redundant information of timelines (e.g., each event has the same type of annotations). Specifically, for the elements in a timeline, we first group them along the timeline orientation into clusters, each of which represents an event. Then, we use the statistics of clusters to verify and attempt to fix failed detections.</p><p>Incorrectly classified elements. Some elements in an infographic can be classified into wrong categories. For example, a short annotation text with a fancy font can be incorrectly classified as an annotation icon. We adopt a voting mechanism to attempt to fix these misclassifications. For instance, if more than half of the events contain annotation text, then an annotation icon, whose bbox has the same shape as these annotation texts, of an event should be classified as an annotation text. Given an event can have multiple annotation texts, we restrict that only the annotation texts with the consistent shape of bbox can vote for each other. This rule is also applied to other categories.</p><p>Missing elements. We also use a similar voting mechanism to infer the undetected elements. For example, in <ref type="figure">Fig. 8a</ref>, more than half of the events have an event text. Thus, for the event without an event text (i.e., the event in 2015), we assume it should have an event text. By using heuristic rules, we can estimate the bbox (i.e., x, y, width, height) of its event text based on those of other event texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Elements to be Reused</head><p>For an extensible template, certain elements must be reused via segmentation from the infographic image. Our model can predict the pixels (i.e., mask) of each element for the segmentation. However, the quality of these predicted pixels <ref type="figure">(Fig. 10b</ref>) may not be accurate enough for template generation. <ref type="figure">Fig. 10</ref>: DL model "interacts" with GrabCut by using the bbox and mask: a) the bbox and mask predicted by our model; b) the predicted mask is coarse; c) the refined result from DL GrabCut.</p><p>To tackle this issue, we use the outputs from the model <ref type="figure">(Fig. 10a)</ref> as the input to GrabCut <ref type="bibr" target="#b42">[43]</ref> algorithm. GrabCut is an interactive segmentation algorithm that has been widely used in production tools, such as Microsoft PowerPoint. It performs well especially when the background and foreground are not similar and the edges of the foreground are crisp, which is a good fit for our scenario. To segment an element, GrabCut needs a bbox around the target element as an input. Then, the user can further refine the segmentation results by drawing strokes to mark the probable foreground and background area.</p><p>Our idea is to automate this process using the outputs of the model to imitate the user interactions. For each predicted element, we use its bbox as the bbox drew by human and its mask as the user's strokes to refine the segmentation. By this means, we leverage the semantic information from the DL model and the advantage of GrabCut on image processing to obtain high-quality masks <ref type="figure">(Fig. 10c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Elements to be Updated</head><p>Among the six categories of elements, three categories of elements need to be updated, namely, event text, annotation text, and annotation icon. Annotation icon can be updated by directly using new icons, whereas event text and annotation text should maintain the same styles with the original infographic, including their font family, color, and size. To identify the font family, we use Font Identifier powered by Fontspring Matcherator <ref type="bibr" target="#b1">[2]</ref>. The font size and color can be calculated and extracted from the pixels of the text in the bitmap image. Some annotation text contains title and body text. We heuristically identify the text with the larger font size as the title and the smaller one as the body. To improve the extensibility of the template, we further use OCR engine (i.e., Tesseract <ref type="bibr" target="#b48">[49]</ref>) to recognize the content of event text to infer the visual encodings of the timeline following the method in <ref type="bibr" target="#b37">[38]</ref>. The final outputs can be depicted using a structural document <ref type="figure" target="#fig_4">(Fig. 11)</ref>.  <ref type="figure">(top, le f t, width, height)</ref>. The mask is a byte array with shape width Ã— height.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Validation</head><p>To evaluate the effectiveness of our pipeline, we reuse the R101 trained in Sect. 4.3. We are interested in two aspects of our pipeline: whether it can correct the failed detections and whether it can refine the segmentation results. We only test our pipeline on D 2 , because the prediction results on D 1 are good enough to skip the steps that we want to access.</p><p>To obtain prediction outputs, we select the confidence level per category based on the precision-recall curve calculated in Sect. 4.3. We calculate the precision and recall of the predictions with IoU at 0.5 and 0.75. We then apply our pipeline on the predictions and calculate the gains of precision and recall on each step. We expected the following:</p><p>NMM can improve the precision of bbox and mask predictions as it removes a few false positives. RR can improve the precision and recall of bbox and mask prediction as it increases the number of true positives. DL GrabCut can improve the precision and recall of mask prediction as it improves the quality of masks. <ref type="table" target="#tab_6">Table 5</ref> presents the gains on precision and recall after each step. The gains at IoU 0.5 are close to those at IoU 0.75, which means the gains from the reconstruction pipeline are strict and stable. As expected, the NMM shows a gain on the precision of bboxes and masks predictions. We also observe a small gain on recall. The analysis results indicate that such small gain is attributed to the merging results increasing the number of true positives in some cases (e.g., two false positives become one true positive after merging). Moreover, RR shows a gain on the recall of bboxes and masks predictions. These results confirm that our technique can correct some failed predictions (i.e wrong and missing).  A surprising finding is the decrease in the precision and recall of mask predictions. Our investigation reveals that this decrease is due to the imperfect labels. <ref type="figure" target="#fig_0">Figures 12a and 12b</ref> show an annotation mark and its label. <ref type="figure" target="#fig_0">Figure 12c</ref> presents the prediction result and <ref type="figure" target="#fig_0">Fig. 12d</ref> shows the result refined by DL GrabCut. The manually labeled mask encapsulates the graphical mark with empty spaces and a border. Meanwhile, the result from DL GrabCut perfectly matches the graphical mark but not the label. Thus, even the result from DL GrabCut is of high quality, it can also be changed from a true positive to a false positive, leading to a decrease in precision and recall. Hence, we manually compare the segmentation results of each element before and after using DL Grab-Cut to verify its effectiveness. The comparison confirms the usefulness of DL GrabCut in the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXTRACTED RESULTS AND GENERATED EXAMPLES</head><p>Example extracted results are shown in <ref type="figure">Fig. 13</ref>. High-resolution results can be found in the supplemental material. Our approach can extract templates from not only the timelines with linear representations (e.g., horizontal <ref type="figure">Fig. 13b and 13d</ref>, and vertical <ref type="figure">Fig. 13e and 13g</ref>), but also those with arbitrary representations <ref type="figure">(Fig. 13a, 13c, and 13f)</ref>. <ref type="figure">Figure 13d</ref> shows that our approach is not affected by the background image. <ref type="figure">Fig. 13</ref>: Example results from D 2 . We visualize the final predicted category, bbox, and mask of each element, following the color legend in <ref type="figure">Fig. 3</ref>. We use gray-scale images for a clear demonstration. The original images and extra results can be checked in the supplemental material.</p><p>To present a usage example of these extensible templates, we further implement a timeline renderer by extending TS <ref type="bibr" target="#b3">[5]</ref>, an open-source tool that allows users to generate timelines for their data automatically. It embeds a collection of heuristics to render timelines based on the timeline representation, scale, and layout chosen by users. Currently, TS provides a set of default styles (e.g., using rectangles as event marks). We reuse and extend the heuristics in the tool and adapt it to our templates, thus enabling generations of embellished timeline infographics. We also add some heuristic rules for effectively using marks in templates, such as looping through the marks when the number of events exceeds that of the marks. We present two examples to illustrate the generation process. <ref type="figure" target="#fig_1">Fig. 14:</ref> A default timeline generated for the data in a) is shown in b). The result of applying the template from <ref type="figure">Fig. 13a</ref> is presented in c).</p><p>Generating timeline reusing graphical elements. In this example, we reuse the graphical elements in <ref type="figure">Fig. 13a</ref> to generate a new timeline <ref type="figure" target="#fig_1">(Fig. 14c)</ref> for the Chinese dynasties data <ref type="figure" target="#fig_1">(Fig. 14a</ref>). To achieve this, we first use TS to render the data with default styles <ref type="figure" target="#fig_1">(Fig. 14b)</ref>. The template is then applied to the default timeline and the underlying data. Specifically, we first substitute marks in <ref type="figure" target="#fig_1">Fig. 14b</ref> (e.g., the rectangles and red circles) with the marks in the template (e.g., event and annotation marks), which are segmented from the existing timeline. The margin and position of each mark are also updated accordingly. Then, the event content is rendered at the position corresponding to each event mark using the font from the template (i.e., annotation text). Next, the time of each event is rendered, though it has not been visualized in <ref type="figure" target="#fig_1">Fig. 14b</ref>, since it exists in the data and the template has slots for the event text. Finally, the icon (i.e., the Chinese character) in the data is displayed in the bbox of the annotation icon for each event. This whole process is finished programmatically and automatically by enumerating and binding the data to the template, as we have the semantic role (i.e., the category) of elements in the template. <ref type="figure" target="#fig_2">Fig. 15: a)</ref> A timeline generated by using the template from <ref type="figure">Fig. 13b</ref>. b) The result of applying the representation from <ref type="figure">Fig. 13f to a)</ref>.</p><p>Generating timeline reusing representations. In this example, we reuse the representation in <ref type="figure">Fig. 13f</ref> to generate a new timeline <ref type="figure" target="#fig_2">(Fig. 15b)</ref> for mock-up data. To this end, we first render a timeline (i.e., <ref type="figure" target="#fig_2">Fig. 15a</ref>) using the graphical elements from <ref type="figure">Fig. 13b</ref>, following the same process mentioned above. The arbitrary representation extracted from <ref type="figure">Fig. 13f</ref> is then reused in <ref type="figure" target="#fig_2">Fig. 15b</ref>. Specifically, we enumerate events in <ref type="figure" target="#fig_2">Fig. 15a</ref> and place them one by one according to the position (represented by bbox) of elements in <ref type="figure">Fig. 13f</ref>. Finally, two events are adjusted to be vertical after the generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>In this section, we first share the lessons learned from our study, which outline the need for human-ML collaborative authoring tool and graphical image-driven deep learning. Then, we discuss how our work can serve as a basis for future research and acknowledge the limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Human-ML Collaborative Authoring Tool</head><p>Efforts have been made to aid visualization authoring by using ML. As a pioneering work, ReVision <ref type="bibr" target="#b45">[46]</ref> uses SVM and multiple rule-based methods (i.e., computational interpretation) to parse the global and local information of bitmap charts. This information is used to help users to redesign problematic charts. Although ReVision successfully decomposes charts, its rules limit its extendibility to tackle more complex visualizations (e.g., infographics). In recent years, given the rapid advances in DL, it is gradually possible to use data-driven models rather than rule-based methods to interpret charts and even infographics. In this work, we explore this direction and contribute a unified model to successfully parse timeline infographics.</p><p>Note that our approach aims to use automation to assist, not replace, human designers in the visualization design. We realize that it is important to keep the human in the design process. This is not only because it is difficult to get a perfect model, but also because the design process is creative and subjective, and thus can hardly be fully automatic. First, human interactions can steer the model and refine the model's results. For example, although we design several automatic post-processing steps to enhance the overall performance of our model, the user can further adjust the model (e.g., the confidence level) for the desired output. Furthermore, the human should be at the center of visualization authoring, while the ML model should assist, rather than replace, the designer. For instance, the generated results of our approach can aid users as stepping stones to initialize the visualization, instead of being the final designs. However, designing human-ML collaborative authoring tools that go beyond asking designers to refine the model results remains underexplored. We envision how to design authoring tools seamlessly integrate imperfect ML models into the design process as an important research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Graphical Image-Driven Deep Learning</head><p>The DL revolution is driven by tasks on natural images. However, the specificity of graphical images (e.g., charts and infographics) leads to requirements that cannot be easily fulfilled by models designed for natural images. We share the lessons learned from our study and hope to inspire more future work on the fundamental designs of DL models.</p><p>Translation invariance vs. translation variance In some cases, our model cannot distinguish event marks from annotation marks, when they look identical. Although our reconstruction pipeline can fix such incorrect classification in most cases by using Redundancy Recovery, we note that this issue is caused by a key feature of CNNs, namely, translation invariance. Translation invariance <ref type="bibr" target="#b16">[18]</ref> enables a CNN to recognize an object wherever it is displayed in an image. This feature is important in recognizing natural elements (e.g., a cat should always be classified as "cat" wherever it is displayed). However, it is difficult to handle graphical images, as some graphical elements are translationinvariant while others are translation-variant. For instance, in a bar chart image, the bars in the plot area should always be classified as "bar mark", which requires translation invariance; by contrast, text labels' roles are usually determined by their positions (e.g., "y-axis label" at the left and "x-axis label" at the bottom) and thus requires translation variance. A possible solution to this problem is to learn and recognize relationships among elements. Capsule network <ref type="bibr" target="#b43">[44]</ref> is a network structure that can learn the relationships among elements. Further investigation is required to adapt it to graphical images.</p><p>High-level semantics vs. low-level semantics Our network can predict the pixel-wise masks of elements, but their quality is far from perfect. The problem is rooted in the difference between natural and graphical elements. In general, natural elements do not have crisp edges. Thus, most of the models are designed to use low-resolution, semantically-strong features for improved detection, while the precision of segmentation is compromised. By contrast, graphical elements require precise segmentation because of their crisp edges, while highlevel semantics are still necessary for detection. This demands a highresolution, semantically strong features, which is non-trivial to attain. One possible future direction is to use various features for various purposes: low-resolution, semantically strong features for detection; and high-resolution, semantically weak features for segmentation. <ref type="figure">Fig. 16</ref>: The animals can be correctly identified as timeline elements.</p><p>Single vs. hybrid Another difference between natural and graphical images is that graphical images can comprise natural elements and graphical elements. For example, in an infographic, a common practice is to show objects with photos and annotate them with graphical shapes. Such kind of hybrid components requires a model that considers the characteristics of natural and graphical elements. However, some of these characteristics may lead to conflict design requirements, and result in challenges in design models. Although our datasets do not include natural elements, we are interested in the performance of our model on timelines contain graphical and natural elements. Thus, we randomly substitute some graphical marks with photos of animals and then feed them to our model. The results show that our model can still correctly classify the categories of these animals ( <ref type="figure">Fig. 16)</ref>. We regard this performance as a benefit of the pre-trained network. Future research is needed to further understand the generality of these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Future Work</head><p>We propose an end-to-end approach to automatically extract extensible templates from infographic timelines. This work is only a starting point towards automated infographic design. Here, we discuss some promising opportunities that can facilitate the design process of infographics.</p><p>From timeline to the others. Although our approach focuses on infographic timelines, it can be generalized to other cases. First, our model can be extended to more than 10 types of timelines once a larger and more diverse dataset is available. The ability of our model to parse infographics mainly depends on the training dataset rather than the rules. Furthermore, the two tasks, namely, parsing the global and local information of a visualization, are not specific to timeline infographics but applicable to other types of visualizations. For example, previous research <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46]</ref> on charts decomposition also contains similar steps to parse the global and local information. Given that our model is data-driven, it is possible to train the model using other types of visualizations, thereby extending the model to a broader scenario. Finally, besides facilitating visualization authoring, the templates can be used to make static infographics interactive, e.g., using the parsed information to support selection and filtering of elements. The parsed information can also be used for other applications, such as indexing of infographics, retargeting of visual styles, and infographics content analysis.</p><p>To sum up, the generalizability of our model is limited by the training data. Generally speaking, DNN models, no matter supervised or unsupervised, require a large amount of training data to achieve high prediction accuracy. For the scenarios where annotated datasets are unavailable, our model has limited applicability. We plan to extend the application scenarios of our approach in the future.</p><p>From hybrid to purely learning-based. Although our approach utilizes a heuristic pipeline, it can be improved by substituting the pipeline with a DL model. Our work shows that after extracting the features of an input image, we can decode different image information (e.g., global and local information) by using multi-functional heads. Given that an extensible template can be represented by a structural document (similar to Vega specifications), a potential improvement is to use a recurrent neural network (RNN) to decode the feature maps and directly output extensible templates. Recently, Dibia and Demiralp <ref type="bibr" target="#b12">[14]</ref> showed the possibility of translating a JSON-encoded data into Vegalite specification by using a RNN. Research in CV field also presents models that take images as inputs and return textual data as outputs. The related work suggests the potential to extend our model to an end-to-end model, which takes infographic images as inputs and directly outputs templates. We consider this area as an important future direction.</p><p>From template-based to freeform. Lastly, our model shows the ability to learn and understand the content of infographic images. This characteristic indicates several potential directions to facilitate the design process. For example, we can use a trained model to interpret a sketch or materials (e.g., data, icons, and textual description) from users and recommend infographic templates. Another step in this direction is to design mixed initiative authoring systems, including automatically completing or generating designs on the basis of users' input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Limitations</head><p>We acknowledge the limitations of our study. First, our datasets are rather limited while the generalizability of our model mainly depends on the data. However, collecting high-quality infographic datasets is not an easy task considering the manual labeling efforts. We plan to open source 1 our datasets and labeling tools for the community and collect larger-scale infographic datasets in the future. Second, given our work is not aimed at high metric values, we did not optimize our model with bells and whistles, including multi-scale train/test, OHEM <ref type="bibr" target="#b46">[47]</ref>, and other techniques. Outside the scope of this work, we expect that such improvement skills are applicable to our model. Third, although our approach can automatically extract templates from infographic timelines, its performance can be further improved by involving users' refinements. For example, we can integrate our approach to infographic authoring tools and thus allow users to interactively refine extracted results. A separate limitation is that the learning process is a 'blackbox', which calls for investigations on the learning process. Finally, when applying our approach for some purposes (e.g., reusing graphical elements or overall layouts), we suggest that users should note the copyright issue, which is complicated and depends on the law varying among countries, the purpose of usage (e.g., commercial vs. noncommercial), the degrees of the redesign, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We contribute an automated approach to extract extensible templates from bitmap infographic timelines. A multi-task DNN is presented to understand and deconstruct bitmap timeline infographics, by classifying the types and orientations of timelines and detecting and segmenting elements on timelines; from these results, a heuristic pipeline is used to reconstruct extensible templates. The extensible templates can be used to automatically generate timeline infographics with updated data. The quantitative experiments and example results confirm the effectiveness and usefulness of our approach. We share lessons learned from our study which make us notice the needs of graphical image-driven deep learning. We also discuss how our work can be extended towards automated infographics design in future researches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Given a bitmap timeline infographic, we seek to extract its extensible template automatically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Example timelines from: a) a synthetic dataset D 1 , which shows two different scales, and b) a real-world dataset D 2 , which shows two different orientations. The resolutions of images are in the range of [512, 3880] Ã— [512, 4330].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Initial architecture to parse the global information. After extracting the feature map of an image, two FC layers are used to classify its type and orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>AP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 :</head><label>11</label><figDesc>The extensible template can be organized in a reusable document. The bbox is a tuple of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 12 :</head><label>12</label><figDesc>The error from the imperfect label. a) an annotation mark and b) its manually labeled mask; c) the predicted mask of the annotation mark; d) the refined result from DL GrabCut.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2019; accepted 1 Aug. 2019. Date of publication 16 Aug. 2019; date of current version 20 Oct. 2019. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2019.2934810</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The design dimensions to depict a timeline from<ref type="bibr" target="#b8">[10]</ref> </figDesc><table><row><cell></cell><cell>Design Options</cell></row><row><cell cols="2">Representation Linear, Radial, Grid, Spiral, Arbitrary</cell></row><row><cell>Scale</cell><cell>Chronological, Relative, Logarithmic,</cell></row><row><cell></cell><cell>Sequential, Sequential + Interim Duration</cell></row><row><cell>Layout</cell><cell>Unified, Faceted, Segmented,</cell></row><row><cell></cell><cell>Faceted + Segmented</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The number of annotations per category of each dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="6">#Event #Event #Annot. #Annot. #Annot. #Main Mark Text Mark Text Icon Body</cell></row><row><cell>D 1</cell><cell cols="2">83498 61324</cell><cell>4030</cell><cell>60036</cell><cell>-</cell><cell>-</cell></row><row><cell>D 2</cell><cell>2318</cell><cell>2305</cell><cell>2227</cell><cell>2937</cell><cell>1497</cell><cell>1340</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Classification of timeline types and orientations.</figDesc><table><row><cell>Dataset /</cell><cell></cell><cell>Type</cell><cell></cell><cell></cell><cell>Orientation</cell><cell></cell></row><row><cell>Backbone</cell><cell cols="6">Pre.% Rec.% F1% Pre.% Rec.% F1%</cell></row><row><cell>D 1 / R50</cell><cell>99.1</cell><cell>99.1</cell><cell>99.1</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>D 1 / R101</cell><cell>99.5</cell><cell>99.5</cell><cell>99.5</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>D 2 / R50</cell><cell>88.7</cell><cell>86.4</cell><cell>87.5</cell><cell>97.7</cell><cell>97.1</cell><cell>97.4</cell></row><row><cell>D 2 / R101</cell><cell>92.2</cell><cell>90.9</cell><cell>91.5</cell><cell>97.7</cell><cell>97.1</cell><cell>97.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Average Precision of parsing local information. AP 50 AP 75 AP 50:95 AP 50 AP 75</figDesc><table><row><cell>Dataset /</cell><cell></cell><cell>BBox</cell><cell></cell><cell></cell><cell>Mask</cell><cell></cell></row><row><cell cols="2">Backbone AP 50:95 D 1 / R50 79.0</cell><cell>93.6</cell><cell>88.0</cell><cell>79.8</cell><cell>96.4</cell><cell>91.6</cell></row><row><cell>D 1 / R101</cell><cell>81.9</cell><cell>93.9</cell><cell>89.1</cell><cell>79.9</cell><cell>96.9</cell><cell>91.1</cell></row><row><cell>D 2 / R50</cell><cell>53.4</cell><cell>79.3</cell><cell>61.8</cell><cell>56.9</cell><cell>80.1</cell><cell>61.6</cell></row><row><cell>D 2 / R101</cell><cell>56.4</cell><cell>81.7</cell><cell>64.9</cell><cell>59.1</cell><cell>82.5</cell><cell>65.1</cell></row><row><cell>COCO  *</cell><cell>39.8</cell><cell>62.3</cell><cell>43.4</cell><cell>37.1</cell><cell>60.0</cell><cell>39.4</cell></row><row><cell cols="7">*A state-of-the-art performance on COCO dataset reported by</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Gains come from Reconstruction at IoU 0.5 and 0.75. BBox Mask Pre 50 Rec 50 Pre 75 Rec 75 Pre 50 Rec 50 Pre 75 Pre 50 Raw 82.9 80.8 74.0 72.1 85.7 81.5 75.8 72.2 +NMM +2.3 +1.0 +2.3 +0.8 +1.9 +0.6 +1.8 +0.3 +RR +1.6 +2.5 +1.6 +2.3 +2.3 +2.1 +2.3 +2.</figDesc><table><row><cell>0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://chenzhutian.org/auto-infog-timeline</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank Weiwei Cui for valuable feedback on this project, Sikai Cai, Zicheng Xu, Kun Xie for assistance in labeling the dataset, as well as the anonymous reviewers for their valuable comments. This work is partially supported by a grant from MSRA (code: MRA19EG02).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://cocodataset.org" />
	</analytic>
	<monogr>
		<title level="j">Common objects in context</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fontssring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matcherator</surname></persName>
		</author>
		<ptr target="https://www.fontspring.com/matcherator" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freepik</surname></persName>
		</author>
		<ptr target="https://www.freepik.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Timelinestoryteller</surname></persName>
		</author>
		<ptr target="https://timelinestoryteller.com/app/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinterest</surname></persName>
		</author>
		<ptr target="https://www.pinterest.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semiology of Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bertin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>University of Wisconsin Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond Memorability: Visualization Recognition and Recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="519" to="528" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Timelines Revisited: A Design Space and Considerations for Expressive Storytelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2151" to="2164" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Understanding Infographics through Textual and Visual Tag Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno>abs/1709.09215</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Visual Importance for Graphic Designs and Data Visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST</title>
		<meeting>UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="page" from="531" to="554" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Data2Vis: Automatic Generation of Data Visualizations Using Sequence to Sequence Recurrent Neural Networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dibia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ã‡</forename><surname>Demiralp</surname></persName>
		</author>
		<idno>abs/1804.03126</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Pascal Visual Object Classes Challenge: A Retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer IJCV</publisher>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ISOTYPE Visualization: Working Memory, Performance, and Engagement with Pictographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Infographic Aesthetics: Designing for the First Impression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Reinecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1187" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VizML: A Machine Learning Approach to Visualization Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ChartSense: Interactive Data Extraction from Chart Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6706" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DVQA: Understanding Data Visualizations via Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gkioxari</forename><surname>Georgia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girshick</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Diagram is Worth a Dozen Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salvato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data-Driven Guides : Supporting Expressive Design for Information Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schweickart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond Sliding Windows: Object Localization by Efficient Subwindow Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR. IEEE</title>
		<meeting>CVPR. IEEE</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Design Semantics for Mobile Apps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Situ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST</title>
		<meeting>UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="569" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SSD : Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepEye: Towards Automatic Data Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automating the Design of Graphical Presentations of Relational Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="141" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show Me: Automatic Presentation for Visual Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1144" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno>abs/1807.10441</idno>
		<title level="m">Synthetically Trained Icon Proposals for Parsing and Summarizing Infographics. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="438" to="448" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reverse-Engineering Visualizations: Recovering Visual Encodings from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chart Images. CGF</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayhua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interactive Graphic Design Using Automatic Presentation Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolojejchick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mattis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page">207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GrabCut&quot;: Interactive Foreground Extraction Using Iterated Graph Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic Routing between Capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vega-Lite : A Grammar of Interactive Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="341" to="350" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ReVision: Automated Classification, Analysis and Redesign of Chart Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chhajta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST</title>
		<meeting>UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training Region-based Object Detectors with Online Hard Example Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">FigureSeer: Parsing Result-Figures in Research Papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="664" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An Overview of the Tesseract OCR Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="629" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Task-Analytic Approach to the Automated Design of Graphic Presentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stephen M</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="111" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">InfoNice: Easy Creation of Information Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Voyager : Exploratory Analysis via Faceted Browsing of Visualization Recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="649" to="658" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Voyager 2 : Augmenting Visual Analysis with Partial View Specifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2648" to="2659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">DataInk: Direct and Creative Data-Oriented Drawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wigdor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI</title>
		<meeting>CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">223</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
