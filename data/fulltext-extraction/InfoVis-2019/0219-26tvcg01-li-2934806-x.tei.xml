<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GenerativeMap: Visualization and Exploration of Dynamic Density Maps via Generative Learning Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiying</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">GenerativeMap: Visualization and Exploration of Dynamic Density Maps via Generative Learning Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934806</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Density map</term>
					<term>deep learning</term>
					<term>spatiotemporal data</term>
					<term>generative model</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. A definition of GenerativeMap. We design a general pipeline for visualization and exploration of the dynamic density map. The whole pipeline is composed of three modules in the definition, and the complete process is described in Section 3.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The spatiotemporal datasets collected by sensors have become larger and cover more research fields in recent years. Exploring the dynamic of data is a long-term challenge; the collected spatiotemporal data are often discrete and static, and we can obtain one series of states in the scenario through visualization. In traditional work, users can analyze the distribution and time-varying patterns based on data features. However, to find more details by observing these data, the dynamic process is important, and a natural representation of data movements helps users to understand the relationship between states. Furthermore, many existing spatiotemporal data lack certain parts of records, and the interpolation approach can help to fill the record gaps.</p><p>The density map is a simple presentation and available for many types of data. The application examples contain data mapping in meteorology, movement of flow in oceanography, geographical distribution of people location, etc. Currently, researchers apply this technology to detect and track patterns from the ensemble dataset, which is a collection of spatio-temporal results <ref type="bibr" target="#b42">[42]</ref>. The density map is also valuable in the simulation field and is often combined with fluid simulation or data sampling. However, the density map is vague, and it is difficult to analyze the features of the image by classical image processing methods. The density map often consists of a large number of data points, and the final visual effect shows the data cluster, which means that the generated rules are often hidden in the formation process. For these reasons, it is meaningful and necessary to explore dynamics on the density maps. <ref type="figure" target="#fig_0">Figure 2</ref> describes the issue that we want to address. First, density maps can be manually created by some rules. The process of artificial creation is given in <ref type="figure" target="#fig_0">Figure 2</ref>(a), which serves as the simulation and visualization. Can we guess the probable change process if we only know a few rules? Second, some researchers generate a large number of visualizations with scientific data, which often occurs with many types of research <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b48">48]</ref>. <ref type="bibr">Figure 2(b)</ref> shows that researchers want to know the continuous process, although there is less context information. How can we describe the possible change in the data? Third, as shown in <ref type="figure">Figure</ref> 2(c), large images are used to contain large-scale information, while users only focus on certain parts (we call these boxed images) <ref type="bibr" target="#b35">[35]</ref>. What happens in these parts that we focus on when other parts contain less information? In different domains, there are many works to solve these problems. However, the complex model and costly computer implementations are limitations of related approaches, and a simple general method can help solve these problems.</p><p>Compared with previous methods, we take advantage of generative models. Generative models provide many novel solutions for data generation in recent years and show excellent performance regarding nonlinear and self-learning. As two major categories of generative models, the variational autoencoder (VAE) <ref type="bibr" target="#b22">[22]</ref> and generative adversarial networks (GANs) <ref type="bibr" target="#b16">[16]</ref> have both shown advantages. The encoder provides a mapping relationship between the image and code, which enables users to establish the feature space. GANs are a popular model for generating high-resolution and photorealistic images, and there are many improved models for different tasks. In our work, we introduce a GAN framework to implement the smooth morphing of density maps, and the approach needs fewer parameters without any human involvement. A known drawback of GANs is that the generating performance is low for large images, and it is usually computationally complex and time consuming. As an improvement, for the region of interest (ROI), we apply image fusion to solve the problem. Another important work is the information visualization of movements, which helps users to correctly identify the change or trend of the dynamic data. A typical method is vector field representation, and the results are often shown by points, lines, and textures. We improve the visual quality and present the direction of a field by adopting blue noise sampling. The technology overcomes the visually disturbing aliasing artifacts and is better defined to our perception. This approach only needs a few parameters to adjust to different applications. For providing a more usable visual experience, we improve the algorithm with the level of detail (LOD). The final result shows a smooth and user-friendly representation.</p><p>Our approach can be combined with many methods of image processing and graphics, providing a novel pipeline that we call Gener-ativeMap. The method aims to visualize the dynamic change of two density maps in a convenient form. GenerativeMap is not meant to replace the dynamic analysis method in other domain fields; rather, it is more of a complementary method for these works, particularly for cases where the data are missing or the state is too uncertain to estimate. Crucially, this method is an attempt to improve visualization with deep learning, and the pipeline can be extended by continuous development of generative models. To achieve a generic technique, we simplified the operation by setting as few user parameters as possible. With traditional approaches, users need to change many parameters and the framework to deal with different tasks. In our work, we introduce several technologies to overcome these disadvantages, and users just need to replace the datasets if the trained model is not suitable. As shown in the experimental section, we apply the proposed approaches to the artificial datasets, eddy datasets, and location-based datasets, which are all based on the density map. The cases verify that Gen-erativeMap is a general pipeline that can extract the dynamics of the density map effectively, and the results are smooth and continuous. In our work, the visual effect and generality are the important points of the work. Our main contributions are summarized as follows:</p><p>A novel pipeline for exploring the dynamic of a density map: A general pipeline can be applied to design several tools, combined with advantages of deep learning and visualization, such that the results help users to identify dynamic evolution in different scenarios effectively.</p><p>A generative model for extracting the dynamics from two discrete images: An improved generative model is promoted to compute the probable dynamic change of images, which is available for many types of datasets. To employ GANs in the large image, we further combine Poisson blending to improve the visual effect, which avoids time-consuming data loading.</p><p>A sampling method for presenting distribution change: To identify the distribution of the density and the dynamic trend of evolution, on the basis of blue noise, the proposed sampling method is combined with LOD, which can enhance visual perception particularly for ROI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Visualization of a dynamic density map is an area in which extracting data features have mostly been implemented by visual analysis. Users can employ their methods to compute the evolution process or forecast potential states, and domain experts help developers to analyze tasks and cases. These kinds of extracted work are often built based on physical or mathematical models, which focus on the accuracy and explanation of the results. To obtain the interpolation of two density maps, the work presented in this paper is related to three broad topics: 1) spatiotemporal dynamic extraction, 2) realistic image generation, and 3) movement field visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spatiotemporal Data Extraction</head><p>In recent years, a large number of spatiotemporal data are produced and collected, particularly in fields such as meteorology, oceanography, and transportation. As classical tasks, the extraction, dynamics and fusion of ensemble datasets are all topics that have been researched <ref type="bibr" target="#b33">[33]</ref>. The topology methods can extract the time-dependent vector fields <ref type="bibr" target="#b18">[18]</ref>. These types of methods can also be extended by other scene-related approaches that can be widely used to solve problems such as detection and tracking <ref type="bibr" target="#b39">[39]</ref>. Similar methods have contributed solutions for the movement analysis of ensemble dataset, which has been verified by many experiments <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b28">28]</ref>. These cases show that applications currently exist that can partly extract the change process of spatiotemporal data.</p><p>In addition to the feature-based methods above, Ayan et al <ref type="bibr" target="#b1">[2]</ref> proposed a visualization method to analyze the weather ensembles. Distance-based approaches and the projection transform were also used to meet this requirement in recent years, and the similarity measure and the dimensionality reduction are the core steps in these works <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b51">51]</ref>. In the traditional gap-filling field, researchers attempt to extract the nonlinear spatio-temporal patterns with a data-driven method such as neural network <ref type="bibr" target="#b34">[34]</ref>. Nowadays, the deep learning approach can make a general model available for more scenarios <ref type="bibr" target="#b23">[23]</ref> according to the intersection of different disciplines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Learning Model</head><p>Deep learning has made great advances in developing generative models, which can transform data between a simple latent distribution and a complex distribution. Three common generative learning models are GANs, VAE and the flow-based model. GANs are used to generate realistic high-quality images by discriminating the random fake distribution from the real distribution of training datasets <ref type="bibr" target="#b16">[16]</ref>. However, the latent distribution is assumed as random in GANs; this characteristic makes GANs unstable, and the high-resolution image generation is a challenge for GANs. VAE is a framework that encodes images as a definitive latent vector space <ref type="bibr" target="#b22">[22]</ref>. The outstanding feature of the method is that it takes the target image as a posterior distribution, although the classical VAE can only obtain approximate results. The flow-based model is a mathematically based method, and it has solved image generation by data space mapping <ref type="bibr" target="#b11">[11]</ref>. The related idea can generate natural and high-definition results; however, its computational cost is high.</p><p>In fact, deep learning is usually used for interpolating discrete density maps. The results proved that linear interpolation in latent space will lead to linear change generation of features <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b45">45]</ref>. Other important research focused on how the interpolation methods influence the continuous change. Piotr et al. <ref type="bibr" target="#b2">[3]</ref> tried many experiments and concluded that the linear interpolation could present enough features when the data are low-dimensional. Taking advantage of VAE and GAN, the hybrid model can encode images as definite vectors distributions and generate high-resolution images. Typical hybrid models are VAEGAN, ALI and BiGAN <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b25">25]</ref>. Our work is based on encoding ability and interpolation validity of these type deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Movement Field Visualization</head><p>There has been much visualization research to present density movement. In previous work on scientific visualization, points, lines, and textures often represent vector fields <ref type="bibr" target="#b3">[4]</ref>. The information can be clearly indicated in a suitable glyph, and then, the results can indicate the evolution process or the relationship among datasets <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b41">41]</ref>. For analyzing spatiotemporal data, describing the varying process of events is a common task. Inspired by flow visualization, novel visual methods help users to identify the flow direction of information based on relating models <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b46">46]</ref>. All these experiments determined the meaning of extracting the dynamic change of data. The image method has attracted more attention in recent years, and the spatial dimensions can be transformed to show states and movements <ref type="bibr" target="#b5">[6]</ref>. This is an important ability currently because data collections are often discrete, and a probable reference can help users infer more information without data.</p><p>The ensemble movement description and forecast are promoted with the big data collection <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b29">29]</ref> to improve the work above. Uncertain visualization in these domains confirms that a probable data inference is meaningful for experts, which is also an important motivation of our research. Blue noise is widely applied in dynamic sampling and stippling patterns since uneven distributions are encountered, and it can avoid aliasing artifacts <ref type="bibr" target="#b47">[47]</ref>. The technology can help users identify multiple classes and spatial positions of samples by extending the original methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">43]</ref>, the results have worked well for improving visual effects and domain problems. The related visualization work also confirms that the glyph, e.g. arrow, can present the change trend <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENERATIVEMAP PIPELINE</head><p>Most visualization methods of dynamic data combine feature analysis and physical modeling, and these works often contain requirements analysis and clear tasks from domain experts. Such ideas have proven to be available for special tasks in many works. However, such extracted data variation needs a large number of prework steps and has a complex data preprocess, and users commonly need to set many parameters. In addition, most evolution estimation methods are designed for special tasks, it is difficult to apply an existing method into other scenarios. Furthermore, the traditional pixel-based image processing approach has a few limitations, particularly for the target that moves a great distance or crosses with multiple source kernels.</p><p>In this paper, we propose GenerativeMap to show the morphing between two density maps, which extract dynamic features by the generative model. Image interpolation is usually used to reason whether model learned relevances and representations, rather than remembering sharps as discussed in the works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b45">45]</ref>. The related theories and experiments are the basis of our work. The results show that the learned space has smooth transitions. Walking in the latent space will result in the semantic changes. Compared with the most of deep learning works, we need a controllable image interpolation approach, since many GANs models generate samples randomly. As we introduced in Section 2.2, the encoder and decoder can help model to construct the relationship between training sample and latent vector. An ideal model should get two selected density map as inputs, and generate a series of smooth and continuous transition samples as outputs.</p><p>We transform visualization images into inputs of the generative model. Since parts of the initial visualization scenarios are not density maps, we design a density map tool. Users can use the tool to transform the real-world visualization scenarios into density maps. Training deep learning model is another important pre-work. We seriously consider how to create suitable training dataset for learning general dynamic features. To make the model generate smooth and continuous samples, we propose a special dataset creation method in Section 4.1. <ref type="figure">Figure 1</ref> shows the overview of the pipeline, and we consider Gener-ativeMap as a general method that can be used in multiple scenarios, as referred to in Section 1. In the first part of GenerativeMap, users choose two discrete frames at two noncontinuous times as the inputs of the module. These visualization results are presented as density maps by the density map tool. Users can upload special existing density maps as inputs. For a special domain application, the model can be trained better with special training datasets, which is not the focus of our work.</p><p>The second part is a simple and effective generative model, which is the core part of the pipeline. Smooth image transition relies on continuous latent vectors interpolation. The trained encoder could encode two input images into two definite distribution in vector form. Linear interpolations of the vector distributions will be decoded as linear output samples. We further improve the performance of the model by designing modules of the network. The model can process larger images and show more details. The modification of modules can be seen in Section 4.2.</p><p>It is not always natural when playing out interpolated samples directly, additional image processing methods are necessary for some complex conditions. High-resolution images, less information, and uneven small change, these are all weaknesses of deep learning. The high-resolution image generation is a challenge for deep learning models, so we propose to use an image fusion approach to solve the problem. Since the interpolations are discrete and the regions are small parts of the whole image, the transition between interpolations may not be remarkable in some conditions. To show the change clearly, we calculate the color gradient of images by diffusion model. Inspired by variation field visualization, the direction glyph can be used to represent the gradient change. For showing clear uneven changes, transition presentation is optimized by an improved sampling method based on a blue noise approach.</p><p>To simplify the procedure and explore a generic method, our work combines deep learning and image processing with visualization, providing an easy-to-use and comprehensive approach. We describe the details in Section 4.2 and 4.3.  . We attempt employing the same data with the original BiGAN and our Mp-GAN, and it easily shows that the result of the latter would be better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Generation</head><p>The dataset generation is an important step to obtain a useful deep learning model, since the model will learn our expected features from data. There are no existing reliable datasets for density map morphing. Initially, considering that the density map is based on the kernel density estimate (KDE) <ref type="bibr" target="#b4">[5]</ref>, we created datasets by filling images with pixels, where the pixels follow Gaussian distributions. According to the obtained results, the model can learn the Gaussian shape well and generates a series of similar figures, which demonstrates the feasibility of this idea. However, the model cannot learn the dynamic change since the samples in training datasets are independent. The ideal model should learn natural and random distributions that contain smooth and continuous features. Perlin noise (PN) is an effective algorithm that is widely used in creating and simulating natural scenes in the virtual world <ref type="bibr" target="#b31">[31]</ref>. It starts with random pixels in a 2D space, and the noise generates pseudorandom gradient vectors for every pixel. The quality of generating samples is controlled by the randomly generated seeds, the Gaussian fuzzy sets and smooth interpolation functions. The formulations of PN are as follows:</p><formula xml:id="formula_0">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ T (t) = αt 5 − βt 4 + γt 3 S ν n = T (g ν 0 − g ν n ) I(x, y, w) = x(1 − w) + y • w Δ(g n ) = (g 0 − g n ) • P[g n ](g 0 = (x, y), n = 1, 2, 3, 4) Θ(g 0 ) = I(I(Δ(g 1 ), Δ(g 2 ), S x 1 ), I(Δ(g 3 ), Δ(g 4 ), S x 1 ), S y 1 )<label>(1)</label></formula><p>where T (t) is a function that generates nonlinear movement, and α, β , and γ are all hyper parameters. t indicates the arbitrary value. g 0 means the current position of the pixel. g 1 ,g 2 ,g 3 , and g 4 are the vertexes of the seperated space grid. S ν n means the smooth weight between g 0 and g n along ν direction. I(x, y, w) represents the interpolation method between x and y, which is influenced by w. Δ(g n ) is the composed gradient of g n and P[g n ] means a gradient vector random selected from a finite number of precomputed gradient vectors. Θ(g 0 ) is the actual movement of the pixel g 0 . Gaussian fuzzy can reduce the additional visual noise.</p><p>We can get the static PN images by steps above, which contain smooth and natural shapes. However, the ideal model should generate a series of continuous and smooth interpolations to describe the transition process of two discrete density maps, which means that the deep learning model can learn the relationship between two density maps from the training datasets. Inspired by the related work <ref type="bibr" target="#b50">[50]</ref>, we use the neural network to extract expected continuous features from keyframes. The keyframes are from the dataset consist of spatiotemporal records. The data collection approach in our work can be summarized as three steps:</p><p>1. Setting parameters to generate dynamic PN images. The parameters are related to the shape, size, lightness, and scale of noise.</p><p>2. We randomly select multiple regions in the PN images and grab a fixed size of image frames at a regular time interval.</p><p>3. After getting a certain amount of data, we repeat Step1 and Step2 until we have enough data collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Density Generative Model</head><p>As described before, there are different ways to generate images' interpolation with a deep learning model. In this paper, we follow a BiGAN framework as the basis of our method, which contains the encoder, generator (decoder) and discriminator. The structure can encode the selected image into a specific distribution, which is the necessary condition to interpolate two target images. The core framework of BiGAN is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a), indicating that the encoder and generator (decoder) will be trained separately. The encoder transforms the real image (real x) into the real distribution (real z), while the decoder generates the fake image (fake x) by prior distributions (fake z). Then we concatenate distributions and corresponding images as pairs, the discriminator will try to distinguish if a pair is from encoder or decoder (C1 or C2). The above training method ensures the trained model can construct the relationship between input images (x1,x2) and latent vector space (z1,z2), since the network learns the image and distribution at the same time. DCGAN, a sound and mature model, provides convolution structure reference for the unsupervised learning and confirms GAN can be used to extract features <ref type="bibr" target="#b32">[32]</ref>. However, the original model processes 32 × 32 images, which is too small to be used in visualization. A classic case of results is shown in <ref type="figure">Figure 4(a)</ref>, where there are shortcomings in the details, particularly for the shape of the edge.</p><p>In our work, the PN images are large, and we hope the model can learn a wider region of features. The improved components are shown in <ref type="figure" target="#fig_1">Figures 3(c,d)</ref> present the structure of the encoder and decoder, respectively, which makes the improved network applicable to the large image, and we refer to other successful image networks such as ResNet <ref type="bibr" target="#b36">[36]</ref>. Inspired by ResNet, we design similar blocks and select larger convolution kernels to enable the model to process a larger image with more information. In our work, we summarize that the encoder needs to collect information, so the encoder should contain more big size kernels and convolution layers ( <ref type="figure" target="#fig_1">Figure 3c</ref>) and decoder employs several deconvolution layers as symmetric parts <ref type="figure" target="#fig_1">(Figure 3d)</ref>.</p><p>In addition to the structure improvement, we refer to the loss function in many classical neural networks. For BiGAN training of the encoder and generator (decoder), two loss functions are designed in the approach. The final losses are described as Equation 2:</p><formula xml:id="formula_1">⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ Dloss = max(μ(ΔT − ΔT 2 2λ d(x,z) )) Gloss = min(μ[ΔT + β 1 |z − E1(G(z))| + β 2 |x − G(E1(x))|]) ΔT = T (E2(x), E1(x)) − T (E2(G(z)), z) d(x, z) = μ((x − G(z)) + (E1(x) − z)) (2)</formula><p>where Dloss and Gloss mean the loss function of the encoder and decoder, respectively, and μ means average operation of the loss. ΔT means the difference between images and distributions, which is calculated by discriminator. <ref type="figure">d(x, z)</ref> is an operator measure distance between two sets, and differences between images x and distributions z are calculated in this paper. E1, E2 and G represent the encoder for generating, the encoder for discriminating and the generator, respectively. x, z are symbols of real images and fake distributions, respectively, as shown in <ref type="figure" target="#fig_1">Figure 3(a)</ref>. λ , β 1 and β 2 are all hyperparameters to be set in our experiment. The interpolation results are shown in <ref type="figure">Figure 4</ref>(b), the shape of which is similar to the inputs, and the morphing result is smoother than the result in <ref type="figure">Figure 4(a)</ref>.</p><p>As the key step, we use the generator of the model to get the interpolation capability, and the <ref type="figure" target="#fig_1">Figure 3</ref>(b) present the detail. Users select two density map as the real images x 1 and x 2 . The trained encoder will encode these images into two vector distributions z 1 and z 2 . After z 1 and z 2 are interpolated, the trainable decoder can decode these interpolations into images set {s 1 , s 2 ...s n }. According to the related deep learning work, the set contains continuous features and the linear interpolation will present the smooth transition <ref type="bibr" target="#b45">[45]</ref>. We further evaluate the generative ability and the interpolation quality in Section 6.</p><p>After the above steps, we ultimately construct a generative model for a 128 × 128 image, and most models can achieve a good performance with this size. To distinguish it from the original BiGAN framework, we call it Morphing GAN (Mp-GAN). Our model is designed for morphing between density maps. We classify the dynamic process as three types: connection, shift, and fusion abilities. <ref type="figure" target="#fig_3">Figure 5</ref> shows the generated results, in which the interpolations are smooth and clear. In our work, both a complex network model and an excessive computational overhead are unavailable characteristics for creating an easy-to-use method. One important aspect to circumvent these issues is that users can focus on parts of the image rather than the full information in many scenarios; the other factor is that images may contain some valueless information. Considering these reasons, it is not worthwhile to design a complex special model for the large-sized image task. <ref type="figure" target="#fig_4">Figure 6</ref> demonstrates a typical scenario, and a large density map illustrates a large range of the information distribution. The selected rectangle is the ROI where information is collected, and it is obvious that the uneven distributions in this region would change in the density map. Initially, we try to obtain the part of an image by image segmentation and then insert the generated interpolation into the source image directly, as shown in <ref type="figure" target="#fig_4">Figure 6(a)</ref>. The density change is unnatural, particularly on the boundary.</p><p>Poisson blending is an approach to blend parts of an image into another one and smooths the boundary of images. The challenge has been solved by taking images as functions, and blending means minimizing the difference between two functions. The core equation can be described as Equation 3:</p><formula xml:id="formula_2">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Γ N H(x, y) = φ (H) + ψ(A) + χ(B) φ (H) = ∑ (dx,dy)+(x,y)∈Ω H(x + dx, y + dy) ψ(A) = ∑ (dx,dy)+(x,y)∈∂ Ω A(x + dx, y + dy) χ(B) = ∑ (dx,dy)+(x,y)∈Ω∪∂ Ω (B(x + dx, y + dy) − B(x, y))<label>(3)</label></formula><p>where  <ref type="figure" target="#fig_4">Figure 6</ref>(c), and the result smooths the evolution of the density map and compensates for the disadvantage of deep learning. We can easily find that the density gradually increases in this region, where it is not influenced by density in the surroundings.</p><formula xml:id="formula_3">Γ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Field Variation Representation</head><p>After the target images are processed by the generative model, we obtain a series of interpolations of the selected image. However, there are two limitations if we provide these interpolated images to users directly. On the one hand, the model generates images based on the encoding vectors of targets, which are asymmetrical sometimes; therefore, the transition is unnatural. On the other hand, it is difficult to confirm the sampling number, for the number of samples we set would influence the continuous change. A direction visualization that helps users to summarize the trend is an important work for the reasons above.</p><p>Initially, we calculate the change in images based on the diffusion model and then draw arrows directly based on the gradient change similar to most of the related work <ref type="bibr" target="#b15">[15]</ref>. As a part of our pipeline, the classical optical flow model can be described as Equation 4:</p><formula xml:id="formula_4">min Ω (D 0 (x) − D 1 • u) 2 dx + λ Ω | ∇u | 2 dx | ∇u | 2 =| ∇u | 2 + | ∇ν | 2<label>(4)</label></formula><p>where x means the pixels in the image, and • represents a composite function. Ω is the range of whole image, D 0 and D 1 indicate two images, which often record two states in continuous time. ∇u is a function to calculate the gradient of a pixel, which is composed of the gradient along the u and ν directions. As shown in <ref type="figure" target="#fig_5">Figure 7</ref>(a), this is a sample with a dynamic noise distribution. Initially we want to use arrows to present the change trend by the diffusion model, for arrows glyph is a common and practical choice in a 2D visualization <ref type="bibr" target="#b30">[30]</ref>. We design a display rule in this work; the red arrows in the experiment mean that the region is expanded, and the blue arrows represent the reduction in the area. However, the presented glyphs may overlap each other or be too small, which are the typical problems encountered using uniform sampling. Users need to set the sampling interval and threshold to avoid these display shortcomings.</p><p>Blue noise is widely used in texture synthesis, data sampling, and realistic rendering, and related applications contain stippling, visualization and reconstruction <ref type="bibr" target="#b26">[26]</ref>. We employ the technology in this work for overcoming aliasing and increasing the space between clusters. Users can easily identify the shape of the density, and the sample seed distribution is less dense, which makes the arrows nonoverlapping without parameter setting. However, it is noteworthy that the brightness of gray images represents the data density, which is not always the same in the density map, as shown in <ref type="figure" target="#fig_5">Figure 7(b)</ref>. The brightness change is important since the density has meaning, and it is obvious that users cannot obtain this brightness through observing the denseness of distributions. For the original blue noise, the key is the minimum acceptable distance between stochastic sampling points, which the main parameters control for the sampling distribution in the whole image.</p><p>We show the uneven brightness by further enlarging the difference of distributions, which promotes the visual effect. In the simulation domain, LOD helps users to form large-scale visualization with fewer data and to quantify the sampling error <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b44">44]</ref>. Inspired by the solution, we classify different sample intervals as multiple levels, and the final result is merged by overlapping multiple sample distributions. The rule of the LBN distribution on the density map is built based on blue noise (BN) sampling <ref type="bibr" target="#b26">[26]</ref>; therefore, the generated points have blue noise properties using the method. BN returns an array containing the position of samples, and the complete LBN algorithm is described as Algorithm 1. We improve the visual effect by employing a level-bluenoise (LBN) method, which combines the advantages of LOD and blue noise sampling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We show the GenerativeMap visualization cases and the effect of the trend presentation in this section. GenerativeMap extracts the continuous interpolation of the density map and presents the morphing trends in a natural visual style. We use various common datasets and show different change trend maps. The first datasets are artificial datasets, whereas the other two are real-world datasets. All datasets are prerendered by Gaussian blur such that we can introduce the data into our generative model directly. Since we are focused on the transform dynamic of density maps rather than the event data following physical rules, we do not emphasize the realistic characteristics of the process details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Artificial Data</head><p>In this work, we use PN in training data for the generative model. The following experiment tests whether the model can learn easier rules in addition to noise activity, which is an important potentiality of this approach. We design hand drawing tools with a heat map and Gaussian fuzzy sets, which can provide random density as the inputs of GenerativeMap. The change process is computed by the diffusion model and GenerativeMap. This experiment illustrates the ability of extracting the change process of two unknown density maps, and we envision it as the basis of extended applications. x min = min x − 3 * ck, 0 , y min = min y − 3 * ck, 0 10: if f x,y (P) &gt; τ then   <ref type="figure" target="#fig_10">Figure 8(a)</ref> are computed by the diffusion model as a basic reference. We can find that the motion process is a challenge for the diffusion model, for there is no overlapping pixel of the input images. As a comparison, <ref type="figure" target="#fig_10">Figure 8</ref>(b) presents the interpolation results by GenerativeMap. The middle frames change smoothly and continuously, particularly the last half of the series. Based on this characteristic, the tools can be extended to other applications, such as the experiment in Section 5.2. We can also conclude Genera-tiveMap can help users to guess the density map change, although the learned datasets do not contain random shapes and possible change to the process.</p><formula xml:id="formula_5">x max = max x + 4 *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dynamic Eddy Data</head><p>Based on the tools designed in Section 5.1, we can extend Genera-tiveMap in real-world scenarios. A classical density map application is eddy visualization, where the identification and tracking are the important tasks to illustrate ocean ensemble movement <ref type="bibr" target="#b6">[7]</ref>. This work collects the eddy data in a stream type, which comes from AVISO. The site provides common flow field data combined with ocean topography  <ref type="figure">Fig. 9</ref>. We create density maps based on the eddy data, and our pipeline extracts and shows the movement of the eddy. The 1st case represents two vortexes moving toward different directions, and the 2nd case means a vortex move, where the change process shows a smooth and continuous movement. and the satellite information, and then, we employ the LIC method to obtain a series of eddy images as our real data <ref type="bibr" target="#b7">[8]</ref>.</p><p>Using the density tools we designed above, the eddy map can easily be drawn as a density map. The original eddy maps are collected from a part of a located region in the ocean, which are shown in <ref type="figure">Figure 9(a)</ref>. We mark the target regions in red boxes, and every ensemble is composed of two eddies. In traditional work, it is difficult to guess the probable move trajectory without enough data because users must analyze the movement features or summarize the basic movement rules in the ocean. However, we can obtain a basic reference by GenerativeMap without any prior knowledge. The processed inputs are presented in <ref type="figure">Figure 9</ref>(b), and as in other experiments in our work, our tools employ Gaussian fuzzy sets and gray processing in this step. A surprise to us is that the change process is not completely linear, which does not influence the results of GenerativeMap. We can find that the two eddies move toward different directions, which means that the movements of eddies are extracted independently. The pipeline still shows available results without additional supports, although the model certainly did not learn the rules to generate the vortex before.</p><p>In order to test the generative ability of GenerativeMap, we further choose a different movement type. Compared with <ref type="figure">Figure 9(a)</ref>, the vortex in <ref type="figure">Figure 9</ref>(c) move from right to left. The middle three interpolations in <ref type="figure">Figure 9</ref>(d) present a natural and smooth transition, and the shadow around core can even simulate the fluid type. We also find that the generated shape of the ensemble can match the existed real data shape in the third and fourth frame.</p><p>We compare the generated interpolations with the real data frame-byframe, and the original motion process cannot be recorded continuously because of the data property. There are only two records between the start record and the end record as shown in <ref type="figure">Figure 9(a)</ref> and (c). To obtain a continuous process, we need to make up the data records, and the generated results play well for the task. The 3rd frame in <ref type="figure">Figure 9</ref>(b) and the 2nd frame in <ref type="figure">Figure 9(d)</ref> are both conjectures, the shadow of which is the reference for the motion process. The result indicated that our model can be used for data interpolation in addition to image interpolation. <ref type="figure">Fig. 10</ref>. The density map shows the air quality in the nation, and we focus on North China, which is influenced by haze. The 1st row is the real data around Beijing, the 2nd row shows the interpolation change between inputs, and the 3rd row presents change trend with arrows using LBN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Air Quality Data</head><p>Big data visualization is often presented on a large scale, in these scenarios the large images are necessary. In this work, we collected the air quality index (AQI) in China, and the data are visualized by a heat map. Because air monitoring stations are not established everywhere, the information distributions are also dispersed geographically. In fact, users should only need to know the air change trend in parts of China, and it is easy to find that the most regions have no data. For showing information on a national scale and guaranteeing the generative quality, it is necessary to generate large-sized images. In this test, the full image is 1024 × 1024, which is commonly defined as a high-resolution image in the image processing domain. As we introduced in Section 4.3.2, our generative model uses 128 × 128 data for obtaining a good performance.</p><p>In recent years, it has been the focus of attention that haze has appeared frequently around Beijing in China <ref type="bibr" target="#b51">[51]</ref>. Therefore, we take this region as the ROI, and Section 5.2 has determined the feasibility to infer the change process by GenerativeMap. However, the national information is too much to observe; therefore, <ref type="figure">Figure 10</ref>(a) is a typical visualization. Another characteristic is that the data are collected every two hours every day, which means that some regions would not change much compared with North China. We select the data from 06:00 on April 16, 2017, to 08:00 on April 17, 2017, and the AQI changes in the ROI. <ref type="figure">Figure 10</ref>(b) and <ref type="figure">Figure 10</ref>(c) present the records in the same ROI, and it is complex to perform visual analysis of the region independently with the traditional method. The GenerativeMap greatly reduces the difficulty of the task. As shown in <ref type="figure">Figure 10(d1-d4)</ref>, the picture series blends the result of generated interpolations and the region map. The result is smooth and seamless, even when the data around the ROI show a slight change. The last generated frame F N−1 is similar to the target image as shown in <ref type="figure">Figure 10(c)</ref>.</p><p>Considering that the data collected are not continuous, Genera-tiveMap can help us infer the probable process. Furthermore, the data change would be fuzzy with blending, which may make it difficult for users to identify the change. We can introduce LBN to show the differences in detail, as shown in <ref type="figure">Figure 10(e1-e4)</ref>. A significant feature is that the red arrows occupy the main area, and the blue arrows decrease over time. The phenomena confirm the haze increases, as shown in <ref type="figure">Figure 10</ref>(b) and <ref type="figure">Figure 10</ref>(c). Another important feature is that the red arrows move from south to north, which presents the probable haze movement. LBN provides a clear and nonoverlapped distribution of the direct density change. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICS</head><p>Measuring the similarity of continuous frames in a series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T DS</head><p>Measuring the time cost of generating a specified number of interpolations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INT</head><p>Describing how many interpolations we want to generate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IGS</head><p>Evaluating how many features of input the interpolations contain. This comparison is made in an interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ILS</head><p>Evaluating whether our method can direct the overall transition trend correctly. We compare the generated frames and the ground truth to get this index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION AND DISCUSSION</head><p>GenerativeMap is tailored to allow users to solve tasks related to the extraction of density maps. As discussed in Section 1, the existing similar work was designed by scholars for their specialized tasks. Model-driven methods are available in these scenarios, which can be defined as problems of time segments and geometrical shape dimensions. Data-driven methods are designed for processing the coarse data or the data that has abnormal discretization. In contrast, our technology aims to help users to achieve references and to empower novice users to obtain density morphing by using existing data to train model, even when they have no experience in the domain.</p><p>For the deep learning model that needs pretraining, the pipeline uses the model as a generator, which generates interpolation samples according to stored parameters. For the trained model, the quick generation ability is the strength of deep learning. However, both quantitative and qualitative evaluation is still necessary. Combined with the characteristic of our pipeline, we design special evaluation methods, which help users to know the generative quality and efficiency. The evaluation indexes and tasks are shown in <ref type="table" target="#tab_2">Table 2</ref>. The hash-based image similarity algorithm <ref type="bibr" target="#b40">[40]</ref> is the basic measurement of in our evaluations, and all of the indexes are built on the algorithm. In our work, we use three hashes (ahash,phash and dhash) <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b49">49]</ref> because they can show good performances in different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Time Performance</head><p>In this work, we compare the generative quality by generating a different number of interpolation vectors, and we set an index that describes the number of targets (INT ). The three series in <ref type="figure">Figure 11</ref> represent INT =4, 8, and 12. Due to space limitations, we only present 4 interpolations that are selected from different INT series. The actual time cost of computing the interpolation is measured by another index, and we call it the time cost for different samples (T DS). The T DS index comparison is shown in <ref type="figure" target="#fig_0">Figure 12</ref>. The T DS would increase as the INT increases, and we can find that the slope of the curve is slight. In general, we do not need to generate too many interpolations, and the useful INT is often less than 24. The result confirms that the computational load does not explode when users need a more detailed process. A common issue in traditional methods for the extraction of more details is employing more parameters, while deep learning achieves this goal in the pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Generative Ability</head><p>The generative ability is evaluated from continuity and validity aspects. <ref type="figure">Figure 11</ref>(a) indicates a series of samples of INT =4, and the motion is not smooth and as continuous in the samples, particularly in the connection area (in the red circle). The disadvantage can also be seen in the blue circle. Considering the principles of GenerativeMap, the continuity of interpolations may be interrupted if the amount of samples is few, which seriously influences the visual effect. Another comparison is shown in <ref type="figure">Figure 11(b)</ref> and (c), where these samples are selected from INT =8 and INT =16, respectively. The final visual effect appears similar, and a further quantitative evaluation of interpolations is necessary.  We design evaluation indexes about the similarity computation of interpolated samples, which refer to the sample test in deep learning. We conduct experiments and design an index to compute continuity similarity (ICS) of one series. To measure the features our model learned, we separate the evaluation as a global one and a local one. The index of global similarity (IGS) describes the correlation of interpolation and input in one series, while the index of local similarity (ILS i ) is designed to describe the similarity of the ith frame and benchmark. The described method can be summarized as Equation 5:</p><formula xml:id="formula_6">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ Θ(P, Q) = 1 3 (ahashSim(P, Q) + phashSim(P, Q) + dhashSim(P, Q)) ICS(F n ) = 1 INT − 2 INT ∑ i=2 Θ(F i , F i−1 ) IGS(F n ) = 1 2(INT − 2) ( INT −1 ∑ i=2 Θ(F i , x1) + ( INT −2 ∑ j=2 Θ(F j , x2))) ILS i (F n , G n ) = 1 j 2 − j 1 j 2 =min(i+1,INT ) ∑ j 1 =max(0,i−1) Θ(F i , G j ) (5)</formula><p>where Θ is a mean similarity of two input images (P and Q). ahashSim, phashSim and dhashSim represent three similarity calculation methods based on three hash algorithms. F n , G n , x1, and x2 are all described in Table1.</p><p>All similarity indexes can be measured as scores, and higher scores mean that the generative performance is better. <ref type="figure" target="#fig_1">Figure 13</ref> and <ref type="figure" target="#fig_3">Figure 15</ref> show the indexes we used to compare details. As we expect, there is no linear correlation between the number and quality. The relationship of IGS and INT is shown in <ref type="figure" target="#fig_1">Figure 13</ref>, and the scores are not always increased as INT increases. We provide ICS in addition to IGS, and the growth of ICS slows down within more interpolations. IGS slightly decreases when INT is changed from 8 to 12. Hence, we conclude that feature loss appears if unnecessary number of interpolations are applied. Considering the performance and ability, INT =8 is an available choice for most of the scenarios.</p><p>It is difficult to select a proper scenario to validate ground truth. In our work, we mainly focus on the long-term discrete data and discon- tinuous data records. An ideal ground truth should be a continuous phenomenon that can be directly observed. We choose the droplet movement as our experiment, which simulates the water tension. We compare the interpolations and water movement to evaluate the generative ability. The droplet movement contains a certain natural dynamic information. <ref type="figure">Figure 14(a)</ref> shows the dynamic process of the simulation, and we grab 5 frames from the process as the benchmarks, where the water droplet falls naturally <ref type="bibr" target="#b37">[37]</ref>. The start-to-end frames of <ref type="figure">Figure 14(a)</ref> are the benchmark of our experiment. To evaluate the generative ability, we first create density maps, as shown in <ref type="figure">Figure14(b)</ref>, by the density tool according to <ref type="figure">Figure 14(a)</ref>. <ref type="figure">Figure 14</ref>(c) presents the result with the traditional diffusion model, and it is obvious that the model cannot solve the process correctly, which is shown in the middle 3 frames. By contrast, <ref type="figure">Figure 14(d)</ref> shows the probable motion of GenerativeMap, and it appears that the model infers the correct change gradually, particularly for the split process in F 3 .</p><p>ILS means the difference of the generative interpolation and the real change process, and the index can help us to analyze the generative ability in addition to the visual effect. As shown in <ref type="figure" target="#fig_3">Figure 15</ref>, the scores will be higher if the generated result is more similar to the benchmark. An interesting result is that the generated ILS scores are more obvious than the diffusion ILS when the sample is close to the end benchmark, which follows the visual effect that we discussed above. The results show that the traditional diffusion model plays well at the start of motion. However, it is influenced by the data distribution and even fails. <ref type="figure" target="#fig_3">Fig. 15</ref>. The evaluation of the ILS-Frame. The generative ILS is higher than the diffusion ILS when the evaluated frames are close to the end frame.</p><p>The generated effect may be better if more information is provided or more interpolations are generated. The final evaluation results indicate that GenerativeMap has advantage in the integrated indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Limitation</head><p>We evaluated our approach with a broad-based comparison that confirmed the effectiveness and necessity of our designs. We take PN to create training datasets; therefore, the model can learn natural movement features. However, the model has also learned the unnecessary noise. To avoid the noise, we have to introduce Gaussian blur into interpolations. Since we use many noise methods to normalize datasets; however, this means the method is insufficient if details of images are important.</p><p>Our system requires training the generative model, while most real data contain special physical or mathematical rules, such as in transportation and meteorology where their long-term activities follow regular patterns. These types of data need to be collected over a long continuous time. As a result, in the approach, we can only present limited random natural variation forms. For some fields, the movements follow some known rules, and the deep learning can be improved as presented in the work of Byungsoo et al. <ref type="bibr" target="#b20">[20]</ref>.</p><p>Another limitation is that our pipeline is still complex. A complex combination introduces an inconvenient implementation, which motivated us to optimize the pipeline in the future. As a well-known problem in deep learning, GANs are not models that are stable enough. The proposed pipeline can be further simplified and integrated to get an end-to-end model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>We present a novel pipeline for extracting the dynamic density map that incorporates image processing methods and an improved generative model. The major contribution of our approach is to promote a general approach that helps users to obtain possible dynamics between selected density maps. In our work, we incorporate many models and design optimization for data evolution visualization, and in particular, we incorporate deep learning with regard to the information visualization.</p><p>An interesting avenue for future work is enhancing the visualization approach that utilizes the training data provided by our system. The research depends on the development of deep learning, combined with the conditions controlled by the generative model and the smooth generating process. The most anticipated expansion is that the approach might be incorporated within a spatiotemporal application that is usable by the general public.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Explanation of problems scenarios that may arise. The three parts are typical applications of density map evolution, they are the main motivation of our work. The details are introduced in Section 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The structure of our improved BiGAN model.(a) The framework of network; (b) The interpolation of images; (c) The structure of encoder; (d) The structure of decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Interpolation of original BiGAN (b) Interpolation of Mp-GAN Fig. 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The generated series shows the multiple types of results with our model. According to the change process of start-end frames, we simply classify the generative form as three types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The series explains why and how we should employ Poisson blending in parts of a large image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>The experiment shows the visual effect of blue noise sampling. (a) Colorful density map; (b) Gray density map; (c) Blue noise sampling result; (d) Field variation representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>N H(x, y) means N points around digital (x, y) in the new merged image H, A is the target image and B is the source image. (dx, dy) is the probable position of the adjacent pixel, Ω represents the area of B, while ∂ Ω is the boundary of A. To summarize, we define φ (H), ψ(A), and χ(B) to demonstrate the area in H, the boundary in A and both elements in B, respectively. The interpolations are shown in Figure 6(b), and the process is smooth and continuous as in other cases. The final visual effect of processing of the image with Poisson blending is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 (</head><label>7</label><figDesc>c) illustrates the sampling result using LBN, and the point indicate the sampling position. Figure 7(d) shows a case of change trend representation. Arrow size indicates the change value. Arrow direction indicates the change trend, and the arrow glyphs are distributed in the major regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 1 4 : 5 : 8 :</head><label>1458</label><figDesc>LBN algorithm. Input: P 1 , P 2 : the target density maps; K: the separate levels; M: the maximum value of the radius; f x,y (P): the significance value of digital locate (x, y) in the density map P; τ: the acceptable minimum significance value; Output: S: The array statistic of the position of samples in all the k levels; 1: (W, H) = size(P 1 ) 2: while k &lt; K do3: ck = M − f x,y (P)/k for range(0, ck) and (x, y) ⊆ P 1 do AS = [(x 0 , y 0 ), (x 1 , y 1 )...(x n , y n )] = BN(x, y, ck) while (x, y) ⊆ AS do 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>17 :Fig. 8 .</head><label>178</label><figDesc>S k ⇐ (x, y) 18: end if 19: end while 20: return S = ∑ K 0 S k ; (a) The morphing result with diffusion model (b) The morphing result with GenerativeMap Morphing visualization of artificial data; the start frame and the end frame are created by hand-drawn methods. (a) The series are generated by the diffusion model and (b) show the smooth change by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 (</head><label>8</label><figDesc>b) shows the result of an artificial data case. The input are two images, which are drawn by a volunteer. Given that they are artificial creations, in fact, no one knows the real change process. As discussed previously, in our model, the inputs are preprocessed by Gaussian blur. The middle 4 frames in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) The real data of two vortexes (b) The morphing process of (a) (c) The real data of vortex movement (d) The morphing process of (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>12 Fig. 11 .</head><label>1211</label><figDesc>A evaluation of interpolation ability. The visual effect is different when INT is different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>The evaluation of INT-TDS. The TDS increases as the INT increases, the slope of curve is slight, and the time cost is available for generating the number of interpolations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>The evaluation of IGS-ICS-INT. The IGS of INT=8 is the highest in all groups, and the growth of ICS slowdown within more INT, comprehensive conclude that INT=8 may be the best choice for most scenarios.(a) The ground truth of droplet (b) The density map of ground truth (c) The morphing process with diffusion model (d) The morphing process with GenerateMap The evaluation of realistic generative results by our pipeline. The highlighted area in (c) is sharp, which is obviously different from spheres. (d) is our generative result, which is more smooth and directs the change on the whole.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Symbol definitions represent the two density maps are the input of our deep learning model. z1, z2 z1, z2 are vectors distributions of x1, x2, which are encoded by encoder of our model. F n F n illustrates the interpolations of our output, n means the nth samples of the continuous interpolations.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell cols="2">x1, x2 x1, x2 G n G</cell></row></table><note>n describes the ground truth images, n means the nth image in the series of ground truth.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ck,W , y max = max y − 3 * ck, H 11:for x min ≤ x ≤ x max and y min ≤ y ≤ y max do</figDesc><table><row><cell>12:</cell><cell>BNS = BN(x, y, ck)</cell></row><row><cell>13:</cell><cell>end for</cell></row><row><cell cols="2">14: end while</cell></row><row><cell cols="2">15: while k &lt; K and (x, y) ⊆ BNS do</cell></row><row><cell>16:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluation indexes</figDesc><table><row><cell>Index Symbol</cell><cell>Task</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">METHOD DESCRIPTIONTo train a general deep learning model to explore dynamic patterns of density maps, it is important to create available datasets. The property of samples in training data should contain different sizes, shapes, and time intervals, and they should also provide smooth and continuous dynamic information. As a visualization system, we want to balance the conciseness and computational capabilities and to introduce as few interactions as possible. The dynamic change may be difficult to be identified, particularly when there are slight changes and noises. We enhance the visual effect to improve our system by using blending, field representation, and sampling. The next parts introduce how to apply these technologies to implement a general framework. For conveniently explaining our methods, we define some symbols as listed inTable 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to acknowledge the support from NSFC under Grants (No. 61802128, 61672237, 61532002).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cvae-gan: Fine-grained image generation through asymmetric training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2764" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualization of time-varying weather ensembles across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="841" to="850" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<idno>abs/1707.05776</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Texture-based visualization of uncertainty in flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Botchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Visualization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="647" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Kernel density estimation via diffusion. The annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">I</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Grotowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2916" to="2957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motionrugs: Visualizing collective trends in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buchmuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jackle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="76" to="86" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Texture-based feature tracking for effective time-varying data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Caban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1472" to="1479" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imaging vector fields using line integral convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Leedom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Lawrence Livermore National Lab., CA (United States</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual abstraction and exploration of multi-class scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Let it flow: a static method for exploring dynamic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Pacific Visualization Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation. arXiv, abs/1410</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8516</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Adversarial feature learning. arXiv, abs/1605.09782</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An exploration framework to identify and track movement of cloud systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doraiswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Nanjundiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2896" to="2905" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Adversarially learned inference. arXiv, abs/1606.00704</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optical flow modeling and computation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fortun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kervrann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Generative adversarial networks. arXiv, abs/1406.2661</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable lagrangian-based attribute space projection for multivariate unsteady flow data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Pacific Visualization Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of topology-based methods in visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leitte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hlawitschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iuricich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Floriani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scheuermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="643" to="667" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Smooth view-dependent level-of-detail control and its application to terrain rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1998 Proceedings Visualization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aguilar-Cázares</surname></persName>
		</author>
		<title level="m">Robust reference frame extraction from unsteady 2d vector fields with convolutional neural networks. arXiv, abs</title>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data flow analysis and visualization for spatiotemporal statistical data without trajectory information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1287" to="1300" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
		<title level="m">Auto-encoding variational bayes. arXiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural networks technique for filling gaps in satellite measurements: Application to ocean color observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krasnopolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nadiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bayler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Behringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Kind of like that</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krawetz</surname></persName>
		</author>
		<ptr target="http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html.Accessed" />
		<imprint>
			<date type="published" when="2013-01-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno>abs/1512.09300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anisotropic blue noise sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">167</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing uncertain tropical cyclone predictions using representative samples from ensembles of forecast tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M K</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Creem-Regehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>House</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="882" to="891" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Illustrative visualization of mesoscale ocean eddies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Bemis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Curchitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="447" to="458" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Uncertainty visualization by representative sampling from prediction ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Boone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Ruginski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M K</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hegarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Creem-Regehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>House</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2165" to="2178" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Higher dimensional vector field visualization: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Laramee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Practice of Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="149" to="163" />
			<date type="published" when="2009" />
			<publisher>The Eurographics Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="681" to="682" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualization in meteorologya survey of techniques and tools for data analysis tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rautenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafirovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siemen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirzargar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3268" to="3296" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using deep learning to fill spatio-temporal data gaps in hydrological monitoring networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cromwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hydrology and Earth System Sciences Discussions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1879" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Proceedings of the Thirty AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A multiscale approach to mesh-based surface tension flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thürey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Usiigaci: Instance-aware cell tracking in stain-free phase contrast microscopy enabled by machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SoftwareX</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="230" to="237" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An exploratory framework for cyclone identification and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Valsangkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1460" to="1473" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust image hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Koon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Jakubowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The International Conference on Image Processing</title>
		<meeting>The International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="664" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mobilitygraphs: Visual analysis of mass mobility dynamics via spatio-temporal graphs and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Von</forename><surname>Landesberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brodkorb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roskosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualization and visual analysis of ensemble data: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-class blue noise sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y.</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">In-situ sampling of a large-scale particle simulation for interactive visualization and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woodring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Figg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wendelberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heitmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1151" to="1160" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Opinionflow: Visual analysis of opinion diffusion on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1763" to="1772" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A survey of blue-noise sampling and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="439" to="452" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical streamline bundles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Shene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1353" to="1367" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Implementation and benchmarking of perceptual image hash functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zauner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Hagenberg Campus</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Upper Austria University of Applied Sciences</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1655" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual analysis of haze evolution and correlation in beijing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="161" to="176" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
