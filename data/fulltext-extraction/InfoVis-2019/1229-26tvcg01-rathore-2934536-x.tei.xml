<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934536</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. We constructed models that estimate human color-concept associations using color distributions extracted from images of relevant concepts. We compared methods for extracting color distributions by defining different kinds of color tolerance regions (white outlines) around each target color (regularly spaced large dots) in CIELAB space. Subplots show a planar view of CIELAB space at L* = 50, with color tolerance regions defined as balls (left column; radius Δr ), cylindrical sectors (middle columns; radius Δr and hue angle Δh), and category boundaries around each target color (right column; Red, Orange, Yellow, Green, Blue, Purple, Pink, Brown, Gray; white and black not shown). Each target color is counted as "present" in the image each time any color in its tolerance region is observed. This has a smoothing effect, which enables the inclusion of colors that are not present in the image but similar to colors that are. A model that includes two sector features and a category feature best approximated human color-concept associations for unseen concepts and images (see text for details).</p><p>Abstract-To interpret the meanings of colors in visualizations of categorical information, people must determine how distinct colors correspond to different concepts. This process is easier when assignments between colors and concepts in visualizations match people's expectations, making color palettes semantically interpretable. Efforts have been underway to optimize color palette design for semantic interpretablity, but this requires having good estimates of human color-concept associations. Obtaining these data from humans is costly, which motivates the need for automated methods. We developed and evaluated a new method for automatically estimating color-concept associations in a way that strongly correlates with human ratings. Building on prior studies using Google Images, our approach operates directly on Google Image search results without the need for humans in the loop. Specifically, we evaluated several methods for extracting raw pixel content of the images in order to best estimate color-concept associations obtained from human ratings. The most effective method extracted colors using a combination of cylindrical sectors and color categories in color space. We demonstrate that our approach can accurately estimate average human color-concept associations for different fruits using only a small set of images. The approach also generalizes moderately well to more complicated recycling-related concepts of objects that can appear in any color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms-Visual Reasoning, Visual Communication, Visual Encoding, Color Perception, Color Cognition, Color Categories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In visualizations of categorical information (e.g., graphs, maps, and diagrams), designers encode categories using visual properties (e.g., colors, sizes, shapes, and textures) <ref type="bibr" target="#b6">[6]</ref>. Color is especially useful for encoding categories for two main reasons. First, cognitive representations of color have strong categorical structure <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b52">52]</ref>, which naturally maps to categories of data <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b15">15]</ref>. Second, people have rich semantic associations with colors called (e.g., a particular red associated with strawberries, roses, and anger) <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b33">33]</ref>, which they use to interpret meanings of colors in visualizations <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43]</ref>. Indeed, it is easier to interpret visualizations if semantic encoding between colors and concepts (referred to as match people's expectations derived from their color-concept associations <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref>.</p><p>Recent research has investigated how to optimize color palette design to produce color-concept assignments that are easy to interpret <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43]</ref>. Methods typically involve quantifying associations between each color and concept of interest, and then using those data to calculate optimal color-concept assignments for the visualization <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43]</ref>. It may seem that the best approach would be to assign concepts to their most strongly associated colors, but that is not always the case. Sometimes, it is better to assign concepts to weakly associated colors to avoid confusions that can arise when multiple concepts are associated with similar colors (see Section 2.2) <ref type="bibr" target="#b41">[41]</ref>. Thus, to leverage these optimization methods for visualization design, it is necessary to have an effective and efficient way to quantify human color-concept associations over a large range of colors. Only knowing the top, or even the top few strongest associated colors with each concept may provide insufficient data for optimal assignment.</p><p>One way to quantify human color-concept associations is with human judgments, but collecting such data requires time and effort. A more efficient alternative is to automatically estimate color-concept associations using image or language databases. Previous studies laid important groundwork for how to do so as part of end-to-end methods for palette design <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b43">43]</ref>. However, without directly comparing estimated color-concept associations to human judgments, it is unclear how well they match. Further, questions remain about how best to extract information from these databases to match human judgments.</p><p>The goal of our study was to understand how to effectively and efficiently estimate color-concept associations that match human judgments. These estimates can serve as input for palette design, both for creating visualizations and for creating stimuli to use for visual reasoning studies on how people interpret visualizations.</p><p>Contributions. Our main contribution is a new method for automatically estimating color-concept associations in a way that strongly correlates with human ratings. Our method operates directly on Google Image search results, without the need for humans in the loop. Creating an accurate model requires fine-tuning the way in which color information is extracted from images. We found that color extraction was most effective when it used features aligned with perceptual dimensions in color space and cognitive representations of color categories.</p><p>To test the different extraction methods, we used a systematic approach starting with simple geometry in color space and building toward methods more grounded in color perception and cognition. We used cross-validation with a set of human ratings to train and validate the model. Once generated, the model can be used to estimate associations for concepts and colors not seen in the training process without requiring additional human data. We demonstrated the effectiveness of this process by training the model using human association ratings between 12 fruit concepts and 58 colors, and testing it on a dataset of 6 recycling-themed concepts and 37 different colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Several factors are relevant when designing color palettes for visualizing categorical information. First and foremost, colors that represent different categories must appear different; perceptually discriminable <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46]</ref>. Other considerations include selecting colors that have distinct names <ref type="bibr" target="#b16">[16]</ref>, are aesthetically preferable <ref type="bibr" target="#b12">[12]</ref>, or evoke particular emotions <ref type="bibr" target="#b4">[4]</ref>. Most relevant to the present work, it is desirable to select semantically interpretable color palettes to help people interpret the meanings of colors in visualizations <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43]</ref>. This can be achieved by selecting "semantically resonant" colors, which are colors that evoke particular concepts <ref type="bibr" target="#b23">[23]</ref>. It can also be achieved when only a subset of the colors are semantically resonant if conditions support people's ability to infer the other assignments <ref type="bibr" target="#b41">[41]</ref>, see Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Creating semantically interpretable color palettes</head><p>Many approaches exist for creating semantically interpretable color palettes <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43]</ref>, but they generally involve the same two stages:</p><p>1. Quantifying color-concept associations.</p><p>2. Assigning colors to concepts in visualizations, using the colorconcept associations from stage 1.</p><p>Assigning colors to concepts in visualizations (stage 2) relies on input from color-concept associations (stage 1), which suggests assignments are only as good as the association data used to generate them. Colorconcept association data are good when they match human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Quantifying color-concept associations</head><p>A direct way to quantify human color-concept associations is with humans judgments. Methods include having participants rate association strengths between colors and concepts <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b41">41]</ref>, select colors that best fit concepts <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b53">53]</ref>, or name concepts associated with colors <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b33">33]</ref>. However, collecting these data is time-and resource-intensive. An alternative approach is to estimate human color-concept associations from large databases, such as tagged images <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b43">43]</ref>, color naming data sets <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b43">43]</ref>, semantic networks <ref type="bibr" target="#b14">[14]</ref>, and natural language corpora <ref type="bibr" target="#b43">[43]</ref>. Each type of database enables linking colors with concepts but has strengths and weaknesses, so automated methods often combine information from multiple databases <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b43">43]</ref>.</p><p>Extracting colors from tagged images (e.g., Flickr, Google Images) provides detailed color information for a given concept because of the large range of colors within images <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b43">43]</ref>. Methods must specify how to represent colors from images and what parts of the image to include. Linder et al. <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref> obtained a color histogram with bin sizes of 15 × 15 × 15 units in CIELAB space from all pixels in an image. Similarly, Lin et al. <ref type="bibr" target="#b23">[23]</ref> calculated color histograms using smaller 5 × 5 × 5 bins, together with heuristics to smooth the histogram and remove black or white backgrounds. In a different approach, Setlur and Stone <ref type="bibr" target="#b43">[43]</ref> and <ref type="bibr">Bartram et al. [4]</ref> used clustering algorithms to determine dominant colors in images. Clustering can be effective for finding the top colors in an image, but does not provide information on the full color range.</p><p>Image extraction methods must also specify how to query image databases to obtain images for each concept. Lin et al. <ref type="bibr" target="#b23">[23]</ref> used queries of each concept word alone (e.g., "apple") and queries with "clipart" appended to the concept word (e.g.,"apple clipart"). They reasoned that for some concepts, humanmade illustrations would better capture people's associations (e.g., associations between "money" and green might be missed from photographs of US dollars, which are more grayish than green). Setlur and Stone <ref type="bibr" target="#b43">[43]</ref> also used "clipart", and filtered by color using Google's dominant color filter.</p><p>Language-based databases provide a different approach, linking colors and concepts through naming databases (XKCD color survey <ref type="bibr" target="#b29">[29]</ref> used in <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b43">43]</ref>), concept networks (ConceptNet <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b44">44]</ref> used in <ref type="bibr" target="#b14">[14]</ref>), or linguistic corpora (Google Ngram viewer used in <ref type="bibr" target="#b43">[43]</ref>). Naming databases provide information about color-concept pairs that were spontaneously named by participants. These data can be sparse if they they lack information about concepts that are moderately associated with a color but not strongly associated enough to elicit a color name. Concept networks and linguistic corpora link concepts to color words (not coordinates in a color space), so these methods tend to be used in conjunction with naming <ref type="bibr" target="#b14">[14]</ref> and image <ref type="bibr" target="#b43">[43]</ref> databases to link color words to color coordinates for use in visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Assigning colors to concepts in visualizations</head><p>Once color-concept associations are quantified, they can be used to compute color-concept assignments for visualizations. Various methods have been explored for computing assignments. Lin et al. <ref type="bibr" target="#b23">[23]</ref> computed affinity scores for each color-concept pair using an entropy-based metric, and then solved the assignment problem (a linear program) to find the color-concept assignment that maximized the sum of affinity scores. They found that participants were better at interpreting charts of fictitious fruit sales with color palettes generated from their algorithm, compared with the Tableau 10 standard order ( <ref type="figure">Fig. 2A)</ref>. In another approach, Setlur and Stone <ref type="bibr" target="#b43">[43]</ref> applied k-means clustering to quantize input colors into visually discriminable clusters using CIELAB Euclidean distance, and iteratively reassigned colors until all color-concept conflicts were resolved.</p><p>In a third approach, Schloss et al. <ref type="bibr" target="#b41">[41]</ref> solved an assignment problem similar to <ref type="bibr" target="#b23">[23]</ref>, except they computed merit functions (affinity scores) differently. They compared three merit functions: (1) isolated, assigning each concept to its most associated color while avoiding conflicts, (2) balanced, maximizing association strength while minimizing confusability, and (3) baseline, maximizing confusability <ref type="figure">(Fig. 2B</ref>). They found that participants were able to accurately interpret color meanings for unlabeled recycling bins using the balanced color set, were less  <ref type="figure">Figure 2</ref>. Examples of designs that use color-concept associations to automatically assign colors to concepts. In (A), data visualizations showed fictitious fruit sales and participants interpreted the colors using legends <ref type="bibr" target="#b23">[23]</ref>. In (B), visualizations showed recycling bins and participants interpreted colors without legends or labels <ref type="bibr" target="#b41">[41]</ref>.</p><p>accurate for bins using the isolated color set, and were at chance for bins using the baseline color set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interpreting visualizations of categorical information, and implications for palette design</head><p>When people interpret the meanings of colors in visualizations, they use a process called assignment inference <ref type="bibr" target="#b41">[41]</ref>. In assignment inference, people infer assignments between colors and concepts that would optimize the association strengths over all paired colors and concepts. Sometimes, that means inferring that concepts are assigned to their strongest associated color (e.g., paper to white in <ref type="figure">Fig. 2B</ref>). But, other times it means inferring that concepts are assigned to weaker associates, even when there are stronger associates in the visualization (e.g., plastic to red and glass to blue-green, even though both concepts are more strongly associated with white) <ref type="bibr" target="#b41">[41]</ref>. This implies that people can interpret meanings of colors that are not semantically resonant (i.e., strongly associated with the concepts they represent) if there is sufficient context to solve the assignment problem (though what constitutes "sufficient context" is the subject of ongoing research). People's capacity for assignment inference suggests that for any set of concepts, it is possible to construct many semantically interpretable color palettes. This flexibility will enable designers to navigate tradeoffs between semantics and other design objectives-discriminabilty, nameablity, aesthetics, and emotional connotation, which are sometimes conflicting <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b12">12]</ref>. To create palette designs that account for these objectives, it is necessary to have good estimates of human colorconcept associations over a broad range of colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENERAL METHOD</head><p>In this section, we describe our approach to training and testing our algorithm for automatically estimating human color-concept associations (illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>). We began by collecting color-concept association data from human participants to use as ground truth. Then, we used Google Images to query each of the concepts and retrieve relevant images. We tested over 180 different methods (features) across Experiments 1A-C for extracting color distributions from images. We selected features (how many and which ones to use) by applying sparse regression with cross-validation to avoid over-fitting and produce estimates that generalized well. Model weights for each feature were chosen by ordinary linear regression.</p><p>For training and testing in Experiments 1 and 2, we used a set of 58 colors uniformly sampled in CIELAB color space (ΔE = 25), which we call the UW-58 colors (see Supplementary Material Section S.1 and <ref type="table">Supplementary Table S</ref>.1). The concepts were 12 fruits: avocado, blueberry, cantaloupe, grapefruit, honeydew, lemon, lime, mango, orange, raspberry, strawberry, and watermelon. We chose fruits, as in prior work <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b43">43]</ref>, because fruits have characteristic, directly observable colors (high color diagnosticity <ref type="bibr" target="#b47">[47]</ref>). We sought to establish our method for simple cases like fruit where we believed there was sufficient color information within images to estimate human color-concept associations. In future work it will be possible to identify edge cases where the method is less effective and address those limitations. In Experiment 3, we tested how our trained algorithm generalized to a different set of colors and concepts using color-concept association ratings for recycling concepts from <ref type="bibr" target="#b41">[41]</ref>. In the remainder of this section, we describe our methods in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Human ratings of color-concept associations</head><p>To obtain ground truth for training and testing our models, we had participants rate association strengths between each of the UW-58 colors and 12 fruits. Participants. We tested 55 undergraduates (mean age = 18.52, 31 females, 24 males) at UW-Madison. Data was missing from one participant (technical error). All had normal color vision (screened with HRR Pseudoisochromatic Plates <ref type="bibr" target="#b13">[13]</ref>), gave informed consent, and received partial course credit. The UW-Madison Internal Review Board approved the protocol. Design, displays, and procedure. Participants rated how much they associated each of the UW-58 colors with each of 12 fruit concepts. Displays on each trial contained a concept name at the top of the screen in black text, a colored square centered below the name (100 × 100 pixels), and a line-mark slider scale centered below the colored square. The left end point of the scale was labeled "not at all", the right end point labeled "very much", and the center point was marked with a vertical divider. Participants made their ratings by sliding the cursor along the response scale and clicking to record their response. Displays remained on the screen until response. Trials were separated by a 500 ms inter-trial interval. All 58 colors were rated for a given concept before going onto the next concept. The order of the concepts and the order of colors within each concept were randomly generated for each participant.</p><p>Before beginning, participants completed an anchoring procedure so they knew what it meant to associate "not at all" and "very much" in the context of these concepts and colors <ref type="bibr" target="#b34">[34]</ref>. While viewing a display showing all colors and a list of all concepts, they pointed to the color they associated most/least with each concept. They were told to rate those colors near the endpoints of the scale during the task.</p><p>Displays were presented on a 24.1 in ASUS ProArt PA249Q monitor (1920 × 1200 resolution), viewed from a distance of about 60 cm. The background was gray (CIE Illuminant D65, x = .3127, y = .3290, Y = 10 cd/m 2 ). The task was run using Presentation (www.neurobs.com). We used a Photo Research PR-655 SpectraScan spectroradiometer to calibrate the monitor and verify accurate presentation of the colors. The deviance between the measured colors and target colors in CIE 1931 xyY coordinates was &lt; .01 for x and y, and &lt; 1 cd/m 2 for Y.</p><p>The mean ratings for all fruit-color pairs averaged over all participants are shown in <ref type="figure">Supplementary Figs</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training color extraction and testing performance</head><p>Our goal was to learn a method for extracting color distributions from images such that the extracted profiles were reliable estimates of human color-concept association ratings from Section 3.1. We will describe our approach here using generic parameter names. The parameter values we actually used in the experiments are specified in the relevant experiment descriptions (Section 4).</p><p>For each of the n con concepts, we queried Google Images using the name of the concept and downloaded the top n img results. We then compiled a list of n feat features. A feature is a function f : (image, color) → R that quantifies the presence of a given target color in a given image. For example, a feature could be "the proportion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCEPT</head><p>"lime"   of pixels in the middle 20% of the image that are within Δr = 40 of the target color". For each of the n img images and each of the n col colors, we evaluated each of the n feat features. This resulted in a matrix X ∈ R n con n img n col ×n feat , where each row was a (concept, image, color) triplet and each column was a different feature. We then constructed a vector y ∈ R n con n img n col ×1 such that the i th element of y contains the average color-concept rating from the human experiments, for the concept and color used in the i th row of X. Note that each rating in y was repeated n img times because for each (concept, color) pair, there are n img images. We used the data (X, y) in two ways: (1) to select how many features to use and (2) to choose feature weights.</p><p>Feature selection. We used sparse regression (lasso) with leave-oneout cross-validation to select features. Specifically, we selected a concept, partitioned (X, y) by rows into test data (X test , y test ), containing the rows pertaining to the selected concept, and training data (X train , y train ), containing the remaining concepts. We then used sparse regression on the training data with a sweep of the regularization parameter. This was repeated with every concept and we plotted the average test error versus the number of nonzero weights <ref type="figure" target="#fig_4">(Fig. 5)</ref>. We examined the plot and found that k = 4 features provided a good trade-off between error and model complexity, so we used k = 4 features for all subsequent experiments. To select which features to use, we ran one final sparse regression using the full data (X, y) and chose the regularization parameter such that k = 4 features emerged.</p><p>Using cross-validation for model selection is standard practice in modern data science workflows. By validating the model against data that was unseen during the training phase, we are protected against overfitting and we help ensure that our model will generalize to unseen concepts and colors or different training images. Choosing feature weights. Once the best k features were identified, we selected the weights by performing an ordinary linear regression with these k features. We ensured that human data used to compute the model weights were always different from human data used test model performance. In Experiment 1 and 2, when we trained and tested on fruit concepts, we chose feature weights using 11 fruits and tested on the 12th fruit (repeated for each fruit). In Experiment 3, we used a model trained on all 12 fruit concepts to test how well it predicted data for recycling concepts (Section 4.5). Testing the model on new data. Once the model has been trained, it can be used to estimate color-concept associations for new concepts and new colors without the need to gather any new human ratings. The concept is queried in Google Images, the k chosen features are extracted from the images for the desired colors, and the trained model weights are applied to the features to obtain the estimates. Reproducible experiments. We used cross-validation in order to ensure our results hold more broadly beyond our chosen concepts, colors, and images. We developed code in Python for all the experiments and made use of the scikit-image <ref type="bibr" target="#b48">[48]</ref> and scikit-learn <ref type="bibr" target="#b36">[36]</ref> libraries to perform all the image processing and regression tasks. Our code is available at https://github.com/Raginii/ Color-Concept-Associations-using-Google-Images. This repository can be downloaded to a local machine to replicate the entire study or adapt it to test new concepts and colors. The repository also contains a write-up with detailed instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In Experiments 1A-1C, we systematically tested methods for extracting colors from images and assessed model fits with human color-concept association ratings. In Experiment 2, we examined model performance using different image types (top 50 Google Image downloads, cartoons, and photographs). In Experiment 3, we tested how well our best model from Experiment 1 generalized to a different set of concepts and colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment 1A: Balls in Cartesian coordinates</head><p>Perhaps the simplest approach for extracting colors from images would be to (1) define a set of target colors of interest in a color space (e.g., CIELAB), (2) query a concept in Google Images (e.g., "blueberry"), (3) download images returned for that concept, and (4) count the number of pixels in each image that has each target color within the set. However, that level of precision would exclude many colors in images, including some that are perceptually indistinguishable from the target colors.</p><p>Moreover, not all pixels in the image may be relevant. For example, when people take pictures of objects, they tend to put the object near the center of the frame <ref type="bibr" target="#b32">[32]</ref>, so it is sensible that images ranked highly in Google Images for particular query terms will have the most relevant content near the center of the frame.</p><p>In this experiment, we varied color tolerance: allowing colors that are not perfect matches with targets to still be counted, and spatial windows: including subsets of the pixels that may be more likely to contain relevant colors for the concept. Our goal was to determine which combination of color tolerance and spatial window best captured human color-concept associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Methods</head><p>In this section, we explain how we downloaded and processed the images, constructed features, and selected features for our model.</p><p>Features. We constructed 30 features from all possible combinations of 5 color tolerances and 6 spatial windows as described below.</p><p>Color tolerances. When looking for a target color within a set of pixels in an image (set defined by the spatial window), we counted the fraction of all pixels under consideration whose color fell within a ball of radius Δr in CIELAB space of the target color. We tested balls with five possible values of Δr: 1, 10, 20, 30, and 40.</p><p>Spatial windows. We varied spatial windows in six ways. The first five extracted pixels from the center 20%, 40%, 60%, 80%, and 100% of the image, measured as a proportion of the total area. The 6 th way used a figure-ground segmentation algorithm to select figural, "object-like" regions from the image, and exclude the background. Here "window" is not rectangular, but rather the shape of the figural region(s) as estimated by the active contour algorithm <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b50">50]</ref>, Snakes. This uses an initial contour binary mask and iteratively moves to find the object boundaries. We used the MATLAB implementation activecontour from the Image Processing Toolbox for 500 iterations, setting the initial contour as the image boundary. Although prior work suggested that figure-ground segmentation might not provide a benefit beyond eliminating borders <ref type="bibr" target="#b23">[23]</ref>, we aimed to test its effects in the context of our other color tolerance and spatial window parameters.</p><p>Feature selection. We applied the feature selection method described in Section 3.2 using the 30 features described above, as well as a constant offset term, which is standard when using regression. Using an offset is equivalent to adding one more feature equal to the constant function 1. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the mean squared error (MSE) for each number of features, averaged across all n con = 12 fruits, using the n col = 58 UW-58 colors, and using n img = 50 Google Image query results for each fruit.</p><p>Based on this plot, we decided to use 4 features, which yields a good trade-off between model complexity and error reduction. We then used the full dataset to select the best 4 features, as described in Section 3.2.  <ref type="table" target="#tab_3">Table 1</ref>. The positive weight on the center of the image and negative weight on 100% of the image can be interpreted as a crude form of background suppression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results and discussion</head><p>We tested the model on each of the fruits using the leave-one-out cross-validation procedure described in Section 3.2. We trained model weights using the 11 × 50 = 550 training images and averaged the model estimates across the 50 test images. We tested the effectiveness of the Ball model by correlating its estimates with mean human ratings over all 12 fruits × 58 colors and found a moderate correlation of .65 <ref type="table" target="#tab_4">(Table 2</ref>). <ref type="figure">Fig. 6</ref> shows the correlations separately for each fruit (light gray points), with fruits sorted along the x-axis from highest to lowest r value for the Ball model. There is wide variability in the model fits, ranging from r = .93 for orange to r = .27 for blueberry.</p><p>To understand why the model performed poorly for some fruits, we plotted estimated ratings for each color as a function of human ratings. <ref type="figure" target="#fig_7">Fig. 7</ref> highlights a subset of three fruits with high, medium, and low correlations. The full set of plots are shown in <ref type="figure" target="#fig_2">Supplementary  Fig. S.3</ref>. Error in performance seemed to arise from underestimating the association strength for colors that did not appear in the images but were associated with the concepts. This was particularly apparent for blueberry, where participants strongly associated a variety of blues that were more saturated and purplish than the blues that appeared in images of blueberries. Model estimates for those blues were as low as model estimates for oranges and greens, which were clearly not associated with blueberry. The model also overestimated values for grays and purples that were not associated with blueberry. These results suggested that different kinds of features would be necessary for capturing human color-concept associations.  <ref type="figure">Figure 6</ref>. Correlations between model estimations and human ratings across each of the UW-58 colors for each fruit from the best 4-feature model in Experiment 1A (Ball model), Experiment 1B (Sector model), and Experiment 1C (Sector+Category model). The Sector+Category model performed best, followed by the Sector model, then the Ball model, see <ref type="table" target="#tab_4">Table 2</ref> and text for statistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment 1B: Sectors in cylindrical coordinates</head><p>A potential limitation of the ball features in Experiment 1A is that varying the size of the ball has different perceptual consequences depending on the location in color space. This is because perceptual dimensions of color are cylindrical (angle: hue, radius: chroma, height: lightness) rather than Cartesian. Balls of a fixed size that are closer to the central L* axis will span a greater range of hue angles than balls that are farther away (i.e., higher chroma). In the extreme, a ball centered on a* = 0 and b* = 0 (e.g., a shade of gray) will include colors of all hues. Thus, if we wanted a ball that was large enough to subsume all the high chroma blues (i.e., colors strongly associated with blueberries), that same large ball placed near the achromatic axis would subsume all hues of sizable chroma (see <ref type="figure">Figure 1</ref>). To have independent control over hue and chroma variability, we defined new features with tolerance regions as cylindrical sectors around the target colors according to hue angle and chroma <ref type="figure">(Fig. 1)</ref>. We tested whether color extraction using sector features, more aligned with perceptual dimensions of color space, would better estimate human color-concept associations. We used the same images as in Experiment 1A, and confined the number of features to four, using the same sparse regression approach as in Experiment 1A for feature selection. We assessed whether the best model included any of these new features, and if so, if that model's estimates fit human ratings significantly better than the Ball model did in Experiment 1A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Methods</head><p>We included the same 30 features from Experiment 1A, plus 150 new features: 25 new color tolerance regions × 6 spatial windows (same spatial windows as Experiment 1A) for a total of 180 features. The 25 new color tolerance regions were defined in cylindrical coordinates in CIELch space using all combinations of 5 hue angle tolerances (Δh: 5°, 10°, 20°, 30°, 40°) and 5 chroma/lightness tolerances (Δr: 1, 10, 20, 30, 40) around each target color, see <ref type="figure">Fig. 1</ref>. The tolerances for chroma and lightness co-varied, so Δr = 10 means that both chroma and lightness had a tolerance of 10. Note that CIELch space is the same as CIELAB space except it uses cylindrical rather than Cartesian coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results and discussion</head><p>As in Section 4.1.1, we first extracted the best 4 features from the pool of 180 features using sparse regression. The 4 top features only  included sector features and no ball features <ref type="table" target="#tab_3">(Table 1)</ref>, so we refer to the model from Experiment 1B as the "Sector" model. We tested the effectiveness of the Sector model by correlating its estimates with mean human ratings over all 12 fruits × 58 colors. This correlation (r = .72) was stronger for the Sector model than for the Ball model ( <ref type="table" target="#tab_4">Table 2)</ref>, and that difference was significant (z = 2.46, p = .014). <ref type="figure">Fig. 6 (medium gray points)</ref> shows that the Sector model improved performance for fruits that had the weakest correlations in Experiment 1A, but there is still room for improvement. The scatter plots in <ref type="figure" target="#fig_7">Fig. 7</ref> show that the model still under-predicts ratings for blues that are strongly associated with blueberries but are not found in the images. A similar problem can be observed for limes, where several greens are associated with limes, but are not extracted from the images. The full set of scatter plots is in <ref type="figure" target="#fig_3">Supplementary Fig. S.4</ref>.</p><p>These results suggest that when people form color-concept associations, they might extrapolate to colors that are not directly observed from visual input. We address this possibility in Experiment 1C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment 1C: Color categories</head><p>In Experiment 1C, we examined the possibility that human colorconcept associations extrapolate to colors that are not directly observed from visual input. One way that extrapolation might occur is through color categorization. Although colors exist in a continuous space, humans partition this space into discrete categories. English speakers use 11 color categories with the basic color terms red, green, blue, yellow, black, white, gray, orange, purple, brown, and pink <ref type="bibr" target="#b5">[5]</ref>. The number of basic color terms varies across languages <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b52">52]</ref>, but there are regularities in the locus of categories in color space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">20]</ref>.</p><p>We propose that when people form color-concept associations from visual input, they extrapolate to other colors that are not in the visual input but share the same category (category extrapolation hypothesis). To test this hypothesis, it was necessary to first identify the categories of each color within images. We did so using a method provided by Parraga and Akbarinia <ref type="bibr" target="#b35">[35]</ref>, which used psychophysical data to determine category boundaries for each of the 11 color terms. Their algorithm enables efficient lookup and categorization of each pixel within and image. We then constructed a new type of feature that represents the proportion of pixels in the image that share the color category of each of the UW-58 colors. For example, if .60 of the pixels in the image are categorized as "blue" (using <ref type="bibr" target="#b35">[35]</ref>), then all UW-58 colors that are also categorized as "blue" will receive a feature value of .60, regardless of how much of those UW-58 colors were in the image. We assessed whether the best model included any category new features, and if so, whether that model's estimates fit human rating significantly better than the Sector model did in Experiment 1B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Methods</head><p>We included the 30 ball and 150 sector features in Experiments 1A and 1B, plus 6 new color category features for a total of 186 features. We generated category features using Parraga and Akbarinia's <ref type="bibr" target="#b35">[35]</ref> method to obtain the color categories of our UW-58 colors and the categories of each pixel within our images. We used the functions available through their Github repository <ref type="bibr" target="#b1">[2]</ref> to convert RGB color coordinates to color categories. Specifically, we converted the 100 × 100 × 3 image arrays in RGB to 100 × 100 arrays, where each element in the array represents the pixel's color category. For each UW-58 color, we defined the features to be the fraction of pixels in the spatial window that belonged to the same color category as the UW-58 color. We repeated the above procedure with the 6 spatial windows as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results and discussion</head><p>Similar to Experiments 1A and 1B, we used sparse regression to extract the best 4 features among 186 total features. The model selected the constant offset and two of the same sector features from Experiment 1B, plus one new category feature (no ball features), see <ref type="table" target="#tab_3">Table 1</ref>). Thus, we refer to this new model as the "Sector+Category" model. We obtained the model weights via linear regression as detailed in Section 3.2.</p><p>We tested the effectiveness of the Sector+Category model by correlating its estimates with mean human ratings over all 12 fruits × 58 colors. This correlation was stronger for the Sector+Category model than for the Ball model or Sector model <ref type="table" target="#tab_4">(Table 2)</ref>, and those differences were significant (z = 6.55, p &lt; .001; z = 4.08, p &lt; .001, respectively). <ref type="figure">Fig. 6</ref> shows that the Sector+Category model (black points) further improved fits for the fruits that had weaker fits using the other two models. As seen in <ref type="figure" target="#fig_7">Fig. 7</ref>, by including category extrapolation, this model increased the estimated values for colors that were strongly associated with limes and blueberries (greens and blues, respectively) that were under-predicted by the previous models because those colors were not in the images. The full set of scatter plots is in <ref type="figure">Supplementary Fig. S</ref>.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment 2: Comparing image types</head><p>As described in Section 2.1.1, Lin et al. <ref type="bibr" target="#b23">[23]</ref> queried concept words alone and concept words appended with "clipart", reasoning that humanmade illustrations might better capture associations for some types of concepts. We propose that humans produce clipart illustrations based on their color-concept associations, not solely based on real-world color input. If color-concept associations are already incorporated into clipart, that would explain why clipart is useful for estimating color-concept associations, especially when natural images fall short. However, if a model contains features that effectively estimate human color-concept associations, it may have sufficient information to do as well for natural images as it does for humanmade illustrations, such as clipart. To examine this hypothesis, we tested our models from Experiments 1A-C on two new image types: cartoons (humanmade illustrations) and photographs (not illustrations).</p><p>In addition, we used the approach of Lin et. al. (mentioned in Section 2.1.2) to compute probabilities, a precursor to their affinity score that most corresponds to color-concept associations. We then compared those probabilities with our human ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Methods</head><p>We downloaded two new sets of images, by querying each fruit name appended with "cartoon" or "photo". We queried "cartoon" rather than "clipart" because clipart sometimes contained parts of photographs with the background deleted, and we wanted to constraint this image set to humanmade illustrations. Unlike Experiment 1 where we used the first 50 images returned by Google Images, we manually curated the photo and cartoon image sets to ensure (a) they were photos for the photo set and cartoons in the cartoon set, (b) they included an observable image of the queried fruit somewhere in the image, and (c) they were not images of cartoon characters (e.g., "Strawberry Shortcake", a character in an animated children's TV show that first aired in 2003). This resulted in 50 images in each set.</p><p>We trained and tested using the same top 4 features from the Ball, Sector, and Sector+Category models in Experiment 1, except we substituted the training images for the manually curated sets of cartoon images or photo images. This yielded three sets of model weights for the different image types. We then compared each model's performance for the three image types. <ref type="figure">Fig. 8</ref> and <ref type="table" target="#tab_4">Table 2</ref> show the overall correlations between color-concept associations and model estimates across all 12 fruits × 58 colors. The correlations for the top 50 images are those previously reported in Experiment 1A-1C, and included here as a baseline. <ref type="figure">Fig. 8</ref> also shows overall correlations with the probabilities computed using the method of Lin et. al. <ref type="bibr" target="#b23">[23]</ref> as another baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Results and discussion</head><p>The results suggest there is a benefit to using human-illustrated cartoons for the Ball model (which does not effectively capture human color-concept associations), but the benefit diminishes for the Sector and Sector+Category models (which better capture human colorconcept associations). Specifically, the Ball model using cartoons was significantly more correlated with human ratings than the Ball model using top 50 images (z = 2.46, p = .014) with no difference between cartoon and top 50 images for the other two models (Sector: z = 0.0,  <ref type="figure">Figure 8</ref>. Correlations for top 50 images, photo images, and cartoon images using the Ball, Sector, and Sector+Category models. The Sec-tor+Category model performed best and was similar across all image types. The Ball model was worst for top 50 and photo images, but less poor for cartoon images. Estimates based on Lin et al. <ref type="bibr" target="#b23">[23]</ref> were strongly correlated with human ratings, but less so than the Sector+Category model (see text for statistics and explanation). p = 1.0; Sector+Category: z = .53, p = .596). There were no significant differences in fits using photo vs. top 50 images for any of the three model types (Ball: z = 0.94, p = .347; Sector: z = 1.11, p = .267; Sector+Category: z = 0.53, p = .596).</p><p>To further understand these differences, we tested for interactions between image type and feature type, using linear mixed-effect regression (R version 3.4.1, lme4 1.1-13, see <ref type="bibr" target="#b7">[7]</ref>). The dependent measure was model error for each fruit (sum of the squared errors across colors for each fruit). We included fixed effects for Model, Image, and their interactions, and random slopes and intercepts for fruit type within each Model contrast and Image contrast. We initially tried including random slopes and intercepts for interactions, but the model became too large and the solver did not converge. We tested two contrasts for the Model factor. The first was Category+Sector vs. average of Ball and Sector, which enabled us to test whether Category+Sector was overall better than the other two models. The second was Sector vs. Ball, which enabled us to test whether the Sector model was better than the Ball model. We tested two contrasts for the Image factor. The first was cartoon vs. average of top 50 and photo, which enabled us to test whether cartoons were overall better than the other two images types. The second contrast was top 50 vs. photo, which enabled us to test whether top 50 images (which included some cartoons) were better than photos. Reported beta and t-values are absolute values.</p><p>The results for the Model contrasts showed that Sector+Category model preformed best, and the Sector model performed better than the Ball model. That is, there was less error for Sector+Category than the combination of Ball and Sector (β = 0.10, t(11) = 4.42, p = .001), and less error for Sector than for Ball (β = 0.083, t(11) = 7.90, p &lt; .001).</p><p>The contrasts for Image were not significant (ts &lt; 1), indicating no overall benefit for human made cartoons.</p><p>However, the first Image contrast comparing cartoons vs. the average of top 50 and photo interacted with both Model contrasts. Looking at the interaction with the first Model contrast (Sector+Category vs. average of Ball and Sector), the degree to which Sector+Category model outperformed the other models was greater for top 50 and photo images than for cartoons (β = 0.01, t(44) = 4.38, p &lt; .001), see <ref type="figure">Fig.  8</ref>. Looking at the interaction with the second Model contrast (Sector vs. Ball), the degree to which Sector model outperformed the Ball model was greater for the top 50 and photo images than the cartoons (β = 0.02, t(44) = 6.65, p &lt; .001). No other interactions were significant (ts &lt; 1).</p><p>In this experiment, we also evaluate how Lin et al.'s estimates of color-concept associations <ref type="bibr" target="#b23">[23]</ref> match our human ratings. Their estimates come from a hybrid of color distributions extracted from top image downloads and clipart, so we provide their model with our top 50 images and cartoons as input. As shown in <ref type="figure">Fig. 8</ref>, the correlation for all fruits and colors was r = .74, which is similar to our Sector models (r = .69 to .72 depending on image type) and less strong than our Sector+Category models (r = .80 to .81 depending on image type). The difference in correlation for the Lin et al. model and our Sec-tor+Category model for the top 50 images was significant (z = 3.29, p &lt; .001). We note that these models are not directly comparable because our models used either top 50 images or cartoons, not both at the same time (except if cartoons happened to appear in the top 50 images).</p><p>In summary, using cartoon images instead of other image types helped compensate for the poor performance of our Ball model. However, image type made no difference for our more effective Sector and Sector+Category models. We interpret these results as showing that cartoons help the Ball model compensate for poorer performance because humans make cartoons in a way that builds in aspects of human color-concept associations. For example, visual inspection suggests that cartoon blueberries tend to contain saturated blues that are highly associated with blueberries yet not present in photographs of blueberries. However, the benefit of humanmade illustrations is reduced if model features are better able to capture human color-concept associations. This suggests that using our Sector+Category models on the top Google Image downloads is sufficient for estimating human colorconcept associations without further need to curate the image set, at least for the concepts tested here. . Correlations between human ratings and estimated ratings across all colors for each recycling-related concept using the Sec-tor+Category model. The range of correlations is similar to fruits <ref type="figure">(Fig. 6</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiment 3: Generalizing beyond fruit</head><p>Experiment 3 tested how well our model trained on fruit generalized to a new set of concepts and colors using the recycling color-concept association dataset from <ref type="bibr" target="#b41">[41]</ref>. The concepts were: paper, plastic, glass, metal, compost, and trash, and colors were the BCP-37 (see <ref type="table">Supplementary Table S.</ref>2). Unlike fruits, which have characteristic colors, recyclables and trash can come in any color. Still, human ratings show systematic color-concept associations for these concepts, and we aimed to see how well our model could estimate those ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Methods</head><p>As in Experiment 1, we downloaded the top 50 images from Google Images for each recycling-related concept. To estimate color-concept associations, we used our Sector+Category model from Experiment 1C with feature weights determined from a single linear regression using all 12 fruits, as described in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Results and discussion</head><p>We tested the effectiveness of the Sector+Category model by correlating its estimates with mean human ratings over all 6 recycling-related concepts × 37 colors. The correlation was r = .68, p &lt; .001, moderately strong, but significantly weaker than the corresponding correlation of .81 for fruit concepts in Experiment 1C (z = 3.84, p &lt; .001). <ref type="figure" target="#fig_9">Fig. 9</ref> shows the correlations separately for each recycling concept. The fits range from .88 for paper to .40 for plastic, similar to the range for fruits in Experiment 1C (.94 to .49) (see <ref type="figure">Supplementary Fig. S.6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GENERAL DISCUSSION</head><p>Creating color palettes that are semantically interpretable involves two main steps, (1) quantifying color-concept associations and (2) using those color-concept associations to generate unique assignments of colors to concepts for visualization. Our study focused on this first step, with the goal of understanding how to automatically estimate human color-concept associations from image statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Practical and theoretical applications</head><p>We built on approaches from prior work <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b43">43]</ref> and harnessed perceptual and cognitive structure in color space to develop a new method for effectively estimating human color-concept associations. Our method can be used to create the input for various approaches to assigning colors to concepts for visualizations <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43]</ref>. By estimating full distributions of color-concept associations over color space that approximate human judgments (as opposed to identifying only the top associated colors), it should be possible to use assignment methods to define multiple candidate color palettes that are semantically interpretable for a given visualization. This flexibility will enable balancing semantics with other important factors in design, including perceptual discriminability <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46]</ref>, name difference <ref type="bibr" target="#b16">[16]</ref>, aesthetics <ref type="bibr" target="#b12">[12]</ref>, and emotional connotation <ref type="bibr" target="#b4">[4]</ref>. Our method can also be used to design stimuli for studies on visual reasoning. For example, evidence suggests people use assignment inference to interpret visualizations <ref type="bibr">[</ref>  requires the ability to carefully manipulate color-concept association strengths within visualizations, which requires having good estimates of human color-concept associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Forming color-concept associations</head><p>In addition to providing a new method for estimating color-concept associations, this study sparked new insights into how people might form color-concept associations in the first place. In <ref type="figure" target="#fig_11">Fig. 10</ref>, the path with solid arrows illustrates our initial premise for how people learn color concept associations from their environment. When they experience co-occurrences between colors and concepts, they extract color distributions and use them to update color-concept associations <ref type="bibr" target="#b39">[39]</ref>. In <ref type="figure" target="#fig_11">Fig. 10</ref>, color-concept association strength is indicated using marker width (e.g., blueberry is highly associated with certain shades of blue). However, in the present study we found that it was insufficient to only extract colors (or nearby colors) from images to estimate human color-concept associations (Experiments 1A and 1B). We needed to extrapolate to other colors that shared the same category as colors within the image to produce more accurate estimates (Experiment 1C).</p><p>Based on these results, we propose there is another part of the process-category extrapolation-illustrated by the path with dashed arrows in <ref type="figure" target="#fig_11">Fig. 10</ref>. While extracting the color distribution from color input, people categorize colors using basic color terms (e.g., "blue"). This categorization process extrapolates to colors that are not in the visual input, but are within the same color category (e.g., extrapolating to all colors categorized as "blue" upon seeing a blueberry, even though only a subset of blues are found in the image). We believe that extrapolated colors augment the color distribution extracted from color input, which in turn further updates color-concept associations. Given that categorization can influence color perception <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b51">51]</ref> and memory <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b21">21]</ref> (see <ref type="bibr" target="#b52">[52]</ref> for a review), category extrapolation may also feedback to influence color experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Open questions and limitations</head><p>Generalizability to other concepts. In this study, we focused on fruit-concrete objects with directly observable colors-so we could study different methods of extracting and extrapolating colors from images where the colors would be systematic. We assessed generalizability to other, recycling-related concepts that are less color diagnostic <ref type="bibr" target="#b47">[47]</ref> than fruit (e.g., paper, plastic, and glass can be any color), but recyclables are still concrete objects. However, there is concern that image-based methods may not be effective for estimating color-concept associations for abstract concepts that do not have directly observable colors <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b43">43]</ref>, though see <ref type="bibr" target="#b4">[4]</ref>. Nonetheless, people do have systematic associations between colors and abstract concepts <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b53">53]</ref>. Future work will be needed to asses the boundary conditions of image based methods and further explore incorporating other, possibly languagebased methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b43">43]</ref> for estimating color-concept associations for abstract concepts. Image segmentation. Our models might also be limited in their ability to generalize for concepts that refer to backgrounds rather than objects (e.g., "sky") <ref type="bibr" target="#b23">[23]</ref>. All three models included a feature that extracted colors from figural regions and segmented away the backgrounds. Further research is needed to evaluate performance for background-related concepts, but limitations might be mitigated using semantic segmentation, in which particular regions of images are tagged with semantic labels <ref type="bibr" target="#b27">[27]</ref>. Cultural differences. Our category extrapolation hypothesis implies that color-concept associations could differ between cultures whose languages have different color terms. Different languages partition color space in different ways <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b52">52</ref>]-e.g., some languages have separate color terms for blues and greens, whereas others have one term for both blues and greens. If a language has separate terms for blues and greens, experiencing blue objects like blueberries should result in color-concept associations that extrapolate only to other blues, not greens. But, if a language has one term for blues and greens, experiencing blueberries should result in associations that extrapolate to blues and greens. This is an exciting area for future research. Structure of color categories. Our model defined color categories using a boundary approach-either a color was in a given category or not, with no distinction among category members. However, color categories have more complex structure, including a prototype, or best example, and varying levels of membership surrounding the prototype <ref type="bibr" target="#b38">[38]</ref>. A model that accounts for these complexities in category structure may improve on the fit to human color-concept associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>The goal of this study was to assess methods for automatically estimating color-concept associations from images. We tested different color extraction features that varied in color tolerance and spatial window, different kinds of images, and different concept sets. The most effective model used features that were relevant to human perception and cognition-features aligned with perceptual dimensions of color space and a feature that extrapolated to all colors within a color category. This model performed similarly well across the top 50 images from Google Images, curated photographs, and curated cartoon images. The model also generalized reasonably well to a different set of colors and concepts without changing any parameters. Through this study, we produced a method trained and validated on human data for automatically estimating color-concept associations, while generating new hypotheses about how color input and category extrapolation work together to produce human color-concept associations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Estimating Color-Concept Associations from Image Statistics Ragini Rathore, Zachary Leggon, Laurent Lessard, and Karen B. Schloss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of our pipeline for automatically extracting color distributions from images. The bottom flow (concepts to color ratings to human associations) describes the slow yet reliable direct approach using human experiments to determine ground-truth associations. The top flow involves querying Google Images, extracting colors using a variety of different methods (features), then weighting those features appropriately to obtain estimated associations. Deciding which features to use and how to weight them is learned from human association data using sparse regression and cross-validation. Once the model is trained, color-concept associations can be quickly estimated for new concepts without additional human data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>UW-58 colors used in Experiment 1 and 2, plotted in CIELAB color space. Exact coordinates are given inSupplementary Table S.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Mean squared test error (MSE) as a function of the number of features selected using sparse regression (lasso). MSE is averaged over 12 models obtained using leave-one-out cross-validation, using each fruit category as different test set. We selected models with four features.The best 4 features were (1) constant offset, (2) 20% window with Δr = 40 (positive weight), (3) 100% window with Δr = 40 (negative weight), and (4) segmented figure with Δr = 40 (positive weight), see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Scatter plots showing relationships between model estimates and human ratings for lemon, lime, and blueberry using models from Experiments 1A-1C. Marks represent each of the UW-58 colors, dashed line represent best-fit regression lines, and r values indicate correlations within each plot. Adding more perceptually relevant (sector) and cognitively relevant (category) features improved fit for fruits where ball features performed poorly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Photo Cartoon</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9</head><label>9</label><figDesc>Figure 9. Correlations between human ratings and estimated ratings across all colors for each recycling-related concept using the Sec-tor+Category model. The range of correlations is similar to fruits (Fig. 6.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>41] (Section 2.1.2), but little is known about how assignment inference works. Studying this processes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Process model for how color-concept associations are formed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2019; accepted 1 Aug. 2019. Date of publication 16 Aug. 2019; date of current version 20 Oct. 2019. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2019.2934536</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. S.1 and S.2 and Supplementary Table S.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Top 4 features selected using sparse regression as more features were made available in Experiments 1A to 1C. Ball features were not selected when sector or category features became available.</figDesc><table><row><cell>Model description</cell><cell>Features selected</cell></row><row><cell>Ball model</cell><cell>constant offset</cell></row><row><cell>(Experiment 1A)</cell><cell>Ball: Δr = 40; 20% window</cell></row><row><cell>Features available:</cell><cell>Ball: Δr = 40; 100% window</cell></row><row><cell>Ball only</cell><cell>Ball: Δr = 40; segmented</cell></row><row><cell>Sector model</cell><cell>constant offset</cell></row><row><cell>(Experiment 1B)</cell><cell>Sector: Δr = 40, Δh = 40°; 20% window</cell></row><row><cell>Features available:</cell><cell>Sector: Δr = 40, Δh = 30°; 40% window</cell></row><row><cell>Ball, Sector</cell><cell>Sector: Δr = 40, Δh = 40°; segmented</cell></row><row><cell>Sector+Cat model</cell><cell>constant offset</cell></row><row><cell>(Experiment 1C)</cell><cell>Sector: Δr = 40, Δh = 40°; 20% window</cell></row><row><cell>Features available:</cell><cell>Sector: Δr = 40, Δh = 40°; segmented</cell></row><row><cell cols="2">Ball, Sector, Category Category; 20% window</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Correlations (r) between mean human color-concept association</cell></row><row><cell cols="4">ratings and estimated associations (12 fruits × 58 colors = 696 items)</cell></row><row><cell cols="4">for each model in Experiments 1 and 2 (also shown Fig.8). All p &lt; .001.</cell></row><row><cell>Model</cell><cell cols="3">Top 50 Photo Cartoon</cell></row><row><cell>Ball</cell><cell>.65</cell><cell>.62</cell><cell>.72</cell></row><row><cell>Sector</cell><cell>.72</cell><cell>.69</cell><cell>.72</cell></row><row><cell>Sector+Category</cell><cell>.81</cell><cell>.80</cell><cell>.80</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank Christoph Witzel, Brian Yin, John Curtain, Joris Roos, Anna Bartel, and Emily Ward for their thoughtful feedback on this work, and Melissa Schoenlein, Shannon Sibrel, Autumn Wickman, Yuke Liang, and Marin Murack for their help with data collection. This work was supported in part by the Office of the Vice Chancellor for Research and Graduate Education at UW-Madison and the Wisconsin Alumni Research Foundation. The funding bodies played no role in designing the study, collecting, analyzing, or interpreting the data, or writing the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Focal colors across languages are representative members of color categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Regier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="11178" to="11183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Color categorisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarinia</surname></persName>
		</author>
		<ptr target="https://github.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arashakbarinia/Colourcategorisation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why some colors appear more memorable than others: A model combining categories and particulars in color working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olkkonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Allred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Flombaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">744</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Affective color in visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bartram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1364" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Basic color terms: Their universality and evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>University of California Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semiology of graphics: diagrams, networks, maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bertin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>University of Wisconsin Press</publisher>
			<pubPlace>Madison</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or withinitems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Curtin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="411" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Color use guidelines for mapping and visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Modern Cartography</title>
		<editor>A. M. MacEachren and D. R. F. Taylor</editor>
		<meeting><address><addrLine>Tarrytown</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science Inc</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="123" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The colors of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>D'andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Egan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Ethnologist</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="63" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hearing words changes color perception: Facilitation of color discrimination by verbal and visual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Forder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1105</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Color naming across languages reflects color use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jara-Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ratnasingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Conway</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="10785" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Colorgorical: Creating discriminable and preferable color palettes for information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Gramazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Rittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<title level="m">HRR pseudoisochromatic plates. Richmond Products</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated color selection using semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Holmgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 AAAI Fall Symposium Series</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Choosing effective colours for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Healey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference on Visualization&apos;96</title>
		<meeting>the 7th Conference on Visualization&apos;96</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Color naming models for color selection, image editing and palette design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1007" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The colour currency of nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Humphrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Colour for Architecture</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="95" to="98" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Colorsmessengers of concepts: Visual design mining for learning color semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Keshvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What is the Sapir-Whorf hypothesis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kempton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Anthropologist</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="79" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Resolving the question of color naming universals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Regier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="9085" to="9089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognition memory for hue: Prototypical bias and the role of labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">955</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What color are emergency exit signs? Egress behavior differs from verbal report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kinateder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Ergonomics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="155" to="160" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selecting semantically-resonant colors for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What is the color of chocolate?-extracting color values of semantic expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Colour in Graphics, Imaging, and Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A large-scale multilingual color thesaurus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Color and Imaging Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ConceptNeta practical commonsense reasoning tool-kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT Technology Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="226" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munroe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>xkcd color survey results</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The measurement of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Osgood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Suci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Tannenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>University of Illinois press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A study of colour emotion and colour preference. Part I: Colour emotions for single colours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Woodcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Color Research &amp; Application</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="240" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aesthetic issues in spatial composition: Effects of position and direction on framing single objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Wickens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="421" to="450" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An ecological valence theory of human color preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="8877" to="8882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual aesthetics and human preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sammartino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="77" to="107" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nice: A computational solution to close the gap from colour perception to colour categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Parraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarinia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Color categories: Evidence for the cultural relativity hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="378" to="411" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Natural categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="350" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A color inference framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Colour Studies: Cognition, Language, and Beyond. John Benjamins</title>
		<editor>G. V. P. L. MacDonald, C. P. Biggam</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mapping color to meaning in colormap data visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Gramazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="810" to="819" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Color inference in visual communication: the meaning of colors in recycling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Walmsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Foley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Research: Principles and Implications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<title level="m">Object color preferences. Color Research &amp; Application</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="393" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A linguistic approach to categorical color assignment for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="698" to="707" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An engineering model for color difference as a function of size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Color and Imaging Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling color difference for visualization design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="392" to="401" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Color diagnosticity in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Presnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1140" to="1153" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">scikit-image: image processing in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouillart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">453</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vasa</surname></persName>
		</author>
		<ptr target="https://github.com/hardikvasa/google-images-download" />
		<title level="m">Google images download</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A level-set approach to 3d reconstruction from range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="231" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Russian blues reveal effects of language on color discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winawer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Witthoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boroditsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="7780" to="7785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Color perception: Objects, constancy, and categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Witzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Vision Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="475" to="499" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The meanings of color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rainwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of General Psychology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="99" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
