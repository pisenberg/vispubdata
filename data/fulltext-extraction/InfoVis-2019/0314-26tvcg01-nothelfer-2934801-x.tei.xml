<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Measures of the Benefit of Direct Encoding of Data Deltas for Data Pair Relation Perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Nothelfer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Franconeri</surname></persName>
						</author>
						<title level="a" type="main">Measures of the Benefit of Direct Encoding of Data Deltas for Data Pair Relation Perception</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934801</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information visualization</term>
					<term>marks</term>
					<term>perception</term>
					<term>attention</term>
					<term>visual comparison</term>
					<term>visual search</term>
					<term>aggregation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. Summary of tasks: participants are far more efficient in searching for a target relation (left), discriminating proportions of relations (center), and estimating the average relational magnitude (right) when values are encoded as deltas (bottom row) than when encoded as individual values (top row).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>In their seminal work investigating graphical perception across various data encodings, Cleveland &amp; McGill <ref type="bibr" target="#b2">[3]</ref> stated that "the power of a graph is its ability to enable one to take in the quantitative information, organize it, and see patterns and structure not readily revealed by other means of studying the data." Data visualization design must consider not only which encodings provide an accurate percept of individual values, but also which encodings allow the human visual system to efficiently perceive and explore the relations and patterns among those values. Consider a bar graph depicting student test scores before and after an intervention program ( <ref type="figure" target="#fig_0">Figure  2A</ref>). The absolute values of the test scores are not the critical information -it is the increase or decrease (i.e., relations) in scores that supports the efficacy of the program. <ref type="figure" target="#fig_0">Figure 2B</ref> illustrates the helpful design technique of explicitly encoding differences (deltas) between those data values. Here we quantify this advantage to understand how much value this approach provides, and assess that advantage across 3 different comparison tasks to understand when it is most advantageous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">BACKGROUND AND RELATED WORK</head><p>There is surprisingly little empirical work exploring the most efficient way to visually forage through relations (among data values). Existing work tends to focus on how precisely a provided pair of data values can be compared. <ref type="bibr">Cleveland &amp; McGill [3,</ref><ref type="bibr" target="#b3">4]</ref> evaluated the precision of comparisons (e.g., What is the ratio of value X1 as a percentage of value X2?) between two provided values, or two values highlighted among others, across several encoding types. Those experiments resulted in a ranking of encodings according to precision for that particular task, e.g., position and length encodings afford more precision for ratio extractions, compared to line slopes or figure areas. Mackinlay <ref type="bibr" target="#b17">[18]</ref> hypothesized rankings of visual feature encodings for relations across more diverse conditions (e.g., whether data are quantitative, ordinal, or nominal), though those rankings were not empirically evaluated. While past work focuses on tasks that measure precision between two provided values, we argue that it is at least equally important to test general perception of data patterns (i.e., how we perceive spatial relationships between objects in order to judge magnitude relations, such as increases and decreases between values), as well as use tasks that measure the efficiency with which an observer can forage through large sets of such comparisons, rather than only one comparison at a time <ref type="bibr" target="#b2">[3]</ref>.</p><p>Others have examined how different visual designs impact graphical perception while visually comparing data values <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. For example, one study <ref type="bibr" target="#b20">[21]</ref> examined how different bar chart designs (including stacked bar charts and simple bar charts) impact accuracy in comparing bar heights, including the negative effect of bar spacing. However, none of these studies address how visual designs impact foraging across large sets of data value comparisons. Such work would encourage the design and evaluation of alternative formats for efficient relation perception within data visualization.</p><p>The data visualization design technique of directly encoding deltas ( <ref type="figure" target="#fig_0">Figure 2B</ref>) is intended to improve the efficiency of viewing and understanding the relations between data values. According to Gleicher et al.'s <ref type="bibr" target="#b6">[7]</ref> taxonomy for categorizing visual displays for visual comparison, this design technique is considered an explicit encoding. However, this design technique does not come without costs -it removes the context of the original individual data values, which can be important for a variety of visual decisions. While removing the context of the original data points is an advantage if one only cares about the differences, it is a disadvantage if one needs to connect deltas to the original values (e.g., 'What is the average delta for only values past a certain threshold?'). Gleicher et al. additionally mention that insights tied to the individual data points, beyond the delta relationships, will not be visible and thus less likely to be discovered by the viewer. This design technique also takes up precious real estate, such as in data visualization dashboards -with each new data series added for potential comparison, there is a combinatorial explosion of the number of possible delta encodings among the possible pairings.</p><p>If one is comparing data values to understand their relations, it seems expected that explicitly encoding these differences as deltas should improve relation perception. However, how much better is relation perception when using deltas versus individual data values?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is this improvement large enough given the costs of displaying only the deltas? For which tasks is this design technique most useful?</head><p>A recent study <ref type="bibr" target="#b18">[19]</ref> explored a similar idea, by showing participants displays that included grouped bar charts (several bar pairs), grouped bar charts with delta overlays encoded as dashes, and only deltas encoded as dashes. They found that the latter two chart types outperformed the grouped bar charts, for response time and accuracy, when participants identified which category has the largest absolute change, but were better than grouped bar charts (for response time only) when participants identified the absolute change for a particular category, suggesting the design technique's benefit may vary by task. However, only 2 of the 6 visual comparison tasks required judging aspects of the differences between data value pairs, because the remaining 4 tasks required the participant to focus only one of the two data series (e.g., "Click on the month with the minimum value for 2016"). We build on this work by testing delta value encodings across 3 additional tasks, to further assess when this design technique is more useful. To preview our results, we find a wide range in improvement due to delta encodings (i.e., 25-95% better performance).</p><p>Beyond using 3 new tasks, we also build on Srinivasan et al. <ref type="bibr" target="#b18">[19]</ref> by exploring a different type of relational judgment. While Srinivasan et al. focused on the magnitude of individual deltas (i.e., What's the largest difference? What's the difference size for this data pair?), we add a task requiring judgment of relational direction across multiple data pairs (Exp. 1: Where is the only increase?; Exp. 2: Do more pairs increase or decrease?). This is an important task because identifying the relation direction is the first step to many tasks. For example, if one is trying to estimate the greatest increase magnitude difference, one must first identify which data pairs show an increase in value. We also extend Srinivasan et al.'s tasks by requiring the viewer to summarize across multiple deltas across multiple value pairs (Exp. 3: What's the average delta?). Together, these are important patterns in datasets: an analyst might want to find a month with the biggest sales increase relative to last year (a task similar to that of Srinivasan et al. <ref type="bibr" target="#b18">[19]</ref>), but also might want to identify which months dropped, whether there were months with a drop or increase, or what the average drop is across months, as our work tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Visual Search</head><p>One well-studied measure for processing efficiency is the visual search task, which operationalizes processing efficiency as the added time needed to find a target item among each additionally introduced distractor item (i.e., search rate) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, assuming constant accuracy (which is typically required to be near ceiling). Finding targets that are unique in the typical data visualization encoding dimensions (e.g. red vs. blue color, long vs. short length), can produce search rates as efficient as 0 ms/item. But one of the robustly hardest visual search tasks is for relations. One study asked participants to search for a specific relation (e.g., dash above a plus) among distractors with the opposite relation (e.g., plus above a dash), and found that response times strongly increased with the number of relations within a display <ref type="bibr" target="#b16">[17]</ref>. The size of this increase suggests that perceiving the relation between two items is a serial -or close to serial -process. Similarly, another study found that search for T's among L's (different spatial relationships of the same two line segments) yielded steep search rates <ref type="bibr" target="#b13">[14]</ref>. In fact, across a number of visual search studies, search rates are far higher for spatial configurations of items than for simple items <ref type="bibr" target="#b22">[23]</ref>. This serial processing may be due to the need to isolate each item within a relation individually with the attentional 'spotlight', in order to extract its location, independent of the other item <ref type="bibr" target="#b5">[6]</ref>. Based on these results, one can imagine how poorly this process would scale to large data sets with complex patterns. Importantly, to our knowledge, all of these studies ask viewers to find relations between qualitative visual identities (object X and object Y), and no existing work has tested the efficiency of search for Two ways of encoding the same data set. How many students performed better on the second (red values) test? Is it easier to judge improvements when the data is encoded by bars depicting individual data points (2A) or differences (deltas) between data pairs (2B). (C) Encoding Types. Six encodings were used as the stimuli in Experiments 1, 2, and 3, though absolute values varied. Encodings on the left half graphically represents two data values (individual value encodings), while encodings on the right half represent the delta between those two values (delta value encodings). Each column shows values encoded by one of three visual feature encodings (position, length, or slope). Encodings on the bottom row (increasing relations) depict the opposite relation as that on the top row (decreasing relations).</p><p>quantitative relations (a tall object left of a short object, among other objects that are mirror-reversals of that pattern). These two possible configurations differ in the shape of the envelope that surrounds them both (when you 'squint', small-large pairings create a triangle with a 'forward-slash' diagonal line at the top; large-small pairings create the mirror-reversal of that shape). The visual system excels at using such global shape-perception heuristics to avoid relying on relations between segmented objects, which is a far more computationally intensive process <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>. And a simpler version of this shape contrast -searching for a forward-slash among backslashes -produces perfectly efficient search <ref type="bibr" target="#b25">[26]</ref>.</p><p>Given that visual search is highly efficient for simple visual feature encodings and inefficient for relations, it is likely that visual search for a specific data relation (e.g., Name a student whose test scores did not improve in <ref type="figure" target="#fig_0">Figure 2A</ref>) would be significantly more efficient when the differences among data value pairs are directly encoded with single visual feature encodings ( <ref type="figure" target="#fig_0">Figure 2B</ref>) than when the same visual feature encoding is used to encode each individual data point ( <ref type="figure" target="#fig_0">Figure  2A</ref>). Yet there are also reasons to believe that it may not offer a strong advantage, and to our knowledge this advantage has never been formally empirically quantified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Ensemble Coding</head><p>The task of detecting the presence of a single anomalous relation between value pairs is a relatively simplistic case study for evaluating processing efficiency for relations in real-world displays. More realistic tasks likely require the collection of information from broader sets of objects at once. Another well-studied measure for this type of processing efficiency is an 'ensemble' snapshot of the visual statistics, such as a mean of the locations, orientations, or luminances of a set of objects <ref type="bibr" target="#b8">[9]</ref>. Such summary measures may subserve many common data visualization tasks, such as distributional information about encoded values, pattern and motif recognition, or segmentation of values according to their place in the distribution <ref type="bibr" target="#b19">[20]</ref>, positioning ensemble coding as a critical tool in understanding data.</p><p>We might predict that any same processing bottleneck that we uncover in Experiment 1's search task could also constrain our ability to extract such 'ensemble' statistics. Given that observers can extract the mean position of glyphs in a scatterplot <ref type="bibr" target="#b7">[8]</ref>, mean size of a set of circles <ref type="bibr" target="#b0">[1]</ref>, and the mean orientation of a set of angled line segments <ref type="bibr" target="#b4">[5]</ref>, Experiments 2 &amp; 3 will quantify how strongly relation perception within similar ensemble tasks (e.g., Did students generally improve after the intervention in <ref type="figure" target="#fig_0">Figure 2A</ref>? What is the average improvement?) benefits when the differences among data value pairs are directly encoded with simple visual feature encodings ( <ref type="figure" target="#fig_0">Figure 2B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">The Current Study</head><p>In summary, there is limited research investigating how we perceive relations across a large set of objects of the type typically used in a data visualization display, and no existing research investigating how we perceive relational direction in these displays, or aggregate over multiple deltas across many value pairs. The design technique of explicitly displaying deltas is a promising solution, but requires empirical testing to evaluate how strongly and when it is effective. The overall aim of this study is to investigate how we can best perceive relations from multiple data pairs (i.e., 2 data values -a 'high' value and a 'low' value). We evaluate whether data should be depicted as individual data values -leaving the visual system to extract relations between object pairs -or as single visual feature encodings representing deltas for efficient relation perception by examining the magnitude of improvement when using delta value encodings across multiple comparison tasks. We tested two formats for visually depicting simple magnitude relations (smaller/larger vs. larger/smaller) -either displaying individual data points or directly encoding the difference (delta) between pairs of data points -with three basic visual feature encodings (position, length, and slope). Participants searched for a particular relation in a display (e.g., Find a 'tall bar to the right of a short bar' among 'short bar to the right of a tall bar' pairs) in Experiment 1, discriminated proportions of relations (e.g., Which relation type was more prevalent, short bar to the left of a tall bar, or vice-versa?) in Experiment 2, and determined the average delta among relations within a display (e.g., What is the average bar-height difference across bar pairs?) in Experiment 3.</p><p>These three tasks provide a cross section of common visual comparisons -we include 2 tasks where only relation direction is relevant (Exp. 1 and Exp. 2), rather than absolute values (Exp. 3), and one task where the goal is to single out one particular data pair (Exp. 1) rather than summarize the entire set (Exp. 2 and Exp. 3). All three experiments revealed a powerful improvement in relational processing efficiency for direct encodings of the difference between data pairs. This delta advantage is most pronounced in our task involving locating a single particular relation (Exp. 1), and less pronounced in our tasks that involve ensemble coding (Exp. 2 and 3).</p><p>Contributions: This study contributes design guidelines built on empirical findings, new tasks that simulate realistic data visualization comparison tasks, and perceptual-psychology-inspired experimental methods to quantify performance differences. While, in all cases, we expect direct encoding of deltas to perform better compared to relational processing of absolute values, when we ask 'How do we know that?' (what empirical data actually underlie that gut response? <ref type="bibr" target="#b15">[16]</ref>), the existing literature relies on importantly different stimuli and tasks.</p><p>Findings: Our results demonstrate that in most cases, it is staggeringly inefficient to perceive relations among data value pairs, but highly dependent on the viewer's task: the delta encoding advantage ranged widely from 25% (as in Exp. 3) to 95% (as in Exp. 1). We were also surprised to see that orientation encodings showed only moderate advantages (20-50% improvement) for direct delta encodings, due perhaps to surprisingly poor performance overall.</p><p>Tasks: This study demonstrates 3 distinct visual comparison tasks. These tasks expand upon the ratio comparison task typically used <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and the two ensemble coding tasks (Exp. 2 and 3) provide an approach for assessing how viewers perceive large sets of data points.</p><p>Methods: We provide perceptual psychology methods (psychophysics) that will allow researchers to assess the quantitative effect of a visual design and measure the advantage. Critically, this approach enables researchers to compare data visualization design techniques across different tasks and dependent measures. For example, the delta value encoding advantage in this study clearly depends on the viewer's task, which highlights a need for assessing a new chart type or visual feature encoding across a variety of visual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ENCODING TYPES</head><p>Experiments 1 and 2 tested visual processing efficiency of relations across 6 encodings (see <ref type="figure" target="#fig_0">Figure 2C</ref>), while Experiment 3 used only 4 <ref type="table" target="#tab_1">Table 1</ref>. Stimuli Relation Values. The 6 encodings depicted values from sets A-C in Experiments 1 and 2. Encodings representing individual data values depicted one low value and one high value within a value set, while encodings representing deltas depicted the difference between low and high values. Position and length encodings (left half) were scaled to fit the display monitor prior to being converted to pixels. While these absolute values are arbitrary given that the stimuli did not include axes, they are provided here for reproducibility.</p><p>encodings (position and length encodings). All experiments used the encodings to depict a 'high' value and a 'low' value of a data pair, though absolute values varied.</p><p>Individual Value Encodings: The position (i.e., distance) from baseline (individual dashes), the bar length (i.e., height) (individual bars), and slope (i.e., orientation in degrees, tilting upward from a horizontal angle (0 degrees)) (individual slopes) represent each value in the data pair. With the individual slopes encoding, all data values are scaled to be less than 90 so that lines are always positively sloped between 0 and 90 degrees.</p><p>Delta Value Encodings: The position (i.e., distance) from baseline (delta dash), the bar length (i.e., height) (delta bar), and slope (i.e., orientation in degrees) (delta slope) represent the difference value of the data pair. The delta dash and delta bar are above or below the baseline when representing an increasing relation (positive delta) or decreasing relation (negative delta), respectively. The delta slope tilts upward or downward from a horizontal angle (0 degrees) when representing an increasing relation (positive delta) or decreasing relation (negative delta), respectively. All data values (prior to calculating delta value) are scaled to be less than 90 for the delta slope encoding so that lines are always positively sloped when representing positive delta values and negatively sloped when representing negative delta values.</p><p>All stimuli were intended to simulate typical graph types: dot plots (horizontal dashes), bar charts, and slope graphs, respectively. Some of these encodings differ slightly from real world use cases (e.g., Does the individual slopes encoding truly simulate a slope graph when the lines are arranged horizontally rather than superimposed?), but this difference was intentional, in order to explore a more controlled space of visual feature encodings. For example, comparing the efficiency of individual slopes to individual bars reveals how the visual system extracts relational information from the visual feature encodings of slope versus length. Likewise, comparing the efficiency of individual dashes to individual bars allows exploration of the importance of the 'tops' of the bar values, compared to the area represented underneath.</p><p>Note that we refer to individual bars and delta bars encodings as visual representations in which values are encoded by length (height of the bars) for simplicity, but that length, position (distance from baseline to opposite end of the bar), and area (the filled region of the rectangle) all redundantly encode each value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GENERAL METHODS FOR EXPERIMENTS 1-3</head><p>Thirty-nine university students participated in this study (13 participants per experiment, a typical sample size in perceptual psychology studies) in exchange for course credit or payment. All experiments lasted a total of 30-45 minutes, including the informed consent process, understanding experiment instructions, doing practice trials, and post-experiment debriefing.</p><p>Conducting this study in a lab environment allows a study facilitator to monitor participant attentiveness during the testing session to ensure high data quality (e.g., that participants are not multitasking while completing an experiment that measures split-second response times). Our set of experiments is a long duration within-subject design requiring participants to remain attentive for the full duration of the testing session, making speeded judgments in Experiment 1, and viewing rapidly-disappearing single-glance displays in Experiments 2 and 3.</p><p>Experiment 1-3's trials followed the same general procedure, summarized in <ref type="figure" target="#fig_1">Figure 3</ref> (see Supplemental Material for more details). Because the stimuli in all three experiments do not display axes or data group labels, participants must rely on the shapes of the data encodings alone to provide their responses. That is, in Experiment 1 participants indicate which rectangle contains the opposite relation instead of the data group label associated with that pair of data points, in Experiment 2 participants indicate which relation (increasing or decreasing) they see more of instead of whether one data group is overall doing better or worse than the other data group, and in Experiment 3 participants adjust the height of a rectangle or dash to indicate the average delta value rather than stating this value numerically. Providing axes and data group labels could introduce a new source of error (e.g., participants not locating a data group label quickly enough, mixing up the two data group labels, or the degree to which one can translate visual shapes to numeric values), which would obfuscate the effects of our experimental manipulations alone. The goal of this study is to understand how our perception of data values is impacted by different encoding types, which is the first step before finding the corresponding numeric value or data group label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 1: VISUAL SEARCH FOR RELATIONS</head><p>The goal of this experiment was to evaluate to what degree using simple visual feature encodings representing deltas (as opposed to encodings representing individual data values) can lead to efficient visual processing of the relations between the data values when an observer searches for a single known relation. A realistic example of this is identifying which states experienced an increase in healthcare enrollment.</p><p>We measured visual processing efficiency from the increase in response times as the number of pairs increased in the display for each , followed by a blank screen, and then a test display. The test display remained until participants indicated which rectangle contained the target relation in Experiment 1, followed by an incorrect screen if they responded incorrectly. In Experiments 2 and 3, the test display was presented briefly, followed by a mask screen to prevent an after-image. In the final response screen, participants indicated which relation was more prevalent (Experiment 2) or moved the mouse to show the perceived average delta (Experiment 3). All trials concluded with a blank screen. Relations were located randomly, but always aligned to a bottom baseline.</p><p>encoding. Response times that are slower with larger set sizes indicate more serial processing, and this metric can reflect relative processing efficiency across the six tested encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods and Procedure Stimuli</head><p>The stimuli in this experiment were 4 sets of data pairs (see Sets A, B, C, and Preview in <ref type="table" target="#tab_1">Table 1</ref>; each data pair contains one low value and one high value). Data pair values were depicted by each of the 6 encodings (see Encoding Types) by either converting values to pixels (position and length values; left half of Half of the encoding types depicted individual data points (i.e., displayed both the low and high value within each data pair; left half in <ref type="figure" target="#fig_0">Figure 2C</ref>), while the other half depicted delta values (i.e., the difference between each data pair's values; right half in <ref type="figure" target="#fig_0">Figure 2C</ref>). Three of these data pair sets (Set A, Set B, and Set C) were used in the test displays, while the fourth set (Preview) was only used in the preview displays. There were 2 possible relations for each data pair (bottom row in <ref type="figure" target="#fig_0">Figure 2C</ref> depicts the increasing relation, while the top row depicts the decreasing relation), yielding a total of 48 unique stimuli. All image files were then scaled down by a factor of 0.62 to fit within the display, with each encoding spanning 1.05-2.39 visual degrees wide and 0.27-4.24 visual degrees tall.</p><p>There were two primary screen types: Preview Screens: These screens featured the target relation for the given trial.</p><p>Test Screens: In the test displays, data pairs were arranged within two side-by-side black rectangle outlines arranged horizontally across a white screen <ref type="figure" target="#fig_1">(Figure 3</ref>). The rectangles contained 1, 2, 4, or 5 data pairs each (always the same number of data pairs per rectangle for any given trial), for a total set size of 2, 4, 8 or 10 data pairs. Each display contained one data pair of a target relation (e.g., small bar to the left of a tall bar), and the remaining (distractor) data pairs were of the opposite relation (e.g., tall bar to the left of a small bar). The target relation (assigned to a specific rectangle for each trial) and distractor relations locations were randomized within each rectangle along 5 evenly-spaced possible positions within each rectangle. Each data pair's absolute values were randomly selected from the three possible sets (Sets A, B, and C in <ref type="table" target="#tab_1">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Task: Participants were asked to quickly indicate which rectangle contained the target relation.</p><p>Trial Procedure: The general procedure is described in Section 3 above. The test display remained on the screen until the participant quickly pressed the left or right rectangle key (the 'b' or 'n' key, respectively, each covered with a sticker showing a small rectangle to represent the corresponding screen rectangle -e.g., the left rectangle ('b') key represented the left rectangle on the screen) to indicate their response.</p><p>Design: Factors in the full factorial design included: 2 data depictions (individual value encodings, delta value encodings) x 3 visual feature encodings (position, length, slope) x 4 set sizes (2, 4, 8 or 10 data pairs in the search display) x 2 target relations (increasing ( <ref type="figure" target="#fig_0">Figure 2C</ref>, bottom row), decreasing ( <ref type="figure" target="#fig_0">Figure 2C</ref>, top row)) x 2 target locations (the target relation appears in the left or right rectangle) x 4 repetitions -yielding a total of 384 test trials. While participants were provided with feedback via an Incorrect Screen when responding incorrect, they were given only one attempt per trial.</p><p>Trial Order: Participants first completed 10 practice trials. Trials were randomly ordered within each of 4 test blocks, one block for each repetition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2</head><p>Results and Discussion Median response times to correct trials were calculated after grouping response times across trial repetitions. We ran a factorial, repeated measures ANOVA on the factors data depiction, visual feature encoding, set size, target relation, and target location on median response times. Degrees of freedom were Greenhouse-Geisser corrected for sphericity violations. Significant effects were followed up by two-tailed paired t-tests. Participants maintained a high number of trials (88% or greater) after incorrect and slow trial removal.</p><p>Search rates were also calculated to quantify visual processing efficiency, which indicates how much time is spent during visual search with each addition of another data pair. Highly efficient visual processing should not be affected too much by an increase in set size. Therefore, the greater the search rate, the worse the impact of increasing the set size, and the worse the visual processing efficiency. The search rate calculation allows for predictions of how performance differences should scale for data displays containing fewer or more relations. The median response times to the smallest set size (2) were subtracted from median response times to the largest set size <ref type="bibr" target="#b9">(10)</ref>, and divided by the difference in set sizes <ref type="bibr" target="#b7">(8)</ref> to obtain search rates. Because search rates are much more indicative of visual processing efficiency, we ran a factorial, repeated measures ANOVA on the factors data depiction, visual feature encoding, target relation, and target location on search rates. <ref type="table">Table 2</ref> and <ref type="figure">Figure 4</ref> (left) shows search rate results for Experiment 1.</p><p>Only the results for our primary experimental factors are listed here (see Supplemental Material for other analyses, including analyses on response times, and an analysis confirming no speed/accuracy tradeoff).</p><p>Data Depiction: If the visual depiction of representing individual data values or their deltas impacts visual processing efficiency, then search rates for each encoding should be different depending on the data depiction. Indeed, search rates were impacted by data depiction, F(1,12) = 72.98, p &lt; 0.001, p 2 = 0.86. That is, search rates were significantly slower for individual value encoding trials than for delta value encoding trials. This means that response times get slower with the addition of each data pair when encoded by individual values in a data display, and at a much slower rate than when encoded by delta values.</p><p>Visual Feature Encoding: The particular visual feature encoding for the data pairs, regardless of depiction type (i.e., encoding individual values or delta values) impacted search rates, F(1.20,14.36) = 23.78, p &lt; 0.001, p 2 = 0.66. Search rates were significantly slower for slope encodings than both length encodings and position <ref type="table">Table 2</ref>. Search Rates for Experiment 1. Descriptive statistics, ANOVA results, and follow-up t-test results shown for significant (p &lt; 0.05) comparisons. Search rates represent visual processing efficiency, in that they summarize how response times improve or degrade with an increase in set size; the greater the search rate, the worse the impact of increasing the set size, and therefore the worse the visual processing efficiency. <ref type="figure">Fig. 4</ref>. Summary of Expriments 1-3 Results. Delta value encodings lead to faster search rates (left) when searching for an opposing relation, higher accuracies (center) for distinguishing which relation there is more of, and lower error (right) when perceiving the average delta value in the ensemble task. Error bars indicate within-subject standard error of the mean. encodings. Search rates for length trials and position trials were not statistically different. This result is mostly consistent with the visual feature encoding ranking proposed by <ref type="bibr" target="#b2">[3]</ref>, which outlines that the visual precision for comparing two values is better when those values are encoded by position (dots in a dot plot) and by length (unaligned subsets within a stacked bar graph), than when encoded by slope (angled line segments in a line chart). However, this work also shows that values encoded by position is better than values encoded by length, which we failed to find here.</p><p>These results indicate that while response times are overall moderately worse for length trials than position trials, performance degrades similarly with the addition of data value pairs. On the other hand, performance is the worst for slope encodings.</p><p>Set Size: As expected, response times were slower as set size increased, F(3,33) = 82.80, p &lt; 0.001, p 2 = 0.88.</p><p>In sum, the data encoding greatly impacted the visual processing efficiency of relations between data values: relations between data pairs are much more efficient to visually extract when directly represented as deltas rather than individual data points. Consistent with prior work <ref type="bibr" target="#b2">[3]</ref>, the particular visual feature encodings of the values impact relation processing as well. What is most striking about the present results is that plotting individual data values is staggeringly inefficienttwo to sixteen times worse than delta value encodings (each additional data pair adds 164-266 ms when represented as individual values, but only 8-135 ms when represented as delta values, resulting in a massive 49-95% improvement)despite that these are perhaps the most ubiquitous encodings for data values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT 2: ENSEMBLE CODING OF PROPORTIONS</head><p>Experiment 1 revealed that the choice of data depiction led to enormous differences in processing efficiency for relations between values for the visual search of a known target -participants were significantly faster to search for a particular relation when its delta was encoded rather than its individual values. Experiment 2 explored whether this result generalizes to other tasks that require a viewer to compute visual relational information across a broader set of value pairs. The goal of this experiment was to emulate situations where an observer judges the proportions of relations. A realistic example of this is judging which gender is earning a higher salary the most often across a range of job titles.</p><p>We manipulated difficulty by adjusting the ratio of relations -it should be easier to perceive proportions of relations the more lopsided the ratio (i.e., 1:9), while it should be more difficult the more even the ratio (i.e., 4:6). If visual processing is more efficient when relations are represented by delta value encodings than by individual data value encodings, then performance on trials with individual data value encodings will drop as ratio difficulty increases (e.g., accuracy is worse with a display containing a 4:6 relations ratio than a 1:9 relations ratio), or at least more so than with delta value encodings. It is also possible that accuracy for individual data value encodings may instead (or additionally) be overall worse than accuracy for delta value encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Materials and Procedure Stimuli</head><p>The stimuli in this experiment used the same 6 encodings (see Encoding Types) depicting the same data pair values as Experiment 1. As before, data pair values were either converted to pixels (length and position values; left half of <ref type="table" target="#tab_1">Table 1</ref>) or degrees (orientation values; right half of <ref type="table" target="#tab_1">Table 1</ref>) to create image files. All image files were then scaled down by a factor of 0.5 to fit within display; this resulted in length/position values that were the exact same numerically as the orientation values, though they were the same relative values as in Experiment 1.</p><p>There were 3 primary screen types: Preview Screens: These screens featured the two possible relations for the given trial, one above and one below the screen's center, showing the type of relation participants were to judge in the test display.</p><p>Test Screens: Each test display always contained 10 data pairs, comprised of both possible relations (e.g., 3 positively sloped lines and 7 negatively sloped lines) of a single encoding (see <ref type="figure" target="#fig_1">Figure 3)</ref>. The specific amount of each relation was one of three possible difficulty ratios (1:9 (easiest ratio), 3:7 (medium difficulty), or 4:6 (most difficult ratio). All data pairs were randomized across 10 evenlyspaced positions, all aligned vertically to a bottom baseline at the center of a white screen.</p><p>Response Screens: These screens were identical to preview screens, except "OR" was written in the center of the screen between the two possible answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Task: Participants were asked to quickly indicate which of two relation types was more prevalent in the test display.</p><p>Trial Procedure: The general procedure is described in Section 3 above. The response screen remained until the participant responded with the answer by pressing the 'T' for the top relation, 'G' for the bottom relation (keyboard keys were covered with stickers showing a "/\" and "\/", respectively).</p><p>Design: Factors in the full factorial design included: 2 data depictions (individual value encodings, delta value encodings) x 3 visual feature encodings (position, length, slope) x 3 difficulty ratio (1:9, 3:7, 4:6) x 2 target relations (relation composing the majority of the test display's relations (i.e., the correct answer): increasing, decreasing) x 8 repetitions -yielding a total of 288 test trials.</p><p>Trial Order: Participants first completed 10 practice trials. Trials were randomly ordered within each of 8 test blocks, one block for each repetition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>Accuracies (percent correct) were calculated after grouping correct/incorrect responses across trial repetitions. We ran a factorial, repeated measures ANOVA on the factors data depiction, visual feature encoding, difficulty ratio, and target relation on the accuracies. Significant effects were followed up by two-tailed paired t-tests. <ref type="figure">Figure 4</ref> (center) and <ref type="table">Table 3</ref> show accuracy results for Experiment 2. Only the results for our primary experimental factors are listed here (see Supplemental Material for other analyses). <ref type="table">Table 3</ref>. Accuracies for Experiment 2. Descriptive statistics, ANOVA results, and follow-up t-test results shown for significant (p &lt; 0.05) comparisons for accuracies, except where otherwse noted (i.e., data depiction differences). Data depiction differences were calculated to further expolore how accurcies for each visual feature encoding and each target relation interacted with the two data depictions. The larger the data depiction difference, the greater the advantage offered by delta value encodings.</p><p>Data Depiction: If the visual depiction of representing individual data values or their deltas impacts visual processing efficiency, then participants' accuracies should be higher or lower depending on the type of data depiction. Indeed, accuracies were impacted by data depiction, F <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b11">12)</ref> = 116.82, p &lt; 0.001, p 2 = 0.91, such that accuracies for delta value encoding trials were significantly higher than those for individual value encoding trials.</p><p>Visual Feature Encoding: The particular visual feature encoding of the data value pairs impacted accuracies, F <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b23">24)</ref> = 28.09, p &lt; 0.001, p 2 = 0.70. Accuracies were significantly lower for slope encodings (i.e., individual slopes and delta slope encodings combined) than both length encodings (i.e., individual bars and delta bar encodings combined) and position encodings (i.e., individual dashes and delta dash encodings combined). Participants' accuracies for length and position trials were not statistically different. Considering the latter result was approaching significance, these results are roughly consistent with the visual feature encoding ranking proposed by <ref type="bibr">Cleveland &amp; McGill [3]</ref>, as in Experiment 1.</p><p>Accuracies for each visual feature encoding also interacted with data depiction, F(2,24) = 17.38, p &lt; 0.001, p 2 = 0.59. To investigate this effect, data depiction differences (delta value encoding accuracy individual value encoding accuracy) were calculated for each visual feature encoding for each participant. The larger the data depiction difference, the greater the advantage offered by delta value encodings. The data depiction differences for position trials were significantly larger than for both length trials and slope trials, suggesting that the greatest delta value encoding advantage can be found when using position encodings. Data depiction differences for length trials were marginally significantly larger than for slope trials. This pattern of results is also consistent with the Cleveland &amp; McGill <ref type="bibr" target="#b2">[3]</ref> visual feature encoding ranking. The worse the visual feature encoding, the lower the accuracies are to chance performance (50% accuracy), and the less possible difference there can be between trials of both data depictions for that visual feature encoding.</p><p>In sum, the data encoding greatly impacted the visual processing efficiency of relations between data values in the same manner as in Experiment 1: relations between data pairs are much more efficient to visually extract when directly represented as deltas rather than individual data points. Plotting individual data values is again incredibly inefficient -accuracy improves by 30% when data values are plotted as delta values instead. This, however, is a very conservative calculation, given that purely guessing yields an accuracy of 50% (our results indicate a delta advantage of 143% if examining accuracy beyond 50%). The effectiveness of the particular visual feature encodings of the values impact processing as well, and in such a way that is mostly consistent with prior work <ref type="bibr" target="#b2">[3]</ref>.</p><p>Participants' low performance on slope trials in both Experiments 1 and 2 was surprising -while slope is a lower-ranked visual feature encoding <ref type="bibr" target="#b2">[3]</ref>, it is still a basic visual feature encoding that tends to be easy to parse in visual search (though this depends on the particular slope angles in the display, <ref type="bibr" target="#b24">[25]</ref>) and ensemble coding <ref type="bibr" target="#b4">[5]</ref> tasks. We believe this discrepancy stems from the difference in display arrangements, given that encodings are arranged in a row in our tasks, instead of scattered across the screen as in other studies. Randomlyarranged slopes tend to form an overall texture that can be efficiently segmented <ref type="bibr" target="#b9">[10]</ref>. Slopes arranged in a row across the screen tend to be grouped as /\ and \/ shapes, which could make it harder to view the encodings as individual slopes to accomplish the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT 3: ENSEMBLE CODING OF MAGNITUDES</head><p>Experiment 2 revealed that participants were significantly more accurate when discriminating proportions of value pairs when they were encoded as deltas rather than as individual values. While participants attended to the relation direction, they ignored the degree to which values differed within a value pair. Experiment 3 explored whether this result extends to cases when one is assessing the magnitude of difference between two values, across a set of data pairs. The goal of this experiment was to emulate situations where an observer computes the average difference between two values across multiples instances. A realistic example of this is judging the average difference in employment across multiple population segments before and after an important policy change.</p><p>We measured error while participants indicated the average data value difference (delta) across 6 data pairs. Deltas needed to be extracted from individual value encodings prior to averaging, but were simply averaged if delta value encodings were presented. If visual processing is more efficient when relations are represented by delta value encodings than by individual value encodings, then error should be lower on trials with delta value encodings than with individual value encodings.</p><p>Participants responded with the average delta (e.g., What's the average difference?) instead of average relation ratio (e.g., Here is the average low value -please draw the average high value relative to this average low value) because the latter would only apply to individual value encodings. That is, if participants were to respond with the average relation ratio among 6 individual value encodings, participants could be shown one value (e.g., a bar representing a high value) and be asked to indicate the appropriate other value (e.g., a bar representing a low value) such that both values are proportional to the average low and average high value of the set. There is no equivalent scenario for delta value encoding trials because participants are providing only one value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Materials and Procedure Stimuli</head><p>We suspected this task would require more effort and be more time consuming than Experiments 1 and 2 since there is a much wider range of responses, so we decided to use only the two best performing visual feature encodings (length and position; see Encoding Types). These two visual feature encodings are also much more commonly used than slope in visualization.</p><p>Randomly selecting a set of values from <ref type="table" target="#tab_1">Table 1</ref> on each trial would have resulted in the same correct response for each trial, on average across trials. Therefore, unlike Experiments 1 and 2, data pair values were generated for each experiment to provide a range of responses across trials. As before, data pair values were converted to pixels. Encodings were all drawn (via Matlab) on the screen rather than presented as image files (as in Experiments 1 and 2), since image values were created at the start of each participant's testing session using our stimulus value algorithm (see Supplemental Material).</p><p>The dashes (in position trials) and bars (in length trials) were always each 0.63 visual degrees wide, while the thicknesses of the dashes were always 0.13 visual degrees. Individual value encoding trials contained 0.47 visual degrees of empty space between the bar/dash pairs which represented each data pair. The data pair shown in the preview displays for individual value encoding trials represented the lowest possible value (19; 0.50 visual degrees) and 38 (1.01 visual degrees) as the higher value, while delta value encoding trial depicted a delta of 19, so that both types of preview displays depicted the exact same delta <ref type="bibr" target="#b18">(19)</ref>.</p><p>A set of 6 data pairs were generated for each ensemble trial according to an algorithm described in the Supplemental Material. This process was repeated for each visual feature encoding (length, position), each relation (increasing, decreasing), each of 3 delta distributions (from which each trial's deltas were selected), and repeated 4 times for a sufficient amount of trials. The same values were used for both individual value encoding trials and delta value encodings trials so that performance could be correlated. The same algorithm was used to create control trials, except only one data pair was created for each trial. This process was repeated for each visual feature encoding (length, position), each relation (increasing, decreasing), each of 3 delta distributions, and repeated 2 times for a sufficient amount of trials. The same values were used for both individual value encoding trials and delta value encodings trials so that performance could be correlated.</p><p>There were 3 primary screen types:</p><p>Preview Screens: These displays featured the general type of data pair for the given trial (i.e., the particular combination of data depiction, visual feature encoding, and relation -regardless of whether it is an ensemble or control trial; e.g., a single bar above the baseline during a length delta value encoding trial featuring increasing relations) that the participant was to judge in the subsequent test display. Each data pair present in the preview (as well as the test) displays depicted each data value during individual value encoding trials, but only their deltas during delta value encoding trials.</p><p>Test Screens: During ensemble trials, each test display always contained 6 data pairs comprised of the same relation within each trial (e.g., 6 pairs of a small bar to the left of a taller bar) of a single encoding (see <ref type="figure" target="#fig_1">Figure 3</ref>). All data pairs were centered across 6 evenlyspaced positions (4.20 visual degrees apart), all aligned vertically to a bottom baseline at the center of a white screen. During control trials, each test display contained 1 data pair located in the 2nd data pair position from the left (i.e., 6.29 visual degrees left of the screen's center). The data value pair always appeared in this position during control trials so that its location was consistent, because the ensemble trials containing 6 data value pairs at always the same 6 locations.</p><p>Response Screens: These displays featured a single bar (during length trials) or single dash (during position trials) arranged above or below the baseline depending on whether it was an increasing or decreasing relation trial, respectively. The data value represented by the bar or dash in these screens (i.e., the height of the bar, and the distance between the baseline and the top of the dash) was randomly selected from the range of possible delta values (19 to 126) so that any bias from the bar/dash's starting position would average out across trials. The height of the bar (the top of the bar if it was above the baseline (increasing relation trials) or the bottom of the bar if it was below the baseline (decreasing relation trials)) or position of the dash adjusted as the participant moved the computer mouse up or down, but was restricted to the range of possible delta values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Task: Participants were asked to determine the average delta across the 6 data pairs (i.e., the average difference between low and high values during individual value encoding trials, and simply the average value during delta value encoding trials) during ensemble trials, and to replicate the delta of the single data pair (i.e., replicate the difference between the low and high value during individual value encoding trials, and replicate the value displayed during delta value encoding trials) during control trials, depending on whether the test display contained 6 data pairs or only 1. Participants otherwise did not know prior to the test display whether they were about to view an ensemble or control trial since those trials' preview screens were identical for any given combination of data depiction, visual feature encoding, and relation.</p><p>Trial Procedure: The general procedure is described in Section 3 above. The response screen remained until the participant responded with the answer by moving the mouse up and down to adjust the bar/dash height, and then left-clicked to submit their response.</p><p>Design: Factors in the full factorial design included: 2 tasks (ensemble, control) x 2 data depictions (individual value encoding, delta value encoding) x 2 visual feature encodings (length, position) x 2 display relations (increasing, decreasing) x 3 delta distributions (distributions from which each trial's deltas were selected; means: 45.75, 72.50, 99.25) x 4 or 2 repetitions (ensemble trials and control trials had 4 and 2 repetitions, respectively) -yielding a total of 144 test trials (96 ensemble trials and 48 control trials).</p><p>Trial Order: Participants first completed 16 'slow' practice trials during which the test display remained on screen for twice as long (1000 ms) because this task is challenging and we suspected participants would need some extra time to fully understand the task. This was followed by 16 practice trials during which the test display remained on screen for the experiment trial duration (500 ms). Each block of practice trials contained trials for every combination of task, data depiction, and visual feature encoding; delta difference distribution was randomly selected for each practice trials. Trials were randomly ordered within each block (slow practice, practice, test trials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results and Discussion</head><p>Mean absolute errors (pixels between participant's response and the true mean delta) were calculated after grouping absolute errors across trial repetitions. We ran a factorial, repeated measures ANOVA on the factors task, data depiction, visual feature encoding, display relation, and delta distribution on the mean absolute errors. Significant effects were followed up by two-tailed paired t-tests. <ref type="figure">Figure 4</ref> (right) and <ref type="table">Table 4</ref> show error results for Experiment 3. Only the results for our primary experimental factors are listed here (see Supplemental Material for other analyses). Task: A control task (i.e., replicate the delta from the single value pair displayed) was included to assess any error that may stem from simply replicating a delta with this experiment's response procedure. We expected error to be greater from ensemble trials than control trials because ensemble trials require the additional step of extracting the mean delta from all 6 value pairs (i.e., the process of ensemble coding). Indeed, task impacted errors, F <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b11">12)</ref> = 64.59, p &lt; 0.001, p 2 = 0.84, such that errors from ensemble trials were greater than those from control trials.</p><p>Identical deltas were displayed between individual value encoding and delta value encoding trials for each type of visual feature encoding. Participants' errors to individual value encodings and delta value encodings (for each visual feature encoding) may correlate to reveal whether there is a common mechanism between the two types of visual processes: a high correlation resulting from performance fluctuating as one replicates (control trials) and averages (ensemble trials) delta(s) within the test display suggests a common underlying process to perceiving deltas from pairs of values and perceiving directly drawn single deltas, while a low correlation suggests error unique to each data depiction type. Errors were correlated between individual value encoding and delta value encoding trials for each visual feature encoding within each task and averaged across all participants. Correlation values were overall quite low, though a little higher for control trials (length trials: M = 0.32, SE = 0.36; position trials: M = 0.20, SE = 0.34) than ensemble trials (length trials: M = 0.18, SE = 0.30; position trials: M = 0.08, SE = 0.26), suggesting either little commonality between the underlying mechanisms, or significant additional error resulting from the additional step of extracting a delta from a value pair prior to replicating (control trials) or averaging (ensemble trials) the delta(s).</p><p>Data Depiction: Critically, if the visual depiction of representing individual data values or their deltas impacts visual processing efficiency, then participants' errors should be higher or lower depending on the type of data depiction. Indeed, errors were impacted by data depiction, F(1,12) = 46.73, p &lt; 0.001, p 2 = 0.80, such that errors for delta value encoding trials were significantly lower than those for individual value encoding trials.</p><p>Task x Data Depiction: Surprisingly, the pattern of errors for each data depiction (lower errors for delta value encodings than for individual value encodings) did not vary by task -there was no interaction between task and data depiction, F(1,12) = 0.73, p = 0.41, p 2 = 0.057. Visual Feature Encoding: The particular visual feature encoding of the data value pairs impacted errors, F(1,12) = 7.75, p = 0.017, p 2 <ref type="table">Table 4</ref>. Errors for Experiment 3. Descriptive statistics, ANOVA results, and follow-up t-test results shown for significant (p &lt; 0.05) comparisons for absolute errors (pixels between participant's response and the true mean delta). = 0.39. Errors were significantly lower for length encodings than for position encodings. While this result runs contrary to Experiment 1 and Experiment 2's visual feature encoding ranking, the encodings' means differ only minimally.</p><p>Error Direction: Participants tended to systematically underestimate their responses in more than half the conditions, notably in almost all length conditions and in almost all delta value encoding conditions (see Supplemental Material for analyses).</p><p>In sum, the data encoding significantly impacted the precision of the visual processing of relations between data values in the same manner as in Experiments 1 and 2: relations between data pairs are visually extracted and averaged much more precisely when directly represented as deltas rather than individual data points. Plotting individual data values is inefficient again -error decreases by 25% when data values are plotted as delta values instead during ensemble trials. However, directly-represented deltas tended to bias responses to underestimate, rather than under-and overestimate.</p><p>Given that this experiment's task was particularly complex, we were surprised by the degree to which delta value encodings improved accuracy, and were expecting a massive result similar to Experiment 1. We suspect this is likely due to a perceptual heuristic which participants employed, and the fact that all test screens always contained value pairs with the same relation direction (i.e., all increasing relations). Since participants did not need to selectively 'filter' for a particular relation, it is possible that (in the individual value encoding trials) they averaged only the 'thin', top portion of each bar pair in the length trials, or averaged only the distance between the higher dash and its neighbouring lower dash. Neither of these approaches would work alone if the task was to estimate the average delta for only one of two types of relations in a display. Indeed, a more likely scenario would include value pairs of the opposite relation which need to be filtered out. Can we accurately filter out irrelevant relations to average the specific relations of interest? Given that opposing relations are significantly harder to distinguish when encoded by individual value encodings rather than delta value encodings (i.e., Experiment 2's results), it is likely that performance in this type of scenario would be far worse than it already is when values are represented individually.</p><p>One limitation is that participants responded by adjusting the height of a single bar or dash, which is most similar to the delta encoding itself. It is possible that method could bias responses in favour of delta value encodings. If so, this would be important to examine further given that it would provide a task-specific instance in which delta value encodings provide less (or even no) benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS 1-3: RESULTS SUMMARY</head><p>The primary results are summarized in <ref type="figure">Figure 4</ref>. The key takeaways across the three experiments are as follows:</p><p>A) Delta value encodings consistently yield better relation perception than individual encodings. Participants searched faster for the opposite relation (Exp. 1), were more accurate in perceiving which relation there was more of (Exp. 2), and had lower error rates when perceiving the average delta (Exp. 3) when deltas were explicitly encoded, than when individual data points were encoded. While <ref type="bibr" target="#b18">[19]</ref> showed delta encoding response time benefits, the present data add that responses times continue to slow down with the addition of each individually encoded data pair. Experiment 2 uniquely shows the benefit of delta encodings in perceiving proportions of relations. Lastly, we show that delta encodings improve perception of the average delta, adding to the finding of Srinivasan et al. <ref type="bibr" target="#b18">[19]</ref> that accuracy for measuring a specific, single delta was comparable across chart types. B) Further, delta value encodings were far more efficient than individual value encodings for processing, but highly depended on the task-delta encodings accelerated search rates by 49-95% in Exp. 1, improved accuracy by 30% in Exp. 2, and lowered error rates by 25% in Exp. 3. While it may be tempting to generalize a guideline across these tasks, testing delta encodings further across a larger swath of tasks is necessary in order to conclude which types of visual decisions benefit most from delta encodings, beyond our three specific tasks. The fact that we find great variability in the delta advantage points to the need for this exact type of additional testing. C) Lastly, delta value encodings tend to bias people to underestimate the average delta (Exp. 3). While this pattern is intriguing, its perceptual root is unclear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">GUIDELINES</head><p>Show deltas, but only when necessary: Across our three tasks, delta encodings consistently yielded the best performance over individually depicted data values: a 25-95% improvement depending on the task. Unfortunately, depicting deltas requires more space in webpages, slides, or dashboards, and they typically cannot simply replace the visualization of the base values, which provide the context for those differences, so they should be used judiciously. If perceiving data value pair differences is central to the task (e.g., identifying whether an intervention improved the majority of test scores in <ref type="figure">Figure  1A</ref>, unlike identifying which student had the single highest test score), encoding differences should offer far better performance, even more so when the viewer is identifying data pairs with a particular relation (like in Experiment 1). The difference overlay technique tested by <ref type="bibr" target="#b18">[19]</ref> presents a potentially powerful way to show both absolute values as well as deltas between pairs of values, in a way that adds visual processing power, with little evidence of drawbacks.</p><p>Use position and length encoding: Consistent with prior work <ref type="bibr" target="#b2">[3]</ref>, position (e.g., dot plots) and length (e.g. bar charts) encodings led to far better performance compared to slope encodings, at least for the present displays and tasks.</p><p>Require less relational extraction: Extracting relations (i.e., perceiving both the difference between a data value pair and the direction of that difference -whether it is an increase or a decrease) is a slow process, and continues to slow down with the addition of each data value pair (Exp. 1 -response times increase as set size increases). Therefore, when it is not possible to explicitly show delta (e.g., for data triplets instead of pairs), one might consider decreasing the resolution of included categories, to decrease the amount of relational processing demanded of the viewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">LIMITATIONS AND FUTURE WORK</head><p>Loss of context: As already stated, naturally much context is lost with the loss of the original individual data values by displaying only deltas. In light of this, our results highlight the need for visualizations that both display original data values and showcase relational differences. In fact, grouped bar charts with deltas values explicitly overlaid were preferred by viewers <ref type="bibr" target="#b18">[19]</ref>. Further, this approach is highly valuable in situations of data exploration, in which the viewer does not know ahead of time which aspects are important or relevant to compare.</p><p>How do these results scale? The present experiments tested relations between two data values as a starting point. How do these results scale when the number of data points, and thus relations, increases (e.g., a bar chart containing multiple bars per data group)? Relatedly, our displays always contained multiple relations, but our participants never made comparisons between any relations (i.e., relations between relations).</p><p>Number of objects, or density? Density is confounded with increased set size in Experiments 1, though equally so across encodings. Because both can sometimes lead to response time increases <ref type="bibr" target="#b23">[24]</ref>, future work should measure their relative contributions to search inefficiency, which may lead to new concrete guidelines (e.g., more data may not slow viewers down as much, as long as it is well-spaced).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>(A-B)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Experiments 1, 2, and 3's designs. Stimuli shown here are not drawn to scale. Participants viewed a preview screen containing a target relation(s) to search for (Experiment 1) or judge (Experiments 2 and 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2019; accepted 1 Aug. 2019. Date of publication 16 Aug. 2019; date of current version 20 Oct. 2019. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2019.2934801</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 )</head><label>1</label><figDesc>or degrees (slope values; right half of Table 1) to create image files (see Supplemental Material for more details).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeing sets: Representation by statistical properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ariely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="162" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<title level="m">The Psychophysics Toolbox. Spatial Vision</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="433" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graphical perception: Theory, experimentation, and application to the development of graphical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">387</biblScope>
			<biblScope unit="page" from="531" to="554" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graphical perception and graphical methods for analyzing scientific data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="issue">4716</biblScope>
			<biblScope unit="page" from="828" to="833" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The computation of orientation statistics from visual texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Dakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Watt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="3181" to="3192" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flexible visual processing of spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Scimeca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Helseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual comparison for information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jusufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perception of average value in multiclass scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nothelfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2316" to="2325" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ensemble perception: Summarizing the scene and broadening the limits of visual processing. From perception to consciousness: Searching with Anne Treisman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Whitney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-speed visual estimation using preattentive processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Enns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction (TOCHI)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="135" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention and visual memory in visualization and computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Enns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1170" to="1188" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crowdsourcing graphical perception: using mechanical turk to assess visualization design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCHI conference on human factors in computing systems</title>
		<meeting>the ACM SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sizing the horizon: the effects of chart size and layering on the graphical perception of time series visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCHI conference on human factors in computing systems</title>
		<meeting>the ACM SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1303" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention capacity and task difficulty in visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="101" to="111" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Complementary solutions to the binding problem in vision: Implications for shape perception and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hummel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual cognition</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-5</biblScope>
			<biblScope unit="page" from="489" to="517" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An empire built on sand: reexamining what we think we know about visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization</title>
		<meeting>the ACM Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="162" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial attention and the apprehension of spatial relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1015</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automating the design of graphical presentations of relational information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (Tog)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="141" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What&apos;s the Difference?: Evaluating Variations of Multi-Series Bar Charts for Visual Comparison Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the ACM CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">304</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Four types of ensemble coding in data visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="11" to="11" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Four experiments on the perception of bar charts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2152" to="2160" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature analysis in early vision: Evidence from search asymmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gormican</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What can 1 million trials tell us about visual search?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="39" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guided search: an alternative to the feature integration model for visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Cave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human perception and performance</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">419</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The role of categorization in visual search for orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Friedman-Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What attributes guide the deployment of visual attention and how do they do it?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="495" to="501" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
