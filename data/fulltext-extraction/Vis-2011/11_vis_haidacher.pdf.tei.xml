<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T14:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Volume Analysis Using Multimodal Surface Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Martin</forename>
								<surname>Haidacher</surname>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Stefan</forename>
								<surname>Bruckner</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Volume Analysis Using Multimodal Surface Similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>and M. Eduard GröllerGr¨Gröller, Member, IEEE Computer Society isovalue k (low energy) 256 256 0 0 isovalue l (high energy)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Multimodal data</term>
					<term>volume visualization</term>
					<term>surface similarity</term>
				</keywords>
			</textClass>
			<abstract>
				<p>1 2 3 4 5 6 7 1 2 3 4 5 6 7 Fig. 1. Iterative control point specification for similarity-based classification of a dual energy CT (DECT) angiography data set. The individual steps are numbered from 1 to 7. Abstract—The combination of volume data acquired by multiple modalities has been recognized as an important but challenging task. Modalities often differ in the structures they can delineate and their joint information can be used to extend the classification space. However, they frequently exhibit differing types of artifacts which makes the process of exploiting the additional information non-trivial. In this paper, we present a framework based on an information-theoretic measure of isosurface similarity between different modalities to overcome these problems. The resulting similarity space provides a concise overview of the differences between the two modalities, and also serves as the basis for an improved selection of features. Multimodal classification is expressed in terms of similarities and dissimilarities between the isosurfaces of individual modalities, instead of data value combinations. We demonstrate that our approach can be used to robustly extract features in applications such as dual energy computed tomography of parts in industrial manufacturing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p> Imaging modalities have different advantages and disadvantages typically related to the physical principles they use to scan a specimen. They may suffer from different kinds of artifacts, can be differently affected by noise, may be able to distinguish different materials or tissues , and can have differences with respect to contrast and resolution. In order to gain insight into the phenomenon under investigation, it is essential to integrate this information effectively. The work presented in this paper focuses on the analysis and fusion of two registered volume data sets of the same specimen. While the data generated by each modality may be visualized separately, it is difficult to mentally integrate multiple three-dimensional sources, particularly if spatial relationships are important. Thus, the effective visual fusion of multiple volume data sets has long been an active area of research. As discussed @BULLET Martin Haidacher, Stefan Bruckner, and M. Eduard Gröller are with the by Cai and Sakas <ref type="bibr" coords="1,360.61,517.25,9.52,8.02" target="#b3">[4]</ref>, this combination can occur at different stages. At the extreme ends of the spectrum, the two data sets are treated separately and are only blended at the image level, or, conversely the data values at each position are combined at the very beginning of the pipeline to form a single merged volume. Most commonly visual fusion is performed during the rendering phase which provides spatial integration and allows for a flexible mapping of data attributes to optical properties <ref type="bibr" coords="1,358.86,586.99,13.74,8.02" target="#b23">[24]</ref>. While straightforward blending can be an effective technique in 2D slice views, it has many disadvantages in 3D visualization. In particular , the projection of multiple volumetric data sets onto a single 2D image can quickly lead to visual clutter. Hence, it is important to provide the user additional guidance about the spatial similarities and differences between the individual modalities to enable goal-directed selection of features. Approaches which attempt to identify correspondences based only on the frequency of data values, however, suffer from the fact that data value ranges of corresponding structures of interest may differ significantly. In order to address this challenge, we propose multimodal surface similarity as a measure for identifying similarities and dissimilarities between two volumetric scalar fields. Instead of collecting statistics about the frequency of data values, we quantify spatial similarities between isosurfaces across two ties, i.e., how much does knowledge about one surface tell us about the others. By generating a multimodal similarity map, which encodes the similarity between all combinations of isosurfaces from two modalities , we can provide a concise overview of the differences between two scalar fields. This information can then be used to guide the identification of structures of interest. Based on this concept, we present a novel method for feature classification in similarity space which enables the user to easily take advantage of the complementary information provided by two modalities. The remainder of the paper is structured as follows. In Section 2 we review related work on the visualization of multimodal volume data and other approaches connected to our work. Section 3 provides background information on the types of data we focus on. In Section 4, the general concept of multimodal surface similarity is introduced. In Section 5, we show how multimodal surface similarity can be used in the visualization process. Results obtained with our approach are presented in Section 6. Section 7 discusses implementation details. The implications of our approach as well as its limitations are discussed in Section 8. Finally, the paper is concluded in Section 9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p> As mentioned in the introduction, two volumes can be fused in different stages of the visualization pipeline <ref type="bibr" coords="2,164.51,556.57,14.19,8.02" target="#b12">[13,</ref><ref type="bibr" coords="2,181.37,556.57,10.64,8.02" target="#b13"> 14]</ref>. The fusion in image space is covered by the field of image processing <ref type="bibr" coords="2,211.13,566.54,13.74,8.02" target="#b35">[37]</ref> . The drawback of the fusion in image space is the loss of 3D information. For the fusion in volume space, the spatial information of the data sets can be used to improve the fusion quality. The first methods for volume fusion were based on extracted surfaces. Levin et al. <ref type="bibr" coords="2,239.03,606.39,14.94,8.02" target="#b26">[27] </ref>generated a surface model from an MRI scan and mapped PET-derived measurements onto this surface. Evans et al. <ref type="bibr" coords="2,189.56,626.31,14.94,8.02" target="#b9">[10] </ref> generated an integrated volume visualization from the combination of MRI and PET. Noz et al. <ref type="bibr" coords="2,59.84,646.24,14.94,8.02" target="#b28">[29] </ref>introduced a framework for 3D registration and fusion of CT/MRI and SPECT data sets based on a polynomial warping technique . These works mainly focused on the combination of anatomical and functional modalities. A more general approach for the fusion of modalities was introduced by Zuiderveld and Viergever <ref type="bibr" coords="2,224.76,686.09,13.74,8.02" target="#b40">[42]</ref>. For this method an additional segmentation of the volumes is necessary to decide which one to show at a given sample point. Heinzl et al. <ref type="bibr" coords="2,257.93,706.01,14.94,8.02" target="#b15">[16] </ref>introduced a processing pipeline for surface extraction in dual energy CT. Alternatively, fusion can be performed without an intermediate feature extraction step. A straightforward method is fusion by linear intermixing of the data values. Such an approach is used for volumetric CSG construction where different volumetric parts are fused into a single object <ref type="bibr" coords="2,356.22,83.27,14.19,8.02" target="#b34">[36,</ref><ref type="bibr" coords="2,372.88,83.27,11.21,8.02" target="#b10"> 11,</ref><ref type="bibr" coords="2,386.54,83.27,6.47,8.02" target="#b7"> 8]</ref>. Hong et al. <ref type="bibr" coords="2,442.18,83.27,14.94,8.02" target="#b16">[17] </ref>describes how fusion techniques in volume space can be efficiently implemented using the graphics hardware. Eusemann et al. <ref type="bibr" coords="2,414.80,103.20,10.45,8.02" target="#b8">[9] </ref> have shown that this intermixing can be improved for dual energy CT by adapting the intermixing ratio to different tissues. A case study on visualization of multivariate data where multiple values are present at each sample point was presented by Kniss et al. <ref type="bibr" coords="2,363.95,143.05,13.74,8.02" target="#b23">[24]</ref>. In this work the idea of multi-dimensional transfer functions for assigning optical properties to a combination of values was used. Akiba and Ma <ref type="bibr" coords="2,405.21,162.97,10.45,8.02" target="#b0">[1] </ref>used parallel coordinates for the visualization of time-varying multivariate volume data. Multimodal visualization of medical data sets by using multi-dimensional transfer functions was discussed by Kniss et al. <ref type="bibr" coords="2,442.85,192.86,13.74,8.02" target="#b24">[25]</ref>. Kim et al. <ref type="bibr" coords="2,503.42,192.86,14.94,8.02" target="#b21">[22] </ref>presented a technique which simplifies transfer function design by letting the user define a separate transfer function for each modality. Their combination defines a two-dimensional transfer function. Haidacher et al. <ref type="bibr" coords="2,305.31,232.71,14.94,8.02" target="#b14">[15] </ref> defined a data fusion and transfer function space for multimodal visualization based on the information content of the individual modalities which aims to reduce the loss of information. In contrast to our method, this approach is only based on the global frequency distribution of values and not on structural similarities between the individual modalities. In our approach, we use information theory <ref type="bibr" coords="2,450.39,292.81,14.94,8.02" target="#b30">[31] </ref> to measure similarities between the different modalities, but it has been applied to many aspects of visualization <ref type="bibr" coords="2,373.38,312.74,13.74,8.02" target="#b33">[35]</ref>. In flow visualization, for instance, Xu et al. <ref type="bibr" coords="2,306.36,322.70,14.94,8.02" target="#b37">[39] </ref>used information theory to select meaningful streamlines. Feixas et al. <ref type="bibr" coords="2,331.07,332.66,14.94,8.02" target="#b11">[12] </ref> presented an information-theoretic approach for optimal viewpoint selection. Chen and Jänicke <ref type="bibr" coords="2,451.51,342.63,10.45,8.02" target="#b6">[7] </ref>discussed a general information-theoretic framework for scientific visualization. For many applications, such as industrial CT <ref type="bibr" coords="2,461.97,362.88,13.74,8.02" target="#b15">[16]</ref>, surfaces are of particular interest. Surfaces can be used to represent the interfaces between different materials. In order to extract a stable isosurface, the selection of the isovalue is crucial. Khoury and Wenger <ref type="bibr" coords="2,492.10,392.76,14.94,8.02" target="#b20">[21] </ref>use the fractal dimension to measure how stable an isovalue is. The lower the dimension, the less noisy the corresponding isosurface is. The contour tree <ref type="bibr" coords="2,302.07,422.65,10.45,8.02" target="#b5">[6] </ref>is used to topologically analyze volume data. It is able to encode the nesting relationships of isosurfaces. Takahashi et al. <ref type="bibr" coords="2,520.55,432.61,14.94,8.02" target="#b31">[32] </ref>employed a volume skeleton tree to identify isosurface embeddings in order to provide additional structural information. Kindlmann and Durkin <ref type="bibr" coords="2,312.66,462.50,14.94,8.02" target="#b22">[23] </ref>introduced a transfer function space in which the gradient magnitude is used as additional classification dimension. Interfaces between materials show up as arches in this transfer function space. In LH histograms, introduced byŠeredabyˇbyŠereda et al. <ref type="bibr" coords="2,458.73,493.43,13.74,8.02">[34]</ref>, the highest and lowest value along a local streamline in the gradient field are used for the classification. Sample points at interfaces between materials form clusters in this space, which represent stable surfaces. Bruckner and Möller <ref type="bibr" coords="2,374.90,533.61,10.45,8.02" target="#b2">[3] </ref> introduced similarity maps which represent the similarity of isosurfaces for different isovalues. For the measurement of the similarity mutual information is used. In a similarity map clusters with high mutual information can be detected. In our approach we extend the idea of the similarity maps to multimodal data. The resulting multimodal similarity maps are used for analysis, fusion, and classification of multimodal data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTIMODAL VOLUME DATA</head><p>There are different reasons for seeking to combine the information from multiple modalities. In medicine, for instance, it is frequently desired to simultaneously depict anatomical and functional data. Functional data contains information about physiological activities, such as metabolism or blood flow, within a certain tissue or organ. Anatomical imaging modalities, on the other hand, present structural information and typically provide higher resolution. In other fields, such as nondestructive testing, multiple industrial CT scans with different parameters are used for scanning an object. These parameters can affect the contrast and amount of artifacts in different regions. Thus, the goal in this case is to combine the advantages of different scans in order to obtain a better visualization of the object.  For the further description of our approach we will differentiate between two types of multimodal data which are depicted in this section. We will introduce synthetically generated data sets which represent the two different types of multimodal data sets. The data sets are all 3D data sets, where the slices are duplicates of the same image with a size of 512 × 512 pixels. To investigate the influence of noise on our method, we added Gaussian white noise with a standard deviation σ = 1.5% (SNR = 17.6) to one modality. In the subsequent sections the synthetic data sets are used to highlight the properties of multimodal similarity maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>distance </head><formula>D l D (x) k D (x) l </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Data </head><p> Multimodal data is often used to eliminate the drawbacks of a certain imaging modality. This is necessary when one modality contains undesirable noise or other artifacts in certain regions. In this case a second modality is used to compensate for these artifacts. In this paper we will refer to this type of data as supplementary data. Basically both modalities depict the same structures, but disadvantages of one modality may be compensated by the other and vice versa. An example for supplementary data is dual energy CT. It is used in medicine and industrial applications. The most common artifacts in CT scans in general are noise-induced streaks, beam hardening, partial volume effects, aliasing, and scattered radiation <ref type="bibr" coords="3,179.05,414.18,9.71,8.02" target="#b1">[2,</ref><ref type="bibr" coords="3,191.66,414.18,10.64,8.02" target="#b17"> 18]</ref>. Due to the fact that different energy levels have different attenuation characteristics, some of these artifacts appear prominently only in one energy level. Hence it is desired to reduce artifacts by the fusion of CT data sets of different energy levels. We generated two synthetic data sets in order to simulate multimodal data with supplementary characteristics. In <ref type="figure" coords="3,214.68,474.28,30.40,8.02" target="#fig_1">Figure 2</ref>these two data sets and their histograms are depicted. Due to scaling reasons the frequency of background points with a value of zero is omitted in the histograms. On the right side of <ref type="figure" coords="3,153.04,504.17,29.66,8.02" target="#fig_1">Figure 2</ref>, data set 2 with additional Gaussian white noise is shown. Both data sets contain four squares with gradually changing data values from left to right. Their value ranges are different in both data sets to simulate the effects of varying attenuation characteristics in modalities such as dual energy CT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complementary Data </head><p>In some cases, it is also advantageous to combine the information of modalities with more distinct characteristics. In such a scenario, a significant amount of information differs or is not represented in one of the modalities. We will refer to this type of multimodal data as complementary data. Complementary data is commonly encountered in medicine. Modalities such as CT and MRI measure different physical characteristics of the human body, and thus there are substantial differences between two such scans of the same patient. An even more pronounced example is the combination of anatomical and functional modalities, such as CT and PET. There is only a rough correspondence between the two modalities, as CT images contain no functional information at all. We will use the synthetic data sets illustrated in <ref type="figure" coords="3,209.02,696.57,29.60,8.02" target="#fig_2">Figure 3</ref>to represent complementary data. Data set 1 contains four squares while data set 2 contains two squares and a circle. The missing square and the circle in data set 2 represent the complementary nature of the data. With the circle we want to show how differences in shape are depicted in the multimodal similarity map. The missing square is used to show the effect if one object is completely omitted from one modality. In the next section these synthetic data sets are used to explain multimodal surface similarity measurement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTIMODAL SURFACE SIMILARITY</head><p>Isosurfaces are important features in volumetric data. An isosurface of a volumetric scalar field f : R 3 → R is the locus of all points in the scalar field at which f attains an isovalue k: </p><formula>L k = x ∈ R 3 : f (x) = k (1) </formula><p>In many cases, different material types correspond to different value ranges in the data set. For example, in medical CT data sets there are typically well-defined intensity ranges associated with soft tissue, fat, and bone. The important characteristic parameter is the intensity isovalue which defines an isosurface representing the boundary of a particular region. Isosurfaces, however, also exhibit a significant amount of redundancy and small variations caused by noise and partial volume effects will result in many similar isosurfaces. Histograms and other isosurface statistics <ref type="bibr" coords="3,367.10,397.61,9.71,8.02" target="#b4">[5,</ref><ref type="bibr" coords="3,379.73,397.61,11.95,8.02" target="#b29"> 30] </ref> can be used to obtain a better characterization of a data set by depicting distributions of isosurface properties over the range of data values. They are limited, however, in that they treat each isosurface in isolation and therefore cannot capture the spatial relationships between multiple structures. As an alternative, the measure of isosurface similarity was introduced by Bruckner and Möller <ref type="bibr" coords="3,407.46,457.43,10.45,8.02" target="#b2">[3] </ref>to quantify how much information two isosurfaces have in common. They used a matrix of isosurface similarity for all combinations of isovalues within a single data set as the basis for identifying relevant isovalues. We will refer to this method as self similarity maps since the measurement of the similarity is between isosurfaces of a single data set. For multimodal data, in particular, it is difficult to investigate differences and similarities based on isosurface statistics as the order and range of corresponding data values may vary significantly. Thus, in order to characterize the correspondences between multiple modalities, we extend the idea to multimodal similarity maps which quantify the similarity between all combinations of isosurfaces from two scalar fields. In the following, we first briefly revisit isosurface self similarity maps as presented by Bruckner and Möller <ref type="bibr" coords="3,453.19,587.00,10.45,8.02" target="#b2">[3] </ref>and then describe our extension to multimodal data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Self Similarity Maps</head><p> A common measure for similarity is mutual information. Mutual information is a basic concept from information theory, measuring the statistical dependence between two random variables or the amount of information that one variable contains about the other. It is a particularly attractive measure because no assumptions are made regarding the nature of this dependence and because of its robustness against perturbations <ref type="bibr" coords="3,344.71,687.50,13.74,8.02" target="#b36">[38]</ref>. Therefore, mutual information has been applied in many areas including shape registration <ref type="bibr" coords="3,455.62,697.47,13.74,8.02" target="#b18">[19]</ref> , multi-modality fu- sion <ref type="bibr" coords="3,312.30,707.43,13.74,8.02" target="#b14">[15]</ref>, and viewpoint selection <ref type="bibr" coords="3,423.16,707.43,13.74,8.02" target="#b32">[33]</ref>. The mutual information of two discrete random variables X and Y can be defined as <ref type="bibr" coords="3,499.28,717.39,13.94,8.02" target="#b38">[40]</ref>: </p><formula>I(X,Y ) = H(X) + H(Y ) − H(X,Y ) (2) </formula><p>1971 where H(X,Y ) is the joint entropy and H(X) and H(Y ) are the marginal entropies of random variables X and Y . Since the mutual information is limited by the average marginal entropies, it can be normalized to a value range in <ref type="bibr" coords="4,121.38,429.27,9.46,7.96">[0,</ref><ref type="bibr" coords="4,131.83,429.41,6.97,8.02" target="#b0"> 1] </ref>by <ref type="bibr" coords="4,152.24,429.41,13.94,8.02" target="#b25">[26]</ref>: </p><formula>ˆ I(X,Y ) = 2I(X,Y ) H(X) + H(Y ) (3) </formula><p>As a measure of isosurface similarity, Bruckner and Möller <ref type="bibr" coords="4,245.42,475.41,10.45,8.02" target="#b2">[3] </ref>proposed the normalized mutual information of the respective isosurface distance fields. For a given isovalue k and an isosurface L k the distance field D k can be defined as follows <ref type="bibr" coords="4,145.77,505.30,13.94,8.02" target="#b19">[20]</ref>: </p><formula>D k (x) = min ∀y∈L k d(x, y) (4) </formula><p>where d is a distance measure between the points x and y. To measure the similarity between two isosurfaces L i and L j , the distance fields for both isosurfaces D i (x) and D j (x) are used as discrete random variables X and Y for the calculation of the mutual information based on Equation 3. This leads to a single quantity between 0 and 1 which expresses the similarity between isosurface L i and isosurface L j . Higher values mean that the isosurfaces are considered to be more similar. If we consider N different isovalues V = {k 1 , ..., k N } then the self similarity map can be defined as an N × N matrix SSM(i, j). Each element (i, j) of the matrix represents the normalized mutual information for a combination of isovalues i and j. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multimodal Similarity Maps</head><p>In this paper, we extend the concept of isosurface similarity maps to multimodal data. Instead of investigating the similarity of isosurfaces in a single data set, we explore the similarity of two different data sets representing the same object. The isosurfaces of both modalities are represented by: </p><formula>˙ L k = x ∈ R 3 : ˙ f (x) = k ¨ L l = x ∈ R 3 : ¨ f (x) = l (5) </formula><p>where k and l are the two isovalues, and ˙ f and¨fand¨ and¨f are the scalar-valued functions representing the two modalities. Based on the two isosurfaces , two distance fields ˙ D k and¨Dand¨ and¨D l can be generated: </p><formula>˙ D k (x) = min ∀y∈ ˙ L k d(x, y) ¨ D l (x) = min ∀y∈¨L∀y∈¨ ∀y∈¨L l d(x, y) (6) </formula><p>Figure 4 illustrates how the mutual information for a combination of isovalues l and k is calculated. The first step is the generation of the distance fields ˙ D k and¨Dand¨ and¨D l for the isosurfaces ˙ L k and¨Land¨ and¨L l . In the next step the distances ˙ D k (x) and¨Dand¨ and¨D l (x) for each point x in the volume space are used to generate a joint distance histogram. The joint distance histogram represents the joint probability for a point x to have the distance ˙ D k (x) to isosurface ˙ L k and¨Dand¨ and¨D l (x) to isosurface ˙ L l . In <ref type="figure" coords="4,471.34,178.07,29.73,8.02" target="#fig_3">Figure 4</ref> an example of a joint distance histogram is shown for two identical isosurfaces. In this case, all points x in the volume space have the same distance to ˙ L k and¨Land¨ and¨L l . Finally, the mutual information is calculated based on Equation 3. The joint and marginal probabilities for the calculation of the joint and marginal entropies can be directly retrieved from the joint distance histogram. For the example in <ref type="figure" coords="4,404.36,247.88,31.41,8.02" target="#fig_3">Figure 4</ref>the mutual information of isosurfaces for the isovalues k and l is maximal since the isosurfaces are identical. To generate the entire multimodal similarity map, the steps in Figure 4 are repeated for every possible combination of isovalues in both modalities. If we assume that modality 1 has N different isovalues ˙ V = {k 1 , ...k N } and modality 2 has M different isovalues¨Visovalues¨ isovalues¨V = {l 1 , ..., l M } then the multimodal similarity map can be defined as an N × M matrix MSM(i, j). Each entry of the multimodal similarity map represents the similarity between the isosurface ˙ L i of modality 1 with the corresponding isovalue i and the isosurfacë L j of modality 2 with the corresponding isovalue j. On the right side of <ref type="figure" coords="4,455.76,357.54,30.56,8.02" target="#fig_3">Figure 4</ref>the complete multimodal similarity map for the example data sets is shown. The dark line represents the combinations of isovalues i and j which result in identical isosurfaces ˙ L i and¨Land¨ and¨L j . For all other combinations of isovalues the similarity of their corresponding isosurfaces is lower. Figures 5 and 6 show the multimodal similarity maps for the synthetic data sets introduced in Section 3. Dark regions denote a high similarity in these figures. For both types of multimodal data, the similarity maps for the combination of data sets without noise and with noise are shown. For the supplementary data in <ref type="figure" coords="4,405.15,457.32,29.18,8.02">Figure 5</ref>, both data sets contain four squares at the same locations. In the MSM each of the squares is represented by a rectangular area of higher similarity. In <ref type="figure" coords="4,488.94,477.24,30.09,8.02">Figure 5</ref>corresponding squares and rectangular regions are emphasized by colored frames. The band with the maximum similarity represents the combinations of isovalues k and l at which both data sets represent exactly the same isosurfaces. Due to different value ranges in both data sets this band does not follow the diagonal of the multimodal similarity map. In contrast to self similarity maps, multimodal similarity maps are not symmetrical along the main diagonal. If we investigate the influence of noise in the multimodal similarity map on the right side of <ref type="figure" coords="4,295.36,566.91,29.54,8.02">Figure 5</ref>, it can be seen that the band with the higher similarity is expanded. The expansion of the band gets smaller the higher the data values are. This is due to a higher SNR and therefore a smaller impact of the noise on the similarity measurement for higher data values. In <ref type="figure" coords="4,304.91,606.83,30.26,8.02" target="#fig_4">Figure 6</ref>the multimodal similarity maps for our complementary test data sets are shown. The regions in which both data sets contain contradictive information are clearly visible in the similarity map. In contrast to <ref type="figure" coords="4,336.35,636.72,29.57,8.02">Figure 5</ref>, the lower left rectangular area (red frame) in <ref type="figure" coords="4,285.12,646.68,31.09,8.02" target="#fig_4">Figure 6</ref>has a considerably lower similarity. Furthermore the band with maximum similarity is missing since there are no isosurfaces for the corresponding isovalues in data set 2 as one square is completely omitted in data set 2. In the same area of the multimodal similarity map with the noisy data set we get a higher variation of similarity values . This is due to the similarity between the square in data set 1 and structures generated by the noise in the background areas of data set 2. Another interesting area in the multimodal similarity map of <ref type="figure" coords="4,520.54,716.50,14.95,8.02;4,285.12,726.46,19.40,8.02" target="#fig_4">Fig- ure 6</ref>is the rectangle in the upper right corner (cyan frame). This rectangular area represents the similarity between the square in one data set and the circle in the other data set. Because of the different shapes of the objects the isosurfaces are similar but not identical. In the similarity map this can be seen by the expanded band of maximum similarity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SIMILARITY-BASED MULTIMODAL VOLUME VISUALIZATION</head><p>In this section, we discuss how the additional information provided by multimodal similarity maps can guide the process of exploring and analyzing multimodal volume data. It directs the user towards regions of high similarity or dissimilarity among the two modalities. We first show how salient regions in the similarity map can assist the user in identifying features. Next, we describe a simple approach for providing insight into the spatial differences in a multimodal data set by automatically identifying the most similar isosurfaces in two modalities. Finally, we present a novel approach for similarity-based classification of multimodal volume data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Similarity-Based Exploration</head><p>Multimodal similarity maps can be used to enable better selection of features in multimodal volume data in a straightforward manner. As the coordinate system of the similarity map is defined by the data values in both modalities, a simple approach is to allow the user to select a region of interest (e.g., by specifying a rectangular selection) and to <ref type="figure" coords="5,31.50,708.30,18.97,7.37">Fig. 9</ref>. Selection of brain tissue (a) without similarity weighting, (b) with similarity weighting, and (c) using the method of Haidacher et al. <ref type="bibr" coords="5,268.58,717.77,13.29,7.37" target="#b14">[15] </ref>after manually adjusting their δ weighting function to achieve the optimal result. restrict the visualization to data values which lie within this range. The color and opacity maps are defined separately for each modality. To demonstrate the advantages of similarity maps over other techniques , we choose a common example: the combination of CT and MRI data. While CT offers a standardized scale for identifying certain types of tissue, MRI provides significantly higher contrast in soft tissue regions. <ref type="figure" coords="5,349.05,113.44,30.38,8.02">Figure 7</ref>shows (a) a CT and (b) an MRI data set each rendered using a simple linear color map. MRI depicts more details in the brain tissue. But bone, due to its low water content, cannot be distinguished from air. In the CT scan, on the other hand, bone can be clearly identified. We compare our method with the multimodal transfer function space presented by Haidacher et al. <ref type="bibr" coords="5,424.62,173.49,14.94,8.02" target="#b14">[15] </ref> as well as a simple dual histogram . The transfer function space of Haidacher et al. <ref type="bibr" coords="5,491.97,183.45,14.94,8.02" target="#b14">[15] </ref>presents a fused data value on the horizontal axis and a fused gradient magnitude on the vertical axis. The fusion is performed based on point-wise mutual information. However, in contrast to our approach this measure is not based on spatial information, but only on the estimated probability of occurrence of a data value combination. In the dual histogram, the frequency of each data value combination is represented by the intensity of the corresponding pixel with darker regions corresponding to higher frequencies. The resulting parameter spaces are depicted in the top row of <ref type="figure" coords="5,349.32,273.12,29.67,8.02" target="#fig_7">Figure 8</ref>, where (a) shows the fused transfer function space, (b) depicts the dual histogram, and (c) presents the multimodal similarity map. It can be seen that all methods give a salient representation for regions corresponding to brain tissue. However, both the fused transfer function space and the dual histogram fail to give a clear indication of bone as they do not take into account spatial information. The data value ranges corresponding to bone are vastly different in CT and MRI data. Using the multimodal similarity map, on the other hand, a region corresponding to bone can be easily identified. The middle and bottom rows of <ref type="figure" coords="5,398.94,362.78,31.27,8.02" target="#fig_7">Figure 8</ref>show 3D visualizations of the data value ranges corresponding to the highlighted selection regions. In the case of the fused transfer function space and the dual histogram, the selection rectangle for bone had to be placed by trial-and-error. The previous example employed a binary selection in the multimodal similarity map. In order to exploit the information provided by the similarity map we can further use the similarity directly to modulate the opacity of a sample within the selected region: </p><formula>(a) (b) (c) </formula><formula>A (x) = A(x) MSM( ˙ f (x), ¨ f (x)) − MSM min MSM max − MSM min (7) </formula><p>where A (x) is the modulated opacity, A(x) is the original opacity, and ˙ f (x) and¨fand¨ and¨f (x) denote the data value of modality 1 and modality 2, respectively, at a sample position x. MSM min and MSM max are the minimum and maximum similarity values in the selected region. The result of this weighting is an enhancement of similar structures in both modalities while dissimilar structures are suppressed. <ref type="figure" coords="5,304.08,544.43,31.48,8.02">Figure 9</ref>illustrates the effect of this weighting. In <ref type="figure" coords="5,499.47,544.43,31.48,8.02">Figure 9</ref>(a) no weighting is applied, while <ref type="figure" coords="5,406.59,554.39,30.49,8.02">Figure 9</ref>(b) shows the result obtained with weighting. For comparison, <ref type="figure" coords="5,418.35,564.36,30.82,8.02">Figure 9</ref>(c) depicts the method of Haidacher et al. <ref type="bibr" coords="5,353.34,574.32,14.94,8.02" target="#b14">[15] </ref> after manually adjusting their δ weighting function . The selected regions used for the fusion correspond to the regions for the brain in <ref type="figure" coords="5,351.23,594.24,29.35,8.02" target="#fig_7">Figure 8</ref>. The results in <ref type="figure" coords="5,439.77,594.24,30.47,8.02">Figure 9</ref>show that similarity weighting produces comparable results to Haidacher et al. <ref type="bibr" coords="5,500.43,604.21,14.94,8.02" target="#b14">[15] </ref>without the necessary user interaction for the adjustment of the δ windowing function. This example illustrates how multimodal similarity maps can be used to provide assistance in identifying features across multiple modalities. A main advantage over the method of Haidacher et al. <ref type="bibr" coords="5,529.55,654.30,14.94,8.02" target="#b14">[15] </ref>is that the original data values can be retained instead of combining them during preprocessing. Furthermore, we do not introduce a new transfer function space which may be unfamiliar to users and difficult to understand. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Maximum Similarity Isosurfaces</head><p>With the multimodal similarity map we gain information about the similarity of certain combinations of isovalues. The multimodal </p><formula>¨ L ˆ k 1 , ¨ L ˆ k 2 </formula><p>). The results in the top row show the isosurfaces for a naive selection of the isovalues, i.e., in both data sets the same isovalue is chosen. ilarity maps for the examples in Section 4 have shown that combinations of isovalues for isosurfaces with a high similarity can be identified easily even if their ranges differ significantly. In many applications , such as industrial CT, users want to compare how well the object of interest is depicted in both modalities. Finding the isovalues which best represent the structure of interest in both scans, however, is difficult and requires time-consuming manual tuning. Using the multimodal similarity map, we can automatically identify the isovalue for the isosurface in one modality which maximizes the similarity to a specific isosurface from another modality. If we assume that a user has specified an isovalue k for an isosurface in one modality, the isovaluê k with the most similar isosurface in the second modality can be obtained by: </p><formula>ˆ k = arg max j MSM(k, j) (8) </formula><p>Using this simple approach, it is possible to specify an arbitrary isovalue in either modality and instantly visualize the corresponding isosurfaces from both modalities. A typical setup may depict these isosurfaces side-by-side in linked views enabling the user to quickly identify the spatial differences between two volumetric data sets by browsing through the range of isovalues. <ref type="figure" coords="6,32.46,534.52,35.39,8.02">Figure 10</ref>depicts an example for a dual energy CT scan. Due to the different attenuation characteristics for different energy levels, the value ranges in both data sets are different. This can be seen in the multimodal similarity map in the center of <ref type="figure" coords="6,179.76,564.41,33.57,8.02">Figure 10</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Similarity-Based Classification</head><p> Selection of simple regions in the multimodal similarity map, as described in the Section 5.1, allows quick exploration of multimodal data. This approach can be useful when only few specific features are of interest. For generating more complex visualizations, which depict multiple volumetric structures and take advantage of the additional information provided by multiple modalities, classification in the joint data space is necessary. The multimodal similarity map also opens up new avenues to assist in this process. Our idea is to use a nearest neighbor classifier in similarity space to determine the optical properties of a sample. Intuitively, instead of trying to relate the two modalities in terms of their data values, we instead classify samples, i.e., combinations of data values from both modalities, according to their similarity to a set of user-specified isosurfaces from both modalities. We assume two continuous three-dimensional scalar fields ˙ f , ¨ f : R 3 → R which represent two co-registered input volumes. For multimodal volume visualization, we assign a color and opacity to every point x ∈ R 3 in space based on the value of these functions. Our method takes as input a set of isovalue pairs </p><formula>h i = ( ˙ h i , ¨ h i ) where ˙ h i , ¨ h i </formula><p>correspond to isovalues of ˙ f and¨fand¨ and¨f respectively. Each pair of isovalues has an assigned color c i , opacity α i , and optional weight w i . For two data values k ∈ ˙ f and l ∈ ¨ f , we evaluate their multimodal similarity to the i-th isovalue pair in the following manner: </p><formula>˙ s i (k) = MSM(k, ¨ h i ) ¨ s i (l) = MSM( ˙ h i , l) (9) </formula><p>where MSM is the multimodal similarity map. This means that ˙ s i is the similarity of the isosurface k of ˙ f and the isosurfacë h i of¨fof¨ of¨f and¨sand¨ and¨s i is the similarity of the isosurface l of¨fof¨ of¨f and the isosurface ˙ h i of ˙ f . Based on the similarities ˙ s i and¨sand¨ and¨s i we can now define a combined measure s i of similarity between h i and the two isovalues k ∈ ˙ f and l ∈ ¨ f in multimodal similarity space: </p><formula>s i (k, l) = ˙ s i (k) ¨ s i (l) (10) </formula><p>The rationale behind this choice is that we interpret the similarities </p><formula>˙ s i (k), ¨ s i (l) </formula><p>as independent probabilities of k being similar tö h i and l being similar to ˙ h i . Thus, the joint probability of (k, l) being similar to h i is the product ˙ s i (k) ¨ s i (l). Alternatively, we could consider ˙ s i and¨sand¨ and¨s i as the membership functions of two fuzzy sets and s i as the membership function of their intersection. In this case, another possible definition would be s i (k, </p><formula>l) = min( ˙ s i (k), ¨ s i (l)</formula><p>) <ref type="bibr" coords="6,411.88,367.69,13.74,8.02" target="#b39">[41]</ref>. In our experiments, we found that both approaches lead to similar results. Having defined a measure of closeness between two points in similarity space, we now let each pair of isovalues h i to determine the optical properties of points that are closer to h i than to any other isovalue pair h j (i = j). This means a pair of data values (k, l) with k ∈ ˙ f , l ∈ ¨ f will assume the color and opacity of the isovalue pair h m(k,l) </p><formula>which maximizes s i (k, l): m(k, l) = arg max i s i (k, l)w i (11) </formula><p>where w i is a weight which allows additional control over the influence of the isovalue pair h i . During rendering, we can now evaluate this maximum for every sample location x ∈ R 3 in space: </p><formula>m x = m( ˙ f (x), ¨ f (x)) (12) </formula><p>Thus, m x denotes the index of the isovalue pair which maximizes the similarity to the data value ˙ f (x), ¨ f (x) at the sample location x. To visually encode the similarity of the sample to h m x , we additionally weight the sample opacity based on the similarity s m x . The color C(x) and opacity A(x) at the sample position x are then simply: </p><formula>C(x) = c m x A(x) = α m x s m x (13) </formula><p>In practice, in order to obtain crisp boundaries, it is convenient to define an additional threshold t which specifies the minimum similarity of a sample with any of the isovalue pairs in order to be visible. If s m x &lt; t, the sample is considered to be fully transparent. In volume rendering, it is common to evaluate a local illumination model using the normalized gradient of the scalar field as the normal vector. To enable volume shading, we can combine the gradient information of both modalities using a similarity-based weighting: </p><formula>g(x) = ˙ s m x ( ˙ f (x))∇ ˙ f (x) + ¨ s m x ( ¨ f (x))∇ ¨ f (x) ˙ s m x ( ˙ f (x)) + ¨ s m x ( ¨ f (x)) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Classification Specification</head><p> The described classification is equivalent to a generalized Voronoi decomposition of similarity space, i.e., using non-Euclidean distances defined by our similarity measure. Every sample, which is a pair of values from the two modalities, is assigned to the most similar isovalue pair which determines its color and opacity. We also visualize this classification on the similarity map itself by simply evaluating Equation 11 for each location, i.e., each combination of data values, in the similarity map and coloring the corresponding pixel accordingly. When depicted on the two-dimensional similarity map, where the coordinate system is defined by data values, these regions may be disconnected and non-convex (see, for example, Figures 11 and 12 which are discussed in detail below). Furthermore, based on the structure of the similarity map, the site, i.e., the isovalue pair that defines a region may not be contained within this region. While this may initially sound counter-intuitive, the following situation exemplifies such a case: Assume two isosurfaces for ˙ h i and¨hand¨ and¨h i which are highly dissimilar. There will likely be other isosurfaces they are more similar to than to each other. To provide an additional means for manipulating the classification regions instead of directly modifying the isovalues themselves, we define a user-specified control point c i = ( ˙ c i , ¨ c i ) for each isovalue pair h i , which can be freely moved. h i is initialized with c i and is then used to compute the similarity-weighted centroid of the region it defines. The isovalue pair h i is then moved to the position of the centroid: </p><formula>h i = ∑ (k,l)∈R(c i ) (k, l)s m(k,l) (k, l) ∑ (k,l)∈R(c i ) s m(k,l) (k, l) (15) </formula><p> where R(c i ) = {(k, l)|m(k, l) = i} is the similarity-space region assigned to c i . This essentially corresponds to one iteration of Lloyd's algorithm <ref type="bibr" coords="7,331.51,63.35,13.74,8.02" target="#b27">[28]</ref> . Note, however, that we do not perform the full relaxation since our goal is not to perform a full centroidal decomposition of the similarity space. Instead, our aim is for regions to follow their control points. </p><p>Based on this approach, we developed a simple user interface for similarity-based classification of multimodal volume data. The user is presented with the multimodal similarity map and can interactively add and remove control points, move them on the similarity map, and change their colors and opacities. When moving control points they behave similar to well-known " magic wand " -type selection tools – the regions they define snap to clusters in the similarity map. Slightly modifying a control point will, in accordance with the structure of the similarity map, not cause major changes of the classification result. Figures 11 and 12 show examples of our classification approach using the previously introduced synthetic data sets. The colored regions encode the nearest neighbors, in similarity space, of each of the white-outlined points for isovalue pairs h i in the corresponding color. This means a point on the similarity map is assigned to a region if it is more similar to the corresponding isovalue pair than to any other one. This also means that during volume rendering a sample with the corresponding value combination will be assigned the respective color and opacity (see Equation 13). The control points c i are depicted as the slightly larger points with dark outlines. It can be seen that in regions of high similarity the control points c i will be close to the corresponding isovalue pairs h i , but in other areas this is not necessarily the case. <ref type="figure" coords="7,294.12,313.37,34.95,8.02" target="#fig_9">Figure 11</ref>(a) illustrates that our approach is successful in identifying the correspondences between both data sets. By placing control points along the band of maximum similarity, the resulting regions will subdivide the map such that all rectangles in the data are assigned the user-specified color even though their data value ranges vary. Small perturbations in the placement of the control points leave the resulting classification unaffected. Adding noise to one of the data sets has little effect on the visual result, as shown in <ref type="figure" coords="7,433.20,383.11,34.59,8.02" target="#fig_9">Figure 11</ref>(b). <ref type="figure" coords="7,485.48,383.11,34.60,8.02" target="#fig_1">Figure 12</ref>shows that this approach makes it possible to easily exploit complementary information from both data sets. The red square, which is only present in one data set, as well as the blue circle and orange square can all be separated. This enables the generation of a combined visualization which contains all these features. Using a conventional approach, where the user defines a 1D transfer function for each modality and the results are blended, it is much more difficult to separate the features, since their corresponding data value ranges significantly vary. The additional weights w i used in Equation 11 therefore allow to control the sizes of the respective regions and can be interactively modified. In Figures 11 and 12 all weights w i are set to one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>An application for which our similarity-based classification approach is particularly suitable is the study of industrial parts where the goal is to detect manufacturing defects. Dual energy CT is of interest in such scenarios, since different materials can cause scanning artifacts at certain energy levels. The low energy scan typically has high precision but is affected by severe artifacts, while the high energy scan is nearly artifact-free but suffers from reduced precision and noise. It is desirable to combine the advantages of both energy levels, i.e., to generate a visualization which uses the global structure from the high energy scan to remove the artifacts from the low energy scan while preserving subtle details. Heinzl et al. <ref type="bibr" coords="7,442.32,626.83,14.94,8.02" target="#b15">[16] </ref>presented a processing pipeline for extracting surfaces from dual energy CT scans. With our similarity-based classification approach it is now possible to directly visualize structures which exhibit high surface similarity between both modalities. An example is given in <ref type="figure" coords="7,420.92,666.68,33.02,8.02" target="#fig_2">Figure 13</ref>. <ref type="figure" coords="7,460.43,666.68,34.52,8.02" target="#fig_2">Figure 13</ref>(a) shows the low energy scan of a 400 Volt power connector rendered using a conventional 1D transfer function. It is not possible to find an opacity setting which suppresses all artifacts but leaves the surface intact. In <ref type="figure" coords="7,294.12,706.53,34.55,8.02" target="#fig_2">Figure 13</ref>(b) the corresponding high energy scan, also rendered using a 1D transfer function, is shown. This result gives a better impression of the actual surface, but is noisy and lacks details. Using our similarity-based classification approach, as shown in <ref type="figure" coords="7,492.88,736.42,35.90,8.02" target="#fig_2">Figure 13</ref>(c), </p><formula>isovalue k (low energy) 256 256 0 0 (a) (b) (c) isovalue l (high energy) (d) </formula><p>Fig. 13. Similarity-based fusion of a dual energy CT scan of a power connector. The low-energy scan (a) and the high-energy scan (b) provide supplementary information which can be used to remove most of their respective drawbacks, as shown the similarity-based classification (c). The corresponding similarity map (d) shows the placement of the control points. In the image to its right the opacity of the outer surface has been reduced to reveal the interior parts of the connector. we can remove the artifacts by choosing control points which select regions of high dissimilarity and setting their opacity to zero. The feature of interest, the outer surface of the connector, is similar in both data sets. Since the opacity of a sample is based on the global surface similarity of its data values to the isosurface pair, holes and artifacts in the low energy scan can be remedied using information from the high energy scan. <ref type="figure" coords="8,74.56,494.63,35.78,8.02" target="#fig_2">Figure 13</ref>(d) shows the similarity map together with the specified control points. The image to the right of the similarity map uses the same control points as <ref type="figure" coords="8,156.16,514.56,35.03,8.02" target="#fig_2">Figure 13</ref>(c), but the opacity of the outer surface has been lowered to reveal the interior parts of the connector. Furthermore, our approach can be used to assist the classification of ambiguous structures. One example is CT angiography, where it is desired to clearly separate contrast-enhanced blood vessels from bone. In a single modality scan this is typically not possible as the data values of the contrast agent partially overlap with lower-density bone regions and cartilage. This is illustrated in <ref type="figure" coords="8,198.65,595.60,34.93,8.02" target="#fig_3">Figure 14</ref>(a), where it was attempted to specify different colors for bones and vessels using a 1D transfer function on a low energy CT scan of the lower extremities . While it is also not possible to achieve this separation using a high-energy scan, as shown in <ref type="figure" coords="8,136.93,635.45,35.43,8.02" target="#fig_3">Figure 14</ref>(b), it can be seen that the classified structures are slightly different in both modalities. Using similarity-based classification, we can therefore achieve a better separation , as depicted in <ref type="figure" coords="8,99.37,665.34,34.60,8.02" target="#fig_3">Figure 14</ref>(c). The corresponding similarity map is shown in <ref type="figure" coords="8,64.17,675.30,34.21,8.02" target="#fig_3">Figure 14</ref> (d). In order to illustrate how regions in the similarity map correspond to structures of different ossification levels, they have been assigned different colors in the rightmost image. Vessels are orange and different bone structures are white, blue, and green. A further example is shown in <ref type="figure" coords="8,143.79,716.50,29.11,8.02">Figure 1</ref>. In this case, a dual energy CT data set of a human head is used. The similarity map, shown on the bottom right, provides good guidance for iteratively selecting the individual tissues numbered from 1 to 7. The information provided by the two energy levels is sufficient to allow differentiation between bone (selected in step 3), major vessels (step 4), and minor vessels (step 5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">IMPLEMENTATION</head><p>The calculation of the multimodal similarity map is a preprocessing step which is implemented in C++ and runs on the CPU. It has to be performed only once for a single multimodal data set. After the preprocessing step the multimodal similarity map is simply represented as a two-dimensional image. During rendering, the similarity of a combination of isovalues from the two modalities can be retrieved by a single lookup in a 2D texture. The user interface for our similarity-based classification approach was implemented using the Qt toolkit. The user-interface widget generates a set of isovalue pairs, colors, and weights, which are passed to a GPU-based volume renderer implemented in GLSL. In the shader, the similarity between the data values at the current sample point and each isovalue pair is determined using two texture lookups (see Equations 9 and 10) and the maximum is computed. While this is more expensive than a conventional transfer function lookup, the additional costs are limited due to the fact than only few control points will be required in many applications. In our implementation, for a typical number of five control points, the average render time increases by a factor of approximately 1.4 compared to a single conventional transfer-function lookup. The color and opacity of the maximally similar isovalue pair then determines the color and opacity of the current sample, as described in Section 5.3. The gradient vectors of both modalities are computed by central differences, combined with Equation 14, and used to evaluate a local illumination model if shading is enabled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>As shown in our examples, multimodal surface similarity can provide a useful tool for the visual analysis of multimodal volume data. However , isosurface similarity as a measure is only useful in cases where there is some correspondence between features and isosurfaces. For example, in data where textures or patterns are of central importance, isosurface similarity will likely fail to provide valuable insights. While this is a clear limitation of our approach, we want to emphasize that also the lack of distinct structures in a multimodal similarity map provides additional information to the user. As our approach deliberately avoids to position itself as a new technique central to the visualization process, the lack of distinct features (like the lack of distinct features in a histogram) simply means that little additional guidance can be provided for the particular data set. However, in our experiments we found that even for challenging data combinations, such as CT and PET, which exhibit little correspondence, multimodal surface similarity is still able to assist in finding joint data value ranges which correspond to joint structures of interest. The computation time for the multimodal similarity map of two data sets is approximately twice the computation time of a self similarity map for a data set of the same size. This is due to the lack of symmetry . As reported by Bruckner and Möller <ref type="bibr" coords="8,429.69,586.98,9.52,8.02" target="#b2">[3]</ref>, a feasible strategy to limit the duration of this pre-processing step is to use downsampled versions of the distance transforms (which are computed at the original data set resolution) for the mutual information computation. The computation times for all data sets used in this paper are given in <ref type="figure" coords="8,473.31,626.83,25.33,8.02" target="#tab_1">Table 1</ref> . The second column in the table lists the downsampling rate for the respective volume which is automatically chosen to limit the computation time to approximately one minute. Even though downsampling is performed quite aggressively, a distance field is a rather redundant representation and the downsampled version essentially acts as a shape descriptor and is not used for precise spatial measurements. To the results of Bruckner and Möller <ref type="bibr" coords="8,362.08,696.57,10.45,8.02" target="#b2">[3] </ref>we can also add information about additional experiments on the effects of quantization in the value domain. We found that for real-world data a quantization to 8 bits results in practically no structural differences in the similarity map, as exemplified in <ref type="figure" coords="8,285.12,736.42,33.12,8.02" target="#fig_14">Figure 15</ref>. <ref type="figure" coords="9,47.93,268.42,7.39,7.37" target="#fig_3">14</ref>. Similarity-based classification of blood vessels in a dual energy CT angiography scan of the lower extremities. When using only the information from the low energy scan (a) or the high energy scan (b), it is not possible to separate blood vessels and bones. Using multimodal similarity (c) this can be achieved. The corresponding control points are shown on the similarity map (d) where different colors have been assigned to vessels and bones of different densities. <ref type="figure" coords="9,31.50,332.70,25.10,7.37" target="#tab_1">Table 1</ref>. Computation times for the multimodal similarity maps shown in the paper as measured in an Intel Core i7 950 CPU with a clock rate of 3.07 GHz and 12 GB RAM. The first column reports the data type and size. The second column gives the downsampling rate for the distance fields. The last column gives the total computation time for the similarity maps which is the sum of the computation times for all distance transforms (third column) and the mutual information of all isovalue combinations (fourth column). Another limitation of our work is that the described approach only considers data sets consisting of two modalities. While this applies to many application scenarios, a solution for a larger number of modalities would be desirable. A multi-dimensional similarity map of similarities between all isovalue combinations of the respective data sets, however, would be computationally infeasible. A potential solution could be to only consider pair-wise similarities between the individual modalities resulting in a matrix of multimodal similarity maps. The investigation of whether such an approach is effective is an interesting topic for future research. Furthermore, our technique could also be applied to investigate time-dependent data by generating a set of similarity maps between subsequent time steps. Temporal similarity maps could help to identify stable features and to pinpoint discontinuities. A further limitation of similarity maps in general is that they do not contain frequency information, i.e., small structures which exhibit high similarity receive the same prominence as very large regions with a similar degree of similarity. This can be regarded as an advantage with respect to histograms, where large regions tend to dominate and logarithmic scaling is typically required. It can also be a drawback since data value combinations which do not occur at all are not clearly indicated. Ideally, a combination of both types of information would be desired, but identifying a good visual encoding for this purpose is not straightforward and remains an area of future research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this paper, we introduced multimodal surface similarity maps as a tool for the investigation of multimodal volume data sets. The multimodal similarity map provides an overview of the differences and similarities between the isosurfaces of two modalities in a compact manner. The analysis of parameter spaces is an increasingly important topic for knowledge discovery in scientific data. Our approach showed that spatial similarity information can assist the visualization process by guiding the selection of features. By exploiting similarity information , we introduced a novel way for the interactive classification and visualization of multimodal volume data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,22.50,151.81,250.38,7.37;2,22.50,161.28,250.38,7.37;2,22.50,170.74,55.95,7.37;2,74.39,189.93,21.66,6.56"><head>Fig. 2. </head><figDesc>Fig. 2. Data sets containing the same structures with different value ranges (supplementary data). The histograms show the distributions of the data values. data set 1 </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,22.50,291.86,250.38,7.37;2,22.50,301.32,230.23,7.37"><head>Fig. 3. </head><figDesc>Fig. 3. Two synthetic data sets which represent complementary data. Data set 2 contains structures which are different from data set 1. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,31.50,153.14,513.00,7.37;3,31.50,162.61,128.21,7.37"><head>Fig. 4. </head><figDesc>Fig. 4. Pipeline for the generation of a multimodal similarity map. The illustration shows which steps are necessary to calculate the similarity of isosurfaces for the isovalues k and l. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,22.50,370.00,206.59,7.37"><head>Fig. 6. </head><figDesc>Fig. 6. Multimodal similarity maps for complementary data. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,31.50,105.69,250.38,7.37;5,31.50,115.16,19.85,7.37"><head></head><figDesc>Fig. 7. Clipping plane through a (a) CT and (b) MRI scan of a human brain. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="5,31.50,308.82,250.38,7.37;5,31.50,318.28,250.38,7.37;5,31.50,327.75,250.38,7.37"><head>Fig. 8. </head><figDesc>Fig. 8. Comparison of (a) a fused transfer function space [15], (b) a dual histogram, and (c) a multimodal similarity map for selection guidance. Region 1 classifies brain tissue and region 2 classifies the cranial bone. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,213.34,564.41,59.53,8.02;6,22.50,574.37,250.38,9.17;6,22.50,584.33,250.37,8.02;6,22.50,594.30,250.37,8.02;6,22.50,605.37,250.38,9.17;6,22.50,616.44,250.37,9.17;6,22.50,626.41,221.80,8.02"><head></head><figDesc>. The images in the bottom row show isosurfaces for isovalues k 1 and k 2 in modality 1. The top row shows the isosurfaces for the same isovalues in modality 2. The middle row shows the isosurfaces in modality 2 for the isovaluesˆkisovaluesˆ isovaluesˆk 1 andˆkandˆ andˆk 2 with the maximum similarity to k 1 and k 2 . The isosurfaces forˆkforˆ forˆk 1 andˆkandˆ andˆk 2 match the isosurfaces in modality 1 much better than the isosurfaces for the naive selection of isovalues. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="7,31.50,204.48,250.38,7.37;7,31.50,213.95,209.65,7.37"><head>Fig. 11. </head><figDesc> Fig. 11. Classification based on multimodal surface similarity for supplementary data (a) without noise and (b) with added noise. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="7,31.50,388.02,250.38,7.37;7,31.50,397.48,209.65,7.37"><head>Fig. 12. </head><figDesc> Fig. 12. Classification based on multimodal surface similarity for complementary data (a) without noise and (b) with added noise. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="9,31.50,268.42,513.00,7.37;9,31.50,277.88,513.00,7.37;9,31.50,287.35,513.00,7.37;9,31.50,296.81,150.07,7.37"><head>Fig. </head><figDesc>Fig. 14. Similarity-based classification of blood vessels in a dual energy CT angiography scan of the lower extremities. When using only the information from the low energy scan (a) or the high energy scan (b), it is not possible to separate blood vessels and bones. Using multimodal similarity (c) this can be achieved. The corresponding control points are shown on the similarity map (d) where different colors have been assigned to vessels and bones of different densities. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="9,294.12,435.33,250.38,7.37;9,294.12,444.80,250.38,7.37;9,294.12,454.26,31.45,7.37"><head>Fig. 15. </head><figDesc>Fig. 15. A comparison between the multimodal similarity map with a isovalue precision of 8 bits (a) and 12 bits (b) for the data set shown in Figure 1. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false" coords="3,30.75,26.18,320.17,7.50"><figDesc coords="3,30.75,26.18,320.17,7.50">HAIDACHER ET AL: VOLUME ANALYSIS USING MULTIMODAL SURFACE SIMILARITY</figDesc><table coords="4,22.50,50.55,209.31,303.67">data set 1 
data set 2 

isovalue k 

isovalue l 

M 

N 
0 
0 

isovalue k 

isovalue l 

M 

N 
0 
0 

data set 2 (with noise) 

Fig. 5. Multimodal similarity maps for supplementary data. 

data set 1 
data set 2 

isovalue k 

isovalue l 

M 

N 
0 
0 

isovalue k 

isovalue l 

M 

N 
0 
0 

data set 2 (with noise) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false" coords="5,30.75,26.18,518.00,7.54"><figDesc coords="5,532.75,26.46,16.00,7.26;5,30.75,26.18,320.17,7.50">1973 HAIDACHER ET AL: VOLUME ANALYSIS USING MULTIMODAL SURFACE SIMILARITY</figDesc><table coords="6,22.50,55.04,250.38,211.64">isovalue k 

isovalue l 

256 

256 
0 

0 k 1 
k 2 

L k 1 

L k 1 

^ 

L k 2 

L k 1 

L k 2 

^ 

L k 2 

Fig. 10. Maximum similarity isosurface detection for two different iso-
values k 1 and k 2 . The results in the middle row show the most similar 
isosurfaces ( </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,40.76,74.58,232.12,7.13;10,40.76,84.05,232.12,7.13;10,40.76,93.51,69.95,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">A tri-space visualization interface for analyzing time-varying multivariate volume data</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Akiba</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroVis 2007</title>
		<meeting>EuroVis 2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,102.97,232.12,7.13;10,40.76,112.44,127.98,7.13"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Artifacts in CT: Recognition and avoidance</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">F</forename>
				<surname>Barrett</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Nicholas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1679" to="1691" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,121.90,232.12,7.13;10,40.76,131.37,104.98,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Isosurface similarity maps</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,140.83,232.12,7.13;10,40.76,150.30,197.90,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Data intermixing and multi-volume rendering</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Cai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Sakas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,159.76,232.12,7.13;10,40.76,169.23,232.12,7.13;10,40.76,178.69,77.26,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">On histograms and isosurface statistics</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Carr</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Duffy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Denby</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1259" to="1265" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,188.16,232.12,7.13;10,40.76,197.62,232.12,7.13;10,40.76,207.08,201.27,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Flexible isosurfaces: Simplifying and displaying scalar topology using the contour tree</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Carr</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Snoeyink</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Van De Panne</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geometry: Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="58" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,216.55,232.12,7.13;10,40.76,226.01,232.12,7.13;10,40.76,235.48,77.26,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">An information-theoretic framework for visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jänicke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1215" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,244.94,232.12,7.13;10,40.76,254.41,116.38,7.13"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Constructive volume geometry</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">V</forename>
				<surname>Tucker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,263.87,232.12,7.13;10,40.76,273.34,232.12,7.13;10,40.76,282.80,232.12,7.13;10,40.76,292.26,232.11,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual energy CT: How to best blend both energies in one fused image?</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Eusemann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Holmes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Iii</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Schmidt</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">G</forename>
				<surname>Flohr</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Robb</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Mc-Collough</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Hough</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Huprich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Wittmer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Siddiki</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">G</forename>
				<surname>Fletcher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE Medical Imaging</title>
		<meeting>SPIE Medical Imaging</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,301.73,232.12,7.13;10,40.76,311.19,232.12,7.13;10,40.76,320.66,232.12,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">MRI-PET correlation in three dimensions using a volume-of-interest (VOI) atlas</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Evans</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Marrett</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Torrescorzo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ku</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Collins</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cerebral Blood Flow and Metabolism</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="69" to="78" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,330.12,232.12,7.13;10,40.76,339.59,232.12,7.13;10,40.76,349.05,232.12,7.13;10,40.76,358.52,29.89,7.13"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Volumetric-CSG -a model-based volume visualization approach</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Fang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Srinivasan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference in Central Europe on Computer Graphics and Visualization</title>
		<meeting>the 6th International Conference in Central Europe on Computer Graphics and Visualization</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,367.98,232.12,7.13;10,40.76,377.45,232.12,7.13;10,40.76,386.91,87.55,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified information-theoretic framework for viewpoint selection and mesh saliency</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Feixas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sbert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>González</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,396.37,232.12,7.13;10,40.76,405.84,232.12,7.13;10,40.76,415.30,170.26,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">A framework for fusion methods and rendering techniques of multimodal volume data: Research articles</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ferre</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Puig</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Tost</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,424.77,232.12,7.13;10,40.76,434.23,167.51,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualization of multi-variate scientific data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Fuchs</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hauser</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1670" to="1690" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,443.70,232.12,7.13;10,40.76,453.16,232.12,7.13;10,40.76,462.63,202.20,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Informationbased transfer functions for multimodal visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Haidacher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kanitsar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Visual Computing for Biomedicine</title>
		<meeting>Visual Computing for Biomedicine</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,472.09,232.12,7.13;10,40.76,481.55,232.12,7.13;10,40.76,491.02,229.54,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Surface extraction from multimaterial components for metrology using dual energy CT</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Heinzl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kastner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1520" to="1527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,500.48,232.12,7.13;10,40.76,509.95,232.12,7.13;10,40.76,519.41,214.43,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient multimodality volume fusion using graphics hardware</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bae</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Kye</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y.-G</forename>
				<surname>Shin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Science 2005</title>
		<meeting>the International Conference on Computational Science 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="842" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,528.88,232.12,7.13;10,40.76,538.34,107.79,7.13"  xml:id="b17">
	<monogr>
		<title level="m" type="main">Computed Tomography: Principles, Design, Artifacts and Recent Advances</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hsieh</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>SPIE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,547.81,232.12,7.13;10,40.76,557.27,232.12,7.13;10,40.76,566.74,232.12,7.13;10,40.76,576.20,17.93,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Shape registration in implicit spaces using information theory and free form deformations</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Paragios</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Metaxas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1303" to="1318" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,583.82,112.36,8.97;10,149.58,585.66,123.30,7.13;10,40.76,595.13,232.12,7.13;10,40.76,604.59,136.69,7.13"  xml:id="b19">
	<analytic>
		<title level="a" type="main">3D distance fields: a survey of techniques and applications</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Jones</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Baerentzen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">ˇ</forename>
				<surname>Srámek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="581" to="599" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,614.06,232.12,7.13;10,40.76,623.52,232.12,7.13;10,40.76,632.99,37.86,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">On the fractal dimension of isosurfaces</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Khoury</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Wenger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1198" to="1205" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,642.45,232.12,7.13;10,40.76,651.92,232.12,7.13;10,40.76,661.38,114.67,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing dual-modality rendered volumes using a dual-lookup table transfer function</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Eberl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Feng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,670.84,232.12,7.13;10,40.76,680.31,232.12,7.13;10,40.76,689.77,185.82,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-automatic generation of transfer functions for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Kindlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">W</forename>
				<surname>Durkin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Volume Visualization</title>
		<meeting>the IEEE Symposium on Volume Visualization</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,699.24,232.12,7.13;10,40.76,708.70,232.12,7.13;10,40.76,718.17,173.05,7.13"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Volume rendering multivariate data to visualize meteorological simulations: a case study</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kniss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Grenier</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Robinson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VisSym 2002</title>
		<meeting>VisSym 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="189" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,727.63,232.12,7.13;10,40.76,737.10,232.12,7.13;10,303.38,54.06,173.05,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Medical applications of multi-field volume rendering and VR techniques</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kniss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Schulze</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Wössner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Winkler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Lang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VisSym 2004</title>
		<meeting>VisSym 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="249" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,63.52,232.12,7.13;10,303.38,72.99,191.27,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">Entropy and correlation: Some comments</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Kvålseth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="517" to="519" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,82.45,232.12,7.13;10,303.38,91.92,232.12,7.13;10,303.38,101.38,232.12,7.13;10,303.38,110.85,17.93,7.13"  xml:id="b26">
	<analytic>
		<title level="a" type="main">The brain: integrated threedimensional display of MR and PET images</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Levin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Hu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Tan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Galhotra</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Pelizzari</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Beck</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Cooper</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mullan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="783" to="789" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,120.31,232.12,7.13;10,303.38,129.78,135.93,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lloyd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,139.24,232.12,7.13;10,303.38,148.70,232.12,7.13;10,303.38,158.17,219.23,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">A versatile functional/anatomic image fusion method for volume data sets</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Noz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">Q</forename>
				<surname>Maguire</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">P</forename>
				<surname>Zeleznik</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">L</forename>
				<surname>Kramer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Mahmoud</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Crafoord</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="297" to="307" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,167.63,232.12,7.13;10,303.38,177.10,232.12,7.13;10,303.38,186.56,158.61,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting histograms and isosurface statistics</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Scheidegger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schreiner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Duffy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Carr</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Silva</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1659" to="1666" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,196.03,232.12,7.13;10,303.38,205.49,151.04,7.13"  xml:id="b30">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">E</forename>
				<surname>Shannon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423623" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,214.96,232.12,7.13;10,303.38,224.42,232.12,7.13;10,303.38,233.88,232.12,7.13;10,303.38,243.35,175.46,7.13"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Emphasizing isosurface embeddings in direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Takahashi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Takeshima</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Fujishiro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Nielson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization: The Visual Extraction of Knowledge from Data</title>
		<editor>G. Bonneau, T. Ertl, and G. Nielson</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="185" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,252.81,232.12,7.13;10,303.38,262.28,232.12,7.13;10,303.38,271.74,60.00,7.13;10,285.12,279.36,29.61,8.97;10,311.19,281.21,224.31,7.13;10,303.38,290.67,232.12,7.13;10,303.38,300.14,229.10,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Importance-driven focus of attention Visualization of boundaries in volumetric data sets using LH histograms</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Viola</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Feixas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sbert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller34 ] P</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">V</forename>
				<surname>Sereda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">W</forename>
				<surname>Bartrolí</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">A</forename>
				<surname>Serlie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Gerritsen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">122</biblScope>
			<biblScope unit="page" from="933" to="940208" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,309.60,232.12,7.13;10,303.38,319.07,98.60,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">Information theory in scientific visualization</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-W</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="254" to="273" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,328.53,232.12,7.13;10,303.38,337.99,184.96,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Volume-sampled 3D modeling</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">W</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Kaufman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="26" to="32" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,347.46,232.12,7.13;10,303.38,356.92,232.12,7.13;10,303.38,366.39,122.64,7.13"  xml:id="b35">
	<analytic>
		<title level="a" type="main">A comparative analysis of image fusion methods</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ziou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Armenakis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1391" to="1402" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,375.85,232.12,7.13;10,303.38,385.32,232.12,7.13;10,303.38,394.78,122.71,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">Multimodal volume registration by maximization of mutual information</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Wells</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Iii</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Viola</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Atsumi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Nakajima</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Kikinis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="51" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,404.25,232.12,7.13;10,303.38,413.71,232.12,7.13;10,303.38,423.17,110.79,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">An information-theoretic framework for flow visualization</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Xu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T.-Y</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-W</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1216" to="1224" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,432.64,232.12,7.13;10,303.38,442.10,232.12,7.13;10,303.38,451.57,206.81,7.13"  xml:id="b38">
	<analytic>
		<title level="a" type="main">Information-theoretic measures for knowledge discovery and data mining</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Yao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Karmeshu, editor, Entropy Measures, Maximum Entropy Principle and Emerging Application</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="115" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,461.03,210.38,7.13"  xml:id="b39">
	<analytic>
		<title level="a" type="main">Fuzzy sets</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zadeh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="338" to="353" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,470.50,232.12,7.13;10,303.38,479.96,232.12,7.13;10,303.38,489.43,160.80,7.13"  xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-modal volume visualization using object-oriented methods</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">J</forename>
				<surname>Zuiderveld</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Viergever</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Volume Visualization</title>
		<meeting>the IEEE Symposium on Volume Visualization</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
