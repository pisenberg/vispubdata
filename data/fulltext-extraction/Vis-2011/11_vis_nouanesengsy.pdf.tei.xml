<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T14:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Load-Balanced Parallel Streamline Generation on Large Scale Vector Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Boonthanome</forename>
								<surname>Nouanesengsy</surname>
								<roleName>Student Member, Ieee</roleName>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Teng-Yok</forename>
								<surname>Lee</surname>
								<roleName>Student Member, Ieee</roleName>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Han-Wei</forename>
								<surname>Shen</surname>
							</persName>
						</author>
						<title level="a" type="main">Load-Balanced Parallel Streamline Generation on Large Scale Vector Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Flow visualization</term>
					<term>Parallel processing</term>
					<term>3D vector field visualization</term>
					<term>Streamlines</term>
				</keywords>
			</textClass>
			<abstract>
				<p>—Because of the ever increasing size of output data from scientific simulations, supercomputers are increasingly relied upon to generate visualizations. One use of supercomputers is to generate field lines from large scale flow fields. When generating field lines in parallel, the vector field is generally decomposed into blocks, which are then assigned to processors. Since various regions of the vector field can have different flow complexity, processors will require varying amounts of computation time to trace their particles, causing load imbalance, and thus limiting the performance speedup. To achieve load-balanced streamline generation, we propose a workload-aware partitioning algorithm to decompose the vector field into partitions with near equal workloads. Since actual workloads are unknown beforehand, we propose a workload estimation algorithm to predict the workload in the local vector field. A graph-based representation of the vector field is employed to generate these estimates. Once the workloads have been estimated, our partitioning algorithm is hierarchically applied to distribute the workload to all partitions. We examine the performance of our workload estimation and workload-aware partitioning algorithm in several timings studies, which demonstrates that by employing these methods, better scalability can be achieved with little overhead.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Because of significant gains in computational power within the past decade, scientists are now able to perform simulations at unprecedented spatial and temporal resolutions. The sheer size of data generated from these simulations, however, becomes a great challenge for effective visualization and data analysis. Not only that, it is difficult to perform such data intensive tasks in machines far away from where the data are stored. In addition, the memory and storage space available to most workstations is hardly enough to host the entire dataset. As a result, it becomes necessary to shift the visualization computations as close as possible to where the data reside, in many cases to the supercomputers where the simulations are performed. The focus of this paper is on parallel streamline generation for large scale vector fields using massively parallel computers. Visualizing streamlines is still one of the most common methods for analyzing three dimensional flow fields. While it is well understood how to compute streamlines sequentially, generating streamlines for large datasets in parallel presents unique challenges. For a very large dataset, it is necessary to partition the data into smaller data blocks and distribute them to the compute processes, since it is not possible for one process to hold the entire dataset in memory. Because the spatial distribution and the lengths of streamlines across the entire domain can vary, load balancing is often difficult to achieve. While several algorithms have been proposed in the past for parallel streamline generation <ref type="bibr" coords="1,112.50,536.47,9.71,8.21" target="#b1">[2,</ref><ref type="bibr" coords="1,125.11,536.47,11.21,8.21" target="#b20"> 21,</ref><ref type="bibr" coords="1,139.21,536.47,11.21,8.21" target="#b23"> 24,</ref><ref type="bibr" coords="1,153.32,536.47,6.47,8.21" target="#b3"> 4]</ref>, there is a lack of robust models to measure the cost of load imbalance and therefore make it difficult to develop an efficient algorithm with any kind of optimality guarantee . In this paper, we propose a novel mathematical model to solve the data distribution and load balancing problem for parallel streamline generation using principles of optimization. To minimize the communication cost, our algorithm partitions the data statically, that is, no data shuffling among the compute processes will take place at run time once the data are distributed. With a given set of seeds, the compute processes generate streamlines in their own blocks, and communicate the particle positions once the particles reach the block boundaries. The need to communicate particle positions at certain synchronization points divides the whole parallel streamline generation process into a sequence of rounds. To ensure maximum parallel speedup, our mathematical model minimizes the difference of workload in each round using quadratic programming. Given a sequence of data blocks and their corresponding workload, our optimization method produces a grouping, or partitioning, of the blocks. Each partition is given to one compute process. One feature of our algorithm is that a data block can be assigned to multiple partitions. The degree of data replication is calculated by our optimization algorithm to achieve optimal load balancing. Besides the mathematical model that is used to solve the load balancing problem, another important ingredient of our algorithm is a graph model used to predict the workload for every data block in every round. Blocks are encoded as nodes in the graph, and adjacent blocks will have edges connecting their corresponding nodes. Using this graph, an estimate of the workload for each data block can be obtained. The workload estimates are then used as input to our partitioning algorithm. The paper is organized as follows. After reviewing existing work for parallel field line generation in Section 2, we present the basic idea of our parallel streamline generation algorithm in Section 3. The mathematical model and the optimization algorithm are presented in Section 4, followed by the workload estimation algorithm in Section 5. We present the results of our algorithm in Section 6, address the limitations and future works in Section 7, and conclude this paper in Section 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Several systems have been proposed in recent years for parallel field line generation. <ref type="bibr" coords="1,352.96,596.76,30.28,8.21">Yu et al. </ref>proposed a hierarchical approach, advecting particles within blocks in different resolutions in parallel. One drawback of this approach is that field lines cannot extend beyond block boundaries, thus limiting their lengths <ref type="bibr" coords="1,434.90,626.64,13.74,8.21" target="#b23">[24]</ref>. <ref type="bibr" coords="1,456.16,626.64,51.40,8.21">Pugmire et al. </ref>proposed another parallel streamline generation system that monitors load balance throughout the computation, and dynamically adjusts the system using different strategies, including loading blocks when neces- sary <ref type="bibr" coords="1,312.46,666.48,13.74,8.21" target="#b20">[21]</ref>. Recently this system was extended to take advantage of multicore architectures <ref type="bibr" coords="1,380.18,676.44,9.52,8.21" target="#b1">[2]</ref>. Dynamically loading data blocks during runtime, however, can cause extra I/O overhead. In contrast, Peterka et al. investigated another system that uses a static partitioning of the vector field <ref type="bibr" coords="1,336.89,706.33,13.74,8.21" target="#b19">[20]</ref>, which was also used by a recent map-reduce like <ref type="bibr" coords="1,534.05,706.33,10.45,8.21" target="#b5">[6] </ref>programming framework DStep for parallel streamline computation <ref type="bibr" coords="1,294.13,726.25,13.74,8.21" target="#b14">[15]</ref>. The partitioning of the blocks was produced using a round-robin scheme, regardless of the flow complexity and the spatial distribution </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>of particles. In </head><p>this paper, we will demonstrate that by considering these pieces of information, our partitioning scheme can lead to better performance for streamline generation. Graph-based representations have been utilized in other aspects for flow field visualization. Because graphs can provide an abstract representation of a flow field, Xu and Shen created a graph-based user interface as an overview of the vector field, with the goal of guiding the user to views with less occlusion <ref type="bibr" coords="2,159.36,243.85,13.74,8.21" target="#b22">[23]</ref>. Also, by representing the flow field as a graph, techniques for graph analysis can be applied for flow field analysis. One example is the flow field partitioning algorithm proposed by Chen and Fujishiro, which uses spectral clustering to partition a vector field to minimize communication overhead. <ref type="bibr" coords="2,254.25,283.70,9.52,8.21" target="#b3">[4]</ref>. The goal of graph partitioning is to decompose the graph into disjoint subsets with a minimum edge cost. While graph partitioning is an NP-hard problem <ref type="bibr" coords="2,110.37,313.87,13.74,8.21" target="#b9">[10]</ref>, because of its applications in different areas, several techniques have been developed in the past, including quadratic programming <ref type="bibr" coords="2,111.15,333.79,13.74,8.21" target="#b12">[13]</ref>, spectral clustering <ref type="bibr" coords="2,201.53,333.79,9.52,8.21" target="#b3">[4]</ref>, and multi-level approaches <ref type="bibr" coords="2,66.18,343.75,9.71,8.21" target="#b6">[7,</ref><ref type="bibr" coords="2,79.23,343.75,10.64,8.21" target="#b13"> 14]</ref>. A detailed review of graph partitioning and its applications for scientific computation can be seen in the article by Scholegal et al. <ref type="bibr" coords="2,80.65,363.68,13.75,8.21" target="#b21">[22]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARALLEL STREAMLINE GENERATION</head><p>An overview diagram of our parallel streamline generation system is shown in <ref type="figure" coords="2,59.33,406.89,20.90,8.21">Fig. 1</ref> . Our goal is to develop a high performance parallel streamline generation system for distributed-memory environments using MPI <ref type="bibr" coords="2,63.87,426.81,13.74,8.21" target="#b10">[11]</ref>. The inputs to our system are a vector field, and an initial set of user specified seed locations. The vector field is decomposed into blocks, and the resulting decomposition is encoded into a flow graph. A flow graph records how data blocks are related to each other according to the underlying flow directions. The flow graph is used in conjunction with the initial seeds to estimate the workload for each block during the course of streamline generation. Section 5 explains the flow graph and block workload estimation method in more detail. Once we have an estimate of workload for each block, our workload-aware partitioning algorithm is used to model the problem of assigning blocks and generating load balanced partitions as an optimization problem. Our partitioning algorithm is explained in detail in Section 4. Since a flow graph only depends on the vector field data, and can be reused for any subsequent runs, the flow graph construction is a preprocessing step. The block workload estimation and partitioning algorithms are performed at run time since they depends on the streamline seed locations. Once the partitions have been calculated, all processes will load their assigned data blocks into memory. By loading all the data at once, MPI I/O can be employed to maximize bandwidth and I/O performance . Once loaded, blocks are kept in memory throughout the computation. The block assignments are static, meaning that block assignments will not change, and no further transfer of block data is performed. Beginning with the initial streamline seeds, the particle tracing process is divided into a sequence of stages, or rounds. <ref type="figure" coords="2,208.87,676.42,20.84,8.21">Fig. 2</ref>illustrates a sequence of three rounds. For each round, each process advects every particle lying within its assigned blocks. A particle is traced until it travels outside the block it initially began in. Once all particles have been advected in this manner, the round is complete. Particles terminate either when they exit the vector field domain, when it is detected that a critical point is reached, or the user-specified maximum </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exchange Particles </head><p>Round <ref type="figure" coords="2,285.12,162.32,19.81,7.37">Fig. 2</ref> . Block-wise streamline generation. The vector field is decomposed into 4 × 3 blocks, which are separated into four partitions. Blocks are colored by their partition. In the beginning, six particles are placed in this vector field. The beginning and ending locations of particles are marked as white and black circles, respectively. tion steps are exceeded. In between rounds, particles are exchanged among processes, with particles being sent to the processes that owns the blocks for the particles to continue the advection. Communication is done via point-to-point nonblocking sends and receives. Although non-blocking communication methods are used, the communication is coordinated synchronously, since a process must wait on its neighbors , who must wait on their neighbors, and so forth until the dependency extends to almost all processes. Even though communication is a crucial part of our system, here we focus on computational load imbalance, as it was found to be the main bottleneck to performance. Computation continues till all particles have terminated or the maximum number of rounds, a user specified parameter, is reached. In our system, once a process finishes advecting its particles for one round, it must wait on all other processes to finish advection before particles can be exchanged. Load imbalance can occur if some processes require more computation time for particle tracing, forcing all other processes to wait. The amount of computation time a process needs for a round is the sum of the work of all its blocks for that round, so the assignment of blocks to processes is the most crucial element in affecting computational load balance. A load balanced partitioning should result in all processes having similar compute time, which will minimize waiting time. This condition should be satisfied for every round. The problem is further compounded by the fact that blocks may require different amounts of work for different rounds, so changing the assignment of a block to another process may improve the load balance of some rounds, but could also worsen the load imbalance of other rounds. An important property of our system is that the workload for a block only depends on the particles within, but not to which partitions it belongs. For example, in <ref type="figure" coords="2,408.70,525.61,20.44,8.21">Fig. 2</ref> , in the first round, the particle in the lower left block is advected, and stops when it crosses the block boundary. Even though the block that the particle traveled to is also owned by the same process, the particle does not continue advection until the second round. The result of this behavior is that blocks will require the same amount of work independent of which process owns it. This simplifies our optimization problem, since block workload amounts stay constant, and do not have to be adjusted based on whether any of the block's neighbors are present in the same partition. If this were not the case, and advection could continue after a block boundary is reached, several issues exists. First, since the workload of a block is dependent on whether its neighbors are also in the same partition, the optimization problem becomes more complex to solve. Second, in our experience, it causes the computation to be more load imbalanced, since it is possible that some partitions may have much more work than others. For example, if a vortex spans two blocks and those two blocks are owned by the same process, then that process will take much longer to finish a round, as many of its seeds will be advected the maximum number of steps. Due to these challenges, existing techniques either analyze the flow field to ensure such a situation never happens, such as the algorithm proposed Yu et al. <ref type="bibr" coords="2,509.24,726.22,14.94,8.21" target="#b23">[24] </ref>by limiting the trace lengths, or allocates extra processes to monitor the workload and dynamically reassigns particles among the processes, the method used by Pugmire et al. <ref type="bibr" coords="3,156.09,332.20,13.74,8.21" target="#b20">[21]</ref>. While stopping particles after a block boundary is reached might lead to poor utilization of resources, Section 6 demonstrates that since our method creates a balanced workload , the idle time between consecutive rounds is minimized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WORKLOAD-AWARE DATA PARTITIONING</head><p>How the partitioning problem is modeled as an optimization problem is detailed in Section 4.1, and the solution to the optimization problem is shown in Section 4.2. For now, it is assumed that the exact workload for each block in every round is known a priori, and the discussion of how the block workload is estimated is deferred to Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mathematical Model 4.1.1 Partitioning and Process Workload</head><p>The purpose of our partitioning method is to distribute the data blocks to the compute processes. The desired outcome of partitioning is to minimize the sum of workload differences among the processes, in all rounds. To help ensure a more even workload distribution, we do not limit blocks to being assigned to only one process. Instead, a bock can be assigned to multiple processes, if necessary. In this case, weights are assigned to each copy of the block, designating the percentage of particles to be processed for this copy. These weights are determined by the solution of the optimization problem. Throughout this section, we assume that the number of data blocks is m, the maximum number of rounds is n, and the number of processes is p. The goal of our partitioning algorithm is to solve for the workload ratio, P ki , which represents the percentage of the total amount of work from block i that process k is responsible for. We note that P ki remains the same for all rounds, which means both the data and the workload assignment is fixed throughout the computation. This is done in order to minimize the communication and I/O cost. In the case that P ki can only be a binary value of 0 or 1, each block is assigned to one process. On the other hand, if the value of P ki is in <ref type="bibr" coords="3,196.20,654.26,9.46,11.66">[0,</ref><ref type="bibr" coords="3,206.66,656.51,6.14,8.21" target="#b0"> 1]</ref>, then a block can be assigned to multiple processes, each of which is responsible for advecting a portion of the particles in the block determined by the value P ki . Being able to have multiple copies of a data block at once is part of the parallel streamline system employed by Pugmire et al. <ref type="bibr" coords="3,31.50,706.32,13.74,8.21" target="#b20">[21]</ref>. <ref type="figure" coords="3,51.45,706.32,20.92,8.21">Fig. 3</ref>shows an example of how the workload of two blocks can be divided among four processes. While we require that the workload ratio P ki be the same for all rounds, the workload of a block will change from round to round. This is because particles will advect throughout the domain and hence a block may have different numbers of particles in different rounds. In our model, we maintain a different workload value for each block at every round. We denote the workload of block i in round j as L i j . Given all workload values for every block in round j, and their workload ratios, P k j , the total workload assigned to process k in round j, C k j , is the sum of the workload from all blocks in round j multiplied by their workload ratios, or more specifically: </p><formula>C k j = m ∑ i=1 P ki × L i j , j = 1 ...n, k = 1 ... p (1) </formula><p>We note that since processes are assigned a percentage of the total workload for a block, the sum of the workload ratios for a specific block is always 1, that is: </p><formula>p ∑ k=1 P ki = 1, i = 1 ...m (2) </formula><p> Based on Equation 1, we can write the workload of all the processors in one round as a column vector. For p processes in the j-th round, </p><formula>C j = [C 1 j ,...,C p j ] T is: C j = ⎡ ⎢ ⎣ C 1 j . . . C p j ⎤ ⎥ ⎦ = ⎡ ⎢ ⎣ P 11 ... P 1m . . . . . . . . . P p1 ... P pm ⎤ ⎥ ⎦ P ⎡ ⎢ ⎣ L 1 j . . . L m j ⎤ ⎥ ⎦ (3) </formula><p>In Equation 3, the p × m matrix P contains all the workload ratios. </p><p>We refer to this as the partitioning matrix. <ref type="figure" coords="3,455.88,337.58,21.70,8.21">Fig. 3</ref>illustrates a 4 × 2 partitioning matrix. Given the block workloads, L i j , for all blocks in all rounds, the goal of our partitioning algorithm is to solve the partitioning matrix to achieve a load balanced streamline computation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Partitioning Cost</head><p>In order to obtain a partitioning matrix that can give us the best load distribution, since we are modeling the partitioning as an optimization problem, we need to define a cost function to minimize. As stated previously , it is important to minimize the difference between the workloads of all processes in each round to reduce process idle time. To model the degree of load imbalance in each round, we can use the variance of the workload of each process. When all processes have similar amounts of workload, the variance will be small; otherwise, a larger variance will be observed if any process ends up having a much larger workload than the average case. Since our goal is to find a partitioning matrix that can lead to balanced workload in all rounds, we define the cost function as the variance of the processor workloads, summed over all rounds. According to its definition, the variance of a set of elements x 1 ,...,x p is the average of the square minus the square of the average, denoted as </p><formula>1 p ∑ p k=1 x 2 k − ( 1 p ∑ p k=1 x k ) 2 . </formula><p>As shown below, this allows us to derive the cost function, f (P), as a quadratic form of the unknown partitioning matrix P. Because P is a matrix of real numbers, both first and second derivatives of f (P) can be analytically computed. As a result , searching for such a matrix can be done by conventional nonlinear programming algorithms such as gradient descent <ref type="bibr" coords="3,474.91,600.71,9.52,8.21" target="#b0">[1]</ref>. Here we show the derivation of the cost function, f (P), as a quadratic form of P. By denoting VAR(C 1 j , ...,C p j ) as the variance of process workloads, C k j , in the j-th round, the cost function, f (P), as the sum of variances from all the n rounds can be expressed by Equation 4 below. By substituting the vector C j as </p><formula>C j = P[L 1 j , ..., L m j ] T from </formula><p>Equation 3, we obtain the cost function f (P): </p><formula>f (P) = n ∑ j=1 VAR(C 1 j , ..., C p j ) = n ∑ j=1 ( 1 p p ∑ k=1 C 2 k j − ( 1 p p ∑ k=1 C k j ) 2 ) </formula><p>1787 NOUANESENGSY ET AL: LOAD-BALANCED PARALLEL STREAMLINE GENERATION ON LARGE SCALE VECTOR FIELDS </p><formula>= n ∑ j=1 C T j ( 1 p ⎡ ⎢ ⎢ ⎣ 1 0 ... 0 0 1 ... 0 . . . . . . . . . . . . 0 0 ... 1 ⎤ ⎥ ⎥ ⎦ − 1 p 2 ⎡ ⎢ ⎢ ⎣ 1 1 ... 1 1 1 ... 1 . . . . . . . . . . . . 1 1 ... 1 ⎤ ⎥ ⎥ ⎦ )C j (4) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solving Partitioning Matrices</head><p>In solving the partitioning matrix to minimize our cost function, we allow the value P ki to be any value in <ref type="bibr" coords="4,157.17,131.89,9.46,11.66">[0,</ref><ref type="bibr" coords="4,167.63,134.14,6.14,8.21" target="#b0"> 1]</ref>, as mentioned earlier. That is, the workload for each block can be shared by multiple processes and thus replication of data blocks is allowed. Empirically, we found that disallowing the workload of a block to be divided often results in worse load balancing. A simple example is to partition two data blocks that have 100 and 1000 units of workload respectively to two processes. Without data replication, each block can only be assigned to one process, and thus the workload difference will be 900 units regardless of the block assignment. On the other hand, we can achieve a better load balancing if we give 45% of the workload from the second block and 100% of the workload from the first block to one process, and the remaining work to the other. The disadvantage of allowing block replication is an increase of the memory requirement for the program, which is discussed in Section 6.2. To find the partitioning matrix P with block replication, conventional optimization algorithms can be applied to minimize the partitioning cost, although we need to address two issues. First, for m blocks and p processors, this optimization problem contains m × p unknowns , which can be slow to solve for data with thousands of blocks, and supercomputers with hundreds to thousands of processors. Second , the solution should satisfy the constraint that workload ratios are bounded in <ref type="bibr" coords="4,65.62,341.83,9.46,11.66">[0,</ref><ref type="bibr" coords="4,76.08,344.08,6.14,8.21" target="#b0"> 1]</ref>, and the sum of workload ratios of all processes for any given block should be 1. It can be costly to solve the optimization problem with these constraints. To accelerate the optimization process, rather than directly solving the partitioning matrix P that minimizes Equation 4, we decompose the partitioning problem that involves more than two processes into a sequence of subproblems that involve two processes only. Hereafter we refer to partitioning data blocks to more than two processes as Multi-way Partitioning and partitioning that involves only two processes as 2-way Partitioning. Solving 2-way partitioning involves fewer unknowns and simpler constraints versus the original cost function, which results in a problem that is much more computationally tractable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">2-Way Partitioning</head><p> To understand why a 2-way partitioning matrix involves fewer unknowns , it is to be noted that in a 2-way partitioning matrix P, any element in the second row P 2i is one minus the corresponding element in the first row, namely, P 2i = 1 − P 1i . In other words, in solving the 2-way partitioning matrix P we only need to consider the first row. The cost function, f (P), for 2-way partitioning is still in a quadratic form, involving the first row of P, which is shown in Equation 6. This quadratic form for the cost function allows us to solve the optimization problem using conventional optimization algorithms, while the cost function contains fewer unknowns. In addition, the only constraint to be satisfied is that all elements in P are to be in the <ref type="bibr" coords="4,209.25,588.86,9.71,8.21">[0,</ref><ref type="bibr" coords="4,221.46,588.86,7.47,8.21" target="#b0"> 1] </ref>range. This makes the optimization problem much easier to solve. First, we require a new derivation of the cost function for 2-way partitioning. The column vector C j contains the workload C 1 j and C 2 j of two processes in the j-th round. From Equation 3, we can rewrite the column vector C j as a linear function of the first row: <ref type="figure" coords="4,285.12,151.35,18.50,7.37">Fig. 4</ref>. An example of hierarchical partitioning. An initial workload matrix is used to generate a partitioning matrix, where each row represents a partition. Then the workload matrix is weighted by the resulting partition weights to generate two new workload matrices. More partitions are generated recursively. </p><formula>C j = C 1 j C 2 j = P 11 ... P 1m P 21 ... P 2m ⎡ ⎢ ⎣ L 1 j . . . L m j ⎤ ⎥ ⎦ = P 11 ... P 1m 1 − P 11 ... 1 − P 1m ⎡ ⎢ ⎣ L 1 j . . . L m j ⎤ ⎥ ⎦ P </formula><formula>= +L 1 j ... +L m j −L 1 j ... −L m j ⎡ ⎢ ⎣ P 11 . . . P 1m ⎤ ⎥ ⎦ + 0 ∑ m i=1 L i j </formula><formula>(5) </formula><p>To simplify the notation, we denote this linear function as C j = B j p 1 + c j where p 1 is a column vector equal to the transpose of the first row vector in the partitioning matrix P. By substituting this linear function for C j in the original cost function in Equation 4, we obtain the cost of a 2-way partitioning matrix P in Equation 6 shown below, which is a quadratic form of the vector p 1 : </p><formula>f (P) = n ∑ j=1 (B j p 1 + c j ) T ( 1 2 1 0 0 1 − 1 4 1 1 1 1 )(B j p 1 + c j ) </formula><formula>(6) </formula><p>4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Penalty for Data Replication</head><p>When solving the partitioning matrix with block replication allowed, a trivial but undesired solution is to copy each data block to all processes , that is, all elements of matrix P have an equal weight. While this partitioning certainly achieves perfect load balancing, it means that each process is required to hold in memory a complete copy of the data, an undesired solution for large datasets. To avoid this trivial solution, we want to distribute the workload of a block to only a few processes to minimize memory and I/O overhead. In order to achieve this goal, we add extra terms to the cost function to penalize the cases where a block is distributed to too many processes . Too much duplication is unwanted because of limited available system memory and to limit I/O overhead. In the case of 2-way partitioning, the penalty is high for a partitioning matrix where all elements have a value of 0.5; otherwise, if most elements are 0 or 1, the penalty is low. This can be realized by using the penalty term shown below in Equation 7, which involves the first row, [P 11 , ..., P 1m ], of the partitioning matrix P. The penalty is the sum of all elements in the first row, each of which is weighted by one minus its value. It can be shown that if all elements are either 0 or 1, this weighted sum will be zero. Otherwise, since the values of the elements in P are in <ref type="bibr" coords="4,500.76,604.28,9.71,8.21">[0,</ref><ref type="bibr" coords="4,512.65,604.28,6.47,8.21" target="#b0"> 1]</ref>, the penalty reaches a maximum when all elements are 0.5. </p><formula>g(P) = m ∑ i=1 (1 − P 1i ) × P 1i (7) </formula><p>By adding the penalty term, the final cost function for partitioning, </p><formula>h(P), becomes: h(P) = f (P) + λ g(P) (8) </formula><p> where λ is used as a trade-off factor between the partitioning cost defined in Equation 4 and the penalty defined in Equation 7. More details about the influence of this parameter is discussed in Section 6.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Solving 2-way Partitioning via Nonlinear Programming</head><p> To find the matrix P that minimizes h(P), the selection of which optimization algorithm to use is crucial for its performance. Since the second derivative of h(P) can contain negative eigenvalues, h(P) can be non-convex. Under these circumstances, there is no analytical form to compute the global minimum. Therefore, we need to iteratively search for the local minimum. Since the second derivative, i.e., Hessian matrix, of h(P) exists, using Newton's method to minimize h(P) should ideally take only a few iterations to converge. Newton's method, however, requires the Hessian matrix of h(P), whose size is m × m for m blocks. Since analytically computing the m × m Hessian matrix can be time consuming, we choose to utilize the quasi-Newton method, which approximates the Hessian matrix locally. We use an implementation of the quasi- Newton method in the nonlinear programming solver OPT++ <ref type="bibr" coords="5,264.66,195.96,13.74,8.21" target="#b17">[18]</ref>, where the Hessian matrix is approximated by the limited-memory version of Broyden–Fletcher–Goldfarb–Shanno (L–BFGS) algorithm <ref type="bibr" coords="5,31.48,225.84,13.74,8.21" target="#b18">[19]</ref>. In our tests, the quasi-Newton method obtained a comparable optimization result with those by Newton's method, but the quasi- Newton method took considerably less time, since the computation of the Hessian matrix is avoided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Hierarchical Multiway Partitioning</head><p>In order to obtain a multi-way partitioning, the 2-way partitioning is hierarchically applied in order to generate the correct number of partitions . In the beginning, our algorithm takes the original workload values of all blocks over all rounds, and computes a 2-way partitioning matrix, where each of the two rows represents a partition. In order to continue subdividing, new workload values must be computed for each new partition. For each partition, if it has a non-zero workload ratio for a block, the workload of this block in all rounds are weighted by the workload ratio. The weighted workloads from the assigned blocks form the new workload values for that partition. Once new workload values are computed for each partition, 2-way partitioning is applied, and the process continues until the desired number of partitions is reached. Because the number of partitions after splitting is always two, currently our method only supports creating partitions that are a power of two. <ref type="figure" coords="5,41.44,435.75,22.21,8.21">Fig. 4</ref>is an example of how the results of 2-way partitioning is used to generate new workload values. Here the block workloads are organized as a matrix in which each row represents the workload of all rounds of a block. For the i-th row, if the workload ratio P ki is nonzero, this row is weighted by the workload ratio to become a row in the new workload matrix for the new partition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FLOW GRAPH AND BLOCK WORKLOAD ESTIMATION</head><p>So far it has been assumed that the exact workload for each block at every round is known beforehand. Unfortunately, this knowledge cannot be obtained unless the actual particle advection calculations are performed. Because of this, we propose a block workload estimation algorithm designed to efficiently estimate the workload of each block. In order to estimate the block workload, first an abstract representation of the vector field, or flow graph, is computed, then initial seed locations are used in conjunction with the flow graph to estimate the workload of each block. A flow graph is an abstract representation of the vector field encoded as a directed graph. Each block in the domain is represented as a node. For every pair of adjacent blocks, their corresponding graph nodes are connected by two weighted, directed edges. The weight of an edge from node A to node B represents the probability that a particle located in A will travel to B. Adjacent nodes will have two edges in opposite directions because the probability of a particle traveling from A to B may be different than the probability going from B to A. In order to calculate the edge weights, particles are placed uniformly within each block, and are advected until they exit the block. The weight of an edge going to a neighbor is the ratio between the number of particles that reach that neighbor block and the total number of particles. The sum of all edge weights originating from one node is always 1 or less. The sum can be less than 1 if particles travel outside the vector field domain, or if particles encounter a critical point within the block. Once the flow graph is computed in the preprocessing step, the workload for each block can be estimated at run time for a given set of streamline seeds. The input is a flow graph and a seed set. The output is an estimated number of particles in each block per round. The initial seed locations are examined to see how many seeds each block will begin with. These values are used to initialize the number of particles for a block's corresponding node. For each round, every edge in the flow graph is traversed, and the number of current particles in the source node is multiplied by the edge weight, and then added to the number of particles of the destination block for the next round. This continues until the maximum number of rounds is reached. At this point, the estimated number of particles for each round can be used directly as the workload of a block, but in practice the number of particles do not correlate perfectly with computation time. This problem arises because of the difference in flow field characteristics for each block. For some blocks, particles may tend to travel only a few steps before exiting. On the other hand, particles in other blocks may have a tendency to require several steps. For example, a block containing a vortex will require more time to trace its particles versus a block without any vortices, assuming they both contain the same number of particles. Therefore, a more accurate measure of the computation time for a block is the number of advection steps generated. In order to estimate this, the average number of advection steps per particle is needed for each block. This value is calculated in the flow graph generation step. As particles are being advected in order to calculate edge weights, the total number of advection steps calculated is kept, and the average can be calculated from there. The final value for the estimated workload for a block is then the estimated number of particles multiplied by the average number of advection steps per particle. These estimated values are then used as input to the previously described workload-aware partitioning algorithm discussed in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p> To test the performance of our algorithm, strong scaling and weak scaling tests were performed. Four datasets were used in our experiments. The first dataset is a thermal hydraulics simulation generated by the Nek5000 solver, which we refer to as Nek. The dataset was created from a simulation of the MAX experiment <ref type="bibr" coords="5,453.99,585.04,13.74,8.21" target="#b16">[17]</ref> , based on the largeeddy simulation of Navier-Stokes equations, and resampled into a regular grid of 2048 × 2048 × 2048. The dataset Ocean, with a resolution of 3600 × 2400 × 40, is the result of an eddy resolving simulation with 1/10 horizontal spacing at the equator <ref type="bibr" coords="5,433.06,624.88,13.74,8.21" target="#b15">[16]</ref>. The third dataset, Flame, is a simulation of fuel jet combustion <ref type="bibr" coords="5,431.85,634.85,9.71,8.21" target="#b7">[8,</ref><ref type="bibr" coords="5,444.12,634.85,10.64,8.21" target="#b11"> 12]</ref>, based on S3D, a solver for fully compressible Navier-Stokes flow <ref type="bibr" coords="5,448.97,644.81,9.52,8.21" target="#b2">[3]</ref>. The spatial resolution is 1408 × 1080 × 1100. The fourth dataset, Plume, was produced by a simulation of the thermal downflow plumes on the surface of the sun, and has a spatial resolution of 504 × 504 × 2048. The size of all four test datasets are summarized in <ref type="figure" coords="5,408.87,684.65,25.50,8.21" target="#tab_1">Table 1</ref>, and an image of each dataset is shown in <ref type="figure" coords="5,336.97,694.61,19.80,8.21">Fig. 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Parallel Streamline Generation Performance</head><p>To test the performance of our parallel streamline generation algorithm with our data partitioning scheme, tests were conducted on Surveyor,  an IBM Blue Gene/P (BG/P) supercomputer at Argonne National Laboratory . This BG/P system includes 1024 compute nodes. Each node contains 4 cores and 2GB of system memory, for a maximum of 4096 cores. Several timings tests were performed to study the performance gain of parallel streamline generation using our data partitioning scheme. For each test, partitions were generated by using our workload estimation and partitioning method. We compare our algorithm with a recent work published in 2011 by <ref type="bibr" coords="6,124.43,706.33,67.21,8.21">Peterka et al. [20]</ref>, where a round-robin data assignment scheme is used, in which blocks are assigned to processes using a three-dimensional block-cyclic distribution. Similar to <ref type="bibr" coords="6,22.50,736.22,78.86,8.21">Peterka et al.'s system</ref>, our streamline generation was also based on a particle advection library called OSUFlow, which is also used in the package VAPOR from National Center for Atmospheric Research <ref type="bibr" coords="6,522.80,636.08,9.52,8.21" target="#b4">[5]</ref>. To study the efficacy of the workload estimation algorithm described in Section 5, we collected the actual workload in each of the blocks in every round and used those exact values to generate partitions using our partitioning algorithm. The results of using these partitions are compared with the results from our estimation method. We note that the purpose of collecting the actual workload is only for experimentation , since it requires the generation of all streamlines beforehand , which is obviously not a plausible solution. In the following, we differentiate the partitioning results generated by using the actual workload numbers and our estimated numbers with the terms </p><formula>(d-1) (d-2) (d-3) (d-4) (d-5) </formula><formula>(a) </formula><p>(b) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>workload-based partitioning and estimated-workload-based partition</head><p>ing , respectively. We conducted both strong scaling and weak scaling tests involving all of our datasets, where strong scaling means the total amount of work remains the same as the process count increases, and weak scaling means the workload increases proportionally with the process count. For ideal parallel speedup, strong scaling results should show that the computation time decreases linearly as the process count increases , while weak scaling results should show the computation time remaining constant. In our strong scaling tests, a constant number of particles were used. In the weak scaling tests, more particles are generated proportionally as more processes are used. <ref type="figure" coords="7,221.89,365.79,27.65,8.21" target="#tab_1">Table 1</ref>lists the number of total particles for strong scaling and number of particles per process for weak scaling tests in our experiments. In all experiments , particles were allowed to go up to 50 rounds. For scaling tests, the particles were placed randomly throughout the domain, while the tests with uneven placed particles are discussed in Section 6.3. Each dataset was divided into 4K blocks, and processor counts of 128, 256, 512, 1024, 2048, 4096 were used. The selection of block size and the parameter λ in Equation 8, and how streamline generation time can be influenced by them, are discussed in more detail in Section 6.2. <ref type="figure" coords="7,41.46,466.34,21.30,8.21" target="#fig_1">Fig. 6</ref>lists the results of all tests. The first two columns graph the total computation time, including communication, for each dataset. <ref type="figure" coords="7,31.50,486.26,21.34,8.21" target="#fig_1">Fig. 6</ref>(a-1) and (a-2) show the timings for the dataset Nek. It can be seen that as more processors were used, the amount of speedup using our algorithm compared to the round-robin scheme increased for both strong scaling and weak scaling tests. For strong scaling tests on 4K processors, both actual-workload-based partitioning and estimatedworkload-based partitioning are 5 times faster than the round-robin method. <ref type="figure" coords="7,64.73,546.02,38.37,8.21" target="#fig_1">Fig. 6 (b-1</ref>) and (b-2) list the timings for the Plume dataset, which also show a similar behavior when using more processors. <ref type="figure" coords="7,31.51,565.94,36.06,8.21" target="#fig_1">Fig. 6 (c-1</ref>) and (c-2) show the timings for the dataset Ocean. Although using our partitioning method still leads to better performance, the performance gain versus round-robin flattened out at high processor counts. For this dataset, the percent speedup between using estimatedworkload-based partitioning and round-robin ranged from 30% to 80% for strong scaling. For weak scaling tests, the performance numbers remain close when using less than 1K processors, but the difference increases to about 20% to 40% speedup over round-robin for larger processor counts. It can also be seen that partitioning based on the actual workload is faster than the partitions based on estimated workload , although estimated-workload-partitioning consistently performs better than round-robin. For the Flame dataset, the speedup compared to round-robin in strong scaling and weak scaling tests were about 40% to 120%, and 40% to 70%, respectively. Partitioning based on the actual workload led to only slightly better performance than that based on the estimated workload. From the first two columns of <ref type="figure" coords="7,155.53,726.23,20.64,8.21" target="#fig_1">Fig. 6</ref> , it can be seen that the performance difference between our method and round-robin increases as the numbers of processors increase for the Nek and Plume datasets, while the performance difference between the two methods flattens out for Ocean and Flame at larger processor counts. To explain this behavior , it was found that the degree of speedup is related to the workload difference among the data blocks. The third column in <ref type="figure" coords="7,497.97,92.99,21.50,8.21" target="#fig_1">Fig. 6</ref>shows the average workload of all blocks for the different datasets, where all blocks are laid out along the x-axis, and the y-axis shows the average workload from all rounds. The distributions of workload in <ref type="figure" coords="7,510.94,122.88,21.15,8.21" target="#fig_1">Fig. 6</ref> (a- 3) and (b-3) contain several peaks, implying that some blocks receive more work than other blocks, mostly due to the convergence of particles . For this case, our algorithm can properly replicate the data blocks and distribute the work more evenly, and hence performed better than the round-robin scheme. In contrast, the block workloads in <ref type="figure" coords="7,511.31,172.68,20.96,8.21" target="#fig_1">Fig. 6</ref> (c- 3) and (d-3) are more uniform than that in <ref type="figure" coords="7,443.76,182.64,20.59,8.21" target="#fig_1">Fig. 6</ref>(a-3) and (b-3), which means that for Ocean and Flame, the blocks generally have more even workloads, which presented a simpler case and helped the round-robin scheme to narrow the performance gap between it and our algorithm. Nevertheless, our algorithm performs consistently better than round- robin. <ref type="figure" coords="7,304.08,242.88,21.09,8.21" target="#fig_2">Fig. 7</ref>(a) displays the data block with highest average workload in Nek, and the streamlines that originate from this block. From these high curvature streamline segments, it can be seen that these streamlines take a large number of steps within this block, therefore causing a longer compute time for particle advection. Similarly, <ref type="figure" coords="7,497.99,282.72,21.15,8.21" target="#fig_2">Fig. 7</ref> (b) reveals the block with the highest average workload in Plume. The image clearly shows that this block contains the center of a circular sink. Therefore, more and more particles are gathered toward this block in the later rounds, thus causing higher block workload. To further demonstrate that our partitioning algorithm produces load balanced partitions, we measured the process idle time in all rounds. The average idle time per process in all rounds is listed in the fourth column of <ref type="figure" coords="7,359.21,362.87,20.51,8.21" target="#fig_1">Fig. 6</ref>. The results were collected from our strong scaling test using 2048 processes. By comparing <ref type="figure" coords="7,469.51,372.83,20.69,8.21" target="#fig_1">Fig. 6</ref>(a-4) and (b-4) with (c-4) and (d-4), it can be seen that when the round-robin partitioning scheme is used, the average waiting time for Nek and Plume is much higher than that of Ocean or Flame, which again confirms that the distribution of workload among blocks in the latter two datasets are more even. We can also see that estimated-workload-based partitioning almost always leads to smaller process idle time than roundrobin . Smaller idle times indicate that the compute times are similar to each other, which proves that our algorithm produced more load balanced partitions. When using estimated-workload-based partitioning for Ocean, we observed a few rounds having higher wait time than round-robin, although the overall parallel performance of the entire run using our algorithm was still better. When using estimated-workload-based partitioning for datasets Plume and Ocean, idle times are consistently longer and streamline generation is slower than partitioning based on the actual workload. Our hypothesis is that the amount of error of the workload estimation was higher for those datasets, which negatively impacted the performance . To verify this hypothesis, we computed the average actual workload and the average estimated workload over all rounds, which are plotted in the fifth column in <ref type="figure" coords="7,416.03,572.52,68.36,8.21" target="#fig_1">Fig. 6. Comparing</ref><ref type="figure" coords="7,487.15,572.52,21.44,8.21" target="#fig_1">Fig. 6</ref>(a-5) and (d-5), we can see that the difference between the actual workload and estimated one in <ref type="figure" coords="7,357.66,592.45,21.78,8.21" target="#fig_1">Fig. 6</ref>(b-5) and (c-5) are larger, which means that for Plume and Ocean, the estimation error is larger than the other two datasets, thus leading to less efficient partitions versus the partitions produced when using the actual workload. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Parameter Specification</head><p> The parameter introduced in Equation 8, λ , represents a trade-off between the variance of the workload of the resulting partitions and the amount of block replication. Smaller values of λ leads to solutions with better load balancing, since it permits a higher degree of data replication, while larger values of λ has smaller memory overhead. In Equation 8, since g(P) is a sum of products of several workload ratios, and f (P) measures the variance of workload in the compute processes, they usually have different scales. Therefore, the choice of λ needs to consider f 's value range, such that λ g will have a value close enough to f (P) so as to influence the final optimization outcome. We found that λ is usually data dependent, which makes it more difficult to choose a proper value without knowing the characteristics of the input data. To address this issue, we do not directly specify the value of λ . Instead, we specify another parameter, λ , which is independent of the scale of f . We then compute λ based on λ and a sample of f , denoted as f sample , as shown in Equation 9. We select f sample using f (P 0 ) where P 0 is the partitioning matrix that assigns the blocks to the compute processes using a round-robin scheme. We use this to get an estimate of the typical value of f (P), since round-robin data assignment is one of many possible choices for data partitioning. </p><formula>λ = λ f sample (9) </formula><p>We conducted tests with different values of λ , and measured the performance of both the optimization solver and the performance of the streamline generation using the resulting partitions. <ref type="figure" coords="8,236.67,576.83,22.46,8.21">Fig. 8</ref>(a) presents the performance of both the partitioning solver and corresponding particle advection using two datasets with varying values of λ . The solver was tested on a machine equipped with two 2.27 GHz Xeon E5520 CPUs. Advection times were the result of parallel runs using the resulting partitions. We note that while our streamline generation algorithm is designed to run on large scale supercomputers, our data partitioning optimization problem currently is solved separately using a sequential workstation. <ref type="figure" coords="8,136.76,656.52,33.78,8.21">Fig. 8 (b)</ref> shows the percentage of extra blocks after partitioning with replication. It can be seen that as λ decreases, more blocks are replicated. As a result of these tests, λ was empirically set to 0.1, since λ values smaller than 0.1 resulted in the optimization solver becoming much slower, while the performance gains of the streamline generation computation diminishes. The solution of our optimization model with different degrees of data replication was studied to see how it can influence the memory requirement for the compute processes. The maximum amount of memory that will be needed by one compute process was studied under different parameter settings. The datasets Nek and Ocean were divided into 1K, 2K, 4K, and 8K blocks, which were used to produce partitions for 128, 256, 512, and 1024 processes, using our partitioning algorithm. <ref type="figure" coords="8,338.66,233.00,20.88,8.21" target="#fig_4">Fig. 9</ref>(a) shows the amount of memory needed for one process, as compared to a partitioning that does not allow replication, where the y-axis shows the additional memory required in as a percentage of the original data size. It can be seen that as the dataset is divided into more blocks, the additional memory needed for partitions that allow data replication continues to decrease. The number of blocks also influences the time to solve the partitioning matrix, since the number of workload ratios, the unknown to solve, depends on the number of blocks and number of partitions. <ref type="figure" coords="8,501.55,313.15,33.94,8.21" target="#fig_4">Fig. 9 (b)</ref>shows the performance of the optimization solver for the datasets Nek and Ocean. It can be seen that as the datasets are partitioned into more blocks, more time is needed to solve the partition matrix. While the time complexity of our optimization algorithm depends on the number of iterations to optimize the cost function, h(P), in Equation 8, our observation was that the computation time was proportional to the squared of the number of blocks. It should be noted that currently our optimization algorithm is solved separately on a single machine. In the future, we would like to parallelize the optimization algorithm to improve the performance. <ref type="figure" coords="8,295.08,423.19,21.95,8.21" target="#fig_4">Fig. 9</ref>(b) also shows that the number of partitions has a smaller influence on the time needed to solve the partitioning matrix, as compared to the number of blocks. This is because that multi-way partitioning is hierarchically solved via a sequence of 2-way partitioning. The first 2-way partitioning involves more blocks to be solved than later 2-way partitioning, and thus the majority of the computation time is spent solving the first one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance for Uneven Seeding</head><p>In addition to placing seeds randomly over the entire domain, it is also important to consider how our algorithm performs when seeds are unevenly distributed. Often times seeds are strategically placed in order to reveal specific flow features. The definition of a feature, though, is application-dependent. Our strategy for uneven seeding is based on <ref type="figure" coords="8,297.52,566.36,21.00,8.21" target="#fig_2">Fig. 7</ref>, where it can be observed that the busiest blocks contain more turbulent flow. Statistics collected from the original strong scaling tests were used to determine the 40 busiest blocks. Seeds were then placed only in those blocks. The strong scaling results of uneven seeding are shown in <ref type="figure" coords="8,507.52,606.67,24.26,8.21" target="#fig_6">Fig. 10</ref>, using 256K seeds and 20K seeds in Nek. By comparing <ref type="figure" coords="8,496.29,616.62,26.20,8.21" target="#fig_6">Fig. 10</ref>(a) with <ref type="figure" coords="8,302.90,626.58,20.51,8.21" target="#fig_1">Fig. 6</ref>(a-1), we can see that even if seeds are unevenly placed, our method still has better scalability than round-robin. In the case of 20K seeds, though, round-robin seems to scale just as well as our method. The red curve in <ref type="figure" coords="8,347.95,656.47,25.83,8.21" target="#fig_6">Fig. 10</ref>(b) shows the results of our method with λ being set to 0.1. Initially, the wall time of our method is faster than round-robin, but beginning with 1K processes, the wall time begins to increase, and at 4K processes is actually greater than round-robin. The main reason for this decrease in performance is because even though better load balancing is being achieved from the data blocks being replicated, this data replication will cause extra communication. For example, sending a set of seeds to a block that is replicated 100 times will require 100 sends, whereas if that block is only replicated 10 times, then only 10 sends are needed. As fewer seeds are placed, streamline computation will take less time overall, and the communication overhead will begin to dominate the wall time. To lessen the communication overhead, we can change the parameter λ to reduce the replication of data. <ref type="figure" coords="9,117.97,93.05,25.91,8.21" target="#fig_6">Fig. 10</ref>(b) shows the results when setting λ to 1.0 and 3.0. It can be seen that as λ increases, the wall time becomes slower for less than 1K processes, but scalability improves for more than 1K processes. Overall, our tests indicate that if a streamline run uses a large number of seeds and requires a large amount of computation, then our method will provide better performance than round-robin, even if the seed placement is uneven. On the other hand, if the number of seeds used is low, when adding in the preprocessing time, our method may not be beneficial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Flow Graph Construction and Estimation</head><p>In addition to partitioning and particle advection, the computation time required for an entire streamline computation also includes workload estimation and flow graph construction. Workload estimation needs to be recomputed when a new set of seeds are specified. The compute time for this step, though, is generally very fast. For example, estimating the workload of 4K blocks with an initial 256K seed set requires roughly 1.5 seconds. The preprocessing step of constructing the flow graph can be quite time consuming compared to the workload estimation step. Because the edge weight calculations for nodes in a flow graph can be independently computed, the graph construction can easily be performed in parallel. Our parallel implementation assigns an equal number of blocks to each process. <ref type="figure" coords="9,117.09,329.11,25.43,8.21">Fig. 11</ref> (a) shows the performance of computing the flow graph in parallel. It can be seen that while using more processes can accelerate the performance, the speedup obtained by using more than 512 processes becomes sub-linear. The reason behind the non-optimal scalability for larger process counts is because each block is assigned to only one process. Since some blocks will take more time than others, the wall clock time is bounded by the slowest processes.  When adding the flow graph preprocessing time to the run time of our tests in <ref type="figure" coords="9,60.62,429.58,20.67,8.21" target="#fig_1">Fig. 6</ref>, the resulting time will often be close to or be slower than the corresponding round-robin time. Fortunately , the flow graph only needs to be calculated once for a dataset, so the pre-processing time will be amortized over multiple runs. In addition to the performance, the accuracy of the flow graph is also crucial . To decide the number of seeds per block for graph construction, we constructed flow graphs with 2.5K, 5K, 10K, and 50K seeds per block, and compared the weights of the same edge between the different versions to see how the number of seeds influenced the resulting edge weight. The average difference between different versions of the same edge is 0.0074. Considering that the possible range of an edge weight is <ref type="bibr" coords="9,65.33,579.82,9.71,8.21">[0,</ref><ref type="bibr" coords="9,76.87,579.82,6.47,8.21" target="#b0"> 1]</ref>, edge weights remain relatively stable when more seeds are used for the flow graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Performance Overhead</head><p> Since our algorithm introduces extra stages, including flow graph construction , workload estimation, and partitioning, it is crucial to verify whether these additional stages will cause too much overhead for our method to be beneficial. To analyze the time complexity of these stages, it should be noted that the performance of these extra stages is independent of the number of particles used at run time. For flow graph construction, its performance mainly relies on the number of blocks and the number of seeds used for preprocessing, not the number of seeds at run time. Therefore, the flow graph can be computed once and be re-used for an arbitrary number of seeds. Moreover, since flow graph construction can be accelerated by parallel processing, the overhead can be further reduced. </p><p> Both workload estimation and flow field partitioning need to be recomputed when given a new set of seeds. Their performance depends on the number of blocks and the number of rounds. For workload estimation , the time complexity is linearly proportional to the number of blocks. For flow field partitioning, from <ref type="figure" coords="9,451.86,92.99,24.72,8.21" target="#fig_4">Fig 9 (</ref>b), we can observe that its time complexity is proportional to the square of the number of blocks. A possible explanation is that the optimization algorithm needs to approximate the Hessian matrix, whose size is the number of blocks squared. Therefore, whether the overhead of partitioning matters depends on the total amount of streamline computation involved. If the number of particles is large, this overhead can be ignored; otherwise, simpler schemes such as round-robin might be preferred. Overall, our algorithm is better suited for large scale seeding, and can be applied to compute-intensive applications, such as the computation of Finitetime Lyapunov exponent (FTLE), which generates an excessive number of flow lines <ref type="bibr" coords="9,354.67,213.02,9.53,8.21" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LIMITATIONS AND FUTURE WORK</head><p>As our approach aims to load-balance streamline computation for steady flow fields, there exist several limitations, which will be addressed in future work. One limitation, found through our tests in Section 6.3, is that our method is better suited for computations involving a large number of seeds, and may not be worthwhile for runs with a small number of initial seeds. Another major limitation is our streamline computation model. Since the current model is designed for steady flow, the blocks are statically assigned to processes in order to avoid extra I/O or communication overhead that could result if dynamic partitioning were used. However, for unsteady flow fields, it is generally unfeasible to load the entire data into memory, especially if there are many timesteps involved . Because of this, using dynamic partitioning may be a more natural choice for unsteady flow fields. Another limitation is that the current model is designed for regular grids. Extending it to unstructured grids is a planned future work. To do so, the current flow graph model and workload estimation will probably have to be revised. Since the grid size in unstructured grids can vary, the flow graph will probably need to store extra information to consider size differences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p> In this paper, we propose a workload model for load-balancing parallel streamline generation. Our model formulates the relationship between the block workload and the corresponding partitioning workload , allowing us to treat the partitioning as an optimization problem that can be solved by conventional optimization algorithms. Since our model requires the knowledge of the block workload, we also propose a graph-based workload estimation algorithm to predict the workload for each block. Testing results show that our partitioning algorithm can reduce the idle time of each process, leading to more efficient parallel streamline generation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,22.50,557.17,513.00,7.37;6,22.50,566.63,512.98,7.37;6,22.50,576.10,512.97,7.37;6,22.49,585.56,513.05,7.37;6,22.49,595.02,421.29,7.37"><head>Fig. 6. </head><figDesc>Fig. 6. Performance tests. The rows from top to bottom represent the datasets Nek, Plume, Ocean, and Flame. The first and second column show the timing for strong scaling and weak scaling, respectively. The performance of round-robin partitioning (Round Robin), actual-workload-based partitioning (Actual), and estimated-workload-based partitioning (Estimated) is compared. The third column lists the average workload per block. The fourth column shows the average waiting time in logarithm scale per process in all rounds, which was measured when doing strong scaling tests for 2048 processes. The fifth column shows the average actual workload and the estimated workload in all rounds. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,31.50,186.18,250.36,7.37;7,31.50,195.64,250.37,7.37;7,31.50,205.11,250.37,7.37;7,31.50,214.57,250.39,7.37;7,31.50,224.04,24.80,7.37"><head>Fig. 7. </head><figDesc>Fig. 7. The blocks with maximum average workload in Nek (a) and Plume (b), highlighted in the white boxes. (a): Streamlines originating from the busiest block in Nek. In the upper left corner, a close-up of the block is shown. (b): Streamlines terminating in the busiest block in Plume. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,22.50,353.03,250.38,7.37;8,22.51,362.50,250.37,7.37;8,22.51,371.96,250.38,7.37;8,22.51,381.43,250.36,7.37;8,22.51,390.89,68.42,7.37"><head>Fig. 9. </head><figDesc>Fig. 9. Analysis of the partitioning algorithm for Nek and Ocean. (a) Maximum memory requirement per process for partitioning with replication . The x-axis is the maximum memory requirement for any one process, as a percentage of the original data size. (b) Time required to calculate partitions. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,285.12,151.76,250.38,7.37;8,285.12,161.23,189.91,7.37"><head>Fig. 10. </head><figDesc>Fig. 10. Performance of uneven seeding when seeding the 40 busiest blocks in Nek with (a) 20K seeds and (b) 256K seeds. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,181.73,494.41,100.15,7.37;9,181.73,503.88,71.34,7.37"><head>Fig</head><figDesc>Fig. 11. Timing of Flow Graph Construction. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false" coords="1,31.48,674.16,246.39,60.51"><figDesc coords="1,34.49,674.16,188.31,10.36;1,43.45,683.62,152.02,10.36">@BULLET The authors are with The Ohio State University, E-mail: {nouanese,leeten,hwshen}@cse.ohio-state.edu.</figDesc><table coords="1,31.48,699.08,246.39,35.59">Manuscript received 31 March 2011; accepted 1 August 2011; posted online 
23 October 2011; mailed on 14 October 2011. 
For information on obtaining reprints of this article, please send 
email to: tvcg@computer.org. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true" coords="5,294.12,411.09,250.40,94.87"><figDesc coords="5,294.12,411.09,250.35,7.37;5,294.12,420.56,250.40,7.37;5,294.12,430.02,67.33,7.37">Table 1. Datasets used for strong scaling and weak scaling tests. The size of each dataset, as well as the number of seeds used in each of our tests are listed.</figDesc><table coords="5,334.83,437.57,168.96,68.40"># Particles 
# Particles 
Dataset 
Size 
per Processor 
(GB) 
(Strong) 
(Weak) 
Nek 
12 
256K 
64 
Plume 
5.81 
256K 
64 
Ocean 
3.86 
256K 
64 
Flame 
18.69 
128K 
32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false" coords="8,22.49,51.25,250.39,289.62"><figDesc coords="8,154.69,84.57,4.26,7.17;8,185.79,51.25,46.87,4.26;8,217.79,60.50,25.09,4.26;8,217.79,65.99,30.47,4.26">(%) Total Block Replication Nek 2K Blks Ocean 4K Blks</figDesc><table coords="8,22.49,133.61,250.39,207.27">(a) 
(b) 

Fig. 8. Impact of λ on Nek (solid lines) with 2048 blocks and 1024 
partitions and Ocean (dashed lines) with 4096 blocks and 1024 parti-
tions. (a): Impact of λ on performance. Note that partition creation 
times were performed on a sequential workstation, while particle advec-
tion times were gathered from parallel runs. (b): Effect of λ on the total 
amount of block replication. The y-axis represents the ratio between 
the total number of blocks after partitioning and the original number of 
blocks. 

0 
1 
2 

128 (Nek) 

256 (Nek) 

512 (Nek) 

1024 (Nek) 

128 (Ocean) 

256 (Ocean) 

512 (Ocean) 

1024 (Ocean) 

(%) 

#Partitions 

Max Memory Footprint per Process 

w/o Rep. 
1K Blks 
2K Blks 
4K Blks 
8K Blks 
0 
5 
10 
15 
20 
25 

1K (Nek) 

2K (Nek) 

4K (Nek) 

8K (Nek) 

1K (Ocean) 

2K (Ocean) 

4K (Ocean) 

8K (Ocean) 

Seconds 

#Blocks 

Partitioning Time 

128 Parts 
256 Parts 
512 Parts 
1K Parts 

(a) 
(b) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>The authors would like to thank the anonymous reviewers for their comments. We would like to also thank Aleks Obabko, Paul Fischer, and Tom Peterka of Argonne National Laboratory, Mathew Maltrud of Los Alamos National Laboratory, Ray Grout of the National Renewable Energy Laboratory, and Jackie Chen of Sandia National Laboratory for providing our test datasets. We would also like to thank Fabian Benitez-Quiroz for the discussion about the optimization algorithm . This research used resources of the Argonne Leadership Computing Facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-06CH11357. This work was supported in part by NSF grant IIS-1017635, US Department of Energy DOE-SC0005036, Battelle Contract No. 137365, Los Alamos National Laboratory Contract No. 69552-001-08, and Department of Energy SciDAC grant DE-FC02- 06ER25779. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,40.76,74.41,232.15,7.30;10,40.77,83.88,17.94,7.30"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonlinear Programming</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">P</forename>
				<surname>Bertsekas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>2nd. edition</note>
</biblStruct>

<biblStruct coords="10,40.77,93.34,232.11,7.30;10,40.77,102.81,232.12,7.30;10,40.77,112.28,232.11,7.30;10,40.77,121.74,64.42,7.30"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Streamline integration using MPI-hybrid parallelism on large Multi-Core architecture</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Camp</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Garth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Childs</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Pugmire</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Joy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,131.21,232.11,7.30;10,40.77,140.67,232.13,7.30;10,40.77,150.14,232.14,7.30;10,40.77,159.61,232.11,7.30;10,40.77,169.07,88.25,7.30"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Terascale direct numerical simulations of turbulent combustion using s3d</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">H</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Choudhary</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>De Supinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Devries</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Hawkes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Klasky</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">K</forename>
				<surname>Liao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mellor-Crummey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Podhorszki</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Sankaran</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Shende</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">S</forename>
				<surname>Yoo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15001</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,178.53,232.13,7.30;10,40.77,187.99,232.12,7.30;10,40.77,197.46,232.12,7.30;10,40.77,206.93,58.67,7.30"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimizing parallel performance of streamline visualization for large distributed flow datasets</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Fujishiro</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PacficVis &apos;08: Proceedings of the IEEE Pacific Visualization Symposium</title>
		<imprint>
			<date type="published" when="2008-03" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,216.39,232.14,7.30;10,40.77,225.86,232.13,7.30;10,40.77,235.33,197.79,7.30"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive desktop analysis of high resolution simulations: application to turbulent plume dynamics and current sheet formation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Clyne</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Mininni</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Norton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Rast</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,244.79,232.11,7.30;10,40.77,254.26,227.41,7.30"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Dean</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ghemawat</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,263.72,232.12,7.30;10,40.78,273.19,232.10,7.30;10,40.78,282.66,232.11,7.20;10,40.78,292.12,161.75,7.30"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel hypergraph partitioning for scientific computing</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">D</forename>
				<surname>Devine</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">G</forename>
				<surname>Boman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">T</forename>
				<surname>Heaphy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">H</forename>
				<surname>Bisseling</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">V</forename>
				<surname>Catalyurek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS &apos;06: Proceedings of the International Parallel and Distributed Processing Symposium</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.78,301.58,232.08,7.30;10,40.78,311.05,157.30,7.30"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Vortical structure in the wake of a transverse jet</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">F</forename>
				<surname>Fric</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Roshko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fluid Mechanics</title>
		<imprint>
			<biblScope unit="volume">279</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.78,320.51,232.10,7.30;10,40.78,329.98,219.44,7.30"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Lagrangian structures and the rate of strain in a partition of twodimensional turbulence</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">G</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of Fluids</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3365" to="3385" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,339.44,232.11,7.30;10,40.78,348.91,232.10,7.30;10,40.78,358.38,160.28,7.30"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Some simplified NPcomplete problems</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Garey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">S</forename>
				<surname>Johnson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Stockmeyer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC &apos;74: Proceedings of the ACM Symposium on Theory of computing 1974</title>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="page" from="47" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,367.84,232.11,7.30;10,40.77,377.31,232.11,7.30;10,40.77,386.77,74.15,7.30"  xml:id="b10">
	<monogr>
		<title level="m" type="main">Using MPI: Portable Parallel Programming with the Message Passing Interface</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Gropp</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Lusk</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Skjellum</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edition</note>
</biblStruct>

<biblStruct coords="10,40.77,396.24,232.10,7.30;10,40.77,405.71,232.11,7.30;10,40.77,415.17,232.14,7.30;10,40.77,424.64,17.94,7.30"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Direct numerical simulation of flame stabilization downstream of a transverse fuel jet in cross-flow</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Grout</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gruber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Yoo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Combustion Institute</title>
		<meeting>the Combustion Institute</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1629" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,434.10,232.11,7.30;10,40.77,443.56,232.11,7.30;10,40.77,453.03,75.49,7.30"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph partitioning and continuous quadratic programming</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">W</forename>
				<surname>Hager</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Krylyuk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="500" to="523" />
			<date type="published" when="1999-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,462.49,232.15,7.30;10,40.77,471.96,232.10,7.30;10,40.77,481.42,17.94,7.30"  xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Karypis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Kumar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,490.89,232.11,7.30;10,40.77,500.36,232.12,7.30;10,40.77,509.82,225.87,7.30"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Simplified parallel domain traversal</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Kendall</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Allen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Peterka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Erickson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;11: Proceedings of the ACM/IEEE Conference on Supercomputing 2011</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>To. appear</note>
</biblStruct>

<biblStruct coords="10,40.77,519.29,232.12,7.30;10,40.77,528.76,151.42,7.30"  xml:id="b15">
	<monogr>
		<title level="m" type="main">An eddy resolving global 1/10 ocean simulation. Ocean Modelling</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Maltrud</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">L</forename>
				<surname>Mcclean</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,538.22,232.14,7.30;10,40.77,547.69,232.12,7.30;10,40.77,557.15,232.11,7.20;10,40.77,566.61,110.07,7.30"  xml:id="b16">
	<analytic>
		<title level="a" type="main">On the numerical simulation of thermal striping in the upper plenum of a fast reactor</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Merzari</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Pointer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Obabko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Fischer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAPP &apos;10: Proceedings of the International Congress on Advances in Nuclear Power Plants</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,576.08,232.14,7.30;10,40.77,585.54,232.11,7.30;10,40.77,595.01,134.07,7.30"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Opt++: An object-oriented toolkit for nonlinear optimization</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">C</forename>
				<surname>Meza</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
				<surname>Oliva</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">D</forename>
				<surname>Hough</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Williams</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,604.47,232.13,7.30;10,40.77,613.94,157.41,7.30"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Updating Quasi-Newton matrices with limited storage</title>
		<author>
			<persName>
				<forename type="first">J</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">151</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.77,623.41,232.11,7.30;10,40.77,632.87,232.13,7.30;10,40.77,642.34,232.11,7.30;10,40.77,651.80,232.12,7.30;10,40.77,661.27,33.00,7.30"  xml:id="b19">
	<analytic>
		<title level="a" type="main">A study of parallel particle tracing for steadystate and time-varying flow fields</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Peterka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Ross</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Nouanesengsey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T.-Y</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-W</forename>
				<surname>Shen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Kendall</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS &apos;11: Proceedings of IEEE International Parallel &amp; Distributed Processing Symposium</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>To. appear</note>
</biblStruct>

<biblStruct coords="10,40.77,670.74,232.12,7.30;10,40.77,680.14,232.10,7.36;10,40.76,689.61,232.12,7.30;10,40.76,699.08,40.08,7.30"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable computation of streamlines on very large datasets</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Pugmire</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Childs</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Garth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ahern</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">H</forename>
				<surname>Weber</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;09: Proceedings of the ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,708.54,232.14,7.30;10,40.76,718.01,232.10,7.30;10,40.76,727.47,232.12,7.30;10,40.76,736.94,232.10,7.30;10,303.38,53.90,168.79,7.30"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph partitioning for highperformance scientific simulations</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Schloegel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Karypis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Kumar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sourcebook of parallel computing, chapter 18</title>
		<editor>J. Dongarra, I. Foster, G. Fox, W. Gropp, K. Kennedy, L. Torczon, and A. White</editor>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002-11" />
			<biblScope unit="page" from="491" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,63.37,232.12,7.30;10,303.38,72.83,232.11,7.30;10,303.38,82.30,84.35,7.30"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Flow web: a graph based user interface for 3D flow field exploration</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Xu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-W</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IS&amp;T/SPIE Visualization and Data 2010</title>
		<meeting>the IS&amp;T/SPIE Visualization and Data 2010</meeting>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,91.76,232.10,7.30;10,303.38,101.22,232.12,7.30;10,303.38,110.69,142.86,7.30"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel hierarchical visualization of large time-varying 3D vector fields</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Yu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC 07: Proceedings of the ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
