<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T15:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Plane Sweep Volume Illumination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Erik</forename>
								<surname>Sundénsund´sundén</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Anders</forename>
								<surname>Ynnerman</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Timo</forename>
								<surname>Ropinski</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">Image Plane Sweep Volume Illumination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Interactive volume rendering</term>
					<term>GPU-based ray-casting</term>
					<term>Advanced illumination</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. A CT scan of a penguin (512 × 512 × 1146 voxels) rendered using image plane sweep volume illumination. Local and global illumination effects are integrated, yielding in high quality visualizations of opaque as well as transparent materials. Abstract—In recent years, many volumetric illumination models have been proposed, which have the potential to simulate advanced lighting effects and thus support improved image comprehension. Although volume ray-casting is widely accepted as the volume rendering technique which achieves the highest image quality, so far no volumetric illumination algorithm has been designed to be directly incorporated into the ray-casting process. In this paper we propose image plane sweep volume illumination (IPSVI), which allows the integration of advanced illumination effects into a GPU-based volume ray-caster by exploiting the plane sweep paradigm. Thus, we are able to reduce the problem complexity and achieve interactive frame rates, while supporting scattering as well as shadowing. Since all illumination computations are performed directly within a single rendering pass, IPSVI does not require any preprocessing nor does it need to store intermediate results within an illumination volume. It therefore has a significantly lower memory footprint than other techniques. This makes IPSVI directly applicable to large data sets. Furthermore, the integration into a GPU-based ray-caster allows for high image quality as well as improved rendering performance by exploiting early ray termination. This paper discusses the theory behind IPSVI, describes its implementation, demonstrates its visual results and provides performance measurements.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p> The benefits of advanced illumination techniques for interactive volume rendering are numerous and have been demonstrated in recent user studies <ref type="bibr" coords="1,78.60,464.21,14.19,8.02" target="#b30">[30,</ref><ref type="bibr" coords="1,96.41,464.21,11.21,8.02"> 41,</ref><ref type="bibr" coords="1,111.24,464.21,10.64,8.02" target="#b17"> 17]</ref> . Accordingly, many researchers have addressed the area of advanced volume illumination while focusing on interactive techniques <ref type="bibr" coords="1,112.67,484.14,13.74,8.02" target="#b28">[28]</ref>. The approaches tackling this area can be roughly divided into two groups. First, approaches exploiting volume preprocessing, which can be done either on the CPU <ref type="bibr" coords="1,253.93,504.07,14.94,8.02" target="#b32">[32] </ref>or, for increased performance, on the GPU <ref type="bibr" coords="1,184.97,514.03,13.74,8.02" target="#b14">[14]</ref> . Techniques belonging to this group precompute the illumination information and store it in an additional volume. While preprocessing can, in principle, be combined with any volume rendering algorithm, another widely used strategy directly exploits the benefits of the slice-based volume rendering paradigm <ref type="bibr" coords="1,105.44,563.84,9.52,8.02" target="#b3">[4]</ref> . In contrast to ray-casting approaches, slicebased rendering inherently synchronizes the ray front and thus reduces the complexity of the illumination computation when focusing on directional effects <ref type="bibr" coords="1,93.94,593.73,14.19,8.02" target="#b12">[12,</ref><ref type="bibr" coords="1,111.83,593.73,11.21,8.02" target="#b13"> 13,</ref><ref type="bibr" coords="1,126.73,593.73,11.21,8.02" target="#b34"> 34,</ref><ref type="bibr" coords="1,141.64,593.73,10.64,8.02"> 40]</ref>. While all methods belonging to the two mentioned groups support advanced illumination at interactive frame rates, they also incorporate the drawbacks of the respective group. For instance, the preprocessing techniques are not applicable to large volumetric data sets since the precomputed illumination volume would consume too much extra graphics memory. While this is not a drawback of the slice-based approaches, they have the downside that they are tightly bound to the slice-based rendering paradigm, which is known to result in inferior image quality as compared to ray-casting based techniques <ref type="bibr" coords="1,356.75,482.53,13.74,8.02" target="#b37">[37]</ref>. To achieve high quality results with slice-based rendering, a rather large number of slices is necessary which directly impacts rendering performance. Furthermore, the slices need a sufficient bit depth to allow high precision compositing during the integration . To incorporate advanced illumination effects, the memory footprint is further enlarged since an additional slice serving as light buffer is crucial. In addition to their superior image quality, ray-casting based techniques also grant more control over the ray marching, allowing adaptive sampling, early ray termination and empty space skipping. Additionally, some of the slice-based volume illumination techniques have restrictions on the light source positioning, for example, either head-lights only <ref type="bibr" coords="1,357.29,592.12,14.94,8.02" target="#b34">[34] </ref>or lights in the viewer's hemisphere <ref type="bibr" coords="1,514.91,592.12,14.94,8.02">[40] </ref>are supported. Until now no interactive illumination technique has been proposed which directly integrates global illumination effects into an interactive ray-casting based volume renderer without requiring precomputation of an illumination volume. </p><p>In this paper we introduce image plane sweep volume illumination (IPSVI), which enables shadowing and scattering effects within GPUbased volume ray-casters. Expressing the illumination computation as an iterative process allows us to exploit the plane sweep paradigm <ref type="bibr" coords="1,529.55,676.65,14.94,8.02" target="#b25">[25] </ref>in order to achieve advanced illumination effects. The plane sweep paradigm has often been applied in computational geometry to reduce problem complexity by transforming a problem into a sequence of problems of lower dimensionality, which are then solved sequentially . In our case we use a sweep line, which sweeps over the image plane to introduce the desired interactive illumination effects. During the sweep we can keep the memory footprint low since we only need to store the state of the sweep line as opposed to the entire volume illumination information. Thus, we only need one additional 2D texture which is interactively updated during rendering, by exploiting the features of state-of-the-art graphics hardware. This makes the additional memory requirements widely independent of the data set size, and IPSVI the first method of its kind, which is applicable to large volumetric data sets (see <ref type="figure" coords="2,104.11,123.12,29.49,8.02">Figure 1</ref>), and the multimodal data sets which frequently arise in medical imaging. In the remainder of this paper, we will first discuss relevant research in Section 2, before introducing the concepts underlying IPSVI in Section 3. All relevant details needed for the implementation of the algorithm are explained in Section 4. In Section 6 we will discuss the visual results as well as the performance achievable with the presented method and discuss our findings regarding certain parameter choices. Finally, the paper will conclude in Section 7 by summarizing our contribution and discussing future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Volume illumination. In the past, different illumination techniques have been proposed in the area of interactive volume rendering. Synchronization is one of the main issues when dealing with on-the-fly volume illumination. Therefore, many of the proposed techniques are inherently bound to a specific rendering paradigm. The half angle slicing technique, presented by Kniss et al. <ref type="bibr" coords="2,194.92,296.66,14.19,8.02" target="#b12">[12,</ref><ref type="bibr" coords="2,211.78,296.66,10.64,8.02" target="#b13"> 13]</ref>, was the first one allowing interactive volumetric illumination. It is based on the slice-based rendering paradigm <ref type="bibr" coords="2,140.91,316.59,10.45,8.02" target="#b5">[5] </ref>and synchronization is achieved by incorporating the light source position when selecting the main slicing direction. Thus an intermediate light buffer can be synchronized with the eye buffer to ensure that the relevant illumination information has been already computed. More recently occlusion-based shading models have been proposed, based on the slice-based rendering paradigm <ref type="bibr" coords="2,74.32,376.36,14.19,8.02" target="#b34">[34,</ref><ref type="bibr" coords="2,91.77,376.36,10.64,8.02"> 40]</ref>. Schott et al. <ref type="bibr" coords="2,159.64,376.36,14.94,8.02" target="#b34">[34] </ref>assume the existence of a headlight and can thus incorporate occlusion information when slicing from front to back. This light source constrain has been loosened byŠoltészová byˇbyŠoltészová et al. <ref type="bibr" coords="2,82.29,407.29,13.74,8.02">[40]</ref>, who introduce the use of warped filter kernels in order to support light sources located anywhere within the hemisphere pointing towards the viewer. Moloney et al. <ref type="bibr" coords="2,188.70,427.22,14.94,8.02" target="#b23">[23] </ref>have also recently proposed how to exploit sort first distributions in parallel volume rendering , while allowing advanced illumination using half angle slicing. Besides the slice-based techniques, Zhang et al. <ref type="bibr" coords="2,195.87,457.11,14.19,8.02" target="#b40">[42,</ref><ref type="bibr" coords="2,212.35,457.11,11.95,8.02" target="#b41"> 43] </ref> have demonstrate how to add shadows into sheet-based splatting techniques. Given the fact, that ray-casting can be considered as the volume rendering technique producing the highest image quality <ref type="bibr" coords="2,237.13,487.36,13.74,8.02" target="#b37">[37]</ref>, it is remarkable that only a few techniques for interactive advanced volume illumination have focused on this paradigm. Rezk-Salama <ref type="bibr" coords="2,257.93,507.28,14.94,8.02" target="#b27">[27] </ref>has proposed a fast Monte-Carlo-based algorithm which incorporates shadowing and scattering effects into a GPU-based volume ray-caster. However, the performance impact can still be considered as high and the technique is optimized by restricting scattering events to a limited set of isosurfaces. Ropinski et al. <ref type="bibr" coords="2,145.83,557.09,14.94,8.02" target="#b31">[31] </ref>present an overview of how to add shadows to GPU-based ray-casting. In fact many of the discussed techniques can also be combined with other rendering paradigms. As the most promising approach, they have identified the deep shadow mapping technique <ref type="bibr" coords="2,93.35,596.94,13.74,8.02" target="#b19">[19]</ref> , which also allows the incorporation of semitransparent shadows. Hadwiger et al. <ref type="bibr" coords="2,163.13,606.91,10.45,8.02" target="#b8">[8] </ref>have demonstrated how to exploit GPU capabilities in order to apply deep shadow maps efficiently . However, the deep shadow mapping paradigm is inherently based on an approximation of the shadowing function, which is stored in an illumination volume, and furthermore user-specified bias values are required. Alternatively, also ray-based techniques have been developed in order to support advanced volume illumination. Hernell et al. <ref type="bibr" coords="2,34.46,676.65,10.45,8.02" target="#b9">[9] </ref>and Ljung et al. <ref type="bibr" coords="2,110.72,676.65,14.94,8.02" target="#b18">[18] </ref>proposed a ray-based technique, which interactively simulates ambient occlusion by incorporating neighboring structures. A similar idea is the foundation of the piecewise linear integration technique proposed by Hernell et al. <ref type="bibr" coords="2,202.46,706.53,13.74,8.02" target="#b10">[10]</ref>. By assuming that distant structures have only little illumination impact, they propose an algorithm which is based on a low-resolution grid and thus allows faster ray marching. All of these ray-based techniques have constraints, which the technique proposed in this paper is not subject to. Thus, we do not restrict the illumination effects to local features or particular surfaces, allow semi-transparent shadows and require no user-specified bias or an illumination volume. Besides the approaches bound to a specific rendering paradigm, more general approaches are also available. These often exploit a pre-computation stage and store the resulting illumination-related information in an additional volume. Behrens and Ratering <ref type="bibr" coords="2,504.20,124.98,10.45,8.02" target="#b1">[2] </ref>were the first to use illumination volumes where they store a precomputed illumination value for each voxel. Ropinski et al. <ref type="bibr" coords="2,481.33,144.91,14.94,8.02" target="#b30">[30] </ref>have also adapted this approach, but instead recompute the volume on the GPU and loosen the constraints regarding the light source. Qiu et al. <ref type="bibr" coords="2,520.55,164.83,14.94,8.02" target="#b26">[26] </ref> have proposed a volumetric illumination method which supports multiple scattering by representing a diffuse photon volume and a density volume as FCC lattices. Recently spherical harmonic lighting has also been considered in the area of interactive volume rendering. Ritschel <ref type="bibr" coords="2,317.17,214.65,14.94,8.02" target="#b29">[29] </ref>was the first to apply this concept to volume rendering. Similar in spirit to the piecewise-linear integration technique <ref type="bibr" coords="2,509.30,224.61,14.94,8.02" target="#b10">[10] </ref>he exploits a level-of-detail sampling scheme when applying ray marching , to recompute the spherical harmonic coefficients. Lindemann and Ropinski <ref type="bibr" coords="2,336.61,254.50,14.94,8.02" target="#b16">[16] </ref> applied a similar strategy and were able to simulate advanced material properties in volume rendering. More recently Kronander et al. <ref type="bibr" coords="2,346.92,274.42,14.94,8.02" target="#b14">[14] </ref> presented an improved approach which recomputes the spherical harmonic coefficients even faster. While all these techniques have demonstrated their potential to generate impressive results, they all need to store the precomputed information, and are thus not applicable when rendering large volumetric data sets. Furthermore , in some cases the pre-processing time hinders interactive transfer function specification. Being a less expensive approximation of global illumination, more general ambient occlusion techniques <ref type="bibr" coords="2,424.70,355.98,14.94,8.02" target="#b42">[44] </ref>have also been applied in the area of volume rendering than the techniques presented by Hernell et al. <ref type="bibr" coords="2,306.79,375.91,9.52,8.02" target="#b9">[9]</ref> . The first occurrences of this concept had the goal to improve the spatial comprehension of isosurfaces <ref type="bibr" coords="2,458.16,385.87,9.71,8.02" target="#b0">[1,</ref><ref type="bibr" coords="2,470.63,385.87,11.21,8.02"> 24,</ref><ref type="bibr" coords="2,484.59,385.87,10.64,8.02" target="#b38"> 38]</ref>. Ropinski et al. <ref type="bibr" coords="2,304.72,395.83,14.94,8.02" target="#b32">[32] </ref> exploit an expensive pre-processing which involves computation and clustering of local histograms, which could have been accelerated later on <ref type="bibr" coords="2,337.03,415.76,13.74,8.02" target="#b22">[22]</ref>. Diaz et al. <ref type="bibr" coords="2,394.05,415.76,14.94,8.02" target="#b11">[11] </ref>have adopted screen-space ambient occlusion techniques within their vicinity occlusion map approach. A more general adaption has been proposed by Ruiz et al. <ref type="bibr" coords="2,493.80,435.68,13.74,8.02" target="#b33">[33]</ref>. Their obscurance-based method is targeted towards illustrative purposes. Sweep-based rendering. Sweeping is an algorithmic paradigm, that has been introduced in the field of computational geometry <ref type="bibr" coords="2,518.31,467.43,13.74,8.02" target="#b25">[25]</ref>. Depending on the problem space, plane-sweep and space-sweep techniques are distinguished. In the 2D plane-sweep case, the problem space is a plane which is swept with a line. In the 3D case, the 3D problem space is swept by a plane. Sweeping sequentializes computations by only considering events encountered by the sweep structure. Thus, an n-dimensional static problem is transformed into an (n − 1)- dimensional dynamic problem. While the classical slice rendering <ref type="bibr" coords="2,525.03,537.17,10.45,8.02" target="#b5">[5] </ref> can be considered as a space-sweep algorithm, other volume rendering techniques make more explicit use of the sweep paradigm. In the following, we only focus on those techniques which have been directly linked to the sweeping paradigm. Giertsen applied the sweeping paradigm to allow a volumetric visualization of sparse irregular grids <ref type="bibr" coords="2,306.96,596.95,9.52,8.02" target="#b7">[7]</ref>. Thus, he was able to transfer the static 3D ray-casting problem into a dynamic 2D cell sorting problem, which allows efficient rendering. Bitter and Kaufman <ref type="bibr" coords="2,423.13,616.87,10.45,8.02" target="#b2">[3] </ref>present ray-slice-sweeping, a slice-parallel volume ray-casting technique. The volume is swept in front-to-back order, whereby volumetric slices are processed before compositing is performed. Silva and Mitchell <ref type="bibr" coords="2,452.35,646.76,14.94,8.02" target="#b36">[36] </ref>have exploited the sweeping paradigm to render disconnected and non-convex irregular grids. Farias et al. <ref type="bibr" coords="2,353.46,666.68,10.45,8.02" target="#b6">[6] </ref> also propose a sweeping algorithm for the rendering of unstructured volume data. They sweep the data front-to-back and project the faces of each occurring cell. More recently the sweeping paradigm has been used to enhance rendering of dynamic height fields <ref type="bibr" coords="2,307.97,706.53,13.74,8.02" target="#b39">[39]</ref>. To compute the occlusion of dynamic height fields the height field is swept in parallel along azimuthal directions for which the occlusion has to be determined. Due to the sweep-based dimensionality reduction the occlusion information can thus be computed very efficiently. While all these approaches convincingly demonstrate the benefits of the sweeping paradigm in rendering, to the authors knowledge it has not been previously exploited in order to achieve advanced volume illumination through ray synchronization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IMAGE PLANE SWEEP VOLUME ILLUMINATION</head><p>Max <ref type="bibr" coords="3,51.05,114.46,14.94,8.02" target="#b20">[20] </ref>emphasizes the importance of advanced optical models in the area of volume rendering. Although the theoretical concepts behind these advanced illumination models are well understood, a practical realization which allows interactive visualization is still demanding . Thus, accurate solutions of the physical equations of light scattering are still too expensive to be computed in real-time. Nevertheless , the techniques proposed in past years already allow convincing illumination effects by approximating accurate solutions. As already stated above, all these techniques, independent of whether they are ray-casting-or slice-based, have to deal with the synchronization problem inherent to advanced volume illumination, i. e. samples scattering light onto the current sample need to be computed beforehand. Supporting the desired synchronization directly within a volume ray-caster is more demanding, since we do not have a unique ray-front as is the case with slice-based volume rendering. However, when considering the theoretical formulation of advanced illumination within the context of volume rendering, it becomes clear that synchronization can also be achieved within volume ray-casters. To clarify this, we briefly review the concepts how advanced light effects can be incorporated into the volume rendering integral. The following can only be considered as a brief introduction, a more detailed explanation of the concepts behind the volume rendering integral and the incorporation of advanced illumination effect is given by Max <ref type="bibr" coords="3,266.93,333.63,14.94,8.02" target="#b20">[20] </ref>as well as Max and Chen <ref type="bibr" coords="3,121.70,343.60,13.74,8.02" target="#b21">[21]</ref>. The standard volume rendering integral which only considers emission and absorption can be used to compute the intensity I at the eye point s e for the direction ω o as follows: </p><formula>I(s e , ω o ) = T (s b , s e ) · I(s b ) + s e s b T (s , s e ) · τ(s ) · c e (s )ds , </formula><formula>(1) </formula><p>where s b is the background position and I(s b ) the intensity at s b . The integral itself considers all samples s between s b and s e to compute their emissive contribution c e (s ), which is absorbed based on the extinction coefficient τ(s ). T (s 1 , </p><formula>s 2 ) = exp(− s 2 s 1 </formula><p> τ(s)ds) is the probability that a ray does not hit a particle between s 1 and s 2 , i. e., the transparency . While Equation 1 only supports local illumination effects, it can be rewritten to incorporate also scattering. Max and Chen present reformulations that either consider only single scattering with a single light direction, or multiple scattering resulting from a rather high albedo <ref type="bibr" coords="3,57.69,509.18,13.74,8.02" target="#b21">[21]</ref>. In IPSVI, we combine both models with the goal to have shadowing effects of higher frequency due to the single scattering approximation , as well as to simulate more diffuse scattering effects for homogeneous regions. To do so, we substitute c e (s) with the sum of the emissive omnidirectional contribution and the light scattered at s in direction ω o : </p><formula>g(s, ω o ) = c e (s) + (1−a(s)) · p(ω l , ω o , s) · I ss (s, ω l )+ a(s) · Ω p(ω i , ω o , s) · I ms (s, ω i )dω i . </formula><formula>(2) </formula><p>Here, ω o is again the outgoing light direction for the light leaving the current sample and usually set based on the viewing ray. ω l is the principal light direction and ω i are all directions from which light may hit a sample. a(s) is the albedo at sample s, which is the probability that light is scattered rather than absorbed at s. p is the phase function and I ss (s, ω l ) describes the single scattering contribution of the light intensity reaching s from direction ω l . In contrast, I ms (s, ω i ) describes the multiple scattering contribution of the light intensity reaching s from direction ω i . We use the albedo a(s) in order to interpolate between the single and the multiple scattering contribution. Hence, when the albedo a(s) is low, we consider mainly single scattering events. We have further chosen to make the albedo as well as the phase function <ref type="figure" coords="3,310.40,176.02,3.32,7.37">2</ref> . When considering single scattering only, the scattering contribution influencing the illumination at sample s only depends on those samples lying on the ray between s and the light source position s l . When multiple scattering is considered, the contributing samples lie in the cone Θ. dependent on the actual sample s to allow a richer set of effects. Similar to the formulation presented by Max and Chen <ref type="bibr" coords="3,487.99,258.07,13.74,8.02" target="#b21">[21]</ref>, the single scattering contribution can be written as: </p><formula>I ss (s , ω i ) = T (s l , s ) · L + s s T (s i , s ) · τ(s i ) · c e (s i )ds i + s s l T (s i , s ) · τ(s i ) · c e (s i )ds i . </formula><formula>(5) </formula><p>Splitting the integration into these two parts allows us to compute the illumination iteratively, as it is similarly done in recent slice-based volume rendering techniques <ref type="bibr" coords="4,128.68,352.68,14.19,8.02" target="#b12">[12,</ref><ref type="bibr" coords="4,145.12,352.68,11.21,8.02" target="#b34"> 34,</ref><ref type="bibr" coords="4,158.58,352.68,10.64,8.02"> 40]</ref>. The only term avoiding the iterative computation based on Equation 5 is T (s i , s ) in the second integral , since it would require us to have knowledge about the structures between s and s when solving the second integral. However, when using back-to-front compositing this is not a practical issue, since the compositing is done iteratively anyway. A similar splitting as in Equation 5 can be found for the multiple scattering contribution: </p><formula>I ms (s , ω i ) =∇ di f f s s 1 |s i − s| · c e (s i )ds i + ∇ di f f s s b 1 |s i − s| · c e (s i )ds i . </formula><formula>(6) </formula><p> While this splitting also allows to compute the multiple scattering iteratively , as done in the slice-based approaches, we can in contrast directly exploit the ray-casting paradigm. This is also illustrated in <ref type="figure" coords="4,22.50,525.47,29.57,8.02">Figure 2</ref> , where the different rays cast through the volume are annotated as r i−2 ...r i+2 . As can be seen in the figure, the illumination at all samples of r i only depends on samples lying on rays closer to the light source. Thus, when we are able to synchronize ray marching, such that r i−1 is always cast before r i is cast, we can reduce the computation at each sample to the first integrals in Equation 5 and Equation 6 instead of the whole integral as in Equation 3 and Equation 4. As depicted by the distance between s and s as compared to the distance between s and s b this results in a vast speedup. To achieve the synchronized ray computation, such that r i−1 is computed before r i , we exploit a modified sweeping paradigm <ref type="bibr" coords="4,237.38,625.09,13.74,8.02" target="#b25">[25]</ref>. The idea behind this approach is, for a directional light source, illustrated in image space, we obtain the direction to which the sweep-lines should be perpendicular. When rendering, we process the sweep-lines starting in the direction where the light comes from, such that all rays cast through one sweep-line form the according sweep-plane. We are able to support illumination effects of both directional as well as point light sources, by performing one or four passes of line rendering in order to realize the sweeping, as seen in <ref type="figure" coords="4,475.76,310.79,28.98,8.02" target="#fig_3">Figure 4</ref>. As can be seen, all rays cast through the same line in screen space are lying in one plane in world space. In the 2D projection shown in <ref type="figure" coords="4,489.94,330.71,30.24,8.02">Figure 2</ref>this plane would be associated with r i . Thus, depending on the coordinate frame under consideration, IPSVI can be considered either as a linesweep algorithm operating in image space, or a plane-sweep algorithm operating in world space. In the latter case it can be considered as modified, since the sweep planes are not parallel, but form a fan-like structure. To store the illumination information accumulated along all light rays between the samples of a specific sweep plane, we use an illumination cache. This illumination cache needs to cover the cross section of the volume, which is formed by all rays cast through one line in the image plane. To be able to capture the relevant illumination information, the illumination cache is updated continuously during the sweep. An example of an illumination cache is shown in the inset in <ref type="figure" coords="4,294.89,460.23,29.58,8.02" target="#fig_1">Figure 3</ref>. It is generated by iteratively solving the second integral in Equation 5 and Equation 6. We will discuss how to compute the coordinates for reading from and writing into the illumination cache in Section 4. Nevertheless, we would already like to point out here, that in any case we do not require any preprocessing and the illumination cache is represented by a 2D image, rather than a 3D volume as it is the case in other volumetric illumination techniques. Thus, IPSVI has a very small memory footprint and can even be applied to large volumetric data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TECHNICAL REALIZATION</head><p> One benefit of the proposed illumination model is its easy implementation . It can be integrated with only little effort into existing volume ray-casting systems and does not require architectural changes. Our technical realization is based on two conceptual steps, the line sweeping performed on the CPU and the actual ray-casting performed along with the update of the illumination cache on the GPU. While in standard GPU-based ray-casters the shader doing the ray marching is triggered by rendering a screen-aligned quad, we trigger the ray-caster line by line to allow the sweeping, such that when a line is drawn it is used to trigger the ray-casting on pixels contained within the line. Thus, when rendering one line, we know that all lines closer to the light source have already been processed. During the processing of each line we iteratively update the illumination cache, to accumulate and store the visibility as seen from the light source, which is further elaborated in Subsection 4.3. In the following subsections we discuss how to determine the line sweeping direction, perform line synchronization and ray marching, before discussing special cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Line Sweeping</head><p>The line sweeping is determined based on the type of light source. In cases where we want to simulate directional lighting we sweep only in one direction (see <ref type="figure" coords="5,109.24,86.98,30.77,8.02" target="#fig_3">Figure 4</ref>(left)), and in cases where we want to simulate a point light source, we sweep into four directions starting at the point light source position in image space (see <ref type="figure" coords="5,213.04,106.90,30.13,8.02" target="#fig_3">Figure 4</ref>(right)). Directional lights. To determine the sweeping direction for directional light sources, we project the light direction into image space coordinates. Based on this direction, we are able to derive the sweeping start point, which lies at the screen border close to the light source, as well as the sweeping direction (see <ref type="figure" coords="5,169.40,156.98,30.23,8.02" target="#fig_3">Figure 4</ref>(left)). To facilitate an easy implementation in our case, we sweep either horizontally or vertically over the image space. When using arbitrarily oriented lines, the realization would be more complex, since overlap between adjacent lines needs to be avoided. Therefore, it would be essential to exactly know how arbitrarily oriented lines are rasterized. Using horizontal and vertical lines does not require this knowledge and is therefore independent of the underlying system. Therefore, we do not directly use the sweeping direction dir sweep (see <ref type="figure" coords="5,168.03,236.68,29.93,8.02" target="#fig_1">Figure 3</ref>), but derive a vertical or horizontal sweeping direction dir sweep , such that the angle between dir sweep and dir sweep is minimized. Point lights. The sweeping for point light sources is slightly more complex, since its propagation is approximated by four sweep passes as illustrated in <ref type="figure" coords="5,91.01,290.24,30.96,8.02" target="#fig_3">Figure 4</ref>(right). To allow light propagation into all directions from the projection of the light source s l , we process lines in the four directions dir0 sweep ...dir3 sweep . Each sweeping originates from s l and is propagated either along the positive/negative x or y direction of the screen. Thus, the image plane is divided into four zones, where the borders intersecting s l are defined by two lines which form an angle of 45 @BULLET with the current sweeping direction, as shown in Figure 4 (right). Having a static 45 @BULLET angle is an advantage over connecting the borders to the edge of the screen, since the latter might result in grazing angles between the image space sweeping directions and the actual projection of the light direction. When handling point lights, lines are also drawn horizontally or vertically to ensure that the lines do not overlap. To avoid lines covering a zone they do not belong to, we use a 2D masking texture, generated by measuring the angle of the vector between the current pixel and s l . We also exploit this texture to perform edge blending between the zones, in order to improve image quality by allowing smoother transitions between the borders. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Synchronization</head><p> Using either of the two sweeping approaches, we have to ensure that the ray-casting is synchronized such that the parts of the volume being closer to the light source are processed earlier, than those being further away from it. To do this, we use OpenGL's atomic image access functions and the glMemoryBarrierEXT() function when building up the illumination cache. glMemoryBarrierEXT() in combination with the bitfield indicator SHADER IMAGE ACCESS BARRIER BIT EXT provides a synchronization point, which should be called after each processed line to ensure that all memory access operations have been performed and the illumination cache has been built up before processing the next line. Thus, we can assume that when processing the fragments of a certain line l i , all lines between l i and the light source have already been processed. Thus all illumination effects resulting from the structures between l i and the light source are already computed when processing l i . By making this illumination information available during processing of l i advanced illumination effects can be achieved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GPU Ray-Casting</head><p>We have implemented the technique using OpenGL 3.0 by exploiting GLSL 1.30 functionality. This is necessary, since it is essential to have fragment scattering functionality in the fragment shader stage in order to write and read from random positions in the illumination cache. By binding textures to image units, we obtain the desired behaviour. The pseudo code in Listing 4.1 shows how we perform the raycasting while accessing the illumination cache at the same time. The Listing 4.1 Pseudo code for the GPU part of IPSVI. The illumination cache data structures illumCacheIn and illumCacheOut serve as ping-pong buffers. Compositing occurs in light direction in order to obtain the illumination value, and along the view direction as in standard DVR. c o l 4 r e s = c o l 4 ( 0 . 0 ) ; i v e c 2 i l l u m P r e v P o s = c a l c I l l u m P o s ( s ) ; i l l u m I n = g e t I l l u m ( i l l u m C a c h e I n , i l l u m P r e v P o s ) ; f o r a l l s a m p l e s s a l o n g t h e r a y { i v e c 2 i l l u m P o s = c a l c I l l u m P o s ( s ) ; i l l u m I n = c o m p o s i t e ( i l l u m I n , c a s t L i g h t R a y S e g ( s ) ) ; f l o a t i n t e n s i t y = g e t I n t e n s i t y ( volume , s ) ; c o l 4 c o l = c l a s s i f y ( i n t e n s i t y ) ; c o l = l o c a l I l l u m ( c o l ) ; / / b a c k t o f r o n t c o m p o s i t i n g ( l i g h t d i r e c t i o n ) i l l u m O u t . r g b =(1.0 − c o l . a ) * i l l u m I n . r g b + c o l . a * c o l . r g b ; i l l u m O u t . a =(1.0 − c o l . a ) * i l l u m I n . a + c o l . a ; c o l = a p p l y I l l u m ( i l l u m I n ) ; i f ( i l l u m P r e v P o s ! = i l l u m P o s ) { i f ( t o B e S a v e d ( s ) ) { s t o r e I l l u m ( i l l u m O u t , i l l u m C a c h e O u t , i l l u m P r e v P o s ) ; } i l l u m P r e v P o s = i l l u m P o s ; i l l u m I n = g e t I l l u m ( i l l u m C a c h e I n , i l l u m P o s ) ; } } r e t u r n r e s ; result of the ray-casting loop is stored in the 4-tuple res and represents the integration along one viewing ray as specified in Equation 1. In order to take into account the indirect illumination, we exploit the illumination cache bound to an image unit. Ping-pong buffering is used to avoid access conflicts, whereby the illumination cache to be read from is bound to illumCacheIn and the one to write to is bound to illumCacheOut. Before reading the indirect illumination contribution , we need to compute the appropriate illumPosIn coordinate for each sample s. This is done with the function calcIllumPosIn. To perform this computation efficiently, we use similar concepts as exploited when realizing projective texture mapping <ref type="bibr" coords="5,484.85,532.95,13.74,8.02" target="#b35">[35]</ref>, but have to consider that adjacent illumination caches are in general not parallel in world space. Once we have obtained the correct position according to the the current sample s in the illumination cache we can fetch the indirect lighting contribution. </p><p>After we have computed the illumination contribution, we can fetch the current intensity from the volume, and optionally apply classification as well as standard local illumination. Now, that we know the contribution col of the current sample s, we can use it to modulate the previously computed incoming illumination which is stored in illumIn. Since we sweep away from the light source, we have to apply a back-to-front compositing as used in standard DVR. Now, that we have computed the outgoing illumination, we use the incoming illumination to modulate the current color col before it is used to perform compositing along the viewing ray. Finally, we have to write out the outgoing illumination to the illumination cache illumCacheOut. For efficiency reasons, this operation is only performed when necessary, i. e., when the writing position on the illumination cache is different as compared to the previous sample. This is not the case for every sample, since usually the sampling rate is higher than the actual number of texels in the illumination cache. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Special cases</head><p> Simply reading the illumination information at the appropriate position in the illumination cache would not be sufficient in some cases. When considering distant camera positions where the volume will cover only a few pixels in image space, it would result in a low number of processed sweep lines and lead to undersampling of the features potentially having an influence on the illumination. Therefore, we perform a piecewise integration within castLightRaySeg(), where we march along short ray segments pointing towards s l until we hit the previous illumination cache plane given in volume space. When performing a perspective projection, the length of these piecewise rays also depends on the distance to the camera, since viewing rays diverge. To account for this variable lengths, we exploit an additional 3-tuple res image storage, in which we store the according sample position in the previous illumination cache. The ray direction and its length for the piecewise integration are then obtained by subtracting the current sample position from the one stored in the res image storage. To combine the result of the previous lookup and the piecewise ray marching, we perform a compositing as used in standard DVR. As we demonstrate in Subsection 6.2, the piecewise integration can also be used when increasing the sweep line width in order to improve performance. However, regardless of the chosen line width, the first processed line always has to have a line width of one, in order to initialize the voxel positions used for the piecewise integration. Furthermore, it should be pointed out that we cannot compute an appropriate sweep direction for the case when the light direction is parallel to the view direction. In this case IPSVI could be disabled or the previous sweeping direction could be used to allow coherency. In practice this proceeding did not lead to visually noticeable behavior. Also, to allow a smoother transition when the main sweeping direction dir sweep changes, the arbitrarily oriented sweep lines can be used. While the presented concepts work for most cases, the described ray-casting paradigm leads to problems when a viewing ray travels through s l . In these cases, the ray traversal needs to be also adapted based on s l . When s l lies in front of the volume, a front-to-back traversal is needed and when s l is located behind the volume a back-to-front traversal is required, such that light is correctly propagated along these rays. In cases where s l lies inside the volume, light propagation along these rays should even be chosen with respect to s l , i. e., front-to-back behind s l and back-to-front in front of s l . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PERFORMANCE OPTIMIZATIONS</head><p> In this section we discuss how to extend IPSVI with early ray termination and how to increase performance based on parameter choices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Early Ray Termination</head><p> Today, early ray termination is widely used in volume ray-casting systems in order to improve rendering performance <ref type="bibr" coords="6,196.41,539.70,13.74,8.02" target="#b15">[15]</ref>. By stopping the <ref type="figure" coords="6,22.50,708.30,18.73,7.37">Fig. 5</ref>. Early ray termination is not inherently supported by IPSVI. Since feature B is occluded by feature A, early ray termination would not allow it to appear in the illumination cache. Thus, the influence of feature B on feature C could not be simulated. </p><formula>(a) (b) (c) </formula><p>Fig. 6. To enable early ray termination, we modify the exit point texture, by color coding volume coordinates (a) where the early ray termination threshold is reached. Based on the light source position, the lengths of the view rays, shown luminance coded in (b), are propagated to obtain the modified exit point texture (c). In this figure, the light source is located top left.  ray marching when a certain opacity threshold is reached, computation time can be decreased without having a visual impact. However, early ray termination as implemented in most of todays ray-casters is in conflict with the volume illumination sweeping proposed in this paper . The problem is illustrated in <ref type="figure" coords="6,404.29,516.35,28.89,8.02">Figure 5</ref>, where a volume containing only opaque structures is shown. As can be seen, the feature B is not visible from the current view position, since it is occluded by the fully opaque feature A. A typical case, where early ray termination would come into play, and the ray marching would be aborted before reaching B. While this does not pose a problem for standard DVR, since B is not visible, it poses a problem for IPSVI, since B affects the illumination of the feature depicted with C in <ref type="figure" coords="6,442.39,586.09,29.60,8.02">Figure 5</ref>. When applying early ray termination, B would not appear in the illumination cache and therefore its influence on other features could not be considered. To be able to benefit from early ray termination, we propose an extension to IPSVI which enables to further traverse only those rays reaching the opacity threshold, which are required for a sufficient illumination cache. We do so, by providing modified exit points for the used ray-caster. To be able to actually benefit from this extension, it is important that it can be implemented efficiently. The proposed extension is based on a simplified ray-casting pass, which is performed before the actual rendering. Within this ray-casting pass, we exploit a lower sampling rate and disable shading, and write out for each ray the volume position s f (r), at which the opacity threshold triggering early ray-termination has been reached, as well as s b (r), where the furthest away structures have been encountered along r. Thus, we obtain two images as shown in <ref type="figure" coords="6,401.52,736.42,30.82,8.02">Figure 6</ref>(a) and <ref type="figure" coords="6,464.03,736.42,30.82,8.02">Figure 6</ref>(b), where  the volume positions are color-coded in (a) and the ray length is depicted by a gray value in (b). To use these results of the pre-rendering pass, we need to further modify them. The goal is to find the maximal ray length for each pixel position, which is needed to build up a correct illumination cache. To do so, we sweep over the images in parallel in a similar fashion as during the CPU line sweeping discussed in Subsection 4.1. However, this time we sweep towards the light source instead of away from it, and only consider 2D information . For each p i encountered on the sweep line, we analyze the predecessor p i−1 on the previous sweep line, which is in the point light case intersected by a line through p i and s l . In order to determine the maximal ray length for p i , we assume that p i−1 lies further away from s l than p i . Thus, the structures encountered by the ray cast through p i might have an influence on the illumination of those structures encountered by p i−1 . Therefore, the maximal ray length l max (r(p i )) for a ray r through pixel p i has to be computed as </p><formula> l max (r(p i )) = min(s b (r(p i )), max(s f (r(p i−1 )), l max (r(p i−1 )))). </formula><p> ing this processing, we write out the l max (r(p i )). Thus, we can transform the initial textures shown in <ref type="figure" coords="7,157.48,577.31,30.93,8.02">Figure 6</ref>(a) and <ref type="figure" coords="7,220.44,577.31,30.94,8.02">Figure 6</ref>(b) into the one shown in <ref type="figure" coords="7,94.24,587.28,30.05,8.02">Figure 6</ref>(c), which can be directly used as exit point texture defining the maximal ray lengths for IPSVI. To reduce the overhead of our early ray termination extension, the described processing is performed at a lower image resolution than the resolution of the final image. Since this only affects the resolution of the exit point texture and the final image is ray-cast using the full resolution, our extension does not result in any visible effects. We will discuss the performance impact of this extension in Subsection 6.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Line Width &amp; Illumination Cache Size</head><p>In both described cases, when using directional or point light sources, we can compute the illumination as introduced in Section 3, since we limit ourselves to forward scattering effects. To limit the amount of draw calls a line width greater than one can be chosen. As shown in Subsection 6.2, this improves performance since it better exploits the block-wise GPU processor layout and therefore results in a higher degree of parallelization. The amount of lines to be processed is further reduced by restricting the line processing to the area inside the axis aligned bounding box of the volume. For optimal use of the illumination cache we have the option to employ a light frustum adaptation. Therefore, the volume bounding box is projected into light space and used to define a rectangular area which contains the entire proxy geometry as seen from the light source. While this is similar to how we define the area to be covered by sweep lines, we use this information here to redefine the near plane seen from the light source for an optimal field of view, when projecting the current sample on the illumination cache. Though we change the size accordingly, we keep the aspect ratio in order to avoid shadow popping artifacts. The cache optimization can further benefit from a camera frustum clipping of the proxy geometry against the volume bounding box, such that the illumination cache can be adapted to cover only the visible parts of the volume. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS &amp; DISCUSSION</head><p>To assess the practical use of IPSVI, we will first provide some visual results before discussing performance measurements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Visual Results</head><p> To investigate the visual quality achievable with IPSVI, we have applied it to various data sets of different modalities. <ref type="figure" coords="7,487.94,646.76,31.12,8.02">Figure 1</ref>shows a rendering of a CT scan of a penguin having a resolution of 512 × 512 × 1146 voxels. As can be seen, shadowing and scattering effects are visible, which increase the degree of realism. Due to the small memory footprint of IPSVI, the effects can be applied despite the relatively large size of the data set, as also demonstrated by the rendering of the giraffe's head and the bear in <ref type="figure" coords="7,433.08,706.53,30.22,8.02" target="#fig_6">Figure 7</ref>. Because the effects achieved with IPSVI are similar to those which can be obtained with half angle slicing <ref type="bibr" coords="7,358.84,726.46,13.74,8.02" target="#b12">[12]</ref>, we have decided to also include a comparison with this technique. <ref type="figure" coords="7,371.43,736.42,30.95,8.02" target="#fig_8">Figure 8</ref>shows this comparison, as well as the (d) With PI, LW:1 (e) With PI, LW:3 (f) With PI, LW:5 </p><p>(g) IC 128 2 (h) IC 256 2 </p><p>(i) IC 1024 2 <ref type="figure" coords="8,22.50,284.68,19.44,7.37">Fig. 9</ref>. Comparison of renderings of the piggybank data set, with and without piecewise integration (PI), different line widths (LW) and with different illumination cache size (IC). As shown, the choice of parameters can have a significant impact on the visual result. The bottom images are rendered with PI as well as a line width of one. differences to unshaded DVR and Phong lighting. As can be seen in the first row of <ref type="figure" coords="8,78.77,364.96,29.31,8.02" target="#fig_8">Figure 8</ref> , both half angle slicing as well as IPSVI improve the degree of realism as compared to unshaded DVR and Phong shading. Furthermore, the difference between half angle slicing and IPSVI is hardly noticeable. While the first statement is also true for the engine data set shown in the second row, the differences between half angle slicing and IPSVI are more prominent. As can be seen, the color saturation of the half angle result is slightly different, while our cases better matches the color of the unshaded DVR as well as the Phong result. Furthermore, there are slight difference regarding the shadow borders when comparing ours with the half angle results. This differences at the shadow borders are more noticeable, since the engine data set has been rendered with a higher degree of opacity, which results in harder shadow borders. <ref type="figure" coords="8,32.46,495.90,30.84,8.02">Figure 9</ref> shows a visual comparison of IPSVI when changing parameters such as line width or illumination cache size, as well as renderings with and without piecewise integration. As shown in <ref type="figure" coords="8,257.92,515.82,14.95,8.02;8,22.50,525.78,18.91,8.02">Fig- ure 9</ref>(a), (b) and (c) the quality without piecewise integration is far less satisfying than in (d), (e) and (f) where this method is used. Line artifacts are, unfortunately, noticeable in some cases when a large line width is used (see <ref type="figure" coords="8,89.40,555.67,30.30,8.02">Figure 9</ref>(c) and (f)). To emphasize the importance of having an optimal illumination cache size, we vary its size in (g), </p><formula>(h) and </formula><p>(i) to such extent that false results and artifacts are introduced. A small illumination cache size might introduce severe line artifacts as less visibility information can be stored. While all the results discussed so far have shown the application to CT data sets, IPSVI is also beneficial when applying it to other modalities . Especially when dealing with modalities where the gradient is not sufficient to be used for shading, IPSVI can improve the image quality. <ref type="figure" coords="8,53.88,646.76,35.30,8.02" target="#fig_10">Figure 10</ref>shows the application to such modalities. In <ref type="figure" coords="8,257.92,646.76,14.95,8.02;8,22.50,656.72,22.79,8.02" target="#fig_10">Fig- ure 10</ref>(a) the application to a seismic data set is shown. Seismic data is known to suffer from a low signal-to-noise ratio, which makes the visual differentiation of horizon layers difficult. As can be seen in <ref type="figure" coords="8,257.92,676.65,14.95,8.02;8,22.50,686.61,22.32,8.02" target="#fig_10">Fig- ure 10</ref>(a), the application of IPSVI adds depth to the image and allows to grasp the relationship between adjacent layers. In contrast to CT data, MRI suffers from noisy gradients. Since IPSVI does not rely on gradient computation, the surfaces appear smoother, as can be seen in <ref type="figure" coords="8,22.50,726.46,34.82,8.02" target="#fig_10">Figure 10</ref> (b). Furthermore, the incorporation of scattering effects results in a more realistic skin appearance. Regarding noise, ultra sound (a) seismic (b) MRI </p><p>(c) US (d) synthetic (US) is probably the most difficult modality to be visualized. While gradient-based shading effects would still produce reasonable images when applied to MRI data, this is not the case for US data. In contrast we can generate high fidelity images even based on US data (see Figure 10 (c)). So far the discussed modalities all suffered from a too low signal-to-noise ratio. A different case is the synthetic data set shown in <ref type="figure" coords="8,295.18,454.16,35.45,8.02" target="#fig_10">Figure 10</ref> (d). When dealing with synthetic data sets or segmentation masks, no partial volume effect is present and thus boundaries are not smoothed out. This is often the reason for staircase artifacts to become visible, when applying gradient-based shading techniques to these data sets. In contrast when applying IPSVI and choosing a rather high albedo, the scattering contribution results in an inherent border smoothing and thus no staircase artifacts are visible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance Analysis</head><p>To analyze the performance of IPSVI, we have measured the average frames per second (fps) over 100 frames while performing a rotation around the camera up-vector. Measurements have been performed for different cases by varying the screen resolution, line width and data set size, as these parameters have a major impact on the overall perfor- mance. <ref type="figure" coords="8,313.36,596.94,25.90,8.02" target="#tab_1">Table 1</ref> shows the results, we could achieve on a standard desktop computer, equipped with an Intel Xeon W3550 processor running at 3.07 GHz, 6 GB of RAM and an nVidia GeForce GTX 580 graphics processing unit. Our results show, that the factor having the highest impact on performance, is an increased image resolution, which was also expected since IPSVI is an image-based algorithm. Furthermore, it is visible and obvious that a higher line width increases performance in most cases. The larger data set are actually in some cases rendered with better performance as compared to smaller data sets. The reason for this is that we only draw lines which cover the volume bounding geometry projected into screen space. As an example, the giraffe's projected bounding volume does not cover as much of the screen space area, therefore it is rendered faster at certain screen resolutions. For additional reference, the penguin dataset in <ref type="figure" coords="8,442.35,726.46,29.11,8.02">Figure 1</ref> , which has a resolution of 512 × 512 × 1146 voxels, is rendered at 0.8 fps (at 1.9 fps with unshaded DVR) with a screen size of 1650 × 800 and with the use of bilinear interpolation when sampling as well as updating the illumination cache. We have also supplied a relative performance factor (normalized to the measured speed of Ambient Only) to provide a performance comparison, apart from the visual comparison in <ref type="figure" coords="9,243.91,454.83,29.01,8.02" target="#fig_8">Figure 8</ref>. We have also measured the performance of the algorithm when utilizing the piecewise integration. As can be seen in <ref type="figure" coords="9,228.70,474.93,34.50,8.02" target="#fig_11">Figure 11</ref>, the piecewise integration reduces performance when increasing the line width. The reason for this is that, as the line width increases the ray length for the piecewise integration also increases. Due to this effect, the performance may actually decrease when increasing the line width, such that we need to consider a balance between the number of draw calls and the size of the integration area to acquire the best possible performance. The incorporation of our modified early ray termination algorithm does in fact increase performance. However, as we have noticed in our tests, it is crucial to apply this extension on a low resolution representation of the exit point texture to achieve a performance increase, due to the overhead of the additional line sweeping. It is also feasible to assume that the performance gain depends on the spatial relation of the data, as well as the choice of transfer function. Furthermore , in comparison to regular ray-casting, IPSVI is slower, mainly due to the amount of CPU processing and synchronization performed during each render pass. Work load might not be optimal as compared to unshaded DVR, but when considering computing the influence of features being closer to the light source, these generally need to be computed and stored in a preprocessing stage for many other global illumination techniques. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,294.12,176.02,250.38,7.37;3,294.12,185.48,250.38,7.37;3,294.12,194.95,250.38,8.20;3,294.12,204.41,250.38,7.37;3,294.12,213.88,40.91,7.49"><head>Fig. </head><figDesc> Fig. 2. When considering single scattering only, the scattering contribution influencing the illumination at sample s only depends on those samples lying on the ray between s and the light source position s l . When multiple scattering is considered, the contributing samples lie in the cone Θ. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,22.50,192.54,250.38,7.37;4,22.50,202.01,250.38,7.37;4,22.50,211.47,250.38,7.37;4,22.50,220.94,250.38,7.37;4,22.50,230.40,205.98,7.37"><head>Fig. 3. </head><figDesc> Fig. 3. The sweeping paradigm is exploited to iteratively compute illumination information. By performing a line-sweep in image space, a plane-shaped illumination cache is fanned through the volume. The illumination cache captures the relevant illumination information between the currently rendered line and the light source (see inset). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,31.56,645.02,240.83,9.64;4,22.50,656.72,249.88,9.64;4,22.50,666.68,250.38,8.77;4,22.50,676.65,250.37,8.02;4,22.50,686.61,250.38,8.77;4,22.50,696.57,250.38,8.77"><head>Figure 3. </head><figDesc>We denote the sweep direction in image space by dir sweep and in volume space as dir sweep . As can be seen in Figure 3, dir sweep is the projection of dir sweep onto the image plane. Since we want to iteratively compute the influence of structures when moving further away from the light source, the sweep direction dir sweep corresponds to the main light direction in volume space. By projecting dir sweep into </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,285.12,229.50,250.38,7.37;4,285.12,238.97,250.38,7.37;4,285.12,248.43,250.38,7.37;4,285.12,257.89,70.10,7.37"><head>Fig. 4. </head><figDesc>Fig. 4. Sweeping is performed based on the light source to be used. When simulating directional light sources, a single sweeping direction (left) is used, while four sweeping directions are used to simulate point light sources (right). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,285.12,414.77,250.39,7.70;6,285.12,424.24,250.37,7.70;6,285.12,434.00,250.38,7.37;6,285.12,443.47,192.82,7.37"><head>Fig. 7. </head><figDesc>Fig. 7. A CT scan of a bear (512 × 512 × 412 voxels) and a giraffe (512 × 512 × 553 voxels) rendered in in real-time using IPSVI. Hard shadows are cast onto the back legs and the chest. The bottom images shows the state of illumination cache at rendering completion. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,31.50,347.80,513.00,7.37;7,31.50,357.26,513.00,7.37;7,31.50,366.73,314.74,7.37"><head>Fig. 8. </head><figDesc>Fig. 8. Comparison of IPSVI with three other commonly used approaches. From left to right: unshaded DVR, gradient-based Phong shading, half angle slicing and image plane sweep volume illumination. The number inside the subcaption brackets represents a normalized speed factor between the different methods, where 1.00 indicates highest measured performance rate. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="8,285.12,345.03,250.38,7.37;8,285.12,354.50,250.38,7.37;8,285.12,363.96,165.61,7.37"><head>Fig. 10. </head><figDesc>Fig. 10. Application of IPSVI to frequently used modalities, which do not allow to facilitate gradient-based shading due to a relatively low signalto-noise ratio or the discrete nature of the data. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="9,31.50,366.11,250.38,7.37;9,31.50,375.57,250.38,7.37;9,31.50,383.77,250.38,8.67"><head>Fig. 11. </head><figDesc>Fig. 11. Performance measurements of IPSVI with/without piecewise integration (PI) while adapting the line width. The results have been achieved for a screen resolution of 256 2 and with a data set of size 256 3 . </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true" coords="9,37.48,59.66,517.29,272.69"><figDesc coords="9,72.98,59.66,430.04,7.37">Table 1. Frame rates of IPSVI with piecewise integration enabled for different data sets, screen resolutions and line widths.</figDesc><table coords="9,37.48,87.55,517.29,244.80">image space 
data set: 
US Baby 
CT Engine 
CT Giraffe 
resolution 
size: 
199 × 159 × 161 
256 × 256 × 256 
512 × 512 × 553 
(pixels) 
spacing: 
1 × 1 × 1 
1 × 1 × 1 
0.977 × 0.977 × 3 
line width: 
LW:1 
LW:2 
LW:4 
LW:1 
LW:2 
LW:4 
LW:1 
LW:2 
LW:4 
64 × 64 
50.28 fps 91.58 fps 116.28 fps 
33.7 fps 
65.40 fps 82.17 fps 58.28 fps 110.50 fps 152.67 fps 
128 × 128 
25.94 fps 48.59 fps 
62.11 fps 
16.23 fps 31.74 fps 40.83 fps 27.75 fps 
52.55 fps 
70.42 fps 
256 × 256 
12.70 fps 25.03 fps 
33.98 fps 
7.79 fps 
15.37 fps 21.16 fps 13.16 fps 
25.85 fps 
32.54 fps 
512 × 512 
5.68 fps 
11.86 fps 
17.60 fps 
3.55 fps 
7.11 fps 
10.74 fps 
6.42 fps 
12.69 fps 
15.30 fps 
1024 × 1024 
2.63 fps 
5.44 fps 
8.33 fps 
1.61 fps 
3.24 fps 
5.02 fps 
3.05 fps 
6.20 fps 
7.75 fps 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 

</table></figure>

			<note place="foot">I ss (s, ω l ) = T (s l , s) · L + s s l T (s , s) · τ(s ) · c e (s )ds , (3) where s l is the sample position coinciding with the light source and L is the initial light intensity. In contrast, multiple scattering has to be considered when a(s) is higher in order to compute I ms (s, ω i ). To simplify the computation of multiple scattering, it is often assumed that a very high albedo is present, such that multiple scattering events are the predominant factor in the illumination computation, which results in a diffusion-like process [21]. We also make this assumption and model multiple scattering as: I ms (s, ω i ) = ∇ di f f s s b 1 |s − s| · c e (s )ds . (4) Here, ∇ di f f represents the diffusion approximation and 1 |s −s| results in a weighting, such that more distant samples have less influence. In practice, multiple scattering is often simulated by applying lowpass convolution kernels to the neighborhood of the current sample [13]. Nevertheless, computing the convolution over the neighborhood of each processed sample is still computationally too expensive to be performed in real-time. Therefore, the assumption is often made that a forward scattering phase function is present. Several authors [12, 34, 40] show that this is a valid assumption which leads to convincingly realistic results. We also make this simplification, and assume that only forward scattering phase functions are present which supports an efficient computation of Equation 2. When making this assumption , the integral in Equation 2 does not integrate over the whole sphere Ω, but only over a cone-shaped region Θ, which is specified through the cone angle θ (see Figure 2). This simplification reduces computing complexity drastically, since scattering becomes directed and thus only samples between the current sample s and s l need to be considered during illumination computation. For the single scattering case, the dependent samples for the current sample s are also illustrated in Figure 2. It can be seen that all relevant samples lie on the light ray between s and s l . A similar observation holds for the samples contributing to the multiple scattering, they all lie in the region of the volume oriented towards s l from s . The same relation as between s and all contributing samples towards the light source holds also for all other samples s along the light ray. Thus, we are able to modify Equation 3, such that we can substitute the existing integral with two integrals, one for the illumination calculation between s l and s and one for the illumination calculation between s and s :</note>

			<note place="foot" n="7"> CONCLUSIONS &amp; FUTURE WORK In this paper, we have presented image plane sweep volume illumination , a novel sweep-based volumetric illumination technique. By sweeping the image plane, we are able to synchronize illumination computation, and thus integrate advanced volume illumination techniques into a standard ray-casting based volume renderer without either performing pre-processing or building up an illumination volume. Thus, we are able to generate high-quality volume renderings with advanced illumination effects at interactive frame rates. To our knowledge , this is the first approach which allows integration of these effects directly into a ray-casting based renderer. We were able to show how to easily implement the concepts and have discussed the competitive performance as well as quality results, which have been obtained when comparing IPSVI to previous approaches. Besides its simple realization , IPSVI has several benefits. Since we do not need to store an intermediate illumination volume, we have a considerably smaller memory footprint as compared to previous techniques and can therefore apply IPSVI even to large volumetric data sets without worrying about extra GPU memory consumption. Furthermore, clipping is inherently supported, since the illumination is computed based on what is actually visible. IPSVI can also be applied when rendering multimodal data sets, because the illumination computation is directly performed within the ray-caster, which makes it independent of the data set resolution and orientation. While the illumination cache can be seen as similar to the light buffer used in slice-based techniques, its resolution and precision can be lower since it is only used for illumination calculation and not for the actual compositing along the view ray. IPSVI also has limitations. In those cases, where the volume is partially outside the screen, we do not compute the illumination for the invisible parts. Thus, structures outside the screen do not affect the illumination, which is not the case in other approaches [12, 34, 40]. While this might probably be a downside in some cases, it can be also beneficial in certain scenarios. For instance, when zooming inside a volume, global shadows may result in a rather dark appearance and a low contrast. On the other hand, when just considering the visible structures during the illumination processing, the relation between the visible structures is enhanced while having a brighter image with a potentially higher contrast. For cases where this behavior is not desired, we plan to solve it by applying a multi-resolution rendering approach, where we render the relevant parts of the volume outside the screen at a lower resolution into an off-screen illumination cache first. In the future we also see interesting ways to extend IPSVI. Rim lighting is often used to further emphasize object shapes, by placing an additional rim light behind the object&apos;s silhouette. IPSVI is extensible to support two light sources, and thus enabling rim lights. By choosing the sweep direction, such that it is perpendicular to the line traveling in image space through two light positions, we are able to propagate illumination for both light sources simultaneously. Furthermore , volumetric light sources could be integrated by modifying the piecewise ray-casting. To investigate the perceptual benefits of IPSVI, we would also like to conduct a user study.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>The presented concepts have been integrated into the Voreen volume rendering engine (www.voreen.org). The CT animal data is courtesy of Center for Medical Image Science and Visualization (CMIV). </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,40.76,67.97,232.12,7.13;10,40.76,77.44,232.12,7.13;10,40.76,86.90,120.87,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Precomputed illumination for isosurfaces</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">M</forename>
				<surname>Beason</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Grant</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Banks</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Futch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">Y</forename>
				<surname>Hussaini</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Visualization and Data Analysis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,96.36,232.12,7.13;10,40.76,105.83,232.12,7.13"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Adding shadows to a texture-based volume renderer</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Behrens</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Ratering</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. on Volume Visualization</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,115.29,232.12,7.13;10,40.76,124.76,232.12,7.13;10,40.76,134.22,135.88,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">A ray-slice-sweep volume rendering engine</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Bitter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kaufman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH/EUROGRAPHICS workshop on Graphics hardware</title>
		<meeting>the ACM SIGGRAPH/EUROGRAPHICS workshop on Graphics hardware</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,143.69,232.12,7.13;10,40.76,153.15,232.12,7.13"  xml:id="b3">
	<monogr>
		<title level="m" type="main">Accelerated volume rendering and tomographic reconstruction using texture mapping hardware</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Cabral</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Foran</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,162.62,163.38,7.13"  xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Symp</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,172.08,232.12,7.13;10,40.76,181.54,232.12,7.13;10,40.76,191.01,232.12,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Accelerated volume rendering and tomographic reconstruction using texture mapping hardware</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Cabral</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Foran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 symposium on Volume visualization</title>
		<meeting>the 1994 symposium on Volume visualization</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,200.47,232.12,7.13;10,40.76,209.94,232.12,7.13;10,40.76,219.40,232.12,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Zsweep: an efficient and exact projection algorithm for unstructured volume rendering</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Farias</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S B</forename>
				<surname>Mitchell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">T</forename>
				<surname>Silva</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 IEEE symposium on Volume visualization</title>
		<meeting>the 2000 IEEE symposium on Volume visualization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,228.87,232.12,7.13;10,40.76,238.33,160.61,7.13"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Volume visualization of sparse irregular meshes</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Giertsen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,247.80,232.12,7.13;10,40.76,257.26,232.12,7.13;10,40.76,266.73,164.80,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">GPU-accelerated deep shadow maps for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hadwiger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kratz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sigg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Bühler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/EG Conference on Graphics Hardware</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="27" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,276.19,232.12,7.13;10,40.76,285.65,232.12,7.13;10,40.76,295.12,223.00,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient ambient and emissive tissue illumination using local occlusion in multiresolution volume rendering</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hernell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume Graphics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,304.58,232.12,7.13;10,40.76,314.05,232.12,7.13;10,40.76,323.51,207.77,7.13"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive global light propagation in direct volume rendering using local piecewise integration</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hernell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume and Point-Based Graphics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,332.98,232.12,7.13;10,40.76,342.44,223.74,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Vicinity occlusion maps -enhanced depth perception of volumetric models</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">V J</forename>
				<surname>Daz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Yela</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer Graphics Int</title>
		<imprint>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,351.91,232.12,7.13;10,40.76,361.37,232.12,7.13;10,40.76,370.84,69.95,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Interactive translucent volume rendering and procedural modeling</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kniss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Premoze</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ebert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,380.30,232.12,7.13;10,40.76,389.76,232.12,7.13;10,40.76,399.23,132.71,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">A model for volume lighting and modeling</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kniss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Premoze</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Shirley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mcpherson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,408.69,232.12,7.13;10,40.76,418.16,232.12,7.13;10,40.76,427.62,232.12,7.13;10,40.76,437.09,51.79,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient visibility encoding for dynamic illumination in direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kronander</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Jonsson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Low</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Unger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>to. appear</note>
</biblStruct>

<biblStruct coords="10,40.76,446.55,232.12,7.13;10,40.76,456.02,214.72,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Acceleration techniques for GPU-based volume rendering</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Krüger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Westermann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;03</title>
		<meeting>IEEE Visualization &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,465.48,232.12,7.13;10,40.76,474.94,232.12,7.13;10,40.76,484.41,69.95,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Advanced light material interaction for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Lindemann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume Graphics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,493.87,232.12,7.13;10,40.76,503.34,232.12,7.13;10,40.76,512.80,228.59,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">About the influence of illumination models on image comprehension in direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Lindemann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vis Proceedings)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,522.27,232.12,7.13;10,40.76,531.73,232.12,7.13;10,40.76,541.20,39.18,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Local ambient occlusion in direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hernell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,550.66,232.12,7.13;10,40.76,560.21,232.12,6.86;10,40.76,569.59,69.95,7.13"  xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep shadow maps</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Lokovic</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Veach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="385" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,579.05,232.12,7.13;10,40.76,588.52,186.15,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Max</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,597.98,232.12,7.13;10,40.76,607.45,232.11,7.13;10,40.76,616.91,49.81,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Local and global illumination in the volume rendering integral</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Max</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization: Advanced Concepts</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="259" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,626.38,232.12,7.13;10,40.76,635.84,232.12,7.13;10,40.76,645.31,103.48,7.13"  xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient acquisition and clustering of local histograms for representing voxel neighborhoods</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Me</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,654.77,232.12,7.13;10,40.76,664.23,232.12,7.13;10,40.76,673.70,85.32,7.13"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Sort first parallel volume rendering</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Moloney</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ament</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>to. appear</note>
</biblStruct>

<biblStruct coords="10,285.12,54.06,250.38,7.13;10,303.38,63.52,232.12,7.13;10,303.38,72.99,136.27,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Isosurface ambient occlusion and soft shadows with filterable occlusion maps</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Penner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Mitchell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume and Point-Based Graphics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,82.45,232.12,7.13;10,303.38,91.92,77.82,7.13"  xml:id="b25">
	<monogr>
		<title level="m" type="main">Computational Geometry: An Introduction</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">P</forename>
				<surname>Preparata</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">I</forename>
				<surname>Shamos</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,101.38,232.12,7.13;10,303.38,113.07,232.12,7.13;10,303.38,122.53,186.30,7.13"  xml:id="b26">
	<analytic>
		<title level="a" type="main">Lattice-based volumetric global illumination</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Qiu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Xu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Neophytos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kaufman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Mueller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1576" to="1583" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,132.00,232.12,7.13;10,303.38,141.46,51.46,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">GPU-based monte-carlo volume raycasting</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rezk-Salama</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Graphics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,150.93,232.12,7.13;10,303.38,160.39,232.12,7.13;10,303.38,169.86,78.18,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Advanced illumination techniques for gpu volume raycasting</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rezk-Salama</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hadwiger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Courses Program</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,179.32,232.12,7.13;10,303.38,188.79,232.12,7.13;10,303.38,198.25,41.84,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast GPU-based Visibility Computation for Natural Illumination of Volume Data Sets</title>
		<author>
			<persName>
				<forename type="first">T</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Short Paper Eurographics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,207.71,232.12,7.13;10,303.38,217.18,232.12,7.13;10,303.38,226.64,69.95,7.13"  xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactive volumetric lighting simulating scattering and shadowing</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Döring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rezk-Salama</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific Visualization</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,236.11,232.12,7.13;10,303.38,245.57,232.12,7.13;10,303.38,255.04,229.39,7.13"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient shadows for GPUbased volume raycasting</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kasten</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">H</forename>
				<surname>Hinrichs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conference in Central Europe on Computer Graphics, Visualization and Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,264.50,232.12,7.13;10,303.38,273.97,232.12,7.13;10,303.38,283.43,232.12,7.13;10,303.38,292.90,69.29,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Interactive volume rendering with dynamic ambient occlusion and color bleeding</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Meyer-Spradow</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Diepenbrock</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mensmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">H</forename>
				<surname>Hinrichs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="567" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,302.36,232.12,7.13;10,303.38,311.82,232.12,7.13;10,303.38,321.29,193.61,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">Obscurance-based volume rendering framework</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ruiz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Boada</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Viola</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Feixas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sbert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume and Point-Based Graphics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,330.75,232.12,7.13;10,303.38,340.22,232.12,7.13;10,303.38,349.68,232.12,7.13;10,303.38,359.15,205.97,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">A directional occlusion shading model for interactive direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schott</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Pegoraro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Boulanger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Bouatouch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics/IEEE VGTC Symposium on Visualization 2009)</title>
		<meeting>Eurographics/IEEE VGTC Symposium on Visualization 2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="855" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,368.61,232.12,7.13;10,303.38,378.08,232.12,7.13;10,303.38,387.62,232.12,6.86;10,303.38,397.00,151.20,7.13"  xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast shadows and lighting effects using texture mapping</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Segal</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Korobkin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Van Widenfelt</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Foran</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Haeberli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th annual conference on Computer graphics and interactive techniques , SIGGRAPH &apos;92</title>
		<meeting>the 19th annual conference on Computer graphics and interactive techniques , SIGGRAPH &apos;92</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="249" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,406.47,232.12,7.13;10,303.38,415.93,232.12,7.13;10,303.38,425.40,123.42,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">The lazy sweep ray casting algorithm for rendering irregular grids</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">T</forename>
				<surname>Silva</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S B</forename>
				<surname>Mitchell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="142" to="157" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,434.86,232.12,7.13;10,303.38,444.33,232.12,7.13;10,303.38,453.79,232.12,7.13;10,303.38,463.26,232.12,7.13;10,303.38,472.72,224.79,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">Mapping high-fidelity volume rendering for medical imaging to cpu, gpu and many-core architectures</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Smelyanskiy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Holmes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Chhugani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Larson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Carmean</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Hanson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Dubey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Augustine</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kyker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">W</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">D</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Seiler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Robb</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1563" to="1570" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,482.19,232.12,7.13;10,303.38,491.65,138.42,7.13"  xml:id="b38">
	<analytic>
		<title level="a" type="main">Vicinity shading for enhanced perception of volumetric data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Stewart</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,501.11,232.12,7.13;10,303.38,510.58,159.54,7.13;10,285.12,518.19,31.50,8.97;10,313.08,520.04,222.42,7.13;10,303.38,529.51,232.12,7.13;10,303.38,539.05,232.12,6.86;10,303.38,548.44,69.29,7.13;10,285.12,556.05,31.05,8.97;10,312.62,557.90,222.87,7.13;10,303.38,567.37,232.12,7.13;10,303.38,576.83,58.61,7.13"  xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable height field self-shadowing A multidirectional occlusion shading model for direct volume rendering Chromatic shadows for improved perception</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Timonen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Westerholm40 ] V</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Soltészová</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Patel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Viola</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics/IEEE VGTC Symp. on Visualization 2010)41] V. ˇ Soltészová, D. Patel, and I. Viola Proceedings of Non-Photorealistic Animation and Rendering (NPAR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="723" to="731883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,586.30,232.12,7.13;10,303.38,595.76,107.45,7.13"  xml:id="b40">
	<analytic>
		<title level="a" type="main">Volumetric shadows using splatting</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Crawfis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,605.22,232.12,7.13;10,303.38,614.69,232.12,7.13;10,303.38,624.15,98.84,7.13"  xml:id="b41">
	<analytic>
		<title level="a" type="main">Shadows and soft shadows with participating media using splatting</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Crawfis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="149" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,633.62,232.12,7.13;10,303.38,643.08,232.12,7.13;10,303.38,652.55,61.98,7.13"  xml:id="b42">
	<analytic>
		<title level="a" type="main">An ambient light illumination model</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Zhukov</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Iones</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Kronin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EGRW &apos;98: Proceedings of the Eurographics workshop on Rendering</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
