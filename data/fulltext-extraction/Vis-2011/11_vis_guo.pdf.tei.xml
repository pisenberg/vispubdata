<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T14:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WYSIWYG (What You See is What You Get) Volume Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Hanqi</forename>
								<surname>Guo</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Ningyu</forename>
								<surname>Mao</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Xiaoru</forename>
								<surname>Yuan</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">WYSIWYG (What You See is What You Get) Volume Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Volume rendering</term>
					<term>Sketching input</term>
					<term>Human-computer interaction</term>
					<term>Transfer functions</term>
					<term>Feature space</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Eraser Rainbow Colorization Fuzziness Contrast Brightness Silhouette Peeling-+ + + 1 7 8 9 10 2 3 4 5 6 Fig. 1. One volumetric flow data set interacts with our WYSIWYG volume visualization system. Starting from an initial rendering of the raw volume on the left, the user selects corresponding tools and applies strokes on top of the rendered image: →, remove the outer and intermediate parts of the volume with the volume eraser tool; →, paint the volume with the colorization tool; →, enhance the contrast of the rendered volume; →, increase the brightness of the top part of the volume; →, add silhouettes to the rendered volume; →, increase the opacity of the outer part of the volume; →, increase the fuzziness of the outer features; →, colorize fuzzy matter with the rainbow tool; →, recover the previously removed volume materials with the peeling tool. Abstract—In this paper, we propose a volume visualization system that accepts direct manipulation through a sketch-based What You See Is What You Get (WYSIWYG) approach. Similar to the operations in painting applications for 2D images, in our system, a full set of tools have been developed to enable direct volume rendering manipulation of color, transparency, contrast, brightness, and other optical properties by brushing a few strokes on top of the rendered volume image. To be able to smartly identify the targeted features of the volume, our system matches the sparse sketching input with the clustered features both in image space and volume space. To achieve interactivity, both special algorithms to accelerate the input identification and feature matching have been developed and implemented in our system. Without resorting to tuning transfer function parameters, our proposed system accepts sparse stroke inputs and provides users with intuitive, flexible and effective interaction during volume data exploration and visualization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Direct volume rendering has been widely applied in various domains, from medical diagnoses to atmospheric simulations, as one of the most effective ways to visualize volumetric data. However, the difficulties of setting proper mapping functions to convert original volumetric data to renderable color and opacity values limited the application of volume rendering. In the process of volume rendering, transfer func- tions <ref type="bibr" coords="1,50.65,559.56,14.95,8.12" target="#b21">[23] </ref>are designed to map the raw volume data values to colors and transparencies, before composite images are constructed. Obtaining appropriate transfer functions is critical to comprehend and analyze the complicated structures of volume data by reducing occlusions on undesired features and highlighting important objects in the volume. The main obstacle related to transfer function design comes from the non-intuitive interface in the current design practice. The most @BULLET <ref type="bibr" coords="1,43.70,647.71,95.18,7.11">Hanqi Guo, Ningyu Mao, and </ref> common transfer function design interface is to directly assign opacity and color value to each volume density value by adjusting curves, which define the mapping. However, It is non-trivial for non-expert users to generate informative and clear visualization results with such a design paradigm. Users have to understand how different density values are mapped to the rendered image features, and establish these connections in their brain during the transfer function design. For example , in medical CT data visualization, if users want to hide the skins and reveal the structure of the bones, they have to first recognize that skin has lower intensity values, and then reduce the opacities of the corresponding values in the feature space. Sometimes, small changes in the transfer function may result in dramatic differences in rendering appearance. Due to the perspective projection and compositing effects, there is no one-to-one mapping between the color effects in the transfer function curves and the final rendered appearance. This steep learning curve makes volume rendering not easily accessible to layman users. Even with the latest development in volume visualization research, with which users can modify the occlusion or other color properties of certain features, the detachment between the image space (the target image of volume rendering) and parameter space (the parameters that users can operate and modify) makes transfer function specification still cumbersome. </p><p>In this paper, we propose a WYSIWYG (What You See Is What You Get) volume visualization system that accepts direct sketch and paint manipulations from the user and enables goal-oriented operation on volume exploration and visualization. With the proposed system , users can directly change the appearances of the volume rendered images with desirable visual effects through sketching and painting. Painting different colors, changing opacities, erasing volume structures , modifying contrast and brightness, adding silhouettes, peeling volume feature layers and other operations can be directly applied to the volume rendered images by different painting tools. Real-time animated visual feedback enables the user to adjust the input interactively to achieve satisfying results. Without the need for switching back and forth between the parameter space (transfer functions) and the image space, an intuitive, goal oriented volume exploration and transfer function design paradigm has been established in our system. The system lets the users focus on the spatial and finer structures of the volume and make desirable modifications without interrupting the observation process or compromising the effectiveness of volume comprehension. The WYSIWYG concept, which is very intuitive and effective for various tasks, has been widely applied in much 2D image editing software like Adobe Photoshop. Instant and real-time results are fed back to users during the painting process in our tool, just like the tools in Photoshop. In addition to image editing tools, there have been a few attempts at direct operation on volume data. For example, the volume catcher <ref type="bibr" coords="2,51.46,272.90,14.94,8.12" target="#b25">[27] </ref>and volume cutout <ref type="bibr" coords="2,140.71,272.90,10.45,8.12">[?] </ref> enable efficient volume segmentation by applying strokes to the volume rendered images. Multiple features in distinct rendered images can be fused into a comprehensive one by adding or deleting features with strokes <ref type="bibr" coords="2,198.08,302.79,13.74,8.12" target="#b35">[37]</ref>. More recently, Ropinski et al. <ref type="bibr" coords="2,77.52,312.76,14.94,8.12" target="#b31">[33] </ref> proposed the stroke-based transfer function. Features are extracted and selected based on the user defined strokes indicating foreground and background, and then classified as layers of transfer functions, which can be further composed to new results by tuning opacities of each layer. In our work, we make volume visualization and exploration more flexible by introducing several simple sketching and painting primitives on the rendered image space. Unlike the operations in Photoshop, our tool takes effects on the global visualization results based on transfer functions, instead of localized operations. Compared to the prior works on volume visualization, our method only relies on the current rendered result and the user input, without explicit specifications of foreground and background, feature layers, or pre-prepared images with specific features. The real-time feedback also allows users to suspend operations when the rendered image is satisfying. The contribution of this paper is two-fold. First, we present a novel WYSIWYG volume visualization system, which accepts user sketch input with a full function set and gives real-time feedback. Second, a series of algorithms, including fast user intention inference and semantic realization are proposed to facilitate the interactive data explo- ration. The remainder of this paper is organized as follows. We summarize the related work in Section 2, and then give an overview of the system in Section 3. The user interactions, algorithms and implementation details are described in Section 4 and Section 5. We briefly present the implementation and performance in Section 6. Results are shown in Section 7 and the limitation of the method are discussed in Section 8, before the conclusions are drawn in Section 9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p> Transfer function design is the central topic in direct volume rendering . Generally speaking, transfer functions can be categorized into two major types, namely image-based and data-centric methods <ref type="bibr" coords="2,255.74,636.70,13.74,8.12" target="#b27">[29]</ref>. Image-based transfer methods, which are goal-oriented, utilize images as the input. For example, transfer functions can be generated using genetic algorithms, which are guided and optimized by user selection on the rendered images in the thumbnails <ref type="bibr" coords="2,175.65,676.55,13.74,8.12">[16]</ref>. The rendered images with different transfer functions can also be organized and presented in Design Galleries for user selection <ref type="bibr" coords="2,156.64,696.48,13.74,8.12" target="#b24">[26]</ref>. Transfer functions can also be defined as a sequence of 3D image processing procedures, which allows users to achieve optimal visualization results <ref type="bibr" coords="2,209.95,716.41,13.74,8.12" target="#b11">[12]</ref>. Our method also utilizes the rendered image space for feature inference, in order to give image feedback to users with new results interactively. Unlike image-based methods, data-centric transfer function design focuses more on the volume data itself and its derivative properties. For example, 1D transfer functions, map the scalar sample values into optical properties, thus classifying the volume data for direct volume rendering <ref type="bibr" coords="2,321.80,93.15,9.52,8.12" target="#b1">[2]</ref> . Better classification results can be obtained with multidimensional transfer functions by utilizing derivative properties of the volume data, e.g. first and second order gradient magnitudes <ref type="bibr" coords="2,504.84,113.07,14.19,8.12" target="#b21">[23,</ref><ref type="bibr" coords="2,521.34,113.07,10.64,8.12" target="#b17"> 19]</ref>, which aid in displaying surface information. Usually, features that are not distinguishable in 1D feature space can be better shown by leveraging proper multi-dimensional transfer functions. In order to obtain better feature separation results with multi-dimensional transfer functions , many other derivative properties have been taken into consideration . For example, Kindlmann et al. <ref type="bibr" coords="2,426.95,172.84,14.94,8.12" target="#b18">[20] </ref> take advantage of curvatures to assist the transfer function design. LH histogram <ref type="bibr" coords="2,495.15,182.80,14.94,8.12" target="#b32">[34] </ref>makes it easier to find boundaries, by accounting for local extremas that are derived from every voxels. Local statistical properties are also used to find sophisticated features <ref type="bibr" coords="2,381.46,212.69,13.74,8.12" target="#b26">[28]</ref>. The approximated relative size <ref type="bibr" coords="2,515.20,212.69,10.45,8.12" target="#b4">[5] </ref>of the objects in volume data helps in finding features with certain sizes. The occlusion spectrum <ref type="bibr" coords="2,373.11,232.62,10.45,8.12" target="#b5">[6] </ref>reveals the occlusion relationships among spatial patterns. Visibility-driven transfer functions <ref type="bibr" coords="2,469.42,242.58,10.45,8.12" target="#b6">[7] </ref> provide a feedback mechanism that highlights the visibility of features from a given viewpoint. In our work, we take advantage of derivative properties, e.g. the visibility <ref type="bibr" coords="2,348.19,272.47,10.45,8.12" target="#b6">[7] </ref>for data analysis. Although the community has contributed a variety of methods for transfer function design, it is still a cumbersome and non-trivial work for untrained non-expert domain users to design transfer functions for volume visualization, because transfer function design is very unintuitive . Several methods have been proposed to make transfer functions more intuitive. For example, users can change the appearance of the visualization results by semantics based transfer functions <ref type="bibr" coords="2,495.55,343.61,14.19,8.12" target="#b30">[32,</ref><ref type="bibr" coords="2,511.99,343.61,10.64,8.12" target="#b28"> 30]</ref>. It is noticeable that sketch-based user interfaces are widely used in various applications, from image editing to 3D modeling, and from NPR effect design to volume segmentation, as one of the most methods for the tasks, because it is very intuitive for users to achieve desired results . For example, 2D gray images can be colorized by painting a few colored strokes on the image <ref type="bibr" coords="2,391.56,404.77,13.74,8.12" target="#b20">[22]</ref>. A sketch-based user interface also help users during 3D modeling. The models can be interactively designed by drawing silhouettes of objects on the 2D canvas <ref type="bibr" coords="2,497.33,424.70,13.74,8.12" target="#b15">[17]</ref>. With WYSIWYG NPR <ref type="bibr" coords="2,350.49,434.66,13.74,8.12" target="#b16">[18]</ref>, designers can directly annotate strokes on a 3D model with strokes to stylize non-photorealistic effects for objects. In addition to image and model design, it is also quite effective to utilize a sketch-based user interface for volumetric data manipulation. Direct volume editing <ref type="bibr" coords="2,341.70,474.51,10.45,8.12" target="#b3">[4] </ref>allows users to edit the volume data by interactive sketching. Users can also perform volume segmentation by sketching on the image <ref type="bibr" coords="2,333.70,494.44,14.19,8.12" target="#b36">[38,</ref><ref type="bibr" coords="2,350.13,494.44,10.64,8.12" target="#b25"> 27]</ref>. A few frameworks have been proposed for facilitating transfer function design by leveraging sketch-based interfaces. For example , users can sketch the volume slices to assign color and trans- parency <ref type="bibr" coords="2,316.29,535.69,14.19,8.12" target="#b34">[36,</ref><ref type="bibr" coords="2,333.41,535.69,10.65,8.12" target="#b33"> 35]</ref> , then artificial neural network based algorithms define the high-dimensional transfer function for rendering. Later, Wu and Qu <ref type="bibr" coords="2,315.33,555.61,14.95,8.12" target="#b35">[37] </ref>proposed a method for users to sketch on the rendered images to fuse multiple features in distinct rendering results into a comprehensive one, by adding or deleting features with strokes. Guo et al. <ref type="bibr" coords="2,306.48,585.50,14.94,8.12" target="#b13">[14] </ref>utilized sketch-based input to facilitate multi-dimensional transfer function design. Sketch queries are accepted by the system and then the corresponding component of the transfer function is highlighted in the design interface. Ropinski et al. <ref type="bibr" coords="2,451.91,615.39,14.94,8.12" target="#b31">[33] </ref> proposed a strokebased user interface for 1D transfer function design. Layers are extracted according to the inputted strokes, which defines foreground and background, then the transfer functions are further refined by adjusting the colors and transparencies of each layer. In our work, real-time feedback is given to users, which is flexible enough for users to stop at any intermediate result when satisfied. Compared to the work by Wu and Qu <ref type="bibr" coords="2,312.95,685.12,13.74,8.12" target="#b35">[37]</ref> , our work does not rely on pre-defined images with different features, but only depend on the current rendered image and user input. Compared to the work by Ropinski et al. <ref type="bibr" coords="2,454.58,705.05,13.74,8.12" target="#b31">[33]</ref>, users do not need to explicitly define either foreground and background, nor the layers of volume data. It is a difficult problem to map image space features to the objects in volume data, which is the key for any sketch-based volume exploration tool. In general, sketch-based user interaction systems infer user intentions according to sketch inputs, and then take effect on the results. Kohlmann et al. <ref type="bibr" coords="3,90.95,264.66,14.94,8.12" target="#b19">[21] </ref> proposed a solution, which analyzes the ray profile and finds the best match from the contextual meta information in the knowledge database. In Volume Cutout <ref type="bibr" coords="3,197.21,284.59,13.74,8.12" target="#b36">[38]</ref> , the objects of interest are located by the optimization with stroke and over-segmented volume segmentation. Malik et al. <ref type="bibr" coords="3,159.11,304.51,14.94,8.12" target="#b23">[25] </ref>proposed a framework of ray profile analysis, which locates the transition points by slope thresholds . Ropinski et al. <ref type="bibr" coords="3,109.41,324.43,14.94,8.12" target="#b31">[33] </ref>identify desired features by comparing the ray profiles of both foreground and background stroke specifications, and then decide the intentional depths by histogram analysis. In addition to transfer function design, other techniques such as illustrative visualization methods <ref type="bibr" coords="3,149.51,364.33,14.94,8.12" target="#b9">[10] </ref>can further enhance effects and convey more details informatively by non-photorealistic rendering techniques. For example, contours and silhouettes can visualize the boundaries in volumes effectively <ref type="bibr" coords="3,156.92,394.22,9.52,8.12" target="#b8">[9]</ref> . Transfer functions for rendering styles can also be utilized in illustrative volume rendering <ref type="bibr" coords="3,254.40,404.18,13.74,8.12" target="#b29">[31]</ref>. In VolumeShop <ref type="bibr" coords="3,80.19,414.14,9.52,8.12" target="#b2">[3]</ref> , a dynamic volume illustration environment is introduced for interactive visualization. In this work, we also show examples of how to define styles by sketching in volume rendering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM OVERVIEW</head><p>The main objective of our system is to supply the user with intuitive and goal-oriented interaction on volume data through a WYSIWYG user interface. Users can select various semantics to change the visual effects of the visualization result, by indicating the desired features with direct sketch. The pipeline of the system (<ref type="figure" coords="3,203.42,505.47,19.97,8.12" target="#fig_0">Fig. 2</ref>) starts from the initial rendering result and the user sketch input, and gives real-time image feedback, which allow users to iteratively tune the rendered image using various tools. Several intermediate steps are key to the whole pipeline. User inputs are first converted to foreground and background patches by image space analysis, and then the corresponding feature clusters (intentional features) in the volume data are derived according to image analysis results via data space analysis. The new transfer functions are generated according to the intentional features and the current tool type, before the rendered image is updated. Various tools are developed based on the WYSIWYG concept, which allows the user to directly change visual appearances and explore the features in the volume data by sketching, without accounting for the parameter space. Specifically, color, transparency, as well as complicated effects like fuzziness, silhouette, and contrast can be adjusted and refined by direct sketch with the corresponding tool. Data navigation with the peeling tool further allows the user to explore the features in the volume rendered images without explicit layer defini- tion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">USER INTERACTION</head><p>Similar to operations in popular image editing software, we provide a full set of tools for direct manipulation on the direct volume rendered image. Real-time results are fed back to users while brushing on the image with various tools. For example, the visual objects will be gradually erased when brushing on the image with the eraser tool (<ref type="figure" coords="3,297.56,536.28,19.23,8.12" target="#fig_1">Fig. 3</ref>). There are eight types of visual effects for the direct changing of appearances in our tool set, which are introduced here: Colorization This tool gradually changes the color of the selected feature. Eraser Users can increase or decrease the visibility of desired objects in the rendered image. Boundary Fuzziness This operation allows users to make the desired boundary fuzzy. Silhouette Boundaries are enhanced with a silhouette effect. Silhouette provides exaggerated boundary information using NPR (nonphotorealistic rendering) styles. Brightness The luminance of features can be directly changed without explicitly changing colors. Structure Contrast The contrast of the structures that are sketched upon is automatically enhanced by indirectly changing the color saturation and lighting configurations. Rainbow This tool forces the maximum color differences for the features that are sketched on. The features are colorized as gradient colors, which are either preset or user-defined. Layer Exploration In addition to appearance changing, a peeling slider (<ref type="figure" coords="3,319.60,736.33,19.12,8.12" target="#fig_2">Fig. 4</ref>), which is a pie-like widget, contains several sectors rep-resenting the appearance of different automatically extracted layers. Layers can be toggled by clicking a small red button. During the interaction , the corresponding layers will be shown in the main image. </p><formula>(a) (b) (c) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ALGORITHMS</head><p> The algorithms of the proposed system consist of two major procedures , the feature inference and the semantic realization. The feature inference process converts the image space input into the corresponding distribution functions that are defined on the parameter space, and then the semantic realization process takes effect on the rendering result by changing the rendering parameters according to feature inference results in real-time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fundamentals</head><p>In this section, we briefly define and formularize several important concepts in volume visualization for convenience. Without loss of generality, we assume that the appearance of the volume rendered image is only related to transfer functions, giving identical camera configurations. The shading effects of different features can also be defined by similar visual mappings. Feature space is generally defined as a m-dimensional parameter space, whose dimensionality m is determined by the number of properties of the volume data. For example, a 1D feature space can be defined in the range of the intensity value of the CT scan. The feature space can also be multidimensional when accounting for derivative dimensions e.g. gradient magnitudes. Referring to the work by Rezk-Salama et al. <ref type="bibr" coords="4,190.95,319.43,13.74,8.12" target="#b30">[32]</ref>, transfer functions can be generalized as a set of parameters based on the primitives in the feature space, regardless of the representation of transfer functions. For example, the simplest primitive for a 1D transfer function is a 1D lookup table. Denote the primitives of the feature space as F = {f 1 , f 2 , . . . , f n } T , then the transfer functions can be defined upon the primitives: </p><formula>T F = n ∑ i=1 c i f i = C T F, </formula><formula>(1) </formula><p>where c i is the tuple of the color and opacity values, and where C T is a n × 4 matrix, which consists of c i . We utilize the CIE L * a * b * as the color space plus alpha channel for convenience, although the actual storage of color is in RGBA space. Except as otherwise noted, transfer functions in following sections are defined in 1D, and they are commonly stored as a lookup table in memory. Approaches for multidimensional and more complicated transfer functions can also be developed by reasonable extensions to the above definitions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature Inference</head><p> Feature inference, which aims to determine the features to act on according to the user input, is the key of the system (<ref type="figure" coords="4,206.11,536.75,19.36,8.12">Fig. 5</ref> ). This problem is very different from picking up opaque objects in common scene rendering, where picking positions can be determined by referencing the depth buffer. There are multiple problems that present themselves when attempting to map from the image space objects to the feature space distribution in volume rendering. First, volume rendered images often contain visual features that are fuzzy, semi-transparent and multi-layered. In such an environment, it is hard to determine the depths and positions of the corresponding objects. Secondly, even if the intended positions in the 3D space can be determined, mapping such information to the feature space is still a problem. The third problem is ambiguity. We need to exclude the obvious interferences although users tend to avoid ambiguity when operating on the images. To solve these problems, we propose a method for inferring the user intended features according to the sketches on the volume rendered image. The algorithm consists of two steps (<ref type="figure" coords="4,185.02,686.52,19.62,8.12" target="#fig_0">Fig. 2</ref>): first, finding out the most likely region in the image space; second, determining the distribution of the feature in the data space and the feature space. One of the major challenges with the system is performance. Since real-time feedback is required during the user interaction, several data reduction and acceleration techniques are utilized. <ref type="figure" coords="4,285.12,187.95,18.83,7.64">Fig. 5</ref>. The result of the feature inference by a user stroke. </p><formula>(a) (b) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Image Space Analysis</head><p>The first step of feature inference is to find the most significant visual objects in the 2D image space, which can be achieved by segmenting the image into foreground and background patches according to the user input. The appearance of the foreground patch should be closer to the pixels that are highlighted by user strokes. After image segmentation , the corresponding data are collected by ray traversal from the pixels for further feature inference. To accelerate the whole pipeline, we only consider the image in the local region. In practice, the window size of the local image is set to 30 × 30. If the window size is too small to cover enough features, further data space analysis may not exclude ambiguity as desired. On the other hand, the time and storage complexity of image segmentation and further data space analysis will prevent the pipeline working in real time. We utilize the Graph Cut algorithm <ref type="bibr" coords="4,433.93,360.41,13.74,8.12" target="#b12">[13]</ref>, which combines high performance with good segmentation quality for extracting the foreground and the background of the local image. Since we only have foreground constraints for the image segmentation, we further employ a two-pass method <ref type="bibr" coords="4,356.45,400.26,14.94,8.12" target="#b36">[38] </ref>to generate the background seeds before the second pass of the standard Graph Cut process. Suppose the image is a graph G V , E , where nodes V are the pixels in the image, and edges E are the relationships between the neighborhoods . Each node i in V will be labeled as l i = 0 or 1 (background or foreground), by optimizing the energy function: </p><formula>E(l) = µ ∑ i R(l i ) + ∑ i, j B(l i , l j ), </formula><formula>(2) </formula><p> where R is the likelihood function, and B presents the boundary properties . In the general Graph Cut algorithm, it requires the initial seed set of the foreground G f and G b , and then the likelihood function can be written as: </p><formula>     R(l i = 1) = 0 R(l i = 0) = ∞ i ∈ G f , R(l i = 1) = ∞ R(l i = 0) = 0 i ∈ G b , R(l i = 1) = −ln( d F i d B i +d F i ) R(l i = 0) = −ln( d B i d B i +d F i ) otherwise. </formula><formula>(3) </formula><p>The distance d F i and d B i are evaluated as the minimal distance between the pixel and any clusters in G f or G b . The clustering is performed by K-means algorithm. However, in our problem, we only have the initial foreground G f available. The two-pass Graph Cut algorithm is used to decide G b in the first pass with a modified R, and then run in the second pass with G f and G b using Eq. 3. The modified R in the first pass is: </p><formula>R(l i = 1) = 0 R(l i = 0) = ∞ i ∈ G f , R(l i = 1) = −ln( d F i K+d F i ) R(l i = 0) = −ln( K K+d F i ) otherwise. </formula><formula>(4) </formula><p>where K is a constant, which is the estimation of the distance d B i in Eq. 3. The second term of the energy function 2 is the boundary property: </p><formula>B(C i ,C j ) = αexp(− ||C i −C j || 2 2σ 2 ), </formula><formula>(5) </formula><formula>(a) </formula><p>(where the distance ||C i − C j || is the difference of pixels in the color space. α and σ are pre-defined parameters. The result of the two-pass Graph Cut algorithm on the local image is illustrated in <ref type="figure" coords="5,239.21,233.16,20.51,8.12">Fig. 6</ref>. The final foreground patch presents the approximation of the bone structure , where it is less occluded by outer skin. The classification results on image space are the seed for further data space analysis, which extracts the corresponding distribution on the feature space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Data Space Analysis</head><p>Data space analysis aims to infer feature clusters from the data space based on the result of the image segmentation results. The data values, as well as their visual properties are collected by ray traversal, and then they are classified into clusters. A synthesized score is calculated for each cluster, as the result of data space analysis. In further processes in the pipeline, most of the algorithms only need to consider the cluster with the maximum score, while some of the algorithms account for multiple clusters (e.g. contrast enhancement, etc.) In the ray traversal process, there are four properties collected at each sample point, including the depth, visibility, opacity, as well as the data value. The depth values can be directly calculated according to ray configurations. The data value and opacity value are looked up from both volume data and the transfer function. In addition, the visibility value, which is the contribution of a sample to the final im- age <ref type="bibr" coords="5,46.20,445.11,9.71,8.12" target="#b6">[7,</ref><ref type="bibr" coords="5,58.15,445.11,6.47,8.12" target="#b7"> 8]</ref>, is formalized as: </p><formula>V (x) = (1 − O(x))α(s(x)), </formula><formula>(6) </formula><p>where O(x) is the current attenuation value of the ray, and s(x) is the sample value at position x. The input of the clustering algorithm is the multivariate data samples collected from the ray traversal, which are visualized in <ref type="figure" coords="5,248.87,513.37,19.71,8.12" target="#fig_4">Fig. 7</ref>(a). Typically, the amount of the input samples can reach 10 6 . To handle the multivariate samples with such size, K-means, which is a light weight clustering tool is exploited before further processing. The performance of the K-means is further boosted by accelerated GPU implementation for real-time interaction. The distance metric of the multivariate samples is weighted Euclidean distance. As we observed in the experiments, the change of the weights only slightly affects the clustering result, if reasonable weights are given to the data value and visibility properties. The choice of the centroid number K is based on the estimation of how many different features (both visual and numerical ) are behind the local image region. A higher centroid number will significantly increase both time and storage complexities. Typically , K is set to 10, and this number can be modified depending on the complexity of the dataset. The above ray traversal and clustering process are applied on both foreground and background image patches that are generated from image space analysis. Thus, we obtain the foreground clusters {F i f } and the background clusters {F i b }. The background clusters help in the exclusion of ambiguity, since features that appear in both the foreground and the background are not likely to be the desired ones. The score for each cluster needs to consider several aspects, including the visual significance (visibility), the depths, as well as the guity. Among the above factors, the principle property is the visibility. Hence we can define the score of the i-th cluster as </p><formula>S(i) = V i , (7) </formula><p>where V i is the average visibility of the i-th cluster. More factors should be considered, when the visibility of the clusters is similar, or serious ambiguity exists. So we amend the score function by adding two auxiliary items: </p><formula>S(i) = V i + β 1 D i + β 2 d F b i , (8) </formula><p>where D i is the centroid value of depth of the i-th cluster, and d F b i is the minimum distance between the centroid and all centroids in the background clusters F b , which measures the ambiguity. The coefficients β 1 and β 2 are the weights, which makes the amended items smaller than V i . Based on our experiments and observation, typical values for β 1 and β 2 are around 10 −4 and 10 −3 . Greater values of β 1 and β 2 may hide the principle property V i , but ignoring the influence of the two factors may make the selection undesirable. For faster user interaction , β 2 can be set to 0, since only the data behind the foreground image patch needs to be calculated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Semantic Realization</head><p>Semantic realization converts the feature inference result into new transfer functions, thus updating the volume rendered image gradually during the interaction. In our tool set, color, transparency and  shading changes are the most basic semantics, and higher level semantics are built upon the basic semantics by decompositing the semantics into lower level ones. For example, changing the brightness indirectly changes the color, and contrast enhancement indirectly changes the brightness. The semantic realization can be denoted as an Incremental Transfer Function (ITF), which is the incremental modification of the current TF, and can in turn be generated by compositing a new color and transparency matrix C ′ : </p><formula>(a) (b) (c) + - + </formula><formula>∆T F = C ′T F. </formula><formula>(9) </formula><p>In most cases, such as for color and transparency change, only the cluster with the maximum score is considered. For multi-feature operations , e.g. contrast enhancement and layer navigation, multiple features are considered. In order to control the speed of feedback during the interaction (<ref type="figure" coords="6,25.94,565.88,19.38,8.12" target="#fig_1">Fig. 3</ref>), we define the time rate λ to control the speed of incremental changes. The time rate λ can be either manually assigned by users or be automatically adjusted according to the difference of consecutive images. By default, λ equals to the reciprocal of the current frame rate. On each time step t, the new T F t is the sum of the TF on the previous time step and the ITF weighted by the time rate λ : </p><formula>T F t := T F t−1 ⊕ λ ∆T F t , </formula><formula>(10) </formula><p>where operation ⊕ is the alpha blending of two colors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Color and Transparency Changes</head><p>Color and transparency are among the most important factors affecting the visualization results. Higher level semantics like fuzziness, brightness and contrast are also based upon color and transparency changes. All the row vectors in the color matrix C ′ of the ITF are set to be the destination color. The visual effect of color and transparency changes is shown in <ref type="figure" coords="6,65.37,736.33,20.93,8.12" target="#fig_7">Fig. 9</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Brightness Change</head><p>The lightening and darkening semantics aim to adjust the luminance of the features. They can be generalized as changing the luminance component of the colors in the CIE L * a * b * color space. The object color C ′ can be transformed from the original color in the transfer function: </p><formula>C ′ = diag(1 ± λ , 1, 1, 1)C, (11) </formula><p> where the diagonal matrix only transforms the luminance component of the original color. The color can be further converted into RGB space for rendering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Rainbow</head><p>As we described in Section 4, the rainbow effect encodes the desired features into different colors, according to the data distribution. We perform a histogram equalization for the desired feature, before mapping the colors to the transfer functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Boundary Fuzziness</head><p>The fuzziness semantic makes the boundary of the visual feature more fuzzy by making the local TF more smooth. This process is done by a feature space anisotropic diffusion equation, whose conductivity is inversely proportional to the data histogram H(F): </p><formula>∂ C ∂t − A 1 H(F) ∆C = 0, </formula><formula>(12) </formula><formula>(a) </formula><p>(b)  where ∆C is calculated by differencing the neighborhood opacity values , and each scalar a i in the conductivity vector A equals to 0 or 1, depending on whether the feature primitive f i is in the selected feature cluster. After a few steps of iterations, the modified color matrix C can be obtained for the transparency transform. During the diffusion, the color is also blended with neighborhood features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Contrast Enhancement</head><p> The contrast enhancement is based upon color changes. In image processing , contrast enhancement can be done by histogram equalization. Since transfer functions indirectly take effect on the image contrast in volume rendering, we need an alternative way to achieve the goal. In our work, the image contrast is enhanced by increasing the contrast of the features in the volume. By utilizing the feature inference result F f , a discrete histogram equalization is conducted. Denote f k as the clusters in F f , and l k as the luminance contribution of feature f k , which is the sum of the luminance of the samples weighted by the visibility values. Firstly, the normalized accumulated histogram of the features is calculated after sorting: </p><formula>c i = i ∑ j l i . </formula><formula>(13) </formula><p> We would like to create a transform to make the accumulated histogram c i linear, i.e. c i = iM, for a constant M. The final objective luminance of each feature </p><formula>l ′ k is l ′ k = c i (l max − l min ) + l min . </formula><formula>(14) </formula><p> After the histogram equalization, the contrast enhancement is decomposed into several brightness operations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.6">Layer Navigation</head><p>The layer navigation aims to help users find hidden features along the ray. Similar to contrast enhancement, it is also a multi-layer operation, which is composited as the sum of transparency changes. The weight of each layer is defined by users with the peeling slider (<ref type="figure" coords="7,232.28,574.03,19.03,8.12" target="#fig_2">Fig. 4</ref>). When the ratio of each layer is changed, a new transfer function is generated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION AND PERFORMANCE</head><p> The system was implemented in C++ and GLSL. A standard raycasting volume rendering engine with a pre-integrated transfer func- tion <ref type="bibr" coords="7,48.69,636.70,14.94,8.12" target="#b10">[11] </ref> is utilized. Currently the feature space is 1D. The silhouette rendering is also based on a pre-integration technique. Various GPU acceleration techniques for user interactions are exploited, e.g. K-means clustering, etc. The feature inference and the volume rendering engine are executed in separate threads asynchronously, in order to hide the lags during the interaction. During the user interaction, the feature inference is invoked when any new stroke point is inputted. Meanwhile, the rendering thread keeps on updating the image result. When the feature inference is faster than the rendering, it runs multiple cycles before the next frame is completed, thus reducing the rendering overhead. The performance of the proposed system was tested on a Dell T3400 workstation, with a 2.66GHz CPU, 4GB memory, and an NVidia GTX 470 graphics card with 1280MB video memory. The timings of the test data are listed in <ref type="figure" coords="7,431.60,352.62,26.49,8.12" target="#tab_1">Table 1</ref>. The steps size is 0.5 voxel, and the rendered image resolution is 800x800. Full shading with silhouette style, which is controlled by shading transfer function, is performed during the rendering. In addition to the step size and image size, other factors also influence the performance, e.g. the window size for local image analysis, which indirectly decides the amount of the samples in data space analysis. In addition to the standard PC environment, touchable devices, e.g. iPad (<ref type="figure" coords="7,315.55,432.66,19.92,8.16">Fig. ?</ref>?) are also supported for users. The proposed WYSIWYG system provides a greater advantage to portable devices, since it is extremely inconvenient and inaccurate to design transfer functions with fingers on limited screen spaces with traditional tools. It also shows convenience for volume data navigation on the iPad with multi-touch support. Gestures can be mapped to take the place of auxiliary key operations . Multiple users can not only share but also collaborate on the same volume data at the same time. Due to the limited computational resources on the iPad, a remote system is implemented. In our system, an iPad acts as the client, and a workstation acts as the server. The touch signals are transferred to the server, and then the server transfers the updated volume rendered images back to the iPad. The average bandwidth of the compressed transmission is about 200kb/s in a WLAN environment. The image quality can be dynamically changed according to the frame rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p> In this section, we demonstrate several cases of volume data exploration with our WYSIWYG data exploration tool. In <ref type="figure" coords="7,313.60,626.36,23.73,8.12">Fig. 15</ref>, we demonstrate how to explore an engine block CT scan with our system in a few steps. In the very beginning, the image is rendered with the linear ramp transfer function. After removing the outer materials, the core part of the block is revealed. This part can be further colorized as orange. The boundary of the outer part can be recovered by utilizing the peeling slider. Then a fine visualization result can be achieved. For medical data, which contains many implicit layers with rich semantics , it is more straightforward to explore the features of the data during 3D navigation. <ref type="figure" coords="7,382.33,716.41,26.75,8.12">Fig. 16</ref> presents intermediate and final rendering results of the Visible Male data set. Domain knowledge can be better integrated into the whole exploration process. Users can quickly + <ref type="figure" coords="8,22.50,273.50,23.03,7.64">Fig. 16</ref> . The final result and intermediate steps of exploring partial visible male data. Our system is also capable of navigating atmospheric simulation data (<ref type="figure" coords="8,43.95,592.57,23.91,8.12" target="#fig_4">Fig. 17</ref>). We choose the wind speed channel of the Hurricane Isabel data set for demonstration. By eraser and rainbow tools, several major structures are revealed in the result. Users can interactively explore and understand the data with our tool. The visual effect can be further improved by contrast enhancement, etc. Compared to traditional transfer function design tools, our tool is more intuitive and natural for scientific data exploration. Based on user feedback from several users, especially from users new to volume rendering, the proposed method is more intuitive than traditional transfer function design. It is natural and direct to operate in image space, without the trial-and-error process necessary with feature space. The goal-oriented data exploration process also makes direct volume rendering more usable and accessible for users. In the future, we plan to perform a formal user study to thoroughly investigate the system. </p><formula>(a) </formula><p>(b) <ref type="figure" coords="8,285.12,210.80,23.00,7.64" target="#fig_5">Fig. 18</ref>. Illustration of the global operation limitation: (a) user sketch on the air bladder of the CT carp data with colorization tool; (b) both the air bladder and the skin, which are with the same properties, are turned to green. </p><p>(a) (b) <ref type="bibr" coords="8,285.12,403.13,23.48,7.64">Fig. 19</ref>. Relationships between the quality of ambiguity exclusion and the window size of local image analysis: (a) the window size is too small to distinguish the semantic foreground (green materials) and the background (yellow materials), which leads to high ambiguity; (b) the window size is enough to ensure the green materials are the intended feature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">LIMITATIONS AND KNOWN ISSUES</head><p>There are a few limitations and known issues of the proposed system. First, all the operations on the rendered image are global, instead of local operations. During the interaction, effects are taken on all features with the same properties as the user sketches on the image. In <ref type="figure" coords="8,507.96,515.57,23.83,8.12" target="#fig_5">Fig. 18</ref>, we demonstrate how the user failed to change the color of the surface of the air bladder, while leaving the other parts unchanged. However, since the surface of the air bladder and the skin of the carp are within the same range of intensity values, they are not able to be distinguished by the classification of 1D transfer functions. Second, the exclusion of ambiguity may be unsatisfactory when the visibility and depths of overlapped transparent materials are too close. Meanwhile, the footprint of these materials on the image are much larger than the window size of local image analysis (c.f. Section 5.2.1). In <ref type="figure" coords="8,295.46,615.22,24.53,8.12" target="#fig_7">Fig. 19</ref>, the visibility of both yellow and green materials are very close, leading to high ambiguity. This problem could be fixed by enlarging the window size (<ref type="figure" coords="8,378.22,635.14,23.50,8.12" target="#fig_7">Fig. 19</ref>(b)), but this would be at the cost of sacrificing more computation time. Third, the initial parameter configurations may influence the exploration process, e.g. initial transfer functions, the viewpoints, etc. In addition, the frame rate of the direct volume rendering also has a big impact on the visual effect. If the frame rate is too low, the interactivity of the system is significantly reduced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS AND FUTURE WORKS</head><p> In this work, we present a novel WYSIWYG volume visualization system that accepts direct sketch on the rendered images. Users can ex-plore and change the appearance of the direct volume rendered images by sketching on the volume rendered image with various tools, and the system gives real-time feedback for users during the interaction. To achieve this goal, several algorithms are utilized for the interactive operations, including image and data space analysis, and the semantic realization process. The proposed system is flexible and intuitive for volume data exploration and visualization. A few extensions and applications for this work can be developed in the future. More NPR effects other than silhouette can be added into the current system to make it available for sketch-based illustrative visualization design. For the intention inference algorithm, we would like to further develop specific strategies for interval volume render- ing <ref type="bibr" coords="9,45.78,173.86,9.52,8.12" target="#b0">[1]</ref>, which provide a wider spectrum of visual classification. We would also like to extend the current system to facilitate multidimensional and multivariate transfer function design for more applications. Furthermore, it is noticeable that local transfer functions <ref type="bibr" coords="9,233.75,203.75,14.94,8.12" target="#b22">[24] </ref> and twolevel volume rendering <ref type="bibr" coords="9,118.49,213.71,14.94,8.12" target="#b14">[15] </ref>with segmentation information provides better classification for local structures of the data, instead of global mapping, but the parameter space is even more complicated. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,31.50,193.90,513.07,7.64;3,31.50,203.37,502.04,7.64"><head>Fig. 2. </head><figDesc>Fig. 2. The pipeline of the system. From user sketch input, the intentional features are derived by image space analysis and data space analysis. The new rendering parameters are determined according to the user tool type, before the new rendering result is fed back to users in real time. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,294.12,324.85,250.39,7.64;3,294.12,334.32,250.40,7.64;3,294.12,343.78,183.25,7.64"><head>Fig. 3. </head><figDesc>Fig. 3. The animation sequence of the eraser operation. The mouse is pressed in (a), and then it is moved along the dashed line in (b) and (c). The skin gradually disappears as the mouse moves. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,294.12,456.42,250.38,7.64;3,294.12,465.89,250.43,7.64;3,294.12,475.35,250.44,7.64;3,294.12,484.82,142.88,7.64"><head>Fig. 4. </head><figDesc>Fig. 4. The peeling slider for layer exploration. After clicking on a seed point on the image, users can explore the automatically extracted outer features by sliding on the widget. The appearance of different feature layers are shown in each slice of the pie. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,294.12,248.07,250.39,7.64;5,294.12,257.53,250.41,7.64;5,294.12,266.99,250.41,7.64;5,294.12,276.46,135.23,7.64"><head>Fig. 7. </head><figDesc>Fig. 7. The clustering result of data space analysis: (a) visualization of the multivariate samples with parallel coordinates, presenting four numerical properties and the cluster membership information; (b) the corresponding features of the clusters. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,294.12,427.90,250.38,7.64;5,294.12,437.36,250.42,7.64;5,294.12,446.83,19.38,7.64"><head>Fig. 8. </head><figDesc>Fig. 8. Dependency relationships of the semantic realization algorithm. The realization algorithm of higher level semantics is built upon the basic ones. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,22.50,212.02,250.38,7.64;6,22.50,221.49,250.42,7.64;6,22.50,230.96,250.39,7.64;6,22.50,240.42,250.40,7.64;6,22.50,249.88,37.09,7.64"><head>Fig. 9. </head><figDesc>Fig. 9. The colorization operation: (a) the initial status; (b) the skin painted with red color; (c) the bone painted with yellow color; (d) the bone painted with red color; (e) the skin colorized with yellow. The black curves illustrate how the results can be achieved by painting on the image. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,22.50,358.42,250.38,7.64;6,22.50,367.88,250.43,7.64;6,22.50,377.35,39.43,7.64"><head>Fig. 10. </head><figDesc>Fig. 10. The eraser operation: (a) the opacity of the skin portion is increased; (b) the initial status; (c) the opacity of the skin portion is decreased. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="6,285.12,137.25,250.39,7.64;6,285.12,146.72,250.43,7.64;6,285.12,156.18,30.56,7.64"><head>Fig. 11. </head><figDesc>Fig. 11. The brightness operation: (a) the brightness of the bone is increased; (b) the initial status; (c) the brightness of the bone is de- creased. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="6,285.12,296.12,250.38,7.64;6,285.12,305.58,220.05,7.64"><head>Fig. 12. </head><figDesc>Fig. 12. Rainbow operation: (a) before operation; (b) after operation. The color strip in (b) is the user selected rainbow for the effect. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="6,285.12,447.55,243.65,7.64"><head>Fig. 13. </head><figDesc>Fig. 13. Boundary fuzziness: (a) before operation; (b) after operation. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="7,31.50,170.72,250.39,7.64;7,31.50,180.18,15.06,7.64"><head>Fig. 14. </head><figDesc> Fig. 14. Contrast enhancement: (a) before operation; (b) after opera- tion. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="8,22.50,510.70,250.39,7.64"><head>Fig. 17. </head><figDesc>Fig. 17. The possible process in the exploration of Hurricane Isabel data </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false" coords="3,39.57,53.62,491.09,115.49"><figDesc coords="3,476.17,112.70,47.64,6.61;3,483.87,121.92,42.76,5.29;3,483.86,129.30,46.81,5.29;3,367.07,118.14,33.98,7.01;3,367.07,127.08,26.29,7.01;3,342.30,72.94,41.40,6.53;3,391.58,118.14,9.48,7.01;3,379.18,118.14,20.12,7.01;3,380.22,127.08,13.14,7.01;3,367.07,118.14,18.67,7.01;3,367.07,127.08,19.69,7.01;3,379.18,118.14,20.12,7.01;3,376.28,72.94,7.43,6.53;3,367.07,118.14,18.67,7.01;3,342.30,72.94,41.40,6.53;3,39.57,70.63,55.15,7.08">New Parameters Transfer Functions Shading Parameters Intentional Features User Tool Type nal ntiona ures Intent Featur ntiona pe Intent User Tool Type User Sketch Input</figDesc><table coords="3,66.57,53.62,462.70,115.49">New 
Rendering 
Result 

Initial 
Rendering 
Result 

Data Space Analysis 

Data Value 
... 
0.853 
0.881 
0.872 
... 

Visibility 
... 
0.853 
0.881 
0.872 
... 

Depth 
... 
0.853 
0.881 
0.872 
... 

Real-Time GPU Clustering 

Seed 
Foreground 

Background 

Image Space Analysis 

Fo Fo Fo Fo Fo Fo Fore re regr gr grou ou ound nd nd nd nd nd nd 

Ba Ba Back ck ckgr gr gr gr gr gr grou ou ound nd nd nd 

Se Se Se Se Seed ed ed ed ed ed 
Se Se 

Semantic 
Realization 

Opacity 

Color 

Shading 

Volume Data 

User Input 

Real-Time Feedbacks 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false" coords="6,88.53,736.33,42.85,8.12"><figDesc coords="6,88.53,736.33,42.85,8.12">and Fig. 11.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false" coords="7,294.12,114.57,250.39,175.94"><figDesc coords="7,294.12,114.57,250.39,8.53;7,294.12,124.04,250.39,7.64;7,294.12,133.50,241.72,7.64">Table 1. The average timings of the system. T r , T i , T d , T s are the timings (in milliseconds) of rendering, image space analysis, data space analysis, semantic realization; T is the total time of feature inference.</figDesc><table coords="7,443.45,176.85,51.25,62.96">+ 
+ 

-

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p> The authors wish to thank the anonymous reviewers for their comments . This work is supported by National Natural Science Foundation of China Project No. 60903062, Beijing Natural Science Foundation Project No. 4092021, 863 Program Project 2010AA012400, Chinese Ministry of Education Key Project No. 109001. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="9,49.76,348.97,232.13,7.22;9,49.76,358.44,205.23,7.22"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Direct interval volume visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ament</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Carr</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1505" to="1514" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,367.90,232.13,7.22;9,49.76,377.37,202.23,7.22"  xml:id="b1">
	<analytic>
		<title level="a" type="main">The contour spectrum</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">L</forename>
				<surname>Bajaj</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Pascucci</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Schikore</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,386.83,232.12,7.22;9,49.76,396.30,232.10,7.22;9,49.76,405.76,69.94,7.22"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Volumeshop: An interactive system for direct volume illustration</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization 2005</title>
		<meeting>IEEE Visualization 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="671" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,415.22,232.12,7.22;9,49.76,424.69,168.72,7.22"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Direct volume editing</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Bürger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Krüger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Westermann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1388" to="1395" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,434.15,232.13,7.22;9,49.76,443.62,232.09,7.22;9,49.76,453.09,37.85,7.22"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Size-based transfer functions: A new volume exploration technique</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Correa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1380" to="1387" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,462.54,232.09,7.22;9,49.76,472.01,232.11,7.22;9,49.76,481.47,37.85,7.22"  xml:id="b5">
	<analytic>
		<title level="a" type="main">The occlusion spectrum for volume classification and visualization</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Correa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1465" to="1472" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,490.94,232.11,7.22;9,49.76,500.41,199.52,7.22"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Visibility driven transfer functions</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Correa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization</title>
		<meeting>IEEE Pacific Visualization</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,509.87,232.13,7.22;9,49.76,519.33,225.38,7.22"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Visibility histograms and visibility-driven transfer functions</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Correa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="204" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,528.80,232.13,7.22;9,49.76,538.26,232.10,7.22;9,49.76,547.73,148.00,7.22"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast visualization of object contours by non-photorealistic volume rendering</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Csébfalvi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Mroz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hauser</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>König</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="452" to="460" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,557.19,232.12,7.22;9,49.76,566.66,232.09,7.22;9,49.76,576.12,69.94,7.22"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Volume illustration: non-photorealistic rendering of volume models</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ebert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Rheingans</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,585.58,232.13,7.22;9,49.76,595.05,232.12,7.22;9,49.76,604.55,232.11,7.11;9,49.76,613.98,123.90,7.22"  xml:id="b10">
	<analytic>
		<title level="a" type="main">High-quality pre-integrated volume rendering using hardware-accelerated pixel shading</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Engel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kraus</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ertl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HWWS &apos;01: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS workshop on Graphics hardware</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,623.45,232.09,7.22;9,49.76,632.91,232.11,7.22;9,49.76,642.37,152.57,7.22"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-based transfer function design for data exploration in volume visualization</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Fang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Biddlecome</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Tuceryan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,651.83,232.13,7.22;9,49.76,661.30,232.11,7.22;9,49.76,670.77,90.09,7.22"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Geman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Geman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,680.23,232.12,7.22;9,49.76,689.70,232.11,7.22;9,49.76,699.16,232.10,7.22;9,49.76,708.62,17.93,7.22"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-dimensional transfer function design based on flexible dimension projection embedded in parallel coordinates</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Guo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Xiao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Yuan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization 2011</title>
		<meeting>IEEE Pacific Visualization 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,718.09,232.15,7.22;9,49.76,727.55,232.09,7.22;9,49.76,737.02,202.23,7.22;9,294.12,53.98,250.38,7.22;9,312.38,63.45,232.10,7.22;9,312.38,72.91,126.83,7.22"  xml:id="b14">
	<analytic>
		<title level="a" type="main">High-quality two-level volume rendering of segmented data sets on consumer graphics hardware Generation of transfer functions with stochastic search techniques</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hadwiger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Berger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hauser</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization 2003 Proceedings of IEEE Visualization</title>
		<editor>16] T. He, L. Hong, A. E. Kaufman, and H. Pfister</editor>
		<meeting>IEEE Visualization 2003 IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,82.37,232.12,7.22;9,312.38,91.83,232.11,7.22;9,312.38,101.30,49.80,7.22"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Teddy: A sketching interface for 3d freeform design</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Igarashi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Matsuoka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Tanaka</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1999</title>
		<meeting>ACM SIGGRAPH 1999</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,110.77,232.12,7.22;9,312.38,120.23,232.11,7.22;9,312.38,129.70,232.10,7.22;9,312.38,139.16,130.36,7.22"  xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">D</forename>
				<surname>Kalnins</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Markosian</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">J</forename>
				<surname>Meier</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Kowalski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">C</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">L</forename>
				<surname>Davidson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Webb</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">F</forename>
				<surname>Hughes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Finkelstein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WYSIWYG NPR: drawing strokes directly on 3D models Proceedings of ACM SIGGRAPH 2002</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="755" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,148.62,232.13,7.22;9,312.38,158.09,232.11,7.22;9,312.38,167.55,180.39,7.22"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-automatic generation of transfer functions for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">L</forename>
				<surname>Kindlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">W</forename>
				<surname>Durkin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VVS &apos;98: Proceedings of IEEE Symposium on Volume visualization</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,177.02,232.13,7.22;9,312.38,186.49,232.09,7.22;9,312.38,195.95,232.09,7.22;9,312.38,205.41,17.93,7.22"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Curvaturebased transfer functions for direct volume rendering: Methods and applications</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">L</forename>
				<surname>Kindlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">T</forename>
				<surname>Whitaker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tasdizen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization 2003</title>
		<meeting>IEEE Visualization 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,214.87,232.13,7.22;9,312.38,224.34,232.09,7.22;9,312.38,233.81,128.14,7.22"  xml:id="b19">
	<analytic>
		<title level="a" type="main">Contextual picking of volumetric structures</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kohlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kanitsar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Groller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,243.27,232.10,7.22;9,312.38,252.74,136.53,7.22"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Levin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lischinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Weiss</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,262.20,232.12,7.22;9,312.38,271.66,78.35,7.22"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Levoy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,281.13,232.10,7.22;9,312.38,290.59,232.10,7.22;9,312.38,300.06,118.08,7.22"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Local histograms for design of transfer functions in direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Lundström</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput . Graph</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1570" to="1579" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,309.52,232.11,7.22;9,312.38,318.99,175.61,7.22"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature peeling</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">M</forename>
				<surname>Malik</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface</title>
		<meeting>Graphics Interface</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,328.45,232.13,7.22;9,312.38,337.91,232.11,7.22;9,312.38,347.38,232.13,7.22;9,312.38,356.85,232.11,7.22;9,312.38,366.31,130.36,7.22"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Design galleries: a general approach to setting parameters for computer graphics and animation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Marks</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Andalman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">A</forename>
				<surname>Beardsley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">T</forename>
				<surname>Freeman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Gibson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Hodgins</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Kang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Mirtich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Pfister</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ruml</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Ryall</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Seims</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">M</forename>
				<surname>Shieber</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1997</title>
		<meeting>ACM SIGGRAPH 1997</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,375.78,232.11,7.22;9,312.38,385.23,232.13,7.22;9,312.38,394.70,49.80,7.22"  xml:id="b25">
	<analytic>
		<title level="a" type="main">Volume catcher</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Owada</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Nielsen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Igarashi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Symposium on Interactive 3D Graphics and Games</title>
		<meeting>the 2005 Symposium on Interactive 3D Graphics and Games</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,404.17,232.16,7.22;9,312.38,413.63,232.11,7.22;9,312.38,423.10,49.80,7.22"  xml:id="b26">
	<analytic>
		<title level="a" type="main">Moment curves</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Patel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Haidacher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J.-P</forename>
				<surname>Balabanian</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Groller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,432.56,232.11,7.22;9,312.38,442.02,232.13,7.22;9,312.38,451.49,187.07,7.22"  xml:id="b27">
	<analytic>
		<title level="a" type="main">The transfer function bake-off</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Pfister</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">E</forename>
				<surname>Lorensen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">L</forename>
				<surname>Bajaj</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">L</forename>
				<surname>Kindlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">J</forename>
				<surname>Schroeder</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">S</forename>
				<surname>Avila</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Martin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Machiraju</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="22" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,460.95,232.13,7.22;9,312.38,470.42,232.11,7.22;9,312.38,479.88,17.93,7.22"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic layers for illustrative volume rendering</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Rautek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1336" to="1343" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,489.35,232.12,7.22;9,312.38,498.81,232.11,7.22;9,312.38,508.27,33.86,7.22"  xml:id="b29">
	<analytic>
		<title level="a" type="main">Interaction-dependent semantics for illustrative volume rendering</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Rautek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="847" to="854" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,517.74,232.14,7.22;9,312.38,527.21,232.10,7.22;9,312.38,536.67,103.90,7.22"  xml:id="b30">
	<analytic>
		<title level="a" type="main">High-level user interfaces for transfer function design with semantics</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rezk-Salama</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Keller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kohlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1021" to="1028" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,546.14,232.10,7.22;9,312.38,555.60,232.11,7.22;9,312.38,565.06,210.64,7.22"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Stroke-based transfer function design</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J.-S</forename>
				<surname>Praßni</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Steinicke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">H</forename>
				<surname>Hinrichs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/EG International Symposium on Volume and Point-Based Graphics</title>
		<meeting>IEEE/EG International Symposium on Volume and Point-Based Graphics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,574.53,232.13,7.22;9,312.38,583.99,232.09,7.22;9,312.38,593.46,125.16,7.22"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualization of boundaries in volumetric data sets using LH histograms</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Sereda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">V</forename>
				<surname>Bartrolí</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Serlie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">A</forename>
				<surname>Gerritsen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="218" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,602.92,232.12,7.22;9,312.38,612.39,232.10,7.22;9,312.38,621.85,119.75,7.22"  xml:id="b33">
	<analytic>
		<title level="a" type="main">A novel interface for higherdimensional classification of volume data</title>
		<author>
			<persName>
				<forename type="first">F.-Y</forename>
				<surname>Tzeng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">B</forename>
				<surname>Lum</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization 2003</title>
		<meeting>IEEE Visualization 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,631.31,232.12,7.22;9,312.38,640.78,232.09,7.22;9,312.38,650.25,125.16,7.22"  xml:id="b34">
	<analytic>
		<title level="a" type="main">An intelligent system approach to higher-dimensional classification of volume data</title>
		<author>
			<persName>
				<forename type="first">F.-Y</forename>
				<surname>Tzeng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">B</forename>
				<surname>Lum</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,659.71,232.10,7.22;9,312.38,669.18,232.11,7.22;9,312.38,678.63,77.24,7.22"  xml:id="b35">
	<analytic>
		<title level="a" type="main">Interactive transfer function design based on editing direct volume rendered images</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Qu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1027" to="1040" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,688.10,232.12,7.22;9,312.38,697.57,137.53,7.22"  xml:id="b36">
	<monogr>
		<title level="m" type="main">Volume cutout. The Visual Computer</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Yuan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">X</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="8" to="10745" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
