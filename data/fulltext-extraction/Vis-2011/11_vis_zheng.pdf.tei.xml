<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T15:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">(b) (c) (d) (e) (f) (g) (h) iView: A Feature Clustering Framework for Suggesting Informative Views in Volume Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Ziyi</forename>
								<surname>Zheng</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Nafees</forename>
								<surname>Ahmed</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Klaus</forename>
								<surname>Mueller</surname>
								<roleName>Senior Member, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">(b) (c) (d) (e) (f) (g) (h) iView: A Feature Clustering Framework for Suggesting Informative Views in Volume Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Fig. 1. The tooth dataset – the set of 7 salient representative viewpoints returned by the SCP solver. (a): the initial entropy map, (b-h): the images rendered from the suggested viewpoints (left) with remaining entropy map (right). (g): modifying the transfer function to see the detailed shape of the tooth surface. N 0.0 0.2 0.4 0.6 0.8 1.0 S (a) Abstract—The unguided visual exploration of volumetric data can be both a challenging and a time-consuming undertaking. Identifying a set of favorable vantage points at which to start exploratory expeditions can greatly reduce this effort and can also ensure that no important structures are being missed. Recent research efforts have focused on entropy–based viewpoint selection criteria that depend on scalar values describing the structures of interest. In contrast, we propose a viewpoint suggestion pipeline that is based on feature-clustering in high-dimensional space. We use gradient/normal variation as a metric to identify interesting local events and then cluster these via k-means to detect important salient composite features. Next, we compute the maximum possible exposure of these composite feature for different viewpoints and calculate a 2D entropy map parameterized in longitude and latitude to point out promising view orientations. Superimposed onto an interactive track-ball interface, users can then directly use this entropy map to quickly navigate to potentially interesting viewpoints where visibility-based transfer functions can be employed to generate volume renderings that minimize occlusions. To give full exploration freedom to the user, the entropy map is updated on the fly whenever a view has been selected, pointing to new and promising but so far unseen view directions. Alternatively, our system can also use a set-cover optimization algorithm to provide a minimal set of views needed to observe all features. The views so generated could then be saved into a list for further inspection or into a gallery for a summary presentation. Index Terms—Direct volume rendering, k-means, entropy, view suggestion, set-cover problem, ant colony optimization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The visual exploration and extraction of relevant information from 3D volume data can be a daunting task as it often requires users to try out many different combinations of views and transfer functions. In this regard, having proper views to start with can greatly improve the efficiency of the data exploration process. Hence, given an arbitrary volume dataset, the suggestion of a set of interesting views has been a research topic of great interest, but also one of challenges. Most of the recent research in view selection uses metrics based on scalar values to locate regions of interest, that is, before selecting the views the user is either required to design a 1D transfer function <ref type="bibr" coords="1,293.94,571.52,10.94,9.96" target="#b4">[5]</ref><ref type="bibr" coords="1,304.88,571.52,14.58,9.96" target="#b33">[35] </ref>or perform a segmentation <ref type="bibr" coords="1,419.66,571.52,10.51,9.96" target="#b5">[6] </ref>to classify these regions. Then, once this has occurred, the viewpoint selection algorithm will search for the best viewpoint to display the maximum amount of information. However, there are a number of potential pitfalls with this methodology. First, before we can start searching for the best view, the user is required to know the scalar values of the hidden structures to be classified. But without having a proper look at the data first (and without the presence of strong domain knowledge), the user might not have a clear idea what these structures actually are, even in a coarse sense. Any initial guess will likely not be able to classify the hidden features successfully, and so the view selection algorithm in turn will not help to find the best viewpoint. Also, due to the fact that these methods require input in form of a transfer function or segmentation, if the user decides to change either of these a re-computation of the entire pipeline is needed to suggest the new best view. This iterative process can potentially take a long time and thus makes exploring transfer functions in an interactive manner impossible. Secondly, many structures in 3D volumetric data require more than scalar values to be classified properly. They may require gradient magnitudes (or even higher-order metrics), in conjunction with multi-dimensional transfer functions. Given all these inherent shortcomings, purely scalar-based view selection algorithms can be quite limited for practical use. Also, current view selection algorithms proposed for volume rendering were built around the concept of selecting a single best possible view to visualize the volume dataset. But in situations in which the viewer is interested in visualizing all of the interesting features in a comprehensive manner, suggesting only one view can be inadequate. While one good view might be able to extract the bulk of the salient features, more views are often necessary in the likely event that some features are occluded in this chosen best view. Acknowledging the fact that good user-specified 1D transfer functions or proper segmentations are difficult to obtain and may be inadequate to be effective, we propose a view suggestion framework rooted in high-dimensional feature space which does not rely on transfer functions or volume segmentations as an initial input. By applying high-dimensional (low-level) feature clustering, our proposed method can automatically detect salient composite features and based on this analysis suggest promising viewpoints. The user may then inspect these promising views more closely by interactively exploring the transfer function, or run one of numerous automatic algorithms to optimize visibility. We henceforth call this method viewpoint suggestion since it helps users to navigate to favorable positions that potentially show interesting structures. In this way our approach is less ambitious than a full-fledged view selection pipeline, but at the same time more versatile and appropriate for data exploration. It supports the extraction of a set of good views appropriately refined with different transfer functions instead of a set of good views limited to one fixed transfer function. Our approach also offers advantages in terms of interactive data exploration. This is especially important when users are not certain about the properties of the structures of interest and need to refine the renderings. Traditional view selection algorithms <ref type="bibr" coords="2,221.74,399.64,10.50,9.96" target="#b4">[5]</ref><ref type="bibr" coords="2,232.24,399.64,10.50,9.96" target="#b5">[6] </ref>involve full volume rendering from all possible viewpoints and calculate the viewpoint entropy from all voxels. This can result in prohibitive wait times if users want to update the transfer function during the search for the best viewpoint. In contrast, our framework splits this process into two separate and subsequent phases: (1) the determination of a set of good viewpoints using relatively inexpensive operations (GPU-accelerated clustering, cluster-based visibility test and entropy calculation), and (2) the interactive refinement of the transfer function from these promising viewpoints to generate the desired salient volume rendered images. This approach allows users to maintain a stable spatial reference (and one at which many features are visible), while exploring these features and their relationships one by one by modifying the transfer function. To address the problem of fully covering all features in a volume, our viewpoints suggestion framework provides a two-fold solution. First, our system not only suggests a single best viewpoint, but also provides an interactive navigation interface where all the views are color-labeled with a relative measure of feature exposure. Also, the system can progressively mark visited features and only show the distribution of the unknown, yet undiscovered information. This type of interaction has already been useful in specialized applications, for example in virtual colonoscopy <ref type="bibr" coords="2,151.50,619.58,14.98,9.96" target="#b16">[18] </ref>where colon wall patches already viewed are painted in green on the fly-through, enabling doctors to focus their attention onto unpainted areas. Second, to effectively guide users in their exploration of the entire volume, our system provides an automated viewpoint suggestion module, effectively minimizing the number of views to be inspected. Our multiple-viewpoint suggestion algorithm seeks to determine the minimal set of views that can cover all features, which maps to the Set Cover Problem (SCP). Our set-cover problem solver together with the interactive viewpoint navigation tool then aids users in gaining a complete understanding of the features in the volume. </p><p>In this paper, we concentrate on the problem of suggesting to the user a set of potentially good views. We leave the design of the transfer function needed to highlight the exposed features to the user, supported by suitable existing interactive algorithms. There are many of these (e.g. <ref type="bibr" coords="2,334.31,89.66,10.64,9.96" target="#b8">[9]</ref><ref type="bibr" coords="2,344.95,89.66,14.19,9.96" target="#b9">[10]</ref>) and so we do not discuss this issue further. Our contributions can then be summarized as follows:  Our high-dimensional clustering-based view suggestion framework is purely feature-based and acts before transfer function design. It informs users of promising views before laborious transfer function exploration even begins and so prevents " dead-end " transfer function exploration experiences.  Our view suggestion framework is adaptive to what the user has already seen. Users can explore the entire view-space with progressive suggestions of promising viewpoints. This facilitates a fully unconstrained volume exploration, but ensures that all important features are eventually seen.  Our system provides viewpoint set solutions that are optimal. We achieve this by using a set cover problem solver, which adapts to the set of views selected by the user so far. Our paper is structured as follows. In Section 2 we discuss related work. Section 3 provides an overview of the theoretical basis and Section 4 presents the practical consideration of individual components in the framework. Section 5 discusses results. Section 6 presents a user study and Section 7 ends with conclusions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In the following we review previous work related to our system. <ref type="bibr" coords="2,433.84,333.68,14.98,9.96" target="#b35">[37] </ref>applied the concept of viewpoint entropy to determine the best viewpoints for polygonbased scenes. Bordoloi et al. <ref type="bibr" coords="2,390.53,353.66,10.51,9.96" target="#b4">[5] </ref>proposed to use voxel-based entropy to select viewpoints in volume rendering, assuming that transfer functions are given. Takahashi et al. <ref type="bibr" coords="2,440.51,373.64,14.98,9.96" target="#b33">[35] </ref>proposed a similar framework based on iso-surface entropy, weighted by a given transfer function. Chan et al. <ref type="bibr" coords="2,405.49,393.67,10.51,9.96" target="#b5">[6] </ref>extended Bordoloi's work by considering spatial relations between structures, after a userspecified segmentation has been given. Our viewpoint suggestion algorithm is fundamentally different from these works as it does not depend on either prior transfer functions or segmentation. Our feature-definition is different from that of <ref type="bibr" coords="2,448.89,443.65,60.49,9.96">Takahashi et al. </ref>where features in the volume are defined as a set of iso-surfaces. Rather, in our case the feature metric is sensitive to local structures. This allows for the detection of very delicate features giving rise to as slight normal perturbations, such as text on a surface. Other importance metrics to define interesting features found in the literature, such as suggestive-contours <ref type="bibr" coords="2,359.55,503.64,15.01,9.96" target="#b10">[11] </ref>could also be readily incorporated. Our framework supports multiple viewpoint selection. The selection of multiple views (or view planning) has found application in many domains. A variety of methods seek to solve the next best view problem, such as the entropy-based method <ref type="bibr" coords="2,466.70,543.65,13.79,9.96" target="#b38">[40]</ref> , the visibilitybased method <ref type="bibr" coords="2,339.05,553.62,13.71,9.96" target="#b13">[14]</ref>, and the silhouette-based method <ref type="bibr" coords="2,481.12,553.62,9.52,9.96" target="#b0">[1]</ref>. They have wide application in the placement of laser sensors <ref type="bibr" coords="2,482.99,563.63,10.52,9.96" target="#b3">[4] </ref>and RFID sensors <ref type="bibr" coords="2,314.56,573.65,14.99,9.96" target="#b39">[41] </ref> and for determining the best circular trajectory in conebeam CT <ref type="bibr" coords="2,326.50,583.61,9.52,9.96" target="#b1">[2]</ref>. These view planning methods cannot be directly applied in volume rendering but they can provide useful insight in creating our pipeline, since we define local features in the volume and then solve a similar set cover problem conceptually. Transfer Function: In volume rendering, material classification is often done via transfer functions. The traditional 1D transfer function is based on scalar values only. Recent research has investigated a plurality of new transfer function domains which have been used together with scalar values and results from these are very promising. They include gradient magnitude <ref type="bibr" coords="2,458.51,673.61,13.71,9.96" target="#b19">[21]</ref>, curvature <ref type="bibr" coords="2,518.39,673.61,13.71,9.96" target="#b18">[20]</ref>, features size <ref type="bibr" coords="2,341.50,683.62,9.47,9.96" target="#b7">[8]</ref>, occlusion spectrum <ref type="bibr" coords="2,442.19,683.62,10.49,9.96" target="#b8">[9] </ref>and visibility <ref type="bibr" coords="2,518.24,683.62,13.70,9.96" target="#b9">[10]</ref>. Perception can be also added into the transfer function design <ref type="bibr" coords="2,522.75,693.64,9.53,9.96" target="#b6">[7]</ref>. Mai et al. <ref type="bibr" coords="2,329.42,703.60,14.98,9.96" target="#b24">[26] </ref>presented a semi-automatic 2D transfer function design method based on segmented data. These user-controlled or semi-automatic transfer functions assume a given viewpoint. Our framework provides a potentially favorable starting point for these transfer function optimization techniques by suggesting good views at which they can be applied. As mentioned, we do not consider the transfer function design before the viewpoint suggestion, due to the additional burden caused from requiring all possible viewpoints to be volume rendered a-priori. Works on designing transfer function based on feature clustering also exist. Sereda et al. <ref type="bibr" coords="3,124.90,119.65,14.99,9.96" target="#b31">[33] </ref>proposed to use clustering to design transfer function. Maciejewski et al. <ref type="bibr" coords="3,167.02,129.67,14.98,9.96" target="#b23">[25] </ref>proposed feature detection in 2D transfer function space automatically or semi-automatically. We focus on using clustering in the context of viewpoint suggestion. View Enhancement: Focus+context techniques are widely used to enhance the volume rendering. Wang et al. <ref type="bibr" coords="3,209.70,169.63,15.01,9.96" target="#b37">[39] </ref>introduced the magnification lens into volume rendering. Viola et al. <ref type="bibr" coords="3,231.01,179.65,15.01,9.96" target="#b36">[38] </ref>proposed an automatic cut-away view based on assigned importance weight on segmentation. Krüger et al. devised the ClearView <ref type="bibr" coords="3,217.35,199.63,15.01,9.96">[23] </ref>system using spherical hot-spots based on discrete curvature based importance. There is also research on adding multiple view information in a single view. Kohlmann et al. <ref type="bibr" coords="3,147.57,229.62,14.94,9.96" target="#b20">[22] </ref>presented a deformed viewing sphere based on history. Sudarsanam et al. <ref type="bibr" coords="3,188.39,239.64,14.98,9.96" target="#b32">[34] </ref>proposed a widget to incorporate multiple views into a single image. In our work, we have focused on views without distortion and without advanced highlighting, but incorporating these advanced rendering techniques would further benefit our view suggestion framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Viewpoint Entropy</head><p>Simply speaking, a good view onto a volume can be defined as the one that reveals the maximum amount of features relevant to the viewer. Exactly what properties of the volumetric data are relevant to the viewer and need exposure depends mainly on the kind of problem at hand and what the viewer is really looking for. But, once we have defined the feature set, what we then need to find are the views that can show the features distinctly on the screen through graphical rendering. To facilitate comparison among all these views, we assign a score to each of them. And to compute this score, we apply concepts from information theory in a similar fashion as in previous work <ref type="bibr" coords="3,85.63,437.36,10.40,9.96" target="#b4">[5]</ref>[6]<ref type="bibr" coords="3,106.44,437.36,13.87,9.96" target="#b33">[35]</ref>. Information theory defines entropy as a measure of uncertainty associated with an information source. Since, to resolve this uncertainty, the amount of data we need to transmit to the receiver defines the amount of information content, entropy of the source hence also measures information. Let us consider any information source A which transmits a random sequence of symbols taken from alphabet a , a , … , a K where occurrence probabilities are p , p , … , p K . Entropy of this information flow is given by, log </p><formula>(1) </formula><p>Now, say, in addition to the given probabilities, the receiver also has the knowledge that a certain symbol is always followed by some other symbol, . Presence of this knowledge to the receiver, let us define it as E, reduces uncertainty regarding the source. The entropy after this knowledge would be, |. In the context of volumetric data, we have an information source – the volume itself, the information receiver – the viewer and a transmission process which includes the whole pipeline of volume rendering. If the volume is not shown to the viewer, then the uncertainty associated with the volume is at maximum and it represents the total information content of the volume, say we denote this by H(X). Now, in the event that we render a particular view , partial information of the volumetric features become revealed to the user. The uncertainty remaining can roughly be defined as | . Since we are interested in finding the view that reveals most information, from the perspective of information theory, what we want is to find a view that minimizes | with respect to all possible views. From the chain rule of entropy we can write, </p><formula>| , (2) </formula><p>Here, , is the information content of the volume and its view taken together. denotes entropy of a particular view and as such is a measure of information content of a rendered view. Since a view is just a projection of the volume data, we can consider , to be constant across views. Hence, minimizing | effectively means maximizing . So, a good view is identified as the one that has large view entropy. A straightforward way <ref type="bibr" coords="3,378.74,131.60,10.50,9.96" target="#b4">[5] </ref><ref type="bibr" coords="3,391.77,131.60,15.00,9.96" target="#b33">[35] </ref>to measure the view entropy requires a transfer function to perform volume rendering for the view. Our entropy estimation is different since we do not want to involve transfer function as an input and later restrict the decision on a single fixed transfer function. Instead, we propose to measure the maximum possible information (across all possible transfer functions) for a viewpoint. This is possible since the 2D image generated by computing the volume rendering equation depends on not only the transfer function but also on the shading (lighting) effect. Shading plays a significant role in conveying information about shape and is well-studied in computer graphics. We define potential information for shading as blobs of voxels and we compute the entropy based on how well one can resolve these feature-clusters at a given view based on the shading (lighting) effect. It serves as an extension of Bordoloi's work <ref type="bibr" coords="3,410.84,271.57,10.50,9.96" target="#b4">[5] </ref>in which the visibility of each voxel is computed together with the transfer function to evaluate the entropy. Here we group voxels in the volume according into clusters. Then probability distributions associated with voxel-clusters are needed to calculate entropy. Let represent the contribution of cluster in a viewpoint. is a special case indicating the background volume (containing all voxels that do not belong to any clusters). Then the view entropy for a certain view is: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>· log </head><p>(3) 1 · where </p><formula>(4) </formula><p>Here is the total number of feature clusters. The factor will make sure that all sum up to 1. is the visibility of cluster in view . the noteworthiness <ref type="bibr" coords="3,417.60,436.28,10.50,9.96" target="#b4">[5] </ref>of cluster , is defined as: </p><formula>log log || | ∑ || | K (5) </formula><p>where represent cluster 's probability, calculated from the number of voxels in cluster normalized by the total number of voxels. We add the consideration of background. is number of voxels that do not belong to any cluster, is the probability of background and is the noteworthiness factor for background. In Sec. 4.1 we present our method for computing the importance of features that is sensitive to shading but independent of any transfer function. Sec. 4.2 defines the computation of view entropy based on the selected metric. Finally, Sec. 4.3 shows how we present the data computed from Sec. 4.2 in both a suggestive and explorable manner. When we are suggesting views onto the volume, besides finding a single view that reveals information in the best way, we also want to guide the user such that they will not miss out on any of the features. In the language of information theory, a single view might be the optimal view in terms of view entropy but it may be the case that | 0. So, we propose to suggest to the user a set of views , , , … so that | , , , . . , , =0, that is, to find a set of views that collectively can give the viewer a total picture of all the important features of the volume data. Sec. 3.2 and Sec. 4.4 show how we compute such set by solving the SCP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ant Colony Algorithm for Set Cover Problem</head><p>We suggest an optimal series of viewpoints by solving the SCP. We first generate a large number of views as candidates to choose from. Each view will then cover a number of features. Thus we make each view a " set " , while the features that need to be covered form " elements " . The optimization objective is to find the minimum number of views that cover all salient features. The SCP was one of Karp's 21 NP-complete problems <ref type="bibr" coords="4,245.65,69.68,13.74,9.96" target="#b15">[17]</ref>. A mathematical model for the SCP is usually described by a 0-1 matrix, . Let be an m-row, n-column, zero–one matrix. We say that a column j covers a row i if 1. Each column is associated with a nonnegative real cost . Let 1, . . . , and 1, . . . , , be the row set and column set, respectively. The SCP can be stated as: </p><formula>minnn · · (6) subject to · · 1 and 0,11 , , </formula><formula>(7) </formula><p>where, 1 if set is selected, otherwise 0 . The matrix encodes all the viewpoints strength to cover the feature clusters in the volume. The SCP can be solved by many algorithms and the ant colony algorithm is one of the fastest solvers <ref type="bibr" coords="4,165.22,254.24,14.29,9.96" target="#b28">[30]</ref><ref type="bibr" coords="4,179.51,254.24,14.29,9.96" target="#b29">[31]</ref>. It is inspired by the observation of real ant colonies. The general mind-set behind ant colony algorithm is that a large amount of artificial ants search for an optimal solution defined by Equation (6). Each artificial ant chooses a set one by one until it achieves a complete cover defined by Equation (7). The decisions for choosing different sets are partially based on Russian-roulette. Additionally, the probability for choosing one set will increase if a large number of ants choose it, which is in the way of pheromone information exchange. In the following, we explain the ant colony algorithm for the SCP in detail. The probability for an ant to choose set j is based on the state transition rule: </p><formula>| ∑ \ \ 0 (8) </formula><p>where denotes the partial solution constructed before step , \ denotes the subset of unselected columns, denotes the set that will be chosen at the step and the parameter 0 determines the relative importance of the heuristic factor with respect to the pheromone. The heuristic is usually defined by a greedy method. If is the set of still uncovered elements and is the cost associated to set . The heuristic value of set is: </p><formula>| |/ (9) </formula><p>The pheromone trials are stored at each set as . They are initially set to one and updated later as: </p><formula>∆∆ 1 ∑ 0 (10) 1 1 1 ∆ (11) </formula><p>where is the current best solution across all ants, is the pheromone evaporation factor (with 0 0 1 ) and is the amount of new pheromone put on column . The range of pheromone should be clamped with a range , , where: </p><formula>1 1 1 1 ∑ (12) · 0 1 (13) </formula><p>For more detailed description of the ant colony algorithm applied for the set covering problem, we refer the reader to <ref type="bibr" coords="4,208.57,671.71,13.75,9.96" target="#b29">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROACH</head><p>Our overall framework is shown in <ref type="figure" coords="4,161.90,707.72,30.85,9.96" target="#fig_1">Figure 2</ref>. The first stage is a multi-dimensional data clustering. Given a certain noise-level for the dataset, we consider voxels with high gradient/normal variation as the important features. We perform k-means clustering to group voxels into blobs, followed by a visibility test. In the next stage we compute the information gain for all viewpoints around the object and create an entropy map that we display on a sphere that doubles as a track-ball interface used to change viewpoints. Thus, by mapping the entropy map directly on the track-ball, users can directly and intuitively identify and navigate to favorable view locations. The user can also add or delete viewpoints by clicking on the sphere and the displayed entropy map is updated accordingly. Alternatively, the SCP solver can be used to suggest to the user a set of optimal or at least near-optimal viewpoints from which to visualize the volume. It provides a series of viewpoints that covers all features. The set of optimal views suggested by the SCP solver is annotated onto the navigation sphere and the user can continue navigating through the sphere with these added suggestions in hand. In the meantime the user can also use the transfer function designer to explore settings that expose features at the given viewpoints. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Extraction and Clustering</head><p>Our framework is based on feature detection and high-dimensional data clustering. As for the question how to define features in highdimensional space as potentially interesting structures, there are many choices and their suitability can be application-dependent. We chose to provide a very general and application-neutral importance metric based on normal-variation. Normal-variation plays a significant role in lighting. In this paper, we shall assume that an area with large normal/gradient variations contains salient information, while regions with similar gradient directions have less information. This metric is an extension of 2D curvature estimation and it generally belongs to the group of Laplacian operators. The discrete importance estimation for a voxel located at , , , , is: </p><formula>, , , , |, , , , , , | ,, ,, ,, </formula><formula>(14) </formula><p>where , , is the volumetric scalar field, is the gradient operator and , , is the set containing a neighborhood of , , . In our estimation, only the 6 closest neighboring points are considered. This metric is different from the classic Laplacian operator which is defined as the divergence of the gradient vector field and thus can be negative. The importance weight here sums up the absolute values of gradient difference individually which can guarantee the weight to be positive. The intuition behind this metric is that we want to have a measure of the perturbations of gradient/normal in a region. Most practical volume data contain a certain level of noise which will affect the feature detection. In the pre-processing stage, we take the ambient noise level as an input to threshold the scalar values. We also consider the noise removal as a thresholding procedure on gradient variation. After applying these thresholds, the resulting voxels are considered the important voxels and are clustered in a five-dimensional space: scalar value, gradient magnitude, and (x, y, z) coordinate. The k-means algorithm is one of the most well-known clustering algorithms. Given an input value , it can partition n objects into k clusters based on some similarity (distance) measure. We apply kmeans clustering to get k blobs in 5D space. We also record the voxels inside each cluster and remove a cluster if the number of voxels is too small (less than 5). The gradient/normal direction for each cluster is computed as a Gaussian-weighted average which enhances spatial coherence. An example of computed gradient vectors is shown in <ref type="figure" coords="5,248.31,149.65,30.03,9.96">Figure 3</ref>. The clustering phase can employ automatic feature detection if the ambient noise level is known. We build our system in the highdimensional feature domain. Hence it can detect local structures with high gradient variation and adjust views for these local features. This provides a general importance metric well suited for non-expert users, to minimize user invention. But we note that our dataclustering pipeline can readily support other more specific metrics if more specific domain knowledge about dataset and task is available. An important aspect in k-means clustering is the input value . The value of controls the resulting number of clusters and as such the grouping of similar features of the input dataset. For a given dataset, to identify the features distinctly, the algorithm requires a certain that will ensure separation of features such that the clusters truthfully represent the features. Having a smaller value will merge a set of features into a big cluster, whereas a larger value will produce clusters covering fine details. From a multi-resolution point of view, the choice of affects the resolution of the features to be extracted. Therefore, the value will reflect the average size of the feature clusters. We base the choice of on the average cluster size (and therefore desired level of detail structures), chosen by the user via a slider interface. Then the value is found by dividing the total number of noise-free voxels by the desired average cluster size. But alternative approaches such as the elbow criterion <ref type="bibr" coords="5,215.99,379.61,15.04,9.96" target="#b17">[19] </ref>and X-means <ref type="bibr" coords="5,31.49,389.63,14.99,9.96" target="#b26">[28] </ref>may also be utilized to obtain an appropriate k. Finally, users may also inspect the clusters by visualizing them in the 3D interface. Another important issue with k-means clustering is also the choice of the initial k seeds, which can produce a certain level of randomness in the clusters formed. To overcome this problem we use the standard practice of clustering the data multiple times with different random seeds and picking the clustering that has the least overall L 2 error with respect to the clustered data points. Since we use the GPU-based standard-version k-means library <ref type="bibr" coords="5,249.28,469.64,15.00,9.96" target="#b12">[13] </ref>the performance hit is relatively minor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entropy Calculation</head><p>The k-means algorithm outputs a series of 5D clusters and gradient/normal values at the centers of the clusters. For each cluster, we extract its spatial information as a 3D ellipsoid and estimate the visibility based on the cluster's normal direction. We assume the center of a cluster to fall within the clipping window. Then there are three major factors that contribute to a good view: (1) the angle between the cluster's normal and the viewing direction (the eye ray), (2) the number of clusters that can be shown, and (3) the total number of voxels within potentially visible clusters. Our goal is to calculate the maximum potential entropy for the feature clusters. We establish a set of criteria for a feature cluster to be classified as invisible. We first set a threshold on a clusters' normal range and later use entropy to measure viewpoint quality. The first criterion is that the gradient direction and eye-ray should be within a certain range, enabling shading effects to enhance small details in the volume rendering. Shading conveys a strong cue for shape (see " shape from shading " in computer vision, graphics and robotics <ref type="bibr" coords="5,346.38,99.65,13.42,9.96" target="#b40">[42]</ref>). An important observation is that at 45° the Lambertian cosine shading functions starts to loose strength, since the derivative of the cosine function has an extreme point in the vicinity of 45°. So if the normal vector of the cluster and viewing vector make an angle of greater than 45°, our framework rates the viewpoint as inadequate to cover the feature cluster's information. Silhouettes can also be salient in conveying shape information (this is a popular method in non-photorealistic rendering). Silhouettes start to become visible when the view and normal vectors are close to 90°. Consequently, we may allow clusters that are close to 90° to be visible as well. This criterion is especially effective in dynamic flow visualization, in which interesting wave fronts can be represented as silhouettes. But in static data with concave shapes, it may produce poor results as shown in a simple example (<ref type="figure" coords="5,508.70,229.61,28.95,9.96" target="#fig_3">Figure 4</ref>). <ref type="figure" coords="5,293.94,239.62,29.91,9.96" target="#fig_3">Figure 4</ref>(b) is a good view via the silhouette criterion but it provides only little information. In contrast, <ref type="figure" coords="5,434.02,249.64,32.02,9.96" target="#fig_3">Figure 4</ref>(c) emphasizes only normal deviations and is a more informative view. Hence, we find that shading effects tend to be a safer way to test the visibility of the features, especially when the user is facing non-convex shapes. Since in this paper we only focus on volume rendering of static datasets, we prefer the shading-based method to point out salient details. Our visibility test so far did not account for the occlusion among clusters, since our target was the maximum possible exposure of all details within a given viewpoint. This is less of an issue since advanced frameworks (such as occlusion-spectrum based transfer function) have the capability to explore data with occlusions. In addition, conceptually we place no limitation on the number of images or transfer functions per viewpoint. As such, at a given viewpoint, the user may theoretically see all the structures within a normal range by applying different transfer functions one by one. In fact, this was our initial design choice: giving the user the freedom to choose any type of transfer function or take any number of rendering results later on. But practically speaking, the occlusion effect among the ellipsoids represents an additional time overhead (and therefore cost) in the data exploration process. So we provide a weight by which the user can set a preference on less occlusion which in turn eases the transfer function design, as discussed in Section 4.5. By applying the visibility test, we measure the quality of a viewpoint in terms of view entropy (Equation (3) and </p><p>(4)), and find the largest entropy by extensive search. As discussed, the major difference between ours and other work <ref type="bibr" coords="5,452.75,628.10,10.47,9.96" target="#b4">[5]</ref><ref type="bibr" coords="5,463.22,628.10,10.47,9.96" target="#b5">[6] </ref>is that instead of representing information according to scalar value, we define it on important features in a high-dimensional feature descriptor domain. For the cluster-based entropy, we mark all voxels that do not belong to any clusters as 'background', which is similar to the background feature definition in viewpoint selection methods for polygonal models <ref type="bibr" coords="5,365.40,688.10,14.29,9.96" target="#b34">[36]</ref><ref type="bibr" coords="5,379.69,688.10,14.29,9.96" target="#b35">[37]</ref>. This will remove singularities where only one cluster is shown in a viewpoint but its entropy is 0. After considering all background voxels, if no feature cluster is shown, the view will have zero entropy. In contrast, if any feature cluster is shown then the entropy will be non-zero. <ref type="figure" coords="5,47.82,722.20,3.33,8.91">3</ref>. K-means feature clustering with gradient vectors shown for (a) a standard cube and (b) a cube with text on the back surface. Finally, <ref type="figure" coords="6,67.21,174.92,32.20,9.96" target="#fig_4">Figure 5</ref>shows how different settings of k affect the entropy map of the tooth dataset. The 79-cluster case (<ref type="figure" coords="6,221.94,184.88,32.91,9.96" target="#fig_4">Figure 5a</ref>) has an average voxel set size of 12.9 and the 109-cluster case (<ref type="figure" coords="6,236.15,194.90,33.06,9.96" target="#fig_4">Figure 5d</ref>) has an average voxel-set size of 11.6. We observe the growing entropy pattern with increasing cluster number. However, the relative importance is quite stable. All entropy maps suggest that the most important view is from the north-pole. Then the next interesting view is from the south-pole. Finally, in the middle region there are two fairly interesting viewing areas and three relatively noninteresting areas. </p><formula>(a) (b) (a) (b) (c) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Viewpoint Information Exploration</head><p>If we assume that all viewpoints have the same camera distance, the possible projection locations will be on a sphere with radius and can be parameterized by longitude and latitude . We further assume that in each view position we use the same field-of-view (FOV) and we are looking at the center of the sphere. Rendering the viewpoint entropy map onto a sphere, every single point on this sphere then represents one view position and the intensity of a point denotes the amount of information this viewpoint can possibly cover. As shown in <ref type="figure" coords="6,71.47,376.88,29.48,9.96">Figure 6</ref>, the nearest point to the user (screen center) is the current viewpoint. The navigation window and volume rendering are displayed side by side. The user can rotate the sphere in the arcball interface and in the meantime the volume will rotate in a synchronous fashion. The user can then check on the map which view position would possibly be more interesting to look at. Deviating from the previous works that use single viewpoint selection interface, we provide progressive navigation tools. Users can select a complete set of views to render the volume data in a greedy manner. The system helps the user to navigate and allows them to select a series of viewpoints. When the user marks a point on the sphere to represent the selection of a viewpoint, the system shows the rendered image and updates the entropy information interactively, reducing the entropy map to that of the undiscovered features. Users may then continue to choose several more views until not much color is left. Also, the user may undo the latest selection and the system will then add the affected clusters back into the map. The user may also undo selections multiple times which will be reflected on the entropy map in reverse order. We next explain how to update the entropy calculation after a user's interaction. Initially, all clusters are set as unknown, denoted as 1 for the j th cluster. The clusters that have been explored by the user are marketed as inactive. Thus: 0 if user visited cluster and 0 1 otherwise </p><formula>(15) · 1 · (16) </formula><p>Undoing a user selection will mark the corresponding as active again. We do not perform further normalization of the noteworthiness factors during user selection, considering the fact that the amount of total information is constant. In this way, the user will observe the color fading from red to blue to convey the amount of information that has now been explored. As most users tend to use this tool in a greedy manner, it may not be the optimal way for choosing the camera positions that cover all detected features. For this purpose, we incorporate our SCP solver to help users to find the optimal viewpoints. This is explained next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Suggesting Best Combination of Views</head><p>As mentioned, a greedy search is not always the most optimal approach when searching for a set of views covering all clusters. The entropy-rated viewpoints are only a result of a local heuristic which is not adequate to find a global optimum. Hence we provide the user with the minimum number of viewpoints needed for full exploration based on an ant colony optimization of the set covering problem. The ant colony optimization method creates L artificial ants to search for the viable solution to Equation (6) and (7). After all L ants find viable solutions the system keeps track of the best ant (with minimum cost), updates the pheromone and runs L more artificial ants until the desirable cost is found by an ant. In our problem, the user can specify the number of views he/she wants to have to expose all feature clusters. Then the SCP solver will run multiple ants to search for the solution. Each ant will make a decision on what is the next viewpoint to choose based on heuristic and pheromone (<ref type="bibr" coords="6,289.24,358.14,46.59,9.96">Equation (8)</ref>). The heuristic is proportional to the number of unknown clusters that can be covered (<ref type="bibr" coords="6,445.55,368.15,46.41,9.96">Equation (9)</ref>). And the pheromone reflects how many other ants previously chose this viewpoint, as shown in Equation (10-13). After all ants have finished, the ant with the minimum number of viewpoints will deposit the pheromone to the viewpoints it chose. The system will report a success if an ant has found the desired solution. If in a limited amount of time, the ants cannot find a set of views under the desired cost, the system will report that no solution has been found. We have implemented the ant colony algorithm in C++ and validated it against several test problem cases <ref type="bibr" coords="6,473.77,458.15,10.48,9.96" target="#b2">[3] </ref>with known solutions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Viewpoint Preference</head><p>The user can further set preferences to refine the viewpoint suggestion results. We have added several metrics to that effect. Cluster Occlusion: In our approach, multiple transfer functions are typically required to extract information from a suggested view when there are occlusions between features. We can add support for preferring views with non-occluded features by adding a cost penalty for views with occluded features. This cost is determined by the maximum number of occlusions for a given view. We compute the penalty during the splatting-based cluster visibility test (Sec. 4.2). As discussed, the clusters are represented as 3D ellipsoids, and we record the maximum number of ellipsoids projected on each pixel. The cost is initially assigned to 1 if there are no overlaps, and it grows with the number of overlapping clusters discovered in the visibility splatting. To visualize the amount of cluster overlap and so the potential difficulty in the subsequent transfer function exploration, we provide two interfaces: (1) we render the cost in grayscale on a separate map/sphere, and (2) we fuse the occlusion cost into the entropy map by using it as a weighting inside the SCP solver. The former can be justified since the cost of occlusions does not add to the concept of entropy which only encodes the maximum potential exposure of information. In <ref type="figure" coords="6,345.29,710.13,30.78,9.96" target="#fig_5">Figure 7</ref>, we show the entropy map alongside the occlusion cost map for the bluntfin dataset. There are some areas with similar entropy but with different overlap counts. During the <ref type="figure" coords="6,287.40,125.56,18.88,8.91">Fig. 6</ref>. Viewpoint navigation for a cube dataset. (a): display window for the rendered object. (b): view navigation sphere (track-ball). Upon rotating the track-ball. Both (c) the rendered object and (d) the view navigation sphere will rotate in a synchronized manner. user interaction, the user mainly relies on the entropy map for viewpoint exploration, but treats the cost map as a guide to estimate the potential difficulty for the subsequent transfer function design. Viewpoint Separation: The SCP solver will generate results with reasonable separations among the viewpoints. If there are several solutions, the user can set a preference for viewpoint separation. This will reflect how evenly the viewpoints are distributed over the sphere. We gauge this evenness of the distribution by calculating the variance of the distances between the selected views on the sphere. </p><formula>1.0 N N N N S S S S (a) (b) (c) (d) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Viewpoint Up-Direction: </head><p>Given a fixed view, the information exposed by the volume on the screen does not depend on the orientation of the image. But users may have a preferred up-direction of the camera based upon common viewing experiences with the given dataset. In our framework there is a default vector specified as up-direction. If the data does not appear to be properly oriented with the default up direction, the user has the option to specify this vector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>In our experiments, we set the viewing angle limit to 45° and assuming a voxel distance of 1 mm we set the viewing distance to 5,000 mm. The image size is always 512 2 pixels. The view positions on a sphere are sampled on a 60 30 grid, with a total of 1,800 viewpoints. All volume-rendered results shown in this paper were obtained using the rendering software Imagevis3D <ref type="bibr" coords="7,216.01,390.14,14.32,9.96" target="#b11">[12]</ref><ref type="bibr" coords="7,230.33,390.14,14.32,9.96">[15]</ref>. We first tested our framework on a simple cube dataset. The cube's size is 80 3 , residing in a 256 3 volume grid. We added a shift vector 10, 20, 300 to the cube which moves it off the volume grid center. <ref type="figure" coords="7,60.15,430.16,31.45,9.96" target="#fig_6">Figure 8</ref>(a) shows this dataset, and <ref type="figure" coords="7,200.10,430.16,32.33,9.96">Figure 9</ref>displays the rendering results obtained with our system. In this case, the SCP solver automatically suggests 4 different views, looking down the cube diagonals. All of these 4 views coincide with the best views provided by Bordoloi et al. <ref type="bibr" coords="7,135.68,470.18,9.52,9.96" target="#b4">[5]</ref>. It appears that two images are not sufficient since each viewpoint will resolve three edges in the center through shading, while the rest of the edge features are partially but not completely visible. As seen in <ref type="figure" coords="7,160.44,500.17,29.93,9.96" target="#fig_6">Figure 8</ref>(c), it is not safe to only have two views to visualize the cube, since we do not have good information about the 6 edges appearing in the silhouettes. Conservatively speaking, only 4 viewpoints will be able to see all 12 edges with full exposure of all features. The corresponding entropy map is shown in the bottom row of <ref type="figure" coords="7,160.92,550.15,29.33,9.96">Figure 9</ref>. The initial high entropy map with no views selected shows 8 favorable regions, identifying the cube's 8 vertices as best views. The entropy map is then updated gradually as views are selected. Here selecting a given view typically removes more than one local maximum from the entropy map. For our next experiment, we add the text " Vis2011 " on one of the surfaces of the cube (normal perturbation), as shown in <ref type="figure" coords="7,238.13,610.14,29.86,9.96" target="#fig_6">Figure 8</ref>(b). The results for this modified cube are shown in <ref type="figure" coords="7,215.77,620.16,34.63,9.96">Figure 10</ref>. In this case, we need 5 views to fully visualize the dataset. In navigation mode, the entropy map clearly highlights the surface with text, indicating that in this region there is something important. In the automatic view suggestion mode, one of the resulting views specifically targets the text while the other viewpoints aim to look along the diagonals. In contrast, scalar-value based methods <ref type="bibr" coords="7,518.71,209.15,10.90,9.96" target="#b4">[5]</ref><ref type="bibr" coords="7,529.61,209.15,14.53,9.96" target="#b33">[35] </ref>will not be able consider the text as an important feature and so will display the entropy map of a uniform cube. Our method, on the other hand, can faithfully detect this type of intricate surface features and suggest something of possible interest is hiding in a certain view. We also tested our pipeline on medical data, which typically do not have strong regular edges like the cube. Our experiment uses the well-known tooth dataset. <ref type="figure" coords="7,397.08,279.11,32.77,9.96">Figure 1</ref>shows the resulting 7 views suggested by the SCP solver (the progressive entropy maps are shown on the right of each image). We note that both the rendering results and the entropy maps have been re-oriented using the userdefined up-directions since the default up-direction does not conform to the user-preferred tooth direction. In accordance with Bordoloi's result, the entropy map of this dataset indicates the north-pole and the south-pole as the two most interesting regions. The SCP solver subsequently chooses the north-pole and the south-pole as the first two viewpoints, and therefore the resulting 2 images are deemed to reveal the most interesting information on the data. We also see that after the first 2-3 views have been selected the remaining entropy is rather low and sparse, but our system includes them in the gallery to provide complete coverage. Another important aspect to note in this particular experiment is the utilization of multiple transfer function in the same view. For some of the viewpoints, the potential information is hidden if only one transfer function is considered, as shown in the 6 th viewpoint in <ref type="figure" coords="7,403.30,449.18,29.54,9.96">Figure 1</ref>(g). But once the user is given a view that can guarantee the presence of useful information, he can always refine the transfer function to freely explore the interesting structures best revealed by this view. Next, we tested the framework on a practical dataset obtained from Computed Tomography (CT). <ref type="figure" coords="7,428.09,499.16,35.69,9.96" target="#fig_9">Figure 11</ref>shows the results for the carp dataset. According to the entropy maps, the first 3 views cover the most important features of the carp. The map also indicates that one side of the carp is slightly more interesting than the other side, due to the carp's bent body as shown in <ref type="figure" coords="7,476.65,539.17,34.89,9.96" target="#fig_9">Figure 11</ref>(d). The remaining two views are less important, but again we provide them for completeness of the gallery. <ref type="figure" coords="7,306.18,569.17,35.60,9.96" target="#fig_1">Figure 12</ref>shows a gallery obtained for the engine dataset where panels (d) and (g) each show images of the same view but rendered with different transfer function settings to visualize different aspects of the engine. This study illustrates the benefit of the two-phase design of our system. First obtain a view direction at which many different types of features can be observed well, and then maintain <ref type="figure" coords="7,33.00,720.76,18.88,8.91">Fig. 9</ref>. The standard cube dataset (4 viewpoints computed by the SCP solver). The cube is shifted from the center. (a): the initial entropy map. (b-e): the suggested viewpoints rendered (top) and the maps with remaining entropy (bottom). </p><formula>(a) (b) (c) (d) (e) N S 0.0 0.2 0.4 0.6 0.8 </formula><p>1.0 that view and visualize these features in a number of ways to accentuate their various relationships in turn. This approach allows the user to maintain a coherent spatial reference, while learning about the dataset through dynamic feature exploration. Finally, we also tested our framework on a fluid simulation dataset, e.g., the well-known blunt fin. <ref type="figure" coords="8,167.32,99.67,35.36,9.96">Figure 13</ref>shows the 5 views suggested by the SCP solver. The view in <ref type="figure" coords="8,180.41,109.64,34.03,9.96">Figure 13</ref>(a) is suggested as the most important view, which in fact is close to the most commonly selected view onto this dataset. All of our experiments were conducted on an NVIDIA GTX 480 GPU, programmed with CUDA 3.2 runtime API, hosted by an Intel Core 2 Duo CPU @ 2.66GHz. <ref type="figure" coords="8,135.87,159.67,27.44,9.96" target="#tab_1">Table 1</ref>shows the performance of the different stages of our framework. The feature extraction part includes the thresholding and randomization of the resulting voxels. The visibility test portion is for testing the cluster normal directions at all views and also includes the splatting-based occlusion number computation. The most time-consuming part is the visibility test which would be much slower without GPU acceleration. The total processing time is about 2-8 times faster than a volume rendering of 1,800 views with a fixed transfer function. <ref type="figure" coords="8,178.10,239.64,27.16,9.96" target="#tab_2">Table 2</ref>shows the optimal number of viewpoints computed by the ant colony-based SCP solver in 10 seconds and the number it finally converges to (marked by ∞). For practical datasets, the voxels were filtered at above 3% of the maximum scalar value to remove noise, and the average cluster width was in the range of 10-20 voxels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATIONS</head><p>We performed a simple user study to evaluate the effectiveness of our iView interface. For this, we invited 9 graduate students, all familiar with volume rendering and transfer function design. At all time, the subjects were permitted to use the 1D or 2D transfer function editor and choose any preferred view to look at the volume, with a fixed front-light and using the track-ball interface. The only testing condition was that they either had access to our entropy map or not. In the latter case the track-ball surface was simply left blank. Each subject/user would render two different dataset in turn – the tooth and the carp. For each dataset, a user was asked to construct two galleries (that is, select a set of viewpoints) that would best expose the salient information of the given dataset. The first gallery was always constructed with no entropy-map guidance, while for the second gallery this guidance was available. Prior to using the system, each user was trained on how to navigate with the entropy sphere (if provided), how to interpret its data, how to observe the clustering results and also how to use keys to add/delete views from the gallery. </p><p>We compared the views selected with and without the iView guidance. We found that in general users would pick fewer views without guidance. The mean of the difference between two sets of views was 0. 90 (tooth) and 0.56 (carp). We used the dependent t-test for paired samples to analyze the view numbers with the hypothesis that those two means (with/without entropy map) are identical. The p-values for tooth and carp were 0.003 and 0.05, respectively. Thus, the fact that users would consistently pick fewer views without guidance indicates that iView helps users in locating commonly overlooked regions. It was also interesting to observe that no gallery generated without guidance would include all of the top three views found with guidance (or with the SCP solver). There were often redundant views in the uninformed gallery or views with low entropy. To capture this behavior more quantitatively, we measured a given gallery's information coverage by the sum of entropy left in the map after gallery composition. When a user was allowed to use the map, the sum of the entropy left dropped from 51% to 24% for the tooth and from 37% to 19% for the carp, on average. Likewise, the percentage of entropy covered per view increased from 11% to 15% for the tooth and from 14% to 16% for the carp. This demonstrates that our navigation interface can clearly help users to optimize a set of viewing positions and with it the information seen. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>We have presented a feature clustering approach that suggests users promising viewpoints for volume visualization prior to transfer function design activities. We believe that such a transfer function neutral approach cuts down on the volume exploration effort since it selects potentially interesting views before laborious transfer function exploration begins. As such, our approach promotes a data exploration procedure in which users first navigate to views that promise to show many features well and then explore and enhance these features and their relations via transfer functions in a stable spatial context. We believe that this is cognitively less challenging than changing viewpoint and transfer function at the same time. Our approach strongly favors interactive volume exploration, mapping the navigation aids directly on the track-ball interface used for spatial transformation. In addition, the system updates the navigation aids in an adaptive manner based on the views selected. Finally, a set cover problem solver is also available to choose a set of optimal views automatically to compose an overview gallery. <ref type="figure" coords="8,286.98,269.38,22.95,8.91">Fig. 10</ref>. The cube with text on one face (5 viewpoints computed by the SCP solver), with transfer function highlighting the text. (a): the initial entropy map. (b-f): the suggested viewpoints rendered (left) and the maps with remaining entropy (right). </p><formula>(e) (f) (d) (b) </formula><p>For future work, we plan to extend our feature descriptor to other known metrics, such as suggestive contours (where the derivative of the normal vector is 0) and other well-known descriptors from computer vision: the multi-scale Harris detector <ref type="bibr" coords="9,213.17,686.42,15.00,9.96" target="#b14">[16] </ref>or SIFT <ref type="bibr" coords="9,264.56,686.42,13.70,9.96" target="#b22">[24]</ref>, which we used already in other work for feature detection <ref type="bibr" coords="9,264.65,696.38,13.71,9.96" target="#b25">[27]</ref>. Further, we also believe that the silhouette metric would be a promising candidate for dynamic flow visualization and we plan to research this more thoroughly. Finally, we also plan to implement the ant colony optimization on the GPU to reduce response time. <ref type="figure" coords="9,30.72,643.06,22.95,8.91">Fig. 13</ref>. The bluntfin dataset (5 viewpoints resulting from the SCP solver). <ref type="figure" coords="9,33.24,538.06,22.95,8.91" target="#fig_1">Fig. 12</ref>. The engine dataset (7 viewpoints computed by the SCP solver). (a-g): the suggested viewpoints rendered with different transfer functions. (d) and (g) shows the need to use multiple transfer functions to explore features. </p><formula>(a) (b) (c) (d) (e) </formula><formula>(a) (b) (c) (d) (e) (f) (g) </formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,297.54,333.68,136.30,9.96"><head></head><figDesc>View Selection: Vazquez et al. [36]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,288.24,392.86,91.53,8.91"><head>Fig. 2. </head><figDesc>Fig. 2. Overall framework. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,32.28,722.20,244.40,8.91;5,32.28,731.68,216.98,8.91"><head>Fig. </head><figDesc>Fig. 3. K-means feature clustering with gradient vectors shown for (a) a standard cube and (b) a cube with text on the back surface. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,296.28,432.28,171.22,8.91"><head>Fig. 4. </head><figDesc>Fig. 4. Silhouettes fail to convey concave shape. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,24.66,151.54,239.53,8.91;6,24.66,161.02,229.28,8.91"><head>Fig. 5. </head><figDesc>Fig. 5. The effect of the number of clusters on the entropy map. The Initial k for k-means: (a) k=100. (b) k=110. (c) k =120. (d) k =130. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,33.12,113.56,236.37,8.91;7,33.12,123.04,214.25,8.91;7,33.12,132.52,80.43,8.91;7,60.12,98.44,69.77,8.91;7,171.54,97.72,57.71,8.91"><head>Fig. 7. </head><figDesc>Fig. 7. (a) Entropy map and (b) cost map for the bluntfin dataset. In the cost map, the maximum number of overlaps is 8 which is mapped to white color. (a) the entropy map (b) the cost map </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,293.94,138.52,240.35,8.91;7,293.94,148.05,173.31,8.91;7,330.36,127.24,9.74,8.91;7,411.78,126.88,9.72,8.91;7,493.91,126.88,9.32,8.91"><head>Fig. 8. </head><figDesc>Fig. 8. The cube datasets. (a): a standard cube. (b): a cube with text on a face. (c): a cube with features on the edges. (a) (b) (c) </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="9,33.06,311.74,506.83,8.91;9,33.06,321.27,361.41,8.91;9,522.54,296.02,7.52,8.91;9,277.50,294.52,9.74,8.91"><head>Fig. 11. </head><figDesc>Fig. 11. The carp dataset (5 viewpoints computed by the SCP solver). (a): the initial entropy map. (b-f): the suggested viewpoints rendered (left) and the maps with remaining entropy (right). The transfer function could be changed in different views. (f) (e) </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true" coords="8,28.68,306.10,233.62,111.79"><figDesc coords="8,36.60,306.10,222.64,8.91;8,112.56,315.58,70.69,8.91">Table 1. The Performance of Different Stages of Our Viewpoint Suggestion Pipeline</figDesc><table coords="8,28.68,334.77,233.62,83.11">Dataset 
Datasize 

Feature Extraction / 
K-Means Clustering 
/ Visibility Test 

Rendering 
Time / 
Speedup 
Cube 
256x256x256 
0.9s / 0.9s / 7.3s 
18s / 2.0× 
Cube + Text 
256×256×256 
1.2s / 0.5s / 7.9s 
18s / 1.9× 
Tooth 
256×256×161 
1.2s / 1.3s / 8.9s 
93s / 8.2× 
Engine 
256×256×110 
1.1s / 0.6s / 10.4s 
53s / 4.4× 
Blunt Fin 
256×128×64 
0.7s / 0.6s / 8.2s 
26s / 2.7× 
Carp 
256×256×512 
2.5s / 2.5s / 9.2s 
87s / 6.1× 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false" coords="8,37.44,425.14,219.53,111.49"><figDesc coords="8,38.82,425.14,28.86,8.91">Table 2.</figDesc><table coords="8,37.44,425.14,219.53,111.49">The Problem Size of the K-Means Clustering and the 
Minimum Number of Views Found by the SCP Solver 

Dataset 
Voxels 
Initial k / 
Resulting k 

Averaged 
Cluster 
Width 

Views 
(10s / ∞) 

Cube 
968 
50 / 47 
2.7 
4 / 4 
Cube+Text 
1278 
60 / 58 
2.8 
5 / 5 
Tooth 
170228 
100 / 79 
12.9 
7 / 6 
Engine 
529050 
120 / 111 
16.4 
7 / 6 
Blunt Fin 
65053 
50 / 50 
10.9 
6 / 5 
Carp 
331894 
80 / 69 
16.9 
5 / 5 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS </head><p>This work was funded in part by NSF EAGER grant 1050477 and NSF grant 0959979. It was also made possible in part by the imageVis3D software from the NIH/NCRR Center for Integrative Biomedical Computing, 2P41 RR0112553-12. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,40.85,64.59,232.28,8.83;10,40.85,74.55,232.18,8.83;10,40.85,84.57,137.63,8.83"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic view selection in multi-view object recognition</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Abbasi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Mokhtarian</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Pattern Recognition</title>
		<meeting>. of International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.85,94.59,232.27,8.83;10,40.85,104.55,232.10,8.83;10,40.85,114.58,232.21,8.83;10,40.85,124.60,38.05,8.83"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual optimality and stability analysis of 3DCT scan positions</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Amirkhanov</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Heinzl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Reiter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1477" to="1487" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.85,134.56,232.26,8.83;10,40.85,144.58,228.91,8.83"  xml:id="b2">
	<analytic>
		<title level="a" type="main">OR-Library: distributing test problems by electronic mail</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Beasley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1069" to="1072" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.85,154.60,232.20,8.83;10,40.85,164.56,232.23,8.83;10,40.85,174.59,106.27,8.83"  xml:id="b3">
	<analytic>
		<title level="a" type="main">View planning and automated data acquisition for 3-D modeling of complex sites</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">S</forename>
				<surname>Blaer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">K</forename>
				<surname>Allen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="865" to="891" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.84,184.61,232.11,8.83;10,40.84,194.57,196.47,8.83"  xml:id="b4">
	<analytic>
		<title level="a" type="main">View selection for volume rendering</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Bordoloi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-W</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization</title>
		<meeting>the IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.83,204.59,232.17,8.83;10,40.83,214.61,232.18,8.83;10,40.83,224.57,204.25,8.83"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Relationaware volume exploration pipeline</title>
		<author>
			<persName>
				<forename type="first">M.-Y</forename>
				<surname>Chan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Qu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-K</forename>
				<surname>Chung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W.-H</forename>
				<surname>Mak</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1683" to="1690" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.83,234.60,232.21,8.83;10,40.83,244.62,232.10,8.83;10,40.83,254.58,232.21,8.83;10,40.83,264.60,38.05,8.83"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptionbased transparency optimization for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">M.-Y</forename>
				<surname>Chan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W.-H</forename>
				<surname>Mak</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Qu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1283" to="1290" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.82,274.62,232.20,8.83;10,40.82,284.58,232.22,8.83;10,40.82,294.61,146.05,8.83"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Size-based transfer functions: a new volume exploration technique</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Correa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1380" to="1387" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.82,304.63,232.28,8.83;10,40.82,314.59,232.24,8.83;10,40.82,324.61,160.04,8.83"  xml:id="b8">
	<analytic>
		<title level="a" type="main">The occlusion spectrum for volume classification and visualization</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Correa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1465" to="1472" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.81,334.63,232.10,8.83;10,40.81,344.59,232.25,8.83;10,40.81,354.62,104.06,8.83"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Visibility histograms and visibility-driven transfer functions</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Correa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="204" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.81,364.64,232.10,8.83;10,43.78,374.60,229.24,8.83;10,40.81,384.62,104.06,8.83"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Suggestive contours for conveying shape</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Decarlo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Finkelstein</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Rusinkiewicz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Santella</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="848" to="855" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.81,394.64,232.11,8.83;10,40.81,404.60,232.22,8.83;10,40.81,414.63,137.42,8.83"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Tuvok -an architecture for large scale volume rendering</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Fogal</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Krüger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Vision, Modeling, and Visualization</title>
		<meeting>the 15th International Workshop on Vision, Modeling, and Visualization</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.81,424.65,232.11,8.83;10,40.81,434.61,232.28,8.83;10,40.81,444.63,180.39,8.83"  xml:id="b12">
	<monogr>
		<title level="m" type="main">Parallel data mining on graphics processors</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Fang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-K</forename>
				<surname>Lau</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Lu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Xiao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C.-K</forename>
				<surname>Lam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">Y</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>He</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">V</forename>
				<surname>Sander</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.81,454.59,232.18,8.90;10,40.86,464.55,232.21,8.83;10,40.85,474.57,68.24,8.83;10,22.85,484.60,250.13,8.83;10,40.84,494.55,232.23,8.83;10,40.84,504.58,47.92,8.83"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic camera placement for image-based modeling, ImageVis3D: A real-time volume rendering tool for large data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Fleishman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Cohen-Or</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lischinski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Computing and Imaging Institute (SCI)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.83,514.60,229.53,8.83;10,40.83,524.56,155.59,8.83"  xml:id="b14">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Harris</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Stephens</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Alvey Vision Conf</title>
		<meeting>. 4th Alvey Vision Conf</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.83,534.58,232.15,8.83;10,40.83,544.60,139.63,8.83"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducibility among combinatorial problems</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Karp</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complexity of Computer Computations</title>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="page" from="85" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.83,554.56,232.25,8.83;10,40.83,564.59,204.24,8.83"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Virtual colonoscopy</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Kaufman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lakare</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Kreeger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Bitter</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="41" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.82,574.61,232.12,8.83;10,40.82,584.57,232.18,8.83;10,40.82,594.59,142.94,8.83"  xml:id="b17">
	<analytic>
		<title level="a" type="main">The application of cluster analysis in strategic management research: an analysis and critique</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J</forename>
				<surname>Ketchen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">L</forename>
				<surname>Shook</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Strategic Management Journal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="441" to="458" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.82,604.61,232.21,8.83;10,40.82,614.57,232.22,8.83;10,40.82,624.60,232.28,8.83;10,40.82,634.62,18.08,8.83"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Curvaturebased transfer functions for direct volume rendering: methods and applications</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Kindlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Whitaker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tasdizen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization</title>
		<meeting>the IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.81,644.58,232.20,8.83;10,40.81,654.60,232.24,8.83;10,40.81,664.62,192.24,8.83"  xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-dimensional transfer functions for interactive volume rendering</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kniss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Kindlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="285" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.81,674.58,232.14,8.83;10,40.81,684.61,232.10,8.83;10,40.81,694.63,232.23,8.83;10,40.81,704.59,38.05,8.83"  xml:id="b20">
	<analytic>
		<title level="a" type="main">LiveSync: deformed viewing spheres for knowledge-based navigation</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kohlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kanitsar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1544" to="1551" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,285.25,50.59,250.10,8.83;10,303.25,60.61,232.25,8.83;10,303.25,70.63,206.23,8.83"  xml:id="b21">
	<analytic>
		<title level="a" type="main">ClearView: An interactive context preserving hotspot visualization technique</title>
		<author>
			<persName>
				<forename type="first">]</forename>
				<forename type="middle">J</forename>
				<surname>Krüger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schneider</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Westermann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="941" to="948" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.24,80.59,229.43,8.83;10,303.24,90.61,153.80,8.83"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lowe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intern. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.24,100.64,232.26,8.83;10,303.24,110.60,232.24,8.83;10,303.24,120.62,232.29,8.83;10,303.24,130.64,112.03,8.83"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Structuring feature space: a non-parametric method for volumetric transfer function generation</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Maciejewski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Woo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ebert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1473" to="1480" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.23,140.60,232.06,8.83;10,303.23,150.62,232.22,8.83;10,303.23,160.65,70.27,8.83"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Visibility-aware direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">W.-H</forename>
				<surname>Mai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M.-Y</forename>
				<surname>Chan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Qu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="228" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.22,170.55,232.29,8.90;10,303.30,180.57,232.20,8.83;10,303.30,190.60,100.09,8.83"  xml:id="b25">
	<monogr>
		<title level="m" type="main">High dimensional feature descriptors to characterize volumetric data Knowledge-Assisted Visualization Workshop</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Nam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Mauer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Mueller</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.29,200.56,232.21,8.83;10,303.29,210.58,232.22,8.83;10,303.29,220.60,181.11,8.83"  xml:id="b26">
	<analytic>
		<title level="a" type="main">X-means: extending k-means with efficient estimation of the number of clusters</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Pelleg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Moore</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th International Conference on Machine Learning</title>
		<meeting>. of the 17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="727" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.28,230.56,232.22,8.83;10,303.28,240.58,232.22,8.83;10,303.28,250.61,80.30,8.83"  xml:id="b27">
	<analytic>
		<title level="a" type="main">A solution to the next best view problem for automated surface acquisition</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Pito</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. of Pattern Analysis and Maching Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1016" to="1030" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.27,260.56,232.23,8.83;10,303.27,270.59,232.21,8.83;10,303.27,280.61,68.92,8.83"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Parallel ant system for the set covering problem</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Rahoual</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Hadji</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Bachelet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2463</biblScope>
			<biblScope unit="page" from="249" to="297" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.26,290.57,232.20,8.83;10,303.26,300.59,232.24,8.83;10,303.26,310.62,145.34,8.83"  xml:id="b29">
	<analytic>
		<title level="a" type="main">New ideas for applying ant colony optimization to the set covering problem</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Ren</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Feng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Ke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Industrial Engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="774" to="784" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.26,320.57,232.20,8.83;10,303.26,330.60,232.16,8.83;10,303.26,340.62,89.17,8.83"  xml:id="b30">
	<analytic>
		<title level="a" type="main">View planning for automated threedimensional object reconstruction and inspection</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Scott</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Roth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Rivest</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="96" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.25,350.58,232.21,8.83;10,303.25,360.60,232.27,8.83;10,303.25,370.62,224.42,8.83"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Automating transfer function design for volume rendering using hierarchical clustering of material boundaries</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Sereda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Vilanova</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">A</forename>
				<surname>Gerritsen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EuroVis</title>
		<meeting>. of EuroVis</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.24,380.58,232.21,8.83;10,303.24,390.61,232.22,8.83;10,303.24,400.63,194.01,8.83"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-linear perspective widgets for creating multiple-view images</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Sudarsanam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Singh</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Grimm</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Nonphotorealistic Animation and Rendering</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="69" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.23,410.59,232.16,8.83;10,303.23,420.61,232.15,8.83;10,303.23,430.63,232.22,8.83;10,303.24,440.59,18.08,8.83"  xml:id="b33">
	<analytic>
		<title level="a" type="main">A featuredriven approach to locating optimal viewpoints for volume visualization</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Takahashi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Fujishiro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Takeshima</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Nishita</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization</title>
		<meeting>the IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="495" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.23,450.62,232.15,8.83;10,303.23,460.64,232.28,8.83;10,303.23,470.60,154.00,8.83"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Viewpoint selection using view entropy</title>
		<author>
			<persName>
				<forename type="first">P.-P</forename>
				<surname>Vazquez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Feixas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sbert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Heidrich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Vision Modeling and Visualization Conference</title>
		<meeting>. of Vision Modeling and Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.22,480.62,232.14,8.83;10,303.22,490.64,232.16,8.83;10,303.22,500.60,197.65,8.83"  xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic view selection using viewpoint entropy and its application to image-based modeling</title>
		<author>
			<persName>
				<forename type="first">P.-P</forename>
				<surname>Vazquez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Feixas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sbert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Heidrich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="689" to="700" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.22,510.63,232.24,8.83;10,303.22,520.65,232.19,8.83;10,303.22,530.61,196.27,8.83"  xml:id="b36">
	<analytic>
		<title level="a" type="main">Importance-driven feature enhancement in volume visualization</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Viola</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kanitsar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="408" to="418" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.22,540.63,232.32,8.83;10,303.22,550.65,232.10,8.83;10,303.22,560.61,196.47,8.83"  xml:id="b37">
	<analytic>
		<title level="a" type="main">The magic volume lens: an interactive focus+context technique for volume rendering</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Mueller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Kaufman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization</title>
		<meeting>the IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.21,570.57,232.29,8.91;10,305.97,580.59,229.55,8.83;10,303.30,590.55,232.13,8.83;10,303.31,600.58,101.35,8.83"  xml:id="b38">
	<analytic>
		<title level="a" type="main">An information theoretic approach for next best view planning in 3-D reconstruction</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Wenhardt</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Deutsch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hornegger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Niemann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Denzler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Pattern Recognition</title>
		<meeting>. of International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="103" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.31,610.60,232.23,8.83;10,303.31,620.56,232.16,8.83;10,303.30,630.58,232.21,8.83;10,303.30,640.60,38.05,8.83"  xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive visual optimization and analysis for RFID benchmarking</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-K</forename>
				<surname>Chung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Qu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Yuan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S.-C</forename>
				<surname>Cheung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1335" to="1342" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.30,650.56,232.25,8.83;10,303.30,660.58,232.21,8.83;10,303.29,670.61,70.27,8.83"  xml:id="b40">
	<analytic>
		<title level="a" type="main">Shape from shading: a survey</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P.-S</forename>
				<surname>Tsai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Cryer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Shah</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. of Pattern Analysis and Maching Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="690" to="706" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
