<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T14:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">About the Influence of Illumination Models on Image Comprehension in Direct Volume Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Florian</forename>
								<surname>Lindemann</surname>
								<roleName>Student Member, Ieee</roleName>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Timo</forename>
								<surname>Ropinski</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">About the Influence of Illumination Models on Image Comprehension in Direct Volume Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Volumetric illumination</term>
					<term>volume rendering</term>
					<term>spatial comprehension</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Phong lighting Half angle slicing Directional occlusion shading Multidirectional occlusion shading Shadow volume propagation Spherical harmonic lighting Dynamic ambient occlusion Fig. 1: An overview of the seven volume illumination techniques which we implemented and tested. The renderings of a CT scan of a human body differ only with respect to the illumination method, as other parameters remain constant. Abstract—In this paper, we present a user study in which we have investigated the influence of seven state-of-the-art volumetric illumination models on the spatial perception of volume rendered images. Within the study, we have compared gradient-based shading with half angle slicing, directional occlusion shading, multidirectional occlusion shading, shadow volume propagation, spherical harmonic lighting as well as dynamic ambient occlusion. To evaluate these models, users had to solve three tasks relying on correct depth as well as size perception. Our motivation for these three tasks was to find relations between the used illumination model, user accuracy and the elapsed time. In an additional task, users had to subjectively judge the output of the tested models. After first reviewing the models and their features, we will introduce the individual tasks and discuss their results. We discovered statistically significant differences in the testing performance of the techniques. Based on these findings, we have analyzed the models and extracted those features which are possibly relevant for the improved spatial comprehension in a relational task. We believe that a combination of these distinctive features could pave the way for a novel illumination model, which would be optimized based on our findings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, the development and improvement of off-the-shelve GPUs has led to the proposition of several interactive advanced volumetric illumination models. While most researchers claim an improved perception of volume rendered images when using their technique , little work has been done verifying these claims and comparing the different approaches. In this paper, we present a user study which evaluates the perceptual qualities of seven state-of-the-art volumetric illumination models. The evaluated models have been chosen based on their novelty, the number of citations as well as actual spread in real-world applications. To ensure comparability, we focused solely on direct volume rendering (DVR) techniques. Besides the conventional gradient-based Phong illumination model <ref type="bibr" coords="1,240.91,567.24,13.74,8.02" target="#b16">[17]</ref>, which we have selected due to its pervasiveness, we selected three models based on slice compositing and three models which can be combined with raycasting-based techniques. The slice-based techniques consist of half angle slicing <ref type="bibr" coords="1,105.48,607.09,13.74,8.02" target="#b13">[14]</ref>, which stands out with a high citation count, and two newer models, directional occlusion shading <ref type="bibr" coords="1,230.70,617.05,14.94,8.02" target="#b29">[30] </ref>tidirectional occlusion shading <ref type="bibr" coords="1,409.49,444.02,13.74,8.02">[34]</ref> . Among the techniques not exclusively based on slicing, we have decided to evaluate the recently proposed shadow volume propagation technique <ref type="bibr" coords="1,474.19,463.95,13.74,8.02">[26]</ref> , spherical harmonic lighting <ref type="bibr" coords="1,348.28,473.91,14.94,8.02" target="#b24">[25] </ref>and dynamic ambient occlusion <ref type="bibr" coords="1,479.91,473.91,13.74,8.02" target="#b26">[27]</ref>. <ref type="figure" coords="1,499.76,473.91,20.61,8.02">Fig. 1</ref>shows a comparison of the results of these seven techniques. Since this is the first study comparing advanced volumetric illumination models, we decided to minimize potential errors by reducing the study complexity, while at the same time reaching as many users as possible. We focused on static images instead of dynamic images or interactive applications since they occur in many everyday tasks. This also implies that no stereoscopic perception cues were available. Therefore, the layman users who participated in our study had to rely on the visual cues of monoscopic perception such as occlusion and cast shadows. For the design of the conducted tests, we have considered the perceptual processes relevant for scene comprehension. First, depth perception is crucial to comprehend the arrangement of a scene <ref type="bibr" coords="1,527.31,593.46,13.74,8.02" target="#b34">[37]</ref>, as it allows the observer to judge the distance between an object and himself as well as other structures in the image. A conceptually more complex perceptual task required to judge scene arrangements is size perception, which strongly depends on depth perception <ref type="bibr" coords="1,501.77,633.31,9.52,8.02" target="#b4">[5]</ref> . Consequently , both depth perception as well as size perception are essential in order to judge the spatial relationships within a scene. Therefore, in the first two tasks, we tested the participants' ability to recognize depth in a volume rendered image, while the third task involved the cognition of the size of different features. Our goal was to reveal the perceptual advantages and disadvantages of the evaluated models based on these tests. Besides this analysis of quantitative measurements, we also examined which techniques were subjectively preferred by the users when given the choice between two images. After describing all conducted tests in detail, we will discuss our results and distill them to  identify perceptually relevant features of volumetric illumination models . Finally, we will present guidelines to select the right illumination model for a given task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Advanced Volumetric Illumination Models</head><p>In recent years, several advanced illumination models for volumetric data have been proposed <ref type="bibr" coords="2,111.85,357.84,13.74,8.02" target="#b23">[24]</ref> . However, due to time reasons and in order to keep the number of test images manageable for the user, we had to limit ourselves to a subset of these. Therefore, we have included mainly representatives from each group of DVR techniques, and those techniques proposed within the last three years. Due to its high impact , the only exception is half angle slicing which has been proposed as early as in 2002 <ref type="bibr" coords="2,95.07,417.62,13.74,8.02" target="#b13">[14]</ref>. Besides the tested approaches, the omitted techniques are well worth mentioning, and should be briefly described in this subsection. We will explain the tested techniques in more detail in Section 3. Ambient occlusion is a shading method which produces a rather rough approximation of global illumination by considering the surrounding structures of a given point. Several publications aim at using ambient occlusion in volume rendering. Stewart <ref type="bibr" coords="2,225.40,487.36,14.94,8.02" target="#b30">[31] </ref>was the first to extend this approach to volume rendering by proposing vicinity shading, which works only for displaying the structures belonging to one isovalue. Similarly, Wyman et al. <ref type="bibr" coords="2,174.95,517.24,14.94,8.02" target="#b37">[40] </ref>as well as Penner and Mitchell <ref type="bibr" coords="2,55.93,527.21,14.94,8.02" target="#b21">[22] </ref>have proposed techniques, which work for isosurfaces only. However, there have been a number of DVR approaches realizing more general ambient occlusion in recent years. Hernell et al. <ref type="bibr" coords="2,262.41,547.13,10.45,8.02" target="#b7">[8] </ref>integrate the opacity of each voxel in a surrounding sphere. These values are then used to approximate the visibility of the voxel and the amount of incoming light. Hernell et al. <ref type="bibr" coords="2,183.48,577.02,10.45,8.02" target="#b8">[9] </ref>further improved this technique by reusing ambient occlusion results to efficiently compute global shadows and first order scattering effects with piecewise linear integration. Ljung et al. <ref type="bibr" coords="2,128.16,606.91,14.94,8.02" target="#b17">[18] </ref>obtain interactivity by exploiting a multiresolution framework, even when refining the transfer function. Ruiz et al. <ref type="bibr" coords="2,60.96,626.83,14.94,8.02" target="#b27">[28] </ref>use a framework of obscurances to produce illustrative renderings and saliency maps. Hadwiger et al. <ref type="bibr" coords="2,196.07,636.80,10.45,8.02" target="#b6">[7] </ref>adapted the Deep Shadow Map approach to DVR with appealing results. In this approach , visibility functions are precomputed and compressed for each texel from the point of view of the light position. Tao et al. <ref type="bibr" coords="2,239.00,666.68,14.94,8.02" target="#b32">[33] </ref>used volumetric unsharp masking to modulate the local contrast of structures on volume rendered images in order to enhance depth perception. Older techniques by Nulkar and Mueller <ref type="bibr" coords="2,174.70,696.57,14.94,8.02" target="#b20">[21] </ref> and Zhang and Craw- fis <ref type="bibr" coords="2,34.16,706.53,14.94,8.02" target="#b39">[42] </ref> use splatting to produce shadows in volume rendering. Recently , Sundén et al. <ref type="bibr" coords="2,97.72,716.50,14.94,8.02" target="#b31">[32] </ref>proposed a technique which uses raycasting combined with a plane-sweep approach to allow interactive advanced lighting effects even for large datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Perceptual Studies</head><p> The influence of lighting on the perception of a rendered scene in general has been the subject of extensive research. Due to the similarity with the qualities of the tested illumination models, we mainly focus on those studies investigating shadows or similar phenomena. <ref type="bibr" coords="2,507.83,317.99,27.66,8.02;2,285.12,327.95,43.18,8.02">Wanger et al. found </ref>that the existence of shadows significantly improved the recognition of spatial features as well as object size <ref type="bibr" coords="2,469.95,337.92,13.74,8.02" target="#b34">[37]</ref> . Wanger conducted a further study on the influence of shadow quality on the recognition of spatial relationships, object size and object shape in computer generated images <ref type="bibr" coords="2,350.17,367.80,13.74,8.02" target="#b33">[36]</ref>. In his tests, users were subjected to images of objects with no shadows, soft shadows and hard shadows. His results showed no significant relation between spatial comprehension and the softness of shadows. However, recognition of the shape of objects improved with hard shadows, but even decreased when using soft shadows as compared to using no shadows at all. Hubona et al. <ref type="bibr" coords="2,344.04,427.58,14.94,8.02" target="#b10">[11] </ref> performed tests in which the subjects had to complete a symmetric arrangement of 3D shapes by either repositioning or resizing an object under different circumstances. Among others, tested variables included again the presence of shadows as well as the number of light sources. It was found that in the repositioning test, shadows increased accuracy, but caused longer response times, while in the resizing test, no significant improvement in accuracy could be found when shadows were present. In both tests, the addition of a second light source and the resulting shadows decreased the performance of the subjects. Langer and Bülthoff <ref type="bibr" coords="2,372.00,527.21,14.94,8.02" target="#b15">[16] </ref> conducted tests on the efficiency of surface shading under diffuse lighting in tasks related to the perception of depth and brightness. They found that the subjects performed generally as good when using a diffuse light source as when using a point light source positioned at different locations. Their results further support the hypothesis that humans benefit from illumination models in which low luminance is correlated with depth ( " dark means deep " ) <ref type="bibr" coords="2,312.31,596.94,13.74,8.02" target="#b19">[20]</ref>. However, based on the very good performance of test subjects, they come to the conclusion that this rather simplistic model is only a rough approximation of the complex mechanism of human depth perception, which seems to employ more sophisticated proce- dures. Hu et al. <ref type="bibr" coords="2,329.21,646.76,14.94,8.02" target="#b9">[10] </ref>performed two tests in which users had to judge the distance between a block of wood and a table in a virtual environment. In the first test, users had to place the block themselves as close to the table as possible without touching it. In the second test, users watched as the block was lowered towards the table up to a certain point, then had to estimate the remaining distance between the two objects. The authors did not only focus on shadows, but also included stereoscopic viewing as well as reflections on the table as visual cues. However, their results confirm that the addition of shadows had a positive effect in all cases. </p><p>With respect to volume rendering, perceptual studies are rather scarce. Boucheny et al. <ref type="bibr" coords="3,116.27,63.35,10.45,8.02" target="#b1">[2] </ref>tested the depth perception of DVR images containing transparent objects. They found that the perception was poor when transparency was the only depth cue. It improved when the test subjects had to detect the direction of rotation of a cylinder, but only when carefully choosing luminance and opacity of the cylinder. Chan et al. <ref type="bibr" coords="3,81.20,113.16,10.45,8.02" target="#b3">[4] </ref>focused on image saturation, shape enhancement and transparency as visual cues. They have presented a framework to automatically optimize transfer function parameters for a more effective user perception of DVR images. Wu et al. <ref type="bibr" coords="3,187.73,143.05,14.94,8.02" target="#b36">[39] </ref> have presented quantitative effectiveness measures for different volume rendering challenges such as the clarity of image contours or depth coherence. They allow the user to receive feedback about the overall effectiveness of a DVR image and provide the ability to further optimize rendering pa- rameters. Ropinski et al. <ref type="bibr" coords="3,95.74,202.82,14.94,8.02">[26] </ref>conducted a user study in which they evaluated their shadow volume propagation technique. They compared their approach to conventional gradient-based shading, and found that their global lighting approximation significantly improved user speed and accuracy of depth perception. ˇ Soltészová et al. <ref type="bibr" coords="3,203.99,243.72,14.94,8.02">[35] </ref> introduced shadows with a chromatic blue-shift to volume rendering which avoid the loss of information due to dark shadows when using a global illumination approach. They conducted a user study which showed that test subjects had an improved perception of surface and contrast when using a bright chromatic shift. Weigle and Banks <ref type="bibr" coords="3,207.49,293.53,14.94,8.02" target="#b35">[38] </ref>investigated the influence of global versus local illumination on the perception of complex 3D flow data and found that global illumination had a strong positive influence on the participants in terms of depth and shape recognition . Gribble and Parker <ref type="bibr" coords="3,121.26,333.38,10.45,8.02" target="#b5">[6] </ref>introduced a user study design to explore the perception of large particle datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EVALUATED VOLUMETRIC ILLUMINATION MODELS</head><p>While the list of used techniques is not exhaustive, we compiled it with the intention to present a cross sectional study that includes a balanced selection of state-of-the-art volume illumination models. Three of the tested techniques were based on texture slicing <ref type="bibr" coords="3,229.65,407.65,9.52,8.02" target="#b2">[3]</ref>. The other techniques which do not require slicing were implemented using ray- casting <ref type="bibr" coords="3,60.60,427.58,13.74,8.02" target="#b28">[29]</ref>. To be able to compare these two different rendering paradigms, we used a rate of two samples per dataset voxel for all renderings, independent of the viewing direction. For raycasting techniques , each ray was advanced for half the width of a voxel in each step in our renderings. For slicing techniques, the number of slices was twice the maximum number of voxels along the viewing axis. This way, the amount of samples used for the generation of an image was independent of the specific technique, which ensured a comparable image quality. Another basic difference in the introduced techniques is whether or not gradient information is incorporated in the illumination computation . Gradients can be helpful when considering the relation between the orientation of a surface contained in the dataset and the location of the light source. However, in noisy datasets (such as ultrasound datasets) the gradients may not be clearly defined and lead to artifacts and false lighting effects. Thus, the use of an illumination model not depending on gradients can be an advantage. For this reason, we avoided the use of gradients in all techniques if the option was available . For those techniques depending on gradients, we used an on-thefly central-difference approach. Finally, the introduced illumination models vary in the quality of the produced shadows. Usually, the terms high frequency and low frequency are used to categorize illumination model features such as shadows. When using a high frequency illumination model, even small opaque features produce clearly defined, sharp shadows which are cast over the whole extent of the scene. Low frequency shadows are softer and more subtle, but may appear more natural than high frequency shadows since their appearance may suggest the existence of penumbral regions. However, in volume graphics, these soft shadows result from light being scattered in the volumetric medium, or from the use of area light sources. We will use the term " soft shadow " for both of these kinds of lighting phenomena. Researchers concerned with the influence of shadows have also differed between self-shadowing of objects onto themselves and cast shadows of objects onto other ob- jects <ref type="bibr" coords="3,313.34,73.31,13.74,8.02" target="#b38">[41]</ref>. We neglected this difference since the division of a scene into objects cannot be applied in volume rendering, as a volume dataset represents a single object as well as the whole scene. While most of the techniques approximating global lighting focus on shadowing, some of them are able to produce additional visual cues such as scattering or color bleeding. However, as shadowing is the only feature which all the tested advanced illumination models have in common, our main focus was on the influence of shadows as a visual cue. Furthermore, a separate evaluation of different lighting phenomena would have introduced too much complexity to complete the study effectively. We also did not include unshaded DVR images in the study. While this case has its own benefits as it allows intensity quantification , our focus was on the improvement of perception of depth and size due to volume illumination. Below, we briefly introduce the tested models. Their features are composed and summarized in <ref type="figure" coords="3,524.79,212.79,19.70,8.02;3,294.12,222.75,3.36,8.02" target="#tab_1">Table  1</ref>. Phong lighting <ref type="bibr" coords="3,366.89,232.71,14.94,8.02" target="#b22">[23] </ref>was first applied to volume rendering by Levoy <ref type="bibr" coords="3,318.94,242.67,13.74,8.02" target="#b16">[17]</ref> , where voxel gradients are used as surface normals to calculate local lighting effects. For our images, we used a raycasting implementation of this technique. Phong lighting was included in the study since it serves as the default technique which is compared to more sophisticated approaches. Half angle slicing was introduced by Kniss et al. <ref type="bibr" coords="3,485.67,292.49,13.74,8.02" target="#b13">[14]</ref> . This slicebased approach performs two rendering passes for each slice, one from the point of view of the observer and one from that of the light source. This scheme allows the attenuation of each voxel with the amount of light that arrives at its location, producing hard shadows in the final image. It was included as it is a popular, often-used model which produces the highest-frequency shadowing effects of the presented tech- niques. Directional occlusion shading was introduced by Schott et al. <ref type="bibr" coords="3,527.32,372.19,13.74,8.02" target="#b29">[30]</ref>. As another slice-based technique, it exploits front-to-back rendering of the view aligned textures. It employs an additional buffer in which opacity from each voxel is blurred to influence neighboring voxels in the following slice. Thus, it simulates shadows which a cone of light originating at the camera position would produce. Due to the involved blurring, the resulting shadows are rather soft. We included this technique in the study since we were interested in the potential of this rather simple, efficient approach compared to more complex models. Multidirectional occlusion shading, introduced byŠoltészovábyˇbyŠoltészová et al. <ref type="bibr" coords="3,306.56,472.86,13.74,8.02">[34]</ref>, extends the directional occlusion technique to allow for a more flexible light source placement. This is achieved by replacing the radial blurring used in directional occlusion shading by a freely orientable elliptical blurring cone, allowing the user to select angle and distance of the light source. This illumination model was included in our study as it allows the direct analysis of the influence of the light source position on the user performance when compared to directional occlusion shading. Shadow volume propagation is a technique that approximates the light propagation direction in order to allow efficient rendering <ref type="bibr" coords="3,527.31,562.52,13.74,8.02">[26]</ref>. It uses a light volume texture to store luminance and scattering information about the dataset with respect to the current transfer function. During raycasting, the light volume is accessed to look up the luminance value of the current voxel and to attenuate the Phong shaded color, producing high frequency shadows. We added this technique to the survey since it is one of the few advanced shading models that are slicing independent while not requiring a noticeable pre-processing. Spherical harmonic lighting was originally used in volume rendering by Beason et al. <ref type="bibr" coords="3,385.19,652.19,10.45,8.02" target="#b0">[1] </ref>for isosurfaces. Ritschel suggested the use of spherical harmonics for DVR with raycasting <ref type="bibr" coords="3,489.88,662.15,13.74,8.02" target="#b24">[25]</ref> . In a preprocess , occlusion information per voxel is projected into spherical harmonic space (in our case, using 16 coefficients). This process has recently been improved for DVR, allowing for interactive transfer function changes <ref type="bibr" coords="3,372.76,702.00,13.74,8.02" target="#b14">[15]</ref>. During raycasting, the spherical integral over the product of an area light source and voxel occlusion is efficiently computed using the pre-computed occlusion information. The result is then used to attenuate the local voxel color, resulting in very soft shadows. We included this technique since it provides the possibility to test the effect of area light sources and more natural images on the user, in contrast to the hard shadows of techniques like half angle slicing. Dynamic ambient occlusion has been proposed by Ropinski et al. <ref type="bibr" coords="4,33.74,103.20,13.74,8.02" target="#b26">[27]</ref> . In a pre-processing stage, this technique computes local histograms for the local neighborhood of every voxel. During raycasting, the appropriate histogram is looked up for the current voxel combined with the transfer function color to produce an attenuated color. It was included in our tests since it belongs to a group of techniques that produce very subtle and subdued lighting effects, which may not be prominent enough to have a notable effect on user performance. <ref type="figure" coords="4,285.12,218.40,21.02,8.02">Fig. 2</ref>: The evaluation applet used for the study during the relative depth perception task. The user had to select the marker highlighting the structure closer to the camera. . However, we also included some datasets not familiar to layman users to increase diversity, such as scans of an aneurysm or a mouse heart. A white point light source was applied in all trials, except for spherical harmonic lighting which requires an area light source. In this case, a monochromatic spherical light distribution modeling light from the sun at the zenith on a clear day was chosen, according to the CIE Clear Sky standard which was proposed by Kittler <ref type="bibr" coords="4,449.14,581.31,13.74,8.02" target="#b12">[13]</ref>. For each image, a light source position in the front-facing hemisphere from the point of view of the observer was chosen. This allowed us to keep the position of the light source between techniques as consistent as the illumination models restrictions allow, while also complying with directional occlusion shading where the light position is not a changeable parameter (see Section 3). The participants were only informed about the fact that different illumination models were used. Detailed information about the illumination models themselves was not given. The users were advised to give their answers as quickly as possible and not to think too long about the solution. One run of the whole session typically lasted about 12 minutes. After the tests had been completed, the average performance per technique was calculated for every task and test subject. We investigated connections between illumination model, correctness of the users' answers and the time the users took to solve each trial. LINDEMANN AD ROPINSKI: ABOUT THE INFLUENCE OF ILLUMINATION MODELS ON IMAGE COMPREHENSION.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.. </head><p>The results were first evaluated using a one-way analysis of variance (ANOVA) to reject the null hypothesis that all correctness means were equal between techniques. If a confidence level of p = 0.05 was reached, a Bonferroni post-hoc test with the same acceptance level was performed to reveal differences between the individual techniques. This test was chosen as it compensates for error inflation caused by the rather large amount of pairwise comparisons between the seven lighting models. To test for a linear correlation between correctness and time, we used Pearson's r statistic. We will continue with a discussion of each of the four tasks as well as their respective outcomes, before drawing conclusions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relative Depth Perception 4.1.1 Material and Methods</head><p>For this task, we prepared 42 volume rendered images which were generated with the different illumination models. Each illumination model appeared 6 times. In each image, two circular markers were added which were located somewhere on the image region covered by the dataset (see <ref type="figure" coords="5,91.08,236.37,20.54,8.02">Fig. 2</ref>). The positions of the markers were selected carefully for each used dataset and camera position to keep the difficulty level as consistent as possible between techniques. The users' task was to select with the mouse the marker which highlighted the region of the dataset that seemed closer to them along the viewing axis. Measured variables were the correctness of the choice as well as the time the users took to process each trial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p> The ANOVA test showed a significant difference in the average correctness for the different techniques (F(6, 378) = 9.809, p &lt; 0.001). The Bonferroni post-hoc test showed that directional occlusion shading with an average correctness of 64.8% performed significantly better that multidirectional occlusion shading (50.2%), Phong lighting (45.1%), half angle slicing (44.8%) and spherical harmonic lighting (43.6%). Shadow volume propagation (60.3%) and dynamic ambient occlusion (60.0%) performed significantly better than Phong lighting , half angle slicing and spherical harmonic lighting (see <ref type="figure" coords="5,254.59,406.17,20.80,8.02" target="#fig_0">Fig. 3</ref>). There were no significant differences between techniques with respect to the elapsed testing time (F(6, 378) = 0.512, p = 0.799). Furthermore , there was no clear relation between time and correctness of the answers (Pearson's r = −0.32). Users took an average of 3.0 seconds for each trial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Discussion</head><p>In general, the performance of users in this task was rather poor, as even with the technique that had the best result, over one third of the trials was solved falsely. This may be due to the fact that most users were not familiar with volume rendering or the presented style of data. The relatively good performance of directional occlusion shading may indicate that rather simple shadows which introduce a natural front-toback hierarchy to dataset features can be beneficial when one has to distinguish the depth of different features in a volume rendered scene. This hierarchy effect may be due to the fact that the blurriness as well as the extent of a shadow from directional occlusion shading clearly depict the relation of the fore-and background object to the viewer. In <ref type="figure" coords="5,31.50,595.90,19.55,8.02">Fig. 4</ref>, a headlight causes an object to cast a shadow onto a box as well as a farther wall. Due to the shadow, the observer is able to distinguish the depth of the box surface from the depth of the wall as the shadow on the wall is bigger and softer. A similar effect is also exploited by the depth darkening technique first described by Luft et al. <ref type="bibr" coords="5,234.68,635.75,14.94,8.02" target="#b18">[19] </ref> who exploit blurring the depth buffer. Thus, both techniques comply with the " dark means deep " paradigm <ref type="bibr" coords="5,140.24,655.68,13.74,8.02" target="#b19">[20]</ref>. The relatively weak performance of multidirectional occlusion shading (of which directional occlusion shading can be considered a subset) could mean that, in contrast to what is stated byŠoltészovábyˇbyŠoltészová et al. <ref type="bibr" coords="5,156.66,686.61,13.74,8.02">[34]</ref>, the possibility to freely place the light source is not always an advantage. The separation of light source position and point of view of the observer leads to an increasing amount of visible shadows which might confuse users rather than help them, especially when the features that the user has to compare are not located next to each other. On the other hand, it could also <ref type="figure" coords="5,294.12,222.19,20.47,8.02">Fig. 4</ref>: When using directional occlusion shading, the blurriness and the extent of shadows indicates relative depth, which helps to distinguish between dataset features. indicate a headlight is closer to the optimal light source position than several other light source positions, which have been chosen to lie randomly within the hemisphere pointing towards the user. Spherical harmonic lighting and half angle slicing had the worst performance, although they differ strongly in the frequency of the produced effects. This may indicate that the softness of shadows does not directly influence user performance in this task. Therefore, we performed an additional ANOVA test in which we compared the average correctness per user for soft shadow versus hard shadow techniques, without including the Phong lighting model. The test did not yield a result at a statistically significant level (F(1, 108) = 1.107, p = 0.295). This may indeed indicate that shadow frequency is not as important for this task as other illumination attributes (such as shadow direction). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Absolute Depth Perception</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Material and Methods</head><p>This task subjected users to 42 volume rendered images, with every technique appearing 6 times. Each trial displayed the image with a single circular marker along with a horizontal slider at the bottom (see <ref type="figure" coords="5,294.12,463.79,20.11,8.02">Fig. 5</ref>). Again, each marker was positioned with caution depending on dataset and camera position to allow for a consistently fair challenge. The users were asked to approximate the absolute depth of the marked region on the slider in percent, in relation to the maximum distance between the closest and the farthest point on the dataset from their point of view. After making their choice, the users had to confirm their selection by clicking the OK button. We measured the error between their choice and the actual absolute depth of the marked region as well as the time the users took to complete each trial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head><p>The ANOVA test revealed a significant difference in the average discrepancy of the users' guesses (F(6, 378) = 8.819, p &lt; 0.001). The Bonferroni post-hoc test showed that half angle slicing (18.5%), shadow volume propagation (20.3%), directional occlusion shading (21.0%) and multidirectional occlusion shading (21.0%) performed significantly better than Phong lighting (25.7%) and dynamic ambient occlusion (26.0%). Additionally, spherical harmonic lighting at 21.8% average discrepancy performed significantly better than dynamic ambient occlusion (see <ref type="figure" coords="5,369.29,654.90,21.92,8.02" target="#fig_1">Fig. 6</ref>). No significant differences in time were measured between techniques (F(6, 378) = 0.415, p = 0.869). There was no clear relation between time and correctness of the answers (Pearson's r = 0.109). Users took an average of 6.7 seconds for each trial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Discussion</head><p> The results showed hardly any differences between the advanced techniques . The presence of global shadows seemed to help absolute depth perception, as Phong lighting and dynamic ambient occlusion had the worst performance. Shadows projected onto a bounding shadow receiver (such as the walls of the synthetic dataset in <ref type="figure" coords="6,210.89,73.31,20.69,8.02">Fig. 7</ref>) may have helped subjects to better judge depth in this task. This resource is missing when using dynamic ambient occlusion, as this technique does not capture occlusion from objects which are not adjacent. Such an effect may be of advantage when working with CT data, as they potentially contain less noise and thus produce less, but better defined shadows. Therefore, we evaluated the results for this modality only. The ANOVA test revealed a significant difference in means of correctness (F(6, 378) = 11.397, p &lt; 0.001), but not of elapsed time (F(6, 378) = 0.272, p = 0.95). The post-hoc test showed that the correctness for half angle slicing was significantly better than for spherical harmonic lighting, directional occlusion shading, Phong lighting and dynamic ambient occlusion for this modality. The other techniques did not significantly improve their performance compared to their results for all modalities. This tendency towards a technique with very hard shadows may be explained by the fact that these shadows are cast across the whole extent of the scene. They may have the effect of a natural ruler when cast onto a homogeneous region of the dataset, which are more often contained in high quality datasets such as CT. Wanger <ref type="bibr" coords="6,80.04,262.60,14.94,8.02" target="#b33">[36] </ref>reported no significant differences between soft and hard shadows for an absolute depth perception task using nonvolume rendered images. To compare our results to his, we performed the previous additional ANOVA test in which we compared the average discrepancy per user for soft shadow versus hard shadow techniques , without including the Phong illumination model. The result showed that there is a small, but significant difference in favor of hard shadow techniques (F(1, 108) = 9.403, p = 0.003), with an average discrepancy of 19.4% for hard shadows versus 21.9% for soft shadows . This may indicate that (at least for volume rendered images), hard shadows provide a better frame of reference when judging the absolute depth of features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relative Size Perception 4.3.1 Material and Methods</head><p> In this task, 21 volume rendered images were shown. Every illumination model appeared 3 times. The participants had to sort features contained in the datasets by volume size, such as the different bones in a CT scan of a human hand. The users had to sort the objects by size with the help of drop-down combo-boxes at the bottom of the image, which were labeled with the letter associated with the object. They had to confirm their choice with an OK button before proceeding . We selected and segmented different features which were labeled in the images with letters and highlighted with different colors so the users knew which parts of the dataset they were supposed to compare (see <ref type="figure" coords="6,39.92,516.12,20.42,8.02">Fig. 7</ref>). Ultrasound datasets were not used for this task as they proved to contain too much noise to achieve a meaningful segmentation . Measured values were again the correctness of the ranking given by the users, as well as the time they took for each trial. A trial was evaluated as correct only if all objects were assigned the correct size rank. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>Again, the differences in average correctness per user were statistically significant between illumination models according to the ANOVA test (F(6, 378) = 33.88, p &lt; 0.001). The post-hoc tests revealed that directional occlusion shading performed significantly better than all other techniques with an average correctness of 86.4%. Shadow volume propagation (63.2%) performed significantly better than multidirectional occlusion shading and dynamic ambient occlusion (both 48.4%), half angle slicing (42.7%) and Phong lighting (33.3%). Spherical harmonic lighting (56.9%) performed significantly better than half angle slicing and Phong lighting, while multidirectional occlusion shading and dynamic ambient occlusion only delivered significantly better results than Phong lighting (see <ref type="figure" coords="6,162.68,706.53,20.06,8.02">Fig. 8</ref> ). Furthermore, a significant difference in timing could be measured (F(6, 378) = 4.332, p &lt; 0.001). The Bonferroni post-hoc test showed that on average, trials with directional occlusion shading were answered significantly faster <ref type="figure" coords="6,285.12,218.40,20.74,8.02">Fig. 5</ref>: The evaluation applet used for the study during the absolute depth perception task. The user had to estimate the depth of the marked structure in relation to the total extent of the scene.  at 4.8 seconds than trials with dynamic ambient occlusion (8.5 seconds ) and shadow volume propagation (9.2 seconds). Additionally, spherical harmonic lighting trials were solved significantly faster at an average of 5.6 seconds than shadow volume propagation trials. There was no clear relation between time and correctness of the answers (Pearson's r = −0.04). Users took an average of 6.4 seconds for each trial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Discussion</head><p> The results of this task are similar to those of the relative depth perception task. The use of directional occlusion shading leads to a success rate of over 85%, followed with some distance by shadow volume propagation. This is in line with previous research which indicates that comprehension of depth is an important prerequisite for comprehension of size <ref type="bibr" coords="6,344.32,616.87,9.52,8.02" target="#b4">[5]</ref>, as objects farther away from the viewer appear smaller while having a potentially larger volume. As directional occlusion shading was the top-performing technique for relative depth perception, it may have helped users to avoid false judgments based on perspective. Furthermore, as the light direction is congruent with the viewing direction using this technique, a shadow cast by an object close to the viewer was in most cases projected onto a shadow receiver, since the camera was pointed at the center of the dataset in almost all of our trials. Less information was lost since the shadows did not " leave " the dataset. Half angle slicing and multidirectional occlusion shading achieved a similar ranking in the first task. These techniques do not seem to yield a considerable difference compared to the simple Phong illumi-nation model in this task. Spherical harmonic lighting, however, yields considerably better results than in the first task as it improves user performance compared to the local lighting model. While very soft shadows seem to be confusing rather than helpful when pin-pointing a single point of depth in a scene, they could be better suited to approximate size. This may be due to the fact that single small features, which were also often marked in the first task, have almost nonexistent shadows when illuminated by a low-frequency illumination model, while their projection is still clearly visible with high-frequency techniques as half angle slicing. The size of the relatively large objects in this task, however, can be possibly estimated rather well even with low frequency techniques because they produce natural, volumetric soft shadows to which users can relate. Since shape and volume of an object are closely related, this conclusion may seem to be in conflict with the results of Wanger <ref type="bibr" coords="7,112.06,192.86,13.74,8.02" target="#b33">[36]</ref>, who reported that diffuse shadowing was detrimental to the subjects' ability to perceive the shape of an object. However, we did not ask the users to assign a shape to a single object, but rather to find a ranking in relative size between several objects. This seems to be a different process in which individual shape recognition may not be the most important tool, but rather the shadowing of the scene as a whole. Dynamic ambient occlusion, another technique with rather lowfrequency effects, failed to reach a similar success rate as in the previously described relative depth task. This is probably due to the fact that objects which are located relatively far from surrounding features do not produce shadows at all with this technique, since they are not included in the local voxel neighbourhood considered during the preprocess (see Section 3). Therefore, low-frequency lighting techniques which still include the whole dataset in the occlusion calculation seem to be a good choice in this context. This is also supported by the fact that directional occlusion shading and spherical harmonic lighting trials were answered faster than trials involving other techniques, while retaining a relatively high rate of correctness. As the results from the previous absolute depth recognition task became less ambiguous when considering only images from high quality datasets, we again conducted an analysis of images from CT data only. There was a significant difference in the average correctness (F(6, 378) = 24.615, p &lt; 0.001). The success rate was generally higher for this modality than when considering all modalities together, except for Phong lighting with a success rate of only 7.3%. The posthoc test showed that even while considering only high quality CT data, directional occlusion shading at a success rate of 83.6% still performed significantly better than dynamic ambient occlusion, half angle slicing and multidirectional occlusion shading, while spherical harmonic lighting performed significantly better than dynamic ambient occlusion . Additionally, all techniques performed significantly better than Phong lighting. The ANOVA test for timing results was also significant for CT datasets (F(6, 378) = 3.357, p = 0.003). Directional occlusion shading performed significantly better in this regard at an average time of 4.7 seconds than dynamic ambient occlusion (8.5 seconds), Phong lighting (8.6 seconds) and multidirectional occlusion shading (8.7 seconds). This result emphasizes the potential of directional occlusion shading and of advanced lighting in general for this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Subjective Evaluation 4.4.1 Material and Methods</head><p>In the final task, users were presented with 26 pairs of images which were each rendered with one volume dataset, but using two different illumination models. The image pairs differed only in the used illumination model (see <ref type="figure" coords="7,109.39,645.63,20.49,8.02">Fig. 9</ref>), and the users were asked to select that image which they subjectively liked better. All 21 possible combinations of illumination models were included in the series. We measured the percentage of times each illumination model was preferred. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Results</head><p>The ANOVA test showed a significant difference in illumination model preference (F(6, 378) = 61.655, p &lt; 0.001). The Bonferroni post-hoc test revealed that Phong lighting was favored significantly more often by users than all other techniques at a rate of 81.2%, followed <ref type="figure" coords="7,294.12,218.40,19.66,8.02">Fig. 7</ref>: The evaluation applet used for the study during the relative size perception task. The user had to sort the labeled objects by size, from biggest to smallest. <ref type="figure" coords="7,294.12,424.68,20.44,8.02">Fig. 8</ref>: Relative size perception results with standard error. The bars indicate the average percentage of correctly ranked objects per participant when the respective technique was used. by half angle slicing at 68.0% being favored significantly more often than the remaining approaches. The middle ranks were occupied by shadow volume propagation (54.0%) and spherical harmonic lighting (52.9%), which performed significantly better than the subjacent techniques. Directional occlusion shading (37.9%) and dynamic ambient occlusion (37.3%) were preferred significantly more often than multidirectional occlusion shading at a preference rate of 21.8% (see <ref type="figure" coords="7,294.12,541.46,23.54,8.02">Fig. 10</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Discussion</head><p>It is rather surprising that the Phong illumination model performed so well in this task, compared to the previous results. There are no shadows present, and the resulting images have a more artificial appearance than those rendered with the other techniques. On the other hand, spherical harmonic lighting, which could be considered the most realistic of the used illumination models due to a natural area light source and diffuse shadows, delivered a rather average result. It is worth noting that we did not specify what quality of the image the users were supposed to judge (such as " Which image appears more realistic to you? " ), but asked only which image appealed more to them aesthetically. Phong images are generally brighter and more colorful due to the missing shadows and have a more illustrative quality. This may have a positive effect on the user if such an image is placed next to one including shadows. The participants also seemed to prefer high-frequency hard shadows over low-frequency techniques, with the half angle slicing and shadow volume propagation illumination models reaching considerably better results than the other advanced illumina-tion models. This was confirmed by an additional ANOVA test of soft shadowing versus hard shadowing techniques without considering the cases involving the Phong model, which showed a significant difference in preference rates (F(1, 108) = 101.14, p &lt; 0.001). Altogether, techniques with hard shadows were preferred in 65.6% of those trials, while techniques with soft shadows consequently had a preference rate of only 34.3%. Hard shadows with no penumbral region do not appear often in daily life since a point light source with an otherwise dark environment would be necessary to produce them. This might indicate a general tendency of users to prefer rather artificial, illustrative representations of a scene over more natural, realistic approximations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION 5.1 Summary of Results</head><p> Our user study shows that depth and size perception in volume rendering is significantly improved by exploiting advanced lighting approximations , since the local technique (Phong) delivers worse results than almost all global models in all related tasks. This result is backed up by previous studies with a similar outcome in polygonal render- ing <ref type="bibr" coords="8,37.36,248.25,14.19,8.02" target="#b10">[11,</ref><ref type="bibr" coords="8,53.49,248.25,10.64,8.02" target="#b34"> 37]</ref> . Therefore, it seems to be preferable to use an illumination model which provides additional cues such as shadows when conducting volume data analysis. Of course, an interesting result is the discrepancy between effectiveness and appeal of the illumination models, as the Phong illumination model was most often favored by the test subjects. Apparently, there is an inverse relationship between realism and preferableness of an illumination model. It may be caused by the presence of too much visual information in the images with shadows, in contrast to the relative simplicity of the Phong model images . Furthermore, this model produces specular highlights. They further brighten the images and let areas of equal density seem more like surfaces. Due to this, the unfamiliar semi-transparent cloud-like appearance of volume rendered images is partially suppressed, which may have a positive influence on user acceptance. This effect may also be connected to the general familiarity of rendered images that were generated with Phong lighting, since it can be considered as a longestablished standard used in many computer-based scenarios. This could be exploited when not analysis, but rather presentation of data is important. For instance, the Phong illumination model (or a similar basic shading method) could be used to illustrate medical information during a doctor / patient communication. The choice of the applied advanced illumination model should depend on the required task as well as the quality of the volumetric data. With respect to relative depth recognition, it seems that positioning the light source at the point of view of the user is beneficial in the tested cases. Directional occlusion shading seems to help the user best in distinguishing the depth of dataset features from another, because the muted shadowing effects produced by this technique introduce a hierarchical order. It seems that cast shadows from the shadow volume propagation, half angle slicing and multidirectional occlusion shading techniques can in same cases be confusing rather than helpful if the shadow direction deviates from the viewing axis. This is especially interesting as medical illustrators prefer a placement of the light source at the top left corner of the field of vision to enhance image percep- tion <ref type="bibr" coords="8,39.35,586.98,13.74,8.02">[34]</ref>. Our results are generally not in line with this assumption, which becomes clear in particular when comparing the performance of directional occlusion shading versus multidirectional occlusion shading . Here, the difference lies not in the quality of the shadows, but in their direction and amount. Thus, while shadows are beneficial for relative depth perception in general, our findings indicate that one should use them in a rather subtle manner when confronted with this kind of task to reach the best results. The relatively good performance of dynamic ambient occlusion in the first task also supports this claim. The shadows of this technique do not depend on a light source position at all, but only reflect the local scene geometry around each voxel. Of course, in all of the presented techniques the light source could be placed at (or around) the point of view of the observer to prevent excessive shadowing, which may be a point of interest for future studies. Using directional occlusion shading to achieve a better relative feature perception has the additional advantage that this technique is <ref type="figure" coords="8,285.12,218.40,20.23,8.02">Fig. 9</ref>: The evaluation applet used for the study during the subjective preference task. The user had to select the image which he subjectively preferred. <ref type="figure" coords="8,285.12,424.68,24.94,8.02">Fig. 10</ref>: Subjective preference results with standard error. The bars indicate the average percentage of times per participant in which the respective technique was preferred over the other shown technique. dent of gradients and does not require a noticeable pre-processing. For these reasons, we recommend directional occlusion shading for tasks related to relative depth perception in combination with all modalities. In turn, if the classification of the absolute depth of a dataset feature is required, our results suggest that an illumination model should be preferred which produces shadows that are not aligned with the viewing axis. Half angle slicing, a global lighting approximation with hard shadows, may have reached better results than others, since it provides a frame of reference by casting clearly outlined shadows onto other dataset features. This effect, however, is not as noticeable as the apparent positive influence of a head light on feature ranking tasks. It seems to strongly depend on the quality of the given data, as the test results were less ambiguous when considering CT data only. The addition of a reference ruler indicating the total extent of a dataset in addition to an advanced lighting model could improve user performance. In the absolute depth task of Wanger <ref type="bibr" coords="8,398.02,621.17,14.94,8.02" target="#b33">[36] </ref>in which he included a ruler, test subjects reached far better results than in our version of this task. However , considering the higher complexity of volume rendered images used by us compared to Wanger's images of simple non-volumetric objects, the potential benefit of this device is at least doubtful. Our tests should be repeated with a ruler in the future to determine if such a tool would improve user accuracy and yield a clearer result even for noisy data. Until this has been investigated, we recommend half angle slicing for CT and other clearly structured volumetric data, as it reached the best test result and requires no long pre-processing. Finally, the recognition of relative feature size seems to benefit from soft lighting effects as well as shadows which are cast over the whole scene and not just towards neighbouring features. This is indicated by the good performance of shadow volume propagation and spherical harmonic lighting as well as the very good performance of directional occlusion shading, which combines both attributes. In addition, the recognition of relative size is a more complex procedure which takes more time to complete than the other experiments. This may have led to a more distinct difference in timing results than in the other tasks. Directional occlusion shading also performed well in this regard , as users needed significantly less time to complete the task with this technique compared to several other models. Therefore, we recommend directional occlusion shading for all modalities for this kind of task, since it combines the best testing results for the quality and the timing of the participants' answers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Derived Guidelines</head><p>Based on our results, we recommend the following guidelines for the application of volume illumination techniques to improve image com- prehension: @BULLET In general, keep lighting as subtle as possible -the principle " less is more " seems to apply. @BULLET A lighting model that does not dominate the appearance of the scene should be used as a default. The image should be enhanced , not distorted. @BULLET To enable the user to understand the relation between all structures in a scene, the lighting model should ideally include the whole dataset when producing lighting effects. @BULLET We recommend an initial light source position at the point of view of the observer, as the produced visual cues are less likely to be obtrusive. If this default configuration does not provide enough information, an optimal model should enable the user to produce more noticeable effects by changing parameters like light source position and shadow frequency. Directional occlusion shading complies with most of these guidelines and could be used as a starting point. We believe that multidirectional occlusion shading is a valuable first step to enhance this technique. Other properties such as the frequency of effects could be added as a changeable parameter in the future to allow for the adjustment of lighting properties according to the given circumstances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p> We have presented a study in which we tested and compared the effectiveness of standard as well as advanced illumination models for DVR. Based on the results, we derived guidelines for choosing an illumination model. Our survey has shown that global illumination models make a difference when it is necessary to assess depth or size in images. Although the time the users took to complete the individual trials was hardly influenced by the use of a particular model, the level of correctness improved significantly. Furthermore, the position of the light source seems to play a pivotal role. Depending on the situation, the direction of shadows can significantly influence depth perception. We believe directional occlusion shading performed so well because the users were intuitively aware of the exact light source position at the point of observation, whereas an arbitrarily placed light source provides an additional factor of uncertainty. While modern hardware allows the inclusion of most of the tested illumination models in realtime , we think that there is a strong indication that at least a relatively simple illumination model as the directional occlusion shading should be employed in the professional analysis of scientific volume data. In the future, additional studies are needed to further clarify the influence of different illumination factors, which should be carried out in a controlled environment. A more in depth recording of the participants approach to the individual tasks (for instance, with eye-tracking) could be useful. More involved, interactive tasks like puzzle solving could provide more information on the potential of illumination models to convey shape and depth in a volume rendered scene. Techniques which we omitted as well as possible combinations of existing techniques should be tested as well. Furthermore, there are several rendering parameters whose influence should be explored. In the presented study, the light source position was not directly part of the investigated parameters. A more careful research of the influence of light source positioning should be conducted, as Hubona et al. <ref type="bibr" coords="9,475.95,103.20,14.94,8.02" target="#b10">[11] </ref> have also suggested . Finding out if a head light is beneficial in conjunction with all illumination models would be especially important. Another point of interest would be how a moving light source (resulting in moving shadows) could improve the perception of a volume rendering scene over static lighting. Research indicates that mobile shadows can be a much stronger visual cue <ref type="bibr" coords="9,388.27,162.97,13.74,8.02" target="#b11">[12]</ref> . Other visual cues such as interreflections , light scattering or color bleeding should be also included in future testing. If possible, each cue should be tested individually to find out exactly which ones should be part of an optimal lighting model. Finally , a task which tests the influence of volume illumination models on shape recognition could be an interesting addition to a prospective study, maybe also when dealing with dynamic data. In our study only laymen with no familiarity with the presented datasets were asked to participate. Thus, we were able to minimize the influence of previous knowledge and expectations. Domain experts could not only benefit from visual cues, but also their experience and training when solving different tasks. As a consequence, a future domain expert study would be of interest, as it could reveal completely different results. This study should include task-driven trials to investigate the effectiveness of volume illumination models in real-life tasks, such as the identification of aneurysms in medical volume datasets. As far as the development of new illumination models is concerned, we find that there is still room for improvement. Even untrained users should be able to reach better results than those demonstrated in our tests, as the respective error rates were rather high. An optimized illumination model as suggested in Section 5 would be as unambiguous and subtle as directional occlusion shading as the default state. This could be combined with the possibility to move the light source away from the observer's point of view temporarily to produce high frequency cast shadows, which might help to relate the absolute depth of features to the whole extent of the dataset. Multidirectional occlusion shading might be a first valuable step into this direction. However, an additional study in which user interaction is included would have to be carried out to test this combination. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,285.12,424.68,250.37,8.02;4,285.12,434.64,250.37,8.02;4,285.12,444.60,174.10,8.02"><head>Fig. 3: </head><figDesc>Fig. 3: Relative depth perception results with standard error. The bars indicate the average percentage of correctly selected markers per participant when the respective technique was used. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,285.12,424.68,250.37,8.02;6,285.12,434.64,250.37,8.02;6,285.12,444.60,185.55,8.02"><head>Fig. 6: </head><figDesc>Fig. 6: Absolute depth perception results with standard error. The bars indicate the average discrepancy from the true depth of the marker per participant when the respective technique was used. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true" coords="2,30.16,50.40,490.52,193.85"><figDesc coords="2,148.48,50.40,261.03,8.02">Table 1: A summary of the features of the surveyed illumination models.</figDesc><table coords="2,30.16,71.05,490.52,173.20">Model 
Rendering 
paradigm 

Uses 
gradients 

Light source 
location 

Pre-processing re-
quired 

Simulated global 
lighting phenomena 

Whole dataset 
affects lighting 

Frequency 
of shadows 

Half angle slicing 
slice-based 
no 
no constraints 
no 
shadows, scattering 
yes 
very high 
Directional 
occlusion 
shading 

slice-based 
no 
headlight only 
no 
shadows 
yes 
low 

Multidirectional 
occlusion 
shading 

slice-based 
no 
user hemisphere 
no 
shadows 
yes 
low 

Dynamic 
ambient 
occlusion 

independent optional 
no constraints 
long, but only once 
per dataset 

shadows, color 
bleeding 

only voxel 
neighbourhood 

low 

Shadow volume 
propagation 

independent yes 
no constraints 
yes, but hardly 
noticeable 

shadows, scattering 
yes 
high 

Spherical 
harmonic 
lighting 

independent optional 
freely rotatable 
area light source 

for every transfer 
function change, 
noticeable 

shadows, color 
bleeding, scattering 

yes 
very low 

Phong lighting 
independent yes 
no constraints 
no 
none 
no 
-

</table></figure>

			<note place="foot" n="4"> USER STUDY The study&apos;s goal was to test the influence of the techniques on the ability of an average layman user to deal with the challenge of understanding volume rendered images. In order to include as many users as possible to reach a representative subset of the population, we decided to realize the testing procedure by using a Java applet which users could start in an internet browser on their office or home computer . Due to the nature of this approach, we did not have direct control over conditions in which the participants used the applet, like lighting, distractions during testing, or monitor setup. Although users were advised to perform the test as focused as possible, an uncontrolled testing environment might affect user performance in unpredictable ways. For instance, a badly calibrated monitor might display images with a large amount of shadows as too dark, which could impede the users&apos; perception. In these cases, the Phong illumination model might actually be beneficial, as it generally generates brighter images. On the other hand, if the monitor contrast is not sufficient, the advanced illumination models may be advantageous, since they usually incorporate a higher degree of contrast. However, in most cases of bad testing conditions (like distractions while running the applet), we believe that every technique would be disadvantaged equally, due to the relatively short time it took to complete the tests. This would lower the average user performance but would not favour a specific technique over another. Furthermore, letting participants run the tests within their familiar working environments allowed for a stress-free atmosphere as opposed to a clinical setup. As we regard this study as an entry point for further tests, we think that our setup is sufficient to reveal general performance tendencies between the techniques which can later be explored in more detail in a more controlled user or even expert study. The applet started with a short questionnaire in which participants were asked for some information about themselves, including age, gender, profession and quality of eyesight. This was followed by the four tasks. Each of them consisted of a screen with instructions, followed by a first easy to solve training problem and a subsequent series of between 21 and 42 images, depending on the task. After completion , the results were transmitted back online. No medical or other expert knowledge was required, as the exercises were kept simple. We received a total number of 61 test results. Six of them were discarded as the participants had either not solved the training problems correctly or had needed a large amount of time to complete the session. The remaining 55 participants had diverse backgrounds, with their age ranging from 17 to 62 years with an average of 27.3 years. 56% of the participants were male. All had normal or corrected to normal vision, with 53% wearing glasses or contact lenses. Most of the users had limited 3D graphics experience or were complete laymen. The applet used a within-subjects design as all participants took part in all four tasks. The users did not receive any feedback on their performance during or after the trials. The images for every task were presented in random order to prevent an ordering bias. They were generated using CT, MR, ultrasound and synthetic datasets which were illuminated using the different illumination models. Each dataset was used two times at most per task to allow for a greater variety of data. If a dataset appeared twice in one of the first three tasks which were aimed at depth and size perception, the lighting model, transfer function , camera position and light source position were changed between images to prevent a learning effect. All images were rendered</note>

			<note place="foot">LINDEMANN AD ROPINSKI: ABOUT THE INFLUENCE OF ILLUMINATION MODELS ON IMAGE COMPREHENSION...</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>The compared illumination techniques have been integrated into the Voreen visualization framework (www.voreen.org). The authors would like to thank Mehraneh Hesaraki for her contributions, as well as Matt Cooper, Camilla Forsell, Frank Steinicke and the reviewers for their comments which helped to improve the paper. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="9,312.38,557.27,232.12,7.13;9,312.38,566.74,232.12,7.13;9,312.38,576.20,120.87,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Precomputed illumination for isosurfaces</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">M</forename>
				<surname>Beason</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Grant</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Banks</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Futch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">Y</forename>
				<surname>Hussaini</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Visualization and Data Analysis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,585.66,232.12,7.13;9,312.38,595.13,232.12,7.13;9,312.38,604.59,221.19,7.13"  xml:id="b1">
	<analytic>
		<title level="a" type="main">A perceptive evaluation of volume rendering techniques</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Boucheny</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G.-P</forename>
				<surname>Bonneau</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Droulez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Thibault</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ploix</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Symp. on Applied perception in graphics and visualization</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,614.06,232.12,7.13;9,312.38,623.52,232.12,7.13;9,312.38,632.99,142.13,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Accelerated volume rendering and tomographic reconstruction using texture mapping hardware</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Cabral</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Foran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symp. on Volume visualization</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,642.45,232.12,7.13;9,312.38,651.92,232.12,7.13;9,312.38,661.38,202.09,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Perception-based transparency optimization for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">M.-Y</forename>
				<surname>Chan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W.-H</forename>
				<surname>Mak</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Qu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1283" to="1290" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,670.85,232.12,7.13;9,312.38,680.31,17.93,7.13"  xml:id="b4">
	<monogr>
		<title level="m" type="main">Sensation and Perception</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Goldstein</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="179" to="184" />
			<pubPlace>Wadsworth</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,689.77,232.12,7.13;9,312.38,699.24,232.12,7.13;9,312.38,708.70,50.91,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">An experimental design for determining the effects of illumination models in particle visualization</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">P</forename>
				<surname>Gribble</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">G</forename>
				<surname>Parker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APGV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">175</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,718.17,232.12,7.13;9,312.38,727.63,232.12,7.13;9,312.38,737.10,164.80,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">GPU-accelerated deep shadow maps for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hadwiger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kratz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sigg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Bühler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/EG Conference on Graphics Hardware</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="27" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,54.06,232.12,7.13;10,40.76,63.52,232.12,7.13;10,40.76,72.99,223.00,7.13"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient ambient and emissive tissue illumination using local occlusion in multiresolution volume rendering</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hernell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume Graphics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,82.45,232.12,7.13;10,40.76,91.92,232.12,7.13;10,40.76,101.38,232.12,7.13;10,40.76,110.85,33.87,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive global light propagation in direct volume rendering using local piecewise integration</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hernell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume and Point-Based Graphics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,120.31,232.12,7.13;10,40.76,129.78,232.12,7.13;10,40.76,139.24,154.96,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual cues for perceiving distances from objects to surfaces</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">H</forename>
				<surname>Hu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">A</forename>
				<surname>Gooch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">H</forename>
				<surname>Creem-Regehr</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">B</forename>
				<surname>Thompson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoper. Virtual Environ</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="652" to="664" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,148.70,232.12,7.13;10,40.76,158.17,232.12,7.13;10,40.76,167.63,232.12,7.13;10,40.76,177.10,17.93,7.13"  xml:id="b10">
	<analytic>
		<title level="a" type="main">The relative contributions of stereo, lighting, and background scenes in promoting 3D depth visualization</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">S</forename>
				<surname>Hubona</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">N</forename>
				<surname>Wheeler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">W</forename>
				<surname>Shirah</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Brandt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput.-Hum. Interact</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="214" to="242" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,186.56,232.12,7.13;10,40.76,196.03,232.12,7.13;10,40.76,205.49,96.77,7.13"  xml:id="b11">
	<monogr>
		<title level="m" type="main">Moving cast shadows and the perception of relative depth. Max-Planck-Institute for Biological Cybernetics</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Kersten</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Mamassian</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Knill</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,214.96,232.12,7.13;10,40.76,224.42,232.02,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Standardisation of the outdoor conditions for the calculation of the daylight factor with clear skies</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Kittler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Sunlight in Buildings</title>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,233.88,232.12,7.13;10,40.76,243.35,232.12,7.13;10,40.76,252.81,49.81,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive translucent volume rendering and procedural modeling</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kniss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Premoze</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ebert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,262.28,232.12,7.13;10,40.76,271.74,232.12,7.13;10,40.76,281.21,232.12,7.13;10,40.76,290.67,17.93,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient visibility encoding for dynamic illumination in direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kronander</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Jonsson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Low</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Unger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,300.14,232.12,7.13;10,40.76,309.60,181.05,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth discrimination from shading under diffuse lighting</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S</forename>
				<surname>Langer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">H</forename>
				<surname>Bülthoff</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="649" to="660" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,319.07,232.12,7.13;10,40.76,328.53,126.41,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Levoy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,337.99,232.12,7.13;10,40.76,347.46,232.12,7.13;10,40.76,356.92,39.18,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Local ambient occlusion in direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hernell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,366.39,232.12,7.13;10,40.76,375.85,232.12,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Image enhancement by unsharp masking the depth buffer</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Luft</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Colditz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Deussen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1206" to="1213" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,385.32,214.00,7.13"  xml:id="b19">
	<analytic>
		<title level="a" type="main">The Natural Way to Draw</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Nicolaides</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Houghton Mifflin</title>
		<imprint>
			<date type="published" when="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,394.78,232.12,7.13;10,40.76,404.25,116.71,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Splatting with shadows</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Nulkar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Mueller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Volume Graphics</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,413.71,232.12,7.13;10,40.76,423.17,232.12,7.13;10,40.76,432.64,136.27,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Isosurface ambient occlusion and soft shadows with filterable occlusion maps</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Penner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Mitchell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume and Point-Based Graphics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,442.10,232.12,7.13;10,40.76,451.57,89.86,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">T</forename>
				<surname>Phong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,461.03,232.12,7.13;10,40.76,470.50,232.12,7.13;10,40.76,479.96,78.18,7.13"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Advanced illumination techniques for GPU volume raycasting</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rezk-Salama</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hadwiger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ljung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH Courses Program</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,489.43,232.12,7.13;10,40.76,498.89,232.12,7.13;10,40.76,508.36,41.84,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast GPU-based Visibility Computation for Natural Illumination of Volume Data Sets</title>
		<author>
			<persName>
				<forename type="first">T</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Short Paper Eurographics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,285.12,54.06,250.38,7.13;10,303.38,63.52,232.12,7.13;10,303.38,72.99,33.87,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">Interactive volumetric lighting simulating scattering and shadowing</title>
		<author>
			<persName>
				<forename type="first">]</forename>
				<forename type="middle">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Döring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rezk-Salama</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PacificVis 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,82.45,232.12,7.13;10,303.38,91.92,232.12,7.13;10,303.38,101.38,232.12,7.13;10,303.38,110.85,69.29,7.13"  xml:id="b26">
	<analytic>
		<title level="a" type="main">Interactive volume rendering with dynamic ambient occlusion and color bleeding</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Meyer-Spradow</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Diepenbrock</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mensmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">H</forename>
				<surname>Hinrichs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (Eurographics)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="567" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,120.31,232.12,7.13;10,303.38,129.78,232.12,7.13;10,303.38,139.24,193.61,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">Obscurance-based volume rendering framework</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ruiz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Boada</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Viola</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Feixas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sbert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/EG Int. Symp. on Volume and Point-Based Graphics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,148.70,232.12,7.13;10,303.38,158.17,51.46,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">GPU-based monte-carlo volume raycasting</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">R</forename>
				<surname>Salama</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Graphics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,167.63,232.12,7.13;10,303.38,177.10,232.12,7.13;10,303.38,186.56,232.12,7.13;10,303.38,196.03,118.08,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">A directional occlusion shading model for interactive direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schott</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Pegoraro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Boulanger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Bouatouch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (Eurographics/IEEE VGTC Symp. on Visualization)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="855" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,205.49,232.12,7.13;10,303.38,214.96,138.42,7.13"  xml:id="b30">
	<analytic>
		<title level="a" type="main">Vicinity shading for enhanced perception of volumetric data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Stewart</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,224.42,232.12,7.13;10,303.38,233.88,198.71,7.13"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Image plane sweep volume illumination</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Sundén</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ynnerman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vis Proceedings)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Accepted</note>
</biblStruct>

<biblStruct coords="10,303.38,243.35,232.12,7.13;10,303.38,252.81,232.12,7.13;10,303.38,262.28,17.93,7.13;10,285.12,269.89,30.56,8.97;10,312.14,271.74,223.36,7.13;10,303.38,282.95,232.12,7.13;10,303.38,292.41,94.13,7.13;10,285.12,300.03,31.05,8.97;10,312.62,301.88,222.87,7.13;10,303.38,311.34,123.22,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Feature enhancement by volumetric unsharp masking A multidirectional occlusion shading model for direct volume rendering Chromatic shadows for improved perception</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Tao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Lin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Bao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Dong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Clapworthy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Soltészová</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Patel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Viola</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35] V. ˇ Soltészová, D. Patel, and I. Viola NPAR</title>
		<imprint>
			<date type="published" when="2009-04" />
			<biblScope unit="page" from="581" to="588883" />
		</imprint>
	</monogr>
	<note>[. 34]. Accepted</note>
</biblStruct>

<biblStruct coords="10,303.38,320.81,232.12,7.13;10,303.38,330.27,232.12,7.13;10,303.38,339.74,93.74,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">The effect of shadow quality on the perception of spatial relationships in computer generated imagery</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">C</forename>
				<surname>Wanger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symp. on Interactive 3D graphics</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="39" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,349.20,232.12,7.13;10,303.38,358.66,232.12,7.13;10,303.38,368.13,85.23,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceiving spatial relationships in computer-generated images</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">C</forename>
				<surname>Wanger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Ferwerda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">P</forename>
				<surname>Greenberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="44" to="51" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,377.59,232.12,7.13;10,303.38,387.06,232.12,7.13;10,303.38,396.52,232.12,7.13;10,303.38,405.99,67.97,7.13"  xml:id="b35">
	<analytic>
		<title level="a" type="main">A comparison of the perceptual benefits of linear perspective and physically-based illumination for display of dense 3d streamtubes</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Weigle</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Banks</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1723" to="1730" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,415.45,232.12,7.13;10,303.38,424.92,232.12,7.13;10,303.38,434.38,54.01,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">Quantitative effectiveness measures for direct volume rendered images</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Qu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-K</forename>
				<surname>Chung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M.-Y</forename>
				<surname>Chan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PacificVis</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,443.85,232.12,7.13;10,303.38,453.31,232.12,7.13;10,303.38,462.77,127.40,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">Interactive display of isosurfaces with global illumination</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wyman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Parker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Shirley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="186" to="196" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,472.24,232.12,7.13;10,303.38,481.70,31.21,7.13"  xml:id="b38">
	<monogr>
		<title level="m" type="main">Attached and Cast Shadows</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Yonas</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>Praeger Publishers</publisher>
			<biblScope unit="page" from="100" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,491.17,232.12,7.13;10,303.38,500.63,107.45,7.13"  xml:id="b39">
	<analytic>
		<title level="a" type="main">Volumetric shadows using splatting</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Crawfis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
