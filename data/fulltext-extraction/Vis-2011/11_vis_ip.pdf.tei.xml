<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T14:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Saliency-Assisted Navigation of Very Large Landscape Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Yiu</forename>
								<surname>Cheuk</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Amitabh</forename>
								<surname>Ip</surname>
							</persName>
						</author>
						<author>
							<persName>
								<surname>Varshney</surname>
								<roleName>Fellow, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">Saliency-Assisted Navigation of Very Large Landscape Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Image Saliency</term>
					<term>Very Large Scale Images</term>
					<term>Scene Perception</term>
					<term>Interactive Visualization</term>
					<term>Anomaly Detection</term>
					<term>Guided Interaction</term>
				</keywords>
			</textClass>
			<abstract>
				<p>—The field of visualization has addressed navigation of very large datasets, usually meshes and volumes. Significantly less attention has been devoted to the issues surrounding navigation of very large images. In the last few years the explosive growth in the resolution of camera sensors and robotic image acquisition techniques has widened the gap between the display and image resolutions to three orders of magnitude or more. This paper presents the first steps towards navigation of very large images, particularly landscape images, from an interactive visualization perspective. The grand challenge in navigation of very large images is identifying regions of potential interest. In this paper we outline a three-step approach. In the first step we use multi-scale saliency to narrow down the potential areas of interest. In the second step we outline a method based on statistical signatures to further cull out regions of high conformity. In the final step we allow a user to interactively identify the exceptional regions of high interest that merit further attention. We show that our approach of progressive elicitation is fast and allows rapid identification of regions of interest. Unlike previous work in this area, our approach is scalable and computationally reasonable on very large images. We validate the results of our approach by comparing them to user-tagged regions of interest on several very large landscape images from the Internet.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We are seeing a significant growth in the interest and relevance of very large images. One of the reasons behind this trend is the development of systems that can automatically capture and stitch photographs to create images of unprecedented detail ranging from a few gigapixels <ref type="bibr" coords="1,70.84,343.73,9.71,8.02" target="#b7">[8,</ref><ref type="bibr" coords="1,83.21,343.73,11.21,8.02" target="#b26"> 27,</ref><ref type="bibr" coords="1,97.08,343.73,11.95,8.02" target="#b45"> 46] </ref>to even a few terapixels <ref type="bibr" coords="1,200.46,343.73,13.74,8.02" target="#b11">[12]</ref>. Recent advances in consumer-grade robotic image acquisition from companies such as Gigapan have further energized social network communities that are interested in building, sharing, and collectively exploring such large images. Some relevant work on processing of very large images includes a streaming multigrid solver for gigapixel scale out-of-core gradient-domain image processing by Kazhdan and Hoppe <ref type="bibr" coords="1,242.59,403.51,13.74,8.02" target="#b21">[22]</ref>. More recently, Summa et al. <ref type="bibr" coords="1,118.70,413.47,14.94,8.02" target="#b39">[40] </ref> present progressive processing of highresolution images with interactive previews. Kopf et al. <ref type="bibr" coords="1,238.24,423.43,14.94,8.02" target="#b26">[27] </ref>discuss how to naturally display the stitched image by cylindrical projections. Luan et al. <ref type="bibr" coords="1,74.97,443.36,14.94,8.02" target="#b31">[32] </ref>adaptively annotate these very large images by text and audio according to the viewing position and scale. While these are very interesting first steps in computational processing and display of very large images, this paper addresses a different challenge for such large images – their effective visual navigation. Consider a gigapixel image shown in <ref type="figure" coords="1,175.28,494.06,20.72,8.02">Fig. 1</ref>. The successive zooms give an indication of the level of detail in such images. When viewing such images, users typically pan at the coarse level and occasionally zoom in to see the fine details. Panning at the finest level of detail is too tedious and panning at the coarsest level of detail does not have enough information for the user to know where to zoom in. Just to convey the magnitude of the problem, let us consider some numbers. Imagine a user is visualizing a 4 Gigapixel image on a 2 Megapixel monitor. This would suggest that every monitor pixel is representing 2000 image pixels and the observable image on the monitor is a mere 0.05% of the total dataset. Further, if it takes a user just a couple of seconds to scan the monitor, it will take more than an hour to scan through the entire image. In this paper, we leverage principles of visualization to ease the task of navigating very large landscape images. In visual exploration of very large images the biggest challenge involves identifying the most salient content and visually presenting this information to the user. Just as the transfer function design in traditional visualization uses opacity to identify what data to show and color to emphasize, we present datadriven techniques to identify and emphasize potential areas of interest in very large images. Our techniques are relevant for visualization of datasets when the data size is several orders of magnitude larger than what the display device can accomodate. We use techniques based on visual knowledge discovery to help in user navigation and adaptive context-and scale-dependent visual overlays to assist in spatial localization of salient detail. </p><p> Challenges: The interactive visual exploration of very highresolution large-scale images presents three challenges: Visual Scalability: The visual scalability challenge arises from the inability of the human visual system to take in all the details that are present in a very large image. This arises from a fundamental limitation of the retina as well as the display hardware which have not kept pace with our ability to acquire ever larger images. <ref type="figure" coords="1,484.43,494.57,21.43,8.02" target="#fig_2">Fig. 2</ref>shows the growth in resolution of the mainstream consumer-grade LCD displays against camera sensors in recent years. The display resolutions correspond to the highest-resolution monitors sold by a mainstream vendor (such as Dell and Apple) and the camera-sensor sizes correspond to the highest-resolution entry-level SLR cameras manufactured by Canon, Nikon, or Sony. The resolution growth of these off-the-shelf cameras has clearly outpaced the resolution of the display monitors. Information Scalability: The challenge here is to design effective computational algorithms to identify nuggets of useful visual information that hide in large-scale images. In very large images, most of the image data is innocuous and unimportant and even considering it wastes precious time and resources. Often relatively small regions in such very large images are accorded a very high information value by human observers. Identifying informative regions in very large images that match human expectations is an ambitious challenge. Data Scalability: The sheer data size of these images poses a computational challenge. Processing such large images along with their auxiliary data structures often necessitate out-of-core methods as well as designing of algorithms that are cache-and memory-efficient. Even routine image-processing operations for very large images require a careful mapping to the many-core and multi-core processor architectures for any reasonable performance. <ref type="figure" coords="2,37.98,193.52,3.32,7.37">1</ref>. A very large image contains fine details. We progressively zoom into the blue, yellow, and red regions in the panorama. There are interesting regions at different scales: The overview panorama shows the landscape. The blue region shows the hotel and parking lot. The yellow region shows the cars. The red region shows a human. Note the red region is less than one pixel at the overview scale.  Contributions: In this paper we present the first steps towards addressing the above challenges for interactive visual exploration of very large images. We outline our main contributions next. 1. We extend classical computational image saliency to very large images. Users often navigate across three or more orders of magnitude scale differences – from the overview to the finest-scale views while viewing a very large image. Classical algorithms for multi-scale image saliency break down at handling such a large span of visual scales. We discuss the issues involved and present a solution in Section 4. </p><p>2. As discussed above in the information scalability challenge, it is important to identify the regions of interest in very large images that characterize areas of high information value to users. The question of how to effectively characterize visual information content is still far from settled. There are a number of measures of visual information content and often the definition depends on the task at hand. Our goal is not to provide a definitive characterization of the visual information content of a region of an image, which is a very deep question related to the issues of task semantics and knowledge. Instead we present here a fairly general information discovery algorithm in Section 5, that can serve as a framework for further research with other measures of information content. </p><p>3. Interactive visual exploration of very large images requires a careful balancing of computational analysis and user preferences . Too much reliance on automatic intelligence-extraction algorithms is currently not feasible since it is often very difficult to codify semantics of what a user is looking for. At the same time a purely interactive visual exploration without any computational assistance proves to be tedious and overwhelming due to the sheer scope of the data that is being visualized. We present an interactive visual exploration and information discovery system in Section 6. 4. Data scalability is an important issue when dealing with very large images and we present advances in this area in Section 7. 5. We present and compare our results with those from a social community of gigapixel image enthusiasts in Section 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p> Over the years a number of techniques have evolved to address limitations of the display medium with respect to the visual data. If the input data has more bits of color information than those that can be displayed , we use tone mapping to approximate the appearance of highdynamic range images <ref type="bibr" coords="2,370.76,241.16,13.74,8.02" target="#b9">[10]</ref>. This involves mapping the color space from a high-dimensional input to a low-dimensional output, while preserving most of the salient content. Similarly, video summarization techniques <ref type="bibr" coords="2,326.76,271.04,10.45,8.02" target="#b8">[9] </ref>typically involve extraction of the salient key-frames with some context that results in warping of the temporal space. Recent research has also looked at how to carry out image warping so that images that are larger than the display can be adaptively resized to preserve their most salient content <ref type="bibr" coords="2,428.39,310.89,9.71,8.02" target="#b0">[1,</ref><ref type="bibr" coords="2,441.83,310.89,10.65,8.02" target="#b36"> 37]</ref> . Most recently, Laffont et al. <ref type="bibr" coords="2,325.67,320.86,14.94,8.02" target="#b27">[28] </ref>present content-aware zooming on images up to 16 megapixels. While such image re-targeting techniques could be effective for images that are an order of magnitude larger than the displays, they do not scale well to three orders of magnitude or more that we desire . In this paper we present distortion-free navigation of very large images, assisted by identification of their most salient content, to allow viewing of very large images on displays of modest sizes. Unlike the previously discussed methods, visualization by navigation maintains the metric fidelity of the image. The pan and zoom interactions allow the users to follow the local image context naturally. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scene Analysis and Image Saliency</head><p> Image saliency has been used in modeling visual attention. Topdown and bottom-up models for building a saliency map have been introduced by Tsotsos et al. <ref type="bibr" coords="2,391.91,465.43,13.74,8.02" target="#b41">[42]</ref>, Itti et al. <ref type="bibr" coords="2,448.60,465.43,14.94,8.02" target="#b16">[17] </ref>and several others. Oliva et al. <ref type="bibr" coords="2,327.01,475.39,14.94,8.02" target="#b34">[35] </ref>apply the top-down model to object detection. Recent approaches include work by Hou and Zhang <ref type="bibr" coords="2,502.47,486.02,14.94,8.02" target="#b13">[14] </ref>that computes image saliency by the difference of the image's original and smoothed log-Fourier spectrum. Bruce et al. <ref type="bibr" coords="2,472.23,505.95,10.45,8.02" target="#b3">[4] </ref>learn a set of sparse code from example images to evaluate saliency of new images. Wang et al. <ref type="bibr" coords="2,329.49,525.87,14.94,8.02" target="#b43">[44] </ref>use random graph walk on image pixels to compute image saliency. Goferman et al. <ref type="bibr" coords="2,402.05,535.84,14.94,8.02" target="#b10">[11] </ref>consider visual organization and high level features such as human faces in saliency computation. In spite of the impressive advances in the recognition of specific objects , such as buildings, cars, and humans, general scene understanding remains a hard problem <ref type="bibr" coords="2,387.88,576.35,13.74,8.02" target="#b12">[13]</ref>. State-of-the-art systems <ref type="bibr" coords="2,497.39,576.35,9.71,8.02" target="#b6">[7,</ref><ref type="bibr" coords="2,509.72,576.35,11.21,8.02" target="#b30"> 31,</ref><ref type="bibr" coords="2,523.55,576.35,11.95,8.02" target="#b37"> 38] </ref>train on web-scale databases of small images and then extract regions of interests for test scenes in a supervised manner. More recently, Kim and Torralba <ref type="bibr" coords="2,332.12,606.24,14.94,8.02" target="#b22">[23] </ref> use alternating optimization on sets of unlabeled images to extract one to three regions of interest per image. Our system needs a more general approach since we expect a variety of regions and objects of interest in a very large image. Although there has been extensive previous work in identifying salient regions using several methods, such techniques typically extract a very small number of regions from a relatively small image. For instance, Goferman et al.'s <ref type="bibr" coords="2,402.12,676.65,14.94,8.02" target="#b10">[11] </ref>program requires 74 seconds to process a 250 × 142 image, and Bruce et al.'s <ref type="bibr" coords="2,459.33,686.61,10.45,8.02" target="#b3">[4] </ref>program requires 30 minutes to process a 3000 × 1500 image. These timings are on a current state-of-the-art workstation described in Section 8. Assuming their running time scales linearly and memory swapping does not become an issue, these approaches will take hundreds of hours to process gigapixel-sized images. The purpose of this is not to downplay Silding-Window Saliency Map Interactive Exploration Detect Anomalous Regions In the first step we build a saliency map by augmenting the traditional multi-scale image saliency approach with a sliding window over scales. In the second step we carry out information discovery by using color descriptors to identify the most unique regions. In the third step we facilitate rapid elimination of false positives through user interaction during visualization. the achievements of these approaches, which are actually quite impressive in what they are targeting, rather to highlight the difference in their approaches from ours. We believe user interaction in the visualization process can greatly assist in rapid culling of false positives and can greatly enhance the overall computational efficiency of the resulting algorithms. Our approach adopts a three-step process of progressive culling of potential regions of interest. We believe this provides a better balance of accuracy and computational efficiency with a user in the middle than a purely computational approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Data Analysis</head><p>There is a rich history of data analysis for visual summarization and identification of information-rich subspaces for effective visual presentation . We next present a few example of how salience is defined and used for visual datasets to enhance their depiction. Notable advances include defining saliency for polygonal meshes <ref type="bibr" coords="3,232.21,378.79,13.74,8.02" target="#b29">[30]</ref> , comparing it to human eye movements <ref type="bibr" coords="3,149.66,388.75,13.74,8.02" target="#b25">[26]</ref>, and illuminating meshes based on saliency <ref type="bibr" coords="3,74.83,398.72,13.74,8.02" target="#b28">[29]</ref>. Howlett et al. <ref type="bibr" coords="3,147.74,398.72,14.94,8.02" target="#b14">[15] </ref>use eye-tracking data to identify salient features on meshes and carry out user studies to validate their findings. Kim et al. present and validate saliency-based enhancement operators to guide visual attention in volume visualization <ref type="bibr" coords="3,250.68,428.60,14.94,8.02" target="#b23">[24] </ref>and geometric meshes <ref type="bibr" coords="3,98.24,438.57,13.74,8.02" target="#b24">[25]</ref>. Machiraju et al. <ref type="bibr" coords="3,100.65,448.79,14.94,8.02" target="#b32">[33] </ref> present a system to detect contextually significant multiscale features in very large datasets directly in the wavelet domain and visualize them progressively. Bordoloi and Shen <ref type="bibr" coords="3,258.12,468.71,10.45,8.02" target="#b2">[3] </ref>select informative views of volumetric data based on saliency defined using entropy measures. Viola et al. <ref type="bibr" coords="3,165.95,488.64,14.94,8.02" target="#b42">[43] </ref> determine the most expressive view for a selected region of interest in a volume using mutual information. Bruckner and Möller <ref type="bibr" coords="3,159.09,508.56,10.45,8.02" target="#b4">[5] </ref>use isosurface similarity maps based on mutual information to automatically select the most salient isosurfaces. Saliency-based summarization of time-varying datasets has been carried out for videos <ref type="bibr" coords="3,149.05,538.45,10.45,8.02" target="#b8">[9] </ref> and molecular dynamics simula- tions <ref type="bibr" coords="3,51.88,548.41,13.74,8.02" target="#b35">[36]</ref>. Wiebel et al. <ref type="bibr" coords="3,123.98,548.41,14.94,8.02" target="#b44">[45] </ref>identify salience with the vortices that originate from walls in three-dimensional time-dependent vector fields and track their evolution using generalized streak lines. Unlike much of the previous work on volumetric or time-varying data, the focus of this paper is on visualization of very large images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW OF OUR APPROACH</head><p> The goal of our paper is to carry out computationally-assisted navigation of very large images. We leverage the principles of visual saliency and statistical similarity to outline a three-step approach. <ref type="figure" coords="3,236.73,641.17,20.82,8.02" target="#fig_3">Fig. 3</ref>shows an overview of our approach. Our first goal is to identify regions of interesting detail in very large images. The challenges here are in dealing with the dramatic span of visual scales and the sheer amount of data as well as information. Our approach addresses each of them. Visual Scalability: Traditional algorithms for multi-scale image saliency work well for small images up to a few megapixels but do not scale up well to gigapixels and beyond. To address this we augment the traditional multi-scale image saliency approach with a sliding window over scales to effectively work with very large images. Our approach only requires Gaussian convolutions on images. It is highly parallelizable and scales linearly with the size of the image. The slidingwindow saliency map phase of our approach discovers thousands of locally salient regions from billions of pixels. Information Scalability: Typical landscape images comprise of a large number of natural elements such as clouds, rocks, grass, and trees. In this paper we assume that such repeating scene elements are not of interest to the viewers. We characterize all image regions using automatic color-structure descriptors. We then argue that the most interesting regions are the ones that are the most different from their k nearest neighbors (k-NN) in such color-structure feature space. We have empirically observed that this definition works well for landscape images. Other descriptors may be found to be more suitable for other datasets. We use a spatial index to accelerate the k-nearest-neighbor queries. This indexing and querying process grows as O(n log n), where n is the number of salient regions identified by the slidingwindow saliency step. For gigapixel images n is typically of the order of a few tens of thousands. We refer to this step as anomaly detection. Interactive Visual Exploration: We have developed an interactive visualization environment to assist in exploration of very large images. We facilitate users to be aware of details that are a fraction of the screen pixel by ensuring that the overlays for such regions are large enough to be visible at every scale. The users can then explore and inspect all such regions interactively. We also have an automatic mode of the system in which the users are led through a smooth camera fly-through over all the informative regions in the image in order of their uniqueness as determined by the Visual and Information Scalability stages of the algorithm. If a user comes across a region of the image that the system claims is important, but the user finds to be unimportant, the user can identify all similar regions (using a slider) and discard them interactively. The spatial index structure built to address the Information Scalability stage of the algorithm allows this operation to occur within a few milliseconds. Data Scalability: The size of the very large images together with their auxiliary data-structures imposes a significant computational and storage burden. We have used a number of approaches to ameliorate this problem including out-of-core computation, efficient storage of salient regions, and building the image viewer using a clipmap-based tiled approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SLIDING-WINDOW SALIENCY</head><p> A very high resolution image is particularly interesting because it exceeds what the human eye can see at a given spot. In a very large image we can zoom from seeing the big picture to scrutinizing the finest details. <ref type="figure" coords="3,349.28,676.62,31.40,8.02">Figure 4</ref>shows three different views of an image at three different scales. We note that the cars seen in <ref type="figure" coords="3,477.69,686.58,29.21,8.02">Figure 4</ref>(c) are not discernible in <ref type="figure" coords="3,344.92,696.54,28.99,8.02">Figure 4</ref>(a). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Traditional Image Saliency</head><p>A saliency map shows which part of an image is likely to attract the most attention of the low-level human visual system. Itti et al. <ref type="bibr" coords="3,529.56,736.42,14.94,8.02" target="#b16">[17] </ref>(a) </p><formula>(b) (c) </formula><p>Fig. 4. We zoom into a very large image (a) to see the lake and the nearby terrain, (b) to discover the hotel and parking lot (b) and then go still further (c) to see the cars in the parking lot.  have proposed a computational model of visual saliency by using multiscale image processing. Multiscale image processing techniques analyze an image at different scales to simulate the retinal receptive fields. Their image saliency model aggregates the results from three features of an image – intensity, color opponencies, and orientation. We have found that the use of the orientation features decreases the quality of our results as it ends up enhancing naturally occurring structures with strong edges such as cracks in rocks or trees, that end up becoming too salient. In this paper we only consider the intensity and the two color-opponency attributes for computing image saliency. The intensity (F I ) is the average of primary colors, red, green, and blue. The color opponency attribute contains two sub channels, Red-Green(F R ) and Blue-Yellow(F B ). The details on computation of these attributes can be found in <ref type="bibr" coords="4,80.27,448.38,13.74,8.02" target="#b16">[17]</ref>. We next briefly review the traditional algorithm for computing the saliency map S of an image. </p><formula>-= | | G(σ) G(4σ) DoG </formula><formula>F i ← Image, i ∈ {I, R, B} G i, j = G ( j) ⊗ F i , j ∈ σ , 2σ , 4σ , 8σ , 16σ · · · D i, j,k = |G i, j − G i,k |, k ∈ {4 j, 8 j} N I = ∑ k ∑ j N (D I, j,k ) N C = ∑ k ∑ j [N (D R, j,k ) + N (D B, j,k )] S = 1 2 [N (N I ) + N (N c )] (1) </formula><p>We first extract the intensity feature, F I , and color features F R , F B from an image. Then, we convolve the feature images F i with Gaussian kernels, G , at different scales j. We find contrasting regions by computing the difference of Gaussians (DoG) images at each scale, G i, j . We compute the DoG images at scales {σ , 4σ }, {σ , 8σ }. The DoG operation mimics the contrast detecting receptive fields of retinal ganglion cells. <ref type="figure" coords="4,66.35,666.62,21.36,8.02" target="#fig_4">Fig. 5</ref>shows the DoG operation extracts contrasting cars from the background. N is a normalization function that promotes the peak salient regions <ref type="bibr" coords="4,109.39,686.55,13.74,8.02" target="#b15">[16]</ref>. The saliency map, S , is an aggregation of the normalized DoG images. We use σ = 2.0 in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sliding Window Aggregation</head><p>To see the difficulties introduced by the traditional saliency method, consider the saliency map for the parking lot image in <ref type="figure" coords="4,229.09,736.42,22.03,8.02">Fig. 4</ref><ref type="bibr" coords="4,360.30,372.11,14.94,8.02" target="#b16">[17] </ref>method. If we aggregate the saliency at all the levels of detail, we obtain the salient region in <ref type="figure" coords="4,471.06,382.07,20.30,8.02" target="#fig_6">Fig. 6</ref> (a). We observe that mostly the center of the parking lot has been included, while most of the surrounding cars have been excluded. It is interesting to note that if we analyze the saliency maps at each scale we find that cars are salient at fine scales σ , 2σ , 4σ and the parking lot is salient at scales 16σ , 32σ , 64σ . This can be seen in Figures 6(b)–(d). Therefore even though the saliency maps at individual scales were able to correctly identify the constituent salient elements, the overall aggregation ended up suppressing a number of them. The reason behind this is that if the salient regions at two different scales overlap, this overlap tends to disproportionately promote the overall salience of that region. Such events are infrequent or otherwise are not of great concern when the image sizes are relatively small. However, this no longer holds true in very large images. As shown in <ref type="figure" coords="4,404.39,511.59,20.30,8.02">Fig. 4</ref>, an observer would recognize the parking lot and the cars quite independently of each other at different scales. Therefore we believe that it is inappropriate to aggregate the saliency of the cars and the parking lot together since they are detected by DoG filters that are 16 scales of difference apart. The key observation here is that while simulating the multiscale capabilities of the human visual system, we have to be aware that our eyes have a finite resolution. We should limit the number of scales in multiscale image processing based on the limits of the human visual system. </p><p>To address the above, we introduce a sliding-window approach to build saliency maps at multiple scales with limited aggregation. In this approach by limiting the scales of saliency aggregation we produce multiple maps that simulate the zooming operation. This allows views of drastically different scales to be analyzed virtually independently of each other. For example, we can extract cars from the aggregation of normalized maps at scales {σ , 2σ , 4σ }. <ref type="figure" coords="4,440.89,666.68,31.49,8.02" target="#fig_6">Fig. 6(b)</ref> shows the resulting saliency map. It highlights most of the cars without highlighting the parking lot. We separate salient regions at widely different scales by limiting the aggregation procedure. We limit the aggregation of normalized maps from scale j to scale j + δ . We modify the aggregation procedure in equation (1) to equation(2). This sliding window approach ensures overlapping regions from drastically different scales do not interfere with one another. </p><formula>N I, j = ∑ k j+δ ∑ j N (D I, j,k ) N C, j = ∑ k j+δ ∑ j [N (D R, j,k ) + N (D B, j,k )] S j = 1 2 [N (N I, j ) + N (N C, j )] </formula><formula>(2) </formula><p>We seek a scale difference δ that truly reflects the human visual system. We use δ = 4 j in this paper. We next use arguments from the human visual system theory to suggest why this may be appropriate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Visual System Considerations</head><p>: We would like to use the scales in multiscale image processing based on the sensitivity difference between foveal and peripheral vision. Perceptual studies have shown that the fovea is the most sensitive region of the retina and the sensitivity drops as the view angle increases. Let us assume that we are viewing an image on a 30-inch monitor at 1 meter. In this case, the view angle subtended from the edge of the monitor to the center of the screen is about 20 @BULLET . At 20 @BULLET , our retina retains approximately 1/5 th of the foveal resolution. Therefore, we have decided to compute the saliency maps with images within the 4 scales (σ − 4σ ). <ref type="figure" coords="5,41.46,302.96,21.36,8.02">Fig. 7</ref> shows the saliency map resulting from our sliding-windowscale approach for an example image. The computational saliency model detects 18 thousand salient regions. The computational saliency model pre-processes the data efficiently and reduces our quest for interesting and meaningful detail from billions of pixels to thousands of regions. Yet this is still too many regions for a user to manually inspect. We next discuss a more discriminating anomaly detection procedure to refine this initial pool of candidate regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INFORMATION DISCOVERY</head><p>It is difficult to quantify the visual information content of a region. Some very interesting advances have been made in this field recently. Jänicke et al. <ref type="bibr" coords="5,86.36,427.68,14.19,8.02" target="#b18">[19,</ref><ref type="bibr" coords="5,104.68,427.68,11.21,8.02" target="#b19"> 20,</ref><ref type="bibr" coords="5,120.01,427.68,11.95,8.02" target="#b20"> 21] </ref> have extended the idea of local statistical complexity to measure the local information content of a region . This provides an application-independent, purely mathematical measurement of information. More recently, a very interesting and expansive treatment of how saliency and information theory can be adapted and adopted for visualization has been carried out by Chen and Jänicke <ref type="bibr" coords="5,77.85,487.46,9.71,8.02" target="#b5">[6,</ref><ref type="bibr" coords="5,90.86,487.46,10.65,8.02" target="#b17"> 18]</ref>. In this paper we wish to slightly side-step the deeply intriguing topic of how to quantify visual information content of a region in the general case, and instead talk about what we have found to work well for large-scale landscape images. We hope that further advances in the field of visual information quantification will seamlessly replace the method that we outline next to work with a wide variety of very large image databases beyond just landscape images. In this paper we have decided to adopt the approach that the most important regions of an image are those that are the most different from every other region. In other words, we are interested in identifying the outliers, or the anomalies, in a very large image. As seen in <ref type="figure" coords="5,245.67,587.65,19.72,8.02">Fig. 7</ref>, the sliding-window saliency aggregation step identifies a large number of salient regions. They range from patches of grass on the ground, to cracks between the rocks in the mountains. <ref type="figure" coords="5,31.50,716.19,19.68,7.37">Fig. 7</ref>. The saliency map of our example image. The sliding-window saliency map detects 18k regions. (The salient regions are enlarged for visibility) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Image Region Descriptors</head><p>To be able to quantify differences amongst different image regions we need to first identify what we mean by an image region and then we need to settle on the space in which such differences will be measured. To identify an image region, we fit oriented ellipses to the salient regions that were detected in the previous section. We then fit bounding boxes to the ellipses, pad them by a few extra pixels (we have used 20 pixels for all the examples in this paper) to ensure that the padded bounding boxes fully enclose the salient regions. These bounding boxes then represent the image regions of interest. To achieve rotational invariance, we use histograms of the colorspace of the pixels belonging to the region of interest. We have tested a number of color spaces – RGB, HSV, CIELab and found that neither of them were very discriminative. We also experimented with shape and orientation descriptors and found that they were excessively discriminative . This search led us towards a descriptor that would represent both color as well as statistical structural information of an image region and would have a discriminating ability that would lie between the two extremes (color-based and edge-based descriptors). The MPEG-7 color-structure image descriptor represents both color and structural information. The MPEG-7 is an ISO standard for describing multimedia content data and facilitates information retrieval. It consists of generic descriptors that cover many basic visual features , such as color, texture, and shape. The MPEG-7 color-structure descriptor embeds color structure information into the descriptor by counting color frequencies in a moving window of 8 × 8 pixels. Color values are represented in the double-coned HMMD color space, which is quantized non-uniformly into 64 bins. The range of histogram is normalized to 0 − 255. The resulting descriptor is a 64-dimensional vector. We follow the recommendation of MPEG-7 standard and compare them using the Euclidean L 2 norm distance. </p><formula>D(p, q) = p − q 2 (3) </formula><p>p and q are the descriptors of two image patches and D(p, q) is the distance in between the descriptors. We compute the MPEG-7 colorstructure descriptor for each image patch using the software provided by the BilVideo-7 <ref type="bibr" coords="5,374.57,432.26,10.45,8.02" target="#b1">[2] </ref>video indexing and retrieval system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">k-Nearest-Neighbors Anomaly Detection</head><p> We compute the uniqueness of each region by considering its knearest-neighbors (k-NN). For each image region, we search for its k-nearest-neighbor regions: </p><formula>U(p) = k ∑ i=1 D(p, q i ) k where p, q i ∈ P, p = q i D(p, q 1 ) ≤ D(p, q 2 ) ≤ . . . ≤ D(p, q k ) ≤ D(p, q k+1 ) . . . </formula><formula>(4) </formula><p>P is a set of image patch descriptors. The uniqueness of image patch p, U(p), is its average distance to its k-nearest neighbors, q 1 . . . q k . Repetitive regions with many close neighbors have a low average distance . Regions such as humans, signs, or vehicles should be distinct from the other regions and have a high average distance. We identify the unique regions of interest by their high average distances. We select the top 3% of salient regions as the regions of interest in our experiments. Approximate nearest-neighbor data structures accelerate the knearest-neighbor search <ref type="bibr" coords="5,388.57,646.76,13.74,8.02" target="#b38">[39]</ref> . The biggest overhead in the knearest-neighbors anomaly detection is the need to retrieve k-nearestneighbors for each region. Linear search of the k-nearest-neighbor queries is computationally expensive. Research in computational geometry provides many spatial data structures to facilitate this nearest neighbor querying. The approximated nearest-neighbors index significantly accelerates this search process. The sum of distances to the top k-approximated nearest-neighbors provides a reliable uniqueness estimate of each region. Our implementation uses a randomized KD-Tree index in the flann library <ref type="bibr" coords="5,393.85,736.42,14.94,8.02" target="#b33">[34] </ref>through the OpenCV library. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">INTERACTIVE VISUALIZATION</head><p>We guide users to explore the image through the detected regions and interactive visualization. We visualize the detected regions, provide automatic fly-through, and allow interactive user refinement. We highlight three features to assist large image exploration. @BULLET Adaptive scaling ensures the regions of interest are visible. @BULLET Automatic exploration guides the user to discover the unique regions of the image. @BULLET User interaction refines the computed detections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Visualizing the Detected Regions</head><p>We want the users to see the detected regions from the macro view of the image and also allow them to zoom-in to inspect. We believe maintaining the zooming procedure gives the users a much more natural context. Small regions are invisible at the macro view, therefore the corresponding overlay regions are also too small to be seen. We need to visualize these regions more effectively. We adaptively scale the overlay tags on the detected regions to ensure the most unique regions are visible. We compute the scale, λ , as follows: </p><formula>λ (r) = s m s r 1 − rank(r) #regions if s r &lt; s m 1 otherwise (5) </formula><p>λ (r) aims to enlarge the overlay tag for the region r according to the viewing scale and the uniqueness of region r. s r is region r's size in pixels on the screen. s m is a user-defined target screen size for the overlay tag. Rank(r) is the relative order of the uniqueness of region r as determined by equation 4 and # regions is the number of regions identified at the end of the anomaly detection phase. Thus, the greater the uniqueness of a region r, the greater the λ (r) would be. As we zoom into an image, s r increases and λ (r) gradually decreases, and the overlay tags will be shown in their actual size once s r ≥ s m . We color the overlay tags from cyan to magenta (the most unique) according to their rank of uniqueness (equation 4). <ref type="figure" coords="6,32.46,696.57,20.49,8.02" target="#fig_8">Fig. 8</ref>shows the example image and the overlay tags on the detected regions. We see very few detected regions in <ref type="figure" coords="6,182.10,706.53,19.69,8.02" target="#fig_8">Fig. 8</ref>(a) because they are too small to be seen. We scale the detected regions in <ref type="figure" coords="6,210.66,716.50,19.67,8.02" target="#fig_8">Fig. 8</ref>(b) by λ (r). <ref type="figure" coords="6,22.50,726.46,33.00,8.02" target="#fig_8">Fig. 8 (b)</ref>shows two groups of regions on the left and on the right. The center region corresponds to a single salient car on the road. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Automatic Exploration</head><p>Our system guides the users to fly through the detected regions. This helps the users to start exploring the image when they know little about it. It smoothly pans and zooms into the detected regions according to their uniqueness. We achieve this by sorting the regions in descending order of their uniqueness (equation 4). This forms a natural exploration sequence that starts from the most unique regions. If users come across misidentified regions during the fly-through, they can suppress the regions by interactive refinement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Interactive Refinement</head><p>Automated systems recognize a lot of regions of interest but they may also make mistakes. Our system allows the users to refine the results by selecting misidentified regions and deleting them. This mechanism allows the users to refine the automatic result but it can be tedious when users need to delete multiple regions. We provide interactions in our system to batch delete misidentified similar regions. The user may delete a batch of misidentified regions that are similar in a single interaction. <ref type="figure" coords="6,390.00,238.93,22.10,8.02">Fig. 9</ref>illustrates this mechanism. This select-slide-delete mechanism can remove many misidentified similar regions at once. In the first step the user identifies a set of regions that in the opinion of the user have been misidentified by the system. Amongst these regions, the user selects one representative region in the second step. This is highlighted by the system. In the third step the user adjusts a slider control to change the similarity distance from the one misidentified representative region to others that are like it. The system interactively highlights other regions that it detects to be similar to the misidentified region. Once the highlighted regions match the user's intent, they can be deleted all at once. The spatial index and color-structure descriptors of regions provide the interactive search capability. The anomaly detection spatial index is used in this step to provide fast similarity queries for regions. The system expands the region selection by querying the spatial index for regions that are similar to the user selection. The slider thresholds the number of regions retrieved. The spatial index performs a fast nearestneighbor query to retrieve similar regions. The system includes these regions in the selection and performs these operations at an interactive speed. <ref type="figure" coords="6,313.49,428.48,26.40,8.02" target="#fig_10">Fig. 10</ref>shows our results after a few user interactions. The number of detected regions reduces from 500 to about 300 with just three such interactions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DATA SCALABILITY</head><p> Processing many gigabytes of image data requires out-of-core algorithms and techniques. We pay special attention to memory constraints when we implement our saliency computation, storage of results, and our interactive viewer. <ref type="figure" coords="6,285.12,716.12,18.41,7.37">Fig. 9</ref>. Users refine the results by deleting misidentified regions in batch. 1. Locate similar misidentified regions. 2. Select one representative. 3. Adjust the slider to cover the desired regions. 4. Delete the selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Out-of-core GPU Saliency Computation</head><p>We use GPUs to accelerate image saliency computation. The required operations for Difference of Gaussians (DoG): image filtering, addition , subtraction, and resizing are highly parallelizable and suitable for GPU implementation. We compute a Gaussian pyramid to approximate the Gaussian blurred images at different scales. We repeatedly downsize the image by a scale of two and convolve it with a fixed Gaussian kernel of scale σ . We store the xσ scale Gaussian image at level log 2 (x) of the pyramid. The DoG images can be computed by simply finding the difference of Gaussians images between two levels. The non-linear normalization function N iteratively applies the DoG operation <ref type="bibr" coords="7,266.94,284.48,14.94,8.02" target="#b15">[16] </ref>to promote the peak regions. We compute each of these normalized maps once and use them to compose sliding-window saliency maps . A significant problem in processing very large images (which currently are a few gigapixels) is that the entire image will not fit in the GPU or main memory. To address this, we divide each image into small tiles (256 × 256). We load the image tiles into the GPU independently for addition, subtraction, and resizing operations. Gaussian filtering requires information on image boundaries. Filtering each tile without overlap results in loss of information at the tile boundaries. To address this we load these tiles into the GPU with overlaps for filtering. For every sub-image that is to be filtered we load two extra rows of tiles (top and bottom) and two extra columns of tiles (left and right) that surround the sub-image. We fill the GPU memory with the largest possible sub-image (with additional surrounding overlapping tiles) to ensure efficient processing and minimize re-filtering of overlapping tiles. Depending on the loading order, either one row or one column will be used for filtering the next consecutive set of tiles. The ability to independently filter these tiles allows parallelization. In our implementation, we use the NVIDIA performance primitives for GPU Gaussian filtering, resizing, addition, and subtraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Salient Regions Storage</head><p>We store ellipses to approximate salient regions to ease storage. The sliding-windows saliency maps incur a storage burden. Similar to storing mipmaps, it takes 1 1 3 times the image size to store the saliency maps at all levels. Although it is a constant factor increase, doubling the storage of the already very large images poses a challenge. To address this, we first threshold the sliding-window saliency maps and locate continuous regions, also sometimes referred to as blobs. The open-source library cvblob provides blob detection in our implementation. We carry out local Principal Component Analysis (PCA) to fit ellipses to these regions. This allows us to reduce the storage of each region from hundreds of pixels to a few parameters (center, orientation, and principal axes intercepts of each ellipse). The threshold of our experiments is 0.25. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Tiled Image Viewer</head><p>Very large images presented in this paper do not fit in the GPU or main memory for display. Each gigapixel in RGB format takes three gigabytes of memory in an uncompressed format for rendering. Images with even a few gigapixels exceed the GPU or main memory of a workstation . We have implemented an out-of-core tiled-image viewer. Our viewer fetches only what can be seen at an appropriate scale from the disk. This is similar in spirit to the concept of a clipmap <ref type="bibr" coords="7,232.29,736.42,13.74,8.02" target="#b40">[41]</ref>. To carry this out we build a mipmap pyramid of the image. We divide the images on each level into tiles of 256 × 256. We load the required image tiles according to the viewing parameters. We pre-fetch two extra rows and columns of image tiles surrounding the viewing region to provide smooth panning. Loading images from the immediately nearby scales prepares for the zooming operation. We store these images in a texture array. This array is independent of the display arrangement of the textures. We map each texture to an array location by a hash function. The loaded texture can be reused on different views without any memory movement. The memory usage of the viewer is independent of the size of the image. It is related to the size of the viewing window on the screen only. Our viewer needs 350 megabytes for viewing a five gigapixel image with a few hundred overlay regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RESULTS</head><p> We evaluate our approach on four multi-gigapixel images. We report the regions identified by our system and compare them against web community tags. We also report timings of our experiments. We perform our experiments on the Linux platform with one Intel E5420 CPU, 4GB RAM, and one NVIDIA GeForce GTX 295 GPU (895MB). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Datasets</head><p>Digital image stitching and consumer grade robotics have made panoramic photography very popular. Compact digital cameras can stitch multiple consecutive pictures into a panorama. Robotic devices such as the Gigapan EPIC can take hundreds of pictures automatically for image stitching. These products allow consumers to create images of several gigapixels. Community panorama websites such as Gigapan and HDView have gained much popularity on the Internet. Users around the world upload their panoramic pictures to these websites . The web community of panoramic photography enthusiasts then explore, tag, and comment on interesting regions in these images. We downloaded four gigapixel images from the Gigapan website as shown in <ref type="figure" coords="7,303.34,389.58,25.41,8.02">Fig. 11</ref>and discussed below. Grimsel Pass: The Grimsel Pass is a high mountain pass in Switzerland . It connects the valley of Rhone River in the canton of Valais and the Haslital in the canton of Bern. The picture shows an overview of the mountain area, the lake, and the road network. There are distinctive areas with building constructions, hotels, and numerous cars on the road. Royal Gorge Bridge: The Royal Gorge Bridge is the highest suspension bridge in the world. This tourist attraction is located in a theme park near Canon City, Colorado, shown in the top right of the picture. The Royal Gorge Route Railroad is a heritage railroad offering scenic and historical train-rides, shown at the bottom left of the picture. The picture shows the valley of Arkansas River with the Royal Gorge Bridge and a small town on the right. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Evaluation</head><p>We show a sample of the detected regions in <ref type="figure" coords="7,455.72,636.39,23.93,8.02">Fig. 11</ref> . We tag the locations with numbers and show the detected region in the corresponding thumbnails. We overlay the ellipses onto detected regions. Our system identifies a variety of regions at multiple scales. It locates humans, vehicles , buildings and even special features of the landscape such as a glacier. This shows the generality of our approach. In the Grimsel Pass picture (<ref type="figure" coords="7,409.99,696.57,23.54,8.02">Fig. 11</ref> (a)), the system detects buildings in thumbnails 1 and 8. In the lower left of the Royal Gorge Bridge picture (<ref type="figure" coords="8,481.84,202.22,22.94,8.02">Fig. 11</ref>(b)), the system finds a train and a river rafting boat along the Arkansas river in thumbnails 1 and 2. We see two cable cars in thumbnails 3 and 4; the one in 3 has just departed the station whereas the one in 4 is much closer to the camera. Thumbnails 5 and 6 show a caravan and a restaurant sign around the town. Thumbnail 7 shows tourists and flags. We found a few hikers among the cacti in <ref type="figure" coords="8,452.82,262.68,24.23,8.02">Fig. 11</ref>(c). Thumbnail 1 shows the back of a hiker; the system has identified him by his jeans. Thumbnail 2 shows a double image of two hikers, who probably moved and were captured at two instances. The hiker in thumbnail 3 was using a camera. We find a man sitting in thumbnail 4. Two other hikers are detected in thumbnail 5. Although Mt. Whitney (<ref type="figure" coords="8,388.27,323.15,24.49,8.02">Fig. 11</ref>(d)) is a popular hiking spot, we detect more than the hikers. Thumbnail 1 and 4 show a bridge and the Alpine lake. We found 4 hikers in thumbnails 2 and 5. A hiking backpack is shown in thumbnail 3. We are able to detect 12 out of 13 hikers in this picture. <ref type="figure" coords="8,295.08,373.64,26.99,8.02" target="#tab_1">Table 1</ref>shows a summary of the detected number of regions after each step. We inspect the results and count the number of detected meaningful objects, such as humans, vehicles, and buildings, after each processing step. These are shown in parentheses. Many detected regions may map to different parts of the same meaningful object. The number of detected objects remains the same until the userguided interactive refinement step. This shows our system is conservative and generally does not remove positive results. Our system finds a few hundred regions from images of a few gigapixels. <ref type="figure" coords="8,376.52,471.09,18.49,8.42">Tags</ref>: Internet users tag and comment on these very large images on Gigapan's website. <ref type="figure" coords="8,453.73,481.45,26.42,8.02" target="#tab_2">Table 2</ref> shows our system is able to detect a majority of the gigapan community tags. <ref type="figure" coords="8,510.41,491.41,25.09,8.02" target="#fig_2">Fig. 12</ref>compares the tags with our user-refined results. These tags contain high-level semantic information. For example, the pattern we detected in thumbnail 1 of <ref type="figure" coords="8,353.78,521.30,25.11,8.02">Fig. 11</ref>(a) is a child's drawing for a construction accident prevention campaign. Although a large number of tags represent regions of interest, such as vehicles or humans, not all interesting regions are tagged. <ref type="figure" coords="8,358.67,551.18,25.99,8.02" target="#fig_3">Fig. 13</ref>shows two examples of tags that contain semantics beyond general visual information. In <ref type="figure" coords="8,468.21,561.15,26.36,8.02" target="#fig_3">Fig. 13</ref>(a), a user has tagged a common cactus as being the original one. Another user has tagged some buffelgrass in <ref type="figure" coords="8,397.81,581.07,25.47,8.02" target="#fig_3">Fig. 13</ref>(b) because it grew after a fire. Clearly such tags require high-level semantic knowledge that a purely low-level perception-based system such as ours is unable to detect. We show the number of tags without semantic information in the second column of <ref type="figure" coords="8,323.97,620.92,25.30,8.02" target="#tab_2">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gigapan Community </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Performance</head><p>We report timings for different stages of our system. <ref type="figure" coords="8,483.23,656.04,27.13,8.02" target="#tab_3">Table 3</ref>shows the pre-processing times for the sliding-windows saliency computation , k-nearest neighbors anomaly detection, and user-guided interactive refinement. The running time for the sliding-window saliency computation averages to 2.5 hours per gigapixel. Since the saliency computation routines are parallelizable, we expect a cluster of machines can easily process each gigapixel within a few minutes. k-nearest neighbor anomaly detection takes less than a minute. This is considerably faster than the <ref type="figure" coords="9,31.50,115.85,24.16,7.37" target="#tab_2">Table 2</ref>. This table compares our results with Gigapan community tags. Our system detects most of the Giganpan community tags. The second column shows the count of tags without semantics as discussed in Section 8.2. There is a difference between the count of tags and detected objects. Each tag may cover multiple objects and not all meaningful objects are tagged. Some of the tags also overlap with one another. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Preprocessing Interaction </head><p>SW Sal. k-NN w/o k-NN w/ k-NN <ref type="figure" coords="9,31.50,248.92,24.65,7.37" target="#tab_3">Table 3</ref> . This table shows the running time of our system. The preprocessing time includes the sliding-window saliency (SW Sal. column) and anomaly detection (k-NN columns). The interaction column show timings for interactive user refinements. This select-slide-delete refinement cannot be interactive without the k-nearest neighbors (k-NN) anomaly detection step. recent image saliency techniques in Section 2.1. This pre-processing can be computed offline. The k-nearest neighbor anomaly detection reduces thousands of salient regions to a few hundreds while it also enables the interactive refinement process. The interactive select-slide-delete refinement process (searching, and redrawing) takes only tens of milliseconds. In contrast, when we try to interactively refine thousands of salient regions , each interaction takes several seconds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS AND DISCUSSION</head><p> In this paper, we show how to use visualization and computation to assist the exploration of very high-resolution large-scale landscape images . We address the visual scale challenge by introducing the slidingwindow computational saliency model. Anomaly detection automatically discovers information while visualization allows the users to explore the image interactively. Our system implementation is scalable to large datasets by using out-of-core methods. We show our system can detect interesting details from various landscape images. The detections largely match community user tags. In this paper we have focused on large-scale landscape images that have been acquired by stitching together of a large number of photographs . However, very large scale images are finding use in a number of different areas. For example, semiconductor wafer manufacturers are using terapixel images for quality inspection of their chips for detecting circuit anomalies. As another example, latest generation microscopes stitch together volumes of very high resolution, multislice imagery that depicts the entire life-cycle of a number of parasites . As yet another example, astronomers are exploring the farthest reaches of the universe through the use of terapixel imagery. Our system is currently targeted for analyzing landscape images using color and appearance. The key to analyzing other domain-specific images is to design appropriate descriptors that characterize similarity across regions of interests. The descriptors should allow fast discovery of locally distinct regions as well as accurate identification of the globally unique regions. We believe the field of visualization will be considerably strengthened by incorporating analysis and visualization of very large images as a first-class data primitive next to volumes and meshes. This paper presents some of the first steps towards that goal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>We thank Derek Juba for the discussions and constructive feedback that led to substantial improvements of the ideas in this paper. We also thank the anonymous reviewers for their tremendously constructive comments and suggestions that have greatly improved the presentation of this work. This work has been supported in part by the NSF grants: CCF 05-41120, CMMI 08-35572, CNS 09-59979 and the NVIDIA CUDA Center of Excellence. Any opinions, findings, conclusions, or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of the research sponsors. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,22.50,193.52,250.38,7.37;2,22.50,202.98,250.38,7.37;2,22.50,212.45,250.38,7.37;2,22.50,221.91,250.38,7.37;2,22.50,231.38,250.38,7.37;2,22.50,240.84,176.59,7.37"><head>Fig. </head><figDesc>Fig. 1. A very large image contains fine details. We progressively zoom into the blue, yellow, and red regions in the panorama. There are interesting regions at different scales: The overview panorama shows the landscape. The blue region shows the hotel and parking lot. The yellow region shows the cars. The red region shows a human. Note the red region is less than one pixel at the overview scale. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="2,22.50,420.12,240.98,7.37"><head>Fig. 2. </head><figDesc>Fig. 2. The growth of camera sensor resolution vs display resolution. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,31.50,157.19,513.00,7.37;3,31.50,166.66,513.00,7.37;3,31.50,176.12,513.00,7.37;3,31.50,185.59,126.72,7.37"><head>Fig. 3. </head><figDesc>Fig. 3. Our system discovers regions of interests in very large images and assists user exploration. In the first step we build a saliency map by augmenting the traditional multi-scale image saliency approach with a sliding window over scales. In the second step we carry out information discovery by using color descriptors to identify the most unique regions. In the third step we facilitate rapid elimination of false positives through user interaction during visualization. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,22.50,264.01,250.38,7.37;4,22.50,273.48,250.38,7.37;4,22.50,282.94,250.38,7.37;4,22.50,292.41,78.36,7.37"><head>Fig. 5. </head><figDesc> Fig. 5. Computational saliency mimics the contrast detection mechanism in the human retina. This image shows how Difference of Gaussians (DoG) operator detects the high contrast cars instead of the low contrast parking grids. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="4,285.12,284.70,250.38,7.37;4,285.12,294.17,250.38,7.37;4,285.12,303.63,250.38,7.37;4,285.12,313.10,250.38,7.40;4,285.12,322.56,250.38,7.40;4,285.12,332.03,162.05,7.40"><head>Fig. 6. </head><figDesc>Fig. 6. This shows a comparison of the thresholded Itti et al.'s [17] method in (a) and our sliding window saliency maps in (b)-(d). (a) The center cars and the parking lot are salient while the surrounding cars are suppressed (σ to 256σ ). (b) All cars are salient (σ to 4σ ). (c) The lower part of the parking lot becomes salient with a few cars (4σ to 16σ ). (d) Only the parking lot is salient (16σ to 64σ ). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,22.50,223.06,250.38,7.37;6,22.50,232.52,250.38,7.37;6,22.50,241.99,244.94,7.37"><head>Fig. 8. </head><figDesc>Fig. 8. We visualize the detected regions with adaptive scaling. (a) Small detected regions are too small to be seen in the macro view. (b) Adaptive scaling ensures the unique regions are enlarged and visible. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="6,32.46,270.95,240.42,8.02;6,22.50,280.91,250.38,8.02;6,22.50,290.87,160.37,8.02"><head>Fig. 8</head><figDesc>Fig. 8(b) shows an image with detected regions overlaid after anomaly detection phase. This process reduces the 18 thousand salient regions to just about 500 anomalous regions. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="7,31.50,122.53,250.38,7.37;7,31.50,131.99,250.38,7.37;7,31.50,141.46,145.97,7.37"><head>Fig. 10. </head><figDesc> Fig. 10. A few user-refinement interactions remove most of the misidentified regions. The number of regions reduces from 500 to about 300 with three select-slide-delete interactions. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="7,294.12,546.99,250.38,8.32;7,294.12,557.26,120.28,8.02;7,294.12,573.00,250.38,8.32;7,294.12,583.27,250.38,8.02;7,294.12,593.23,250.38,8.02;7,294.12,603.19,55.81,8.02"><head></head><figDesc>Cacti: This picture shows a cactus field in Arizona. A few hikers are hidden among thousands of cacti. Main Mt. Whitney Trail: This is a trail in the Sequoia National Park, California. This image of mountains and lakes includes many hikers. They all took the challenge to hike the highest peak (14,497') in the lower 48 states. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="9,294.12,572.80,250.38,7.37;9,294.12,582.26,250.38,7.37;9,294.12,591.73,133.22,7.37"><head>Fig. 12. </head><figDesc> Fig. 12. This figure shows the Gigapan community tags and our detected regions. The rectangles are the Gigapan tags and the ellipses are our user-refined detected regions. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="9,294.12,705.21,250.38,7.37;9,294.12,714.67,250.38,7.37;9,294.12,724.14,44.11,7.37"><head>Fig. 13. </head><figDesc>Fig. 13. Gigapan community tags: (a) The original cactus. (b) Some buffelgrass after a fire. These tags contain semantics beyond general appearance. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false" coords="7,294.12,706.53,250.38,37.90"><figDesc coords="7,397.58,706.53,146.91,8.02;7,294.12,716.50,250.38,8.02;7,294.12,726.46,250.38,8.02;7,294.12,736.42,161.27,8.02">Thumbnail 2 shows a blue Swiss Tardis. Cars, coaches, and road signs are found in thumbnails 3, 4, and 5. Thumbnail 6 shows a glacier in the mountain. Thumbnail 7 gives a view of the parking lot example in Section 4.</figDesc><table coords="8,297.63,50.39,225.36,48.26">Image 
SW Saliency k-NN 
Interactions 
Grimsel Pass 18k (64) 
525(64) 
400(64), 325(62) 
Royal Gorge 19k (49) 
567(49) 
226(49), 121(45) 
Cacti 
50k(10) 
1.5k(10) 
761(10), 513(7) 
Whitney 
40k(15) 
1069(15) 
604(14), 259(12) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false" coords="8,285.12,105.84,250.38,73.62"><figDesc coords="8,285.12,105.84,29.39,7.37">Table 1.</figDesc><table coords="8,285.12,105.84,250.38,73.62">This table shows the quality of results after each step of 
processing. The SW Saliency column shows the results after sliding-
window saliency map. The k-NN column shows the top 3% regions se-
lected by anomaly detection. The Interaction column shows the results 
after three and five user interactions. In each of these columns, the 
number shows the count of computer-selected regions and the number 
in parenthesis is the count of the detected objects of interest. An object 
of interest may contain multiple detected regions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false" coords="9,66.57,50.39,175.13,58.23"><figDesc coords="9,66.57,50.39,22.41,8.02;9,131.42,50.39,49.29,8.02;9,198.62,50.39,43.07,8.02;9,198.62,60.36,39.10,8.02">Image Gigapan Tags Detected by our system</figDesc><table coords="9,66.57,61.71,160.64,46.92">All 
Non-Semantic 

Grimsel Pass 35 
32 
25 
Royal Gorge 35 
30 
24 
Cacti 
24 
17 
15 
Whitney 
11 
11 
10 

</table></figure>

			<note place="foot">IP AND VARSHNEY: SALIENCY-ASSISTED NAVIGATION OF VERY LARGE LANDSCAPE IMAGES (a) Grimsel Pass (1.3 gigapixels) http://www.gigapan.org/gigapans/30463/ (b) Royal Gorge Bridge (1.4 gigapixels) http://www.gigapan.org/gigapans/7295/ (c) Cacti (4.0 gigapixels) http://www.gigapan.org/gigapans/14937/ (d) Mt. Whitney (5.0 gigapixels) http://www.gigapan.org/gigapans/44272/ Fig. 11. Gigapan picture datasets with samples of numbered detected regions shown in the thumbnails.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,40.76,74.58,232.12,7.13;10,40.76,84.05,232.12,7.13;10,40.76,93.51,42.32,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Seam carving for content-aware image resizing</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Avidan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Shamir</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,102.97,232.12,7.13;10,40.76,112.44,232.12,7.13;10,40.76,121.90,232.12,7.13;10,40.76,131.37,66.23,7.13"  xml:id="b1">
	<analytic>
		<title level="a" type="main">An MPEG-7 compatible video retrieval system with integrated support for complex multimodal queries</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Bas¸tanbas¸tan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Güdükbay</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Ulusoy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="62" to="73" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,140.83,232.12,7.13;10,40.76,150.30,232.12,7.13;10,40.76,159.76,99.71,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">View selection for volume rendering</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Bordoloi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-W</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,169.23,232.12,7.13;10,40.76,178.69,232.12,7.13;10,40.76,188.16,90.14,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Saliency, attention, and visual search: An information theoretic approach</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">D B</forename>
				<surname>Bruce</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Tsotsos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,197.62,232.13,7.13;10,40.76,207.08,232.13,7.13;10,40.76,216.55,66.23,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Isosurface similarity maps</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,226.01,232.12,7.13;10,40.76,235.48,232.12,7.13;10,40.76,244.94,173.11,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">An information-theoretic framework for visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jänicke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1215" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,254.41,232.12,7.13;10,40.76,263.87,232.12,7.13;10,40.76,273.34,61.45,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">An exemplar model for learning object classes</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Chum</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zisserman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,282.80,232.12,7.13;10,40.76,292.26,232.12,7.13;10,40.76,301.73,232.12,7.13;10,40.76,311.19,80.58,7.13"  xml:id="b7">
	<monogr>
		<title level="m" type="main">Streaming aerial video textures In Scientific Visualization: Advanced Concepts, volume 1 of Dagstuhl Follow-Ups</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">S</forename>
				<surname>Co</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Duchaineau</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">I</forename>
				<surname>Joy</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,320.66,232.12,7.13;10,40.76,330.12,217.74,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Video visualization</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Daniel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,339.59,232.12,7.13;10,40.76,349.05,232.12,7.13;10,40.76,358.52,206.86,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">STAR: Tone reproduction and physically based spectral rendering</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Devlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Chalmers</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Wilkie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Purgathofer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">State of the Art Reports</title>
		<meeting><address><addrLine>Eurographics</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09" />
			<biblScope unit="page" from="101" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,367.98,232.12,7.13;10,40.76,377.45,232.12,7.13;10,40.76,386.91,90.14,7.13"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Goferman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zelnik-Manor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Tal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="2376" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,396.37,232.12,7.13;10,40.76,405.84,172.91,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Terapixel: A spherical image of the sky</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Guo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Poulain</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microsoft Environmental Research Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,415.30,232.12,7.13;10,40.76,424.77,232.12,7.13;10,40.76,434.23,232.12,7.13;10,40.76,443.70,61.45,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">The holy grail of multimedia information retrieval: So close or yet so far away?</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Hanjalic</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Lienhart</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W.-Y</forename>
				<surname>Ma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">R</forename>
				<surname>Smith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="547" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,453.16,232.12,7.13;10,40.76,462.63,232.12,7.13;10,40.76,472.09,61.45,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Hou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,481.55,232.12,7.13;10,40.76,491.02,232.12,7.13;10,40.76,500.48,213.23,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting and evaluating saliency for simplified polygonal models</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Howlett</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hamill</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">O</forename>
				<surname>Sullivan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Applied Perception</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="286" to="308" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,509.95,232.12,7.13;10,40.76,519.41,232.12,7.13;10,40.76,528.88,180.11,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparison of feature combination strategies for saliency-based visual attention systems</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Itti</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Koch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="161" to="169" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,538.34,232.12,7.13;10,40.76,547.81,232.12,7.13;10,40.76,557.27,125.21,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Itti</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Koch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Niebur</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11 4</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,566.74,232.12,7.13;10,40.76,576.20,232.12,7.13;10,40.76,585.66,162.37,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">A salience-based quality metric for visualization</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jänicke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1183" to="1192" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,595.13,232.12,7.13;10,40.76,604.59,232.12,7.13;10,40.76,614.06,123.62,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual analysis of flow features using information theory</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jänicke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Scheuermann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications IEEE</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,623.52,232.12,7.13;10,40.76,632.99,232.12,7.13;10,40.76,642.45,202.30,7.13"  xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring complexity in lagrangian and eulerian flow descriptions</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jänicke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Scheuermann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1783" to="1794" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,651.92,232.12,7.13;10,40.76,661.38,232.12,7.13;10,40.76,670.84,232.12,7.13;10,40.76,680.31,118.84,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Multifield visualization using local statistical complexity</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jänicke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Wiebel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Scheuermann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Kollmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1384" to="1391" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,689.77,232.12,7.13;10,40.76,699.24,232.12,7.13;10,40.76,708.70,118.84,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Streaming multigrid for gradient-domain operations on large images</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kazhdan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hoppe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,718.17,232.12,7.13;10,40.76,727.63,151.53,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Detection of Regions of Interest using Iterative Link Analysis</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Torralba</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,737.10,232.12,7.13;10,303.38,54.06,232.12,7.13;10,303.38,63.52,169.13,7.13"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Saliency-guided enhancement for volume visualization</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Varshney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="925" to="932" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,72.99,232.12,7.13;10,303.38,82.45,232.12,7.13;10,303.38,91.92,189.98,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Persuading visual attention through geometry</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Varshney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="772" to="782" />
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,101.38,232.12,7.13;10,303.38,110.85,232.12,7.13;10,303.38,120.31,158.77,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">Mesh saliency and human eye fixations</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Varshney</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Jacobs</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Guimbretì Ere</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Applied Perception</title>
		<imprint>
			<biblScope unit="volume">712</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,129.78,232.12,7.13;10,303.38,139.24,232.12,7.13;10,303.38,148.70,118.84,7.13"  xml:id="b26">
	<analytic>
		<title level="a" type="main">Capturing and viewing gigapixel images</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Uyttendaele</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Deussen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,158.17,232.12,7.13;10,303.38,167.63,232.12,7.13;10,303.38,177.10,119.53,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive content-aware zooming</title>
		<author>
			<persName>
				<forename type="first">P.-Y</forename>
				<surname>Laffont</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">Y</forename>
				<surname>Jun</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wolf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y.-W</forename>
				<surname>Tai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Idrissi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Drettakis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">E</forename>
				<surname>Yoon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface 2010</title>
		<meeting>Graphics Interface 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,186.56,232.12,7.13;10,303.38,196.03,232.12,7.13;10,303.38,205.49,118.84,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency-guided lighting</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">H</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Varshney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Information and Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="373" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,214.96,232.12,7.13;10,303.38,224.42,232.12,7.13;10,303.38,233.88,42.32,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">Mesh saliency</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">H</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Varshney</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Jacobs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="659" to="666" />
			<date type="published" when="2005-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,243.35,232.12,7.13;10,303.38,252.81,232.12,7.13;10,303.38,262.28,85.36,7.13"  xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Sun</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N.-N</forename>
				<surname>Zheng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Tang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-Y</forename>
				<surname>Shum</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,271.74,232.12,7.13;10,303.38,281.21,232.12,7.13;10,303.38,290.67,80.58,7.13"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Annotating gigapixel images</title>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Luan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">M</forename>
				<surname>Drucker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y.-Q</forename>
				<surname>Xu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">F</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,300.14,232.12,7.13;10,303.38,309.60,232.12,7.13;10,303.38,319.07,215.52,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Evita: Efficient visualization and interrogation of tera-scale data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Machiraju</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Fowler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B S W</forename>
				<surname>Thompson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Schroeder</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining for scientific and engineering applications. Kluwer</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,328.53,232.12,7.13;10,303.38,337.99,215.60,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Muja</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">G</forename>
				<surname>Lowe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISAPP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,347.46,232.12,7.13;10,303.38,356.92,232.12,7.13;10,303.38,366.39,184.49,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Top-down control of visual attention in object detection</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Oliva</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Torralba</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Castelhano</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Henderson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICIP</title>
		<imprint>
			<date type="published" when="2003-09" />
			<biblScope unit="page" from="253" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,375.85,232.12,7.13;10,303.38,385.32,232.12,7.13;10,303.38,394.78,232.12,7.13;10,303.38,404.25,157.09,7.13"  xml:id="b35">
	<monogr>
		<title level="m" type="main">Saliency guided summarization of molecular dynamics simulations In Scientific Visualization: Advanced Concepts, volume 1 of Dagstuhl Follow-Ups</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Patro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">Y</forename>
				<surname>Ip</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Varshney</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="321" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,413.71,232.12,7.13;10,303.38,423.17,232.12,7.13;10,303.38,432.64,118.84,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">A comparative study of image retargeting</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Rubinstein</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gutierrez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Sorkine</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Shamir</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="160" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,442.10,232.12,7.13;10,303.38,451.57,232.12,7.13;10,303.38,461.03,232.12,7.13;10,303.38,470.50,109.27,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">Using multiple segmentations to discover objects and their extent in image collections</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Russell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Freeman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Efros</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Sivic</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zisserman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1605" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,479.96,232.12,7.13;10,303.38,489.43,232.12,7.13;10,303.38,498.89,232.12,7.13;10,303.38,508.36,37.54,7.13"  xml:id="b38">
	<analytic>
		<title level="a" type="main">A fast all nearest neighbor algorithm for applications involving large point-clouds</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Sankaranarayanan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Samet</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Varshney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="174" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,517.82,232.12,7.13;10,303.38,527.28,232.12,7.13;10,303.38,536.75,232.12,7.13;10,303.38,546.21,80.58,7.13"  xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive editing of massive imagery made simple: Turning Atlanta into</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Summa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Scorzelli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Jiang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P.-T</forename>
				<surname>Bremer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Pascucci</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atlantis. ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="1944846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,555.68,232.12,7.13;10,303.38,565.14,232.12,7.13;10,303.38,574.61,71.01,7.13"  xml:id="b40">
	<analytic>
		<title level="a" type="main">The clipmap: a virtual mipmap</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">C</forename>
				<surname>Tanner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">J</forename>
				<surname>Migdal</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">T</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,584.07,232.12,7.13;10,303.38,593.54,232.12,7.13;10,303.38,603.00,232.12,7.13;10,303.38,612.47,42.82,7.13"  xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling visual-attention via selective tuning</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Tsotsos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">M</forename>
				<surname>Culhane</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">Y K</forename>
				<surname>Wai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Lai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Davis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Nuflo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1295 2</biblScope>
			<biblScope unit="page" from="507" to="54510" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,621.93,232.12,7.13;10,303.38,631.39,232.12,7.13;10,303.38,640.86,169.13,7.13"  xml:id="b42">
	<analytic>
		<title level="a" type="main">Importance-driven focus of attention</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Viola</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Feixas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sbert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="933" to="940" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,650.32,232.12,7.13;10,303.38,659.79,232.12,7.13;10,303.38,669.25,128.40,7.13"  xml:id="b43">
	<analytic>
		<title level="a" type="main">Measuring visual saliency by site entropy rate</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Gao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="2368" to="2375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,678.72,232.12,7.13;10,303.38,688.18,232.12,7.13;10,303.38,697.65,232.12,7.13;10,303.38,707.11,182.68,7.13"  xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalized streak lines: Analysis and visualization of boundary induced vortices</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Wiebel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Tricoche</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Schneider</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jänicke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Scheuermann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,716.57,232.12,7.13;10,303.38,726.04,232.12,7.13;10,303.38,735.50,118.84,7.13"  xml:id="b45">
	<analytic>
		<title level="a" type="main">Visorama 2.0: a platform for multimedia gigapixel panoramas</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Zonenschein</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Velho</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH ASIA 2010 Posters</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
