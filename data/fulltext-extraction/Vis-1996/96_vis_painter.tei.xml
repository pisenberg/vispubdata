<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Case Study: Mantle Convection Visualization on the Cray T3D</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">S</forename><surname>Painter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Bunge</surname></persName>
							<email>bunge@kokopelli.lanl.gov</email>
							<affiliation key="aff4">
								<orgName type="department">Institute of Geophysics and Planetary Physics</orgName>
								<orgName type="laboratory">Los Alamos National Laboratory</orgName>
								<address>
									<postCode>87545</postCode>
									<settlement>Los Alamos</settlement>
									<region>New Mexico</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarden</forename><surname>Livnat</surname></persName>
							<email>ylivnat@cs.utah.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<addrLine>Salt Lake City</addrLine>
									<postCode>84112</postCode>
									<settlement>Utah</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Computing Laboratory Los Alamos National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Institute of Geophysics and Planetary Physics Los Alamos National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Advanced Computing Laboratory, Los Alamos National Laboratory</orgName>
								<address>
									<postCode>87545</postCode>
									<settlement>Los Alamos</settlement>
									<region>New Mexico</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Case Study: Mantle Convection Visualization on the Cray T3D</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The recent years have seen rapid advancement towards viewing the Earth as an integrated system. This means that we have come to understand the interdependence of the major planetary subsystems-atmosphere, biosphere, oceans and the deep earth interior-on a large range of time and length scales. One of the longest time scales of the planet is imposed by solid state convection within the silicate Earth mantle. Mantle convection modeling, and other earth science modeling efforts, now are producing simulation data on grids that are large enough to strain the memory and processing power of even the largest high-end graphics workstations. Another alternative is to use parallel visualization tools running on the massively parallel computers that generated the data. This is the approach that we have taken for the visualization of mantle convection simulation data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction to Mantle Convection Simulation</head><p>Solid state convection within the Earth's mantle determines one of the longest time scales of our planet. The Earth's mantle, the 2900 km thick silicate shell that extends from the iron core to the Earth's surface, though solid, is deforming slowly by viscous creep over long time periods. While gradual in human terms, the vigor of this subsolidus convection is impressive, producing flow velocities of 1-10 cm/year. Plate tectonics, the piecewise continuous movement of the Earth's surface, is the prime manifestation of this internal deformation, but ultimately all large scale geological activity of our planet, such as mountain building and continental drift, must be explained dynamically by mass displacements within the mantle.</p><p>A major problem for researchers in computational mantle dynamics is to resolve the Earth's outer 100 km deep skin, or lithosphere. This lithosphere is an integral part of the mantle and thus a 100 km wide spatial resolution has to be achieved throughout the volume. The resulting computational problem is formidable and numerical discretizations with 1-10 million grid points have to be formulated to resolve the mantle volume on scales of 50 km or less. The resulting computational problem has been largely intractable on conventional sequential computers, as it is necessary to follow the time evolution of pressure, temperature and velocity over the entire volume, requiring gigabytes of memory and computational speeds of many gigaflops. However, such problems are well in reach of modern parallel computers, such as the massively parallel Cray Research, Inc. T3D system.</p><p>To address this problem, we use the 3D spherical mantle dynamics code TERRA, which solves the Navier-Stokes equations in the infinite Prandtl number limit using a multigrid approach <ref type="bibr" target="#b0">[1]</ref>. Discretization of the spherical shell is based on subdivision of of the regular icosahedron producing a data structure that is well suited for modern parallel hardware using domain decomposition and message passing <ref type="bibr" target="#b1">[2]</ref>. A message passing version of TERRA runs on the 256 processor CRAY T3D at the Advanced Computing Laboratory (ACL) at Los Alamos National Laboratory <ref type="bibr" target="#b2">[3]</ref>. On this machine our numerical modeling code shows excellent parallel performance, displaying a communication overhead of less than ten percent. The computational memory afforded by the T3D has allowed us to investigate convection employing a numerical grid of more than 20 million finite elements. We are thus able to resolve a large range of dynamical length scales within the mantle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Visualization Tools</head><p>Visualization of the vast simulation data is a serious challenge, as it is necessary to display the large-scale flow without compromising resolution of small scale structure. Small scale structure is generated primarily in thermal boundary layers, such as the lithosphere at the top of the mantle, but swept around by the large scale flow throughout the mantle. Moreover the temporal evolution must be visualized by displaying long time-series of data, requiring capacity many thousand times that of the individual timestep.</p><p>To address this problem, we have developed visualization tools that run on the massively parallel computer where the data was generated. This allows for both a rapid and high resolution display of simulation results too large for visualization on even high-end graphics workstations. In addition, by running the visualization tools on the parallel computers used to generate the massive data, we avoid the need for time consuming and cumbersome data transfers of the simulation results. The ability to display fine simulation details, such as the very localized generation and evolution of thermal structures along major boundary layers, greatly enhances the physical interpretation and presentation of these new high resolution convection experiments.</p><p>The parallel visualization tools consist of an isosurface extractor, a parallel polygon renderer, and a parallel slicer that can interpolate arbitrary planar slices through field data. These tools use a message passing and active message programming model <ref type="bibr" target="#b10">[11]</ref>. The tools operate directly on the TERRA grid structure. While the TERRA grid is not a structured grid, the recursive subdivision basis of the grid allows the grid geometry to be implicitly represented rather than explicitly stored, saving memory and allowing for efficient geometric queries of the grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Rendering</head><p>Many researchers have studied parallel algorithms for polygon and volume rendering in recent years <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Molnar et al., provide a useful taxonomy for parallel rendering which classifies parallel rendering methods as sort-first, sort-middle, or sort-last according to where interprocessor communications occurs in the rendering pipeline <ref type="bibr" target="#b8">[9]</ref>.</p><p>Our T3D parallel renderer uses a sort-middle based rendering algorithm. Both the data domain and the image are partitioned evenly among the processors. Each processor first handles the geometric processing for the portion of the data it holds: isosurface extraction, arbitrary slicing and geometric transformation. The resulting geometric primitives are partitioned into scanline segments according to the portion of screen space they cover and sent to the processor responsible for that portion of the image using an active message communications model. Scanline segments are buffered and sent in groups to amortize the cost of a message over several scanline segments.</p><p>When the active message arrives at its destination processor, a handler function is invoked that completes the rasterization of the primitives it contains. Opaque scanline segments are directly z-buffered. Transparent scanline segments are buffered and handled after all processors complete geometric processing. The transparency segments are first depth sorted via a Newell-Newell-Sancha depth sort then composited front to back <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Slicing</head><p>Arbitrary slicing is handled through software based texture mapping. Each slice plane is clipped against a bounding box for each contiguous portion of data held on a processor. The resulting polygons are scan converted with a texture lookup at each pixel. The texture lookup function maps the pixel screen coordinates back to the simulation grid in two steps. First, the screen coordinates are mapped backwards into world coordinates via a four-by-four matrix multiply. These world coordinates are then mapped to a grid cell through a hierarchical grid search. The field values are interpolated within the grid cell and mapped, through a color map, to a surface color. Colored scanline segments are sent, via an active message, to the processor responsible for their scanline, as described earlier.</p><p>The mapping from world coordinates to grid coordinates can be done in Olog N time or the TERRA grid because of the subdivision nature of the grid. We have also implemented slices for fields defined over uniform regular grids, which can be indexed directly in O1 time. While we have not implemented slicing on arbitrary unstructured grids, the texture mapping method could be applied but would require a more elaborate search strategy. An alternative approach to slicing is to generate a full geometric intersection of the grid with the slice plane and render the resulting polygons using the normal rendering pipeline. Our method has two benefits over this alternative. The texture mapping based slice rendering time grows very slowly with the size of the grid (log N for the TERRA grids). Further, the slice plane can be interactively changed, since no expensive preprocessing work is done when the slice plane is changed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Isosurface Extraction</head><p>In traditional isosurfacing, e.g. marching cubes <ref type="bibr" target="#b7">[8]</ref>, the entire data set is traversed in order to extract a single isosurface. This method is not applicable in our case where interactive speed over huge data sets is essential.</p><p>The disadvantages of the marching cubes algorithm are two fold. First, the number of cells in a single isosurface are usually very N is the number of grid cells small compare to the the total number of cells. Second, the decision of whether a cell does or does not intersect the isosurface is as expensive as actually computing the intersection, i.e. no trivial rejection.</p><p>More advanced algorithms employ a pre-process stage where the data set in examined and some key information is retained. Trivial rejection can be achieved by retaining the minimum and maximum values attained in each cell. The first issue, reducing the search time, i.e. checking as few cells as possible against the giving isovalue, is much tougher and many algorithms have been developed to address this issue.</p><p>The TERRA data sets can be characterized as both structured and unstructured. The geometry of a cell can be inferred quickly from its index and thus it does not have to be saved explicitly for each cell. On the other hand, the overall structure of the data set makes it difficult to utilizes structured methods such as Wilhelms and Van Gelder's octree <ref type="bibr">[12]</ref>.</p><p>We chose to use the NOISE algorithm <ref type="bibr" target="#b6">[7]</ref> which is both nearoptimal in time and is flexible enough to handle any geometry. The algorithm has a complexity of only OK + p N in the worst case, where N is the number of cells in the data set and K is the number of cells which actually intersect a given isosurface. The algorithm is near-optimal in the sense that in practice, K is almost always greater then p N. NOISE is based on the projection of the data set onto the span space, where each cell is represented by a single point with 2 coordinates: the minimum and maximum values attained in that cell. The projection is perform only once as either an offline stage or, as in our case, in each processor after the data is distributed. In order to achieve a near optimal performance, the algorithm takes advantage of a kd-tree to hold the projected points in the span space. The memory requirement for the kd-tree itself is only a single id per cell. The kd-tree structure is saved implicitly via the order in which these id's are sorted. This property of the algorithm makes it particular useful for very large data sets where both the speed up of the search and memory requirements are of great importance. Furthermore, NOISE can be used with or without saving explicitly the minimum and maximum values of each cell. Again, the trade off is between speed and memory. We took advantage of the large amount of memory on the CRAY T3D and aimed at accelerating the search by saving this additional information.</p><p>The parallel version of NOISE takes advantage of the flexibility of the algorithm with respect to the structure of the data set and the sort-middle parallel renderer discussed earlier. The data distribution is left to the renderer and NOISE is applied locally to the data on each processor. Isosurface extraction is then initiated by distributing only the new isovalue. The extracted local isosurface on each processor is temporarily added to the data set to be rendered. It is then left to parallel renderer to distribute the rendering of the isosurface to the other processors, a task which the renderer performs for the rest of its local geometric data.</p><p>The integration of the isosurface extraction and the parallel renderer enable us to take advantage of the renderer transparency capability. The use of transparency and the rapid isosurface extraction achieved by the algorithm also enabled the rendering of several isosurfaces at the same time. Each such isosurface is extracted independently from the others. The NOISE algorithm can determine, virtually at no cost, the amount of storage needed for the isosurface before it is computed. We take advantage of this capability and refrained from allocating memory by reusing the memory allocated to the previous isosurface if it is large enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Delivery</head><p>Delivering images to the desktop is often the limiting factor for interactive parallel renderers. While a parallel renderer may be able to render frames at interactive rates, it can be difficult to get them off the parallel machine and onto a users display quickly enough to allow interaction. In our case, the final image is gathered and displayed using either a HIPPI frame buffer or X11 output to the user's workstation. HIPPI frame buffer output provides the fastest performance: up to 10 1280 by 1024 high resolution frames per second. Our HIPPI frame buffer is switchable to a number of monitors, providing high speed interaction for users with access to one of these monitors.</p><p>Image delivery over X11 is useful for users remotely located from the T3D. High performance image display over X11 is difficult. A full color high resolution (1280x1024) frame uses 5 MB, uncompressed, requiring 50MB/sec of bandwidth, much higher than is available across 10Mb ethernet or over long haul internet connections. To alleviate these bandwidth needs, our parallel renderer compresses each frame to be display, in parallel. The compressed image data is gathered to a single processor, and then sent, via a socket, to a display process running on the users workstation. We use zlib, a general purpose lossless compression library. Interactive control is provided by a graphical user interface (GUI) running on the user's workstation. Typical frame rates on 64 T3D nodes are 2-10 frames per second for 1280x1024 HIPPI output, including slicing and isosurface generation on each frame. X11 output is slower and highly depends on network speed, however we have observed greater than 2 FPS at 640x512 resolution between our T3D at LANL and remote workstations across the country.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Figures 1 and 2 illustrate two example stills from the TERRA visualization tools. These figures and the accompanying video are rendering a static data set at a resolution of 1.25 million grid cells. <ref type="figure" target="#fig_0">Figure 1</ref> shows the temperature field over the spherical model of the Earth. In this simulation the viscosity (or stiffness) of the mantle fluid increases with depth by a factor of 30, as suggested by geophysical observation. This one parameter change results in a dramatic difference in convection structure displaying elongated downwellings from the upper surface instead of pointlike patterns typical for isoviscous flow <ref type="bibr" target="#b3">[4]</ref>. Such elongated downwellings are analogous to Earth's linear subduction zones where plates dive under one another.</p><p>The outer surface of the sphere is a spherical radial shell cutting through the grid structure. Note that the sphere is hollow and <ref type="figure">Figure 2</ref>: Two Iso-Temperature Surfaces, With Transparency an inner shell is shown as well. The simulation model covers only the outer 50% of the Earth's diameter where the mantle exists. A "wedge" has been removed by 3 slicing planes. Within the wedge opening an isosurface has been extracted with a "hot" temperature value. The isosurface and grid slices give insight on how hot material convects upwards through the Earth mantle. <ref type="figure">Figure 2</ref> again shows the temperature field with two isosurfaces over an inner spherical radial shell. The outer blue transparent isosurface used a relatively cold temperature, indicating where cold material moves back toward the interior of the mantle. The inner orange opaque surface is a relatively high iso-temperature surface and again illustrates hot material moving outwards.</p><p>The accompanying video illustrates an exploration of a single time step snapshot of the TERRA simulation results. While the video was rendered in batch mode, the rendering rate was still over 3 frames per second, excluding image write time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>Parallel visualization tools have proven to be an invaluable aid in interpreting mantle convection simulation results. Because of the grid sizes involved, commercial visualization software running on high end graphics workstations was too slow and used too much memory to be an effective interactive exploration tool. A much faster and more convenient visualization system is possible for these "massive data" problems by running the visualization tools directly on the massively parallel processor where the data was generated.</p><p>Current and future work with TERRA is aimed at incorporating the information of the geological record into the mantle convection simulations, primarily the effects of continents and tectonic plates at the surface of the Earth. These new simulations are expected to improve substantially our understanding of the temporal evolution of Earth's mantle.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Slicing and Isosurfacing of the Temperature Field</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">[12] J. Wilhelms and A. Van Gelder. Octrees for faster isosurface generation. ACM Transactions on Graphics, 11(3):201-227, July 1992.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Three dimensional treatment of convective flow in the Earth&apos;s mantle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Baumgardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Phys</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="501" to="511" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Icosahedral discretization of the two sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Baumgardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Fredrickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam J. Numer Anal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1107" to="1115" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mantle convection modeling on parallel virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Bunge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Baumgardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Physics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="215" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effect of depth-dependent viscosity on the planform of mantle convection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Bunge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Baumgardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page" from="436" to="438" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Parallel Rendering Symposium Proceedings. ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1993-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Parallel Rendering Symposium Proceedings. ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1995-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A near optimal isosurface extraction algorithm using the span space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarden</forename><surname>Livnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="84" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3d surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A sorting classification of parallel rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="1994-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A solution to the hidden surface problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sancha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ACM National Conference</title>
		<meeting>ACM National Conference</meeting>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Colin De Verdière</surname></persName>
		</author>
		<title level="m">The ACL message passing library. EPFL Supercomputing Review</title>
		<imprint>
			<date type="published" when="1995-11" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
