<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualization of Complex Models Using Dynamic Texture-based Simplification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">G</forename><surname>Aliaga</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visualization of Complex Models Using Dynamic Texture-based Simplification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>geometry</term>
					<term>textures</term>
					<term>morphing</term>
					<term>visual complexity</term>
					<term>space partitioning</term>
					<term>simplification</term>
					<term>visibility culling</term>
					<term>interactive</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We are investigating methods for simplifying complex models for interactive visualizations using texture-based representations. This paper presents a simplification method which dynamically &quot;caches&quot; distant geometry into textures and trades off accurate rendering of the distant geometry for performance. Smooth transitions and continuous borders are defined between the geometry and textures thus the representations can be switched without sudden jumps (as is the case with many current texturing techniques). All the computations for the transitions can be done a priori without the need to change the textures each frame thereafter.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Geometric models have become very large and difficult t o render at interactive rates. As a result, many algorithms have been developed to simplify models until they are (hopefully) small enough to be rendered at interactive rates. Two popular approaches are visibility culling and level-of-detail management. Visibility culling algorithms determine which subset of the model is visible and only render that portion of the geometry <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b11">[12]</ref>. Unfortunately, these algorithms do not work well when many primitives are still visible. For example, complex rooms, used in architectural walkthroughs, have a large amount of visual complexity that must be rendered. Level-of-detail (LOD) algorithms <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b9">[10]</ref>, which typically require some manual interaction for generating the multiple LODs, cannot easily simplify scenes with a large number of visible objects.</p><p>A relatively new simplification approach is t o dynamically represent geometric complexity using textures.</p><p>Textures have the advantage of taking constant time to render regardless of the complexity of the portion of the model they represent. We are investigating how to create a system that renders the nearby subset of a model as geometry and the distant, but visible, subset of a model with a texture-based representation. As the viewpoint changes, the system dynamically changes the geometry into textures or the textures back into geometry. Preliminary results indicate that representing the distant geometry with textures produces an adequate image quality for architectural walkthrough visualizations. The error introduced is proportional to the ______________________________ * aliaga@cs.unc.edu, (919) 962-1722 CS Dept., CB #3175, UNC-CH, Chapel Hill, NC 27599-3175 viewpoint's distance from the original texture sample point. The system can bound the error by resampling the texture as needed.</p><p>There are two major problems in developing this rendering system. First, since a texture represents an arbitrary subset of the model from a single viewpoint, changing the viewpoint causes the image displayed by the texture to be incorrect (unless image warping is used <ref type="bibr" target="#b8">[9]</ref>). Consequently, the geometry surrounding the texture does not match the geometry sampled in the texture causing a discontinuity or "crack" to appear. Previous methods <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b10">[11]</ref> have either ignored this or used an error metric to decide whether to resample the texture or display another texture from a set of precomputed textures. Unfortunately, storing many texture samples requires a vast amount of texture memory or fast texture paging while frequently resampling the texture can significantly reduce the performance gain of using textures. Furthermore, the algorithms in <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b10">[11]</ref> have only been applied to outdoor scenes and are not well suited for indoor scenes (architectural walkthroughs, radiosity-illuminated rooms, etc.). We present a solution to the discontinuity problem (without warping the texture), thus providing a continuous border between geometry and texture. This gives us the freedom to unnoticeably place textures anywhere in a model.</p><p>The second problem occurs when switching between geometry and texture. Once the viewpoint has changed from the texture sample point, a transition from geometry to texture (or vice versa) will cause a sudden jump in the image (as i n previous texture-based simplification methods). Therefore, we need to define transitions to smoothly change the geometry into texture and texture back into geometry. We present smooth transition operations to convert geometry into textures (and vice versa).</p><p>The following section presents an overview of the visualization algorithm (preprocessing and run-time phases). Section 3 describes, how the transitions from geometry t o texture (and vice versa) are performed while maintaining a continuous border. Section 4 presents how multiple textures can be created. Section 5 shows some results we have obtained by using our algorithm with three complex models. Finally, Section 6 will end with some conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TEXTURE-BASED SIMPLIFICATION</head><p>We have devised a simplification method which dynamically replaces arbitrary portions of a model with textures and morphs the nearby geometry to match the texture. This method is capable of simplifying models of high visual complexity in cases where visibility culling and object-based LOD are insufficient. The algorithm can be combined with an automatic scheme to decide what portion of the model t o simplify to texture.</p><p>Our simplification method has a preprocessing phase and a run-time phase. The preprocessing phase statically partitions the model into a 3D grid of boxes. Space partitioning and view frustum culling are used so that the amount of work to be done for rendering and for transitions to texture or back to geometry is proportional to the number of primitives in the view frustum. The run-time phase performs smooth transitions t o texture and back to geometry while maintaining a continuous border between the geometry and texture. The following two subsections describe the major elements of the preprocessing and run-time phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing Phase</head><p>The input to our visualization algorithm is a 3D model. For interactive visualizations (or walkthroughs), typically the model does not fit completely within the field of view. Thus during rendering, only the subset of the model inside the view frustum needs to be sent down the graphics pipeline <ref type="bibr" target="#b3">[4]</ref>.</p><p>In order to efficiently perform view frustum culling, the model needs to be space partitioned. Various space partitioning algorithms have been proposed (BSP trees, octtree subdivision, etc.). In our current implementation, we found i t sufficient to use a uniform grid space partitioning algorithm. The bounding box of the entire model is subdivided into a 3D grid of boxes. Each box contains a list of all the primitives that lie within it. Primitives that intersect the boundary between two or more boxes are split (this slightly increases the number of primitives but makes the implementation simpler).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-time Phase</head><p>The user is allowed to move through the model. In order t o increase interactive performance, the user can replace a distant (and visible) subset of the model with a texture. The texture i s used to represent the subset of the model from the local view area. Unfortunately, once the viewpoint moves again, geometry surrounding the texture will appear discontinuous with the texture (Color <ref type="figure" target="#fig_0">Figure 1)</ref>. In order to maintain a continuous border between the texture and the geometry around it, either:</p><p>• The texture must be warped to match the geometry.</p><p>• The geometry must be warped to match the texture.</p><p>The former case corresponds to image warping <ref type="bibr" target="#b1">[2]</ref>[3] <ref type="bibr" target="#b8">[9]</ref> i n which the sampled texture has depth information and i s reprojected every frame by warping the texture to the viewpoint of the current frame. The adjacent geometry is rendered normally.</p><p>We use the second approach, namely morphing the vertices of the geometry to match the texture and maintain C0 continuity (higher orders of continuity are also possible). This approach is more attractive because: (a) it allows texturing hardware to be efficiently used, (b) the texture does not need t o be warped every frame, (c) all of the work is performed at one time (at geometry-to-texture transition time or as a precomputation), (d) it does not introduce visible artifacts as the viewpoint changes as may be the case with image warping. The visible artifacts introduced by image warping include cracks in the image due to incorrect splatting of the pixels and "empty areas" produced by previously occluded regions becoming visible with no rendering information available for the newly visible pixels. Although this method could be considered less "realistic" than warping the texture, it takes advantage of the fact that geometry is re-rendered every frame, so by slightly modifying the geometry you are able to use static textures and achieve higher frame rates.</p><p>After replacing a subset of the model with a texture (Color <ref type="figure" target="#fig_1">Figure 2</ref>), the user cannot walk forward beyond the texture plane (without returning the subset to geometry). Furthermore, geometry near the viewpoint is rendered normally while the geometry surrounding the texture maintains a continuous border with the texture but is not rendered completely accurately. This is not a bad tradeoff for the improved performance. In order to return the texture to geometry (for example, if the viewpoint gets too close to the texture), a smooth transition operation from texture back to geometry i s performed over the next few frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SMOOTH TRANSITIONS</head><p>The algorithm presented in this paper performs smooth transitions between geometry and texture by morphing the nearby geometry. Initially, the entire model is represented as geometry. Then, an arbitrary subset of the model is simplified to a texture by a geometry-to-texture transition. Further rendering of the subset of the model requires only displaying the texture and not the geometry represented by the image of the texture. In order to return the texture to geometry, a textureto-geometry transition must occur. Any number of subsets of the model can be replaced by textures (the textures can be created from a common viewpoint or from different viewpoints; details are in Section 4). The basic steps for these two transitions are given below.</p><p>The computations involved in the transition operations are relatively simple. In fact, they are proportional to the number of primitives rendered (more precisely, to the number of primitives in the view frustum prior to replacement of geometry with texture). Displaying the textures themselves i s proportional to the number of pixels in the textures and independent of scene complexity. If the location of the textures and their sampling viewpoints are determined beforehand, all the computations for the projection and morphing operations can be done a priori at the expense of additional storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometry-To-Texture Transition</head><p>1. The user selects a subset of the model to replace with a texture. This can be done in various ways. We adopted the following strategy: select all geometry inside the view frustum and beyond a distance d from the viewpoint. Since we are using view frustum culling for rendering, determining what geometry is in the view frustum is trivial. The space partitioning allows us to easily determine which boxes (and thus what geometry) are behind the texture plane. The texture plane is defined as the plane whose normal is the current view direction v d and contains the point t o which is at a distance d from the eyepoint along the view direction ( <ref type="figure" target="#fig_0">Figure 1</ref>). The subset of the model t o be replaced with a texture is called the culled geometry.</p><p>2. The current rendered image of the culled geometry i s copied from the framebuffer to texture memory. A texture primitive (a texture-mapped quadrilateral covering the subset of the model being replaced) is added to the model. Since the texture contains an image of shaded geometry, lighting i s temporarily turned off when rendering the texture primitive (in our test cases, we used static lighting: directional lights or precomputed radiosity-illuminated scenes).</p><p>To reduce texture memory requirements, the texture can be sampled at a resolution lower than the framebuffer's resolution. The texturing hardware is used to magnify the texture using bilinear interpolation between the texels. On the other hand, the texture can be sampled at a higher resolution than it will be displayed and prefiltered to achieve apparent high-quality antialiased imagery (in addition to MIP mapping the texture).</p><p>3. The culled geometry is removed from the set of rendered geometry. The space partitioning boxes that intersect the view frustum can be further partitioned or not culled at all. In our implementation, we choose not to cull these boxes. Thus, some geometry is rendered "behind" the edges of the texture and is never actually visible; in practice this amounts to only a small amount of geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>The geometry in front of the texture plane is rendered normally (near geometry). The geometry behind the texture plane (that has not been culled) and the geometry surrounding the texture are morphed to match the geometry represented b y the texture (this is the continuous border problem). Both these sets become the morphed geometry <ref type="figure" target="#fig_0">(Figure 1)</ref>. We defined the texture plane to always be parallel to the screen plane at sampling time (though it does not necessarily cover the entire view frustum). Therefore, during the first geometry-to-texture transition of a texture there is no need t o gradually morph the geometry surrounding a texture from its original position to its projected position on the texture plane. Instead the surrounding geometry is immediately projected onto the texture plane.</p><p>Once a texture has been computed it might undergo various geometry-to-texture and texture-to-geometry transitions. All subsequent transitions (after the first geometry-to-texture transition) will generally be from viewpoints other than the texture sample point. Thus the surrounding geometry i s gradually morphed from its original position to its projected position on the texture plane. In any case, since space partitioning and view frustum culling are used, only the visible geometry is actually morphed or projected. The morphing operation can be described by the following equation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v m (s) = sv p + (1-s)v o</head><p>This describes a linear interpolation between the modelspace vertex positions (v o ) and the vertex positions projected onto the texture plane (v p ), as s varies from 0 to 1. The latter set of vertices are almost the same as the screen-space projection of the vertices at the time of the first geometry-totexture transition of a given texture. In fact, they are obtained by transforming the model-space vertices to screen-space (v s ), then setting their z-value to be the screen-space projected z-value of the texture plane (t sz ) and transforming them back t o model-space:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v s = Tv o t s = Tt o → v sz = t sz → v p = T -1 v s</head><p>In order to morph the vertices from their original position to their projected position, the value of s is gradually incremented from 0 to 1 over the next few frames after the start of the transition <ref type="figure" target="#fig_1">(Figure 2</ref> and Color <ref type="figure" target="#fig_2">Figure 3)</ref>. The above morphing operation maintains C0 continuity (i.e. positional continuity) between the texture and the geometry surrounding the texture (morphed geometry). This implicitly achieves C0 continuity of the texture and morphed geometry's border with the near geometry. It does not require morphing the near geometry. Higher order interpolation could be used t o achieve smoother continuity between the texture, near geometry and morphed geometry. In this case, the near geometry, close to the texture, would be modified to obtain smoother continuity with the texture and morphed geometry. This would improve the texture and morphed geometry's border with the near geometry when viewed far from the original texture sample point. Unfortunately, this would violate the desire to keep near geometry undistorted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Texture-to-Geometry Transition</head><p>1. The culled geometry is reintroduced into the model. The vertices are set to their projected position on the texture plane. This is done by computing the vertex positions with s = 1.</p><p>2. The vertices of the geometry reintroduced into the model are morphed from their projected position on the texture plane to their original position (note that if the texture plane is currently not in the view frustum, an instantaneous transition can be performed). The value of s is gradually reduced from 1 t o 0 over the next few frames (Color <ref type="figure" target="#fig_2">Figure 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MULTIPLE TEXTURES</head><p>So far we have described how to replace a single subset of a model with a texture. Multiple subsets of a model can also be simultaneously represented by using multiple textures. Additional textures can be created either from a common viewpoint and constant distance or from different viewpoints and distances. The main difference between the two strategies is the effect they have on how geometry is morphed. In our system, we implemented both strategies except for the cases that require morphing of geometry to match two textures simultaneously.</p><p>• Common Viewpoint and Constant Distance: The multiple textures are created by changing only the view direction and selecting non-overlapping subsets of the model. The same distance from viewpoint to texture (d) is used. There is no need to morph the geometry surrounding each individual texture until all the texture samples have been obtained. Therefore, each texture will have an image of unmorphed geometry. If adjacent textures share a common edge (i.e. n o geometry is rendered between the textures), the textures combine to form a single large texture with piecewise planar components. This last variation can be used to create textures that cover a complex region of the model or even completely surround the viewpoint. The geometry surrounding each texture is morphed in the same way as described in the previous subsections ( <ref type="figure" target="#fig_2">Figure 3a</ref>).</p><p>If two textures (a "left" texture and a "right" texture) are created using a common viewpoint but different d values and not sharing an edge, the geometry in between the textures will have to be morphed to match both textures simultaneously. While this is possible, the distortion introduced by the two textures might be very apparent from certain view directions. This is especially true if very different d values are used. For example, assume the left texture was sampled at a significantly closer distance than the right texture. Thus, viewing the left texture from the left side might occlude some of the right texture and all of the geometry in between both textures.</p><p>• Different Viewpoints and Distances: If multiple textures are created from different viewpoints each at a potentially different distance from its viewpoint, the geometry surrounding each texture must be morphed before continuing on to create the next texture <ref type="figure" target="#fig_2">(Figure 3b)</ref>. Consequently, the first texture will have an image of unmorphed geometry. Subsequent textures that are created by using geometry surrounding the previous textures, will contain images of morphed geometry. Textures that are created from viewpoints and view directions that do not contain the geometry in the plane of the previous textures are sampled using unmorphed geometry. Thus, it might be the case that the distortion introduced by the morphing operations will be magnified after several instances of textures from different viewpoints. Typically, this will not be the case since the number of textures needed to surround the local view area i s small. If the view area migrates to another portion of the model (by a series of transitions), a new set of textures is used.</p><p>The subset of the model morphed for a particular texture can intersect with another morphed geometry subset. This i s similar to the case of two textures using a common viewpoint but different d values. Both morphed geometry subsets will have to be morphed to match the textures simultaneously. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>We implemented this algorithm on a SGI graphics workstation using OpenGL. The algorithm was applied to three models: a procedurally generated pipes model, a radiosityilluminated church <ref type="bibr" target="#b0">1</ref> and an auxiliary machine room of a nuclear submarine 2 .</p><p>For each model, several textures were created. We recorded various paths through the models (spline-interpolated paths and some freehand manipulation of the viewpoint). Interactive performance is significantly improved when textures are introduced. The distortion caused by the morphing is not very noticeable as you can see in the color figures (and video). Furthermore, no discontinuities or "cracks" are perceivable at the border between geometry and texture, though some color disparities are present. Since we are using static lighting, the texture and geometry's colors should match at the border. The small disparity may be a consequence of the texture sampling or of exactly how the texturing hardware processes the texel colors.</p><p>The procedurally generated pipes model provides a very good test case of a high depth complexity model where visibility culling (for example, cells and portals <ref type="bibr" target="#b0">[1]</ref> 10x1x10 grid of boxes (most of the complexity lies in the plane of the observer, namely the XZ plane). If the viewpoint was near the edge of the model looking outward, view frustum culling alone was able to produce decent interactive performance (18.52 frames/second). But if the view direction rotated to look inside the model, or if the viewpoint was approximately in the middle of the model, performance without texture-based simplification is very poor (1.21 frames/second at the edge of the model looking inward, 1.98 frames/second i n the middle of the model). Three textures were introduced covering the field of interest at a far distance. A single path was traversed from the border of the model towards the middle of the model. The average frame rate at the edge of the model looking inwards was 9.09 frames/second and in the middle of the model was 22.20 frames/second <ref type="figure" target="#fig_3">(Figure 4a</ref> and Color <ref type="figure" target="#fig_3">Figure 4a</ref>). We do not regard the "average frame rate" as the best means t o measure the performance. The frame rate with textures present depends greatly on how much geometry is actually rendered. A decent view of the model can be produced with very little geometry and a few textures. We are still investigating better metrics of performance.</p><p>The radiosity-illuminated church model is an example of a single room that is visually complex (158,604 triangles, 8x3x8 grid of boxes). Geometric-based LOD algorithms which simplify enough to significantly improve rendering performance would lose much of the shape and color details of the room. A texture on the other hand reduces rendering complexity, but maintains the apparent detail. In our test case, we recorded a path rotating and translating about the middle of the room <ref type="figure" target="#fig_3">(Figure 4b</ref> and Color <ref type="figure" target="#fig_3">Figure 4b</ref>). The average frame rate without texture-based simplification was 2.19 frames/second. After we introduced three textures, the frame rate rose to an average of 7.25 frames/second. The video demonstrates the dynamic transitions of each texture with this model and the subsequent performance increases and decreases.</p><p>Finally, the auxiliary machine room is an example of a visually complex model with high depth complexity (273,531 triangles, 10x1x14 grid of boxes). The recorded path <ref type="figure" target="#fig_3">(Figure 4c</ref> and Color <ref type="figure" target="#fig_3">Figure 4c</ref>) takes approximately 113.90 seconds t o render using only view frustum culling (1.17 frames/second average). We created two textures representing the geometry i n the distance and it took only 17.29 seconds to traverse the same recorded path (7.69 frames/second average; we introduced a third texture and the frame rate increased to an average of 9.47 frames/second).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS &amp; FUTURE WORK</head><p>Visibility culling algorithms alone cannot simplify models when a large subset of a model is still visible. For example, models consisting of many rooms are well suited for visibility culling algorithms, but if the geometry inside one room is still very complex there is inherently a large amount of geometry in the view frustum. Object simplification requires highly structured models in order to separate the model into objects. Furthermore, in scenes where there is high visual complexity, object simplification is not always sufficient.</p><p>The method presented here is able to simplify models and increase performance in the cases where visibility culling breaks down. Furthermore, it does not require highly structured models as with object simplification. Scenes can be rendered quickly regardless of visual complexity and we are able t o achieve a rendering time proportional to the amount of nearby geometry and the number of textures used.</p><p>We are currently exploring algorithms to decide automatically when to perform the transitions. A cost-benefit style function <ref type="bibr" target="#b5">[6]</ref> determines when and where in the scene the transitions should occur in order to maintain an interactive frame rate. This will enable us to visualize complex models while automatically "caching" distant geometry into texturebased representations.</p><p>Furthermore, we are developing methods to (quickly) measure the perceptual error introduced by the morphing operations. This will help us to decide when a new texture i s needed (since the texture-caches are only valid for the local view area) and how to perform any necessary morphing operations, including higher order interpolation of geometry near and in front of the textures.</p><p>In addition, we are investigating ways to remove the restriction of using static lighting. For example, by including per-texel normals and other information it might be possible to recompute the shading for the geometry represented by the texture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Model Partitioning Strategy. Each box corresponds to a space-partitioning box. The boxes are classified: near, culled and morphed. Intersected boxes can be optionally partitioned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Morphing Sequence. A geometry-to-texture transition goes from (a) to (c). At the end of the transition, the texture is introduced. A texture-to-geometry transition goes from (c) to (a). From the texture sample point, the objects look the same at all times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Multiple Textures. (a) Common viewpoint and constant distance. (b) Different viewpoints and distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b11">[12]</ref>) i s hard to apply. The model has 205,536 primitives (triangles). The space partitioning algorithm divided the model into a(a) Outline of pipes model and the path through the model (3 textures present). (b) Radiosity-illuminated church model and the recorded path (3 textures). The viewer is always looking in the general direction of the textures. (c) Auxiliary machine room model with its flythrough path (2 textures). The path is traversed in both directions but always looking towards the complex region of the model. In all figures, the 'x' marks the spot where the corresponding color figure was taken.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. The vertices of the morphed geometry are similarly returned to their original position. Again, since space partitioning and view frustum culling are used, only the visible geometry is actually morphed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Courtesy of Lightscape Technologies Inc.<ref type="bibr" target="#b1">2</ref> Courtesy of Electric Boat Division, General Dynamics Corporation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work would not have been possible without the support and advice of my advisor, Anselmo Lastra. I would also like to thank Gary Bishop, Frederick J. Brooks, Bill Mark, Michael North and Peggy Wetzel for her great job helping me with the video. This work was supported in part by NSF MIP-9306208 and ARPA Order No. A410.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards Image Realism with Interactive Update Rates in Complex Virtual Building Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Airey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">View Interpolation for Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIGGRAPH &apos;93)</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">QuickTime VR -An Image-Based Approach to Virtual Environment Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">S E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIGGRAPH &apos;95)</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical Geometric Models for Visible Surface Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="547" to="554" />
			<date type="published" when="1976-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simplification of Objects Rendered by Polygonal Approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehaemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zyda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive Display Algorithm for Interactive Frame Rates During Visualization of Complex Virtual Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sequin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Proceedings (Proceedings of SIGGRAPH &apos;93)</title>
		<imprint>
			<publisher>ACM SIGGRAPH</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Portals and Mirrors: Simple, Fast Evaluation of Potentially Visible Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Georges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="105" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual Navigation of Large Environments Using Textured Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Interactive 3D Graphics</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Plenoptic Modeling: An Image-Based Rendering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIGGRAPH &apos;95)</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-resolution 3D Approximations for Rendering Complex Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rossignac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">;</forename><surname>Borrel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Ibm T</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watson Research</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Center</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-02" />
			<pubPlace>Yorktown Heights, NY</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hierarchical Image Caching for Accelerated Walkthroughs of Complex Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snyder</surname></persName>
		</author>
		<idno>TR#UW-CSE-96-01-96</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>University of Washington CSE Dept</orgName>
		</respStmt>
	</monogr>
	<note>to appear in Proceedings of SIGGRAPH &apos;96</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<idno>TR#92/708</idno>
		<title level="m">Visibility Computation in Densely Occluded Polyhedral Environments</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
		<respStmt>
			<orgName>UC Berkeley CS Dept</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
