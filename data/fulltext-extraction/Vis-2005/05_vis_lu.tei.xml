<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Example-based Volume Illustrations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidong</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University &amp; UNC Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Example-based Volume Illustrations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-color</term>
					<term>shading</term>
					<term>and texture Volume Illustration</term>
					<term>Example-based Rendering</term>
					<term>Wang Cubes</term>
					<term>Texture Synthesis</term>
					<term>Color Transfer</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Scientific illustrations use accepted conventions and methodologies to effectively convey object properties and improve our understanding. We present a method to illustrate volume datasets by emulating example illustrations. As with technical illustrations, our volume illustrations more clearly delineate objects, enrich details, and artistically visualize volume datasets. For both color and scalar 3D volumes, we have developed an automatic color transfer method based on the clustering and similarities in the example illustrations and volume sources. As an extension to 2D Wang Tiles, we provide a new, general texture synthesis method for Wang Cubes that solves the edge discontinuity problem. We have developed a 2D illustrative slice viewer and a GPU-based direct volume rendering system that uses these non-periodic 3D textures to generate illustrative results similar to the 2D examples. Both applications simulate scientific illustrations to provide more information than the original data and visualize objects more effectively, while only requiring simple user interaction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Scientific illustrations play an essential role in education and training. For example, illustrations are a vital medium in teaching anatomy, explaining biological processes, highlighting anomalies, and explaining surgical procedures. They are commonly used as examples to explain structures (e.g., shape, size, appearance, etc.) and provide stylized or additional information to acquired datasets (e.g., CT and MRI). Therefore, exploring an example-based rendering method can adapt traditional illustration techniques to more effectively present information and provide a familiar environment to those who have been trained with similar images for years.</p><p>Because of the inevitable information loss during the acquisition of scientific datasets, many illustrators employ textures to enrich object details. These textures usually provide more information beyond the original data resolution and improve the understanding of the real objects. For example, <ref type="figure" target="#fig_0">Figure 1</ref> shows an illustration of a slice through the upper abdomen and a corresponding magnetic resonance image (MRI) <ref type="bibr" target="#b26">[27]</ref>. The illustrative section provides much more information than the MRI by adding fine structural detail and clearly delineating organs with accepted conventions, such as drawing the veins in blue and arteries in red <ref type="bibr" target="#b12">[13]</ref>.</p><p>Although scientific illustrations do not exactly replicate real subjects, the illustrators follow specific methodologies and procedures to concisely, accurately, and effectively convey the important aspects of the subject, such as shape, location, orientation, and structure. <ref type="figure" target="#fig_1">Figure 2</ref> shows two pairs of scientific illustrations and high- * e-mail: {alu,ebertd}@purdue.edu resolution medical images from the Visible Human Project <ref type="bibr" target="#b0">[1]</ref> comparing the same organs of the human body. These examples demonstrate a strong similarity between scientific illustrations and the real subjects. They also show that illustrators usually modify and/or simplify the size or position of a real subject to achieve a coherent structure, and change real colors to distinguish one object from the surroundings and improve understanding. For example, in <ref type="figure" target="#fig_1">Figure  2</ref>(a), the geometry and detail of the spinal disk (in blue) has been simplified since it is not the focus of the illustration. On the whole, these examples demonstrate the strong ability of scientific illustrations to provide expressive and additional information compared to medical images. Similar to scientific illustrations, the utilization of textures can provide more information and serve as an additional method to distinguish different objects, matching the visualization objective of feature rendering and exploration. However, the large memory requirements and the need for both high-resolution 3D texture synthesis methods and tedious user interaction limit the rendering features, resolutions, and styles.</p><p>In this paper, we present a method to generate illustrative renderings of volume datasets using example illustrations and photographs. Our method is composed of a texture synthesis and color transfer method to generate illustrative 3D textures. Once the illustrative textures are generated, volume datasets of similar subject matter can be interactively rendered using any of these illustrative textures. As in scientific illustrations, these 3D textures can be used in volume applications to enrich detail and achieve illustrative and artistic rendering styles. Specifically, to emulate example 2D illustrations, the 3D textures are automatically generated by recoloring available 3D samples using color distributions. Wang Cubes <ref type="bibr" target="#b4">[5]</ref> are then synthesized from the colored 3D samples and used to generate non-periodic volumetric textures. The textures for all the materials/objects are efficiently mapped into a volume, enriching both detail and the rendering, while using a small amount of texture space.</p><p>For volume illustration, volume textures of real or similar objects are chosen as input to provide meaningful information. First, their colors are transferred using selected sample illustrations to generate illustrative textures (Section 3). Next, the illustrative textures are used to synthesize a set of Wang Cubes (Section 4). These synthesized Wang Cubes can tile any sized volume texture and are used in interactive rendering (Section 5). To match object features, we generate one texture for every object in the volume. The users are only required to provide the input textures and illustration examples prior to the interactive rendering.</p><p>We have developed two applications using these new methods.</p><p>Please see supplementary material on conference DVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE Visualization 2005</head><p>October 23-28, Minneapolis, MN, USA 0-7803-9462-3/05/$20.00 ©2005 IEEE. Our real-time 2D slice viewer allows users to create continuous illustrative slice views for any cut direction through the volume, while our GPU-based direct volume renderer generates volumetric dataset illustrations, where the illustration styles and effects can be interactively modified through multiple transfer functions. Both applications take advantage of the abilities of scientific illustrations to effectively convey object properties, and save tedious user interaction through our new example-based texture generation and color transfer methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Volume illustration techniques take advantage of the effectiveness of scientific and artistic illustrations to improve volume visualizations in many aspects. For instance, Kirby et al. <ref type="bibr" target="#b14">[15]</ref> combined multiple data values to generate 2D flow images by utilizing concepts from paintings. Saito explored the usage of simple primitives on isosurfaces to depict volume shapes <ref type="bibr" target="#b22">[23]</ref>. Owada et al. <ref type="bibr" target="#b19">[20]</ref> presented an approach for polygon meshes using texture synthesis on a surface. Here we concentrate on general illustrations of volume datasets, extending the early work of combining non-photorealistic rendering (NPR) and volume rendering techniques to enhance important features and regions <ref type="bibr" target="#b6">[7]</ref>. Artistic rendering effects were achieved by implementing artistic procedures and simulating illustration techniques <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>. Recently, these techniques have been extended with hardware-accelerated rendering techniques to render large datasets and produce high quality results <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>. Wang Tiles <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> are used to generate non-periodic textures in computer graphics by following their placing rules. Analogous to Wang Tiles, Culik and Kari <ref type="bibr" target="#b4">[5]</ref> introduced Wang Cubes with colored faces. Several researchers have used Wang Tiles and Wang Cubes to generate textures and patterns in computer graphics, including Jos Stam <ref type="bibr" target="#b25">[26]</ref> for water simulation, Neyret and Cani <ref type="bibr" target="#b18">[19]</ref> for stochastic surface tiling, Cohen et al. <ref type="bibr" target="#b3">[4]</ref> for non-periodic image generation, Sibley et al. <ref type="bibr" target="#b23">[24]</ref> for video synthesis and geometry placement, and Lu et al. <ref type="bibr" target="#b15">[16]</ref> for illustrative volume rendering using geometric primitives.</p><p>Example-based approaches have been used in many fields, such as machine translation, image processing, and rendering. Because of their ability to "learn" from examples, these approaches take advantage of useful information that is difficult to summarize, and simplify user interaction. Within this field, we are especially interested in two categories, example-based rendering and texture synthesis. Example-based rendering generates new results by simulating the drawing styles of examples. Freeman et al. <ref type="bibr" target="#b7">[8]</ref> used Markov random fields to learn the transformation from captured image to scene interpretation. Hertzmann et al. <ref type="bibr" target="#b11">[12]</ref> presented the image analogy framework. Drori et al. <ref type="bibr" target="#b5">[6]</ref> and Hamel and Strothotte <ref type="bibr" target="#b9">[10]</ref> generated new images by extrapolating multiple example style fragments. The example-based idea has also been explored for texture synthesis methods in various applications, including the work of Haro and Essa <ref type="bibr" target="#b10">[11]</ref> and Chen et al. <ref type="bibr" target="#b2">[3]</ref>. Also, Wang and Mueller <ref type="bibr" target="#b30">[31]</ref> generated missing details from available high-resolution data for virtual microscopy by matching the data source details on multiple levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COLOR TRANSFER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem and Assumptions</head><p>To simulate arbitrary illustration examples for volume applications, we must solve the problem of synthesizing high resolution threedimensional textures from two-dimensional illustration examples. Achieving high quality 3D textures from 2D examples is a difficult task <ref type="bibr" target="#b31">[32]</ref>. Another limiting factor is that many texture examples have insufficient resolution for texture synthesis since they are cropped from illustrations and bounded by the size of the real objects. On the other hand, these small texture examples provide useful color distributions. Since scientific illustrations usually represent important, real object features, it is common to see strong similarities between the illustrations and the real objects, and between two illustrations of the same objects even when they use different drawing styles ( <ref type="figure" target="#fig_1">Figure 2</ref>). We can use these similarities to change the problem of three-dimensional texture synthesis to a color transfer problem by using available 3D source textures (e.g., color section volumes, MR, CT scalar volumes).</p><p>To simulate the styles of example illustrations, we transfer both the chromatic (two channels) and luminance values from the illustrations to the source dataset. Since a 3D texture is treated as an array of color (or grey-scale) information, the color transfer problem has no fundamental differences from color transfer between 2D images. Our work is based on Reinhard's method <ref type="bibr" target="#b20">[21]</ref>, which transfers colors between colored images by mapping the mean and standard deviations along each color channel and uses distance-weighted colors from separate swatches to improve the results, since the quality depends on the composition similarity between the example and source images. To save some color blending, Welsh et al. <ref type="bibr" target="#b32">[33]</ref> transfer colors between corresponding swatches and they target transferring colors to grey scale images by matching luminance and texture information. The effectiveness of the result is highly dependent on the user's selection of corresponding regions in each image. However, the user interaction required by their method to adjust the corresponding regions manually becomes much more difficult and tedious for 3D textures. Therefore, we present a fully automatic color transfer method for 3D volumes from user-provided example photographs or illustrations to source data volume.</p><p>In medicine and many biological fields, photographic volumes for many subjects are readily becoming available (e.g., The Visible Human Project). Our technique works for transferring color from 2D sources to three-dimensional volume datasets using these color volumes when available and also for directly transferring from the 2D source images to more readily available scalar volumes (e.g., high resolution CT datasets) by considering the scalar volumes as grey-scale color volumes.</p><p>Our solution is composed of three steps: clustering, mapping, and transferring. We use the following two assumptions based on similarities between the example images and source volumes:</p><p>1. Simplicity: Illustrations can improve our understanding of the subject by omitting some unnecessary details. Therefore, we assume an illustration is a simplified drawing of the real object. Since illustrations usually employ different colors for different objects, we assume that if two objects do not have the same colors in the example, they will not share the same colors in the source. This assumption is unavoidable in an automatic method with no user interaction.</p><p>2. Similarity: Scientific illustrations often effectively capture object features, including relative area and volume propor- tions. Therefore, we assume that object distributions are similar between the sources and examples. This assumption is exhibited by most illustrations and we only need a rough correspondence for the mapping step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Color Transfer Process</head><p>During the clustering step, we find color cluster sets using color distribution information from the example image and source volume, respectively. The color histograms are gathered from the whole texture for the 3 channels of the lαβ color space <ref type="bibr" target="#b21">[22]</ref>. We use this color space because of the independence between the luminance and two chrominance channels. Since the color transfer is based on the means and variances of the clusters, a set of Gaussian functions is fit onto the color histograms. These functions correspond to the color clusters in the textures. For example, <ref type="figure" target="#fig_2">Figure 3</ref>(g) and (h) show the clustering results for the liver volume (a) and an illustration example (b), in which the red line corresponds to the liver body and the black line corresponds to the veins and arteries. Therefore, each function must have the same area on the three color distributions, although different shapes are possible. The best parameters for the set of quantized Gaussian functions (center locations, weights, and heights) are searched under this area restriction. The algorithm starts with the set of one function and stops when the maximum error is smaller than a user-specified threshold (e.g., 5%). Based on the first assumption, the color distribution of the source has at least as many clusters as the example. Therefore, we first fit the example image, then the source volume until both the error threshold is satisfied and the number of clusters is greater than or equal to that of the example. For the clustering process, the color histograms are scaled from 0 to 1. The mapping step finds a correspondence map from the source color clusters Based on assumption two, this mapping should satisfy similar object distributions between the examples and sources. Assuming S = {s i = Area(G si )} and E = e j = Area(G e j ) are the normalized area sets of the color clusters of the source and example, respectively. The mapping problem can be described as finding a multiple-to-one mapping f (S) → E for the minimization of the following mapping error:</p><formula xml:id="formula_0">G s = {G si , i = 1 • • • N s }</formula><formula xml:id="formula_1">M Error = N s ∑ i=1 (s i − f (s i )) 2 , where N s ∑ i=1 s i = 1, N e ∑ j=1</formula><p>e j = 1, and N s ≥ N e .</p><p>(1) When N s = N e , this map is a one to one correspondence. The mapping errors from the N s factorial combinations are calculated to find a solution with the minimum error. Because of the algorithm complexity, a greedy algorithm can be used to produce a relative optimization solution. The two cluster sets S and E are sorted separately, and then the two clusters are mapped from the source to the example directly if they share the same sequence in their clusters, as f (s i ) = i. The mappings are designed randomly if multiple items with the same value exist, which rarely happens with real data.</p><p>When N s &gt; N e , the map is a multiple to one correspondence, which means the examples omit some of the details by merging multiple real objects into the same region. We generate intermediate source cluster set S with size N e by merging clusters together. Then, we use the N s = N e case to find the mapping error for each intermediate cluster set, and choose the mapping f with the minimum mapping error. The mapping function f can be easily built from f by reversing the merging sequence.</p><p>After building the correspondence map, we perform the color transfer on the three lαβ channels at the same time, since they are combined in the color cluster sets. For each voxel c v in the source volume, we first calculate the distance d i from c v to each center of the source color clusters E si in 3D. The norm D i is calcuated by using the value ranges V to balance the three channels. Then, we calculate the transferred color c i to every source cluster from the corresponding example cluster. The final transferred color vector C is calculated by using an inverse function q() of the distance</p><formula xml:id="formula_2">D i . C = ∑ N s i=1 ( c i * q(D i )) ∑ N s i=1 (q(D i ))</formula><p>, where</p><formula xml:id="formula_3">D i = ∑ j=l,α,β (d i, j /V j )<label>(2)</label></formula><p>and</p><formula xml:id="formula_4">c i, j = (c v, j − E si, j ) * (σ e f (s i ), j ) σ si, j + E e f (s i ), j , j = l, α, β (3)</formula><p>Since scalar volumes only have a luminance distribution, the cluster set of the source is searched using this channel, and the mapping is still built based on cluster area proportions. During the transfer step, D i is calculated as |c v − E si |. The color vector c v is composed by the source scalar value as [c v , c v , c v ] and so are the mean E si and deviation σ si . The final transferred color C is calculated using the rest of the equations (2) and (3) as for a colored volume. <ref type="figure" target="#fig_2">Figure 3</ref> (c)-(f) show the color transfer results from (b) to (a) with different source and example cluster numbers. Since (e) and (f) save some color blending by transferring colors between the corresponding regions, they produce more vivid colors than (c, d). The result (e) is dependent on the matching of the correspondences, which requires user interaction to achieve a satisfying result. With an automatic process to detect the clusters from color distributions, the cluster numbers and parameters can be calculated more accurately; thereby producing a natural color range in (f) which matches the average color tone of the examples. Using the two assumptions based on the similarity between examples and sources, we are able to build the correspondence between the clusters automatically to save user interaction. As the 4 other results for colored and scalar volumes shown in <ref type="figure" target="#fig_4">Figure 4</ref> illustrate, this method can be used to generate 3D textures for a wide range of examples and source volumes, including artificial 3D textures <ref type="bibr" target="#b13">[14]</ref>. <ref type="figure" target="#fig_10">Figure 9</ref> and 10 demonstrate 13 results (6 for the hands and 7 for the abdomen) on 10 different objects. Although we only transfer colors between one example and one source, multiple examples or sources will also work since our method only needs the color distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Color Transfer Results and Discussions</head><p>While texture synthesis methods may produce more continuous textures, they are not practical for small examples. We choose to use a color transfer approach instead of texture synthesis because of the similarities between scientific illustrations and real objects ( <ref type="figure" target="#fig_1">Figure 2</ref>), and these results also demonstrate the effectiveness of illustrations capturing at the subject features. This approach may be used for other types of source images, with the requirement of the similarities between the sources and examples. However, the two assumptions impose some limits on their application. For instance, if the blood vessels in CT data have the same values, the color transfer process works, but cannot separate veins from arteries automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TEXTURE SYNTHESIS FOR WANG CUBES</head><p>The requirements of large memory space and long synthesis time for high-resolution 3D textures limit their use in volume applications. Therefore, we use Wang Cubes, the 3D extension of 2D Wang Tiles, to overcome both issues and create non-periodic textures for our illustrative renderings. In this section, we present an automatic cube synthesis method from a 3D sample volume, discuss an edge discontinuity problem with Wang Cubes, and give two methods to solve the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Automatic Cube Synthesis</head><p>Wang Tiles are square tiles with "colored" edges. They are placed on a plane edge-to-edge only if the adjacent edges share the same "color". Similarly, Wang Cubes are cubes with "colored" faces in the sense that two cubes can be put together only if the adjacent faces have matching "colors." Let's denote the cube faces as N, S, W, E, F, and B, as shown in <ref type="figure" target="#fig_6">Figure 5</ref>(h). Since Wang Cubes are not supposed to be rotated, two faces in the same direction (NS, WE, and FB) must share one set of colors, while the face colors on different directions are independent. <ref type="figure" target="#fig_6">Figure 5</ref>  cube with a color to show the tiling is non-periodic. (c) shows the same set of Wang Cubes (a) filled with 3D textures and (d) shows the composed larger volume texture by putting the cube contents (c) into the corresponding positions of the cube tiling in (b). The essential point of using Wang Cubes is that the cube face colors are only used to generate the non-periodic tilings, while the contents of the cubes can be filled with arbitrary textures. Once the cube contents are generated, new non-periodic 3D textures can be quickly generated using these cube tilings.</p><p>For 2D Wang Tiles, Cohen et al. <ref type="bibr" target="#b3">[4]</ref> construct each tile by finding cutting paths to combine the four sample diamonds that correspond to the edge colors of the tile, as shown in <ref type="figure" target="#fig_6">Figure 5</ref>(e). Sibley et al. <ref type="bibr" target="#b23">[24]</ref> and Lu et al. <ref type="bibr" target="#b15">[16]</ref> extend the 2D tile generation to 3D cubes by using an octahedron to correspond to a face color and synthesizing a cube with 6 octahedra ( <ref type="figure" target="#fig_6">Figure 5(f)</ref>). Four different corner-tocorner synthesis processes are needed to distinguish the synthesis directions during the cube generation. Here, we make the following modifications to simplify this method. Replacing the octrahedra, a "face volume" is randomly chosen from the sample volume and corresponds to a cube face color. The face volumes have the same size as the Wang Cubes and are used in a similar way as the octahedra to compose a cube. Similar to the generation of 2D tiles, the usage of the face volumes changes the synthesis problem along the cube faces into synthesis inside each cube. Since the cutting surfaces inside a cube do not need to follow a special synthesis shape, two half elliptical spheroids are used to restrict the quilting regions between an initially randomly selected cube and a face volume for each cube face. <ref type="figure" target="#fig_6">Figure 5</ref>(g) shows this process for the W face of a cube. As in Sibley et al. <ref type="bibr" target="#b23">[24]</ref> and Wang and Mueller <ref type="bibr" target="#b30">[31]</ref>, we adopt the graph cut algorithm which uses max-flow (min-cut) <ref type="bibr" target="#b1">[2]</ref> for 3D texture synthesis. This process is repeated for each face of every cube until the accumulated synthesis errors E i along all the cutting surfaces inside the cubes are below a desired threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Edge Discontinuity Problem</head><p>However, both direct and simplified cube generation methods have an edge discontinuity problem, where the textures are discontinuous on the cube edges along the cube faces. As the 2 × 2 × 1 cube tiling in <ref type="figure" target="#fig_7">Figure 6(a)</ref> shows, the adjacent faces of the four cubes share the same colors and, therefore, the same textures. Since A(E) and C(E) come from two randomly selected samples, they are not necessarily continuous at the edge a1 along this cube face. <ref type="figure" target="#fig_7">Figure 6</ref>(b) shows that this problem generates a texture with obvious discontinuities on all the cube edges, although the textures are continuous inside each cube and within the adjacent faces. Therefore, considering only the face colors is not sufficient to generate an everywhere continuous 3D texture for Wang Cubes.</p><p>For the synthesis of general Wang Cube sets, we add an additional modification phase for the face volumes to ensure edge continuity. Since the 6 faces of a cube are independently designed, an edge in the cube tiling may be adjacent to any face color in the two vertical directions. When the joints of 2 arbitrary faces are not continuous, the cube tiling cannot guarantee edge continuities. Because of this face independence, the cube edges must be made the same in each direction to assure edge continuity. Therefore, for each of the NS, WE, and FB directions, an "edge volume" is randomly chosen from the sample volume and tied to this direction. For the face volumes that correspond to the colors in the WE directions, the NS and FB edge volumes are used to modify the 4 edge regions of the WE middle plane. <ref type="figure" target="#fig_7">Figure 6(c)</ref> shows the NS edge volumes for this case. Similar to the synthesis of a cube with face volumes, a quilting surface is searched within the two half cylinders between the face and edge volumes. The face volumes corresponding to the colors on the NS and FB directions are synthesized with WE-FB and NS-WE edge volumes respectively. The newly generated face volumes are used to generate the cubes using the previous method, thereby guaranteeing texture continuity.</p><p>An alternative method for textures without sharp boundaries is to introduce an edge error factor into the synthesis process. The accumulated synthesis errors, E s , for a cube set are the weighted sum of the inside errors, E i , decided previously and the newly added edge errors, E e :</p><formula xml:id="formula_5">E s = p i × E i + p e × E e</formula><p>. E e is used to measure the average edge discontinuties for the composed textures. We assume each cube is equally randomly picked during the cube tiling generation. Since the discontinuities along an edge come from the 2 vertical pairs of adjacent cube faces, for example, A(E)-C(E) and C(N)-D(N) for the edge a1 − b2 − c3 − d4 in <ref type="figure" target="#fig_7">Figure 6</ref>(a). For each face of a cube, the errors of the 4 edges are calculated from all the cubes which have the same color on the adjacent face, so that the 2 face errors of the 12 edges from all the combinations are collected once and only once. To balance the inside and edge errors, their weights p i and p e are set as the inverse of their cutting path size. This simple modification works well for many example textures. <ref type="figure" target="#fig_7">Figure 6</ref> shows two synthesized volumes of a liver (d) and a heart (e) with a set of 16 3 cubes in a 8 3 cube tiling. The liver uses the extra edge volume fixing phase and the heart uses the edge error factor approach. Both methods take around 10 to 20 minutes to generate a 16 cube set with 16 3 cubes, while arbitrary sized new 3D textures are composed with a few seconds.</p><p>Instead of faces, the cubes can also be colored by corners. We can choose "corner volumes" that correspond to the corner colors and synthesize a cube with them in a similar way as the "face volumes." While this ensures texture continuity for the whole space, the minimum cube set will be increased to 128 cubes <ref type="bibr" target="#b15">[16]</ref>.</p><p>The tradeoff of using Wang Cubes to save texture memory is the repetitive appearance of the synthesized textures. Since a set of Wang Cubes has only a limited number of samples, it is inevitable that these samples will repeat themselves somewhere in the volume. This issue is also discussed for Wang Tiles in <ref type="bibr" target="#b3">[4]</ref>. Therefore, Wang Cubes are especially useful to generate textures with similarities because of their non-periodic tiling. The synthesized volumes using Wang Cubes are not limited to homogeneous textures <ref type="bibr" target="#b15">[16]</ref>, and this repetitive appearance issue can be improved by larger cube sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VOLUME APPLICATIONS</head><p>Simulating scientific illustrations, we have developed two volume applications that use our synthesized non-periodic 3D textures to enrich the details of the original datasets: a 2D slice viewer and for the FB edges of a WE face volume. (d) A liver texture generated using edge volumes. (e) A heart texture generated using the edge error factor. a direct volume rendering system. The systems basically use the voxel values in the volumes to be visualized to provide the shape and opacity for each material and object. The user-chosen example illustration provides the colors and textures to generate threedimensional textures used for rendering. To generate these detail textures for each object, a high-resolution 3D scalar or color volume corresponding to a representative real object is chosen. For segmented volume datasets, the segmentation masks containing one object ID for each voxel can be used to specify the texture index. For unsegmented datasets, we can select the desired texel value using transfer functions to generate voxel opacities that are then blended with the texel values. Moreover, transfer functions can always be used to modify the shape and opacity of an object for both segmented and unsegmented datasets.</p><p>Once the illustrative detail textures are generated, they are used for visualizing any scalar volume containing similar objects (e.g., abdominal CT scans for all patients). The rendering style is mainly determined by these color transferred textures, but can be modified by several selected rendering parameters and transfer functions. For example, using interactive 2D transfer functions, we usually make the skin transparent, so that both the skin and the volume interior can easily be seen. Comparing our method with traditional volume visualization, the illustrative 3D textures are used to color the scalar volume, replacing the traditional color transfer function. Therefore, our method preserves the fidelity of the original datasets and provides informative additional detail. The design of both applications preserves the flexibility of the direct volume rendering approach to interactively select which objects in a volume are visualized through simple user interaction.</p><p>Our sample texture volumes are chosen from high resolution volume datasets, such as the full colored Visible Woman photographic and CT datasets <ref type="bibr" target="#b0">[1]</ref>. The datasets we used in <ref type="figure" target="#fig_8">Figure 7</ref>, 9, and 10 are segmented CT scalar volumes. <ref type="figure" target="#fig_9">Figure 8(b)</ref> is an unsegmented CT feet dataset. The selection of 3D texture samples proceeds by automatically searching the largest sample volume from the whole dataset, matching several standard statistical texture features with the user-selected sample images. The 3D texture samples can then be used directly to synthesize sets of cubes (producing rendered texture emulating the high resolution source volume), or after color is transferred from 2D example illustrations. We use one minimum Wang Cube set with 16 cubes for all the textures. Since all of them have the same cube face color set, one cube tiling for the whole volume is shared by all the synthesized cube sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">2D Slice Viewer</head><p>The 2D slice viewer uses the synthesized 3D textures and segmentation masks to generate illustrative slices. From the object ID of the segmentation masks, the pixel color is fetched from the corresponding composed 3D textures by using the transformed world position. Although 2D images can be generated with Wang Tiles or other 2D texture synthesis methods, extra operations, such as smoothing adjacent images, are needed to avoid jumping and popping effects when the user moves through the slices. 3D textures provide smooth transitions naturally and the slice can be rotated into any view direction at any position in the volume. Gradient information from the dataset can also be used in the lighting to create a relief texture effect. These illustrative slices are generated and rendered in real-time. <ref type="figure" target="#fig_8">Figure 7</ref> shows a series of hand slice images as the slice moves through the volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Direct Volume Rendering</head><p>We also built a GPU-based volume rendering system (slicing) to directly illustrate volume datasets. Since the cube tiling has the same size as the volume data, it is combined with the volume data and segmentation masks into one texture unit to save texture memory. Assuming M sets of cubes are used and C is the length of the cube side, all the cube textures are grouped into a grid of 4C × 4C × MC, where each cube is still stored as a volume so that tri-linear interpolation can be achieved. One problem that can occur is that the details from the composed textures may be too small to be seen if we only allow each cube to map onto one voxel. Therefore, we add a rendering scale parameter for each cube set, which corresponds to the voxel numbers in each direction. Additionally, some textures might not be seen clearly after rendering, mainly because of the lack of texture variations from the sample volumes and the usage of specific rendering settings. We sharpen the cube textures while maintaining the average colors to overcome this problem.</p><p>The main difficulty in implementation is the calculation of texture coordinates in the fragment program. Since the composed 3D textures (scaled by the rendering scale parameter) are stored separately as the synthesized cubes and a cube tiling, we need to calculate the transformed coordinates for each fragment. Also, since the cubes for all the textures are packed into one texture unit, the texture coordinates must be accurately mapped inside a cube instead of between the cubes. For each fragment, we calculate two positions from the world coordinates: the voxel center position and the relative shift of the current world position to the left-down-back corner of the voxel. Then, the current segmentation mask is used to retrieve the render scale for this object. The voxel center position and the render scale are used to calculate the cube index from the cube tiling. Finally, the relative position and the render scale are used to retrieve the color of the current position in the cube.</p><p>After retrieving the cube color, we can use silhouette enhancement and lighting effects <ref type="bibr" target="#b8">[9]</ref>, which are commonly used in illustrations, in addition to transfer functions to calculate the final color for the current world position. The two examples of the hand come from <ref type="bibr" target="#b24">[25]</ref> and the abdomens are from <ref type="bibr" target="#b26">[27]</ref>. The rendering time for these final, high-resolution renderings is around 1 second per frame for 500x500 images and 500 slices on a Nvidia Quadro FX 3400 graphics card.</p><p>The illustration in <ref type="figure" target="#fig_0">Figure 1</ref> includes more object details than the right MR image although they have the same size. Similarly, in our rendering, the source volume data has limited information, while we generate cube textures from high-resolution data and use them to enrich the previous volume data. To provide meaningful information, we usually choose corresponding cube textures for each object/region in a volume. For example, we choose liver textures to render the liver, while skin textures are used to render skin. These corresponding textures contain the information about object appearance, color, component, and shape and can be used to improve the understanding of the data for education and training.</p><p>To further simulate the example illustrations, we interactively adjust the rendering parameters. For instance, we add silhouettes in the volume rendering if there are silhouettes in the example, and we use more transparent skin in the volume rendering if the skin region is not the focus point in the example. In our direct volume rendering approach, the users can interactively adjust the lighting direction, transfer functions for opacities, and four rendering parameters (1 for render scale, 2 for lighting, and 1 for silhouette) per object. <ref type="figure" target="#fig_9">Figure 8</ref>, 9, and 10 have 2, 3, and 8 objects respectively. After the cubes are generated, it takes a skilled user a few minutes to visualize an unknown dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">User Interaction and Storage Requirements</head><p>Because of the ability to emulate examples by transferring their color distributions, our method requires much less user interaction than traditional rendering approaches. Only the 2D illustration and sample examples need to be selected by the user and the 3D textures are automatically generated. Our approach can render both segmented and un-segmented datasets; therefore, most user interaction occurs when adjusting the transfer functions and limited rendering parameters. Moreover, once the cube set is synthesized, it can be used to generate textures for the same or similar subject of any size and the cube sets are usually reusable. For example, the skin, vessel, and bone textures generated for the hand data <ref type="figure" target="#fig_10">(Figure 9</ref>) can be used to render the feet data <ref type="figure" target="#fig_9">(Figure 8(b)</ref>). What's more, once the cube set is synthesized, arbitrary sized 3D textures can be composed quickly.</p><p>Another advantage of Wang Cubes for volume rendering is that these non-periodic textures occupy much less texture space. Since the cube tiling is combined with the volume data and segmentation masks into one texture unit, it does not use an extra texture unit. The main memory difference is for storing the 3D textures. With Wang Cubes, the texture space of M sets of 16 cubes is M ×16×C 3 . To achieve the same effect of non-periodic textures without Wang Cubes, M volumetric textures having the same size of the dataset are needed using M × V 3 space. Therefore, the difference of the space requirement is 16C 3 /V 3 . In <ref type="figure" target="#fig_10">Figure 9</ref> and 10, both datasets are 256 × 256 × 128 and the cubes are 16 <ref type="bibr" target="#b2">3</ref> . Therefore, the required texture memory with Wang Cubes is only 1/128 of that required without Wang Cubes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND FUTURE WORK</head><p>This paper employs 3D textures to improve the visualization of volume datasets by enriching meaningful details and achieving illustrative styles. Based on the methods from scientific illustrations for conveying object features, we have built an automatic process to simulate illustration "rendering styles" using color transfer and illustration similarities. These "rendering styles" capture important illustration information, such as accepted conventions and usage of colors. By simulating these styles, the proposed method can generate expressive and pleasing rendering results with a much simpler process than by manual adjusting all parameters and colormaps.</p><p>To synthesize 3D textures from 2D examples is a very challenging task. By carefully observing the features of scientific illustrations, we use color transfer methods and available volume datasets to automatically generate illustrative 3D textures, which can generate high resolution 3D textures from small 2D examples with little user efforts. The usage of Wang Cubes significantly alleviates the issue of limited texture space. These synthesized 3D textures can be used in both surface-based methods and direct volume rendering. Our new rendering method keeps the advantage of direct volume rendering that a user can arbitrarily choose the object/region shape with transfer functions and change their focus of intent.</p><p>Since many medical anatomy textbooks and atlases uses illustrations in addition to medical images (such as CT or MR), we believe that this method is useful for educational applications. This method can be extended to several areas of 3D texturing and color transfer, including video synthesis and texture generation for other types of models.</p><p>We plan to further analyze and simulate the abilities of scientific illustrations to extract important object features and render volumetric datasets in a coherent and artistic way, such as highlystructured textures and illustrative lighting effects. Since the subtle effects of lighting and color usage around different portions of an object, some examples have inhomogeneous properties that affect the texture synthesis results. We plan to investigate new methods to remove or balance these issues. We also plan to explore artistic and scientific composition principles for adjusting rendering parameters to achieve the desired effects with much less user interactions.</p><p>Since texture memory size is a limited resource, we will further explore texture usage for volume rendering to provide more rendering styles for different applications, while accelerating the rendering speed by avoiding irrelevant texture memory accesses and texture coordinate manipulation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A scientific illustration shows more general information with colored textures than the corresponding MRI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The comparison of scientific illustrations (left, which are enlarged fromFigure 1) and high-resolution medical images from the visible woman (right) around the vertebra and liver. The images are scaled to match each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) A liver source volume. (b) An example cropped from the examples of Figure 10(b). (c)-(f) The color transfer results from (b) to (a) with different cluster numbers. (g)-(h) The clustering results of (a) and (b), while blue shows color histograms, and red and black show the clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>to the example color clusters G e = G e j , j = 1 • • • N e . Based on assumption one, each source cluster only corresponds to one example cluster; therefore, N s ≥ N e and there exists such a multiple to one mapping from G s to G e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The sources, examples, and color transfer results of colored volumes (top: fat and vessel) and scalar volumes (bottom: spleen and kidney).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a)-(d) show the usage of a 16 Wang Cube set. (a) shows a set of Wang Cubes with colored faces and 2 colors for each NS, WE, FB direction. (b) is a 8 3 tiling generated with the Wang Cube set in (a). We render each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>(a) A set of 16 Wang Cubes with 2 colors on each face. (b) A 8 3 cube tiling with each cube a color. (c) A set of synthesized cubes. (d) A composed 3D texture. (e) Tile generation using diamond samples. (f)Cube generation using an octahedron for the W cube face. (g) Cube generation using "face volumes" with two elliptical spheroids to restrict the synthesis shape for the W cube face. (h) The cube face directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>(a) A 2x2 cube tiling. (b) A result showing the edge discontinuity problem along the cube faces. (c) Edge volume usage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>A series of hand slices as the cutting plane moves through the volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>(a) Two CT slices of the hand and feet show the limited resolution of the original datasets. (b) Volume illustration of the unsegmented feet dataset. The skin and bone textures reuse the cube textures generated for Figure 9(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 (</head><label>9</label><figDesc>a) and 10(a) are generated from the full colored slices of the visible human by synthesizing cube textures directly from the input textures. Figure 9(b, c) and 10(b) are illustrations with color transferred textures from the corresponding illustrative examples. All the results share the same data resolution, but differ in style. To generate the image in Figure 9(c), an elliptical spheroid is used to gradually cut away the fat, while parallel cutting planes are used in generating Figure 10(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Volume rendering of a hand dataset. (a) is rendered with the cubes synthesized from the Visible Woman photographic dataset. (b) and (c) are rendered with recolored textures using their respective example illustrations. The three small images are cropped from the corresponding example regions and show the textures of fat, vessel, and bone, respectively. The silhouette colors in (b) and (c) are also selected from the examples and shown as the box colors around the samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Volume rendering of an abdomen dataset. (a) is rendered with the cubes synthesized from the Visible Woman photographic dataset. The three slices on the right show the lung, liver, and kidneys respectively. (b) is rendered with recolored textures using the corresponding examples from the two right illustrations.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>The datasets are courtesy of Visible Human Project and Tiani Medgraph. We would like to thank Lujin Wang for texture synthesis and Jingshu Huang for Gaussian fitting. This project is supported by the National Science Foundation under Grant Nos. 0081581, 0121288, 0222675, 0328984, 9978032.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The visible human project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="511" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An experimental comparison of mincut/max-flow algorithms for energy minimization in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shell texture functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="343" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wang tiles for image and texture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An aperiodic set of wang cubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Culik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarkko</forename><surname>Kari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J.UCS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="675" to="686" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Example-based style synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iddo</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezy</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Volume illustration: Non-photorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vis</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning lowlevel vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-quality two-level volume rendering of segmented data sets on consumer graphics hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Capturing and re-using rendition styles for non-photorealistic rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strothotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CGF</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="173" to="182" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exemplar based surface texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-11" />
			<pubPlace>Munich, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PROC. of SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Guild Handbook of Scientific Illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hodges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stereological techniques for solid textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jagnow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dorsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PROC. of SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="329" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing multivalued data from 2d incompressible flows using concepts from painting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="333" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Volume illustration using wang cubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mora</surname></persName>
		</author>
		<idno>TR-ECE-04-05</idno>
		<ptr target="https://engineering.purdue.edu/ECE/Research/TR/TR/2004.whtml" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hardware-accelerated parallel non-photoralistic volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In PROC. of NPAR</title>
		<imprint>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interactive volume illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rdiger</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VMV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pattern-based texturing revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Neyret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Paule</forename><surname>Cani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 1999</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Volume illustration: Designing 3d models with internal textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Owada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PROC. of SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CG&amp;A</title>
		<imprint>
			<biblScope unit="page" from="34" to="40" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistics of cone responses to natural images: Implications for visual coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Cronin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Chiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optical Soc. Of America</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2306" to="2045" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time previewing for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of symposium on Volume visualization</title>
		<meeting>of symposium on Volume visualization</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Wang cubes for video synthesis and geometry placement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Sibley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Marai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>SIGGRAPH Poster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gross Anatomy in the practice of medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Slaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mccune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lea and Febiger</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aperiodic texture mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stam</surname></persName>
		</author>
		<idno>R046</idno>
	</analytic>
	<monogr>
		<title level="j">European Research Consortium for Informatics and Mathematics</title>
		<imprint>
			<date type="published" when="1997-01" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sobotta Atlas of human anatomy</title>
		<editor>J. Staubesand and Anna N. Taylor</editor>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>Urban and Schwarzenberg Baltimore-Munich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Volumes of expression: Artistic modelling and rendering of volume datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M F</forename><surname>Treavett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Satherley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics International</title>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Proving theorems by pattern recognition ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Systems Technical Journal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Games, logic, and computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="page" from="98" to="106" />
			<date type="published" when="1965-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating sub-resolution detail in image and volumes using constrained texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Texture synthesis by fixed neighborhood searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transferring color to greyscale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Welse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PROC. of SIGGRAPH</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="277" to="280" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
