<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VolumeShop: An Interactive System for Direct Volume Illustration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bruckner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Graphics and Algorithms</orgName>
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Eduard</forename><surname>Gröller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Graphics and Algorithms</orgName>
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VolumeShop: An Interactive System for Direct Volume Illustration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation-Display algorithms; I.3.3 [Computer Graphics]: Picture/Image Generation-Viewing algorithms illustrative visualization</term>
					<term>volume rendering</term>
					<term>fo-cus+context techniques</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: Annotated direct volume illustrations of a carp. (a) The swim bladder is highlighted using cutaways and ghosting. (b) The swim bladder is displayed enlarged.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A considerable amount of research has been devoted to developing, improving and examining direct volume rendering algorithms for visualization of scientific data. It has been shown that volume rendering can be successfully used to explore and analyze volumetric data sets in medicine, biology, engineering, and many other fields. In recent years, non-photorealistic or illustrative methods employed to enhance and emphasize specific features have gained popularity. Although we base our paper on this large body of research, our focus is somewhat different. Instead of using these techniques to improve the visualization of volume data for common applications such as diagnosis, we want to combine existing and new methods to directly generate illustrations, such as those found in medical textbooks, from volumetric data.</p><p>Illustrations are an essential tool in communicating complex relationships and procedures in science and technology. However, the time needed to complete an illustration is considerable and varies widely depending on the experience and speed of the illustrator and the complexity of the content. The more complicated the subject matter, the longer it will take the illustrator to research and solve a complex visual problem. Different illustration methods and styles can also have a significant impact on the time involved in the creation of an illustration. Therefore, illustrators are increasingly using computer technology to solve some of these problems. This, however, is mostly restricted to combining several manually created parts of an illustration using image processing software.</p><p>Volume rendering has gained some attention in the illustration community. For example, Corl et al. <ref type="bibr" target="#b3">[4]</ref> describe the use of volume rendering to produce images as reference material for the manual generation of medical illustrations. We aim to take this development one step further. Our goal is to create a fully dynamic threedimensional volume-based illustration environment where static images have the aesthetic appeal of traditional illustrations. The advantages of such a system are manifold: Firstly, the whole process of creating an illustration is accelerated. Different illustration methods and techniques can be explored interactively. It is easy to change the rendering style of a whole illustration -a process that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>671</head><p>Please see supplementary material on conference DVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE Visualization 2005</head><p>October 23-28, Minneapolis, MN, USA 0-7803-9462-3/05/$20.00 ©2005 IEEE.</p><p>would otherwise require a complete redrawing. Moreover, the research process is greatly simplified. Provided that the object to be depicted is available as a volumetric data set, it can be displayed with high accuracy. Based on this data, the illustrator can select which features he wants to emphasize or present in a less detailed way. Illustration templates can be stored and reapplied to other data sets. This allows for the fast generation of customized illustrations which depict, for instance, a specific pathology. Finally, the illustration becomes more than a mere image. Interactive illustrations can be designed where a user can select different objects of interest and change the viewpoint.</p><p>This paper is subdivided as follows: In Section 2 we discuss related work. Section 3 gives a conceptual overview of our approach. In Sections 4, 5, and 6, we cover in detail the three fundamental building blocks of our direct volume illustration system, multi-object volume rendering, illustrative enhancement, and selective illustration, respectively. Section 7 discusses strategies and results for an efficient implementation of the presented concepts. The paper is concluded in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Non-photorealistic or illustrative rendering methods are a very active field of research. In volume visualization, Levoy <ref type="bibr" target="#b13">[14]</ref> was the first to propose modulation of opacity using the magnitude of the local gradient. This is an effective way to enhance surfaces in volume rendering, as homogeneous regions are suppressed. Based on this idea, Rheingans and Ebert <ref type="bibr" target="#b18">[19]</ref> present several illustrative techniques which enhance features and add depth and orientation cues. They also propose to locally apply these methods for regional enhancement. Using similar methods, Lu et al. <ref type="bibr" target="#b14">[15]</ref> developed an interactive volume illustration system that simulates traditional stipple drawing. Csébfalvi et al. <ref type="bibr" target="#b4">[5]</ref> visualize object contours based on the magnitude of local gradients as well as on the angle between viewing direction and gradient vector using depth-shaded maximum intensity projection. Lum and Ma <ref type="bibr" target="#b15">[16]</ref> present a hardwareaccelerated approach for high-quality non-photorealistic rendering of volume data. Exploring the variety of traditional illustration styles, selective emphasis of certain structures is an important technique. The concept of two-level volume rendering, proposed by Hauser et al. <ref type="bibr" target="#b7">[8]</ref>, allows focus+context visualization of volume data. Different rendering styles, such as direct volume rendering and maximum intensity projection, are used to emphasize objects of interest while still displaying the remaining data as context. Methods for combining multiple volume data sets have been investigated in the context of multi-modal data. For instance, Cai and Sakas <ref type="bibr" target="#b1">[2]</ref> discuss different methods for data intermixing in volume rendering. Wilson et al. <ref type="bibr" target="#b24">[25]</ref> propose a hardware-accelerated algorithm for multi-volume visualization. Leu and Chen <ref type="bibr" target="#b12">[13]</ref> present a system for modeling scenes consisting of multiple volumetric objects which is restricted to non-intersecting volumes. The approach by Grimm et al. <ref type="bibr" target="#b6">[7]</ref> uses alternating sampling for combining multiple volumes in dynamic scenes. An automated way of performing clipping operations has been presented by Viola et al. <ref type="bibr" target="#b22">[23]</ref>. Inspired by cut-away views, which are commonly used in technical illustrations, they apply different compositing strategies to prevent an object from being occluded by a less important object. Konrad-Verse et al. <ref type="bibr" target="#b9">[10]</ref> perform clipping using a mesh which can be flexibly deformed by the user with an adjustable sphere of influence. Zhou et al. <ref type="bibr" target="#b26">[27]</ref> propose the use of distance to emphasize and de-emphasize different regions. Lum and Ma <ref type="bibr" target="#b16">[17]</ref> use two-dimensional scalar-based lighting transfer functions to enhance material boundaries using illumination. Volume sculpting, proposed by Wang and Kaufman <ref type="bibr" target="#b23">[24]</ref>, enables interactive carving of volumetric data. Islam et al. <ref type="bibr" target="#b8">[9]</ref> discuss methods for spatial and temporal splitting of volume data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>The architecture of VolumeShop, our direct volume illustration system, discriminates between two basic types of volumes: data volumes and selection volumes. A data volume stores the actual scalar field, for example acquired by a CT scanner. A selection volume specifies a particular structure of interest in a corresponding data volume. It stores real values in the range [0,1] where zero means "not selected" and one means "fully selected". While both multiple data and selection volumes can be defined, only one pair is active at a time. Both volumes are stored in a bricked memory layout using reference counting, i.e., they are subdivided into small cubes which are accessed using an index data structure. Redundant information is not duplicated, thus, if two bricks contain the same data, they are stored in memory only once. The copy-on-write idiom is used for handling modifications. This is most useful for the selection volume due to its sparse nature. Furthermore, several pieces of meta information (e.g., min-max octrees, bounding boxes, and transformations) are stored for both volumes and updated on modification. This allows, for instance, the quick extraction of tight bounding volumes, which are used to skip empty space during rendering. At the heart of the system lies a multi-object volume rendering algorithm which is responsible for the concurrent visualization of multiple user-defined volumetric objects. It makes use of illustrative enhancement methods and selective illustration techniques defining the visual appearance of objects. A conceptual overview of our system is given is <ref type="figure" target="#fig_0">Figure 2</ref>. In the following sections, we will describe each of these components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTI-OBJECT VOLUME RENDERING</head><p>When illustrating a volumetric data set, we want to enable interactive selection and emphasis of specific features. The user should be able to specify a region of interest which can be highlighted and transformed, similar to common image editing applications. We also want to permit arbitrary intersections between objects and control how the intersection regions are visualized.</p><p>Our approach identifies three different objects for the interaction with a volumetric data set: a selection is a user-defined focus region, the ghost corresponds to the selection at its original location, and the background is the remaining volumetric object. A transformation T can be applied to the selection, e.g., the user can move, rotate, or scale this object. While the concept of background and selection is used in nearly every graphical user interface, ghosts nor- mally exist, if at all, only implicitly. In the context of illustration, however, such an explicit definition of a ghost object is advantageous.</p><p>We assume a scalar-valued volumetric function f V and a selection function f S , which are defined for every point p in space. The selection function f S has a value in [0, 1] which indicates the degree of selection. Based on this degree of selection we define three fuzzy selection sets S S , S G , and S B (see <ref type="figure" target="#fig_1">Figure 3</ref>, first row) with their respective membership functions µ S , µ G , and µ B :</p><formula xml:id="formula_0">µ S S (p) = f S (T (p)) µ S G (p) = f S (p) µ S B (p) = 1 − f S (p) (1)</formula><p>where T is the transformation that has been applied to the selection.</p><p>To control the appearance of our three objects, i.e., selection, ghost, and background, we define color and opacity transfer functions based on the values of f V , which we denote c S , α S , c G , α G , and, c B , α B . We use the opacity transfer functions to define the membership functions of three volume sets, V S , V G , and V B (see <ref type="figure" target="#fig_1">Figure 3</ref>, second row):</p><formula xml:id="formula_1">µ V S (p) = α S ( f V (T (p))) µ V G (p) = α G ( f V (p)) µ V B (p) = α B ( f V (p)) (2)</formula><p>For each of our three objects we can now define an object set as the intersection between the corresponding selection and volume set (see <ref type="figure" target="#fig_1">Figure 3</ref>, third row):</p><formula xml:id="formula_2">S = S S ∩V S G = S G ∩V G B = S B ∩V B (3)</formula><p>These sets correspond to our basic objects selection, ghost, and background. Thus, in the following we will use these terms to refer to the respective object sets and vice versa. For volume rendering, we now need a way to determine the color and opacity at a point p in space depending on its grade of membership in these sets. We assume n sets X 1 , X 2 , . . . , X n and their corresponding color transfer functions c 1 , c 2 , . . . , c n . We can then define the color at a point p as a weighted sum using the respective membership functions as weights:</p><formula xml:id="formula_3">c(p) = n ∑ i=1 c i (p)•µ i (p) n ∑ i=1 µ i (p) (4)</formula><p>As the membership functions of our sets are based on the opacity and the degree of selection, we define the opacity at p as the grade of membership in the union of all sets:</p><formula xml:id="formula_4">α(p) = µ X 1 ∪X 1 ∪...∪X n (p)<label>(5)</label></formula><p>Using Equations 4 and 5 for our three sets S, G, and B and the color transfer functions c S , c G , and c B leads to a meaningful combination of colors and opacities when used in direct volume rendering. However, we want to provide the user with additional control over the appearance of regions of intersection. Frequently, for example, illustrators emphasize inter-penetrating objects when they are important for the intent of the illustration.</p><p>To achieve this we first need to identify potential regions of intersection. According to our definitions B ∩ G = / 0, i.e., background and ghost never intersect. The selection, however, can intersect either the background, the ghost, or both. Thus, we direct our attention to the sets GS = G ∩ S and BS = B ∩ S . For every point which is a member of one of these sets, we want to be able to specify its appearance using special intersection transfer functions for color and opacity. Thus, we define two new sets V GS and V BS with the following membership functions:</p><formula xml:id="formula_5">µ V GS (p) = α GS ( f V (p), f V (T (p)) µ V BS (p) = α BS ( f V (p), f V (T (p))<label>(6)</label></formula><p>The intersection transfer functions are two-dimensional. Their arguments correspond to the value of volumetric function f V at point p and at T (p), the value of the function at p transformed by the selection transformation T . Based on these two sets, we now define two alternative sets GS and BS for the regions of intersection:</p><formula xml:id="formula_6">µ GS (p) = 0 µ GS (p) = 0 µ S G ∩S S ∩V GS (p) otherwise µ BS (p) = 0 µ BS (p) = 0 µ S B ∩S S ∩V BS (p) otherwise (7)</formula><p>To evaluate the combined color and opacity at a point p in space, we use Equation 4 and 5 with the sets S − ( GS ∪ BS), G − GS, B − BS, GS, and BS and the respective color transfer functions c S , c G , c B , c GS , and c BS . We use the standard definitions for fuzzy set operators where the minimum operator is used for the intersection and the maximum operator is used for the union of two fuzzy sets <ref type="bibr" target="#b25">[26]</ref>.</p><p>The intersection transfer functions can be used to control the color and opacity in the region of intersection between two objects based on the scalar values of both objects. In our implementation we provide a default setting which is an opacity-weighted average between the one-dimensional color transfer functions of the two respective objects (background and selection, or ghost and selection). Further, we provide presets where the opacity is computed from the one-dimensional opacity transfer functions by one of the compositing operators derived by Porter and Duff <ref type="bibr" target="#b17">[18]</ref>. The color can be <ref type="figure">Figure 4</ref>: Using intersection transfer functions to illustrate implant placement in the maxilla. As the selection (green) is moved into the ghost (faint red), the intersection transfer function causes it to be displayed in blue. specified arbitrarily. Additionally, the user can paint on the twodimensional function using a gaussian brush to highlight specific scalar ranges. <ref type="figure">Figure 4</ref> shows an example where the ghost/selection intersection transfer function is used to illustrate the placement of an implant in the maxilla. This kind of emphasis is not only useful for the final illustration, but can act as a kind of implicit visual collision detection during its design.</p><p>While we use the concept presented in this section for concurrent visualization of multiple objects derived from the same data set, this restriction is not necessary -objects could also be derived from multiple data sets. The approach could be straight-forwardly used for general multi-volume visualization. However, we note that the use of intersection transfer functions might not be feasible in a setup consisting of a large number of objects. Increasing the number of objects will quickly lead to a combinatorial explosion in the number of possible regions of intersection. In such a case the objects for which such a fine-grained control is required should be limited by application-specific constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ILLUSTRATIVE ENHANCEMENT</head><p>Illustration is closely related to non-photorealistic rendering methods, many of which attempt to mimic artistic styles and techniques. In this section we present a simple approach which integrates several presented models and is thus well-suited for a volume illustration system. Most illumination models use information about the angle between normal, light vector and viewing vector to determine the lighting intensity. In volume rendering, the directional derivative of the volumetric function, the gradient, is commonly used to approximate the surface normal. Additionally, the gradient magnitude is used to characterize the "surfaceness" of a point; high gradient magnitudes correspond to surface-like structures while low gradient magnitudes identify rather homogeneous regions. Numerous distinct approaches have been presented that use these quantities in different combinations to achieve a wide variety of effects. Our goal is to present a computationally inexpensive method which integrates many of these models.</p><p>We define a two-dimensional lighting transfer function. The arguments of this function are the dot product between the normalized gradientN and the normalized light vectorL and the dot product between the normalized gradient and the normalized half-way vector H, whereĤ is the normalized sum ofL and the normalized view vectorV . A two-dimensional lookup table stores the ambient, diffuse, and specular lighting contributions for everyN •L andN •Ĥ pair. Additionally, a fourth component used for opacity enhancement is stored. Shading is then performed by using these four values in the following way to compute the shaded color c s and shaded opacity α s : where c u and α u are the unshaded color and opacity, and s a , s d , and s s are the shading transfer function components for ambient, diffuse, and specular lighting contributions. The opacity enhancement component of the transfer function denoted by s α is combined with the gradient magnitude |N| to modulate the unshaded opacity value (we assume that the gradients have been scaled such that |N| is between zero and one).</p><formula xml:id="formula_7">c s = (s a (N •L,N •Ĥ) + s d (N •L,N •Ĥ)) • c u + s s (N •L,N •Ĥ) α s = (min(1, s α (N •L,N •Ĥ) + (1 − |N|))) −1 • α u (8) N LˆN ĤˆN LˆN Ĥ( a) (b) N LˆN ĤˆN LˆN Ĥ( c) (d)</formula><p>We use the terms "ambient", "diffuse", and "specular" to illustrate the simple correspondence in case of Phong-Blinn lighting. However, the semantics of these components are defined by the model used for generation of the lighting transfer function. Thus, a lighting transfer function might use these terms to achieve effects completely unrelated to ambient, diffuse, and specular lighting contributions. In a similar matter, when examining Equation 8 it can be seen that the ambient and diffuse components could be combined without loss. We only choose to keep them separate for the sake of consistency and simplicity.</p><p>It is straight-forward to use this kind of lighting transfer function for common Phong-Blinn lighting. However, many other models can also be specified in this way and evaluated at constant costs. For example, contour lines are commonly realized by using a dark color where the dot product between gradient and view vectorN •V approaches zero, i.e., these two vectors are nearly orthogonal. If we haveN</p><formula xml:id="formula_8">•L andN •Ĥ withĤ = L +V , thenN •V = 2(N •Ĥ) −N •L.</formula><p>We can thus create a lighting transfer function where we set ambient, diffuse and specular components to zero whereN •L ≈ 2(N •Ĥ). One advantage of this approach is that artifacts normally introduced by using a threshold to identify contour lines can be remedied by smoothing them in the lighting transfer function (e.g., using a gaussian) with no additional costs during rendering. Using the opacity enhancement component of the lighting transfer function also allows for a meaningful combination of contour enhance-ment and transparency: the opacity of contour regions is increased, but only where the gradient magnitude is high. Without taking the gradient magnitude into account opacity enhanced contour lines would lead to a cluttered image in translucent views. This is due to rapidly varying gradient directions in nearly homogeneous regions. Pure gradient-magnitude opacity-enhancement without directional dependence just requires a constant s α . Other methods, such as cartoon shading <ref type="bibr" target="#b2">[3]</ref> or metal shading <ref type="bibr" target="#b5">[6]</ref> can be realized straight-forwardly and combined with effects like contour enhancement. <ref type="figure" target="#fig_2">Figure 5</ref> shows an image rendered using four different lighting transfer functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SELECTIVE ILLUSTRATION</head><p>In this section we present techniques for selective illustration. Selective illustration techniques are methods which aim to emphasize specific user-defined features in a data set using visual conventions commonly employed by human illustrators. They are closely related to focus+context approaches frequently found in information visualization. The general idea is to highlight the region of interest (focus) without completely removing other information important for orientation (context).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Volume Painting</head><p>Volume Segmentation, i.e., the identification of individual objects in volumetric data sets is an area of extensive research, especially in medical imaging applications. Approaches range from very general methods to algorithms specifically designed to identify certain structures. An important criterion is the exactness of the segmentation, i.e., the ratio between correctly and incorrectly classified voxels. In practice, due to limited information, this criterion is difficult to measure. For volume illustration, however, voxel-exact classification of features is not necessarily of primary concern. Rather, it is important that the illustrator can quickly and easily add and remove structures of interest to and from the selection. Furthermore, as our approach is based on a fuzzy selection function, this fuzzyness should be also supported by the selection definition method. For this reason, we use a simple three-dimensional volumetric painting approach for selection definition. When the user clicks on the image, a ray is cast from the corresponding position on the image plane into the data volume. At the first non-transparent voxel that is intersected by the ray, a volumetric brush (e.g., a threedimensional gaussian) is "drawn" into the selection volume for each non-transparent voxel within the bounding box of the brush. Different composition methods can be chosen, for example addition (i.e., actual painting) or subtraction (i.e., erasing). We have found that this approach is intuitive and capable of achieving good results in a short time: the user specifies a transfer function which displays the object of interest and then just paints on it until it is fully selected. However, it is clear that a real-world application should also include more sophisticated algorithms. Just like image editing software normally supports manual and semi-automatic selection mechanisms (e.g., the common "magic wand tool"), a volume illustration system should include volume painting as well as region growing or watershed segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Cutaways and Ghosting</head><p>Cutaways (also referred to as cut-away views) are an important tool commonly employed by illustrators to display specific features occluded by other objects. The occluding object is cut out to reveal the structure of interest. Viola et. al. <ref type="bibr" target="#b21">[22]</ref> introduced importance-driven volume rendering, a general framework for determining which object is to be cut by using an importance function. Our simplified  three-object setup allows static definition of this importance function, which enables us to skip costly importance compositing and thus allows for an efficient implementation. Cutaways are only performed on the background and can be independently defined for ghost and selection. Ghosting refers to a technique which is frequently used in conjunction with cutaways. Instead of removing the occluding regions completely, opacity is selectively reduced in a way which attempts to preserve features such as edges. This tends to aid mental reconstruction of these structures and generally gives a better impression of the spacial location of the object in focus. In our approach, the user can smoothly control the degree of ghosting from no ghosting (opacity is not reduced at all) to full cutaway view (occluding structures are completely suppressed) as shown in <ref type="figure" target="#fig_3">Figure 6</ref>. This is achieved by combining a user-defined ghosting factor with the opacity-enhancement component of the lighting transfer function. Thus, for a lighting transfer function which enhances the opacity of contours, increasing the degree of ghosting will preserve these regions. Again, in the context of importance-driven volume rendering this approach can be seen as a special level-of-sparseness which is designed to closely correspond to traditional illustration techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Visual Conventions and Interaction</head><p>As the selection can undergo a user-defined transformation there are a number of possibilities for combining the effects of transfer functions, cutaways and ghosting, and spacial displacement. In its simplest form, this can be used to illustrate the removal or insertion of an object. Furthermore, "magic views" on a structure of interest can be generated, where the object is displayed using a different degree of detail, orientation, or rendering style.</p><p>Illustrators commonly employ certain visual conventions to indicate the role of an object in their works. In our illustration environment, we provide the user with different kinds of visual enhancements inspired by these conventions:</p><p>Boxes: For three-dimensional interaction, bounding boxes provide useful cues on the position and orientation of an object if occlusions are handled correctly. The display of bounding boxes is most useful when the selection is arranged during the design of an illustration. For the presentation of the illustration, however, the bounding boxes can be distracting and potentially occlude important details.</p><p>Arrows: Arrows normally suggest that an object actually has been moved during the illustrated process (e.g., in the context of a surgical procedure) or that an object needs to be inserted at a certain location (e.g., in assembly instructions). Analogously, we use arrows to depict the translation between ghost and selection, i.e., the arrow is automatically drawn from the object's original position to its current location. To avoid very short arrows in case the selection and the ghost project to nearby positions in image space, we use the screen-space depth difference to control the curvature of the arrow. This leads to the kind of bent arrows frequently found in illustrations. <ref type="figure" target="#fig_4">Figure 7 (a)</ref> shows an example for the use of arrows.</p><p>Fans: A fan is a connected pair of shapes, such as rectangles or circles, used to indicate a more detailed or alternative depiction of a structure. It can be easily constructed by connecting the screen-space bounding rectangles of ghost and selection.</p><p>In combination with cutaways and ghosting, this type of enhancement can lead to very expressive visualizations, depicting, for example, two different representations of the same object (see <ref type="figure" target="#fig_4">Figure 7</ref> (b)).</p><p>Apart from controlling visual appearance, it is useful to provide different interaction types based on the role of an object in the illustration. A selection can be in one of three states which influence the way it behaves in relation to the remaining scene:</p><p>Integrated: The selection acts as fully belonging to the threedimensional scene. This is intuitive, but has certain drawbacks. For example, when the viewpoint is rotated, the selection's movement is dependent on its distance to the origin. It can easily move out of the viewport or can be occluded by other objects.</p><p>Decoupled: The opposite to the integrated approach is to fully decouple the selection from the scene. It can be independently manipulated and is not affected by the viewing transformation. This is, for instance, useful when it is required to depict an object at a specific orientation regardless of the viewing transformation.</p><p>Pinned: A useful hybrid between the two modes above is to allow the object to be pinned to its current position in image space. Its on-screen location remains static, but it is still affected by rotations. A rotation of the viewpoint causes the same relative rotation of the object. For example, this can be used to generate a special view which always shows the part of an object facing away from the viewer in the background object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Annotations</head><p>Hand-made illustrations in scientific and technical textbooks commonly use labels or legends to establish a co-referential relation between pictorial elements and textual expressions. As we allow multiple selections to be defined, annotations are important for both recreating the appearance of static illustrations and simplifying orientation in our interactive environment. For placing annotations we need their screen-space bounding rectangles and anchor points. We use the following guidelines to derive a simple layout algorithm for optically pleasing annotation placement (for a more complete description of annotation layout styles and guidelines refer to <ref type="bibr" target="#b0">[1]</ref>):</p><p>• Annotations must not overlap. • Connecting lines between annotation and anchor point must not cross.</p><p>• Annotations should not occlude any other structures.</p><p>• An annotation should be placed as close as possible to its anchor point.</p><p>In many textbook illustrations, annotations are placed along the silhouette of an object to prevent occlusions. We can approximate this by extracting the convex hull of the projections of the bounding volumes of all visible objects. The resulting polygon is radially parameterized. Thus, the position of an annotation is defined by one value in the range [0, 1]. Based on its location in parametric space, a label is always placed in such a way that it remains outside the convex hull. All annotations are initially placed at the position along the silhouette polygon which is closest to their respective anchor point. We then use a simple iterative algorithm which consists of the following steps:</p><p>1. If the connection lines of any two labels intersect, exchange their positions.</p><p>2. If exchanging the positions of two labels brings both closer to their anchor points, exchange their positions.</p><p>3. If a label overlaps its predecessor, it is moved by a small delta.</p><p>These three steps are executed until either all intersections and overlaps are resolved or the maximum number of iterations has been reached. Remaining intersections and overlaps are handled by disabling annotations based on priority. We use the screen-space depth of the anchor point to define these priorities, i.e., annotations whose reference structures are farther away will be disabled first. While this basic algorithm does not result in an optimal placement, it is very fast for a practical number of labels (usually no more than 30 annotations are used in a single illustration) and generally leads to a visually pleasing layout. Due to the initialization of annotation locations at the position on the silhouette closest to the anchor point, the annotations generally move smoothly in animated views. Discontinuities only occur when intersections and overlaps need to be resolved. As some annotated structures might not be visible from every viewpoint, we use the screen-space depth of the anchor point to control the opacity of the connection line between anchor point and label. <ref type="figure" target="#fig_5">Figure 8</ref> shows an annotated illustration of a human foot highlighting the current selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">IMPLEMENTATION</head><p>In this section, we briefly describe the implementation of our algorithm for illustrative multi-object volume rendering with support for cutaways and ghosting. It is integrated into VolumeShop, a prototype application for interactive direct volume illustration (see <ref type="figure" target="#fig_6">Figure 9)</ref>. VolumeShop has been implemented in C++ and Cg using OpenGL. While we have clear indications that the current version of NVidia's Cg compiler does not produce optimal code in all circumstances, we have refrained from hand-optimizing assembly language shaders for the sake of portability.</p><p>It is possible to implement all presented methods in one single rendering pass. However, this would introduce considerable computational overhead, as, for example, multi-object compositing would have to be performed for every sample point even if it only intersects one object. While current graphics hardware supports dynamic branching, it still introduces severe performance penalties. It is therefore favorable to choose a multi-pass approach. A wellestablished strategy is to use the early-z culling capability of modern hardware for computational masking. Employing this approach we can identify those regions where less work has to be performed and use simplified vertex and fragment programs in these areas.</p><p>We can quickly extract bounding volumes for background, ghost, and selection by traversing our hierarchical data structures and rendering the corresponding geometry. Initially, we set up two depth maps by rendering the bounding volumes of ghost and selection each into a separate depth texture with the depth test set to LESS. These depth maps are used in the subsequent rendering passes to discard fragments, thus emulating a two-sided depth test. For smooth cutaways we additionally filter these depth maps using a large kernel.</p><p>In principle, our implementation is comprised of three volume rendering passes using three sets of vertex and fragment programs with increasing complexity:</p><p>Background pass: The first volume rendering pass is responsible for the background object. We set the depth test to LESS and render the bounds of the background object into the depth buffer. Depth buffer writes are then disabled to take advantage of early-z culling and the depth test is set to GREATER. Thus, empty space up to the first intersection point of a viewing ray with the background bounding volume is skipped without executing the fragment program. We then render viewaligned slices in back-to-front order and perform shading in a fragment program. Shadow mapping hardware is used to discard fragments whose depth is greater or equal than the corresponding value of the ghost or selection depth texture. Thus, regions which might contain the ghost and/or the selection are effectively cut out from the background object.</p><p>Ghost pass: In the second volume rendering pass we start by clearing the depth buffer and rendering the bounding volume of the ghost object with the depth test set to GREATER. Then depth buffer writes are disabled again and the depth test is set to LESS. The fragment program needs to perform shading for background and ghost. Fragments whose depth value is greater or equal than the corresponding value of the selection depth map are discarded. If cutaways are enabled then the opacity of the background is additionally modulated by a user-defined ghosting factor for fragments whose depth value is greater or equal than the corresponding value of the ghost depth map.</p><p>Selection pass: For the final pass we render the selection bounds into the cleared depth buffer with the depth test set to GREATER. Depth buffer writes are then disabled again and the depth test is set to LESS. The selection transformation is handled by passing in two sets of texture coordinates: one unmodified set for background and ghost, and an additional set for the selection which is transformed accordingly. In the fragment program we need to perform shading for background, selection, and ghost. We also handle background/selection and ghost/selection intersections by using the colors and opacities defined in the intersection transfer functions. For cutaways, the background's opacity is additionally modulated for fragments whose depth value is greater or equal than the corresponding values in one or both of the depth maps.</p><p>For handling of intersections with opaque geometry an additional depth map is generated before the background pass. The color contributions of the geometry are blended into the frame buffer. The depth texture is used in all three rendering passes to discard fragments which are occluded by geometry. Visual enhancements are either displayed as real three-dimensional objects with correct intersection handling (e.g., bounding boxes) or as overlays (e.g., fans). As larger selections will require more fragments to be processed in the more complex rendering passes, the performance of the presented algorithm mainly depends on the size of the selection. Thus, if no selection has been defined we achieve almost the same frame rates as conventional slice-based volume rendering due to the effectiveness of early-z culling. Selections, by definition, typically will be rather small compared to the background. Additionally, if we can determine that the selection does not intersect background or ghost (e.g., by means of a simple bounding box test) we execute a simplified fragment program in the selection pass.</p><p>For obtaining performance results we used the following setup: The chosen data set was the standard UNC CT head (256 3 ) rendered using √ 3 • 256 2 ≈ 444 slices -a realistic number for highquality rendering. The selection was set to a cube sized 16 <ref type="bibr" target="#b2">3</ref> , 32 <ref type="bibr" target="#b2">3</ref> , and 64 3 voxels centered in the middle of the data set. The selection transformation was set to identity. The transfer functions for background, ghost, and selection were set to zero opacity for values up to 1228 and to an opacity of one for all values above. The frame rates given in <ref type="table">Table 1</ref> are average figures for three 360 • rotations about the x-,y-, and z-axis for a 512 2 viewport. An Intel Pentium 4 3.4 GHz CPU and an NVidia GeForce 6800 GT GPU were used to obtain these measurements.</p><p>These results indicate that our approach is well-suited for highquality rendering in interactive applications. In the future, we expect to further increase the rendering performance by integrating early ray termination as proposed by Krüger and Westermann <ref type="bibr" target="#b10">[11]</ref>. selection frame rate none 8.28 16 <ref type="bibr" target="#b2">3</ref> 8.04 32 <ref type="bibr" target="#b2">3</ref> 6.81 64 <ref type="bibr" target="#b2">3</ref> 4.86 <ref type="table">Table 1</ref>: Performance results for rendering 444 slices of the UNC CT head (256 3 ) using different selection sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In this work, we introduced the general concept of a direct volume illustration environment. Based on this concept, Vol-umeShop (http://www.cg.tuwien.ac.at/volumeshop), an interactive system for the generation of high-quality illustrations from volumetric data, has been developed. An intuitive three-object setup for the interaction with volumetric data was discussed. We contributed a general technique for multi-object volume rendering which allows for emphasis of intersection regions via twodimensional transfer functions. Furthermore, we introduced a unified approach to efficiently integrate different non-photorealistic illumination models. Techniques for selective illustration were presented which combine cutaways and ghosting effects with artistic visual conventions for expressive visualization of volume data.</p><p>In addition, we proposed volume painting as an interactive selection method and presented an algorithm for automated annotation placement. A hardware-accelerated volume renderer was developed which combines the presented techniques for interactive volume illustration. While we believe that the results achieved with our prototype system are promising, a lot of work remains to be done. In the future we aim to integrate further artistic styles and techniques for the creation of aesthetically pleasing illustrations <ref type="bibr" target="#b19">[20]</ref>. We also want to investigate methods for automatically guiding viewpoint specification <ref type="bibr" target="#b20">[21]</ref> and light placement <ref type="bibr" target="#b11">[12]</ref>. Finally, improved interaction metaphors and techniques could significantly contribute to the usability of a volume illustration system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Conceptual overview of our direct volume illustration environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the basic multi-object combination process for background, ghost, and selection: the intersection between selection sets and volume sets results in object sets which are then combined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The same data set rendered with four different lighting transfer functions (the lighting transfer function for each image are displayed in the lower left corner -ambient, diffuse, specular, and opacity enhancement components are encoded in the red, green, blue, and alpha channel, respectively).(a) Standard Phong-Blinn lighting. (b) Phong-Blinn lighting with contour enhancement. (c) Cartoon shading with contour enhancement. (d) Metal shading with contour enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Different degrees of ghosting -from no ghosting (a) to full cutaway (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Using different artistic visual conventions. (a) Illustrating a tumor resection procedure using an automatically generated arrow. (b) Detailed depiction of a hand bone using a fan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Annotated illustration of a human foot -the current selection is highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Screenshot of VolumeShop (http://www.cg.tuwien.ac. at/volumeshop) during operation.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The work presented in this publication is carried out as part of the ex vis ation project (http://www.cg.tuwien.ac.at/research/vis/ exvisation) supported by the Austrian Science Fund (FWF) grant no. P18322. We would like to thank Sören Grimm and Ivan Viola for several fruitful discussions. Furthermore, we thank the anonymous reviewers for their valuable comments.</p><p>The carp data set is courtesy of Michael Scheuring, University of Erlangen, Germany. The stag beetle data set has been provided by Georg Glaeser, Vienna University of Applied Arts, Austria and Johannes Kastner, Wels College of Engineering, Austria. The visible human data set is courtesy of the Visible Human Project, National Library of Medicine, USA. The engine block data set is courtesy of General Electric, USA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label layout for interactive 3D illustrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strothotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the WSCG</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data intermixing and multi-volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast 3D cartoon rendering with improved quality by exploiting graphics hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Claes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Di</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vansichem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Image and Vision Computing</title>
		<meeting>Image and Vision Computing<address><addrLine>New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Role of computer technology in medical illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Corl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Roentgenology</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1519" to="1524" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast visualization of object contours by non-photorealistic volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Csébfalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="452" to="460" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A non-photorealistic lighting model for automatic technical illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH 1998</title>
		<meeting>ACM SIGGRAPH 1998</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="447" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flexible direct multi-volume rendering in interactive scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vision, Modeling, and Visualization</title>
		<meeting>Vision, Modeling, and Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="386" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two-level volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Bischi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial and temporal splitting of scalar fields in volume graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dipankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Volume Visualization and Graphics</title>
		<meeting>the IEEE Symposium on Volume Visualization and Graphics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtual resection with a deformable cutting plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Konrad-Verse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Littmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Simulation und Visualisierung 2004</title>
		<meeting>Simulation und Visualisierung 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="203" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acceleration techniques for GPU-based volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Light collages: Lighting design for effective visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization</title>
		<meeting>the IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modelling and rendering graphics scenes composed of multiple volumetric datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="171" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonphotorealistic volume rendering using stippling techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hardware-accelerated parallel nonphotorealistic volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Non-photorealistic Animation and Rendering</title>
		<meeting>the International Symposium on Non-photorealistic Animation and Rendering</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lighting transfer functions using gradient aligned sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compositing digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="259" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Volume illustration: Nonphotorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The lit sphere: A model for capturing NPR shading from art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface</title>
		<meeting>Graphics Interface</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Viewpoint selection using viewpoint entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vision Modeling and Visualization</title>
		<meeting>Vision Modeling and Visualization</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Importance-driven volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization</title>
		<meeting>IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="139" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Importance-driven feature enhancement in volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="408" to="418" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Volume sculpting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Interactive 3D Graphics</title>
		<meeting>the Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive multi-volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Science</title>
		<meeting>the International Conference on Computational Science</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zadeh. Fuzzy sets. Information and Control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="353" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distance based enhancement for focal region based volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Döring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Tönnies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Bildverarbeitung für die Medizin</title>
		<meeting>Bildverarbeitung für die Medizin</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="199" to="203" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
