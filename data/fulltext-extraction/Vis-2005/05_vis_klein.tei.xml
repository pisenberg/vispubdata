<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Frame-to-Frame Coherence for Accelerating High-Quality Volume Raycasting on Graphics Hardware</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Klein</surname></persName>
							<email>klein@vis.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Visualization and Interactive Systems</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Strengert</surname></persName>
							<email>strengert@vis.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Visualization and Interactive Systems</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stegmaier</surname></persName>
							<email>stegmaier@vis.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Visualization and Interactive Systems</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ertl</surname></persName>
							<email>ertl@vis.uni-stuttgart.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Visualization and Interactive Systems</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Frame-to-Frame Coherence for Accelerating High-Quality Volume Raycasting on Graphics Hardware</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation-Viewing Algorithms; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Raytracing Volume Raycasting</term>
					<term>Programmable Graphics Hardware</term>
					<term>Frame-to-Frame Coherence</term>
					<term>Space Leaping</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: Various high-quality volume renderings of anatomical data sets generated with our GPU-based raycasting system. All visualizations use a combination of isosurface rendering and semi-transparent volume rendering to provide both focus (bone) and context (tissue).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many researchers rely on volume rendering as a tool for analyzing and understanding their data. The degree to which this undertaking succeeds is highly dependent on the quality of the visualizations obtained with the volume renderer. In this context, the term quality has several aspects. On the one hand it means that the images are artifact-free and no information contained in the data has been lost due to, e.g., numerical inaccuracies. On the other hand it means that quality is also high in terms of the information content of the visualization, i.e. the researcher is offered a choice of visualization techniques (direct volume rendering allowing for an examination of inner structures, indirected volume rendering with isosurfaces, combinations modeling material properties like translucency) from which the researcher can choose the approach which reveals the optimum of information relevant for the analysis. This means a practical volume renderer has to be very flexible and easy to extend.</p><p>Finally, both accuracy and flexibility become virtually meaningless if the computations required for synthesizing the images are so computationally expensive that animations require the precomputation of video sequences. The latter boils down to the property of interactivity. So, in a nut shell, in order to be of value for data analysis-especially for critical applications like medicine, <ref type="figure">Fig. 1</ref>-a volume renderer must be fast, flexible, and able to generate visually pleasing, useful, and accurate visualizations.</p><p>While traditional slice-based volume rendering meets two of these criteria it fails to meet the third: flexibility. This is due to the fact that in slice-based volume rendering the discretized volume rendering integral is evaluated implicitly by blending semitransparent slices.</p><p>On the other hand, the classical raycasting approach explicitly performs the numerical integration required for solving the volume rendering integral and it explicitly performs the intersection calculations required for generating isosurfaces. Accordingly, raycasting is also not subject to inaccuracies involved in, e.g., framebuffer blending but instead can take advantage of the maximum numerical accuracy offered by the underlying processing unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE Visualization 2005</head><p>October 23-28, Minneapolis, MN, USA 0-7803-9462-3/05/$20.00 ©2005 IEEE.</p><p>However, until recently this processing unit was the CPU which meant that, again, one criterion-interactivity-was unfulfilled. The advent of modern GPUs supporting programmable vertex and fragment processors with Shader Model 3.0 functionality providing full dynamic flow control capabilities has changed this and there are now GPU-based raycasting implementations providing significant performance boosts compared to software implementations. Nevertheless, the exceptional performance of slice-based methods has not been reached, yet. Thus, a volume renderer being superior in every aspect is still unavailable.</p><p>In this paper we present a completely GPU-based empty-spaceleaping technique for optimizing the performance of GPU-based raycasting and for alleviating the performance issue. The speedup gained by the space leaping allows us to further increase the quality by antialiasing the images using a selective object space super-sampling approach.</p><p>Many acceleration techniques for raycasting have been proposed but not all of them are easily adapted to the GPU. The basic idea behind most acceleration techniques is to avoid traversing and sampling empty voxels, i.e. voxels that do not contribute to the volume rendering integral since they have zero opacity or, in the case of isosurface rendering, do not contain parts of the isosurface.</p><p>Two major groups of techniques have been developed. The first employs regular or hierarchical space-partitioning data structures, e.g. BSP-trees, to distinguish between non-empty and empty voxels and skip the latter. The second exploits the fact that during user interaction spatio-temporal coherence for rays shot through the volume is very high in volume rendering to skip empty regions using information from previously rendered images and directly leap to the first data voxel or ray sampling position that exhibits significant data values. Ray traversal is then started at this point, reducing the volume sampling cost significantly.</p><p>Although, hierarchical space-partitioning techniques provide by far the best speed-up, especially in the case of isosurface raycasting, they also have disadvantages. For instance they does not allow for easy integration of self-shadowing and the required depth ordering makes it difficult to include shading techniques based on changing ray directions, e.g refraction or reflection. And, what is especially important in our case, such hierarchical data structures cannot be easily mapped on current graphics processing units. In contrast an empty-space-leaping approach can be fitted easily in the rendering pipeline of GPU raycasting and, as we will show, can directly benefit from the native data models and the parallel processing power of the GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Raycasting of volumetric data on the GPU has only recently attracted the attention of researchers. Although using graphics hardware has a long tradition in volume rendering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, only recent advances in programmability and the incredible pace at which the performance of graphics processors increased over the last years, provides the means of carrying the benefits of raycasting to hardwareaccelerated volume rendering.</p><p>Several GPU-based implementations of the basic raycasting algorithm for both structured <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> and unstructured <ref type="bibr" target="#b20">[21]</ref> grids have been presented. These solutions can be divided in two classes. The first <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref> group performs multiple rendering passes-similar to traditional slice-based volume rendering-in order to traverse the volume and stores intermediate results computed on a per-fragment basis in temporary buffers that are accessed in subsequent rendering passes. The second <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> executes the whole raycasting algorithm in a single rendering pass exploiting the dynamic branching functionality introduced with Pixel Shader Model 3.0 available on contemporary graphics processors. In <ref type="bibr" target="#b14">[15]</ref> a simple technology demo is shown that demonstrates the use of ad-# Determine ray direction SUB direction, fragment.texCoord <ref type="bibr" target="#b0">[1]</ref>  vanced fragment shader functionality to implement a basic singlepass raycasting algorithm for regular volume data in a single fragment program. In <ref type="bibr" target="#b18">[19]</ref> we presented a flexible framework for single pass GPU-raycasting that takes advantage of the easily extensible raycasting approach to demonstrate a number of non-standard volume rendering techniques, including refracting material and selfshadowing isosurfaces.</p><p>Although a wide range of acceleration techniques for volume raycasting have been proposed and many optimizations from traditional raytracing of polygonal geometry can be applied as well, we will not give an exhaustive overview of all techniques here. We rather focus on the class of space-leaping techniques and their application in volume rendering that are directly related to our work. In particular, we will not discuss the large field of empty-spaceskipping techniques based on spatial subdivision <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref> and distance fields <ref type="bibr" target="#b17">[18]</ref>.</p><p>The first to describe empty-space-leaping techniques that exploit the spatio-temporal coherence in successive animation frames to speed up volume raycasting have been Gudmundson and Randén <ref type="bibr" target="#b4">[5]</ref> and Yagel and Shi <ref type="bibr" target="#b22">[23]</ref> in the early 1990s. While the solution shown in <ref type="bibr" target="#b4">[5]</ref> was limited to parallel projections, the introduction of an intermediate coordinates buffer that stores the object space coordinates of the first non-transparent voxel encountered during ray traversal by Yagel and Shi <ref type="bibr" target="#b22">[23]</ref> allows also for perspective projections. By reprojecting the content of the coordinates buffer of the previously rendered frame using point-splatting according to the new viewing parameters they obtain an estimate of the initial ray position for computing the current image, thereby skipping voxels that do not contribute to the final image.</p><p>Several extensions and improvements to the original algorithm have been proposed. Wan et al. <ref type="bibr" target="#b19">[20]</ref> use a cell-splatting approach, reprojecting voxels instead of points and combine it with precomputed distance-based empty-space skipping. Another approach was demonstrated by Yoon et al. <ref type="bibr" target="#b23">[24]</ref>. Instead of transforming the point coordinates they project the rays into the coordinates buffer of the previous animation frame in order to rapidly find intersections with isosurfaces. A recent approach by Lakare and Kaufman <ref type="bibr" target="#b9">[10]</ref> exploits ray coherence instead of inter-frame coherence to build the space leaping structure. The basic empty-space-skipping approach-based on ray coherence, frame-to-frame coherence, and space partitioning-is also widely used in traditional raytracing of polygonal objects.</p><p>The only approach, to our knowledge, that applies graphics hardware to compute empty-space-leaping data for raycasting was presented by Westermann and Sevenich <ref type="bibr" target="#b21">[22]</ref>. They accelerate software raycasting using depth information obtained from slice-based volume rendering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RAYCASTING ON THE GPU</head><p>The basic approach to volume raycasting on the GPU is to render only the front faces of the bounding box to generate one fragment for each pixel that is possibly affected by the volume. Attributes interpolated per-fragment, such as texture coordinates, can be used to easily determine the ray starting positions on the bounding box front faces and also allow for the inexpensive computation of ray directions with respect to the position of the camera. The actual ray traversal and the incremental integration of the volume rendering integral with a given optical model and sampling distance is carried out completely in a fragment program within a single rendering pass. In <ref type="figure" target="#fig_0">Fig. 2</ref> code for a completive implementation of a volume raycaster using pre-integrated lookup tables <ref type="bibr" target="#b3">[4]</ref> is listed. While this simple approach already produces usable results, some optimizations in terms of usability and performance can be added easily.</p><p>First, the maximal loop count so far is limited to 255 iterations for actual hardware, i.e. NVIDIA GeForce 6x, which imposes an upper bound of possible sample points that is too low for sub-voxel sampling of reasonably sized data sets. By simply adding a second, nested loop this upper limit can be raised to 255 2 iterations, which should prove to be sufficient for the data sets focused on nowadays. Second, the performance of the raycaster can be significantly increased by additionally terminating rays as soon as they have left the volume. Third, the raycasting approach can be easily extended by early ray termination with respect to the already accumulated opacity during the traversal to avoid unnecessary texture sampling, reduce the number of arithmetic operations, and overall minimize fragment shader expenses. Having the possibility of datadependent loop termination, this functionality can be implemented with a single instruction.</p><p>Such a single-pass GPU-based raycasting approach provides several benefits over slice-based volume rendering. It is flexible in terms of an easy integration of different optical models and shading styles. While raycasting can easily incorporate even very sophisticated optical models, for example continuous refraction with possible change of the ray direction at every sample point, some methods are quite challenging to apply to slice-based volume rendering and impose quite often multiple render passes with additional computational overhead. Nevertheless, many advanced techniques for volume visualization were integrated into slice-based rendering systems. For example Kniss et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> used half-angle slices to incorporate volumetric light attenuation effects, like volumetric shadows or translucency, into volume rendering.</p><p>In terms of quality, raycasting provides some characteristics that lead to superior results. On the one hand, the sampling distances remain constant throughout the whole volume when used in connection with perspective projection, while using multiple slices as proxy geometry results in varying sample distances for each ray. On the other hand, raycasting offers higher accuracy due to the use of full precision floating point calculations throughout the complete pipeline. Slice-based volume rendering as well as multi-pass GPU raycasting suffer from the lack of high-accuracy floating point blending. Although with the introduction of application-created framebuffer objects to OpenGL by the EXT framebuffer object extension it is now possible to render to floating point buffers that support 16 and 32 bit blending, this issue is still not solved. While blending in a 16 bit RGBA floating point color buffer is still significantly slower than blending 8 bit color values, full precision 32 bit blending is, by now, not officialy supported, prohibitively slow, and as a result not feasible for interactive applications.</p><p>However, GPU raycasting so far is slower compared to slicebased rendering although both approaches have in principle the same computational complexity. This is mainly due to the overhead imposed by the dynamic flow control instructions in the fragment shader program. Even if this penalty will hopefully alleviated with the next generations of graphics processors, further optimizations can help to significantly increase the raycasting performance. Of the many other optimization strategies that have been originally proposed for traditional CPU-based raycasting the most promising is to exploit frame-to-frame coherence in connection with emptyspace leaping in order to speed up raycasting on GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPTY SPACE LEAPING BY REPROJECTION ON GRAPHICS HARDWARE</head><p>Before giving detail on how to implement empty-space leaping on graphics hardware, we start the discussion with a brief description of how the basic approach as described in <ref type="bibr" target="#b22">[23]</ref> can be integrated into an existing volume raycasting system. A schematic overview of the algorithm is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Up to three off-screen render targets B 0 to B 2 are used to store intermediate information, such as ray starting positions, the final image of the rendered volume for post-processing, e.g. antialiasing or tone mapping, and the position of the first non-transparent sampling points along the ray, similar to the coordinates buffer introduced in <ref type="bibr" target="#b22">[23]</ref>. All render targets have floating point precision. Then, the accelerated raycasting proceeds as follows: For the first frame a full-scale raycasting of the volume has to be performed, thus the initial ray positions are set to the objects space positions corresponding to the front faces of the volume's bounding box. Then, the volume is raycast and two render targets are written: One containing the usual raycast color image of the volume and a second that stores the positions of the first relevant ray sampling points, in the following referred to as the hit points, encountered during ray traversal. The raycast image is either directly copied to the framebuffer, possibly involving a tone mapping, or further processed as described in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization Reprojection</head><p>Reinit?</p><p>Raycasting B0 B0 B1 B2 Sec. 5. For consecutive frames, the previously stored hit points are reprojected, i.e. transformed regarding to the new viewing parameters, in order to obtain a new estimate for the ray starting positions for the subsequent raycasting step. The graphics processor is perfectly suited for this kind of point projection technique as it is primarily designed for fast vertex processing. Besides, the vertex processing units are mostly idle in a GPU-based raycasting approach as it is heavily rasterization-bound and the only geometry involved so far is the proxy-geometry represented by the faces of the volume bounding box. Thus, the overhead imposed by projecting the hit points onto the image plane can be regarded insignificant compared to the overall computational cost of raycasting. Furthermore, the necessary depth sorting of the reprojected points comes virtually for free. We have implemented this approach as an extension to the single pass raycasting system 1 described in <ref type="bibr" target="#b18">[19]</ref> using OpenGL 2.0 on a NVIDIA GeForce 7800 GTX based MS Windows system. The NV fragment program2 and NV vertex program3 extensions are used for dynamic flow control in fragment programs necessary for implementing the raycaster and performing texture lookups in a vertex program necessary in the reprojection steps as will be described in Sec. 4.3. The graphics memory buffers needed to store the space leaping information have to be 32bit RGB floating point buffers since they store object space vertex coordinates. Basically, nowadays, there are two possibilities for implementing such off-screen buffers. Either P-Buffers in combination with the renderto-texture extension or the recently introduced application-created framebuffer objects can be used. We decided to use framebufferattached textures provided by the new EXT framebuffer object extension for directly rendering to texture memory due to the simplicity of handling and the good performance compared to the other possibility. Further detail on how the different steps of the accelerated raycasting are actually implemented will be given in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initialization</head><p>Before raycasting the first image in an animation sequence, the start points for the rays cast through the volume have to be initialized in order to start the pipeline with a well-defined state. As no further information is available, yet, the intersections between the rays and the bounding box of the volume have to be used, similar to the standard algorithm described in Sec. 3. Thus, the front faces of the bounding box are rendered into the texture bound to buffer B 0 mapping interpolated object space coordinates to RGB color values.</p><p>Reinitialization of the ray start positions is also necessary if, for example, the window is resized, thus invalidating the render targets, or large changes of the volume or view parameters, e.g. isovalue, transfer function or large camera movements, have been performed by the user. However, moderate changes in the volume parameters are compensated by the conservative estimate taken for the ray start positions. Thus, reinitialization is only necessary if, e.g., the isovalue is changed in very large steps or a completely new transfer function is loaded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Raycasting</head><p>The raycasting process is started for all pixels of the projected volume by rendering the front faces of the volume's bounding box as a proxy geometry. The single pass raycasting program described in Sec. 3 has to be slightly modified to accomplish the space leaping. For each pixel covered by the volume the texture bound to B 0 holds the estimated starting positions for the rays provided by the reprojection of the hit point coordinates from the previous pass or set by the initialization. Thus, the use of the interpolated object space 1 http://www.vis.uni-stuttgart.de/eng/research/fields/current/spvolren coordinates provided as fragment attribute is replaced by a texture lookup based on the current fragment position that yields the initial ray sampling position from where the volume traversal is to be started.</p><p>During raycasting, the position of the first sampling point contributing to the final image is written to the texture bound to buffer B 2 . For opaque isosurface rendering this corresponds to the intersection point of the ray with the surface, whereas when using semitransparent volume representations the first sampling point is stored that corresponds to a scalar value that is not mapped to a completely transparent color. To cope with inaccuracies during the following reprojection step that would lead to inexact results or even artifacts when, for example, the foremost isosurface would be missed, the hit point has to be moved slightly back along the ray before it is written to the buffer. Depending on the sampling step length h the upper limit for the number of samples we have to step back to make sure not to miss a significant data voxel can be estimated. A conservative estimation would be</p><formula xml:id="formula_0">n max = ∆x 2 + ∆y 2 + ∆z 2 h ,</formula><p>with ∆i denoting the edge length of the voxel in the respective coordinate direction. Although, this upper limit can be quite high for small sampling distances, one to three sample lengths have been proven to be sufficient for the volumetric data sets we have used to evaluate our implementation (Sec. 6). For rays completely missing the volume, i.e. no non-transparent voxel has been encountered during ray traversal, the point where the ray leaves the volume is written to B 2 . Note that pixels lying outside the volume's projection also have to be initialized to some meaningful value to ensure that those points do not interfere with the following transformation and projection step as described below.</p><p>The accumulated color and opacity values for each fragment that amount to the raycasted image is written to another render target bound to buffer B 1 . Afterwards, the content of that buffer can be either directly copied to the framebuffer, e.g., with tone mapping applied, or further processed, e.g., antialiased as described in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reprojection</head><p>The next step is to obtain a new estimate for the initial ray sampling positions for the next image. For this, the hit points determined during the raycasting of the previous animation frame have to be reprojected into the image plane specified by the viewing parameters of the new frame. As the point coordinates are stored in object space it is sufficient to initialize the OpenGL modelview and projection matrix according to the new view parameters and to feed the positions through the geometry pipeline to compute the new coordinates of their screen space projection, i.e. the origin of the ray that is expected to hit them. Only the necessary points are rendered, i.e. those corresponding to pixels inside the screen aligned bounding rectangle of the projected volume, to save on vertex processing. A vertex program is used to set the color of the resulting fragment to the input object space coordinates of the respective hit point position.</p><p>Unfortunately, it is not possible to use the positions stored as RGBA colors in the texture bound to B 2 directly as input to vertex processing, e.g., as a vertex array. NVIDIA GPUs by now, do not support direct rendering to vertex buffers. Although, the SuperBuffers extension <ref type="bibr" target="#b11">[12]</ref> proposed by ATI and supported by their recent families of graphics processors provides the possibility of binding a graphics memory buffer to arbitrary OpenGL objects, i.e. render targets, texture objects or vertex arrays, this functionality has, regrettably, not been incorporated into the new ARB frame buffer objects extension we use to render directly to a texture. On the other hand, current ATI GPUs do not support the dynamic looping and branching functionality of Shader Model 3.0 necessary for implementing our raycasting solution. Hence, at the moment the only solution to use rendered pixels as vertex input on NVIDIA GPUs that does not involve a prohibitively expensive copy to and from main memory is to use a separate vertex array containing dummy vertex positions in combination with a texture lookup in a vertex program. The dummy vertex positions, in fact, have to represent the texture coordinates with which the input texture bound to B 2 has to be sampled to get the actual vertex coordinates. Then, the vertex position is replaced by the value returned by the texture lookup before it is projected to screen-space via the modelview projection transform. Additionally, each fragment of the projected point is assigned the object space coordinate of the point as the RGB color value, serving as the ray starting position for the following raycasting, as already mentioned above.</p><p>It should be noted that pixels lying outside the projection of the volume bounding box already have to be initialized before the raycasting step writes the actual hit points. This is crucial in order to avoid problems caused by such points being projected inside the volume's footprint in image space during reprojection. Therefore, we clear buffer B 2 with a coordinate value that is guaranteed to lie outside the current view frustum after applying the view transformation by rendering a viewport filling polygon of that color. A simple clear will not work here, as the clear color gets clamped to the [0, 1] range. Then, view frustum culling ensures that those points do not have an effect on the projected point set by setting their coordinates to a position outside the viewing frustum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hole Filling</head><p>Although after reprojection most pixels have assigned the correct starting position for the subsequent raycasting step, there are still two problems that have to be addressed. First, holes, i.e. pixels containing no information of the reprojected positions, can occur in the image. This is because the screen-space positions of the reprojected points do not necessarily coincide with the integer framebuffer pixel locations but instead are assigned to the pixels nearest to their normalized window coordinates. Thus, no space-leaping information is available and we have to start a full-scale raycasting from the first meaningful position, i.e. the intersection of the ray with the volume boundary. The second problem is closely related to the first. Although, in theory, multiple reprojected points that are mapped to the same pixel location should be sorted in the correct depth order it is still possible to end up with a wrong estimate for the initial ray position in certain pixels. Due to the described discretization problems and the nature of the perspective transformation it is likely that values of points that should have been actually covered by points lying closer to the viewer are still visible. Hence, the wrong start positions would be chosen and important parts of the volume would be missed during raycasting. Several solutions for this problem have been proposed, including the use of pointand cell-splatting methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Both splatting approaches exploit the fact that projecting points that exhibit a screen space footprint larger than a single pixel can solve the occlusion problem as long as the points are spread densely over the image plane. Depth sorting guarantees that for overlapping points the most conservative estimate, i.e. the point nearest to the viewer, is always chosen. Choosing a point size of two or four we have not noticed any artifacts due to wrong depth ordering in our test cases. On the other hand, rendering points that are larger than one pixel poses no significant overhead. As the fragment processors of all modern GPUs process pixels in groups rather than individually, it is even slower to render many unconnected single pixels than a primitive with a large number of pixels, as coherence between the pixels is typically much higher. Although rendering enlarged points significantly reduces the number of holes in the image there are still some pixels left for which no information is available. These pixels have to be identified and preset to a meaningful value. A very conservative measure is to set them to the intersection of the ray with the volume's bounding box. This is accomplished by initializing the alpha components of the destination buffer to zero prior to reprojecting the point coordinates. While rendering the points the alpha value for the generated fragments is set to one. Rendering the front faces of the volume bounding box again and updating the pixels only where alpha is not already equal to one corrects pixel values that previously contained no valid start positions.</p><p>Another solution to the hole filling problem that could come to the mind would be to render a closed triangle mesh instead of single points. By its very nature this mesh will not exhibit any holes. Unfortunately, there are other problems that prevent that solution. First of all, rendering the triangle mesh involves a much larger overhead since much higher vertex processing costs would arise from rendering the necessary triangle strips or fans. But the main drawback originates from the triangles themselves. This is explained most easily using an example. Let us consider three adjacent pixels forming a triangle situated at the silhouette of the volumes projection, e.g., two vertices of the triangle have object space positions inside the volume and the remaining vertex lies on the backface of the volume's bounding box. It is very likely that this triangle will be mapped to a single pixel during rasterization as its normal will be nearly orthogonal to the view direction. As the depth value for that pixel will be linearly interpolated from the depth values of the triangle's vertices and since there is no control over which depth value and, accordingly, color value of the three vertices is chosen by the hardware rasterizer, it is in fact very likely to end up with a completely wrong estimate of the new hit point positions.</p><p>The images shown in <ref type="figure" target="#fig_2">Fig. 4</ref> illustrate the effect of the two hole filling steps. The left-most image shows the start positions buffer after reprojecting hit points of size one for an isosurface rendering of the Bucky Ball data set. White values represent pixels for which no space-leaping data is available, yet. The few outliers in color on the front of the isosurface depict pixels with wrong information due to incorrect occlusion. The middle image shows the result of reprojection using points of size four. Obviously, all pixels that previously suffered from incorrect occlusion now have valid information. The right image shows the result after updating the remaining holes with positions on the volume's front faces.</p><p>On the other hand, the assumption about frame-to-frame coherence is obviously only correct for small differences in the viewing parameters for consecutive images. Nevertheless, artifacts can occur when violent interaction and low framerates are involved. Thus, artifacts due to incorrect ray start positions are unavoidable. Fortunately, splatting of extended points provides a certain degree of selfcorrecting behavior, i.e. artifacts that arise from incorrect spaceleaping information are mostly corrected automatically, as soon as the view differences in consecutively rendered images fall below a certain level. This is a result of the propagation of depth infor-mation in the vicinity of projected points caused by their enlarged screen space footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SELECTIVE SUPER-SAMPLING</head><p>Due to the discrete sampling of the image plane, the raycasting approach generates images with visible artifacts predominantly at silhouettes, feature lines and regions of high gradient magnitudes. These aliasing artifacts are most apparent in images of static views and negatively affect the otherwise high quality provided by the described volume raycasting approach. To cope with this problemwhich is not just limited to raycasting-graphics hardware provides full-scene antialiasing (FSAA) for application-independent usage. The techniques used are based on sampling the area covered by a single pixel in image space multiple times and combining the results to the final color. The number of additional samples and their arrangement are manifold and directly influence the resulting image. Finally, all super-sampling algorithms boil down to a performance vs. quality tradeoff, since additional rays result in higher quality but also in higher computational costs.</p><p>Unfortunately, this built-in functionality cannot be utilized for our optimized raycasting system, since the rendering of the image has to take place in an application-based framebuffer object due to the need of writing into two separate floating point buffers simultaneously while rendering the volume (see Sec. 4.2). However, the EXT framebuffer object extension does not allow for creating multi-sampled buffers, therefore the application itself is required to provide an anti-aliasing mechanism.</p><p>We integrated both full-scene antialiasing as well as selective super-sampling into our raycasting system to further improve the visual quality of the generated images. All additionally generated rays significantly benefit from the applied empty-space-leaping optimization since the hit points of the actual rendering can be used directly without the need for prior transformation. The raycasting is performed as described above, followed by the additional supersampling, which operates directly on the generated color values as well as the actual hit points. The resulting image is passed to the window-system-provided framebuffer and presented to the user.</p><p>While using FSAA requires every pixel in image space to be refined, selective super-sampling first analyzes the vicinity of each pixel and generates additional rays only at pixels that possibly suffer from aliasing artifacts. The classification criterion is based on the differences in color values of a pixel to its four-neighborhood compared to a user defined threshold. If the magnitude of any of the four differences exceeds the given threshold, super-sampling is applied to this pixel, while in the other case the original color value remains unchanged. This allows for improving the image quality in the most significant regions without the need to refine the complete image-space at a higher sampling rate. We investigated three different sampling patterns comparable to patterns used for FSAA on recent graphics hardware. Besides two regular sub-pixel schemes for two-fold and four-fold super-sampling, the third sampling pattern uses rotated-grid sampling (RGAA) <ref type="bibr" target="#b1">[2]</ref>, which leads to a diamondshaped arrangement for optimized sub-pixel coverage. This proves to be superior to a regular pattern since each row and colum of the  In order to determine ray directions for sub-pixels, we employ a linear interpolation scheme based on the ray directions of the current pixel and the directions of two of its neighbors as depicted in <ref type="figure" target="#fig_5">Fig. 7</ref>. The parameters of the linear interpolation are specified via a set of texture coordinate values, allowing for arbitrary sampling positions other than the already described. Since the four direct neighbors need to be looked up anyway, more accurate bilinear interpolation could also be used if required.</p><p>The influence of the different sub-sampling methods on the final image and the original unaltered reference image are depicted in <ref type="figure" target="#fig_4">Fig. 6</ref>. As expected four-fold super-sampling outperforms the two-fold approach, but as mentioned above, the increase in quality is traded for speed. Rendering the shown isosurface representation of a 380×340×110 distance volume data set without antialiasing results in 30.7 fps. Two-fold full-scene super-sampling reduces this performance to 12.6 fps, while casting four rays results in a framerate of 9.4 fps. The high performance loss in the first case is primarily due to the overhead imposed by additional texture lookups required for determining values of the four direct neighbor pixels. Accordingly, there is only a relatively small drop in performance when using higher super-sampling rates since no further texture lookups are necessary. Reducing the number of refined pixels with a selective super-sampling approach does not automatically increase performance for the case of volume raycasting. As all modern GPUs process pixels in larger groups, rendering only a single pixel poses a non-negligible overhead that diminishes the advantages of selective refinement.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS AND DISCUSSION</head><p>In this section we show the effectiveness of the empty-spaceleaping technique we have implemented for accelerating our GPUbased raycasting system. All measurements were carried out on a standard PC equipped with an NVIDIA GeForce 7800 GTX graphics board running MS Windows XP. For all images and measurements the sampling step size was chosen to be at least as small as the minimal slice distance of the volume data set. All images were rendered to a 512 2 viewport. No super-sampling was used for this measurements. The speed-up of more than a factor of two that we achieve for isosurface renderings is promising. When rendering the isosurface of the Abdomen data set <ref type="figure" target="#fig_7">(Fig. 8d</ref>) without empty-space leaping an average of 177 sampling steps per pixel was necessary <ref type="table">Table 1</ref>: Performance of the GPU-based raycasting with and without empty-space leaping (ESL). Shown are the average framerates measured while rotating the volumes according to the views shown in <ref type="figure" target="#fig_7">Fig. 8</ref>  until the surface was hit. Using empty-space leaping this number is reduced to 25 steps, requiring only ten iterations for more than 60% of the pixels (compared to 1.8% for the unoptimized solution).</p><p>Since the reprojection step-especially the rendering of the points-imposes additional overhead, the effectiveness of the presented approach is strongly dependent on the sampling step size. A significant speed-up can only be expected if the cost for sampling empty regions is significantly higher than that for point splatting. But this is almost always true for high-quality renderings due to the typically very high sampling rate.</p><p>Obviously, an empty-space-leaping technique is not very effective for semi-transparent volume renderings if most parts of the volume are visible. As shown in Tab. 1 a speed-up of only 20% is achieved for the Abdomen data set if the skin is shown <ref type="figure" target="#fig_7">(Fig. 8h)</ref>. Much higher performance could be expected if a space partitioning approach, e.g. an octree, would be used to also skip transparent voxels that ly within or behind visible volume parts. Furthermore, bricking approaches have the additional advantage of reduced texture memory consumption. This would allow for visualizations of larger volume data sets. By now our solution is limited to 512 3 volumes, due to the maximum 3D texture size of our graphics card. Although, there are graphic cards with 512MB of memory available, the size of 3D textures is still restricted to at most 256MB per texture. But bricking has disadvantages, as well. Bricking requires the rendering and blending of depth-sorted bricks one after another. However, raycasting with non-linear rays, e.g., refracting volumes, does not exhibit such simple ordering characteristics. Thus, brickbased solutions are not feasible in such a scenario. More important, bricking-optimized GPU-raycasters do not allow for shadow rays to be traced through the volume since the dynamic selection of the necessary bricks for traversing the shadow rays cannot be done inside a fragment shader program. Nevertheless, shadows are an important visual cue in the perception of spatial relationships in complex volume renderings or high-quality representations. Both techniques can be relatively easily integrated in our accelerated singlepass raycasting solution <ref type="bibr" target="#b18">[19]</ref>.</p><p>It is difficult to compare the results with other volume raycasting solutions, namely optimized software raycasting systems since the performance of a raycasting system depends strongly on the underlying hardware, the used viewport size and the fraction of the viewport covered by the image of the volume, the current view direction, and other rendering parameters such as sampling step size, isovalue, or transfer function. Especially for volume rendering techniques that involve view-depended acceleration methods it is important to note whether the presented numbers represent a fixed view point or the average performance that can be expected during interactive manipulation of the volume. Lakare and Kaufman <ref type="bibr" target="#b9">[10]</ref>, for example, report a performance of 5.4 fps for their CPU-based emptyspace-leaping scheme when rendering an isosurface of the Head data set shown in <ref type="figure" target="#fig_0">Fig. 8c on a 2</ref>.6GHz Pentium4 PC, using approximately the same viewing conditions. Unfortunately, no precise information about the used sampling step size and image resolution is provided.</p><p>Another CPU-based raycasting system that implements a number of optimization techniques was presented by Mora et al. <ref type="bibr" target="#b13">[14]</ref>. They report a much higher performance of up to 7.7 fps for rendering this data set to a 512 2 viewport on a 1.4GHz Athlon processor, but their system is limited to orthographic projection and heavily exploits pre-computations to speedup the rendering.</p><p>It is also interesting to compare our results to special purpose volume rendering hardware, like the VolumePro <ref type="bibr" target="#b15">[16]</ref> or the VizardII <ref type="bibr" target="#b12">[13]</ref> boards. For the VolumePro board a theoretical value of 30 fps for a 256 3 data set on a 256 2 viewport is reported. No timings for real measurements are given. Also, only orthographic projection is possible with this hardware solution. In contrast, the VizardII hardware allows for perspective views. For static views with precomputed rays the VizardII hardware can render up to 16 fps and measurements of 3-7 fps for a 256 3 data set using 256 2 rays are given. For dynamic views, i.e. where it is not possible to use precomputed rays, the VizardII hardware is limited to a maximum of 4 fps when using 256 2 rays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have presented a solution for accelerating GPU-based raycasting based on an empty-space-leaping technique. The GPU is used both for performing the complete raycasting on a per-fragment level and also for accelerating the computation of optimized ray starting positions. By exploiting frame-to-frame coherence we experience a speed-up of more than a factor of two for the rendering of isosurfaces compared to the basic GPU-raycasting technique. Allowing for framerates that come close to values that can be achieved using special purpose volume rendering hardware. Our system is complemented by antialiasing techniques because, eventually, speed means nothing without quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Basic fragment program code of a volume raycaster using pre-integrated lookup tables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Schematic overview of GPU-based space leaping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the effects of the two hole filling steps. Left: Start positions after reprojection. Middle: Splatting extended points. Right: Final start points after both hole filling steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Sampling patterns used for antialiasing. Solid lines indicate pixel boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of antialiasing quality. Left to right: no antialiasing, 2× super-sampling, 4× regular super-sampling, and 4× rotated-grid antialiasing. sub-pixel is exactly covered by a single sample point, as illustrated in the comparison of all implemented sampling patterns in Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Two alternative schemes for determining the sub-pixel ray directions. Left: linear interpolation, right: bilinear interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Images rendered with accelerated GPU-raycasting for various data sets. Data set sizes and a performance comparisons of the basic GPU-raycasting technique with our empty-space-leaping implementation for both rendering styles are given in Tab. 1. The images in the first row show isosurface renderings, those in the second row semi-transparent volume renderings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>multiple times about the y-axis. The sampling step size h is given with respect to the shortest edge of the voxels.</figDesc><table><row><cell>data set</cell><cell>size</cell><cell>h</cell><cell cols="2">no ESL ESL speed-up</cell></row><row><cell>Engine (8a) Head (8c) Foot (8b) Foot (8f) Abdomen (8d) Abdomen (8h) Aneurysm (8g) Aneurysm (8g) Backpack (8e)</cell><cell>256 2 ×110 256 2 ×225 160×430×183 160×430×183 512 2 ×174 512 2 ×174 512 3 512 3 512 2 ×373</cell><cell>0.5 0.5 0.4 0.8 0.7 0.7 0.5 1.0 1.0</cell><cell>13.2 24.2 7.7 17.1 6.7 18.8 9.7 13.2 3.9 10.4 5.6 6.8 3.6 6.2 7.0 11.9 2.6 3.9</cell><cell>1.8 2.2 2.7 1.4 2.7 1.2 1.7 1.7 1.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accelerated Volume Rendering and Tomographic Reconstruction using Texture Mapping Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Volume Visualization &apos;94</title>
		<meeting>the Symposium on Volume Visualization &apos;94</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The GeForce 6 Series of GPUs High Performance and Quality for Complex Image Effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia Corp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NVIDIA Corp</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Accelerating Volume Reconstruction With 3D Texture Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Cullip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
		<idno>TR93-027</idno>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-Quality Pre-Integrated Volume Rendering Using Hardware-Accelerated Pixel Shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics / SIGGRAPH Workshop on Graphics Hardware &apos;01</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental Generation of Projections of CT-Volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gudmundsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Randén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The First Conference on Visualization in Biomedical Computing</title>
		<meeting>The First Conference on Visualization in Biomedical Computing</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive Translucent Volume Rendering and Procedural Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Premoze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;02</title>
		<meeting>IEEE Visualization &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Model for Volume Lighting and Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Premoze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Mcpherson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The ULTRAVIS System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunter</forename><surname>Knittel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Volume Visualization &apos;00</title>
		<meeting>the IEEE Symposium on Volume Visualization &apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acceleration Techniques for GPU-based Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rüdiger</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;03</title>
		<meeting>IEEE Visualization &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Light Weight Space Leaping Using Ray Coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarang</forename><surname>Lakare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;04</title>
		<meeting>IEEE Visualization &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Empty Space Skipping and Occlusion Clipping for Texture-based Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;03</title>
		<meeting>IEEE Visualization &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><forename type="middle">Mace</forename><surname>Opengl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Superbuffers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Research</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Inc</surname></persName>
		</author>
		<ptr target="http://www.ati.com/developer/gdc/SuperBuffers.pdf" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VIZARD II: A Reconfigurable Interactive Volume Rendering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meißner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetekam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hirche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ehlert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Straßer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Doggett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Forthmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Proksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Workshop on Graphics Hardware &apos;02</title>
		<meeting>the ACM SIGGRAPH/EUROGRAPHICS Workshop on Graphics Hardware &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new Object-Order Ray-Casting Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Pierre</forename><surname>Jessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Caubet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;02</title>
		<meeting>IEEE Visualization &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">NVIDIA SDK 9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia Corp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The VolumePro Real-Time Ray-Casting System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hardenbergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Knittel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Seiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH &apos;98</title>
		<meeting>ACM SIGGRAPH &apos;98</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Smart Hardware-Accelerated Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Röttger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Guthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EG/IEEE TCVG Symposium on Visualization VisSym &apos;03</title>
		<meeting>EG/IEEE TCVG Symposium on Visualization VisSym &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast Ray-Tracing of Rectilinear Volume Data Using Distance Transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Sramek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="236" to="252" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Simple and Flexible Volume Rendering Framework for Graphics-Hardware-based Raycasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stegmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Strengert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Volume Graphics &apos;05</title>
		<meeting>Volume Graphics &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="187" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and Reliable Space Leaping for Interactive Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aamir</forename><surname>Sadiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;02</title>
		<meeting>IEEE Visualization &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hardware-Based Ray Casting for Tetrahedral Meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Merz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;03</title>
		<meeting>IEEE Visualization &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="333" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerated Volume Raycasting Using Texture Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rüdiger</forename><surname>Westermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Sevenich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;01</title>
		<meeting>IEEE Visualization &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accelerating Volume Animation by Space-Leaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Yagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;93</title>
		<meeting>IEEE Visualization &apos;93</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accelerating Volume Visualization by Exploiting Temporal Coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ilmi Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization &apos;97, LBHT</title>
		<meeting>IEEE Visualization &apos;97, LBHT</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
