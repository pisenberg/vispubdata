<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COTS Cluster-based Sort-last Rendering: Performance Evaluation and Pipelined Implementation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Cavin</surname></persName>
							<email>cavin@loria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lorraine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Mion</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lorraine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Filbois</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lorraine</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COTS Cluster-based Sort-last Rendering: Performance Evaluation and Pipelined Implementation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.2 [Computer Graphics]: Graphics Systems-Distributed/network graphics</term>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation-Viewing algorithms</term>
					<term>C.2.4 [Computer-Communication Networks]: Distributed Systems-Distributed applications</term>
					<term>C.2.5 [Computer-Communication Networks]: Local and Wide-Area Networks-Ethernet cluster-based visualization, sort-last rendering, parallel image compositing</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: Views of the head section (512x512x209) of the visible female CT data with 16 nodes (a space has been left between the subvolumes to highlight their boundaries). Using a 3 years old 32-node COTS cluster, a volume dataset can be rendered at constant 13 frames per second on a 1024 × 768 rendering area using 5 nodes. On a 1.5 years old, fully optimized, 5-node COTS cluster, the frame rate obtained for the same rendering area reaches constant 31 frames per second. We truly expect our future work, including further algorithm optimizations and hardware tuning on a modern PC cluster, to provide higher frame rates for bigger datasets (using more nodes) on larger rendering areas.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>As PC clusters become widely available as a huge and cheap computing and storage resource, datasets obtained from 3D acquisition (3D scanners, CT scanners, MRI, . . . ) or resulting from large-scale numerical simulations (FEM, CFD, . . . ) become bigger and bigger (datasets of several Gigabytes are now a common place). Visualizing such datasets requires a similar amount of computing and graphics resources. As shown in <ref type="bibr" target="#b3">[4]</ref>, using a PC cluster for this goal slowly appears as an efficient and viable solution, compared to high-end High Performance Computing (HPC) systems.</p><p>Sort-last parallel rendering (as described in <ref type="bibr" target="#b11">[13]</ref>) is an efficient technique to visualize huge datasets on PC clusters. As illustrated on <ref type="figure" target="#fig_0">Figure 2</ref>, the dataset is subdivided and distributed across the cluster nodes. For every frame, each node renders a full resolution image of its data using its local GPU and the images are blended together using a parallel image compositing algorithm.</p><p>Several software and hardware parallel image compositing methods are available, either in the literature, or commercially. In this paper, we focus on the use of Commodity Off-The-Shelf (COTS) PC cluster to perform this task. By COTS cluster, we mean here standard PCs with high-end graphics card, connected by a high speed (Gigabit) network, excluding specialized networks (such as Myrinet and Infiniband) and hardware image compositors. We demonstrate in this paper why and how a COTS cluster can be a viable competitor compared to more expensive solutions.  or polygon rendering. The most important ones include: direct send <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">18]</ref>, binary tree, binary swap <ref type="bibr" target="#b9">[11]</ref> and parallel pipeline <ref type="bibr" target="#b7">[9]</ref>. Although these algorithms were not designed with PC clusters in mind, we will show in Section 2 that these methods are equivalent (see Section 2.3) and efficient on a moderately sized COTS cluster. A similar study has been done in <ref type="bibr" target="#b20">[22]</ref> for shared memory architectures.</p><p>Many optimizations, taking advantage of the sparsity of the images locally generated on each node, have been proposed for the software methods, including parallel pipeline <ref type="bibr" target="#b7">[9]</ref>, binary tree <ref type="bibr" target="#b0">[1]</ref>, direct send <ref type="bibr" target="#b26">[28]</ref> and binary swap <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b22">24]</ref>. In this paper, we have not implemented any of those methods to concentrate on the worst case situation, but we will show in Section 2.6 that any compression could be advantageously integrated into sort-last rendering methods, including ours, as long as Equation 21 is respected.</p><p>Recent works using software parallel image compositing on graphics PC cluster include optimized direct send <ref type="bibr" target="#b26">[28]</ref>, optimized binary swap <ref type="bibr" target="#b28">[30]</ref>, binary tree over the Photonic Computing Engine <ref type="bibr" target="#b6">[8]</ref>, visualization of compressed volume datasets using direct send <ref type="bibr" target="#b27">[29]</ref> and binary swap with Chromium <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Stompel et al. <ref type="bibr" target="#b26">[28]</ref> obtain several (1 or 2) frames per second for a 600 × 800 × 129 volume dataset on a 1024 2 viewport, using a 64-node (1 GHz CPU) PC cluster with 100BaseT interconnect.</p><p>Takeuchi et al. <ref type="bibr" target="#b28">[30]</ref> report 45 frames per second (taking only image compositing into account) for a 512×512×730 volume dataset on a 512 2 viewport, using a 64-node (dual-Pentium III 1 GHz) PC cluster with Myrinet-2000 interconnect.</p><p>Kirihata et al. <ref type="bibr" target="#b6">[8]</ref> obtain 14 frames per second for a 256 3 volume dataset on a 512 2 viewport, using a 16-node (dual-Xeons 1.8 GHz, PNY NVIDIA Quadro FX3000) with Gigabit Ethernet.</p><p>Strengert et al. <ref type="bibr" target="#b27">[29]</ref> report 5 (resp. 8) frames per second for a 2048 × 2048 × 1878 volume dataset on a 1024 2 (resp. 512 2 ) viewport on a 16-node (dual-AMD 1.6 GHz, NVIDIA GeForce 4 Ti 4600) PC cluster with Myrinet interconnect, and 2 frames per second for a 256 3 time-varying volume dataset on a 8-node (Pentium4 2.8 GHz, NVIDIA GeForce 4 Ti 4200) PC cluster with Gigabit Ethernet interconnect (compared to 5 frames per second with the Myrinet cluster).</p><p>Houston <ref type="bibr" target="#b3">[4]</ref> reports compositing performance on the SPIRE <ref type="bibr" target="#b24">[26]</ref> cluster using the binary swap SPU of Chromium <ref type="bibr" target="#b5">[6]</ref>. The SPIRE cluster is a 16-node (Dual 2.4 GHz P4 Xeons, ATI Radeon 9800 Pro) PC cluster with Infiniband 4X interconnect. The reported compositing performances for a 1024 2 rendering area using 16 nodes are given on <ref type="table">Table 1</ref>. However, overall performance for a 1024 3 volume dataset is reported to only 8 frames per second.</p><p>Moreland et al. <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b12">14]</ref> introduced a method to efficiently perform sort-last rendering of extremely large data sets onto tile displays. Their implementation, called ICE-T, handles 450 million triangles on a 63 million pixels display at 0.06 frame per second.  <ref type="table">Table 1</ref>: Binary swap image compositing performance in frames per second on the SPIRE cluster (CPU and GPU blending).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2">Hardware compositors</head><p>Since 1999, several hardware architectures have been designed to support parallel image compositing for PC clusters: Sepia <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">12]</ref> and Sepia-2 <ref type="bibr" target="#b8">[10]</ref>, Lightning-2 <ref type="bibr" target="#b25">[27]</ref>, Metabuffer <ref type="bibr" target="#b30">[32]</ref>, MPC Compositor <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b17">19]</ref>, Orad's DVG. While some of them have become commercially available (Sepia-2 is available through HP and Sepia 3 is under development <ref type="bibr" target="#b23">[25]</ref>, MPC Compositor is marketed by Mitsubishi Precision Co., Ltd.), they are complex and expensive for a COTS system. We will show that our approach is highly competitive compared to these hardware solutions.</p><p>Stoll et al. <ref type="bibr" target="#b25">[27]</ref> obtain 13 frames per second for polygon rendering on a hybrid 1280 × 1024 (frame buffer)/800 × 600 render area using a 9-node (Pentium4 1.5 GHz, NVIDIA GeForce2 Ultra) PC cluster with a Lightning-2 matrix.</p><p>Lombeyda et al. <ref type="bibr" target="#b8">[10]</ref> report compositing performance of 24 to 28 frames per second for volume datasets on a 1024 2 render area using a 8-node PC cluster with SeverNet-II interconnect and Sepia-2 hardware compositing. Frank et al. report in a recent technical report <ref type="bibr" target="#b1">[2]</ref> 6 frames per second on huge volume datasets for a 1280 × 1024 render area using 9 nodes of a 33-node PC cluster with SeverNet-II interconnect and Sepia-2 hardware compositing.</p><p>Nonaka et al. <ref type="bibr" target="#b17">[19]</ref> obtain compositing performance of 13.8 frames per second for volume datasets on a 1024 2 render area using a 9-node (Pentium4 2.4 GHz, NVIDIA GeForce FX5950 Ultra) PC cluster with Gigabit Ethernet and a MPC compositor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Organization of the paper</head><p>The remainder of this paper is organized as follows. In Section 2, we present a performance evaluation of standard sort-last parallel rendering methods, based on a detailed analysis of the algorithm and of recent hardware components. In Section 3, we describe our novel implementation of sort-last rendering, that fully overlaps CPU(s), GPU and network at all stages of the process. Section 4 reports performance results of our implementation on a 3 years old 32-node PC cluster and on a 1.5 years old 5-node PC cluster, both with Gigabit interconnect. Our best results show volume rendering at constant 31 frames per second and polygon rendering at constant 17 frames per second on a 1024 × 768 render area. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PERFORMANCE EVALUATION OF SORT-LAST RENDERING</head><p>In this Section, we evaluate the time needed to display a frame on a cluster of n + 1 nodes (n slaves and one master) using sort-last rendering. The frame resolution is x × y for a total of xy pixels. For each pixel, we use bpp bits to store its color and zdth bits to store its depth.</p><p>The general algorithm for sort-last rendering is depicted on Each step will be detailed in the remaining of this Section. In the general case, the overall performance is limited by the slowest node. To simplify, we will assume that we have an homogeneous PC cluster, that the dataset is distributed in a balanced way, that no compression of any kind is applied to image and depth buffers. In other words, the amount of work for each frame is the same on all slave nodes. The Z component of each step is optional and may be removed in the case of back to front image compositing (for instance for volume rendering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Rendering</head><p>The render term (green) is the time needed to render the assigned dataset on a x × y frame. Let f ps denote the rendering speed in frames per second of the rendering method at the given x × y resolution. Then:</p><formula xml:id="formula_0">render = 1 f ps xy (2)</formula><p>The render term is an important component in the sort-last rendering approach, for two main reasons. First, it clearly impacts on the overall frame rate. Second, it consumes GPU and CPU (for instance for scene graph traversal), which can not be used for other tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reading pixels</head><p>The read term (blue) is the time needed to read back the color buffer and the depth buffer into main memory, and is defined as:</p><formula xml:id="formula_1">read = l read xy + xy × bpp b read xy + l readZ xy + xy × zdth b readZ xy (3)</formula><p>where l is the latency in seconds and b is the bandwidth in bits per second of the GPU operation. The latency l is in the order of a few us, and is negligible when reading large buffers. In theory, the peak bandwidth b of AGP x2 bus is 4.2 Gb/s and of AGP x8 bus is 16.8 Gb/s [7]. Upcoming PCI Express buses are expected to provide a theoretical peak bandwidth of 260 Gb/s (bounded by the memory peak bandwidth).</p><p>In practice, the sustained bandwidth is much lower, mostly because this functionality is not requested by game developers. Moreover, experimental measurements show that the bandwidth b is related to the size xy of the buffer <ref type="bibr" target="#b3">[4]</ref>.</p><p>For a while, the maximum readback performance commonly reported on most graphics hardware was bounded by 1.6 Gb/s (see for instance the the NVIDIA General FAQ <ref type="bibr" target="#b18">[20]</ref>). Things have been evolving recently. Tests on the SPIRE cluster <ref type="bibr" target="#b24">[26]</ref> report read peak bandwidth of 6.9 Gb/s for RGBA frame buffer and 2.4 Gb/s for depth buffer on the ATI Radeon 9800 Pro <ref type="bibr" target="#b3">[4]</ref>. Our own tests on the NVIDIA 6800 Ultra report read peak bandwidth of 4 Gb/s for both RGBA frame buffer and depth buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image compositing</head><p>The compose term (s× red and yellow) is the time needed for the image compositing, and can usually be decomposed into s successive steps:</p><formula xml:id="formula_2">compose = s ∑ i=1 compose i<label>(4)</label></formula><p>Several software parallel image compositing algorithms have been proposed. It can easily be shown that direct send, binary swap and parallel pipeline can be expressed in s steps with a single send and a single receive operation per node per step. The time needed for each step i is decomposed into:</p><formula xml:id="formula_3">compose i = send i + recv i + blend i<label>(5)</label></formula><p>The send i and recv i terms (red) are the time needed for a node p to send a part of the image (xy i pixels) to a node p 1 and to receive another part of the image to be composed with (generally also xy i pixels) from a node p 2 . Then:</p><formula xml:id="formula_4">send i = l send xy i + xy i × bpp b send xy i + l sendZ xy i + xy i × zdth b sendZ xy i<label>(6)</label></formula><p>and:</p><formula xml:id="formula_5">recv i = l recv xy i + xy i × bpp b recv xy i + l recvZ xy i + xy i × zdth b recvZ xy i<label>(7)</label></formula><p>where l and b are the latency in seconds and the bandwidth in bits per second of the network operations.  If the network and the interconnect support full duplex send and receive operations (which is the case for Gigabit Ethernet), it may be possible to overlap them, so that:</p><formula xml:id="formula_6">compose i = max(send i , recv i ) + blend i<label>(8)</label></formula><p>We will now assume:</p><formula xml:id="formula_7">compose i = sendandrecv i + blend i<label>(9)</label></formula><p>with:</p><formula xml:id="formula_8">sendandrecv i =    send i + recv i or max(send i , recv i ) (10)</formula><p>A critical point at this stage is the aggregate communication bandwidth sustained by the interconnection. If the sum of all communications is more than the aggregate bandwidth, then a severe performance degradation occurs. To simplify, we will assume for now an infinite aggregate communication bandwidth, allowing full scalability in terms of nodes and communications.</p><p>The blend i term (yellow) is the time need to blend the two subimages at step i and is defined as:</p><formula xml:id="formula_9">blend i = l blend +</formula><p>xy i × (bpp + zdth) b blend <ref type="bibr" target="#b9">(11)</ref> where l blend is the latency of the blending operation and b blend is the number of bits (color and depth) per second the blending method can handle. We assume in this Section that blending is completely done in the CPU (GPU blending will be discussed in Section 3.4): in general, the latency l blend is close to zero. The theoretical peak value of b blend is hard to evaluate. It clearly depends on the CPU clock speed and of the memory efficiency. Strengert et al. <ref type="bibr" target="#b27">[29]</ref> provide an optimized MMX code but no performance figures. As an example, our SSE2 implementation of alpha blending (no depth) described in Section 3.4 can handle 4 Gb/s. To summarize, the compose term is equal to:</p><formula xml:id="formula_10">compose = s ∑ i=1 (send i + recv i + blend i ) = s × (l sendandrecv + l sendandrecvZ + l blend ) + ( s ∑ i=1 xy i ) × bpp b sendandrecv + zdth b sendandrecvZ + bpp + zdth b blend (12)</formula><p>The number of steps s and the amount of data transferred at each step xy i depend on the chosen compositing algorithm. We will study two cases, first binary swap, second direct send and parallel pipeline, and we will show that all these methods are mostly equivalent on a moderately sized COTS cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Binary swap image compositing</head><p>Classical binary swap image compositing requires n = 2 k slave nodes. Then, the algorithm completes in s = k = log 2 (n) steps. At each step i, the amount of transferred data is </p><formula xml:id="formula_11">xy i = xy 2 i . Then: s ∑ i=1 xy i = log 2 (n) ∑ i=1 xy 2 i = xy × (1 − 1 n )<label>(13)</label></formula><formula xml:id="formula_12">i = xy n . Then: s ∑ i=1 xy i = n−1 ∑ i=1 xy n = xy × (1 − 1 n )<label>(14)</label></formula><p>which is exactly the same as Equation 13 for the binary swap case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Comparison</head><p>In terms of performance on a PC cluster, binary swap differs from direct send and parallel pipeline only in the number of calls to the network functions. In the binary swap, each node makes log 2 (n) calls, while in the direct send and parallel pipeline, each node makes n − 1 calls. Considering a latency of 100 us (Gigabit Ethernet), <ref type="figure" target="#fig_5">Figure 5</ref> gives an upper bound of the maximum obtainable frame rate for the different methods, only due to latency problems. The binary swap algorithm clearly scales better with a high latency (100 us) and a high number of nodes (over 128).</p><p>The choice between the different algorithms should be motivated by the total number of nodes. For instance binary swap is limited to using a power of two slave nodes. For a large number of nodes (over 128) and a high latency for network operations (about 100 us), binary swap would be the best choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Collecting pixels</head><p>The collect term (orange) is the time needed to collect the n subimages that compose the final image:</p><formula xml:id="formula_13">collect = n ∑ i=1 l collect xy/n + xy n × bpp b collect xy/n = n × l collect xy/n + xy × bpp b collect xy/n<label>(15)</label></formula><p>where l and b are again the latency in seconds and the bandwidth in bits per second of the network operations (see Section 2.3). We assume here that we do not collect the depth buffer on the master node, although it might be necessary for some applications. On moderately sized COTS cluster, this time is independent of the number of slave nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Drawing pixels</head><p>The draw term (purple) is the time needed by the master node to draw the color buffer from main memory into the graphics memory, and is defined as:</p><formula xml:id="formula_14">draw = l draw xy + xy × bpp b draw xy<label>(16)</label></formula><p>where l and b are again the latency in seconds and the bandwidth in bits per second of the GPU operation. They have the same theoretical values as for the read term in Section 2.2 In practice, the sustained bandwidth to send data from the main memory to the GPU is higher than the one to read the data back. Once again, this is due to game developers, that require this functionality.</p><p>For instance, the NVIDIA General FAQ <ref type="bibr" target="#b18">[20]</ref> reports writes performance of 5.44 Gb/s on the Quadro FX family (probably with AGP x8), and even 13.6 Gb/s (AGP x8) and 7.68 Gb/s (AGP x4) when using the NV pixel data range extension. Our own tests on the NVIDIA 6800 Ultra report draw peak bandwidth of 7.6 Gb/s for RGBA frame buffer (without using the extension).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Compression</head><p>Some compression (bounding rectangles, RLE, . . . ) can be applied before sending the frame buffer and the depth buffer to the network (as mentioned in Section 1.2), in order to save bandwidth and speed-up the sort-last rendering. We have chosen not to implement compression in order to keep the network load constant: this is the worst case situation, and performance could be enhanced (at the price of instable frame rates) if the following is respected.</p><p>Let us assume a compression speed of b c Gb/s, a compression ratio of r c , a decompression speed of b d Gb/s. Then, the time needed to send xy pixels with bpp bits per pixel is defined as: and has to be compared to the time needed to send the uncompressed pixels:</p><formula xml:id="formula_15">l c + xy × bpp b c + l send + (xy × bpp) × (1 − r c ) b send + l d + xy × bpp b d<label>(17)</label></formula><formula xml:id="formula_16">l send + xy × bpp b send (18)</formula><p>Then, in order to benefit from compression, we need to ensure (assuming that latencies are negligible) that: <ref type="bibr" target="#b17">(19)</ref> in other words:</p><formula xml:id="formula_17">1 b c + 1 − r c b send + 1 b d &lt; 1 b send</formula><formula xml:id="formula_18">b send b c + b send b d &lt; r c<label>(20)</label></formula><p>In the case of a Gigabit Ethernet network, this leads to: <ref type="figure" target="#fig_7">Figure 6</ref> plots a curve showing the dependencies between the three variables. As an illustration, our SSE2 implementation of alpha blending (no depth) described in Section 3.4 can handle 4 Gb/s: the compression ratio at this speed should be over 50%!</p><formula xml:id="formula_19">1 b c + 1 b d &lt; r c<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Summary</head><p>To illustrate this Section, we will use a sort-last volume rendering application (no Z term) on an ideal COTS cluster with no latencies. Then the time to render an image of x × y pixels is:</p><formula xml:id="formula_20">time = render + read + compose + collect + draw = 1 f ps + (xy × bpp) × 1 b read +(1 − 1 n ) × ( 1 b sendandrecv + 1 b blend ) + 1 b collect + 1 b draw<label>(22)</label></formula><p>If we now assume that our ideal COTS cluster is equipped with 3 GHz processors (using our SSE2 implementation of alpha blending described in Section 3.4, b blend = 4 Gb/s), full duplex Gigabit Ethernet with infinite aggregate bandwidth, AGP x8 graphics, then: </p><formula xml:id="formula_21">time = 1 f ps + (xy × bpp) × ( 1 16.8Gb/s + (1 − 1 n ) × ( 1 1Gb/s + 1 4Gb/s ) + 1 1Gb/s + 1 16.8Gb/s )<label>(23)</label></formula><p>So, the overhead in second to render a frame (with a classical RGBA 32 bits per pixel), only due to sort last rendering, is bounded by:</p><formula xml:id="formula_22">32 × xy × 18.8 16.8Gb/s ≤ overhead ≤ 32 × xy × ( 18.8 16.8Gb/s + 5 4Gb/s )<label>(24)</label></formula><p>The corresponding rendering speed in frames per second are given on <ref type="figure" target="#fig_8">Figure 7</ref> for different resolutions: they give an upper bound of the obtainable frame rate on our COTS cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PIPELINED SORT-LAST RENDERING</head><p>We present in this Section our new sort-last rendering algorithm -"pipelined sort-last" -that fully overlaps CPU(s), GPU and network usage at all stages of the parallel process. This idea is similar in the spirit to the hardware pipelining introduced in Sepia <ref type="bibr" target="#b10">[12]</ref>, but on a COTS cluster without dedicated hardware.</p><p>In this Section, we will focus on pipelined sort-last volume rendering. Pipelined sort-last polygon rendering (with depth compositing) can easily be extended from the volume version. <ref type="figure" target="#fig_9">Figure 8</ref> gives an overview of our new algorithm, using the same color coding as <ref type="figure" target="#fig_2">Figures 3 and 4</ref>. Each slave nodes is composed of three concurrent threads: the GPU thread (in charge of GPU), the Compose thread (in charge of parallel image compositing) and the Send thread (in charge of sending the final subimage to the master node).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-threading the slave nodes</head><p>In a classical implementation, the master node proceeds as follows. It starts by sending the point of view for the image f to be rendered. The n slave nodes then compute the subimage they have been assigned to, before sending it to the master node (orange). The master node receives the n subimages (orange), draws the final image to the GPU (purple), and then sends a new point of view for image f + 1. During this time, the slave nodes are idle, waiting for the new point of view.</p><p>Our first optimization consists in starting the rendering of image f for the next point of view while the master node is receiving the subimages and drawing the final image f − 1 to the GPU for the current point of view. This is done by splitting the work of each slave node into three concurrent threads. The GPU thread repeatedly gets a new point of view, renders the image, and reads back the frame buffer. The Compose thread is in charge of parallel image compositing once a frame buffer has been read and is available in memory. For now, we can assume that these two threads work sequentially. The Send thread just wait for a local subimage to be computed by the Compose thread and sends it to the master node, while the GPU thread is already working on the next point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overlapping reading pixels with image compositing</head><p>In a classical implementation, the first step of parallel image compositing (red) starts when the readback operation (blue) is completed, as described in Section 2.3.</p><p>When reading back the frame buffer from GPU to main memory, the CPU and the network are not busy. The ideal solution would be to send the frame buffer content directly to the network (not passing through main memory). This is unfortunately not possible with actual technologies.</p><p>Our second optimization consists in overlapping the readback operation (blue) with the network send and receive operations (red) of the first step of parallel image compositing. This way, sending and receiving subimages can start before the readback operation is completed.</p><p>The frame buffer is read by regions by the GPU thread, and the regions are sent and received through the network by the Compose thread as soon as one region is available (using a producer/consumer model). This is possible because the graphics card and the network card are connected to different buses (respectively AGP and PCI-X) via the north and the south bridges.</p><p>A read operation actually consists in two read operations. The first reads the region to be sent, the other reads the region corresponding to the subimage to be received. As soon as the subimage has been received, subimages blending (yellow) can be done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overlapping sending, receiving and blending</head><p>As described in Section 2.3, parallel image compositing is really efficient when a send and a receive operation can be done simultaneously. This supposes that the network and the interconnect support full duplex (which is the case for Gigabit Ethernet), but also that the application (i.e. the Compose thread) does the job correctly. Achieving full duplex send and receive operations could be done using two threads (one for the send, one for the receive). We have implemented this solution and we have found that, since each slave node already uses three threads, adding one more thread becomes difficult to handle for the operating system, even with hyperthreading enabled.</p><p>We have chosen to implement the full duplex send and receive operations with another strategy. The Compose thread repeatedly calls an asynchronous send and an asynchronous receive, that returns (quasi) immediately the number of bits they have been able to send or receive. If an asynchronous receive call returns zero (no packets have been received) and that enough data is available for blending, then the blending operation is done on a cache line, before the next asynchronous calls. This allows to overlap send, receive and blend operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Blending optimization</head><p>Another optimization concerns the blending of subimages (yellow) at each step of the parallel image compositing method, as described in Section 2.3.</p><p>Several optimizations have been proposed in the literature, including using the GPU accelerated blending <ref type="bibr" target="#b3">[4]</ref> or software compositing using MMX <ref type="bibr" target="#b27">[29]</ref> instructions. Other optimizations could include using multiple threads.</p><p>GPU accelerated blending has been proven to be very efficient <ref type="bibr" target="#b3">[4]</ref> thanks to very fast read and draw operations. As shown on <ref type="table">Table 1</ref>, the parallel image compositing performance jumps from 14 frames per second with software blending to 45 frames per second with GPU accelerated blending. However, using the GPU for the blending makes it unavailable for rendering. When rendering is overlapped with blending, this solution has to be avoided, except if a second GPU unit can be dedicated to blending.</p><p>Using multiple threads for image blending is highly efficient, since the blending algorithm is trivially parallel. However, when several threads are already running on the machine, the speed-up is not so high. This solution could be applied if one ore more CPUs are available on the machine.</p><p>A lower level parallelism can be obtained with vector instructions. Using MMX operations, Strengert et al. <ref type="bibr" target="#b27">[29]</ref> blend two pixels together in 20 operations. We have implemented a similar blending using SSE2 operations, and we can blend 2 pixels together 2 times in 21 operations. On a 3 GHz Pentium4 processor, we can blend 13 millions pairs of 32-bit RGBA pixels per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTATION</head><p>We have implemented our pipelined sort-last algorithm in C using OpenGL. Several rendering applications have been plugged in it, including volume and polygon rendering.</p><p>We have run our tests on a 3 years old 32-node (dual-Xeon 1.7 GHz, NVIDIA GeForce3 NV20) COTS cluster with Gigabit Ethernet interconnect on a Extreme Networks 6816 BlackDiamond switch, and on a 1.5 years old 5-node (dual-Pentium4 3 GHz, NVIDIA 6800 Ultra) COTS cluster with Gigabit Interconnect on a Cisco 3750 switch.</p><p>On the newest cluster, we have been able to test two different values of the Maximim Transmission Unit (MTU): 1500 and 9000 (Jumbo frames). On the oldest one, we have unfortunately not been able to change the default 1500 MTU (due to hardware limitations).</p><p>We have performed several runs of both polygon and volume rendering using our pipelined sort-last method. <ref type="figure" target="#fig_10">Figure 9</ref> reports the performance obtained on our 1.5 years old 5-node cluster, for different MTU (1500 and 9000), different rendering areas (1024 × 768 and 1280 × 1024), and different values of f ps (i.e. the frame rate on each slave). The results are clearly better for volume rendering, because much less data has to be transferred over the network.</p><p>Using a bigger MTU not only increases the raw performance, but also ensures the stability of the frame rate. Using a MTU of 9000 ensures a constant frame rate, while the MTU of 1500 gives unstable frame rates, due to the overusage of the CPUs.</p><p>On the 3-years old 32-node cluster, the obtained performances are two to three times lower than on the newer cluster. This is mostly due the MTU of 1500, as simple experiences have proven. The maximum bandwidth for a half duplex communication is 0.9 Gb/s. The maximum cumulated bandwidth for a full duplex communication is 1 Gb/s (which is too low compared to the expected 2 Gb/s) with a CPU usage of 40% on the send side and 100% on the receive size.</p><p>The scalability of our method is hard to demonstrate, since our newer cluster has only 5 nodes and our older cluster does not support a MTU of 9000. On the 32-node cluster, it clearly does not scale, since for volume rendering on a 1024 × 768 rendering area (with f ps = 59.8), the obtained frame rates are respectively 13, 6 and 3 frames per second with 5, 9 and 17 nodes, compared to 31 frames per second on our 5-node cluster. However, our theoretical analysis of the work balancing between CPU(s), GPU and network gives us reasons to be optimistic.</p><p>Compared to other existing works described in Section 1.2, our performances on the 1.5 years old 5-node cluster are clearly better, even for hardware compositing solutions. The two only exceptions are the compositing performance of 24 frames per second reported in <ref type="bibr" target="#b8">[10]</ref> and the 45 frames per second reported in <ref type="bibr" target="#b3">[4]</ref>; however, when taking into account the rendering time, their frame rates drop respectively to 6 and 8 frames per second, which is way below to our results.</p><p>Let us now compare to the theoretical optimal rendering speed computed for our cluster and shown in <ref type="figure" target="#fig_8">Figure 7</ref>. We recall that the plotted curves correspond to a non optimized (no pipelining) sortlast volume renderer running on an ideal cluster, not taking into account the rendering time (render = 0 in Equation 23). The upper bound of the obtainable frame rates for a 5-node cluster are respectively of 16 and 13 frames per second for a 1024 2 and a 1280×1024 rendering area. Our best results as shown by <ref type="figure" target="#fig_10">Figure 9</ref> report best performance of respectively 31 and 19 frames per second for the same rendering areas (with render = 1/60 in Equation 23). This definitely proves the efficiency of our pipelined algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have presented a new pipelined sort-last parallel rendering method that fully overlaps CPU(s), GPU and network usage at every level. By overlapping the different computations and communications, we have clearly reduced the overhead of parallel image compositing. Our best results show volume rendering at constant 31 frames per second and polygon rendering at constant 17 frames per second on a 1024 × 768 render area.</p><p>As part of our future work, we want to benefit both from our detailed analysis of sort-last rendering methods and our experimentations to specify and build a new 16-node graphics cluster, that should be able to prove the scalability of our method. Experimentations on other research groups graphics clusters would also be welcome. Future work also include the implementation of dynamic load balancing <ref type="bibr" target="#b21">[23]</ref>, SLIC image compositing <ref type="bibr" target="#b26">[28]</ref> and volume compression <ref type="bibr" target="#b27">[29]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1. 2</head><label>2</label><figDesc>Related work 1.2.1 Software parallel image compositing Many software parallel image compositing algorithms have been proposed in the literature, and can be applied either to volume IEEE Visualization 2005 October 23-28, Minneapolis, MN, USA 0-7803-9462-3/05/$20.00 ©2005 IEEE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sort-last parallel rendering: the dataset is subdivided and distributed across the cluster nodes, each node renders a full resolution image (left), a parallel image compositing algorithm is applied to compose the final image (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Flow diagram of sort-last parallel rendering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig- ure 3 .</head><label>3</label><figDesc>Figure 4shows a profiling plot for the binary swap image compositing scheme. The same color coding is used in bothFigures. For each new point of view, the time needed to display a frame can be decomposed in successive steps: time = render + read + compose + collect + draw (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Typical run of a non optimized sort-last volume renderer (no Z term) using binary swap image compositing with 8 nodes (the first slave node is also the master node).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Binary swap (100 us) Direct send and parallel pipeline (100 us) Scalability of binary swap versus direct send and parallel pipeline. ThisFigure showsan upper bound of the obtainable frame rate due to latency (100 us) in the (Gigabit Ethernet) interconnect (full duplex case, infinite aggregate bandwidth, no depth, independently of the resolution of the rendering area).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2. 3 . 2</head><label>32</label><figDesc>Direct send and parallel pipeline image compositing Direct send and parallel pipeline algorithms complete in s = n − 1 steps. At each step i, the amount of transferred data is xy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Dependencies between compression ratio r c , compression speed b c (Gb/s) and decompression speed b d (Gb/s) on a Gigabit Ethernet interconnect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Best case(1 node, no compositing)   Our case (4 nodes) Worst case (infinite number of nodes) Theoretical optimal rendering speed in frames per second in the best and worst cases (volume rendering case).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Flow diagram of pipelined sort-last parallel rendering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Frame rates observed on the master node for different rendering speeds ( f ps) on the 4 slave nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>reports theoretical latency and peak bandwidth for different interconnects, including Gigabit Ethernet. In practice, latency and sustained bandwidth are different from the theoretical values: precise figures are reported in<ref type="bibr" target="#b15">[17]</ref>.</figDesc><table><row><cell>Interconnect</cell><cell>Bandwidth (half-full)</cell><cell>Latency</cell></row><row><cell>Gigabit</cell><cell>1-2 Gb/s</cell><cell>100-150 us</cell></row><row><cell>Infiniband 4x</cell><cell>10-20 Gb/s</cell><cell>3.5-7 us</cell></row><row><cell>Myrinet</cell><cell>2-8 Gb/s</cell><cell>3.5-7 us</cell></row><row><cell>SGI NUMAlink4</cell><cell>8-16 Gb/s</cell><cell>1-2 us</cell></row><row><cell>Quadrics</cell><cell>9 Gb/s</cell><cell>1-2 us</cell></row><row><cell>SCI/Dolphin</cell><cell>4 Gb/s</cell><cell>1-2 us</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Bandwidth and latency for different interconnects.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Joe Kniss for providing the volume rendering code used in our experimentations. We also want to thank to the anonymous reviewers for their helpful comments. This work is supported by grants from the Inria and the Region Lorraine (Pôle de Recherche Scientifique et Technologique "Intelligence Logicielle"/CRVHP).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient sort-last rendering using compression-based image compositing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Painter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Eurographics Work. on Parallel Graphics and Visualization</title>
		<meeting>of the 2nd Eurographics Work. on Parallel Graphics and Visualization</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Massive volume rendering on a volume visualization cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Stony Brook University</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable distributed visualization using off-the-shelf components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heirich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1999 IEEE Symp. on Parallel Visualization and Graphics</title>
		<meeting>of the 1999 IEEE Symp. on Parallel Visualization and Graphics</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Designing graphics clusters. Parallel Rendering Work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>IEEE</publisher>
			<pubPlace>Vis</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmented ray casting for data parallel volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1993 Symp. on Parallel Rendering</title>
		<meeting>of the 1993 Symp. on Parallel Rendering</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chromium: a streamprocessing framework for interactive rendering on clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Klosowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A sort-last rendering system over an optical backplane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kirihata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Murata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CITSA 2004</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image composition schemes for sort-last polygon rendering on 2d mesh multicomputers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Nicholas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable interactive volume rendering using off-theshelf components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombeyda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Breen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heirich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE 2001 Symp. on parallel and large-data visualization and graphics</title>
		<meeting>of the IEEE 2001 Symp. on parallel and large-data visualization and graphics</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel volume rendering using binary-swap compositing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Krogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable 3D compositing using PCI Pamette</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heirich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sepia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Seventh Annual IEEE Symp. on Field-Programmable Custom Computing Machines</title>
		<meeting>of the Seventh Annual IEEE Symp. on Field-Programmable Custom Computing Machines</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A sorting classification of parallel rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From cluster to wall with VTK</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Moreland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE 2003 Symp. on Parallel and Large-Data Visualization and Graphics</title>
		<meeting>of IEEE 2003 Symp. on Parallel and Large-Data Visualization and Graphics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sort-last parallel rendering for viewing extremely large data sets on tile displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Moreland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wylie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pavlakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE 2001 Symp. on Parallel and Large-Data Visualization and Graphics</title>
		<meeting>of IEEE 2001 Symp. on Parallel and Large-Data Visualization and Graphics</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nextgeneration visual supercomputing using PC clusters with volume graphics hardware devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ogata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koshizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kaji-Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shimokawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the</title>
		<meeting>of the</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netpipe</surname></persName>
		</author>
		<ptr target="http://www.scl.ameslab.gov/Projects/NetPIPE/" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parallel volume-rendering algorithm performance on mesh-connected multicomputers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1993 Symp. on Parallel Rendering</title>
		<meeting>of the 1993 Symp. on Parallel Rendering</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hybrid hardware-accelerated image composition for sort-last parallel rendering on graphics clusters with commodity image compositor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nonaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kukimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Watashiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/SIGGRAPH Symp. on Volume Visualization and Graphics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<ptr target="http://developer.nvidia.com/object/GeneralFAQ.html" />
	</analytic>
	<monogr>
		<title level="j">nVIDIA Developer General FAQ</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="http://spire.stanford.edu/raptor/" />
		<title level="m">Raptor</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comparison of parallel compositing techniques on shared memory architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Third Eurographics Work. on Parallel Graphics and Visualisation</title>
		<meeting>of the Third Eurographics Work. on Parallel Graphics and Visualisation</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parallel rendering with k-way replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE 2001 Symp. on Parallel and Large-data Visualization and Graphics</title>
		<meeting>of the IEEE 2001 Symp. on Parallel and Large-data Visualization and Graphics</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Differential coding scheme for efficient parallel image composition on a PC cluster system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<ptr target="http://www.hp.com/techservers/hpccn/scivis/index.html" />
		<title level="m">Scientific/engineering visualization collaboration (Sepia)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<ptr target="http://spire.stanford.edu/" />
		<title level="m">SPIRE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lightning-2: a high-performance display subsystem for PC clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caywood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Han-Rahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH</title>
		<meeting>of SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SLIC: Scheduled linear image compositing for parallel volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stompel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patch-Ett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. on Parallel and Large-Data Visualization and Graphics</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical visualization and compression of large volume datasets using GPU clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strengert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symp. on Parallel Graphics and Visualization</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An improved binaryswap compositing for sort-last parallel rendering on distributed memory multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hagihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="11" to="12" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient compositing methods for the sort-last-sparse parallel volume rendering system on distributed memory multicomputers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable isosurface visualization of massive datasets on cots clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Blanke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE 2001 Symp. on Parallel and Large-data Visualization and Graphics</title>
		<meeting>of the IEEE 2001 Symp. on Parallel and Large-data Visualization and Graphics</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
