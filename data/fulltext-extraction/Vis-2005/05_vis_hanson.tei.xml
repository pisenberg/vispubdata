<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Exploration of the Fourth Dimension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
							<email>hanson@cs.indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Indiana University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
							<email>huizhang@cs.indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Indiana University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Exploration of the Fourth Dimension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques</term>
					<term>H.5.2 [Information Interface and Presentation]: User Interfaces-Haptic I/O</term>
					<term>G.4 [Mathematics of computing]: Mathematical Software-User Interfaces</term>
					<term>I.2.9 [Artificial Intelligence]: Robotics-Kinematics and Dynamics multimodal, haptics, visualization</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: Multimodal perceptual exploration of the 3D projection of the spun trefoil, a knotted sphere embedded in 4D.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>"Words and language, whether written or spoken, do not seem to play any part in my thought processes. The psychological entities that serve as building blocks for my thought are certain signs or images, more or less clear, that I can reproduce and recombine at will. The elements that I have mentioned are, in my case, visual and sometimes motor." -Albert Einstein, in a letter to mathematician Jacques Hadamard <ref type="bibr" target="#b9">[10]</ref>.</p><p>We now possess interactive graphics tools incorporating touchresponsive features that make it possible for a computer interface to implement and even to improve upon the multisensory mental paradigm described by Einstein. By exploiting such tools, we feel that we can make a non-trivial contribution to building intuition about classes of geometric problems whose intuitive, non-symbolic comprehension lies beyond the reach of the unaided human intellect. The challenge is to find specific examples where, in fact, we IEEE Visualization 2005 October 23-28, Minneapolis, MN, USA 0-7803-9462-3/05/$20.00 ©2005 IEEE.</p><p>have problems that are simple enough to formulate clearly, yet complex enough that even Einstein would have had trouble working out the details in "images that [one] can reproduce and recombine at will."</p><p>In this paper we take the first steps towards this goal by designing and implementing novel multimodal methods for exploring surfaces (two-manifolds) embedded in 4D Euclidean space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK ON 4D VISUALIZATION</head><p>The idea of cross-dimensional understanding has long been a subject of fascination, starting with Flatland's conundrum of how two-dimensional creatures might attempt to understand threedimensional space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. Banchoff's pioneering work on the corresponding question of how 3D computer-based projections can be used to study 4D objects <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> has been particularly influential. Other representative efforts include a variety of ways to render 4D objects (see, e.g., Noll <ref type="bibr" target="#b20">[20]</ref>, Hollasch <ref type="bibr" target="#b13">[14]</ref>, Banks <ref type="bibr" target="#b3">[4]</ref>, Roseman <ref type="bibr" target="#b25">[25]</ref>, and Egli, Petit, and Stewart <ref type="bibr" target="#b7">[8]</ref>), and to extend lighting model techniques to 4D (see, e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">28]</ref> and <ref type="bibr" target="#b11">[12]</ref>).</p><p>Typically, 4D visualization methods employ a projection to 3D as a fundamental step; this helps the viewer to identify salient global features of the four-dimensional object, and provides structural continuity when rotating either the object's (rigid) 3D projection, or, with more difficulty, the 4D orientation matrix, which causes nonrigid deformations in the 3D projection. While the resulting imagery supplies a sense of the object's overall shape, structural continuity is often difficult to discern. What is really needed is an enhancement of the 3D visual representation that allows intuitionbuilding exploration of the shape itself. Thus the question arises, "How can we touch the fourth dimension?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION</head><p>People learn about the everyday world by combining sensory modalities, and knowledge of shape comes from a combination of sight, touch, and exploration. By combining computer graphics with computer haptics, which imitate the 3D sense of touch, we can provide multimodal exploration tools that can in principle improve on real life. This improvement is possible because, for example, the image of a knotted rope is interrupted where one part crosses another, and if we try to trace a real knotted rope with a fingertip, we will eventually collide with some part of the rope and have to interrupt our smooth passage. With a touch-based computer interface, there are no physical obstructions to the motion of the pictured "computer hand," and the entire object can be traced without the interruptions imposed by sight or a real physical model. We can therefore use the same methods to help us understand and manipulate the much more complicated case of shapes with selfintersecting surfaces (which arise naturally when projecting 4D to 3D) using the touch-based multimodal paradigm. If sound cues are added to describe the passage of the computer hand across a visual obstruction, one can explore a knot or a surface without necessarily having to use vision at all; when used in combination with a visual representation, the auditory cues provide additional redundant feedback enabling improved intuitive perception of spatial structure. 3D Example. To create a visual representation of a 4Dembedded surface, we typically project the entire object to 3D, construct a standard 3D computer graphics representation of the result, and render that to a 2D screen image, possibly as a stereo pair. We may then, in effect, depend on the second order effects of our practical experience with 3D structures to reconstruct a 3D shape in our minds. Full 3D information can be obtained from stereography or motion parallax, or, given the resources, an actual physical model. However, while the physical model for the type of problem we are considering can be very interesting, they may not be as useful as one might think. In <ref type="figure" target="#fig_0">Figure 2</ref>, for example, we see images of an actual surface corresponding to a particular cubic polynomial projected from 4D to 3D <ref type="bibr" target="#b10">[11]</ref>; although the shape of the surface is completely without self-intersections in 4D, the model of the 3D projection has numerous very complex intersections, and, even with the physical model in hand, the self-intersections make it very difficult to trace and comprehend the intrinsic shape of the surface. We are thus motivated to consider how 4D shapes projected to 3D might be explored to advantage with a 3D haptic probe. <ref type="figure">Figure  3</ref>(a) shows a pair of intersecting surfaces that apparently pass right through one another, so that we would naturally think of a path on the green surface as blocked by the brown "wall." However, if these two surface patches were distinct in 4D and the 3D intersection only an artifact of projection, one surface might be "above" and the other "below;" using either an analogy to the cutaway method used to draw a knot on a blackboard, or a corresponding analogy to depth-buffered rendering, we could erase the clashing section of the brown wall as in <ref type="figure">Figure 3</ref>(b), and continue our path along the 4D green surface without interruption. Another alternative is to add an additional visual cue by assigning a surface color keyed to 4D depth relative to the projection center, as in <ref type="figure">Figure 3</ref>(c).</p><p>Finally, as suggested in <ref type="figure">Figure 3</ref>(d) as well as <ref type="figure">Figure 1</ref>, we can explore a more complex continuous 4D surface using a 3D haptic probe that avoids all physical entanglement: this is the main innovation that we shall present in this paper -a 3D probe that respects a surface's local 4D continuity despite the self-intersections of its 3D projection. By creating auditory cues that mark crossing events we achieve a full multisensory modality that can be used with or without supplementary visual representations.</p><p>Overriding Conflicting Evidence. The key ideas of the overall scenario should now be clear. The logical series of modeling steps, the problems they induce, and the ultimate resolution of the problems are as follows:</p><p>• Create a model of a smoothly embedded object. Examples are knotted curves embedded in 3D and knotted surfaces embedded in 4D, though there are obviously many other shapes that could be used.</p><p>• Project to one lower dimension. Various parts of the shape appear to touch each other when the smooth (non-intersecting) original shape is projected. What is seen is essentially the (N − 1)-dimensional "shadow" of the original N-dimensional object.  <ref type="figure">Figure 3</ref>: (a) Two intersecting surface segments, typically resulting from the projection of a 4D embedded object. (b) Crossing diagram of the surfaces corresponding to the green surface being closer to the 4D projection point than the brown surface. (c) A continuous 4D surface, where the pieces of the single surface do not touch in reality, but appear to intersect in the projection shown; depth is color-coded to show the absence of intersection. (d) Schematic summary of a tactile path with auditory cues enabling the meaningful 3D exploration of the spun trefoil, a knotted sphere embedded in 4D.</p><p>• The "shadow" object is visually discontinuous. The projected image has self-intersections.</p><p>• Tracing the "shadow" object is discontinuous. If you attach a haptic probe to the projection or "shadow" object, you must detach the probe wherever self-intersections occur, pull away, and re-attach on the other side of the intersection in order to trace the logically continuous surface. Alternatively, you can allow the probe to stay attached at the intersections and choose a direction, but this does not maintain consistent local continuity.</p><p>• Create a crossing-diagram object. By performing a depthbuffer operation in (N − 1) dimensions, you can do slightly better than the shadow; the part of the self-intersecting collision that is "in front" (nearer the projection point) can be made continuous (as in a depth-buffered rendering), and the part that is farther from the projection point can be interrupted.</p><p>• Tracing the crossing-diagram object is discontinuous. A haptic probe attached to the crossing-diagram object is continuous as long as we cross the self-intersection on the near segment, but once again we hit a discontinuity when we come around from the other direction and try to cross at the far segment; at this point, one again must detach the probe and move away to find the matching piece of the continuous object.</p><p>• Solution: Haptically override the conflicting evidence. To answer the question "How do we touch the fourth dimension?" we override the apparent visual collisions, conflicts, and gaps in the rendered image of the projection, and keep the haptic probe anchored to the higher-dimensional continuity that underlies the whole structure, regardless of whether it is above or below another conflicting part relative to the projection point.</p><p>• Exploit and manipulate the "phantom effect" to advantage.</p><p>As noted, e.g., by Massie and Salisbury <ref type="bibr" target="#b17">[17]</ref>, users may initially find it disturbing to have a single point on a haptic probe constrained to a surface, while the rest of the virtual hand is passing, ghost-like, through other parts of the object. Our paradigm focuses on learning to take advantage of the lack of physical obstruction, which, when present, makes even the most elaborate physical model unexplorable. In addition, we have experimented with one additional feature, a dynamic cutaway algorithm that opens a path from the viewpoint to the haptic probe contact point, thus making the tip of the haptic probe visible to the eye even when buried behind additional layers of the surface.</p><p>In summary, we facilitate perception of the intuitive structure of an object such as a 4D-embedded surface projected to 3D by emphasizing the contrast between apparent discontinuities in the visual representation; the interface enforces continuity in the haptic representation that emphasizes the true topology, even if this conflicts with what we see, while at the same time keeping visual contact with what is being touched as much as possible.</p><p>Overview of Interface Elements. Our interface design includes a variety of methods for providing redundant representation of 4D shape information, including the following:</p><p>• Local continuity of the haptic freedom of movement throughout the entire higher dimensional shape; visual conflicts that are artifacts of the projection have no haptic effects.</p><p>• Intelligent force-guided mechanisms for assisting the user in the haptic exploration. This assists the user in concentrating on navigating the shape itself, rather than being distracted by snags and discontinuities in the haptic path.</p><p>• 4D visual cues, including crossing diagram depth ordering methods and depth color coding.</p><p>• Attention-driven cutaways to "see through" occluding layers intersecting the line of sight between the viewpoint and the probe contact point.</p><p>• "Rolling manifold" methods that enable the user to optimize the local projection of the current local segment of the shape being touched by the cursor, providing accurate local metric information in the screen plane.</p><p>• Auditory cues that reinforce the occlusion information available from visual cues, and can potentially enable the exploration of an entire space without depending on visual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HAPTIC METHODS</head><p>Haptic interfaces can be effectively exploited to improve the sense of realism and to enhance the manipulation of virtual objects (see, e.g., <ref type="bibr" target="#b4">[5]</ref>). The haptic exploration of unknown objects by robotic fingers (see, e.g., <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>, <ref type="bibr" target="#b16">[16]</ref> and <ref type="bibr" target="#b30">[30]</ref>) is another variant that has requirements similar to ours. While we of course exploit many techniques of force-feedback and haptic user assistance that have been widely used in other interfaces (see, e.g., the work of <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[23]</ref>, and <ref type="bibr" target="#b24">[24]</ref>), we have found that many of the problems we encounter have been fairly unique, and thus have required customized hybrid approaches. In this section, we introduce a "predictive navigational assistance" model for traversing a 4D polygonal object using a virtual proxy. To be effective, the navigation assistance must meet two goals: on the one hand, it must constrain the probe to the surface of the 4D object to give the impression of a " magnetic object" to which the haptic probe is drawn; on the other hand, the force should be applied in a way that facilitates exploring the local continuity of 4D objects such as knotted spheres, whose visual pictures are interrupted by massive self-intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulated Sticky Stylus</head><p>Our basic force model implements a "sticky" stylus using a conventional damped-spring force to attach the stylus to the object surface. The proxy, a graphics point that closely follows the position of the haptic device, is constrained to the surface in question. The haptic rendering engine <ref type="bibr" target="#b26">[26]</ref> continually updates the position of the proxy, attempting to move it to match the haptic device position, and applying the damped-spring force between the haptic device position and the proxy position.</p><p>We use the following model to compute the force in the normal direction constraining the 3D cursor or proxy to the neighborhood of the object surface,</p><formula xml:id="formula_0">f n = αr 1+β ,<label>(1)</label></formula><p>where α is a constant and β = 0 for the standard case of an ideal (linear) spring. No force is applied in the direction tangent to the surface, thus allowing free motion and facilitating exploration of the structure. The damping force</p><formula xml:id="formula_1">f d = −K d V ,<label>(2)</label></formula><p>where V is the radial velocity, is used to smooth the force feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Constrained Navigation on the Local Surface Model</head><p>The force model presented so far in fact extends trivially to arbitrary dimensions of the vertex coordinates. The only essential difference is that in a 4D projection to 3D, each vertex has a 4D "eyecoordinate," or depth w, in addition to the coordinates (x, y, z) of the 3D projection. To constrain the proxy to a continuous surface embedded in 4D, we introduce local submodels of the geometry that cover a neighborhood of the current contact point in the 3D projection and serve as the current active haptic contact domain.</p><p>The following steps describe the haptic servo loop model:</p><p>1. Get coordinates. Let the current proxy coordinate data be denoted as (x cur , y cur , z cur , w cur ), and the previous proxy coordinate as (x pre , y pre , z pre , w pre ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Estimate local continuity.</head><p>In order to construct a continuous local structure to guide the user's movements, we first compute the depth range for a single local update period as</p><formula xml:id="formula_2">∆ w = w cur − w pre .<label>(3)</label></formula><p>The estimated local continuity of 4D depth is then given by:</p><formula xml:id="formula_3">w cur − ∆ w ≤ w ≤ w cur + ∆ w<label>(4)</label></formula><p>3. Construct local model. The search for candidate facets begins at the current proxy location and ends when Eq. (4) no longer holds. These facets form the local model for computing forces. An alternate implementation could directly exploit mesh continuity in parameter space, while the current method uses only 4D proximity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Haptically override the conflicting evidence. Now the sticky stylus can be applied trivially to the local model, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. The virtual proxy is constrained to a locally continuous domain to fully exploit the model's topological information, and haptic operations on this local model are fast and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Update.</head><p>Set (x pre , y pre , z pre , w pre ) to the values of (x cur , y cur , z cur , w cur ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.P P . P .</head><p>: This rendering rule allows us to slide through the visual intersections as though they were ghost images while constraining motion to the smooth surface that exists in 4D but cannot be seen without interruption in 3D. As shown in <ref type="figure" target="#fig_3">Figures 5(a)</ref> and (b), we can feel the four-dimensional continuity even though it is not apparent in the visual representation. <ref type="figure" target="#fig_3">Figure 5</ref>(c) depicts the continuous depthencoded 4D path on the 3D projection with the shaded surface removed. The path inherits the 4D depth cues from the original 4D shape and reveals the fact that the user is navigating the true continuous underlying surface and exploring a "feelable" structure. projection. (b) The proxy slides through the visual intersections and explores the continuous 4D structure. (c) The projected 3D path with color-coded 4D depth is exposed in wireframe mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adding Force Suggestions</head><p>The sticky stylus in principle is sufficient to allow the viewer to explore the full continuity of a given surface. However, in practice, many interesting geometric objects such as 4D knotted surfaces produce complex images that snag the stylus in sharp bends and obscure the structural continuity and local surface features (see, e.g., <ref type="figure" target="#fig_0">Figure 2</ref>). This presents a significant challenge to the user if only the bare attractive force interface is supplied. Our purpose in this section is to present a predictive navigation interface that assists and guides the user towards the neighboring surface features without depending on visual feedback concerning the direction the hand should move. Assuming we know the desired position and desired velocity for the stylus, we propose the following supplementary force</p><formula xml:id="formula_4">f pd = K p (P des − P) − K v (V des − V)<label>(5)</label></formula><p>where f pd is the force due to the predictive navigation assistance, P and V are the actual (current) contact point positions and velocities, and P des and V des are the desired contact point positions specified by the user if the default coordinates are to be overridden. K p and K v serve as stiffness and damping matrices. Starting from this point, we next investigate how Kalman Filters can be used as predictors to assist navigation control.</p><p>Noise Analysis. The position and velocity data obtained from haptic sensors are typically noisy due to both the motion of the human hand and the construction of the sensors themselves, as illustrated by the data in <ref type="figure" target="#fig_4">Figure 6</ref>; the measured velocity data (blue) are obtained by sliding the probe over a curved surface. The noise present in the measured data makes position and velocity prediction difficult.</p><p>Kalman Filtering. In order to predict the desired position and velocity from noisy data, we need to apply an appropriate filter. A logical candidate is the Kalman Filter (KF), which is essentially a recursive solution of the least-squares problem <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b18">18]</ref>. Among applications similar to ours, we note Simon's <ref type="bibr" target="#b27">[27]</ref> application of Kalman filtering to vehicle navigation, and Negenborn's <ref type="bibr" target="#b19">[19]</ref> study of the robot localization problem.</p><p>The KF and discrete stochastic dynamical equations for our system can be written as</p><formula xml:id="formula_5">State equation:x k+1 = 1 T 0 1 x k + T 2 /2 T u k + w k Output equation:ŷ k = 1 0 x k + z k wherex k = P k V k</formula><p>, and u k is the random, time-varying acceleration and T is the time between step k and step k + 1. We assume that the process noise w k is white Gaussian noise with noise covariance S w = E(w k w T k ), and the measurement noise z k is white Gaussian noise with noise covariance matrix S z = E(z k z T k ), and that it is not correlated with the process noise. Q is called the estimation error covariance, which is initialized as S w . The KF estimation is then formulated as follows:</p><formula xml:id="formula_6">Gain: K k = AQ k C T (CQ k C T + S z ) −1 Prediction:x k+1 = (Ax k + Bu k ) + K k (ŷ k+1 − Cx k ) Update: Q k+1 = AQ k A T + S w − AQ k C T S −1 z CQ k A T (A = 1 T 0 1 ; B = T 2 /2 T ; C = 1 0 )</formula><p>The KF provides optimal (minimum variance, unbiased) estimation of the statex k+1 with the given observed data. In <ref type="figure" target="#fig_4">Figure 6</ref>, we have plotted 30 time units of true velocity (red), measured velocity (blue), and the estimated velocity (green). We notice that the measured data introduce significant jitter (human and mechanical), and thus cause difficulty determining the desired position and velocity for predictive force rendering. We see explicitly that the noisy observed signal can be improved using the Kalman Filter to accurately predict the velocity for the haptic stylus (green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steering towards desired position</head><p>With the desired position P k+1 and velocity V k+1 , the force suggestion can be computed using Equation 5 at each servo loop. This force is then projected to local facet of the contact point, and applied to the haptic probe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments and Results</head><p>The proposed predictive "power assisted" force model was tested on a 2.8 GHz Pentium 4 PC running Windows XP with NVIDIA's GeForce FX 5200 graphics card. The graphics process routines were updated at about 20 Hz, whereas the haptics process ran at a 1 kHz update rate. As illustrated in <ref type="figure">Figure 1</ref> and <ref type="figure" target="#fig_3">Figure 5</ref>, the motion conforms to the local continuity constraints so that the user's probe effectively follows the shape of the projected 4D object.</p><p>Next, a series of tests were carried out to verify the effectiveness of our proposed method for constructing the local model. Haptic rendering of large and detailed 4D objects requires a significant computational load; by choosing to render only a local subset of the polygons in the haptic process, we can significantly reduce this load. <ref type="table" target="#tab_0">Table 1</ref> illusrates the typical saving in polygon count that we are able to achieve without any degradation of the user's perception of the interface; the local model strategy thus improves the system response at no observable cost. On the other hand, our algorithm exploits Kalman Filtering for more accurate estimations of a noisy dynamic system. The KF estimates the state of a noisy system using noisy measurements. In this way, our algorithm significantly reduces the effect of noise in obtaining clean path prediction, and this facilitates smooth navigation on the surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">AUDITORY CUES</head><p>The addition of sound cues to an environment is well-known to improve presence and realism (see, e.g., <ref type="bibr" target="#b31">[31]</ref>). Audible annotation of a user's exploration of a 4D object can be used to coordinate and accent particular events. Creating auditory "advice" to supplement the haptic exploration is significant because it makes it possible for users to build mental models of 4D geometry without necessarily depending on sight, though the visual feedback can be very useful as a redundant information source. We remark that using the multimodal interface with one's eyes closed creates an intense experience that could conceivably improve one's internal mental model because, paradoxically, it removes potentially confounding visual distractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Problem</head><p>Traditional techniques for perceiving visual pictures of 4D surfaces intersecting in a 3D projection rely on associating 4D depth with visual cues such as color coding, texture density, etc. Images are undoubtedly important for understanding complex spatial relationships and structures, but they may also serve to confuse the user when the visual evidence is difficult to interpret. There are currently only limited methods for presenting information non-visually, e.g., haptics and 3D hardcopy methods such as those used to produce the models in <ref type="figure" target="#fig_0">Figure 2</ref>. Physical models obstruct efforts at tactile exploration of continuous but self-intersecting surfaces by definition, leaving us with haptic methods; however, haptics methods alone do not necessarily convey the context of an exploration such as the location of a crossing or intersection. (Note that in 3D knot theory, the marking of over/under crossings in the 2D projection is an essential step.) While the location of a crossing can be flagged by the visual interruption itself, or by adding a slight jitter or jump to the haptic interface at the transition point, extending the modality to include auditory cues for these events is perhaps more versatile, like the narration of a "teacher" who is guiding us through a touch-based world. We are thus led to exploit sound, using word labels such as "over" and "under" to perceptually mark those points at which the continuous topological motion of the haptic device passes a visual disruption in the graphics image, as illustrated in <ref type="figure" target="#fig_5">Figure 7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Solution</head><p>We can thus choose to supplement our design by allowing the user to explore the 4D surfaces visually and haptically, but accompanied in addition by appropriate sounds triggered, e.g., by passing the locations of illusory 3D surface intersections. Using these multiple sensory modalities to present information can overcome contradictions in the visual feedback, or even make the visual feedback superfluous.</p><p>Example. When performing a haptic exploration revealing local 4D continuity, users can feel the shape of the object via the haptic interface as described above. They can trace out the shape of a path with their computer-idealized finger, hear, for example, a change in pitch to indicate 4D depth, and have specific auditory token sounds or spoken narration triggered at special locations such as visual obstruction transitions. A schematic picture for the auditionbased supplement to the mental-model building exploratory interface is shown in <ref type="figure" target="#fig_5">Figure 7</ref>(b).</p><p>As the user interacts with the 4D object in its 3D projection, the visual, haptic, and auditory stimuli seamlessly contribute to a new mental picture corresponding to 4D multimodal visualization. Thus our paradigm provides an interface that assimilates the three sensory modalities of our perceptual model and makes them all work together to achieve the goal of perceiving the nature of 4D shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">4D HAPTIC ROLLING MANIFOLD</head><p>There are some tasks that can be performed better by automated manipulation. For example, we can explore all points of a ball by rolling it around until the desired point comes up to face us; as advocated by Hanson and Ma <ref type="bibr" target="#b12">[13]</ref>, the presentation of arbitrary curves and surfaces to the user's view can be automatically optimized by "walking" along a local (typically geodesic) path. With the aid of the haptic probe and a 3D projection, we can adapt this family of methods to produce not only a maximal viewable but also a maximal touchable local region with each step of the walk; this permits the user with a haptic stylus to continuously sense a maximally presented portion of a neighborhood around the central point, and thus to get a richer tactile sense of the surface shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Maximizing Viewable and Touchable Aspects at Each</head><p>Step</p><p>Consider a surface represented by quadrilateral facets in ordinary 3D space. Our basic maximal projection procedure then requires that the center of the facet of interest lie completely in the screen plane, thus facilitating both viewing and touching. As shown in the <ref type="figure" target="#fig_6">Figure 8</ref>(a), we begin by choosing a point of interest P in the facet, and then use an algorithm such as shortest-path motion to transform the relation between the object and the viewpoint to maximize viewable area as we move to a new interest point P . If P lies in the current face, no rotation is needed. If P lies in an adjacent face, we rotate as prescribed below and then translate P to the screen center. Translation and rotation are applied in tandem in the haptics and graphics rendering threads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Rotating Between Facets</head><p>We now compute the transition across a given edge between two facets , assuming we know the unit vectors T (the probe's current incremental motion vector), and T , the projection of T straight down onto the target facet. WithN = T × T /|T × T | giving the rotation axis, and cos θ = T • T giving the rotation angle, the axis-angle rotation matrix Rotate(θ ,N) re-orients the user to match the projected geodesic direction, as shown in <ref type="figure" target="#fig_6">Figure 8</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Tunneling the Probe to Target the Surface</head><p>During the rolling process, visual perception of the interest point can be further assisted by opening a visual window into the interior of the 3D projection; this permits the viewer to keep the haptic probe continuously in view as it traverses any touchable part of the object, thus enabling both touching and seeing the true continuous topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Fixing the Stylus Using a "Rubber Band Line"</head><p>Since, in the paradigm we are describing, the facet of interest is fixed to the screen center both graphically and haptically, one would in principle imagine fixing the haptic probe to the screen center where the interest facet is projected; however, this is not necessarily a useful interface choice, since the user gets no tactile feedback. Instead, we implement a supplementary interface feature based on a simulated "rubber band line" from the current haptic position to the screen center, and then compute a haptic force rendered using Eq. (1) with β = 3, and r the distance from proxy position to screen center. This pulls the displaced haptic probe gently back to the screen center (like a weak rubber band tether), and attaches it to the newly projected interest facet. In this way, we can keep the facet of interest maximally projected, and yet allow the user to feel a limited continuous neighborhood of the central focus of attention, thus getting a tactile sense of the surface shape. <ref type="figure">Figure 9</ref> illustrates a 4D "rolling manifold" with the proxy constrained to the maximal viewable and touchable region with each step of the walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">USER ENVIRONMENT AND FURTHER EXAMPLES</head><p>Our implementation is based on a standard OpenGL graphics system with a high-performance graphics card and sound card, combined with SensAble Technology's Omni PHANToM forcefeedback haptic device. Our user interface is based on OpenGL, SensAble's OpenHaptics toolkit, and a locally customized GLUI API. Various options provide haptic forces and selected auditory signals to enhance the interactive feedback. <ref type="figure" target="#fig_7">Figure 10</ref> shows a representative view of the interface.</p><p>The user can load or create a variety of geometric objects for investigation, and is also presented with a wide selection of options to help maintain an optimal presentation of the object being studied. <ref type="figure" target="#fig_8">Figure 11</ref> shows the user exploring a 2-torus (technically the two-manifold S 1 × S 1 ) embedded in Euclidean 4-space and projected orthographically to 3D. We can see that the user can effectively slide through the visual interruption and walk on the continuous 4D structure.</p><p>In <ref type="figure" target="#fig_0">Figure 12</ref>, we show a geodesic path followed by the hapticsbased walking algorithm on an ordinary torus embedded in 3D (which happens to coincide with a particular perspective projection of the 2-torus from 4D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>Current computer interfaces can support multimodal representations that integrate visual information with haptic feedback and interaction. Exploiting these capabilities permits us to build a particular type of kinesthetic intuition about continuous complex geometric surfaces, and is especially suited to the families of such surfaces that result naturally from projecting 4D-embedded surfaces to 3D. In accordance with the best visualization science practice, different features can be applied to the overall images, using attention-driven cutaway algorithms to reduce the cognitive load still further.</p><p>In this paper, we discuss a set of methods to touch and see the geometric structures with a combination of haptic and visual sensing. We can begin to imagine the way that Einstein might have thought <ref type="figure">Figure 9</ref>: A haptic interface for constrained motion keyed to automate the manifold/knot orientation toward the viewer. The viewer can then see and touch the surface feature of interest locally. Perceiving the touched interest point is furthermore assisted for surfaces using the automated cutaway algorithm to suppress obscuring polygons. about the description of higher spaces by combining "visual and sometimes motor" sensations. With the multimodal approach advocated here, many facets of our intuitive real-life experiences can in principle be transferred to the fourth dimension.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Two views of a physical model representing a highly self-intersecting surface, constructed by 3D projection from the four-dimensional mathematical description. It is nearly impossible to trace the shape continuously and with clear comprehension even when you are holding the physical object in your hands. (Model courtesy of Stewart Dickson.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Despite apparent surface conflicts, only the local model with 4D continuity constraints is actively involved in the haptic process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>(a) This 4D object contains massive self-intersections in its 3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The KF helps to predict the desired velocity in the presence of noise. The desired position is calculated based on the desired velocity and provides an accurate prediction of the user's behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a) With visual feedback, the user is acutely aware of sliding through visual interruptions caused by the 3D projection. Without using visuals, however, one would be totally unaware of encountering a significant interruption unless supplementary cues such as sound feedback are supplied. (b) Sound cues supplement or replace visual cues to assist in building a clear mental model of a 4D surface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>(a) Surface facet with interest point P, maximally projected onto the 2D screen to facilitate viewing and touching. (b) 3D facet rotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Overview of the multimodal 4D exploration interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Exploration by following the continuous structure of the torus embedded in 4D, ignoring or overriding the disruptive self-intersections of the 3D projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Local Model vs Overall Model</figDesc><table><row><cell>4D Model</cell><cell>Polygon Number in Overall Model</cell><cell>Polygon Number in Local Model</cell></row><row><cell>4-Torus</cell><cell>900</cell><cell>220</cell></row><row><cell>Spun Trefoil Knot</cell><cell>2800</cell><cell>600</cell></row><row><cell>Steiner's Roman Surface</cell><cell>1600</cell><cell>360</cell></row><row><cell>Twist Spun Knot</cell><cell>2800</cell><cell>600</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by National Science Foundation grants IIS-0430730 and CCR-0204112. We gratefully acknowledge Yin Wu's contribution to the utility software packages, particularly the 4D cutaway and transparent blending tools, and Sidharth Thakur's contribution to the concepts and design of the haptic interface that was extended to support the functionality used in this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flatland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
			<publisher>Dover Publications, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualizing two-dimensional phenomena in fourdimensional space: A computer graphics approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Banchoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Image Processing and Computer Graphics</title>
		<editor>E. Wegman and D. Priest</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker, Inc</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="187" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond the third dimension: Geometry, computer graphics, and higher dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Banchoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American Library</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive display and manipulation of twodimensional surfaces in four dimensional space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Interactive 3D Graphics</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="197" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">dAb: Interactive haptic painting with 3D virtual brushes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">V</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Scheib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Proceedings</title>
		<editor>Eugene Fiume</editor>
		<imprint>
			<biblScope unit="page" from="461" to="468" />
			<date type="published" when="2001" />
			<publisher>ACM SIGGRAPH, ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shades of a higher dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics World</title>
		<imprint>
			<biblScope unit="page" from="93" to="94" />
			<date type="published" when="1987-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Planiverse: Computer Contact with a Two-Dimensional World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dewdney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Poseidon Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Moving coordinate frames for representation and visualization in four dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Egli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="905" to="919" />
			<date type="published" when="1996-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Intelligent support of interactive manual control: Design, implementation and evaluation of look-ahead haptic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>The University of British Columbia</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Mathematician&apos;s Mind: The Psychology of Invention in the Mathematical Field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Hadamard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>Revised from original 1945 edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A construction for computer visualization of certain complex curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Notices of the Amer.Math.Soc</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1156" to="1163" />
			<date type="published" when="1994-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Illuminating the fourth dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="54" to="62" />
			<date type="published" when="1992-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Space walking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Visualization &apos;95</title>
		<meeting>Visualization &apos;95</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="126" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Four-space visualization of 4D objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hollasch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Arizona State University</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Following an approximate geodesic path, illustrating the enhanced navigation mode on the ordinary torus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The conference Figure</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Optics and haptics: The picture</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m">on Multimodality of Human Communication: Theory, Problems and Applications</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An implicit-based haptic rendering technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrikou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sukhatme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Desbrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The PHANToM haptic interface: a device for probing virtual objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kenneth</forename><surname>Massie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salisbary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASME Dynamic Systems and Control</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="295" to="301" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The first chapter is available at www</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">S</forename><surname>Maybeck</surname></persName>
		</author>
		<ptr target=".cs.unc.edu/∼welch/Kalman/index.html" />
	</analytic>
	<monogr>
		<title level="m">Stochastic Models, Estimation, and Control</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1979" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Robot localization and kalman filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudy</forename><surname>Negenborn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Utrecht University</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A computer technique for displaying n-dimensional hyperobjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Noll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="469" to="473" />
			<date type="published" when="1967-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Haptic Exploration of Unknown Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><forename type="middle">M</forename><surname>Okamura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-06" />
			<pubPlace>California, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University, Department of Mechanical Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature detection for haptic exploration with robotic fingers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><forename type="middle">M</forename><surname>Okamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">R</forename><surname>Cutkosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="925" to="938" />
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Haptic rendering with predictive representation of local geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June Gyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Niemeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HAPTICS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Steering behaviours for autonomous characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">W</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Game Developers Conference</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Twisting and turning in four dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Roseman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Department of Mathematics, University of Iowa, and the Geometry Center</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sensable</surname></persName>
		</author>
		<title level="m">Inc. 3D Touch SDK OpenHaptics Toolkit Programmer&apos;s Guide</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Embedded Systems Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-06" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
	<note>Kalman filtering</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hidden volumes: The 4th dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Burton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics World</title>
		<imprint>
			<biblScope unit="page" from="71" to="74" />
			<date type="published" when="1987-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An introduction to the Kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
		<idno>TR 95-041</idno>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="27599" to="3175" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina at Chapel Hill, Department of Computer Science, Chapel Hill</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Haptic graphs for blind computer users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramloll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Brewster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on Haptic Human-Computer Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="102" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Auditory, graphical and haptic contact cues for a reach, grasp, and place task in an augmented environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihaela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">L</forename><surname>Zahariev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackenzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 5th Intl. Conf on Multimodal Interfaces (ICMI)</title>
		<meeting>of 5th Intl. Conf on Multimodal Interfaces (ICMI)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="273" to="276" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
