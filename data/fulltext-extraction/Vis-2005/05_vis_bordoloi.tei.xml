<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">View Selection for Volume Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udeepta</forename><forename type="middle">D</forename><surname>Bordoloi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State Univesity</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Wei</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State Univesity</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">View Selection for Volume Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.3.3 [Computer Graphics]: Picture/Image Generation-Viewing algorithms viewpoint selection</term>
					<term>view space partitioning</term>
					<term>volume rendering</term>
					<term>entropy</term>
					<term>visibility</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In a visualization of a three-dimensional dataset, the insights gained are dependent on what is occluded and what is not. Suggestion of interesting viewpoints can improve both the speed and efficiency of data understanding. This paper presents a view selection method designed for volume rendering. It can be used to find informative views for a given scene, or to find a minimal set of representative views which capture the entire scene. It becomes particularly useful when the visualization process is non-interactive-for example, when visualizing large datasets or time-varying sequences. We introduce a viewpoint &quot;goodness&quot; measure based on the formulation of entropy from information theory. The measure takes into account the transfer function, the data distribution and the visibility of the voxels. Combined with viewpoint properties like view-likelihood and view-stability, this technique can be used as a guide which suggests &quot;interesting&quot; viewpoints for further exploration. Domain knowledge is incorporated into the algorithm via an importance transfer function or volume. This allows users to obtain view selection behaviors tailored to their specific situations. We generate a view space partitioning, and select one representative view for each partition. Together, this set of views encapsulates the &quot;interesting&quot; and distinct views of the data. Viewpoints in this set can be used as starting points for interactive exploration of the data, thus reducing the human effort in visualization. In non-interactive situations, such a set can be used as a representative visualization of the dataset from all directions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the advent of faster hardware and better algorithms, volume rendering has become a popular method for visualizing volumetric data. The traditional challenge of speeding up the rendering to achieve interactive frame-rates has been overcome for small datasets, but large datasets still pose problems for users who do not have access to supercomputing facilities. Slow frame-rates, coupled with large datasets, result in an increase in time needed to gain insights from the data. The efficiency of the visualization process can be increased by guiding the user to more informative parts of the data (or parameters) and thus saving time she would have otherwise spent in a trial-and-error search. Another option is to show more information on the screen without having a negative effect (such as, due to occlusion or cluttering). For example, various alternative rendering techniques can be used to provide a more understandable picture to the user <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b3">[4]</ref>. Users can be guided to interesting features, isosurfaces and transfer functions by methods that suggest such candidates <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b18">[19]</ref>.</p><p>In this paper, we present a novel view selection method for volume rendering that can help improve the effectiveness of visualization by guiding the user to views that convey more information. Such interesting viewpoints are helpful both for data exploration and data presentation. For complex datasets, it is very difficult to manually find a view that maximizes the visibility of the relevant part of the data and minimizes occlusion. Currently, users can only use subjective judgment to evaluate and compare views. Our view selection technique introduces a measure to evaluate a view based on the amount of information displayed (and not displayed). It gives the users the ability to objectively compare two different views. The algorithm can be used to generate viewing positions to be used as starting viewpoints for browsing. Such suggested starting camera positions prove very beneficial in rendering situations with non-interactive frame-rates. Because of the time-lag between frames, users do not want to, and should not be made to <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b1">[2]</ref>, search the whole view-space for desirable views. The algorithm can also be used when presenting data in a non-interactive setting. It creates a smart partitioning of the view space, and selects representative views from each view group for rendering.</p><p>For this paper, we assume that the dataset is centered at the origin and that the camera is always looking at the origin from a fixed distance. We will refer to this set of camera positions as the view sphere. To evaluate and compare viewpoints, we use three viewpoint characteristics associated with each view:</p><p>• View goodness: The view-goodness measure tries to capture how closely the voxel visibilities for a given view match a user-input importance function. We define a view to be good if more important voxels in the volume are highly visible, and vice versa. It is maximized when the voxel visibilities are proportional to their importance. When selecting viewpoints, it is desirable that they have high goodness scores.</p><p>• View likelihood : Intuitively, the view likelihood of a given view is the number of other viewpoints on the view sphere which yield a view that is similar (defined by a threshold) to the given view. We define the view similarities in terms of voxel visibilities and importance that are used for view goodness. A highly likely view is a good candidate for representing the dataset from different views. On the other hand, low likelihood views are interesting because they display information that is not seen from most other viewpoints, and hence is likely to be missed by users during an interactive search.</p><p>• View stability : View stability of a view denotes the maximal change in view that can occur when the camera position is shifted within a small neighborhood (defined by a threshold). A small change implies a stable view, and a large change would make a view unstable. Unstable views make good starting viewpoints during interactive visualization, because the user can see a large change in view with a small mouse movement.</p><p>The contribution of this paper is as follows. We introduce a goodness measure of viewpoints based on the information theory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE Visualization 2005</head><p>October 23-28, Minneapolis, MN, USA 0-7803-9462-3/05/$20.00 ©2005 IEEE.</p><p>concept of entropy, also called average information. We propose that good viewpoints are ones which provide higher visibilities to the more important voxels. This interpretation leads us to the formulation of viewpoint information presented in this paper. We utilize a property of our entropy definition which indicates that when the visibilities are close to their desired values, the viewpoint information is maximized. This measure allows us to compare different viewpoints and suggest the best ones to the user. The voxel importance can be specified by the users based on their domain knowledge and the desired output. Given a desired number N of views, our algorithm can be used to find the best N viewpoints over the view space. A GPU-based algorithm is used to find the visibilities at the exact voxel centers of the volume. We also use the framework to find similarity between views, which is then used to create a view space partitioning and to find the likelihood and stability of views. Representative views for each partition can be chosen either by taking the highly likely or highly unlikely views. In interactive situations, our method can suggest unstable viewpoints, so that a small change in the camera position will yield a large change in view. For time-dependent data, we present a modification of the goodness measure of a viewpoint by taking into account not only the static information but also the change in each time-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The idea of comparing different views developed much before computer graphics and visualization matured. As early as 1976, Koenderink and van Doorn <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref> had studied singularities in 2D projections of smooth bodies. They showed that for most views (called stable views), the topology of the projection does not change for small changes in the viewpoint. The topological changes between viewpoints can be stored in an aspect graph. Each node in the graph represents a region of stable views, and each edge represents a transition from one such region to an adjacent one. These regions form a partitioning of the view space, which is typically a sphere of a fixed radius with the object of interest at its center. The aspect graph (or its dual, the view space partition) defines the minimal number of views required to represent all the topologically different projections of the object. A lot of research has been done since the early papers, mainly in the field of computer vision, which extended the ideas to more complex objects. In the case of volume rendering, a similar topology based partitioning can not be constructed. Instead, we find a visibility based partitioning by comparing visibilities of voxels in neighboring views, and clustering together viewpoints that are similar. Viewpoint selection has been an active topic of research in many fields. For instance, viewpoint selection solutions have been proposed for the problem of modeling a three-dimensional object from range data <ref type="bibr" target="#b21">[22]</ref> and from images <ref type="bibr" target="#b5">[6]</ref>, for object recognition <ref type="bibr" target="#b0">[1]</ref>, and also for cinematography <ref type="bibr" target="#b8">[9]</ref>. However, the topic has not been well investigated in the fields of computer graphics and visualization, possibly because applications in these domains have relied heavily on human control. Recently, Vázquez et al. <ref type="bibr" target="#b19">[20]</ref>[21] have presented an entropy based technique to find good views of polygonal scenes. They define an entropy for any given view, which is derived from the projected area of the faces of the geometric models in the scene. Their motivation is to achieve a balance between the number of faces visible and their projection areas. The entropy value is maximized when all the faces project to an equal area on the screen. In this conference, two solutions are being presented for selecting good viewpoints for volumetric data. Takahashi et al. <ref type="bibr" target="#b17">[18]</ref> calculate the view entropy for different isosurfaces and interval volumes, and then find a weighted average of the individual components. The weights are assigned using the transfer function opacities. The viewpoint measure presented in this paper is also based on the entropy function, but is defined on voxels as opposed to geometric objects. Each voxel in the data is assigned a visual significance, and the entropy is maximized when the visibilities of the voxels approach the respective significance values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIEWPOINT EVALUATION</head><p>The essential goal of this paper is to have a computer suggest 'good' viewpoint(s) to the user. This naturally leads us to the question: "what is a good viewpoint?", or, "what makes a viewpoint better than another?". The answer will depend greatly on the viewing context and the desired outcome. For example, a photographer will choose the view which best contributes to the chosen mood and visual effect. For this paper, the context is the process of volume rendering, which is being used to get visual information from the data. Hence, for our purposes, a viewpoint is better than another if it conveys more information about the dataset. In this section, we present a method for quantifying the information contained in a view using properties of the entropy function from information theory.</p><p>The information that is transferred from a volumetric dataset to the two-dimensional screen is governed by the optical model which is used for the projection. In this paper, we assume the popular absorption plus emission model <ref type="bibr" target="#b14">[15]</ref>. The intensity Y at a pixel D is given by</p><formula xml:id="formula_0">Y (D) = Y 0 T (0) + D 0 g(s)T (s)ds (1)</formula><p>where, T (s) is the transparency of the material between the pixel D and the point s. We will refer to T (s) as the visibility of the location s. The first term in the equation represents the contribution of the background, Y 0 being its intensity. The second term adds the contributions of all the voxels along the viewing ray passing through D. A voxel at point s has an emission factor of g(s), and its effect on the pixel intensity is scaled by its visibility T (s). If two voxels have the same emission factor, then the one with a higher visibility will contribute more toward the final image. The emission factors of voxels are usually defined by the users. They set the transfer function to highlight the group of voxels they want to see, and to make the others more transparent. We use this fact to define a noteworthiness factor for each voxel (section 3.2), which captures, among other things, the importance of the voxel as defined by the transfer function. Users can also specify the noteworthiness by other means -for example, using a separate importance volume. Based on the preceding discussion, we have the following two (not necessarily disjoint) guidelines for defining a good viewpoint:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A viewpoint is good if voxels with high noteworthiness factors</head><p>have high visibilities.</p><p>2. A viewpoint is good if the projection of the volumetric dataset contains a high amount of information.</p><p>In the following section, we present the details of our view information function and its properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entropy and View Information</head><p>Consider any information source X which outputs a random sequence of symbols taken from the alphabet set {a 0 , a 1 , . . . , a J−1 }. Suppose the symbols occur with the probabilities p = {p 0 , p 1 , . . . , p J−1 }. Alternatively, we can think of it as the random variable X which gets the value a j with probability p j . The information associated with a single occurrence of a j is defined in information theory as I(a j ) = − log p j . The logarithm can be taken with base 2 or e, and the unit of information is bits or nats respectively. In a sequence of length n, the symbol a j will occur </p><formula xml:id="formula_1">p = {p 0 , p 1 , p 2 }.</formula><p>The function is defined only over the plane</p><formula xml:id="formula_2">p 0 + p 1 + p 2 = 1, within the triangular region specified by 0 ≤ p 1 , p 2 , p 3 ≤ 1.</formula><p>The maximum occurs at the point p 0 = p 1 = p 2 = 1/3, and the value falls as we move away from that point in any direction. So, increasing the entropy has the effect of making the probabilities more uniform.</p><p>np j times, and will carry −np j log p j units of information. Then the average information of the sequence, also called its entropy, is defined as</p><formula xml:id="formula_3">H(X) ≡ H(p) = − J−1 ∑ j=0 p j • log 2 p j bits/symbol (2)</formula><p>with 0 • log 2 0 defined as zero <ref type="bibr" target="#b2">[3]</ref>. Even though the entropy is frequently expressed as a function of the random variable X, it is actually a function of the probability distribution p of the variable X. We will use the following two properties of the entropy function in constructing our viewpoint evaluation measure:</p><p>1. For a given number of symbols J, the maximum entropy occurs for the distribution p eq , where {p 0 = p 1 = . . . = p J−1 = 1/J}. (See figure 1, which gives an example of the entropy values for a three dimensional distribution.)</p><p>2. Entropy is a concave function, which implies that the local maximum at p eq is also the global maximum. It also implies that as we move away from the equal distribution p eq , along a straight line in any direction, the value of entropy decreases (or remains the same, but does not increase).</p><p>We will use probability distributions associated with views to calculate their entropy (average information). For each voxel j, we define an importance factor W j . We will call it the noteworthiness of the voxel, and it indicates the visual significance of the voxel. (More details about W j are given in section 3.2). Suppose, for a given view V , the visibility of the voxel is v j (V ). We are using the term 'visibility' to denote the transparency of the material between the camera and the voxel. It is equivalent to T (s) in equation <ref type="bibr" target="#b0">(1)</ref>. Then, for the view V , we define the visual probability, q j , of the voxel as</p><formula xml:id="formula_4">q j ≡ q j (V ) = 1 σ • v j (V ) W j where, σ = J−1 ∑ j=0 v j (V ) W j (3)</formula><p>where the summation is taken over all voxels in the data. The division by σ is required to make all probabilities add up to unity. Thus, for any view V , we have a visual probability distribution</p><formula xml:id="formula_5">q ≡ {q 0 , q 1 , . . . , q J−1 },</formula><p>where J is the number of voxels in the dataset. Then, we define the entropy (average information) of the view to be</p><formula xml:id="formula_6">H(V ) ≡ H(q) = − J−1 ∑ j=0 q j • log 2 q j<label>(4)</label></formula><p>The view with the highest entropy is then chosen as the best view. This satisfies the two guidelines presented earlier in section 3:</p><p>1. The best view has the highest information content (averaged over all voxels).</p><p>2. The visual probability distribution of the voxels is the closest (of all the given views) to the equal distribution {q 0 = q 1 = . . . = q J−1 = 1/J}, which implies that the voxel visibilities are closest to being proportional to their noteworthiness.</p><p>To calculate the view entropy, we need to know the voxel visibilities and the noteworthiness factors. Visibilities can be queried through any standard volume rendering technique such as ray casting. The noteworthiness, described in the next section, is view independent, and needs to be calculated only once for a given transfer function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Noteworthiness</head><p>The noteworthiness factor of each voxel denotes the significance of the voxel to the visualization. It should be high for voxels which are desired to be seen, and vice versa. Considering the diverse array of situations volume rendering is used in, it is practically impossible to give a single definition of noteworthiness that satisfies expectations of all users. Instead, we can rely on the user-specified transfer functions to deliver us a definition which is tailor-made for the particular situation. The opacity of a voxel, as assigned by the transfer function, is part of the emission factor g(s) in equation <ref type="formula">1</ref>, and controls the contribution of the voxel to the final image. We use opacity as one element of the noteworthiness of the voxel. Another consideration is that voxels of a particular color can be more visually meaningful to the viewer than voxels of another color. Consider a scene with voxels of two different colors. If there are fewer voxels of the first color compared to the second, then it is possible that the second group of voxels completely occludes the first one. Also, Gestalt principles <ref type="bibr" target="#b15">[16]</ref> suggest that the human mind extrapolates the larger object (called ground) behind the smaller one (called figure). Hence, other factors (such as opacities) being equal, we would want the voxels of the first color to have a higher importance than the others.</p><p>Based on these observations, we construct the noteworthiness W j of the jth voxel as follows. We assign probabilities to voxels in our dataset by constructing a histogram of the data. All the voxels are assigned to bins of the histogram according to their value, and each voxel gets a probability from the frequency of its bin. The information I j carried by the jth voxel is then − log f j , where f j is its probability (bin frequency). Then, W j for the voxel is α j I j , where α j is its opacity. We ignore voxels whose opacities are zero or close to zero, and these voxels are not included in the evaluation of equation <ref type="bibr" target="#b3">(4)</ref>. This reduces the computational and memory requirements for the entropy and similarity calculations (section 4.1).</p><p>The answer to what is interesting and what is not is very subjective. Our algorithm can be made to suit the goals of any particular visualization situation just by changing the noteworthiness factors. Domain specific knowledge can be readily incorporated into the framework -for example, using a separate importance volume which gives importance values for each voxel. It can be used in conjunction with, or in place of, the noteworthiness criteria described in this section. Irrespective of the method used to specify the interestingness of the voxels, maximizing the entropy serves to give better visibility to the more interesting voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A Simple Example</head><p>To demonstrate our concept of view information, we constructed the test dataset shown in figure 2. The voxel opacities of the cube dataset increase linearly with distance from the boundary of the cube. <ref type="figure" target="#fig_1">Figure 2(a)</ref> shows the volume rendering the dataset when the camera is looking directly at one of its faces. Next, we revolve the camera about the vertical axis of the dataset at 1 • increments (or equivalently, rotate the dataset in the opposite direction about the vertical axis). The view entropy steadily increases (figure 2(b)) as more and more voxels on the side face start becoming visible. It reaches a maximum when camera has moved by 45 • , which is the view that shows the two faces equally (figure 2(c)). Further movement of the camera results in greater occlusion of voxels near the first face, and the entropy begins to drop again. Upon evaluating the entropies for all camera positions around the dataset, the view in figure 2(d) results in the highest entropy. Clearly, this is one of the more informative views about the cube dataset for a human observer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Finding the Good View</head><p>The view selection proceeds as follows. The dataset is centered at the origin, and the camera is restricted to be at a suitable fixed distance from the origin. This spherical set of all possible camera positions defines the view sphere, and represents all the view directions. The view space is then sampled by placing the camera at sample points on this sphere. We create a uniform triangular tessellation of the sphere and place the viewpoints at the triangle centroids. The camera position and the origin specify the eye and the center points respectively for the modelview transformation. Since the roll of the camera does not affect the visibilities, the up vector can be arbitrarily chosen. Next, the voxel visibilities are calculated for each sample view position. Our technique is not dependent on any particular volume rendering method, and both software and hardware renderers can be used by modifying them to output voxel visibilities. (Please note that the transparency or voxel visibility as given in equation <ref type="formula">1</ref>is numerically the same as the accumulated opacity subtracted from unity.) Since we will be comparing the visual probability distributions (q) and the entropies (H(q)) of different views, it is desirable that we compute the visibilities at the same locations for all views (say, at the voxel centers). Most renderers, however, do not perform the opacity calculations exactly at the voxel centers. Ray-casters accumulate opacities along the rays, and texture based renderers accumulate opacities at frame-buffer pixel locations, neither of which are necessarily aligned with voxel centers. We use the GPU to calculate the visibilities at the exact voxel centers by rendering the volume slices in a front-to-back manner using a modified shear-warp algorithm. We give a brief description of our implementation below.</p><p>The object-aligned slicing direction is taken along the axis which is most perpendicular to the viewing plane. We use a floating point p-buffer with the same resolution as the volume slices. The first slice has no occlusion, so all the voxels in this slice have their visibilities set to unity. We iterate through the rest of the slices in a front-to-back order. In each loop, we calculate the visibilities of a slice based on the data opacities and the visibilities of the immediate previous slice. In each iteration, the following actions are performed. The frame-buffer (p-buffer) is cleared, and the camera set such that the current slice aligns perfectly with the framebuffer. Then, the immediate previous slice is rendered with a relative shear <ref type="bibr" target="#b12">[13]</ref>, and a fragment program combines its opacities with its visibility values. The frame-buffer now contains the visibilities of the current slice, and these will be used in the next loop. Render to texture techniques are used to prevent a copy from the framebuffer to a texture. After all the slices are processed, the entropy for the given view direction is calculated using equations (3) and <ref type="bibr" target="#b3">(4)</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> shows a time-step of a 256-cube shockwave dataset. The camera was rotated about the vertical axis in a complete circle, with the dataset centered at the origin. Entropy was evaluated at 5 • increments. The first figure shows the view at 55 • rotation which was the view with the highest entropy. <ref type="figure">Figure(b)</ref> shows the worst view, which occurred at 180 • . <ref type="figure" target="#fig_3">Figure 4</ref> shows a 128 × 128 × 80 tooth dataset. The view sphere was sampled at 128 points. <ref type="figure">Figures  (a)</ref> and (b) have the highest view entropy values. <ref type="figure">Figures (c)</ref> and (d) have the lowest entropy, and not surprisingly, are highly occluded views. It is notable that the viewpoints for (c) and (d) are not very far apart, and that (a) and (b) show much of the same voxels. This shows that if the user wants a few (say, N) good views from the algorithm, returning the N highest entropy views might not be the best option. Instead we can try to find a set of good views whose view samples are well distributed over the view sphere. The next section presents such a solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VIEW SPACE PARTITIONING</head><p>The goodness measure presented in the previous section can be used as a yardstick to measure the information captured by different volume rendering views and select the best view. But the calculation of goodness considers information from only the given view, and ignores the information that might be contained in other views. In particular, neighboring viewpoints tend to have similar visibilities, and comparing a viewpoint with its neighbors can provide additional properties of the viewpoint. Also, for most datasets, a single view does not give enough information to the user. The user will almost certainly want to look at the dataset from another angle. Instead of a single view, it is desirable to present to the user a set of   views such that, together, all the views in the set provide a complete visual description of the dataset. This can also be thought of as a solution to the best N views problem: given a positive number N, we want to find the best N views which together give the best visual representation of the dataset. We propose to find the N views by partitioning the view sphere into N disjoint partitions, and selecting a representative view for each partition. A similar partitioning is defined by aspect graphs <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b11">[12]</ref>, where each node (aspect) of the graph represents a set of stable views. Each set shows the same group of features on the surface of the object. However, the aspect graph creation methods deal mostly with algebraic and polygonal models and their topology, and cannot be applied in a straightforward manner to volume rendering. Instead, we compute the partitioning by grouping similar viewpoints together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">View Similarity</head><p>To find the (dis)similarity of viewpoints, we use the visual probability distributions associated with each viewpoint (section 3.1). Popular measures for computing the dissimilarity between two distributions p and p are the relative entropy (also known as the Kullback-Leibler (KL) distance), and its symmetric form (known as divergence) which is a true metric <ref type="bibr" target="#b2">[3]</ref>. (Please note that some texts refer to the KL distance as divergence instead.)</p><formula xml:id="formula_7">D(p p ) = J−1 ∑ j=0 p j log p j p j (5) D s (p, p ) = D(p p ) + D(p p)<label>(6)</label></formula><p>Although these measures have some nice properties, there are some issues with these measures that make them less than ideal. If p j = 0 and p j = 0 for any j, then D(p p ) is undefined. In our case, any voxel which is fully occluded (zero visibility) will get a visual probability q j of zero (equation <ref type="formula">3</ref>). If it is visible in one view but occluded in the other, we cannot evaluate equation 5 for these views. Also, D(p p ) and D s (p, p ) do not offer any nice upper-bounds. To overcome these problems, we instead use the Jensen-Shannon divergence measure <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_8">JS(p, p ) = JS(p , p) = K(p, p ) + K(p , p)<label>(7)</label></formula><p>where</p><formula xml:id="formula_9">, K(p, p ) = D(p ( 1 2 p + 1 2 p ))<label>(8)</label></formula><p>The distance between two views V 1 and V 2 , with distributions q 1 and q 2 , is then defined as JS(q 1 , q 2 ). This measure does not have the zero visual probability problem, since the denominator of the log term is zero iff the numerator is zero. It is also nicely bounded by 0 &lt; JS(q 1 , q 2 ) &lt; 2. Moreover, it can be expressed in terms of entropy <ref type="bibr" target="#b13">[14]</ref>, which allows us to reuse the view information calculations given in equation <ref type="formula" target="#formula_6">4</ref>:</p><formula xml:id="formula_10">JS(q 1 , q 2 ) = 2H( 1 2 q 1 + 1 2 q 2 ) − H(q 1 ) − H(q 2 )<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">View Likelihood and Stability</head><p>We can now use the definition of view-distance given by equation <ref type="bibr" target="#b8">(9)</ref> to define two additional characteristics of viewpoints -view likelihood and view-stability. View likelihood of a view V is defined as the probability of finding another view anywhere on the view-sphere whose view-distance to V is less than a threshold. In our scenario, it is given the number of view samples on the view sphere that are within the threshold of V . If a view has a (relatively) high likelihood, it implies that the object or dataset projects a similar image for a (relatively) large number of views. On the other hand, a view with low likelihood provides information that is unique to a few views. This property is indirectly taken into consideration when we partition the set of all the view samples (that is, the view sphere). Large partitions have views with high likelihoods.</p><p>Sometimes it is not the view itself but the change in view that provides important information. If the view is changed from one viewpoint to another very similar view, the user is not shown much new information. But, if the rendering changes by a large amount, the user sees not just the new information in the visualization but also derives knowledge from the change that has occurred. Occlusion is one of the most important depth cues that is available to the user when visualizing three-dimensional renderings on a twodimensional surface. A large change in occlusion implies a large change in visibilities, which results in a large JS distance between two viewpoints. View stability is a view property that captures this information and can be used to select viewpoints during interaction. It is defined as the maximal change that occurs when the viewpoint is moved anywhere within a given radius from its original position. The greater the change, the more unstable a viewpoint is. We calculate the (un)stability as the maximum view-distance between a view sample and its neighboring view samples in the triangular tessellation of the view-sphere. The 180 • viewpoint in <ref type="figure" target="#fig_2">figure 3(b)</ref> is an unstable viewpoint for this particular viewpoint sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Partitioning</head><p>Once the visual probability distributions (q) and their entropies (H(q)) are calculated as described in section 3, we use the JSdivergence to find the (dis)similarities between all pairs of view samples. We then cluster the samples to create a disjoint partitioning of view sphere. The number of desired clusters can be specified by the user. Each partition represents a set of similar views, i.e., these views show the voxels at similar visibilities. If desired, the JS-measure can be weighted using the physical distance between the view samples to yield tight regional clusters.</p><p>The best (highest entropy) views within each partition are selected as representatives of the cluster and displayed to the user. Together, this set of images gives a good visualization of the dataset from many different viewpoints. Sometimes, it might happen that the selected representatives of two neighboring partitions lie on the common boundary and next to each other. If the view distance between two selected view samples is less than a threshold, we use a greedy approach and select the next best sample. <ref type="figure" target="#fig_5">Figure 5</ref> shows the results of a 5-way partitioning of the view space for the tooth dataset. 128 view samples were used with a JS-divergence measure. The largest partition contains 39 samples, while the smallest one has 18. The representative views from four of the partitions are shown. The view for the fifth partition is figure 4(a). We would like to point out that figures 4(a) and 4(b) both are in the same partition. In fact, the top ten high entropy viewpoints all belong to the same partition, illustrating the need for selecting representative views from different partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TIME VARYING DATA</head><p>Suggestion of good views becomes all the more useful in the case of time dependent data. The time required to compute a volume rendering animation of the dataset grows with the number of time steps. In an interactive setting, this creates a large lag between a viewpoint update and the completion of rendering all the frames. Moreover, it takes more tries by the user to find the desired viewpoint because the data changes with time, and the user has to consider not only the current time step but also the previous and future ones. The user's job is made harder by cases where an interesting view in a few time steps turns out to be a dull view in the rest.</p><p>In section 3, we discussed the notion of a good view and presented a measure of view information for a volume dataset. For time-dependent data, using equation <ref type="formula" target="#formula_6">4</ref>separately for each timestep is not the best solution -it can yield viewpoints in adjacent time-steps that are far from each other, thus resulting abrupt jumps of the camera during the animation. A natural solution is to constrain the camera, but it still does not guarantee the most informative viewpoint. For instance, it can result in a viewpoint which has a high information value for each individual time-step, but does not show any time-varying changes. It is contrary to what is expected from an animation -it should show both the data at each time-step, and also the changes occurring from one frame to the next. In the next section, we present an alternate version of viewpoint information tailored to capture the view information present in one time-step, taking into account the information present in the previous step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">View Information</head><p>Consider two random variables X and Y with probability distributions r and s respectively. If X and Y are related (not independent), then an observation of X gives us some information about Y . As a result, the information carried by Y , conditional on observing X, becomes H(Y |X) ≡ H(s|r). Then the information carried together by X and Y is H(X,Y ) = H(Y, X) = H(X) + H(Y |X), as opposed to H(X) + H(Y ). We will use this concept to create a modified viewpoint goodness measure for time dependent data.</p><p>Suppose there are n time-steps {t 1 ,t 2 , . . . ,t n } in the dataset. For a given view V , we denote the entropy for time-step t i as H(V,t i ) ≡ H V (t i ). The view entropy for all the time-steps together is H V (t 1 ,t 2 , . . . ,t n ). We will assume a Markov sequence model for the data, i.e., the data in any time-step t i is dependent on the data of the time-step t i−1 , but independent of older time-steps. Then the information measure for the view, for all the time-steps taken together, is given by equation <ref type="bibr" target="#b10">(11)</ref>. (10) is a standard relation <ref type="bibr" target="#b2">[3]</ref>, and (11) follows from the independence assumption.</p><formula xml:id="formula_11">H(V ) = H V (t 1 ,t 2 , . . . ,t n ) = H(t 1 ) + H(t 2 |t 1 ) + . . . + H(t n |t 1 , . . . ,t n−1 ) (10) = H(t 1 ) + H(t 2 |t 1 ) + . . . + H(t n |t n−1 )<label>(11)</label></formula><p>The conditional entropies will be defined following the same principles outlined in section 3. We consider a view to be good when the visibilities of the voxels are in proportion to their noteworthiness. But in the time-varying case, the significance of a voxel is derived not only from its opacity, but also from the change in its opacity from the previous time-step. For the time-step t i , we then define the noteworthiness factor of the jth voxel as W</p><formula xml:id="formula_12">j (t i |t i−1 ) = {k • |α j (t i ) − α j (t i−1 )| + (1 − k) • α j (t i )} • I j (t i )<label>(12)</label></formula><p>where, 0 &lt; k &lt; 1 is used to weight the effects of voxel opacities and the change in their opacities. A high value of k will highlight the changes in the dataset. Suppose the visibility of the voxel for the view V is v j (V,t i ). Then, the conditional visual probability, q j (t i |t i−1 ), of the voxel is</p><formula xml:id="formula_13">q j (t i |t i−1 ) ≡ q j (V,t i |t i−1 ) = 1 σ • v j (V ) W j (t i |t i−1 )<label>(13)</label></formula><p>where, σ is the normalizing factor as in equation <ref type="bibr" target="#b2">(3)</ref>. The entropy of the view V is then calculated using equations <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b12">(13)</ref>. Voxels   with both low opacities and small changes (as defined by thresholds) are ignored for these calculations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS AND DISCUSSION</head><p>We have implemented our technique using a hardware-based visibility calculation (section 3.4). 128 sample views were used for each dataset. The camera positions were obtained by a regular triangular tessellation of a sphere with the dataset place at its center. The tests were run on a 2GHz P4, 8x AGP machine with a GeForce 5600 card. For a 128-cube dataset, the visibility calculations for all 128 views were completed in 42s, resulting in an average time per view of 0.33s. In case of a 256-cube dataset, the calculations for 128 views took 310s, with an average time per view of 2.42s. View selection results for the 128 × 128 × 80 tooth dataset have been shown in figure 4. <ref type="figure" target="#fig_5">Figure 5</ref> shows the results of a 5-way view space partitioning for the dataset using the JS divergence measure. The partitioning helps to avoid selection of a set of good views which happen to be similar to each other. Even though we have not considered the physical distance between the viewpoints during partitioning, it forces the selected viewpoints to be well distributed over the view sphere. <ref type="figure" target="#fig_6">Figure 6</ref> shows view evaluation results for a 128-cube vortex dataset. Both high and low quality views are shown for comparison.</p><p>For time-varying data, we used the view information measure presented in section 5. A sequence of 14 time-steps of the 128cube vortex data was used. The entropy for each view was summed over all the time-steps, as given by equation <ref type="bibr" target="#b10">(11)</ref>. The conditional entropy for each time-step was calculated with k = 0.9 in equation <ref type="bibr" target="#b11">(12)</ref>. A high value of k gives more weight to the voxels which  are changing their values with time compared to high opacity voxels which remain relatively unaltered. <ref type="figure" target="#fig_8">Figure 7</ref>(a) shows the view with the best cumulative entropy for the time-series. Although the summed entropy gives a good overall view for the whole timeseries, there might be other views which are better for particular segments of the time-series. <ref type="figure" target="#fig_8">Figure 7</ref>(b) plots the conditional entropies (H(t n |t n−1 )) for four selected views of the vortex dataset. The best overall view (figure 7(a)), which is represented by the blue curve (highest curve on the right edge), is not the best choice for the first half of the series. For long time sequences, it might be beneficial to consider different good views for different segments of time. View entropy was calculated over fifty time-steps of the 256cube shockwave dataset with 128 view samples. <ref type="figure">Figure 8</ref> shows four time-steps from a viewpoint which had a good entropy using the time-varying criteria, and figure 9 shows the corresponding time-steps for a viewpoint which resulted in a bad score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>We have presented a measure for finding the goodness of a view for volume rendering. We have used the properties of the entropy function to satisfy the intuition that good views show the noteworthy voxels more prominently. The user can set the noteworthiness of the voxels by specifying the transfer function, or by using an importance volume, or a combination of both. The algorithm can be used both as an aid for human interaction, and also as an oracle to present multiple good views in less interactive contexts. Furthermore, view sampling methods such as IBR can use the sample similarity information to create a better distribution of samples. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Entropy Function for three dimensional probability vectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of the change in view entropy (equation 4) with camera position for a test dataset. Figure (a) shows the initial position of the camera. Figure (b) shows the behavior of entropy as the camera revolves around the dataset (around the vertical axis in the figure) at 1 • increments. The entropy increases steadily and reaches a maximum for a movement of 45 •(figure (c)), and then begins to decrease again. The maximum entropy for the whole view space is obtained for the view infigure (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The figure shows a time-step of a 256-cube shockwave dataset. The camera was rotated about the vertical axis in a complete circle, with the dataset centered at the origin. Entropy was evaluated at 5 • increments.Figure (a)shows the view at 55 • rotation which was the view with the highest entropy.Figure(b) shows the worst view, which occurred at 180 • . Figure (c) plots the change of entropy with change in angle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The two highest entropy views for the tooth dataset are shown in (a) and (b), and the two worst ones in (c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Representative views for a 5-way partitioning of the view-sphere for the tooth dataset. The view for the fifth partition isfigure 4(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>View Evaluation results for a 128-cube vortex dataset.Figure (a) shows the recommended view with a high entropy value, (b) shows a bad view for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>View Evaluation for time-varying dataset. (a) The best overall view for 14 time-steps. (b) The conditional entropies of four selected views for each of the 14 time-steps. The view in (a) is represented by the blue plot (highest curve, top-right corner).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>View entropy results over 50 time-steps of the 256-cube shockwave dataset. Time steps 1, 16, 31 and 46 for a good view. Low entropy viewpoint for 50 time-steps of the 256-cube shockwave dataset. Time steps 1, 16, 31 and 46 are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Entropy vs Viewing Angle</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>12.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>12.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>12.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Entropy</cell><cell>12.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>11.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>11.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>45</cell><cell>90</cell><cell>135</cell><cell>180</cell><cell>225</cell><cell>270</cell><cell>315</cell><cell>360</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Viewing-angle</cell><cell></cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c)</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank Antonio Garcia for his help with our implementation. This work was supported by NSF grant ACI-0329323, NSF ITR grant ACI-0325934, DOE Early Career Principal Investigator award DE-FG02-03ER25572, and NSF Career Award CCF-0346883.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Viewpoint selection by navigation through entropy maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th IEEE International Conf. on Computer Vision (ICCV-99)</title>
		<meeting>of the 7th IEEE International Conf. on Computer Vision (ICCV-99)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="248" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">System response time operator productivity, and job satisfaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Lucas</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. of the ACM</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="972" to="986" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Principles and practice of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Blahut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Addison-Wesley Publ. Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Illustrative context-preserving volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EuroVis &apos;05</title>
		<meeting>of EuroVis &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Volume illustration: Non-photorealistic rendering of volume models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization &apos;00</title>
		<meeting>of IEEE Visualization &apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic camera placement for image-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Impact of system response time on state anxiety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Guynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. of the ACM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="347" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High-quality two-level volume rendering of segmented data sets on consumer graphics hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization &apos;03</title>
		<meeting>of IEEE Visualization &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="301" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The virtual cinematographer: A paradigm for automatic real-time camera control and directing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH &apos;96</title>
		<meeting>of SIGGRAPH &apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-automatic generation of transfer functions for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Durkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Symposium on Volume Visualization &apos;98</title>
		<meeting>of IEEE Symposium on Volume Visualization &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The singularities of the visual mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The internal representation of solid shape with respect to vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="211" to="216" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast volume rendering using a shear-warp factorization of the viewing transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lacroute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 1994</title>
		<meeting>of SIGGRAPH 1994</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Divergence measures based on the shannon entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding out about filling-in: A guide to perceptual completion for visual science and the philosophy of perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pessoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noë</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="723" to="748" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Response time and display rate in human performance with computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="285" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A feature-driven approach to locating optimal viewpoints for volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujishiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takeshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Nishita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization &apos;05</title>
		<meeting>of IEEE Visualization &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Salient iso-surface detection with model-independent statistical signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenginakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization &apos;01</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Viewpoint selection using viewpoint entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Vision, Modelling, and Visualization &apos;01</title>
		<meeting>of Vision, Modelling, and Visualization &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic view selection using viewpoint entropy and its application to image-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feixas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="689" to="700" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Next best view system in a 3-d modeling task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Symposium on Computational Intelligence in Robotics and Automation (CIRA)</title>
		<meeting>of International Symposium on Computational Intelligence in Robotics and Automation (CIRA)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="306" to="311" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
