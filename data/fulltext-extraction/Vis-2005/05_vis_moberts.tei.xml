<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of Fiber Clustering Methods for Diffusion Tensor Imaging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Moberts</surname></persName>
							<email>bmoberts@home.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Technische Universiteit Eindhoven Eindhoven</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vilanova</surname></persName>
							<email>a.vilanova@tue.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Technische Universiteit Eindhoven Eindhoven</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarke</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
							<email>vanwijk@win.tue.nl</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics and Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Technische Universiteit Eindhoven Eindhoven</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of Fiber Clustering Methods for Diffusion Tensor Imaging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-20T19:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diffusion Tensor Imaging</term>
					<term>Fiber tracking</term>
					<term>Clustering</term>
					<term>Clustering Validation</term>
					<term>External Indices</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: (a)Cluttered image showing the fibers in a healthy brain by seeding in the whole volume. The color coding shows main eigenvalue. (b)(c)(d) Clustering results. The color coding represents the clusters.(b) Hierarchical clustering with single-link and mean distance between fibers. (c) The same as (b) but with closest point distance between fibers. (d) Shared nearest neighbor with mean distance between fibers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Diffusion Tensor Imaging (DTI) is a Magnetic Resonance (MR) acquisition technique that measures the directional dependence of motion of water molecules in tissue. During diffusion, molecules probe tissue structure at microscopic scale, well beyond the usual image resolution. Experimental evidence has shown that water diffusion is anisotropic in organized tissue, such as white matter or muscle. DTI is the only non-invasive technique that can show in vivo the internal structure of white matter <ref type="bibr" target="#b0">[1]</ref>. Therefore, it is mostly used for brain imaging research in a variety of fields including brain development, brain tumor, focal epilepsy, and multiple sclerosis among others.</p><p>Several techniques to visualize DTI data exist <ref type="bibr" target="#b13">[13]</ref>. The most popular technique is to reconstruct the individual fibers from the tensor information, e.g., by tracing streamlines. Usually fibers are defined by manually setting seed points. In this case, the result is biased by the user who can miss important structures. Some methods propose to seed through the whole volume to avoid manual seeding <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b11">12]</ref>. However, white matter is a complex structure and the image gets easily cluttered (see <ref type="figure">figure 1a)</ref>. Therefore, it is difficult to get insight into the data using these visualizations.</p><p>Fibers form anatomically meaningful entities called bundles that define the connection of different grey-matter areas. Several authors have proposed to cluster the streamlines to obtain bundles <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. The enormous amount of individual fibers is reduced to a limited number of logical fiber clusters that are more manageable and understandable. Once a clustering is obtained, the DTI data can be viewed at different levels of detail; a global view which shows the fiber clusters and a local view which shows the individual fibers of a specific cluster. Furthermore, clustering might also be used to obtain quantitative comparisons by unbiased measurements in anatomically structures.</p><p>Different clustering algorithms and different options within a clustering algorithm (e.g., distance measure between fibers) can be chosen. Furthermore, clustering algorithms have parameters to tune such as the amount of clusters to obtain. Many combinations exist and therefore physicians are not able to evaluate all possible combinations. In figure 1b, 1c, and 1d, three different clustering results are shown. For clarity, fibers with a length shorter then 20 mm have been removed. Section 4 describes the methods that have been used to create these clusterings.</p><p>What we need is more insight in which combinations of algorithms and parameter settings give good results. But this requires the ability to assess the quality of a clustering. In section 3, we present an automatic evaluation process in which a quantitative evaluation of clustering results is done using clustering quality measures. These measures indicate the agreement between two partitions of a set of items; a partition produced by a clustering method and a ground truth (i.e., the ideal clustering defined by physicians). Several clustering quality measures exist in the literature <ref type="bibr" target="#b7">[8]</ref>. However, the question that arises is which measure meets the criteria of the physicians. To answer this question, the existing clustering quality measures are evaluated in section 5, and improvements are proposed to better match the physician's quality criteria.</p><p>In section 6, we evaluate different clustering algorithms using the new clustering quality measures and a limited data test. We implemented the hierarchical clustering algorithm and several fiber similarity measures. A shared nearest neighbor clustering algorithm that has not been used before in this context has also been implemented. We chose this algorithm because it can find clusters of different sizes and shapes in data that contains noise and outliers.</p><p>Finally, in section 7 conclusions are drawn and suggestions for future work are done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we present fiber tracking and currently used algorithms for clustering of white-matter fibers.</p><p>Diffusion is represented by a positive symmetric tensor of second order. Several techniques have been presented in the recent years for visualization of tensor fields <ref type="bibr" target="#b13">[13]</ref>. The most common approach to visualize this data, called fiber tracking, is by reconstructing the linear structures represented by the diffusion tensors. Fiber tracking can be divided into streamline tracing and probabilistic methods. In streamline methods, the tensor is simplified to a vector field defined by the main eigenvector. A streamline is the result of the integration of the vector field given an initial position. Therefore, initial positions or seed points need to be defined. Probabilistic methods propose to simulate the diffusion process given a starting point and find all possible paths with a measure of connectivity. The drawback of this approach is the computational cost and the fact that any pair of points in space is connected. Therefore, it is necessary to define not just a starting point but also end points, or establish criteria for which points are considered to be connected.</p><p>In both methods, the user normally defines seed points by specifying a Region Of Interest (ROI). A disadvantage of ROI fiber tracking is that the result is user biased, not reproducible and often fails to show all information. This also makes difficult unbiased comparison. If there is knowledge of the expected result (e.g., in a healthy person) the users can reasonably guess where the bundles of interest should be. However, when there is no real clue about the possible underlying structure (e.g., in pathological cases), the manual seeding can miss important structures. Some methods propose to seed through the whole volume <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b11">12]</ref>. However, in this case the image gets easily cluttered (see <ref type="figure">figure 1a)</ref>. In this article, we used the DTITool and the whole volume seeding method of Vilanova et al. <ref type="bibr" target="#b11">[12]</ref>.</p><p>A number of research groups have proposed algorithms for clustering fibers. Corouge et al. <ref type="bibr" target="#b3">[4]</ref> use a clustering method that propagates cluster labels from fiber to neighboring fiber. It assigns each unlabeled fiber to the cluster of its closest neighbor, if the closest neighbor is below some threshold. A partition of the data with a specific number of clusters can be acquired by setting a threshold on the maximal accepted distance. This is similar to the algorithm employed by Ding et al. <ref type="bibr" target="#b4">[5]</ref>.</p><p>Brun et al. <ref type="bibr" target="#b2">[3]</ref> use a spectral embedding technique called Laplacian eigenmaps in which the high dimensional fibers are reduced to points in a low dimensional Euclidean space. Next, these positions are mapped to a continuous RGB color space, such that similar fibers are assigned to similar colors. In another paper by Brun et al. <ref type="bibr" target="#b1">[2]</ref>, a clustering method based on normalized cuts is used to group fibers.</p><p>Shimony et al. <ref type="bibr" target="#b10">[11]</ref> employ a fuzzy c-means algorithm in which each fiber is associated with a cluster by a membership function that indicates the confidence that a fiber belongs to a cluster.</p><p>Finally, Zhang and Laidlaw <ref type="bibr" target="#b16">[16]</ref> use a hierarchical clustering algorithm for fiber clustering. An agglomerative hierarchical clustering method starts by putting each data point into an individual cluster, next at each stage of the algorithm the two most similar clusters are joined. By varying the definition of similarity between clusters, several variations of the agglomerative hierarchical clustering method can be devised.</p><p>Apart from a clustering algorithm, a fiber similarity measure is also needed to cluster fibers. A fiber similarity measure is a function that computes the (dis)similarity between pairs of fibers. Most fiber similarity measures are based on the Euclidean distance between certain parts of the fibers.</p><p>Corouge et al. <ref type="bibr" target="#b3">[4]</ref> form point pairs by mapping each point of one fiber to the closest point on the other fiber. The resulting point pairs are then used to define the distance between fiber pairs. Three distances are defined. The closest point distance is the minimum distance between a pair of points. The mean of closest point distances or mean distance is the average of the point pair distances. The Hausdorff distance is the maximum distance between a pair of points.</p><p>Brun et al. <ref type="bibr" target="#b2">[3]</ref> find fibers similar if they start and end in the same area, and define a measure that uses the distance between the end points. Zhang and Laidlaw <ref type="bibr" target="#b16">[16]</ref> define the distance between two fibers as the average distance from any point on the shorter fiber to the closest point on the longer fiber, and only distances above a certain threshold contribute to this average. Ding et al. <ref type="bibr" target="#b4">[5]</ref> first establish a corresponding segment, which are the parts of a pair of fibers that "overlap". Their fiber similarity measure is then defined as the mean distance between the corresponding segments. Finally, Brun et al. <ref type="bibr" target="#b1">[2]</ref> map the fibers to a Euclidean feature space and use a Gaussian kernel to compare the fibers in this new space.</p><p>These methods generate different partitions of the fibers. It is unclear which clustering methods and parameter settings give the best results. To the best of our knowledge, there is no literature that deals with the evaluation of fiber clustering methods. <ref type="figure" target="#fig_3">Figure 2</ref> shows the steps that we used in the validation process. The set of fibers created with the fiber tracking algorithm is clustered using a fiber similarity measure and a clustering algorithm. Two clustering algorithms are used: hierarchical clustering and shared nearest neighbor clustering. Using a particular fiber similarity measure, each clustering method produces different clusterings. The basic question here is what clustering method and what fiber similarity measure produce the result that is closest to the optimal clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLUSTERING AND VALIDATION FRAMEWORK</head><p>The first step of the validation process involves the creation of a ground truth, which is considered our optimal clustering. This is done by manually classifying the fibers into a number of bundles that correspond to actual anatomical structures.  Once a ground truth is established, a clustering quality measure is chosen to determine the agreement between the manually defined bundles and the automatically generated clusters. There are a number of clustering quality measures available in the literature. In the context of fiber clustering, the goal is to find a measure that meets the criteria of physicians. Therefore, the various clustering quality measures are validated. This is done by letting physicians create a ranking of a number of clusterings. This ranking is then used as a ground truth to which the rankings created by the clustering quality measures are compared. We propose several adjustments to the measures available in the literature such that they match the physicians criteria better. The measure that produces the ranking that has the highest correlation with the ranking of the physicians is considered the best measure, and is used to evaluate the cluster results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CLUSTERING ALGORITHMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ground Truth</head><p>The first step of the validation process is to establish a ground truth to which the cluster results can be compared. For our purposes, the ground truth is a manually defined classification of a set of fibers. The fibers are classified into a number of anatomical structures, called bundles, for which it is known that they can be reliably identified using the fiber tracking technique. Ideally, the classification is done by physicians. However, for this study we did the classification ourselves, and it was verified by physicians from the Máxima Medical Center (MMC) in Eindhoven.</p><p>Our ground truth includes the following bundles: the corpus callosum (cc), the fornix (fx), the cingulum (cgl, cgr) (both hemispheres) and the corona radiata (crl, crr) (both hemispheres) (see figure 3a and 3b). These anatomical structures are identified in a number of studies (e.g., <ref type="bibr" target="#b14">[14]</ref>) and can be reconstructed with the fiber tracking technique.</p><p>Manually specifying for each individual fiber to which bundle it belongs is a tedious and time-consuming task. Therefore, classification was done using regions of interest (ROIs). Each bundle is defined by a number of manually defined ROIs. Fibers are classified as belonging to a particular bundle if they pass through a specific number of the ROIs.</p><p>Fibers that cannot be assigned to a bundle are labelled "Unclassified" and are not part of the ground truth. The complete set of fibers is clustered, but only the classified fibers are used for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Clustering methods</head><p>The first method that we have used for fiber clustering is the wellknown hierarchical clustering algorithm, used for fiber clustering by Zhang and Laidlaw <ref type="bibr" target="#b16">[16]</ref>.</p><p>An agglomerative hierarchical clustering method starts by putting each data point into an individual cluster. Then at each stage of the algorithm the two most similar clusters are joined.</p><p>Based on the way similarity between clusters is defined, several variations of the agglomerative hierarchical clustering method can be devised. The two most basic cluster similarity measures are single-link and complete-link <ref type="bibr" target="#b8">[9]</ref>.</p><p>With the single-link measure, the distance between two clusters is the distance between the closest pair of items (one item from the first cluster, the other item from the second cluster). The single-link method works well for elongated and well separated clusters and it allows to find clusters of different sizes and complex shapes. It performs poorly on data containing noise, because noise may act as a bridge between two otherwise separated clusters. This is known as the chaining effect.</p><p>With the complete-link measure, the distance between clusters is the maximum distance between a pair of items (one item from either cluster). This tends to produce compact, more tightly bound clusters. The complete-link measure is less versatile than the singlelink algorithm because it is unable to find clusters of varying sizes or complex shapes.</p><p>The weighted-average cluster similarity measure is the average of the minimum and maximum distance between pairs of items from the different clusters.</p><p>Shared Nearest Neighbor (SNN) clustering <ref type="bibr" target="#b5">[6]</ref> is a clustering algorithm that has not yet been used for fiber clustering. We want to use the SNN algorithm because it has a number of beneficial characteristics in the context of fiber clustering. In particular, it can find clusters of different sizes and shapes in data that contains noise and outliers.</p><p>The SNN algorithm is based on the notion that two data points that share a lot of neighbors probably belong to the same cluster. In other words, "the similarity between two points is confirmed by their common (shared) neighbors" <ref type="bibr" target="#b5">[6]</ref>.</p><p>In the SNN algorithm, a k-nearest neighbor graph is constructed in which each data point corresponds to a node which is connected to the nodes of the k-nearest neighbors of that data point. From the k-nearest neighbor graph a shared nearest neighbor graph is constructed, in which edges exist only between data points that have each other in their nearest-neighbor lists. A weight is assigned to each edge based on the number and ordering of shared neighbors. Clusters are obtained by removing all edges from the shared nearest neighbor graph that have a weight below a certain threshold τ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VALIDATION OF CLUSTERINGS</head><p>Clustering validation is done because we want to be able to measure to which extent clustering methods and fiber similarity measures produce clusters that match the bundles of the ground truth, according to the preferences of physicians.</p><p>There are two important aspects, which we call correctness and completeness, that must be considered when comparing two partitions of fibers. Correctness implies that fibers of different anatomical structures are not clustered together; completeness means that fibers of the same anatomical structures are clustered together.</p><p>In practice there is a tradeoff between these two aspects. Achieving 100% correctness is not difficult: put every fiber into a singleton cluster, but this results in a completeness of 0%. On the other hand, achieving 100% completeness is also not difficult: put every fiber into the same cluster, but this results in a correctness of 0%. The comparison methods discussed in this section are all based on the notion that a good clustering must be both correct and complete with respect to the ground truth. <ref type="figure" target="#fig_1">Figure 3</ref> shows different partitions of the same set of fibers: the ground truth and the results of clusterings for the cc and cgl. The clustering in figure 3c is incorrect, because several bundles from the ground truth are together in the same cluster, which is not the case of figure 3e. The clustering in figure 3d is incomplete because a bundle from the ground truth is subdivided into several clusters. Only classified fibers are shown in these figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Clustering Quality Measures</head><p>An external index is a statistical measure that indicates the agreement between two partitions of a set of items <ref type="bibr" target="#b7">[8]</ref>. External indices can be seen as clustering quality measures. In our case the items are fibers, and the segmentations to be compared are the ground truth, which is thought of as being external to the clustering process, and a segmentation produced by a clustering algorithm. The level of agreement between these two partitions is expressed in a score between 0 (total disagreement) and 1 (perfect agreement).  <ref type="table" target="#tab_0">Table 1</ref> shows a contingency table, which is defined as follows: Let cell n i j be the number of fibers that are both in bundle b i as well as in cluster c j . The row sum u i is the number of fibers in bundle b i and the column sum v j is the number of fibers in cluster c j .</p><formula xml:id="formula_0">Bundle/Cluster c 1 c 2 . . . c S Sums b 1 n 11 n 12 . . . n 1S u 1 b 2 n 21 n 22 . . . n 2S u 2 . . . . . . . . . . . . . . . b R n R1 n R2 . . . n RS u R Sums v 1 v 2 . . . v S n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Same Cluster Different Cluster Sums</head><p>Same Bundle The number of pairs of fibers that can be generated given n fibers is M = If the two partitions agree completely then the Rand index returns a value of 1.00. Although the lower-limit of this index is 0.0, this value is rarely returned with real data <ref type="bibr" target="#b9">[10]</ref>. This is because the Rand index is not corrected for agreement by chance.</p><formula xml:id="formula_1">a = R ∑ i=1 S ∑ j=1 n i j 2 b = R ∑ i=1 u i 2 − a m 1 Different Bundle c = S ∑ j=1 v j 2 − a d= n 2 − a − b − c M − m 1 Sums m 2 M − m 2 M</formula><p>The Adjusted Rand index <ref type="bibr" target="#b6">[7]</ref> is the Rand index corrected for chance agreement. The general form of a statistic S that is corrected for chance is:</p><formula xml:id="formula_2">S = S − E(S) Max(S) − E(S)</formula><p>.</p><p>In this equation, Max(S) is the upper-limit of S, and E(S) is the expected value of S. In the case of the Rand E(S), a hypergeometric  distribution <ref type="bibr" target="#b6">[7]</ref> is assumed. If S returns its expected value then S is 0.0, and if S returns a value of 1.0 then S also returns 1.0. The Adjusted Rand index is defined as:</p><formula xml:id="formula_3">AR = ((a + d)/M) − E ((a + d)/M) 1 − E ((a + d)/M) = a − (m 1 m 2 )/M (m 1 + m 2 )/2 − (m 1 m 2 )/M .</formula><p>Milligan and Cooper <ref type="bibr" target="#b9">[10]</ref> compared the Rand, Adjusted Rand and a number of other external indices and concluded that the Adjusted Rand index is the measure of choice for cluster validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Validation of Clustering Quality Measures</head><p>The goal is to identify the best measure for determining the agreement between the cluster results and the ground truth. Our approach is based on the notion that the optimal cluster quality measure assigns scores to clusterings that are similar to the scores assigned by a physician. For this purpose, two physicians from the Máxima Medical Center were asked to rank simultaneously a number of clusterings. These clusterings were also ranked according to the various cluster quality measures discussed in the last section. The ranking of the physicians was then compared to the rankings from the cluster quality measures.</p><p>The ranking of the physicians and the scores assigned by the various cluster quality measures are given in table 3. A "++" means that the physicians found that particular aspect very good, a single "+" means that they found that aspect good, a "0" means that they found it average (depending on the context), and a "−" means they found this aspect bad in every situation. Notice that no aspect has been labelled "very bad". This is because it is very difficult for physicians to distinguish between a "bad" and a "very bad" aspect; a "bad" aspect is already something they cannot relate to.</p><p>The clusterings can be categorized based on the overall quality:</p><p>Good. Clusterings A and B were considered good by the physicians. The Adjusted Rand index agrees with the physicians and returns fairly high values. The Adjusted Rand index does not return a 1.0 for these clusterings because there were some fibers from the smaller bundles that were in different clusters. The physicians did not mind that these outliers were clustered apart, because they were visually different.</p><p>Average. The physicians found the clusterings C, D, E and F average. All four clusterings suffered from the same defect: some bundles were subdivided. Although this might be desirable in some situations, the subdivision was not part of the manual classification. The physicians did not mind the subdivision in some cases, because large bundles like the corpus callosum and corona radiata can be further subdivided. The physicians found it less desirable that a small bundle like the cingula was subdivided. The Adjusted Rand index returns very low scores for clusterings in which the corpus callosum was subdivided into a number of smaller clusters (clustering C, D and E).</p><p>Bad. The clusterings G and H were considered bad by the physicians, because several bundles from the manual classification were clustered together. The Adjusted Rand index returns very high scores for these clusterings because the largest bundle (the corpus callosum) is complete.</p><p>Very bad. Clustering I was considered very bad because it was both incorrect as well as incomplete. Here the Adjusted Rand index agrees with the opinion of the physicians and returns very low values.</p><p>The linear correlation between the index and the AR results is 0.25. Notice that just the ranking or order has been used to calculate the correlation.</p><p>In summary, the Adjusted Rand index does not reflect the preferences of the physicians. </p><formula xml:id="formula_4">Bundle/Cluster c 1 c 2 . . . c S Sums b 1 n 11 k u 1 n 12 k u 1 . . . n 1S k u 1 k b 2 n 21 k u 2 n 22 k u 2 . . . n 2S k u 2 k . . . . . . . . . . . . . . . b R n R1 k u R n R2 k u R . . . n RS k u R k Sums v 1 v 2 . . . v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S Rk</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Weighted Normalized Adjusted Rand (WNAR)</head><p>In this section, we use the criteria that the physicians have used for their evaluation to improve the AR index. A problem with the Adjusted Rand index is that it does not account for bundles that are of widely varying sizes. That is, the Adjusted Rand index measures agreement on the level of fibers, not on the level of bundles. As a result, a bundle with a large number of fibers is weighted more than a bundle with a small number of fibers. In table 3, it can be noticed that whenever the corpus callosum is complete, the Adjusted Rand index returns a high value whatever the situation of the other bundles is.</p><p>Another problem is that, as table 3 shows, the physicians found an incorrect clustering worse than an incomplete clustering. In an incorrect clustering, fibers belonging to different anatomical bundles are clustered together, which makes it difficult to distinguish between bundles. This makes a correct clustering visually more appealing than a complete clustering.</p><p>To take into account the requirement that bundles should be weighted equally, we define a Normalized Adjusted Rand (NAR) index. The idea is to modify the contingency table such that each bundle has the same weight. A way to achieve this is by setting the row sum u i of each bundle b i in the contingency table to some nonnegative value k and to multiply each entry n i j by a factor k/u i (see <ref type="table" target="#tab_4">table 4</ref>).</p><p>The column sum v j is computed by taking the sum of the new cell values,  A remaining question is which value to use for k. We chose k → ∞, thereby pretending that we have an infinite amount of fibers, which gives more stable results. The definition of the Normalized Adjusted Rand becomes:</p><formula xml:id="formula_5">v j = k ∑ R i=1 (n i j /u i ).</formula><formula xml:id="formula_6">a = R ∑ i=1 S ∑ j=1 k n i j u i 2 b = R k 2 − a m 1 Different Bundle c = S ∑ j=1 v j 2 − a d = Rk 2 − a − b − c M − m 1 Sums m 2 M − m 2 M</formula><formula xml:id="formula_7">NAR = lim k→∞ a − (m 1 m 2 )/ Rk 2 (m 1 + m 2 )/2 − (m 1 m 2 )/ Rk 2 = 2 f − 2Rg 2 f − R f − R 2 with f = S ∑ j=1 R ∑ i=1 n i j u i 2 , g = R ∑ i=1 S ∑ j=1 n 2 i j u 2 i .</formula><p>We propose a final modification to the Adjusted Rand index that enables us to weigh correctness and completeness differently. The indices that are based on the Rand index assume that the correctness and completeness of a clustering are equally important, but we found that physicians assign different weights to the aspects of correctness and completeness.</p><p>Let us first define the Rand index in terms of the normalized contingency table:</p><formula xml:id="formula_8">NR = a + d a + b + c + d = 1 − b M − c M .</formula><p>In this equation the fraction b /M indicates the incompleteness of the clustering. The fraction c /M indicates the incorrectness of the clustering. We propose the following definition for a Weighted Normalized Rand index WNR:  If α = 0.5 then correctness and completeness are weighted equally, for higher values correctness is weighted more, for lower values completeness is weighted more.</p><formula xml:id="formula_9">WNR = 1 − 2(1 − α) b M − 2α c M .</formula><p>The expected value of WNR becomes  <ref type="figure" target="#fig_6">Figure 4</ref> shows the relation between α and the rank correlation of the physicians and WNAR ordering of the clusters. It shows that the optimal value for α is around 0.75 for validating the clustering that were used in this experiment. <ref type="table" target="#tab_7">Table 6</ref> shows the values of the WNAR index for the clusterings that were ranked by the physicians. The WNAR index with α = 0.5 does not distinguish between average and bad clustering, while setting α = 0.75 does make a difference.</p><formula xml:id="formula_10">E(WNR) = 1 − 2(1 − α)E b M − 2αE c M = 1 − 2(1 − α) m 1 (M − m 2 ) M 2 − 2α m 2 (M − m 1 )<label>M</label></formula><formula xml:id="formula_11">WNAR = lim k→∞ WNR − E(WNR) 1 − E(WNR) = f − Rg f − αR f − R 2 − αR 2</formula><p>This experiment was too small to be statistically significant, and a larger experiment with a more complete ground truth is necessary to confirm this results. Nevertheless, based on this experiment, the ranking created with WNAR index with α = 0.75 has the most correspondence with the criteria of the physicians and will be used in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION OF CLUSTERING METHODS</head><p>For the experiments, three different DTI data sets from healthy adults were used. Each data set has a resolution of 128 × 128 × Fiber similarity measure HSL HWA HCL SNN  30 with a voxel size of 1.8 × 1.8 × 3.0mm. For each data set, we defined a ground truth which consisted of the structures described in section 4.1. The data sets were selected at random: the only selection criterium was that the structures of the ground truth could be found using fiber tracking. Fiber tracking with seeding throughout the whole volume <ref type="bibr" target="#b11">[12]</ref> gives us a set of 3500-5000 fibers, which can be clustered in approximately 15-20 minutes on a Pentium 4 with a 2.5 GHz processor, depending on the chosen fiber similarity measure and clustering method. Furthermore, each bundle of the manual classification contains at least 10 fibers.</p><formula xml:id="formula_12">D 1 D 2 D 3 D 1 D 2 D 3 D 1 D 2 D 3 D 1 D 2 D</formula><p>As a starting point, we implemented the fiber similarity measures based on Corouge et al. <ref type="bibr" target="#b3">[4]</ref>: closest-point distance, mean distance and Hausdorff distance. We also included the end point distance presented by Brun et al. <ref type="bibr" target="#b2">[3]</ref>.</p><p>Hierarchical clustering has been used for fiber clustering by Zhang and Laidlaw <ref type="bibr" target="#b16">[16]</ref>. Hierarchical clustering gives different results by varying the cluster similarity measure. Three hierarchical variations were implemented: single-link (HSL), complete-link (HCL) and weighted-average (HWA) (see section 4.2).</p><p>Hierarchical clustering methods have a single parameter that controls the output of the algorithm: the level at which the dendrogram is cut. We compare the clustering at each level of the dendrogram to the manual classification using the WNAR index with α = 0.75 (see <ref type="figure" target="#fig_8">figure 5a</ref>). This comparison is done for each of the algorithm combinations. The arrow shows the optimal clustering for this method, shown in figure 1b.</p><p>The second method that we have used for fiber clustering is the shared nearest neighbor (SNN) algorithm described in section 4.2. The SNN algorithm has two parameters: the number of neighbors k η and the edge threshold τ. In general, an increased edge threshold results in an increased number of clusters. <ref type="figure" target="#fig_8">Figure 5b</ref> shows a density plot of the WNAR for the mean distance between fibers combined with the SNN algorithm. The axes are the number of neighbors versus the number of clusters. The value of the WNAR index is represented by a grey value: black corresponds to 0 and white to 1. The arrow indicates the optimal clustering which is shown in <ref type="figure">figure 1d</ref>.</p><p>If the number of neighbors is fixed the plot of the number of clusters versus WNAR is similar to the ones obtained by hierarchical clustering (see figure 5a and 5c). <ref type="table" target="#tab_9">Table 7</ref> gives the maximum values obtained from the WNAR index for each combination of fiber similarity measure and clustering method for the three data sets D 1 , D 2 , and D 3 . These values were obtained by varying the parameters of each clustering method, and comparing each resulting clustering to the ground truth. For the hierarchical clustering variations, the single-link method combined with the mean of closest points measure produces a clustering that has the best correspondence with the ground truth. This clustering is obtained by cutting the dendrogram at the level of 141 clusters (see <ref type="figure">figure 1b)</ref>. The worst optimal clustering (W NAR = 0.46) has 933 clusters and is also created with the single-link method, but now combined with the closest point measure (see <ref type="figure">figure 1c)</ref>.  For three of four fiber similarity measures, the single-link performs better than the weighted-average and complete-link. These higher values can be explained by the fact that the single-link method manages to keep the fibers from the larger bundles together. This is largely due to the chaining effect of the single-link <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure">Figure 1d</ref> shows the optimal SNN clustering for the first data set, which is also obtained with the mean of closest point measure. The SNN algorithm seems to be able to find both the small and the large bundles of the manual classification. Indeed, a visual inspection reveals that the clusterings produced by the SNN algorithm are very similar to the hierarchical single-link clusterings. This is reflected in the scores of the WNAR index which are also similar (see table 7). For the second data set, the SNN algorithm combined with the mean of closest points measure obtains an optimal value of 1.0: this means that the clustering is perfect for the fibers that have been manually classified.</p><p>However, the difficulty with the SNN algorithm is choosing appropriate values for the parameters: the number of neighbors k η and the edge threshold τ. <ref type="table" target="#tab_11">Table 8</ref> gives the optimal parameter settings for the first data set (D 1 ). Noticeable is the apparent lack of a relation between k η , τ and the optimal value for the WNAR index. When a manual classification is available, an exhaustive search can find the optimal value for τ for a particular k η . Without such an aid however, the number of possible values for τ is very large.</p><p>Concerning the fiber similarity measures, the mean distance between fibers achieves the highest values for the WNAR index. The results are similar to the end-points distance and the Hausdorff distance. The closest-point distance performs poorly with single-link, but performs reasonably well with complete-link and weightedaverage. This is probably because the conservative nature of these methods counterbalances the overly optimistic nature of the closestpoint measure.</p><p>In summary, the difference in clustering quality between the hierarchical single-link method and SNN method is minimal. A larger experiment with more data sets is necessary to confirm these results. However, if we look from a practical point of view then the hierarchical clustering algorithm is better for our purposes. In SNN, the number of neighbors and the edge threshold need to be set. The values of these parameters did not show any relation with the optimal clusterings. Specifying the number of clusters is also more intuitive than the number of neighbors and edge threshold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>Fiber clustering can overcome the visual cluttering that occurs when doing fiber tracking with seeding throughout the whole volume. In this paper, the shared nearest neighbor clustering algorithm has been applied in the context of fiber clustering. A framework to evaluate fiber clustering methods has been presented. Our approach is based on the manual classification of the fibers in a number of bundles that correspond to anatomical structures. By comparing the manually defined bundles to the automatically created clusters we can get an estimation of the cluster quality. We presented a new measure to validate the fiber clusters based on the preferences of physicians. We created the WNAR clustering quality measure after we found that the available measures in the literature were not suited to the task of fiber clustering. Finally, we compared different clustering methods using the new measure. We demonstrated how the validation and clustering techniques can be used on DTI data sets of human brains. We concluded that from the tested methods, hierarchical clustering using single-link and mean distance between fibers gives the best results.</p><p>The results of the experiments presented in this paper can be seen as a demonstration of the described techniques. We had to restrict ourselves to a limited number of data sets, physicians, fiber similarity measures and clustering methods. Therefore, we cannot give definitive answers. As future work, a larger experiment with more data sets and more physicians involved needs to be done. The current manual classification only contains six anatomical structures. A more complete manual classification will enable a more accurate assessment of the cluster results. A validation of the WNAR ranking with new data by the physicians can help to confirm the results.</p><p>The presented techniques are not constrained to white-matter fibers. It would be interesting to examine how these methods perform on non-brain fibers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Ground truth and different partitions of the same set of fibers for the cc, cgl and cgr. Correctness implies that fibers of different anatomical structures are not clustered together; completeness means that fibers of the same anatomical structures are clustered together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the validation process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The manual classification B = {b 1 , b 2 ,...,b R } and the clustering result C = {c 1 , c 2 ,...,c S } are both partitions of n items. The ground truth consists of R bundles and the clustering result consists of S clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>n 2 .</head><label>2</label><figDesc>In table 2, the pairs of fibers are categorized in four groups: a, b, c, and d, according to whether pairs of fibers are in the same bundle and/or cluster or not. The number of pairs that are in the same bundle is m 1 = a + b, and the number of pairs that are in the same cluster is m 2 = a + c. Notice that the number of pairs on which the manual classification and the automatic clustering agree is a + d. Consequently, b + c is the number of pairs on which the ground truth and the clustering result disagree. The Rand index [8] is defined as the number of "agreement" pairs divided by the total number of pairs, Rand = (a + d)/M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Relation between α and the rank correlation. 0.77 0.80 0.85 0.90 0.96 average 0.38 0.58 0.64 0.71 0.82 0.95 bad 0.92 0.89 0.81 0.74 0.68 0.64 very bad 0.01 0.34 0.33 0.32 0.30 0.29</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 since</head><label>2</label><figDesc>the expected value of b is m 1 (M − m 2 )/M and the expected value of c is m 2 (M − m 1 )/M . Now the Weighted Normalized Adjusted Rand index (WNAR) is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Graphs showing the evaluation of the clustering methods using mean distance between fibers measure for one data set. The arrows indicate the optimal clustering for the respective method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Contingency table<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Categories of pairs of fibers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ranking of the physicians compared with the Adjusted Rand (AR) index. In this table, cc stands for corpus callosum, cr for corona radiata (both hemispheres), cg for cingula (both hemispheres) and fx for fornix. The linear correlation of the rankings is 0.25.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Normalized contingency table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>With this contingency table we can calculate new values for a, b, c, d, m 1 , m 2 , M (see table 5).</figDesc><table><row><cell>Same Cluster</cell><cell>Different Cluster</cell><cell>Sums</cell></row><row><cell>Same</cell><cell></cell><cell></cell></row><row><cell>Bundle</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Categories of pairs of fibers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ranking of the physicians compared with the WNAR index.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Highest WNAR values for each combination of clustering method and fiber similarity measure.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Optimal parameter settings for the first data set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank J. Buijs, F. Roos and C. van Pul from Máxima Medical Center in Veldhoven for the successful collaboration and providing the data sets and evaluations used in this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">In vivo fiber tractography using DT-MRI data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Basser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pajevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pierpaoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aldroubi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MR in Medicine</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="625" to="632" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering fiber tracts using normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Knutsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Shenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Westin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI&apos;04, Conf. Proc</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="368" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coloring of DT-MRI fiber traces using laplacian eigenmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Jeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Knutsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl-Fredrik</forename><surname>Westin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUROCAST&apos;03,Conf. Proc., Lecture Notes in Computer Science 2809</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="564" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards a shape model of white matter fiber bundles using diffusion tensor MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Corouge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouttard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Biomedical Imaging, Conf. Proc</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="344" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Case study: reconstruction, visualization and quantification of neuronal fiber pathways</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization&apos;01, Conf. Proc</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding clusters of different sizes, shapes, and densities in noisy, high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ertöz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM -Data Mining, Conf. Proc</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A study of the comparability of external criteria for hierarchical cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Milligan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behavioral Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="441" to="458" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated fuzzy clustering of neuronal pathways in diffusion tensor tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Shimony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Conturo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Proc</title>
		<imprint>
			<date type="published" when="2002-05" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">DTI visualization with streamsurfaces and evenly-spaced volume seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berenschot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Van Pul</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m">Joint EG -IEEE TCVG Symposium on Visualization, Conf. Proc</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visualization and Image Processing of Tensor Fields, chapter An Introduction to Visualization of Diffusion Tensor Imaging and its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laidlaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer Verlag series Mathematics and Visualization</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fiber tract-based atlas of human white matter anatomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wakana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Nagae-Poetscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C M</forename><surname>Van Zijl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing diffusion tensor MR images using streamtubes and streamsurfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="454" to="462" />
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hierarchical clustering of streamtubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
		<idno>CS-02-18</idno>
		<imprint>
			<date type="published" when="2002-08" />
		</imprint>
		<respStmt>
			<orgName>Brown University Computer Science Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
