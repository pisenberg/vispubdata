<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T15:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluation of Fast-Forward Video Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Markus</forename>
								<forename type="middle">H</forename>
								<roleName>Student Member, Ieee</roleName>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Kuno</forename>
								<surname>Kurzhals</surname>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Benjamin</forename>
								<forename type="middle">H</forename>
								<roleName>Student Member, Ieee</roleName>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Gunther</forename>
								<surname>Heidemann</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Daniel</forename>
								<surname>Weiskopf</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluation of Fast-Forward Video Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>(a) (b) (c) (d) Fig. 1. Comparison of four fast-forward video visualization techniques: (a) frame-skipping, (b) temporal blending, (c) object trail visualization, (d) predictive trajectory visualization. Video available at: www.vis.uni-stuttgart.de/index.php?id=vva. Abstract—We evaluate and compare video visualization techniques based on fast-forward. A controlled laboratory user study (n = 24) was conducted to determine the trade-off between support of object identification and motion perception, two properties that have to be considered when choosing a particular fast-forward visualization. We compare four different visualizations: two representing the state-of-the-art and two new variants of visualization introduced in this paper. The two state-of-the-art methods we consider are frame-skipping and temporal blending of successive frames. Our object trail visualization leverages a combination of frame-skipping and temporal blending, whereas predictive trajectory visualization supports motion perception by augmenting the video frames with an arrow that indicates the future object trajectory. Our hypothesis was that each of the state-of-the-art methods satisfies just one of the goals: support of object identification or motion perception. Thus, they represent both ends of the visualization design. The key findings of the evaluation are that object trail visualization supports object identification, whereas predictive trajectory visualization is most useful for motion perception. However, frame-skipping surprisingly exhibits reasonable performance for both tasks. Furthermore, we evaluate the subjective performance of three different playback speed visualizations for adaptive fast-forward, a subdomain of video fast-forward. Index Terms—Video visualization, adaptive fast-forward, controlled laboratory user study.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Video visualization, as an integral part of multimedia visualization and multimedia visual analytics <ref type="bibr" coords="1,135.85,434.85,13.74,8.02" target="#b12">[13]</ref>, has recently been attracting much attention because it addresses today's and future data-analysis challenges that come with the enormous amount of video contents from widely available low-cost cameras, internet-connected databases, or personal data collections; see Borgo et al. <ref type="bibr" coords="1,188.96,474.70,10.45,8.02" target="#b4">[5] </ref>for a recent survey of techniques for video visualization. We focus on one aspect of video visualization: the effectiveness of fast-forward video presentation, where the output is a timecompressed video. Fast-forward video visualization is a typical example of animated visualization in which time of the original data (here, the raw video stream) is mapped to animation time; the time-to-time mapping may be nonlinear, for example, for adaptive control of the speed of fast-forward. We consider fast-forward video visualization as the first step of temporal aggregation or abstraction applied to the raw video. Therefore, it readily fits in visual exploration strategies such as the visual information-information seeking mantra <ref type="bibr" coords="1,218.36,585.13,14.94,8.02" target="#b32">[33] </ref>or the visual analytics mantra <ref type="bibr" coords="1,95.26,595.09,13.74,8.02" target="#b21">[22]</ref>. In top-down exploration, fast-forward video playback can be seen as a stage of data drill-down right before the raw video material is watched. It can also be employed in bottom-up @BULLET Markus Höferlin, Kuno Kurzhals, and Daniel Weiskopf are with the  processes, for example, in early stages of information foraging and fil- tering <ref type="bibr" coords="1,317.23,433.60,13.74,8.02" target="#b29">[30]</ref>. In this context, it is an early-stage abstraction of the video with a representation close to the raw data, such as the space-time volume rendering of video <ref type="bibr" coords="1,383.60,453.53,14.94,8.02" target="#b10">[11] </ref>or the VideoPerpetuoGram <ref type="bibr" coords="1,500.46,453.53,9.52,8.02" target="#b6">[7]</ref>. Besides video fast-forward, the visualizations we evaluate could also be applied to other animated content, such as interactive or animated visualizations (e.g., <ref type="bibr" coords="1,351.99,483.41,14.19,8.02" target="#b18">[19,</ref><ref type="bibr" coords="1,368.84,483.41,10.31,8.02" target="#b30"> 31]</ref>). Playback-speed of such animations, which usually show between 15 and 20 frames per second <ref type="bibr" coords="1,484.32,493.38,9.52,8.02" target="#b1">[2]</ref>, can often be steered interactively, using slow motion or fast-forward <ref type="bibr" coords="1,494.47,503.34,13.74,8.02" target="#b36">[37]</ref>. In general, we can distinguish two types of video fast-forward: conventional fast-forward, which plays the video at constant pace, and adaptive video fast-forward, which adjusts the playback speed for each frame, according to some measure of relevance. By playbackspeed adaption, annoying or irrelevant parts (e.g., static parts) of the video are reduced while emphasis is put on relevant parts (e.g., parts of high activity)—provided that the relevance measure matches the users' expectations. Typical measures used to rate the relevance of video frames are: optical flow and visual complexity <ref type="bibr" coords="1,495.86,594.97,13.74,8.02" target="#b26">[27]</ref> , similarity to target clip <ref type="bibr" coords="1,357.88,604.94,13.74,8.02" target="#b27">[28]</ref>, motion, semantics, and user preference <ref type="bibr" coords="1,527.31,604.94,13.74,8.02" target="#b11">[12]</ref>, information theory <ref type="bibr" coords="1,365.54,614.90,13.74,8.02" target="#b19">[20]</ref>, and visual attention <ref type="bibr" coords="1,461.03,614.90,13.74,8.02" target="#b20">[21]</ref> . Contrary to conventional fast-forward, the users are more confused in estimating the speed of an object in videos accelerated by adaptive fast-forward <ref type="bibr" coords="1,527.31,634.82,13.74,8.02" target="#b19">[20]</ref>. The adaptive method is especially suitable for unedited material like video surveillance footage. In this paper, we focus on the evaluation of fast-forward visualization regarding video visual analytics for surveillance applications. In the following, we describe the main challenges in this context. Due to the physical constraint of video display devices (usually 60-200 Hz), higher accelerations are typically achieved by discarding frames (frame-skipping). Frame-skipping at normal playbackspeed (in this context termed time-lapse) affects the human perceptual performance. Keval and Sasse <ref type="bibr" coords="1,420.30,736.42,14.94,8.02" target="#b23">[24] </ref>experienced a strong decrease Source Frames </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.. . </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.. . </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X .. . </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.. . </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X Destination Frames </head><p>Fig. 2. Frame-skipping. To achieve the desired frame rate, source frames are discarded. of crime detection performance for time-lapse video presentation in their experiment and Scott-Brown and Cronin <ref type="bibr" coords="2,195.93,208.50,14.94,8.02" target="#b31">[32] </ref>pointed out that video in time-lapse format disrupts motion perception, and thus, increases change blindness. The authors of both works illustrate the importance of proper object/event identification and motion perception for video surveillance and the challenges that may arise by time-lapse video. However, the psychological influence of frame-skipping in fastforward scenarios has not been investigated yet and remains an open research question. To overcome the issues of frame-skipping a temporal blending method was proposed by Höferlin et al. <ref type="bibr" coords="2,167.40,298.54,13.74,8.02" target="#b19">[20]</ref>. From the perspective of computer graphics, temporal antialiasing by temporal blending is usually called motion blur <ref type="bibr" coords="2,105.90,318.47,14.94,8.02" target="#b34">[35] </ref>(see Navarro et al. <ref type="bibr" coords="2,190.53,318.47,14.94,8.02" target="#b25">[26] </ref>for a review of the state-of-the-art of motion blur rendering). In this paper, we compare the performance of frame-skipping, temporal blending of successive frames, and two new variants of visualization methods for video fastforward: object trail visualization and predictive trajectory visualization . The last one is inspired by storyboarding techniques <ref type="bibr" coords="2,241.75,368.28,14.94,8.02" target="#b15">[16] </ref>and Augmented Keyframes <ref type="bibr" coords="2,105.52,378.24,13.74,8.02" target="#b9">[10]</ref>, which use arrows to show movements in still frames. The evaluation covers user performance and user experience according to the taxonomy of Lam et al. <ref type="bibr" coords="2,189.79,398.17,13.74,8.02" target="#b24">[25]</ref>. In the evaluation, we focus on the trade-off between motion perception and object detection and identification, to account for the main challenges of fastforward visualization in the context of video surveillance. Although the perception of animation has been playing an important role in visualization research (see, for example, the textbook by Ware <ref type="bibr" coords="2,238.72,447.98,13.44,8.02" target="#b34">[35]</ref>), our work is—to the best of our knowledge—the first user study to evaluate different visualization techniques for compressed-time rendering of motion images. Since playback-speed adaption of adaptive video fast-forward methods hinders correct object speed estimation in video, users must be made aware of the adaption factor used for adaptive video fastforward . Communication of the current playback-speed should not distract attention from the video and should be presented in a way that matches users' expectations. To convey the playback-speed information , a speedometer was utilized by Cheng et al. <ref type="bibr" coords="2,197.48,547.99,13.74,8.02" target="#b11">[12]</ref> . Besides a modified version of this speedometer, we introduce two visual representations of the playback-speed (color frame, analog VCR fast-forward) and evaluate their performance in terms of subjective effectiveness, level of distraction, and workload. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FAST-FORWARD VIDEO VISUALIZATION APPROACHES</head><p>In general, fast-forward video visualizations have the objective to communicate the information of a certain number of frames from a source video, n src , within n dst frames in the destination video. If we use the same frame rate for the destination video that we have in the source video, then the relation between these quantities depends solely on the fast-forward acceleration factors a i , which can vary frame-wise in the case of adaptive fast-forward: n dst = ∑ n src i=1 1/a i . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Frame-skipping</head><p>The typical approach to boost the playback speed in video fast-forward is to discard as many frames as required to obtain the desired acceleration factor <ref type="bibr" coords="2,73.42,736.42,13.74,8.02" target="#b19">[20]</ref>. In detail: The j-th destination video frame f j </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dst is </head><p>Blending before they are blended with respect to their acceleration factors a i (center). After blending in linear RGB space, the resulting frames are gamma-encoded again (bottom). the i-th frame of the source video. The indices i ∈ I are determined by Eq. 1. All other frames are skipped (see <ref type="figure" coords="2,430.07,391.26,28.89,8.02">Figure 2</ref>). </p><formula>I = i ∃ j : j ≤ i ∑ k=1 1 a k ∧ j &gt; i−1 ∑ k=1 1 a k (1) </formula><p>Since the original video frames are displayed in frame-skipping, the appearance of objects is preserved and should be as well observable as in the original video (see <ref type="figure" coords="2,381.21,467.37,33.44,8.02">Figure 1a</ref>). Another advantage is the low computational cost. If an appropriate video codec is chosen and the number of keyframes is adequate, the video can nearly be accelerated without limits: the number of frames considered for visualization remains constant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Temporal Blending</head><p>The temporal blending approach <ref type="bibr" coords="2,407.16,539.44,14.94,8.02" target="#b19">[20] </ref>was developed to alleviate the interruptions of frame-skipping including their perceptual issues. It is motivated by the human visual system, which summates visual stimuli over time <ref type="bibr" coords="2,322.45,569.33,9.71,8.02" target="#b2">[3,</ref><ref type="bibr" coords="2,335.03,569.33,10.64,8.02" target="#b16"> 17]</ref>. Fast moving objects thus appear blurred (see <ref type="figure" coords="2,520.54,569.33,14.95,8.02;2,285.12,579.29,20.52,8.02">Fig- ure 1b</ref>). This effect can be observed, for example, by swinging torches at night, as the integration time of the human eye depends on the luminance of the environment. Motion streaks from such blurring can support the perception of motion direction, as reported in psychophysical experiments <ref type="bibr" coords="2,346.36,619.14,13.74,8.02" target="#b13">[14]</ref>. Temporal blending emulates the blurring effect by integrating the frames in a similar way. The integration time for visualization depends on the acceleration factors a i . In detail, each source frame f i src is added with the weight 1/a i to the destination frame ( f j </p><formula>dst = ∑ i 1 a i · f i src ) </formula><p>until the sum of weights is one (∑ i 1 a i = 1). To satisfy this, the weight of a source frame 1/a i may be split and the source frame will affect multiple destination frames (see <ref type="figure" coords="2,301.80,696.31,28.89,8.02">Figure 3</ref>). Typically, video frames are captured with gamma encoding, and are decoded by the monitor while displaying. To integrate the frames in a physically correct way, the original video frames are first gammadecoded to a linear color space (i.e., from sRGB to linear RGB). After <ref type="figure" coords="3,31.50,298.63,20.33,7.37">Fig. 4</ref>. Rendering process of object trail visualization. Left: object enhancement responsible for proper object identification; right: frame blending responsible for proper motion perception; bottom right: destination frame with superimposed objects on the blended frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Superimposed </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . . </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>blending </head><p>in the linear color space, the resulting frames are encoded again, since the monitor devices require gamma-encoded input. The whole rendering process is depicted in <ref type="figure" coords="3,172.28,379.80,29.15,8.02">Figure 3</ref> . In contrast to frameskipping , temporal blending communicates object movement by integrating all available frames. Nevertheless, object identification may become difficult due to their blurred appearance (see <ref type="figure" coords="3,222.49,409.69,32.75,8.02">Figure 1b</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Object Trail Visualization</head><p> Object trail visualization has been developed to combine the advantages of frame-skipping (object identification) with the support of motion perception of temporal blending: we visualize multiple entities of objects simultaneously with increasing transparency (see <ref type="figure" coords="3,241.82,475.84,33.21,8.02">Figure 1c</ref>). The appearance is inspired by Dynamic Stills <ref type="bibr" coords="3,193.59,485.80,9.52,8.02" target="#b8">[9]</ref>, where a whole video clip is summarized in one static image. The difference is that the duration we summarize is much shorter and used for each destination frame. This effect is also known from Microsoft's pointer trails that can be used since Windows 3.1 or from high-density cursors <ref type="bibr" coords="3,248.81,525.66,10.45,8.02" target="#b3">[4] </ref> to enhance the visibility of mouse movement. We render older entities of an object with higher transparency, similar to the Salient Video Stills <ref type="bibr" coords="3,264.70,545.58,13.74,8.02" target="#b33">[34]</ref>. The main difference between object trail visualization and the two mentioned approaches (Dynamic Stills and Salient Video Stills) is that we do not summarize whole sequences in a single image. Object trail visualization creates many summarizations: one for each destination frame. The rendering process has two stages (see <ref type="figure" coords="3,195.84,606.21,29.16,8.02">Figure 4</ref>), which can be computed in parallel: frame blending and object enhancement. The first stage is dedicated to movement perception; the second stage facilitates object identification. Stage 1: Frame Blending. A particular number of frames (m − 1) are blended. The distance k between the frames is determined according to the acceleration factors a i . The blended image is calculated by f j </p><formula>blend = ∑ m−1 o=1 w o · f l src , where l = i − o · k and </formula><p>the normalized weights w o of older frames are reduced according to w o = 2 −o ∑ m−1 ω=1 2 −ω (see also <ref type="figure" coords="3,31.50,705.68,28.89,8.02">Figure 4</ref>). Stage 2: Object Enhancement. The blended image is superimposed by objects present in the recent frame. First, we extract the background image by the running Gaussian average method <ref type="bibr" coords="3,201.36,736.42,13.74,8.02" target="#b28">[29]</ref>, then we calculate a foreground mask by background subtraction and thresholding, and finally apply morphological operators. Besides that, we highlight the object movements by additionally blending the object masks of the previous frames semi-transparently: older entities receive a higher luminance here (see <ref type="figure" coords="3,474.70,93.32,32.33,8.02">Figure 1c</ref> ). Optionally , the movement emphasis can be disabled (as in <ref type="figure" coords="3,479.54,103.28,28.89,8.02">Figure 4</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Predictive Trajectory Visualization</head><p> We have designed predictive trajectory visualization—similar to object trail visualization—to allow for good object identification and motion perception. It uses abstract trajectory illustration to improve the perception of moving objects. A special characteristic is that the visualization contains movement information from past and information about future object movements alike. In detail, it consists of a keyframe that is chosen in the same way as in frame-skipping, a tail for each object that depicts motion from the past, and arrows to forecast the objects' positions in future (see <ref type="figure" coords="3,294.12,224.00,32.54,8.02">Figure 1d</ref> ). The design decision to use arrows to indicate future movements is founded in related work of flow visualization: arrows are widely-used to indicate motion <ref type="bibr" coords="3,405.55,243.93,13.74,8.02" target="#b35">[36]</ref>. The rendering process (illustrated in <ref type="figure" coords="3,302.82,253.89,28.86,8.02">Figure 5</ref>) extracts and tracks objects with their bounding boxes. This can be done by standard computer vision pipelines. Since object extraction and tracking is not in the focus of this paper, we refer to the survey of Yilmaz et al. <ref type="bibr" coords="3,378.03,283.78,13.74,8.02" target="#b37">[38]</ref>. In predictive trajectory visualization, objects are visible for a longer duration: their arrows are visible before they enter the scenery. Thus, the observer can identify the objects earlier. However, this benefit comes at the expense of visual clutter, which may occur in videos with many moving objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> 3 ADAPTIVE FAST-FORWARD PLAYBACK SPEED </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TION </head><p>In adaptive video fast-forward, the playback velocity of each frame varies according to a measure of relevance. A user study by Höferlin et al. <ref type="bibr" coords="3,305.51,395.35,14.94,8.02" target="#b19">[20] </ref>showed that it is difficult to differentiate between variations in video playback speed and changes of the movement velocity of objects . To interpret video material correctly, it is indispensable to estimate the velocities of objects. An event including a moving person, for example, would be interpreted completely different if the observers realize that the person is running or if they imagine that the person is walking. To avoid such misinterpretations, the participants of the user study " suggested to add visual feedback to increase awareness of playback speed " <ref type="bibr" coords="3,339.93,475.05,13.74,8.02" target="#b19">[20]</ref>. In this section, we discuss three possibilities to indicate the playback speed of adaptive fast-forward: the speedometer (<ref type="figure" coords="3,493.17,495.06,28.37,8.02" target="#fig_0">Figure 6</ref>, left), the color frame approach (<ref type="figure" coords="3,390.50,505.02,28.26,8.02" target="#fig_0">Figure 6</ref> , center), and the analog VCR fastforward visualization (<ref type="figure" coords="3,377.01,514.98,28.23,8.02" target="#fig_0">Figure 6</ref>, right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Speedometer</head><p>The idea to communicate the velocity of adaptive fast-forward by a speedometer was introduced by Cheng et al. <ref type="bibr" coords="3,454.07,555.92,13.74,8.02" target="#b11">[12]</ref>. They show a needle that swivels to the current playback speed as well as the numeric value. We adopt this visualization and enhance it by color mapping (see <ref type="figure" coords="3,529.54,575.85,14.95,8.02;3,294.12,585.81,11.45,8.02">Fig- ure</ref>6, left). Therefore, a heated body scale color map is applied, which is appropriate for ordinal data <ref type="bibr" coords="3,403.52,595.77,9.52,8.02" target="#b5">[6]</ref>. The applied variant is the Matlab's color map hot, depicted in <ref type="figure" coords="3,390.25,605.74,30.14,8.02" target="#fig_0">Figure 6</ref>in the bottom center. The speedometer communicates the playback speed by a metaphor known from cars. Since the peripheral vision has low spatial resolution compared to the fovea <ref type="bibr" coords="3,394.54,635.71,13.74,8.02" target="#b17">[18]</ref>, identifying the exact position of the needle or reading the numeric speed value requires visual focus of the observer. Since attention should be given to the video, we add a halfcircular area at the center of the speedometer that shows the current speed by the color from the color map. The area is larger than the thin needle and the displayed digits, and thus, is better identifiable by peripheral vision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Color Frame</head><p>In the color frame visualization, the video is enclosed with a thick frame that is inked according to the playback velocity (see <ref type="figure" coords="3,511.52,736.42,29.61,8.02" target="#fig_0">Figure 6</ref>, <ref type="figure" coords="4,22.50,267.54,19.22,7.37">Fig. 5</ref>. Rendering process of predictive trajectory visualization. The current frame i is selected as in frame-skipping. For tail rendering (left), the edge points of the objects' bounding boxes (green points) are used from the current frame i, and the two previous frames i − k and i − 2k. For arrow rendering, the center bottom points of the objects' bounding boxes (yellow points) are used from the current frame i, and the two future frames i + 2k and i + 4k (doubled frame distance). The distance k can be adapted to the playback speed (here: k = 5). Each group of three temporally shifted control points is used to create a quadratic B ´ ezier curve. The tail is rendered by two surfaces that show a semi-transparent fading white gradient. Each is defined by the two vertical B ´ ezier curves coming from the top and bottom vertices of one of the bounding box sides. The arrow body (defined by its B ´ ezier curve) is superimposed by an arrow head pointing in the direction of motion. center). The applied color map is identical with the one used for the speedometer. We implemented and included this visualization in the user study since it does not require visual focus, and thus may reduce the required attention to assess the playback speed. In contrast to the speedometer, only a coarse impression of the playback speed can be provided by the color frame. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">124</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analog VCR Fast-Forward</head><p>The third playback speed visualization, analog VCR fast-forward, is based on the characteristic horizontal distortion lines that occur for fast-forwarding analog video data (see <ref type="figure" coords="4,165.85,622.85,29.68,8.02" target="#fig_0">Figure 6</ref>, right). We map the playback speed to the amount and thickness of the distortion rows, as shown in <ref type="figure" coords="4,57.77,642.78,29.33,8.02" target="#fig_2">Figure 7</ref> . Horizontal distortion lines are generated by shifting the pixels inside the black rectangles of <ref type="figure" coords="4,187.04,652.74,31.04,8.02" target="#fig_2">Figure 7</ref>into a random direction (left, right) with a random magnitude (denoted by horizontal arrows): ∆ x = −1 rand() mod 2 · (rand() mod ∆ x ), with the maximum amplitude ∆ x = 10 px. The shifting direction and magnitude vary between frames. The playback speed is mapped in 10 levels to the distortion width (up to 3) and the amount of distortion rows (up to 3), where the absence of distortion lines indicates original playback speed. In contrast to the approaches using color mapping, the users do not need to learn any color scheme. The visualization uses a known metaphor, and thus, can be interpreted without training. Similar to the color frame visualization, the playback speed can be recognized without focusing on particular screen regions. Nevertheless, the visualization can be characterized as qualitative visualization since it is difficult to identify the exact playback speed, and the video signal is distorted, which may negatively influence the perception of the video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">USER STUDY</head><p>In the user study, we compare the four fast-forward visualizations in terms of object identification (measured objective and subjective) and motion perception (subjective) and the three playback speed visualizations in terms of playback speed feedback (subjective). We used three different videos in which we partially added artifical search targets. We use a within-subjects design. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hypotheses</head><p>We expect a trade-off between support for object identification and motion perception for the fast-forward visualizations and a trade-off for the speed visualizations between conveying precise information and inducing less distraction. Thus, we test the following hypotheses:  @BULLET Hypothesis 1 – Object identification. We expect that frameskipping shows the best results in supporting object identification because it preserves the original video frames without distortion or superimposed information inducing visual clutter. This hypothesis is also motivated by findings from perception research that indicate that there is no active deblurring mechanism for motion perception in the human visual system <ref type="bibr" coords="5,147.06,113.16,9.52,8.02" target="#b7">[8]</ref> . Although object trail visualization and predictive trajectory visualization augment information and modify the video frames, we expect them to show good results: at high playback speed, the time an object is depicted at a particular position lasts longer for the object trail visualization (previous entities fade out), which should improve identification, and predictive trajectory visualization highlights objects, which attracts the attention of the users. However, too many objects may cause visual clutter . Temporal blending may show the worst results: motion blur at high playback speed may hinder correct identification of search targets. </p><p> @BULLET Hypothesis 2 – Motion perception. By discarding frames, frameskipping may disrupt motion perception. Both temporal blending and object trail visualization additionally merge objects of the current frame with previous instances, which may improve motion perception. Predictive trajectory visualization includes information about the past and future movement of objects. Therefore, we hypothesize that it shows the highest performance in motion percep- tion. </p><p>@BULLET Hypothesis 3 – Playback speed feedback. We expect the speedometer to be the most efficient visualization for playback speed feedback due to its detailed information representation. However , since it requires visual focus, it may be most distracting. The color frame visualization has least influence on the video scene and shows only marginal distraction. The color mapping can represent a wide range of possible fast-forward accelerations; thus, it may be the best trade-off between visual distraction and the accuracy of conveyed information. Estimating the pace of playback via analog VCR fast-forward does not require visual focus. However, this benefit comes at the expense of distorted video rendering and a rather coarse granularity of speed feedback. We expect it to be the least preferred visualization. </p><p>We separately tested each hypothesis, by a specifically designed task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stimuli and Tasks</head><p> In our experiments, the independent variable of interest is the visualization type. Object identification and movement perception were tested with the fast-forward visualizations frame-skipping, temporal blending, object trail visualization, and predictive trajectory visualization . Playback speed feedback was evaluated with the speed visualizations speedometer, color frame, and analog VCR fast-forward. Each participant had to perform each of the three tasks T1, T2, and T3. Each of the three tasks was designed to check one of the three hypotheses. We used three videos in this study to compare the visualizations with each other. The videos are benchmark datasets used in different areas, such as tracking, and have in common that they show real world scenarios and originate from CCTV cameras. Video V1 (from the BE- HAVE dataset 1 , resolution: 640 × 480 px, 25 fps) and video V2 (from the AVSS dataset 2 , resolution: 720 × 576 px, 25 fps) are used for the two tasks concerning the fast-forward visualizations (T1 and T2). We choose these videos to account for different amount of activity and complexity of movement. The first video (V1) is less complex and less populated than V2, it includes less perspective distortion, and covers a smaller area. Video V3 (from the multi-camera i-LIDS dataset 3 , resolution: 720 × 576 px, 25 fps) is used for the playback speed visualization task. This video is chosen since it was also used to evaluate relevance measures for adaptive fast-forward <ref type="bibr" coords="5,460.18,217.59,14.19,8.02" target="#b19">[20,</ref><ref type="bibr" coords="5,477.22,217.59,10.64,8.02" target="#b20"> 21]</ref>. The tasks the participants had to perform were: @BULLET T1 – Object identification (objective and subjective). The participants had to watch V1 accelerated by factor 20 and V2 accelerated by factor 10, using each of the four visualizations. These acceleration factors were determined during the pilot study. The stimuli had lengths of 45 s and 106 s, respectively. As search target, animated cartoon figures were artificially inserted into the videos, walking through the scene like normal persons (see <ref type="figure" coords="5,461.90,304.36,29.36,8.02" target="#fig_3">Figure 8</ref> ). The participants had to detect and recognize the cartoon figures. The brightness and contrast of the cartoon figures were adapted to the video to avoid pre-attentive perception. Comic figures have the advantage that they can be easily rendered into an existing scene without recapturing the video and feature a good recognition value. The detection had to be indicated by pressing a buzzer. A detection was counted to be correct, if the buzzer was pressed while the cartoon figure was present in the sequence or had left not earlier than two seconds before. Each appearance of the cartoon figure was only counted once. Additionally to this objective measure, the visualizations had to be rated by the participants. The spatial and temporal positions of the cartoon figure varied to avoid learning effects—in total, we used 4 different versions of each video (V1 and V2). The cartoon figure appears 13 times in each version of V1 and 8 times in each version of V2 (the appropriate numbers were determined in the pilot study). To counter-balance the experiment, the presentation order of the visualizations was permuted. To balance the video versions, version 1 of the video was always shown first, then version 2, and so on. This assured that each visualization was presented the same number of times with each version of the video and potential side effects arising from different difficulties of the variants were avoided. @BULLET T2 – Motion perception (subjective). Each combination of pairs of the different visualizations (i.e., six visualization pairs) were presented and had to be compared by each participant. For each pair, two visualization stimuli of 10 s were successively presented, with a pause of 3 s in between. Three of the pairs showed a snippet of V1, for the other three pairs a snippet of V2 was used. The order and pair composition of the presented visualizations was permuted to counter-balance the experiment. The participants had to judge for each of the pairs which performed better in terms of motion perception . There was also the option to rate the performance of the two visualizations as equal. We used paired comparisons since the number of items from which to choose increases the cognitive burden and negatively influences the quality of the results <ref type="bibr" coords="5,500.47,659.87,13.74,8.02" target="#b14">[15]</ref>. @BULLET T3 – Playback speed feedback (subjective). The participants had to watch V3 in adaptive fast-forward three times, with each of the three speed visualizations applied. After the presentation of the stimuli, the participants had to rate each visualization in terms of effectiveness of playback speed feedback, perceived workload, and distraction from the video content. The presentation order of the visualizations was counter-balanced between the participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pilot Study</head><p>Before conducting the study, we checked the study design by a pilot study with four participants. The pilot study allowed us to identify potential issues with the tasks and stimuli. It turned out that V1 with acceleration factor of 16 and nine search targets was too easy for the first task. We increased the acceleration factor to 20 and inserted four additional search targets to the scene. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Environment Conditions and Technical Setup</head><p>The user study was conducted in a laboratory isolated from outside distractions. The room was artificially illuminated and only objects relevant for the study were contained inside. The participants were instructed to turn off their mobile phones. The videos were presented in full screen on a laptop with a 16 inch monitor and a screen resolution of 1366 × 768 px. The distance between the participants and the screen was 50 − 80 cm. A video player was implemented that allowed us to record the user input during the first task, in which the participants used a buzzer to indicate a detection of the search target. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Participants</head><p>The study was designed as within-subjects study and was conducted with 25 participants. Due to the lack of interest of one participant, the test results of 24 participants are considered. Seven of those participants were female and 17 were male. Gender was not considered as relevant factor. The average age was 25.5 years; the youngest participant was 20 and the oldest 31. Most of the participants were students from our university. Six participants studied at other universities or had already graduated. Half of the participants were students of computer science or software engineering. The others had majors in humanities or other subjects of study. All participants were tested with a Snellen chart and had normal or corrected-to-normal vision. The experiment took 45 − 55 min, depending on the particular speed of each participant. Each participant was compensated with EUR 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Study Procedure</head><p>The participants were first asked to fill out a questionnaire about their gender, age, and field of study. Then, a short tutorial (about 6 min) introduced the fast-forward visualizations using snippets of V1. Afterwards, a video snippet with a cartoon figure was shown to the participants to explain task T1. The video snippet was not reused in the tasks and it was presented in original playback speed. The first task was performed using V1. The participants watched all four visualizations and had to identify the cartoon figures. The start of the playback of each visualization was initiated by the participants. Hence, small breaks of individual length were possible between the visualizations. After completing the task, the participants were asked to fill out a questionnaire about the effectiveness of the visualizations (subjective effectiveness), how comfortable they felt with the visualizations (comfort ), how useful they considered each of the visualizations (usefulness ), and the effort it made to watch them. For this rating, a 10-point Likert scale was provided. They were also asked which visualization they preferred for performing the task (forced choice). Finally, the possibility was provided to give free comments about the visualizations . The same procedure was repeated with V2. For task T2, the participants were briefed to pay attention to object movement. The visualizations were presented pairwise, using snippets of V1 and V2. After presentation of a pair, the participants had to decide which visualization supported motion perception better, or if both performed similar. After T2, the subjects were asked to comment on the visualizations. Before the experiment with the third task was conducted, the different playback speed visualizations were introduced in a short tutorial (about 4 min in length). Then, one stimulus for each of the three visualizations (calculated on V3) was successively presented to the participants. The duration of each stimulus was approximately 1 min. Afterwards, the participants were asked to fill out a questionnaire to judge the playback speed visualizations in terms of the effectiveness of the speed feedback, the effort required to interpret them, and the distraction from video they induced, on a 10-point Likert scale. In the end, they also provided their preferred visualization technique (forced choice) and could comment on all speed visualizations. There was a " Give Up " option throughout the study; however, it was not used by the participants. The time to perform the tasks was limited by the duration of the videos. There was no time limitation between the videos and the participants decided when to continue. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">STUDY RESULTS</head><p>In the statistical analysis, we include the results of 24 participants. We test the significance of the results with non-parametric tests since not all results are normal distributed. For statistical computing, we used the software from the R Project <ref type="bibr" coords="6,400.33,404.08,9.52,8.02" target="#b0">[1]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results of the Object Identification Task T1</head><p> The results of task T1 are divided into the measured effectiveness (objective results) and the results of the questionnaire (subjective results). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Results of the Measured Effectiveness</head><p> To measure the performance of the participants in task T1 (object identification ), we calculate F 1 -scores (F 1 = 2 · Precision·Recall Precision+Recall ) by determining the precision and recall using the buzzer input. The boxplots of the F 1 -scores of all subjects for each visualization are depicted in <ref type="figure" coords="6,285.12,516.97,29.01,8.02" target="#fig_5">Figure 9</ref>. The non-parametric Kruskal-Wallis test shows a significant effect of the visualization type on object identification (H(3) = 46.63, p &lt; 0.005). Post-hoc pairwise U-tests confirm that frame-skipping (median of F 1 -scores: 0.87) and object trail visualization (0.84) show significantly better results (p &lt; 0.005, Bonferroni-corrected for multiple comparisons ) than the other two visualizations (temporal blending: 0.67, predictive trajectory visualization: 0.70), respectively. According to the comments of the participants, the latter visualizations suffer from information overload. Object trail visualization and frame-skipping do not show any significant differences. However, some subjects described the trails to be useful for identifying the objects. With temporal blending and predictive trajectory visualization, it was more difficult for the participants to identify an object. The test showed no significant differences between these two visualizations. According to the participants' comments, the objects in the video were too blurry when visualized by temporal blending. Some subjects also had problems with predictive trajectory visualization. They reported that the visualization provided too much information to focus on object identification. Nevertheless, they also mentioned that this visualization was useful in cases where only few objects were present.  As outlined in Hypothesis 1, frame-skipping showed the best results in object identification, and object trail visualization was able to produce similar results. However, we have to reject the hypothesis that predictive trajectory visualization can achieve results comparable to object trail visualization. According to the median, temporal blending performed worst, but it was not significantly worse compared to predictive trajectory visualization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Results of the Questionnaire</head><p>Figure 10 depicts boxplots that summarize the questionnaire results of task T1: @BULLET Subjective effectiveness. The Kruskal-Wallis test shows that the visualization type had a significant influence on the rating (H(3) = 35.66, p &lt; 0.005), and the U-test reveals significant differences (p &lt; 0.05, Bonferroni-corrected) between all visualizations except for two pairs: frame-skipping / object trail visualization and predictive trajectory / object trail visualization. The boxplots in <ref type="figure" coords="7,247.31,424.24,34.56,8.02" target="#fig_7">Figure 10</ref> (left) show that the subjective effectiveness and the measured objective effectiveness (cf. <ref type="figure" coords="7,133.05,444.16,30.53,8.02" target="#fig_5">Figure 9</ref>) in object identification exhibit qualitatively similar results. However, in comparison to the other methods, the effectiveness of temporal blending was rated much worse than the objectively measured results indicate. @BULLET Usefulness. The visualizations show significant differences (Kruskal-Wallis: H(3) = 36.91, p &lt; 0.005) in terms of usefulness. Predictive trajectory visualization (median: 5.5) was considered significantly less useful than frame-skipping (median: 7.25, U-test: p &lt; 0.05, Bonferroni-corrected). According to the comments of the participants, predictive trajectory visualization was only useful as long as a small number of objects were present in the scene. No significant difference was found between frame-skipping and object trail visualization (median: 7.0). Temporal blending was rated the least useful with a median of 4.0 and with significant differences to all other visualizations (U-test: p &lt; 0.05, Bonferroni-corrected). @BULLET Comfort. Significant differences (Kruskal-Wallis: H(3) = 33.51, p &lt; 0.005) were found for the evaluation of comfort among the visualizations . Frame-skipping—as the familiar method to watch fastforward videos—was considered the most comfortable to watch (median: 7.5). Frame-skipping performed significantly better than predictive trajectory visualization and temporal blending (U-test: p &lt; 0.05, Bonferroni-corrected). A quite similar rating with significant difference to temporal blending was received for object trail visualization (median: 7.0; U-test: p &lt; 0.05, Bonferronicorrected ). However, there was no significant difference between frame-skipping and object trail visualization. The least comfortable visualization was temporal blending (median: 3.5), which performed significantly worse than the others, except for predictive trajectory visualization (median: 5.5). @BULLET Effort. There were significant differences (Kruskal-Wallis: H(3) = 15.36, p &lt; 0.005, U-test: p &lt; 0.05, Bonferroni-corrected) between frame-skipping (median: 6.75) / temporal blending (median: 8.5) and between object trail visualization (median: 6.5) / temporal blending. According to the medians, temporal blending demands highest effort, predictive trajectory visualization follows with a median of 7.25, but there were no significant differences between predictive trajectory visualization and the other three visualizations. </p><p>For the forced choice question in task T1, which visualization the participants prefer, 43% answered with frame-skipping, 27% preferred predictive trajectory visualization, 23% object trail visualization, and 7% temporal blending. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results of the Motion Perception Task T2</head><p> A ranking of the pairwise compared visualizations was generated according to Kendall <ref type="bibr" coords="7,365.53,576.03,13.94,8.02" target="#b22">[23]</ref>: A visualization receives one point for each won comparison, and half a point for each draw. The resulting scores of the pairwise results are depicted in <ref type="figure" coords="7,433.17,595.96,25.71,8.02" target="#tab_1">Table 1</ref>, summarized results in <ref type="figure" coords="7,294.12,605.92,33.46,8.02" target="#fig_8">Figure 11</ref>. Following the test of Kendall <ref type="bibr" coords="7,443.61,605.92,14.94,8.02" target="#b22">[23] </ref>for paired comparsions with ties, the coefficient of agreement u shows a significant but relative low accordance between the participants (u = 0.07, χ 2 (6) = 26.90, p &lt; 0.05). Predictive trajectory visualization won most of the comparisons, as Hypothesis 2 outlined, and received a score of 91. Surprisingly, frameskipping is second with 86.5 points. Expected advantages of temporal blending (last rank, 51.5 points) in motion perception, and object trail visualization (third, 59 points) could not be confirmed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results of the Playback Speed Feedback Task T3</head><p> The Kruskal-Wallis test showed no significant effects of the visualization type on the given answers. <ref type="figure" coords="7,421.48,726.46,34.98,8.02">Figure 12</ref>shows the results of the questionnaire for this task. Based on the participants' free comments  and the medians, we summarize these results with the following ob- servations: @BULLET Effectiveness. The participants attested analog VCR fast-forward the highest effectiveness in playback speed feedback (median: 8.0). Many participants explained their rating by their familiarity to such visualization from VCR. In terms of playback speed feedback, the speedometer also showed reasonable performance (median: 7.0). @BULLET Effort. The participants rated analog VCR fast-forward also best according to the median in terms of effort while watching (median: 3.0). The effort required to keep track of the speed with the speedometer was higher according to its median (5.0). Following the comments of the participants, a reason for that originated from the frequent changes of focus between the video and the speedometer . The participants reported that the frequent color changes of the color frame visualization (median: 6.0) resulted in flicker, which led to stress. @BULLET Distraction. The visualizations only marginally (according to the medians) differ in terms of distraction. The medians of the visualizations are 5.5 (speedometer), 5.0 (color frame visualization), and 4.5 (analog VCR fast-forward). The participants mentioned the frequent changes of focus between the video and the speedometer as a problem. </p><p>For playback speed visualization, 40% of the participants preferred the speedometer in the forced choice question, 36% the analog VCR fast-forward, and 24% the color frame visualization. As assumed in Hypothesis 3, the speedometer was the preferred visualization for speed estimation. However, the color frame visualization was not as good as expected, probably because of the induction of stress by flicker. Also remarkable is that the issue of distortion and coarse playback speed feedback for analog VCR fast-forward was not considered that problematic as we assumed. Nevertheless, the difference between the visualizations is relatively small, thus no significant differences could be detected by Kruskal-Wallis test as mentioned above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND CONCLUSION</head><p>In this paper, we have evaluated different visualization techniques for video fast-forward. Four visualizations, of which two were introduced in this paper, have been compared with each other in terms of their support for object identification and motion perception. Additionally, we have evaluated the performance of three methods for visual playback speed feedback in the context of adaptive fast-forward. Two of the methods compared were introduced in this paper. The user study exhibited some remarkable results concerning the performance in object identification of the different fast-forward visualizations . Frame-skipping appeared to be the preferred method for object identification and performed also well in motion perception. This result is contrary to the initial hypothesis that motion perception would be negatively affected by discarded frames. Temporal blending failed in both tasks: object identification and motion perception. It was especially surprising that most of the participants rejected this visualization also for the task of motion perception. The object trail visualization showed comparable results to frame-skipping in the object identification task. Nevertheless, the trail of an object did not improve motion perception. Although predictive trajectory visualization provides the most information on object motion (it was ranked best for this task), the additional information results in visual clutter and hinders object identification in crowded scenes. Hence we recommend, depending on the task, using either frame-skipping or predictive trajectory visualization as fast-forward visualization. Due to the dependencies of the visualization on different characteristics of the video stimuli, we plan to evaluate the video visualizations in detail according to the amount of action (i.e., crowded vs. empty scenes) and according to different playback speeds in future work. Especially the results of temporal blending indicated that this visualization depends on the acceleration factor; a reason could be that faster playback causes stronger blur. Another direction for future work is the adaption of predictive trajectory visualization according to the number of objects present in scene to reduce visual clutter. Moreover, switching automatically to the most promising visualization according to the current characteristics of video stimuli may support users. The results of the evaluation of playback speed visualizations for adaptive fast-forward sum up to: feedback of playback speed was considered best for the speedometer visualization. However, the participants of the study criticized the need to constantly switch the visual focus between the video and the speedometer. The color frame, in contrast, induced stress by visual flicker, which limits its application to cases of non-permanent usage. Analog VCR fast-forward seems to be the best solution to perceive speed feedback while watching video in adaptive fast-forward despite its coarse feedback. Especially in the context of playback speed feedback, the results of the study point out the strong benefit of using established metaphors in visualization. Although analog VCR fast-forward distorts the video signal and the speedometer visualization requires the switch of visual focus, their acceptance was surprisingly high. The conducted user study focused on real-world videos that originated from surveillance cameras. For the first task, we superimposed the video by an artificial object, a cartoon figure, which should be identified. Although we put much effort in adapting the brightness and contrast of the objects to appear as normal persons, we cannot fully rule out effects from this choice of stimulus. Therefore, future work should also feature real objects for the identification task. One direction for future work is also to generalize the results to completely different video footage, and visualization in general. As mentioned in the introduction, the visualization techniques can generally be applied to interactive and animated visualizations (e.g., <ref type="bibr" coords="8,455.28,579.00,14.19,8.02" target="#b18">[19,</ref><ref type="bibr" coords="8,471.53,579.00,11.20,8.02" target="#b30"> 31]</ref> ) and other animated content. Due to interactive steering of the playback speed of such animations, the playback speed visualizations can be applied as well. For them, we also plan to evaluate further solutions, such as a progress bar or a rotating bar. Such visualizations seem promising due to good motion perception in the peripheral field of view. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness </head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,22.50,470.46,250.38,7.37;4,22.50,479.93,250.38,7.37;4,22.50,489.39,224.55,7.37"><head>Fig. 6. </head><figDesc> Fig. 6. The three proposed adaptive fast-forward playback speed visualizations . Left: speedometer; center: color frame; right: analog VCR fast-forward. The color scheme is depicted in the bottom center. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,285.12,471.35,250.38,7.37;4,285.12,480.81,250.38,7.37;4,285.12,490.27,250.38,7.37;4,285.12,499.74,68.98,7.37"><head>Fig. 7. </head><figDesc>Fig. 7. Analog VCR fast-forward visualization. Pixels inside the black rectangles are randomly shifted to generate horizontal distortion lines. The playback speed is indicated by the number of distortion rows and the distortion width. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,294.12,169.63,250.38,7.48;5,294.12,179.10,250.38,7.48;5,294.12,188.64,156.19,7.37"><head>Fig. 8. </head><figDesc>Fig. 8. In T1, the search target to be identified was a cartoon figure (left) that was added to the video. Right: Example frame of V2 with the artificially added search target (marked red). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,22.50,230.59,158.89,8.22"><head>Fig. 9. </head><figDesc>Fig. 9. F 1 -scores for object identification (T1). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,31.50,217.37,513.00,7.48;7,31.50,226.92,119.85,7.37"><head>Fig. 10. </head><figDesc>Fig. 10. Boxplots of the results of the questionnaire of task T1 (the whiskers represent the lowest / highest values within one and a half times interquartile range to the median). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,294.12,365.15,250.38,7.48;7,294.12,374.70,250.38,7.37;7,294.12,384.16,191.01,7.37"><head>Fig. 11. </head><figDesc>Fig. 11. Summarization of the motion perception ranking (task T2). From left to right: The first, second, third and fourth place of the ranking. Green/bottom: won; yellow/middle: draw; red/top: lost. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false" coords="3,41.74,48.76,223.95,237.72"><figDesc coords="3,116.01,269.63,20.47,6.66">Objects</figDesc><table coords="3,41.74,48.76,223.95,237.72">Foreground Mask 

Selected Previous Frames 

Destination Frame 

Blended Frames 

Background Subtraction 

Morphological Operators 

. . . 

Current Frame 
Background Image 

Blending 

0.5 
0.25 
0.125 
. 
. 
. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false" coords="4,18.25,26.18,518.00,7.54"><figDesc coords="4,18.25,26.46,16.00,7.26;4,132.60,26.18,403.65,7.50">2098 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 18, NO. 12, DECEMBER 2012</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true" coords="8,25.43,59.58,488.36,85.33"><figDesc coords="8,159.74,59.58,238.53,7.48">Table 1. Scores of the pairwise motion perception ranking (task T2).</figDesc><table coords="8,25.43,79.58,488.36,65.33">Frame-skipping 
Temporal Blending 
Object Trail 
Visualization 

Predictive Trajectory 
Visualization 
Sum 

Frame-skipping 
-
31 
31.5 
25 
86.5 
Temporal Blending 
17 
-
20 
14.5 
51.5 
Object Trail Visualization 
17.5 
28 
-
13.5 
59 
Predictive Trajectory Visualization 
23 
33.5 
34.5 
-
91 

</table></figure>

			<note place="foot" n="1"> BEHAVE Interactions Test Case Scenarios http://groups.inf.ed.ac.uk/vision/BEHAVEDATA/INTERACTIONS/ 2 i-LIDS dataset for AVSS 2007 3 i-LIDS multi-camera tracking scenario dataset http://www.homeoffice.gov.uk/science-research/hosdb/i-lids/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>We thank Michael Wörner for voice acting. This work was funded by German Research Foundation (DFG) as part of the Priority Program " Scalable Visual Analytics " (SPP 1335). </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="8,303.38,718.17,232.12,7.13"  xml:id="b0">
	<monogr>
		<title level="m" type="main">The R Project for Statistical Computing</title>
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,303.38,727.63,232.12,7.13;8,303.38,737.10,178.27,7.13;9,426.27,127.58,8.64,20.63;9,425.95,89.27,8.64,22.35;9,69.92,88.94,8.64,20.63;9,70.24,126.14,8.64,22.35;9,468.83,61.61,46.48,8.64;9,468.84,82.48,43.02,8.64;9,468.79,102.06,42.59,8.64;9,468.79,111.73,46.02,8.64;9,31.50,195.32,272.32,7.48"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualization of Time-Oriented Data Better Worse Better Worse Speedometer Color Frame Analog VCR Fast-Forward Fig</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Aigner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Miksch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Schumann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Tominski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Boxplots of the results of the playback speed visualizations (task T3)</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,227.33,232.12,7.13;9,49.76,236.80,232.12,7.13;9,49.76,246.26,17.93,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal and spatial summation in human vision at different background intensities</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Barlow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="350" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,255.73,232.12,7.13;9,49.76,265.19,232.12,7.13;9,49.76,274.66,203.16,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">High-density cursor: A visualization technique that helps users keep track of fast-moving mouse cursors</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Baudisch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Cutrell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Robertson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interact 2003</title>
		<meeting>Interact 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,284.12,232.54,7.13;9,49.76,293.59,232.12,7.13;9,49.76,303.05,232.12,7.13;9,49.76,312.52,110.69,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on videobased graphics and video visualization</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Borgo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Grundy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Daubney</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Heidemann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Höferlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Höferlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jänicke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Xie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics 2011 – State of the Art Reports</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,321.98,232.12,7.13;9,49.76,331.44,218.59,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Rainbow color map (still) considered harmful</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Borland</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Taylor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Ii</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="14" to="17" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,340.91,232.12,7.13;9,49.76,350.37,232.12,7.13;9,49.76,359.84,221.57,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Action-based multifield video visualization</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">P</forename>
				<surname>Botchen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bachthaler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Schick</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Mori</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ertl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="885" to="899" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,369.30,232.12,7.13;9,49.76,378.85,232.12,6.86;9,49.76,388.23,85.23,7.13"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion deblurring in human vision</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Burr</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Morgan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Royal Society of London. Series B: Biological Sciences</title>
		<meeting>the Royal Society of London. Series B: Biological Sciences</meeting>
		<imprint>
			<date type="published" when="1380" />
			<biblScope unit="page" from="431" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,397.70,232.12,7.13;9,49.76,407.16,181.82,7.13"  xml:id="b8">
	<monogr>
		<title level="m" type="main">Dynamic stills and clip trailers. The Visual Computer</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Caspi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Axelrod</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Matsushita</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gamliel</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,416.62,232.12,7.13;9,49.76,426.09,231.62,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Augmented keyframe</title>
		<author>
			<persName>
				<forename type="first">G.-C</forename>
				<surname>Chao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y.-P</forename>
				<surname>Tsai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S.-K</forename>
				<surname>Jeng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="682" to="692" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,435.55,232.12,7.13;9,49.76,445.02,232.12,7.13;9,49.76,454.48,173.00,7.13"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual signatures in video visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Botchen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Hashim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ertl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Thornton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1093" to="1100" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,463.95,232.12,7.13;9,49.76,473.41,232.12,7.13;9,49.76,482.88,196.35,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Smartplayer: user-centric video fast-forwarding</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Cheng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Chu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Human Factors in Computing Systems</title>
		<meeting>the 27th International Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="789" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,492.34,232.12,7.13;9,49.76,501.81,232.12,7.13;9,49.76,511.27,184.96,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimedia analysis + visual analytics = multimedia analytics</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Chinchor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">C</forename>
				<surname>Wong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">G</forename>
				<surname>Christel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ribarsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,520.73,232.12,7.13;9,49.76,530.20,103.09,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion streaks provide a spatial code for motion direction</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">S</forename>
				<surname>Geisler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="issue">6739</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,539.66,232.12,7.13;9,49.76,549.13,232.12,7.13;9,49.76,558.59,232.12,7.13;9,49.76,568.06,37.86,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Conjoint analysis to measure the perceived quality in volume rendering</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Giesen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Mueller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Schuberth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Zolliker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1664" to="1671" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,577.52,232.12,7.13;9,49.76,586.99,232.12,7.13;9,49.76,596.45,102.82,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Schematic storyboarding for video visualization and editing</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B</forename>
				<surname>Goldman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Curless</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Salesin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">M</forename>
				<surname>Seitz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="862" to="871" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,605.92,232.12,7.13;9,49.76,615.38,232.12,7.13;9,49.76,624.84,17.93,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Area and the intensity-time relation in the peripheral retina</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Graham</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Margaria</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="305" />
			<date type="published" when="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,634.31,232.12,7.13;9,49.76,643.77,206.03,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Regional variations in the visual acuity for interference fringes on the retina</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Green</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="356" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,653.24,232.12,7.13;9,49.76,662.70,232.12,7.13;9,49.76,672.17,77.26,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Animated transitions in statistical data graphics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Heer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Robertson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1240" to="1247" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,681.63,232.12,7.13;9,49.76,691.10,232.12,7.13;9,49.76,700.56,113.57,7.13"  xml:id="b19">
	<monogr>
		<title level="m" type="main">Informationbased adaptive fast-forward for visual surveillance</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Höferlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Höferlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Heidemann</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Multimedia Tools and Applications</publisher>
			<biblScope unit="page" from="127" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,710.02,232.12,7.13;9,49.76,719.49,232.12,7.13;9,49.76,728.95,232.12,7.13;9,312.38,227.33,223.80,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a visual attention model for adaptive fast-forward in video surveillance</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Höferlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Pflüger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Höferlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Heidemann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,236.80,232.12,7.13;9,312.38,246.26,232.12,7.13;9,312.38,255.73,232.12,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual analytics: Scope and challenges</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Keim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Mansmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schneidewind</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Ziegler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="76" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,265.19,232.12,7.13;9,312.38,274.66,44.93,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Rank Correlation Methods</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">G</forename>
				<surname>Kendall</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Charles Griffin and Company</title>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,284.12,232.12,7.13;9,312.38,293.59,232.12,7.13;9,312.38,303.05,232.12,7.13;9,312.38,312.52,110.69,7.13"  xml:id="b23">
	<analytic>
		<title level="a" type="main">To catch a thief – you need at least 8 frames per second: The impact of frame rates on user performance in a CCTV detection task</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Keval</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Sasse</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="941" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,321.98,232.12,7.13;9,312.38,331.44,232.12,7.13;9,312.38,340.91,212.05,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Empirical studies in information visualization: Seven scenarios</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Lam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Bertini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Isenberg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Plaisant</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Carpendale</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1520" to="1536" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,350.37,232.12,7.13;9,312.38,359.84,172.59,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">Motion blur rendering: State of the art</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Navarro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">J</forename>
				<surname>Sern</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gutierrez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,369.30,232.12,7.13;9,312.38,378.77,232.12,7.13;9,312.38,388.31,232.12,6.86;9,312.38,397.70,111.64,7.13"  xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive fast playback-based video skimming using a compressed-domain visual complexity measure</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Peker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Divakaran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2055" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,407.16,232.12,7.13;9,312.38,416.62,167.97,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive video fast forward</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Petrovic</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Jojic</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="344" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,426.09,232.12,7.13;9,312.38,435.63,232.12,6.86;9,312.38,445.02,134.21,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Background subtraction techniques: a review</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Piccardi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>the IEEE International Conference on Systems, Man and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3099" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,454.48,232.12,7.13;9,312.38,463.95,232.12,7.13;9,312.38,473.41,219.00,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pirolli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Card</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Intelligence Analysis</title>
		<meeting>International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,482.88,232.12,7.13;9,312.38,492.34,232.12,7.13;9,312.38,501.81,173.00,7.13"  xml:id="b30">
	<analytic>
		<title level="a" type="main">Effectiveness of animation in trend visualization</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Robertson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Fernandez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Fisher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1325" to="1332" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,511.27,232.12,7.13;9,312.38,520.73,232.12,7.13;9,312.38,530.20,17.93,7.13"  xml:id="b31">
	<monogr>
		<title level="m" type="main">An instinct for detection: Psychological perspectives on CCTV surveillance. The Police Journal</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Scott-Brown</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Cronin</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="287" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,539.66,232.12,7.13;9,312.38,549.13,232.12,7.13;9,312.38,558.59,108.76,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">The eyes have it: a task by data type taxonomy for information visualizations</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Shneiderman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Visual Languages</title>
		<meeting>IEEE Symposium on Visual Languages</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,568.06,232.12,7.13;9,312.38,577.52,232.12,7.13;9,312.38,586.99,102.72,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">Salient video stills: content and context preserved</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Teodosio</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Bender</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACM International Conference on Multimedia</title>
		<meeting>the First ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,596.45,232.12,7.13;9,312.38,605.92,193.92,7.13"  xml:id="b34">
	<monogr>
		<title level="m" type="main">Information Visualization: Perception for Design</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ware</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edition</note>
</biblStruct>

<biblStruct coords="9,312.38,615.38,232.12,7.13;9,312.38,624.84,232.12,7.13;9,312.38,634.31,17.93,7.13"  xml:id="b35">
	<monogr>
		<title level="m" type="main">The Visualization Handbook, chapter Overview of flow visualization</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Erlebacher</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="261" to="278" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,643.77,232.12,7.13;9,312.38,653.24,232.12,7.13;9,312.38,662.70,77.26,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">A time model for time-varying visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Wolter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Assenmacher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hentschel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schirski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Kuhlen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1561" to="1571" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,672.17,232.12,7.13;9,312.38,681.63,150.51,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Yilmaz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Javed</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Shah</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
