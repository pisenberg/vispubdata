<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T15:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Interpolation of Data with Normally Distributed Uncertainty for Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Steven</forename>
								<surname>Schlegel</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Nico</forename>
								<surname>Korn</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Gerik</forename>
								<surname>Scheuermann</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">On the Interpolation of Data with Normally Distributed Uncertainty for Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Gaussian process</term>
					<term>uncertainty</term>
					<term>interpolation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>—In many fields of science or engineering, we are confronted with uncertain data. For that reason, the visualization of uncertainty received a lot of attention, especially in recent years. In the majority of cases, Gaussian distributions are used to describe uncertain behavior, because they are able to model many phenomena encountered in science. Therefore, in most applications uncertain data is (or is assumed to be) Gaussian distributed. If such uncertain data is given on fixed positions, the question of interpolation arises for many visualization approaches. In this paper, we analyze the effects of the usual linear interpolation schemes for visualization of Gaussian distributed data. In addition, we demonstrate that methods known in geostatistics and machine learning have favorable properties for visualization purposes in this case.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Almost all data suffer from some kind of uncertainty and it is always important to know whether there is uncertainty and how large it is. Otherwise, we risk making decisions that rely on data we should not rely upon or doubting data that we think might be uncertain when it is not. This can obviously lead to poor decision making. Therefore, it is vital to integrate our knowledge about the uncertainty of the data into our visualizations. As we will see, this not only helps to raise awareness for the existence and magnitude of the uncertainty, but will also allow us to make better estimates about the data than simply assuming a single dataset to be the truth. An important open problem in uncertainty visualization and related fields is how uncertainty and interpolation interact. This is particularly relevant for data that suffers from missing values or low resolution. There are many established interpolation schemes available, but how reliable are the interpolated values? We want to address this problem by analyzing the traditional linear interpolation and its impact on Gaussian distributed data. We will see that there are counter-intuitive and unwanted side effects. Therefore, we will incorporate Gaussian process regression to interpolate uncertain data. By doing so, we will show that the interpolation scheme is independent of the data itself – similar to non-random data. Furthermore, we will give an analytical description of the basis functions for the interpolation. The rest of the paper is structured as follows. In Section 3, we explore the effects of linear interpolation on the uncertainty of the interpolated values. Section 4 gives the mathematical background for Gaussian process regression and Section 5 shows how this can be used to build a complete stochastic model of arbitrary scalar data that takes our knowledge about the uncertainties and correlations in the data into account. We then show that for every Gaussian process an equivalent interpolation scheme exists. Finally, we tested Gaussian process regression and compared it to classic linear interpolation using synthetic and real-world data in Section 6. We will see, that the quality of interpolation in Gaussian processes is superior in many ways when compared to linear interpolation. @BULLET Steven Schlegel is with the University of Leipzig, E-mail: schlegel@informatik.uni-leipzig.de. @BULLET Nico Korn is with the Helmholtz-Centre Dresden-Rossendorf, E-mail: n.korn@hzdr.de. @BULLET Gerik Scheuermann is with the University of Leipzig, E-mail: scheuermann@informatik.uni-leipzig.de. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling Uncertainty</head><p> This paper will deal with Gaussian process regression, a statistics technique that has applications in machine learning <ref type="bibr" coords="1,463.83,305.87,14.94,8.02" target="#b19">[20] </ref>and is also known in the GIS scene, there called Kriging <ref type="bibr" coords="1,440.48,315.84,14.94,8.02" target="#b9">[10] </ref>(named after the South African mining engineer Daniel Krige who developed the method). It can be used to model uncertain data, i.e. a set of Gaussian-distributed random variables. However, it has not been used for general purpose visualization yet and there is not much material for heteroscedastic Gaussian processes, i.e. Gaussian processes with different uncertainty levels at the data points. There does not seem to be other widespread methods for general purpose uncertainty modeling, indeed not even Kriging seems to be known outside the GIS community. But some approaches for special applications do exist. For very simple cases, it may be sufficient to simply assume identical distributions, perhaps two-dimensional or more, at every data point, for example a Gaussian/normal, log-normal, exponential, Poisson or some other, and just store the parameters (e.g. (co)variance) at every data point. For more complicated situations, other schemes are needed. Pauly et al. <ref type="bibr" coords="1,351.19,486.28,14.94,8.02" target="#b15">[16] </ref>visualize possible surfaces that fit a given point cloud. To do this, they created their own empirical model to estimate the probability of a line going through a particular point and compute that probability for all pixels. That probability is designed in such a way that there is a low angle between the point and a possible line segment, and a small length of that line segment leads to a high probability . All the probabilities with regard to all line segments are merged into a distance-weighted sum which gives the final probability for the surface to pass through that point. A big advantage of that method is, that distinct alternative routes are possible, which is not usually the case as regression normally seeks to find a single mean function. Pöthkow et al. <ref type="bibr" coords="1,362.47,596.23,14.94,8.02" target="#b17">[18] </ref>presented a generalization of isocontours for uncertain fields. They compute the level-crossing probability – the probability for an interval to have a crossing of the given threshold in it. A simple, but convincing method that is easy to understand and compute. They interpolate the expected values and the roots of the central moments in order to interpolate the probability density function . This work was extended to correlated data by Pöthkow et al. <ref type="bibr" coords="1,294.12,665.97,13.74,8.02" target="#b18">[19]</ref>. Inspired by <ref type="bibr" coords="1,348.38,676.29,13.74,8.02" target="#b17">[18]</ref>, Pfaffelmoser et al. <ref type="bibr" coords="1,440.82,676.29,14.94,8.02" target="#b16">[17] </ref>developed an algorithm to incremetally compute isosurface-first-crossing-probability (IFCP). This probability is visualized by volume rendering. They enhance the visualization with the rendering of the stochastic distance function (SDF-surfaces). Kniss et al. <ref type="bibr" coords="1,353.70,726.46,10.45,8.02" target="#b8">[9] </ref>try to perform classification of medical volume data under uncertainty. They base their transfer function on what they (a) 300 random lines with Gaussian distributed endpoints (b) colormap of the linear interpolated probability density func- tion <ref type="figure" coords="2,22.50,220.83,19.53,7.37">Fig. 1</ref>. Illustrations on the interpolation of the probability density function: X 1 ∼ N (µ, σ 2 ) and X 2 ∼ N (µ, σ 2 ) are Gaussian distributed random variables. </p><formula>(a) (b) (c) </formula><p>Fig. 2. Interpolation of the variance σ 2 1 = 1 of X 1 at s = 0 and the variance σ 2 2 = 1 of X 2 at s = 1 with (a) cov(</p><formula>X 1 , X 2 ) = 0.5, (b) cov(X 1 , X 2 ) = 1 and (c) cov(X 1 , X 2 ) = 2. </formula><p>call the decision boundary distance that is computed for every class, which is a maximal log-odds ratio of all the other classes. Roughly speaking, it is a measurement of the risk of being wrong to assume that the current class is the correct one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ambiguity for Uncertainty Visualization</head><p>The idea behind intentionally ambiguous visualization is that if the data is uncertain, the viewer should be equally uncertain about the visualized value. There are several ways to achieve this goal, but it is worth noting that they can often be interpreted as visualizations of the probability density function of the uncertain value. A common method for ambiguous visualization is to make lines or other glyphs " fat " , i.e. scale their width proportionally to the uncertainty . From a probabilistic point of view, this corresponds to a uniform distribution of values (in contrast to a Gaussian blurring). Fat glyphs can be applied to extend a wide array of existing methods to include uncertainty. Li et al. <ref type="bibr" coords="2,118.49,566.81,14.94,8.02" target="#b10">[11] </ref>for example, use an ellipse to show the anisotropic uncertainty of the star positions they are visualizing with their system. Lines in 3D become ribbons or tubes, see <ref type="bibr" coords="2,225.04,586.73,14.94,8.02" target="#b22">[23] </ref>and <ref type="bibr" coords="2,257.94,586.73,14.94,8.02">[12] </ref>for example. More elaborate applications are also possible. Hlawatsch et al. <ref type="bibr" coords="2,44.82,606.66,10.45,8.02" target="#b6">[7] </ref> designed a glyph for vector data that shows both the variation of flow over time and the angular uncertainty over time. Since we want to show the results of our approach on scalar data, the following approach will be suitable: For heightfields, fat surfaces can be implemented as multiple surfaces or direct volume renderings. Their inherent problem is that the surfaces will obscure each other and are easily confused. To avoid this, the mean surface can be of different color and high specularity (<ref type="bibr" coords="2,126.36,676.40,13.23,8.02" target="#b17">[18]</ref>). Zehner et al. on the other hand <ref type="bibr" coords="2,22.50,686.36,14.94,8.02" target="#b24">[26] </ref>added extra geometry to isosurfaces in order to depict positional uncertainty. Another way of visualizing uncertainty information is to simply not display anything at all or make uncertain data transparent. An interesting application of non-visualization comes from map-making. Lowell et al. <ref type="bibr" coords="2,43.81,736.42,14.94,8.02" target="#b12">[13] </ref>only draw boundaries of areas when they are certain about where they actually are. If they are uncertain, the line dividing two areas simply stops. Strothotte et.al.<ref type="bibr" coords="2,413.17,414.89,12.95,8.02" target="#b21">[22] </ref>developed a visualization tool for ancient architecture that avoids photorealism, instead mimicking a line drawing with the strokes becoming thinner, transparent and shaky in areas of high uncertainty. For direct volume rendering, transparency is the natural way to implement non-visualization, see Djurcilov et al. <ref type="bibr" coords="2,285.12,464.71,10.45,8.02" target="#b4">[5] </ref>for example. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> 3 ANALYSIS OF LINEAR, BILINEAR AND TRILINEAR INTERPO- LATION OF UNCERTAIN DATA </head><p>Most visualization methods for grid-based scalar, vector, or tensor data require interpolation. This is also true if the data comes with Gaussian distributed noise. In this section, we analyze the effect of linear, bilinear, or trilinear interpolation of such data. First, we consider the 1-d case, i.e. linear interpolation on the line. In the simplest case, we are given two uncorrelated scalar Gaussian distributions X 1 ∼ N (µ 1 , σ 2 1 ) and X 2 ∼ N (µ 2 , σ 2 2 ) at the positions s 1 and s 2 . This is also the typical case in one dimension, because each line segment is interpolated without any dependence on data outside the line segment. Before we turn to the theory, let us do a simple experiment . We draw a finite number of random lines between these two points, i.e. looking at the linear interpolation between the endpoints. Each line has its endpoints normally distributed according to X 1 and X 2 . It is obvious that more lines will appear that have both of their endpoints near the means of X 1 and X 2 than for instance lines with both endpoints near 2 · σ 1 and 2 · σ 2 , respectively. Since we are interpolating , let us have a look at an arbitrary position s in between s 1 and s 2 . <ref type="figure" coords="2,285.12,696.57,19.77,8.02">Fig. 1</ref>(a) shows 300 random lines with Gaussian distributed endpoints. Judging from the image it appears that the variance at s (s 1 &lt; s &lt; s 2 ) is smaller than at the endpoints. In fact, if we consider the value at s as a scalar random variable X, the usual linear interpolation formula X = X 1 α + X 2 (1 − α) (with interpolation parameter α ∈ <ref type="bibr" coords="2,489.70,736.28,9.46,7.96">[0,</ref><ref type="bibr" coords="2,500.14,736.42,6.64,8.02" target="#b0"> 1]</ref>) allows to compute the variance σ 2 of X as because we assumed that X 1 and X 2 are independent which means cov(X 1 , X 2 ) = 0. Furthermore, it can be shown (by evaluating the integral over the joint probability density function) that the linear interpolation of two arbitrary, uncorrelated, Gaussian-distributed variables X 1 ∼ N (µ 1 , σ 2 1 ) and X 2 ∼ N (µ 2 , σ 2 2 ) yields another Gaussian-distributed random variable X with </p><formula>X ∼ N µ 1 α + µ 2 (1 − α), σ 2 1 α 2 + σ 2 2 (1 − α) 2 (2) </formula><p>We see that the mean of X is acquired by linear interpolation of µ 1 and µ 2 . For the variance, we can observe that it is the result of quadratic interpolation of σ 2 1 and σ 2 2 . This interpolation function has its local maxima at the points where X 1 and X 2 are given, i.e. the gridpoints in visualization terms. Consequently, the minimum variance lies somewhere in between the two points depending on σ 2 1 and σ 2 2 . An illustrative example for the interpolation of the probability density function is shown in 1(b). This theoretical result appears counter-intuitive, but it is a consequence of the linear interpolation of independent, Gaussian-distributed variables: the (weighted) average of two random variables is always more stable than each of those variables (in the sense of having less variance). We leave it as an easy task to the reader to extend this result to the linear interpolation of three Gaussian variables in a triangle, or four Gaussian variables in a tetrahedron. </p><p>So far, we have only considered uncorrelated Gaussian variables. Examining Eq. 1 yields that the covariance influences the behavior of the variance of the interpolated values. We can see that, depending on the values of σ 2 1 , σ 2 2 and cov(X 1 , X 2 ), the interpolation function behaves differently (see <ref type="figure" coords="3,118.85,393.43,20.30,8.02">Fig. 2</ref>): </p><formula>@BULLET If σ 2 1 + σ 2 2 &gt; 2 · cov(X 1 , X 2 )</formula><p>, we have the behavior described above (e.g. see <ref type="figure" coords="3,107.21,418.44,19.72,8.02">Fig. 2</ref>(a)). </p><formula>@BULLET If σ 2 1 + σ 2 2 = 2 · cov(X 1 , X 2 )</formula><p>, the quadratic interpolation of the variance reduces to linear interpolation (e.g. see <ref type="figure" coords="3,225.79,444.90,30.43,8.02">Fig. 2(b)</ref>). </p><formula>@BULLET If σ 2 1 + σ 2 2 &lt; 2 · cov(X 1 , X 2 )</formula><p>, the described behavior is negated, i.e. the variance has its maximum between X 1 and X 2 and its local minima at X 1 and X 2 (e.g. see <ref type="figure" coords="3,180.01,481.31,20.26,8.02">Fig. 2</ref>(c)). </p><p> Those findings can be expanded to the bilinear case in two dimensions , see <ref type="figure" coords="3,68.11,506.33,21.47,8.02" target="#fig_1">Fig. 3</ref>for example. We want to point out that this problem is not restricted to linear interpolation. Interpolation with cubic splines for example also suffer from those implications. Consider a Bezier curve given by </p><formula>Y = X 1 (1 − α) 3 + X 2 3α(1 − α) 2 + X 3 3α 2 (1 − α) + X 4 α 3 . </formula><formula>(3) </formula><p>Assuming that X 1 , X 2 , X 3 , X 4 are independent, the variance along this curve is given by </p><formula>var(Y ) = var(X 1 ) * (1 − α) 6 + var(X 2 ) * 9α 2 (1 − α) 4 + var(X 3 ) * 9α 4 (1 − α) 2 + var(X 4 ) * α 6 . </formula><formula>(4) </formula><p>Plotting this function (for 0 ≤ α ≤ 1), would reveal similar results: depending of the value of the variances of X 1 , . . . , X 4 , the interpolated variance has its (local) maxima at the positions of the given data points. </p><p>By our belief, traditional interpolation (regardless of the specific interpolation scheme) is in general not well suited to estimate uncertain values. A key element of interpolation is that the interpolation result equals the value at the data points. In the case that the value is uncertain, this property may be unwanted because the variance of that data point is disregarded. And the result of the variance might be that the best estimator for that data point is actually not the given value at that data point. The goal of this section was to point out that the traditional linear interpolation, which is widely used in grid based visualization, may yield very counter-intuitive and unwanted results when dealing with uncertain data. If the covariance between neighbouring points is weaker than the variance at these points, then the resulting interpolated data is most certain between the grid points (where we have no data) and the variance has its maxima at the grid points (where the data is given). If one encounters such data, it should be obvious by now that one needs other methods. Such methods are presented in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GAUSSIAN PROCESSES</head><p>As denoted earlier, we suggest to use an interpolation scheme based on suitable mathematical foundations without the described deficits of linear interpolation. It turns out that stochastic processes provide such a foundation. Because we focus on Gaussian-distributed data, we will concentrate on Gaussian processes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Interpolation Task</head><p>The visualization of normally-distributed scalar, vector, and tensor field data typically faces the following setting: There is a domain S which is a nice subset (stratified manifold for the mathematically inclined reader) of R 2 or R 3 . The data is given at a finite number of positions s 1 , . . . , s N ∈ S. At these positions, we are given normally-distributed random variables X 1 , . . . , X N as data. We denote by µ 1 , . . . , µ N the means and by σ 2 1 , . . . , σ 2 N the variances at each position . Basically, interpolation means to define normally distributed random variables at all points s ∈ S. In addition, properties of the interpolation may be demanded, like smoothness, limited variation between the positions, or minimal interpolation error. Posed differently, we are looking for a distribution of fields over the domain S where the values at each position s ∈ S have a Gaussian distribution that is in some (yet to define) sense related to the given distributions at the positions s 1 , . . . , s N . Gaussian processes are exactly the mathematical concept to provide all this <ref type="bibr" coords="3,392.66,525.54,9.71,8.02" target="#b1">[2,</ref><ref type="bibr" coords="3,405.28,525.54,10.65,8.02" target="#b19"> 20]</ref>. In the following, we assume that we are given a distribution of scalar values and are looking for a scalar Gaussian process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gaussian Processes</head><p>Actually, our linear interpolation of normally distributed values in the last section was already a Gaussian process. Definition: Multivariate Gaussian A random variable X = (X 1 , ..., X n ) ∈ R d is multivariate Gaussian if any linear combination of X i is univariate Gaussian, i.e. a T X = ∑ n i=1 a i X i ∼ N (µ, σ 2 ), ∀a∈R n and suitable µ, σ ∈ (R). Definition: Gaussian Process For any domain S, a Gaussian Process (GP) f on S is a set of random variables ( f t : t ∈ S), such that ∀n∈N and ∀t 1 , ...,t n ∈S, ( f t 1 , ..., f t n ) is a multivariate Gaussian distributed random variable. At first glance, this definition is directed more towards pure mathematics , but there is practical use to the concept. A first hint is given by the fact that one just needs to define a mean value at each point s ∈ S and a covariance between any two points s, s ∈ S to define a unique Gaussian process, (see <ref type="bibr" coords="3,377.92,736.42,25.14,8.02">[2, p.5]</ref>). If we have a nice set S, the positions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Covariance Function Defines Interpolation Scheme</head><p>In the previous subsection, we showed that interpolation schemes in visualization are actually Gaussian processes if applied to normally distributed data. Basically, the Lagrangian weight functions of the scheme determine the covariance function of the Gaussian process. The interesting fact is that one can also perform this construction in the opposite direction (<ref type="bibr" coords="4,354.61,73.31,44.60,8.02">[1, pp.17-19]</ref>). This means that we will start with a covariance function and derive a corresponding interpolation scheme. Of course, one can also use such a construction for approximation by assuming some (zero mean Gaussian) error of the data. We will do this in the next section and it should be no surprise as we assume that we have uncertainty in the data. From a visualization point of view, this might look useless at first, because it is not clear where the covariance function should come from. However, using covariance functions is actually the method of choice in other disciplines like geospatial statistics or machine learning. In geospatial statistics, the method is called Kriging. A classical presentation of this approach was published by Cressie <ref type="bibr" coords="4,314.23,182.90,9.52,8.02" target="#b3">[4]</ref> . In machine learning, the method is called Gaussian process regression <ref type="bibr" coords="4,341.26,192.86,13.74,8.02" target="#b19">[20]</ref>. Let us assume that we have a bilinear, symmetric, and positive definite function k : S × S → R. k is our covariance function. We want to define a set of basis functions that allow us to calculate weights from our data to derive a Gaussian process with this covariance function. For this purpose, we define the set of functions </p><formula>H = {g : S → R|g(s) = M ∑ i=1 a i k(s,t i ), M ∈ N,t i ∈ S, a i ∈ R} (12) </formula><p>It should be noted that we really allow any M ∈ N and any points t i ∈ S in this space. On this set, we define the scalar product </p><formula>&lt; f , g &gt; =&lt; N ∑ i=1 a i k(t,t i ), N ∑ j=1 b j k(t,t j ) &gt; = N ∑ i=1 N ∑ j=1 a i b j k(t i ,t j ) </formula><formula>(13) </formula><p>which has the so called reproducing kernel property </p><formula>&lt; f (·), k(t, ·) &gt;= f (t) (14) </formula><p> This construction is also called the reproducing kernel map construction (see <ref type="bibr" coords="4,320.32,441.03,13.44,8.02" target="#b19">[20]</ref>). Furthermore, we extend H to its completion under its scalar product which is the so-called reproducing kernel Hilbert space (RKHS). The Moore-Aronszain Theorem (<ref type="bibr" coords="4,463.67,460.95,9.86,8.02" target="#b2">[3]</ref> ) promises a oneto-one correspondence between k and the RKHS. We can now take any basis </p><formula>{φ i : S → R, i = 1, . . . , dim(H)} (15) </formula><p>to define our approximation scheme. However, for many covariance functions, dim(H) will be infinite, so we define as possible Gaussian processes </p><formula>g : S → R, g(s) = ∑ i∈I A i φ i (s) </formula><formula>(16) </formula><p> for a finite subset I ⊂ {1, . . . , dim(H)} and Gaussian-distributed random variables A i . Every g will have k as its covariance function, so we can set the φ i to fit our data according to some approximation condition . This will be shown in the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GAUSSIAN PROCESS REGRESSION 5.1 Regression in Gaussian Processes</head><p> Now we show how we can use the results of Section 4 to give an analytical description for the interpolation of the Gaussian-distributed variables X i ∼ N (µ i , σ 2 i ) at the gridpoints s i . The continuous uncertain tensorfield will be conceived as a Gaussian process. Information on the distribution of the X i can be derived empirical, for example if we have a high number of realizations of a random experiment or a high number of simulations. In some cases we may even be able to derive the Gaussian distributions directly. We assume the standard linear model for the data: </p><formula>f (s) = y(s) + ε(s), </formula><formula>(17) </formula><p>where f (s) is the observed value at an arbitrary position s, y(s) is the true (but unknown) value at s and ε(s) is the Gaussian-distributed error with zero mean and variance σ 2 (s). We assume that the error and the true value are uncorrelated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Covariance Function</head><p>In Section 4, we learned that the key for the interpolation is the choice of a covariance function k(s, s ). Unfortunately an exact formulation for the covariance of the Gaussian process exists only in very rare cases. Therefore, the covariance is often modeled by a specific covariance function. Which function fits best for the data varies from case to case. If not denoted otherwise, we will use the squared exponential as our covariance function throughout this paper: k(s, s ) = exp(− 1 2 |s − s | 2 ). </p><formula>(18) </formula><p>It is interesting to see, that by using the squared exponential, the covariance does not depend on the data itself but only on the positions . We can even compute a covariance between two arbitrary points whether we have data about them or not. The use of the squared exponential is very common throughout the Gaussian process literature, see [14, p.14] for explanation. Pfaffelmoser et al. <ref type="bibr" coords="5,215.62,273.33,14.94,8.02" target="#b16">[17] </ref>also chose an exponential function for modeling correlation. As we see, the covariance function models spatial correlation between the data points. It turns out that this is the case with most covariance functions <ref type="bibr" coords="5,249.55,303.21,13.74,8.02" target="#b19">[20]</ref>. In many applications the covariance function is adjusted with extra parameters , that permit more flexibility when modeling the data. These parameters are called hyperparameters. The squared exponential, for instance, can have the following form </p><formula>k(s, s ) = σ 2 p exp(− 1 2 l 2 |s − s | 2 ), (σ 2 p , l &gt; 0) (19) </formula><p>The parameters are σ 2 p – the signal variance and l – the length scale. We see that the overall variance can be controlled by σ 2 p as it is a positive prefactor. The length scale determines the width of the squared exponential, i.e. how far away one point has to be from another point, so that their influence on each other is negligible. As the squared exponential has the shape of a Gaussian bell curve, l is similar to the variance in a Gaussian density function. Therefore, the squared exponential is also referred to as Gaussian kernel. To derive suitable hyperparameters is a topic on its own. The ideal case would be, that we can derive the hyperparameters directly from a given model or theory (just like the covariance function). But this is often not the case. So the hyperparameters have to be estimated from the data. A natural choice for the parameter σ 2 p is the maximum variance of our input data. A common method to evaluate the length scale is to try to maximize the log-marginal-likelihood of the Gaussian process for various values of l (see <ref type="bibr" coords="5,163.27,544.30,38.62,8.02">[20, p.123]</ref>). Roughly speaking, we use different values for l and compute the likelihood of the data to actually show up with that particular value for l. At last we take the value for l that produced the highest likelihood. </p><p>When doing regression in Gaussian processes, it is important to specify a prior distribution. The prior gives us information at points in the field, where we have no data. If we do not have specific information about the prior, we can use heuristics. The prior mean is taken to be zero. This is the so-called simple Kriging assumption. We can model a zero mean process on our data by subtracting the empirical mean from the input data and add it back after processing, if necessary . For Kriging, it is important to have a constant mean throughout the input domain. An obvious (and widely used) heuristic for the prior variance is the maximum empirical variance found in the dataset. It is loosely speaking a worst case approximation of the variance at a position without any data. And the worst case which we can anticipate, given the data, is the maximum variance in the data. Now we can see that the parameter σ 2 p in the squared exponential is in fact the prior variance, because k(s, s) = σ 2 p for arbritrary s in our domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">The Covariance Matrix</head><p> Now that we have a covariance function, we can compute the covariance matrix K: </p><formula>K =   k(s 1 , s 1 ) . . . k(s 1 , s N ) . . . . . . . . . k(s N , s 1 ) . . . k(s N , s N )   , (20) </formula><p>where k(s i , s j )(i, j = 1, ..., N) is the covariance between the i-th and j-th datapoint and can be evaluated by using the covariance function. It is important to note that for every diagonal entry, the variance has to be added to the covariance. So we get </p><formula>K i j = k(s i , s j ) + σ 2 i , i = j k(s i , s j ), i = j . </formula><formula>(21) </formula><p>That is because (see eq. 17) </p><formula>Cov f (s), f (s ) = Cov y(s) + ε(s), y(s ) + ε(s ) = Cov y(s), y(s ) +Cov y(s), ε(s ) +Cov ε(s), y(s ) +Cov ε(s), ε(s ) </formula><formula>(22) </formula><p>The error ε and the true value y are uncorrelated, so we get </p><formula>Cov f (s), f (s ) = Cov y(s), y(s ) + 0 + 0 +Cov ε(s), ε(s ) = k(s, s ) +Cov ε(s), ε(s ) = k(s, s ) + σ 2 (s), s = s k(s, s ), s = s (23) </formula><p>In the next section, it can be noticed that adding the variances to the diagonal of the covariance matrix in fact incorporates the uncertainty in the interpolation scheme and -loosely speaking -turns the interpolation into a regression. We want to mention that it is possible not to construct the covariance using the covariance function but using the empirical covariances. Despite the implications on the model that is assumed for the uncertain data it might be the case that this matrix cannot be inverted. In the next section, we will see that we need to invert the covariance matrix in order to compute the interpolation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Gaussian Process Regression</head><p>The interpolation of the observed values will be defined as (see eq. 16) </p><formula>f (s) = N ∑ i=1 f i φ i (s), </formula><formula>(24) </formula><p>where f i are the observed values at the datapoints s i . When using Gaussian Process regression, the weight functions φ i (s) will be chosen in a way that the estimated value f (s) minimizes the variance of the interpolation error. Before we can minimize that variance , we will show how it can be computed. X(s) is the real Gaussiandistributed value at s and X * (s) denotes the result of the interpolation. The error is therefore given by X * (s) − X(s) and its variance by: </p><formula>σ 2 (s) = var (X * (s) − X(s)) = E (X * (s) − X(s)) 2 − E [X * (s) − X(s)] 2 (25) </formula><p>Since we assume a zero mean Gaussian process, we get </p><formula>E [X * (s) − X(s)] = E[X * (s)] − E[X(s)] = 0 (26) </formula><p>Therefore, we get for the variance: </p><formula>σ 2 (s) = E (X * (s) − X(s)) 2 = E[X * (s)X * (s)] − 2E[X * (s)X(s)] + E[X(s)X(s)] </formula><p>(27) </p><p>2309 SCHLEGEL ET AL: ON THE INTERPOLATION OF DATA WITH NORMALLY DISTRIBUTED UNCERTAINTY FOR VISUALIZATION We can evaluate this formula, by using the fact that E</p><formula>[X(s)X(s )] = Cov (X(s), X(s )) = k(s, s </formula><p> ) (because of the zero mean Gaussian process ) and by using equation 11: </p><formula>σ 2 (s) = N ∑ i, j=1 φ i (s)φ j (s)k(s i , s j ) − 2 N ∑ i=1 φ i (s)k(s i , s) + k(s, s) (28) </formula><p>To minimize the variance, we have to take the the derivative of eq. 28 with respect to the weight functions and set it to zero: </p><formula>N ∑ j=1 φ j (s)k(s i , s j ) − k(s i , s) = 0, ∀i = 1 . . . N ⇔ N ∑ j=1 φ j (s)k(s i , s j ) = k(s i , s), ∀i = 1 . . . N (29) </formula><p>This equation induces a linear system (involving the covariance matrix K), which can be solved to compute the optimal weights φ i (s): </p><formula>  k(s 1 , s 1 ) . . . k(s 1 , s N ) . . . . . . . . . k(s N , s 1 ) . . . k(s N , s N )   *   φ 1 (s) . . . φ N (s)   =   k(s 1 , s) . . . k(s N , s)   (30) </formula><p>Now that we have the optimal weight functions, we can compute f (s) (see eq. 24). The variance can be computed by inserting eq. 29 into eq. 28: </p><formula>σ 2 (s) = k(s, s) − N ∑ i=1 N ∑ j=1 φ i (s)φ j (s)k(s i , s j ) = k(s, s) − N ∑ i=1 φ i (s)k(s i , s), </formula><formula>(31) </formula><p>which can be evaluated, if the optimal weights were computed using eq. 30. We want to remind that k(s, s) is the prior variance, which is σ 2 p if using the squared exponential. Evaluating this formula is very inefficient, because for every s, a linear system of size N × N has to be solved. For that reason it it common to invert the covariance matrix once (it does not change for varying s) and compute the basis functions as </p><formula>φ i (s) = N ∑ j=1 (K i j ) −1 k(s j , s), </formula><formula>(32) </formula><p>where (K i j ) −1 are the elements of the inverted covariance matrix. To summarize, the outcome of the Gaussian process at s is a Gaussian-distributed variable </p><formula>X * (s) ∼ N N ∑ i=1 φ i (s) f i , k(s, s) − N ∑ i=1 φ i (s)k(s i , s) . </formula><formula>(33) </formula><p>The basis functions φ i (s) are computed as in eq. 32. As a side note, we want to mention that eq. 31 is the so-called Kriging variance. There is a discussion in the Kriging community regarding the interpretation of the kriging variance (e.g. see <ref type="bibr" coords="6,244.00,586.56,14.19,8.02" target="#b14">[15,</ref><ref type="bibr" coords="6,260.93,586.56,11.95,8.02" target="#b23"> 25] </ref>and references therein) and also papers discussing alternatives for the variance of the interpolated Gaussian process (e.g. see <ref type="bibr" coords="6,219.52,606.49,13.44,8.02">[24]</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Model Flexibility</head><p>A big concern on the regression in Gaussian processes might be that we need to have a constant mean and one covariance function throughout the input domain. But these restrictions can be overcome by considering the uncertain field not as one Gaussian process but as many Gaussian processes with smaller input domains (they also may overlap ). To accomplish this, we only have to figure out which of the neighboring data points s i are to be considered for the calculation of the distribution at s. For example, let the squared exponential be the covariance function. Depending on the parameter l, positions that have a large Euclidian distance have a covariance that is almost zero. So they have a negligible influence on the interpolation result. That means these points can be left out. Since the length scale l is similar to the variance in a Gaussian density function, the data points within a 3l neighborhood contribute more than 99% to the interpolation result. So we can leave out data points lying outside of that range without loosing too much precision. We see that it may be sufficient to consider only n &lt; N data points in order to compute the interpolation result. There are two main advantages using this approach. Firstly, we can assign each Gaussian process its own mean and its own covariance function. That enables us to work with random fields where the spatial correlation cannot be described by one covariance function. This also applies, if we have to use different hyperparameters for the squared exponential, for instance. Secondly, there is an advantage regarding the computational costs. For every random field, we have to invert the covariance matrix, which is of size N × N. Using a decomposition in many smaller processes leads to the inversion of many smaller covariance matrices. That in general is much faster to compute and moreover can be done in parallel. Furthermore, we can assume that within a grid cell, the Gaussian process does not change significantly. That means, that the data points that contribute (significantly) to the interpolation result do not vary within the cell. Thus we can compute one covariance matrix for each cell (and not for each interpolation point within the cell). So for each cell, we can precompute an inverted covariance matrix that enables us to compute the interpolation result very fast compared to the inversion of a N × N matrix. Especially multi-core computers will benefit from this approach, since each cell can be treated in parallel. Regarding the varying length scale, we want to note that Gibbs <ref type="bibr" coords="6,525.04,342.97,10.45,8.02" target="#b5">[6] </ref>proposed an exponential covariance function, where the length scale is in fact a function of the position s. That shows it is also possible to vary the length scale within the uncertain field by using a suitable covariance function. Consider the quad cell given in <ref type="figure" coords="6,407.83,516.91,20.29,8.02" target="#fig_2">Fig. 4</ref>. The distributions on the grid points are the Gaussian distributed values </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Example</head><formula>X 1 , X 2 , X 3 , X 4 with f 1 = 1, f 2 = 1, f 3 = −1, f 4 = 0 and σ 2 i = 1, i = 1</formula><p>, . . . , 4. The quad cell is a square with a side length of 1. This is important to know, if we want to adjust our covariance function. Recall, that the covariance function used (the squared exponential k) is a function of the distance. The mean of the GP is zero. The hyperparameter σ 2 p will be the maximum variance, which is 1. As a start, we will set the other parameter l to 0.2, which indicates a very short covariance function and results in very small covariance between the 4 given points. To admit that the observations at the grid points are uncertain, we have to add the variance to the covariance matrix, i.e. cov(s i , s j ) = k(s i , s j ) + σ 2 i δ i j . In <ref type="figure" coords="6,305.44,636.80,22.26,8.02">Fig. 5</ref>(a), we can see the result of the interpolation. There are two important observations. At first, we can see that the values at s i are not the observed values f i , because they are assumed to be an uncertain observation. Loosely speaking, they tend towards the prior mean, which is zero. And the " degree of tendency " is determined by the variance. That means the higher the variance is, the more the f i tend towards zero. We can conclude that the best estimator for f (s i ) is usually not f i . Another observation is, that the samples tend towards the prior mean in the middle of the cell. This is due to the short covariance function. The given data points barely influence each other, so that they tend towards the prior mean very fast. The variance tends <ref type="figure" coords="7,31.50,237.42,19.36,7.37">Fig. 5</ref>. Interpolation in a sample quad cell using squared exponential kernel with (a) l = 0.2 and σ 2 p = 1, (b) l = 0.2 and σ 2 p = 1 but certain input (σ 2 i = 0, i = 1, . . . , 4), and (c) l = 0.7 and σ 2 p = 1. The upper row depicts interpolated values f (s), the lower row depicts the corresponding variance σ 2 (s). The black isolines in the upper pictures enclose the area that is numerical evaluated to 0. towards the prior variance accordingly. In <ref type="figure" coords="7,187.29,291.05,21.82,8.02">Fig. 5</ref> (b), we set the variance of the inputs to zero. This implies, that the 4 observations f i of the Gaussian process are certain. But to admit that the interpolated values are uncertain, we set the prior variance to 1. This is of course a rather theoretical assumption to have certain input and the rest of the field is uncertain. But it serves the purpose to show that the interpolation result behaves as we would expect it. Comparing with <ref type="figure" coords="7,247.37,350.83,21.17,8.02">Fig. 5</ref>(a), we can see that the values at the data points are not changed anymore, because they are assumed to be certain. In this case the best estimator for f (s i ) is indeed f i . The grid points in <ref type="figure" coords="7,175.69,380.71,20.55,8.02">Fig. 5</ref>(c) have uncertain input values, like in <ref type="figure" coords="7,85.83,390.68,22.48,8.02">Fig. 5</ref>(a). But the length scale l was changed to 0.7. For the samples, we can observe that the area which is evaluated to zero became thinner. That is because the given data at the data points gain more influence on the interpolated values within the cell, because of the broader covariance function. As the values at the data points f i are not all equal to zero, so are the interpolated values within the cell. For the corresponding variance, we can see that it is almost constant within the cell. where p = (−1, 0, 0), q = (1, 0, 0). The value of this field is zero near the points p and q and increases with distance. The isosurface for the value f = 1 (shown in <ref type="figure" coords="7,386.80,291.05,30.80,8.02">Fig. 6(a)</ref>) is particularly interesting, because at this value two separate isosurfaces surrounding p and q respectively connect at (0, 0, 0). This is a difficult, but important situation. To put the interpolation capabilities of Gaussian process regression to the test, we applied it to a very-low-resolution (12 × 12 × 12) version of the field as shown in <ref type="figure" coords="7,373.28,340.86,22.47,8.02">Fig. 6</ref>(b). Adding to the difficulty of the task, the critical point at (0, 0, 0) lies in the center between 4 grid points, so the low-resolution field contains no information about this point. The variance of the data was defined to be 0.1 on the whole field. For the Gaussian process, the prior variance was set to be 0.2 and for the length scale, a value of 0.3 was chosen, which corresponds to the cell width of the grid. <ref type="figure" coords="7,304.08,411.33,24.56,8.02" target="#fig_5">Fig. 7</ref>(b) shows the surface where the posterior mean produced by the Gaussian process, i.e. the expected value equals one. It very closely resembles the original field (<ref type="figure" coords="7,425.73,431.26,20.35,8.02">Fig. 6</ref>(a)). It is smooth but at the same time reconstructed the critical point at (0, 0, 0) pretty well, even though the low-resolution field did not even include a sample at that position. The trilinear interpolation (<ref type="figure" coords="7,427.15,461.15,20.42,8.02" target="#fig_5">Fig. 7</ref>(e)) on the other hand fails to reconstruct the field near the critical point properly and is much less smooth. The Gaussian process regression gives us a mean and a variance for every point in the field. We chose to compute and visualize the probability for the field to fall below the interesting value 1: P( f &lt; 1). The results can be seen in <ref type="figure" coords="7,370.46,521.66,21.32,8.02" target="#fig_5">Fig. 7</ref>for different probabilities. Enclosed by the isosurfaces are the areas where the probability P( f &lt; 1) is greater than the given thresholds 95%, 50% and 5% (from left to right). The results based on Gaussian processes (top row of <ref type="figure" coords="7,439.39,551.54,20.59,8.02" target="#fig_5">Fig. 7</ref>) are consistently closer to reality than those based on trilinear interpolation (bottom row). In particular, the latter suffers from heavy artifacts as described in Section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Climate Dataset</head><p>We also want to show the results of the interpolation on simulations of the Intergovernmental Panel of Climate Change carried out at DKRZ by the Max-Planck-Institute for Meteorology. In particular, we have used the IPCC A1B scenario results <ref type="bibr" coords="7,425.83,646.76,9.52,8.02" target="#b7">[8]</ref>. We used the 2m temperature dataset. The data is placed on a uniform 192 × 96 grid. The side length of a cell is 1. Each gridpoint has a time series of the simulated monthly means of the years 1860-2100. We constructed a Gaussian distribution on each gridpoint by eliminating the annual cycle and removing the linear trend of each time series. We checked the result with the Kolmogorow- Smirnow-Test. The prior mean was set to the empirical mean of all of these time series and for the prior variance, we chose the maximum variance in the dataset. The covariance function is again the squared </p><formula>) P( f &lt; 1) = 95%, (b) and (e) P( f &lt; 1) = 50%, (c) and (f) P( f &lt; 1) = 5%. </formula><p>For example, the balls in 7(a) contain all the points that have more than 95% probability to have a value smaller than 1. exponential. We interpolate the empirical means and the variances at the gridpoints for a part of the northern american continent. The results are shown in <ref type="figure" coords="8,72.03,344.27,25.40,8.02">Fig. 8.</ref><ref type="figure" coords="8,101.91,344.27,22.29,8.02">Fig. 8</ref>(a) depicts the linear interpolation of the means. The according interpolation result for the variance is displayed in <ref type="figure" coords="8,32.44,364.20,22.59,8.02">Fig. 8</ref>(d). The other images show the regression of the Gaussian process with differing values for l. We can see, that the variance of the linear interpolation (<ref type="figure" coords="8,109.95,384.12,20.21,8.02">Fig. 8</ref>(d)</p><p>) is highest at the grid points. In <ref type="figure" coords="8,258.68,384.12,14.20,8.02;8,22.50,394.09,3.34,8.02">Fig.  8</ref>(e), we have a short covariance function (l = 0.7) and the variance is highest within the cells. Also, smoothing effects can be seen when using a longer covariance function (<ref type="figure" coords="8,151.83,414.01,20.14,8.02">Fig. 8</ref>(c) and 8(f)). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The Level-Crossing Probability</head><p>The level-crossing probability (LCP) field was introduced by Pöthkow et al. (<ref type="bibr" coords="8,49.39,459.50,13.44,8.02" target="#b17">[18]</ref>, <ref type="bibr" coords="8,69.48,459.50,14.34,8.02" target="#b18">[19]</ref>) and shows the probability that a certain level γ is crossed between two sample points. It is used to depict the positional uncertainty of an isocontour. By visualizing this field using volume rendering, one can identify regions that have a higher or lower probability to be intersected by the uncertain isocontour. We used the low-sampled (12 × 12 × 12) synthetic data set from the previous subsection (with the same setup) and calculated the LCP field for γ = 1 based on mean and variance values from Gaussian process regression. We used three different values for the length scale to investigate its influence on the LCP. <ref type="figure" coords="8,135.01,549.91,21.58,8.02">Fig. 9</ref>shows direct volume renderings of the LCP fields and the corresponding isosurface, i.e. the surface where the mean equals γ = 1. We succeeded in recovering the smooth underlying structure of the original data. It can be seen, that the LCP values become smaller as the length scale increases. This is because of the increased covariance between the points, which decreases the variances and thus the positional uncertainty of the isocontour. This behavior is consistent with the findings of <ref type="bibr" coords="8,76.55,630.35,13.74,8.02" target="#b18">[19]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUMMARY AND CONCLUSIONS</head><p> We showed that the traditional linear interpolation yields counterintuitive results when dealing with uncertain data. To overcome these problems, we consider the uncertain tensorfield to be a Gaussian process which is defined by a covariance function. That allows to apply methods widely used especially in geostatistics and machine learning. We showed how to derive an interpolation scheme using Gaussian process regression to describe the continuous uncertain field. This enables us to interpolate Gaussian-distributed variables. We showed that the <ref type="figure" coords="8,285.12,705.19,19.79,7.37">Fig. 9</ref>. LCP fields using squared exponential as covariance function: (a) with l = 0.3 (b) with l = 0.6, and (c) with l = 0.9. LCP is depicted by direct volume rendering, additionally the mean surface for γ = 1 is shown in blue (transparent). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,33.49,699.32,244.40,6.86;1,31.50,708.78,144.33,6.86"><head></head><figDesc>Manuscript received 31 March 2012; accepted 1 August 2012; posted online 14 October 2012; mailed on 5 October 2012. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,31.50,612.79,250.38,7.37;3,31.50,622.25,230.68,7.37"><head>Fig. 3. </head><figDesc> Fig. 3. color coded variance as the result of linear interpolation of uncorrelated Gaussian distributed variables given on the grid points. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,285.12,494.85,120.38,7.37"><head>Fig. 4. </head><figDesc>Fig. 4. Interpolation in a quad cell. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,22.50,274.23,513.00,7.37;8,22.50,283.69,513.00,7.40;8,22.50,293.16,415.32,7.40"><head>Fig. 7. </head><figDesc>Fig. 7. Results of Gaussian Process regression (top row) compared to trilinear interpolation (bottom row). The images show the probability that the value of the field is below 1, i.e. P( f &lt; 1) for different isovalues: (a) and (d) P( f &lt; 1) = 95%, (b) and (e) P( f &lt; 1) = 50%, (c) and (f) P( f &lt; 1) = 5%. For example, the balls in 7(a) contain all the points that have more than 95% probability to have a value smaller than 1. </figDesc></figure>

			<note place="foot">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 18, NO. 12, DECEMBER 2012 (a) (b) (c) (d) (e) (f) Fig. 8. Interpolation of a climate dataset: (a) and (d) linear interpolation with l = 0.7, (b) and (e) Gaussian process regression with l = 0.7, (c) and (f) Gaussian process regression with l = 1.5. The upper row depicts the means of the GP µ(s), the lower row depicts the corresponding variances σ 2 (s). implications from linear interpolation do not arise anymore, especially if the Gaussian variables are weakly correlated. An important property of the Gaussian process regression is, that the basis functions of the interpolation are independent of the data itself – as it is the case for the standard interpolation schemes. They depend on the chosen covariance function. So choosing the covariance function is similar to chosing an interpolation scheme. For the nonrandom case this equals the question, if for example one should use linear interpolation or B-splines on the data. And by providing the analytical description of the basis functions, we are able to implement Gaussian process regression independently of the given tensor field (e.g. for the use in visualization tools). These conclusions and the description of the basis functions were -to our knowledge -missing in the Kriging literature. In contrast to traditional interpolation methods, Gaussian processes do not merely create an isostructure that runs through the data points, but respects the uncertainty in them. This way, noise, errors or outliers in the data do not disturb the model inappropriately. Most importantly, the model shows the variance in the interpolated values, which can be higher but also lower than that of its neighbouring data points. That provides us with a lot more insight into the quality of our data and how it influences our uncertainty. In Section 6.1, we showed that Gaussian process regression is able reconstruct a low sampled grid and also to find a critical point within a grid cell. Furthermore, Gaussian processes work without a grid. The data can be a point cloud without any topology or neighbouring information whatsoever as long as there is a meaningful covariance function. The biggest concerns (especially in the visualization community) about Gaussian process regression might be its complexity and its choice of parameters, i.e the covariance function. Nevertheless, we believe that the GIS community and the machine learning community provide us with reasonable heuristics in order to find suitable covariance functions: MacKay and Gibbs for example [14, 6] give good reasons why the covariance structure in a Gaussian process can be estimated by an exponential function. Heuristics and algorithms for the estimation of hyperparameters are mentioned in Section 5.1.1. The GIS community often uses the variogram to fit a correlation function to the data. A more detailed research on the free parameters – especially for applications in visualization – is beyond the scope of this article and will be part of our future work. One of the problems, that also are to address in the future, is to extend the general interpolation scheme to arbitrary stochastic processes in order to apply the interpolation scheme to other models that exhibit non-gaussian noise. ACKNOWLEDGMENTS We want to thank Michael Böttinger who is with the DKRZ for providing the climate data set.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,312.38,462.63,232.12,7.13;9,312.38,470.45,24.81,8.58;9,332.96,470.45,30.92,8.58;9,359.63,472.09,184.86,7.13;9,312.38,481.55,122.99,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Topological complexity of smooth random functions: ´ Ecole d&apos; ´ Eté de Probabilités de Saint-Flour XXXIX -2009</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Adler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Taylor</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Mathematics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,491.02,232.12,7.13;9,312.38,500.48,165.06,7.13"  xml:id="b1">
	<monogr>
		<title level="m" type="main">Random Fields and Geometry</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Adler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Taylor</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007-06" />
			<publisher>Springer Monographs in Mathematics Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,509.95,232.12,7.13;9,312.38,519.41,118.87,7.13"  xml:id="b2">
	<monogr>
		<title level="m" type="main">Theory of reproducing kernels. Transactions of the</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Aronszajn</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,528.88,232.12,7.13;9,312.38,538.34,49.81,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">The origins of kriging</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cressie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Geology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="252" />
			<date type="published" when="1990-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,547.81,232.12,7.13;9,312.38,557.27,232.12,7.13;9,312.38,566.74,49.81,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Visualizing scalar volumetric data with uncertainty</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Djurcilov</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Lermusiaux</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Pang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="248" />
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,576.20,232.12,7.13;9,312.38,585.66,159.18,7.13"  xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient implementation of gaussian processes</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">N</forename>
				<surname>Gibbs</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J. C</forename>
				<surname>Mackay</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
	<note>submitted. to Statistics and Computing</note>
</biblStruct>

<biblStruct coords="9,312.38,595.13,232.12,7.13;9,312.38,604.59,232.12,7.13;9,312.38,614.06,222.92,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Flow radar glyphs; static visualization of unsteady flow with uncertainty. Visualization and Computer Graphics</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hlawatsch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Leube</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Nowak</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1949" to="1958" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,623.52,232.12,7.13;9,312.38,633.07,232.12,6.86;9,312.38,642.45,232.12,7.13;9,312.38,651.92,56.01,7.13"  xml:id="b7">
	<monogr>
		<title level="m" type="main">Climate Change 2007 -The Physical Science Basis: Working Group I Contribution to the Fourth Assessment Report of the IPCC</title>
		<author>
			<persName>
				<surname>Intergovernmental</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007-09" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK and New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,661.38,232.12,7.13;9,312.38,670.85,232.12,7.13;9,312.38,680.31,49.37,7.13"  xml:id="b8">
	<monogr>
		<title level="m" type="main">Statistically quantitative volume visualization</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">M</forename>
				<surname>Kniss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Van Uitert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Stephens</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">S</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tasdizen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,689.77,232.12,7.13;9,312.38,699.24,232.12,7.13;9,312.38,708.70,171.57,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">A statistical approach to some basic mine valuation problems on the witwatersrand</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">G</forename>
				<surname>Krige</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Chemical Metallurgical and Mining Society of South Africa</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,718.17,232.12,7.13;9,312.38,727.63,232.12,7.13;9,312.38,737.10,149.35,7.13;9,532.75,26.46,16.00,7.26"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing large-scale uncertainty in astrophysical data. Visualization and Computer Graphics</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C.-W</forename>
				<surname>Fu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Hanson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1640" to="1647" />
			<date type="published" when="2007-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,30.75,26.18,469.75,7.50;10,22.50,54.06,250.38,7.13;10,40.76,63.60,232.12,6.86;10,40.76,72.99,69.95,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Accuracy in 3d particle tracing</title>
		<author>
			<persName>
				<forename type="first">Schlegel</forename>
				<surname>Et</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Al</forename>
				<surname>On</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Interpolation</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Data</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Normally</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Uncertainty</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="middle">A</forename>
				<surname>Visualization12</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Lopes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Brodlie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Visualization</title>
		<editor>Hans- Christian Hege and Konrad Poltheir</editor>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="329" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,82.45,232.12,7.13;10,40.76,91.92,232.12,7.13;10,40.76,101.38,17.93,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Outside-in, inside-out: Two methods of generating spatial certainty maps</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">E</forename>
				<surname>Lowell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Casault</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">University of Otago</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="26" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,110.85,232.12,7.13;10,40.76,120.31,232.12,7.13;10,40.76,129.78,77.38,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Introduction to Gaussian processes</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J. C</forename>
				<surname>Mackay</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks and Machine Learning, NATO ASI Series</title>
		<editor>C. M. Bishop</editor>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="133" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,139.24,232.12,7.13;10,40.76,148.70,232.12,7.13;10,40.76,158.17,232.12,7.13;10,40.76,167.63,182.00,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparison between kriging variance and interpolation variance as uncertainty measurements in the capanema iron mine, state of minas geraisbrazil</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Monteiro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Da Rocha</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Yamamoto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Resources Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="223" to="2351010195701968" />
			<date type="published" when="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,177.10,232.12,7.13;10,40.76,186.56,79.82,7.13"  xml:id="b15">
	<monogr>
		<title level="m" type="main">Uncertainty and variability in point cloud surface data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Pauly</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Mitra</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Guibas</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,196.03,232.12,7.13;10,40.76,205.49,232.12,7.13;10,40.76,214.96,232.12,7.13;10,40.76,224.42,45.30,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualizing the positional and geometrical variability of isosurfaces in uncertain scalar fields</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Pfaffelmoser</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Reitinger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Westermann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="951" to="960" />
			<date type="published" when="2011" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,233.88,232.12,7.13;10,40.76,243.35,232.12,7.13;10,40.76,252.81,190.28,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Positional uncertainty of isocontours: Condition analysis and probabilistic measures</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Pöthkow</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-C</forename>
				<surname>Hege</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1393" to="1406" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,262.28,232.12,7.13;10,40.76,271.74,163.53,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic marching cubes</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Pöthkow</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Weber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-C</forename>
				<surname>Hege</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="931" to="940" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,281.21,232.12,7.13;10,40.76,290.67,89.43,7.13"  xml:id="b19">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">E</forename>
				<surname>Rasmussen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Williams</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,300.14,232.12,7.13;10,40.76,309.60,232.12,7.13;10,40.76,319.07,209.86,7.13"  xml:id="b20">
	<monogr>
		<title level="m" type="main">Mathematical foundations of uncertain field visualization. submitted to Dagstuhl Follow Ups, Workshop on Scientific Visualization</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Scheuermann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hlawitschka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Garth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hagen</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,328.53,232.12,7.13;10,40.76,337.99,232.12,7.13;10,40.76,347.46,117.04,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing knowledge about virtual reconstructions of ancient architecture</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Strothotte</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Masuch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Isenberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics International Conference</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,356.92,232.12,7.13;10,40.76,366.39,232.12,7.13;10,40.76,375.85,232.12,7.13;10,40.76,385.32,49.81,7.13;10,22.50,394.78,250.38,7.13;10,40.76,404.25,232.12,7.13;10,40.76,413.71,89.89,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Glyphs for visualizing uncertainty in environmental vector fields An alternative measure of the reliability of ordinary kriging estimates</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wittenbrink</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Saxon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Furman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Pang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lodha</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics Mathematical Geology</title>
		<imprint>
			<biblScope unit="volume">2410</biblScope>
			<biblScope unit="issue">32 10</biblScope>
			<biblScope unit="page" from="266" to="279489" />
			<date type="published" when="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,423.17,232.12,7.13;10,40.76,432.64,232.12,7.13;10,40.76,442.10,232.12,7.13;10,40.76,451.57,78.80,7.13"  xml:id="b23">
	<analytic>
		<title level="a" type="main">A study on the effects of parameter estimation on kriging model&apos;s prediction error in stochastic simulations</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">H</forename>
				<surname>Ng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">M</forename>
				<surname>Ng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Simulation Conference, WSC &apos;09</title>
		<imprint>
			<publisher>Winter Simulation Conference</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="674" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,461.03,232.12,7.13;10,40.76,470.50,232.12,7.13;10,40.76,479.96,81.25,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualization of gridded scalar data with uncertainty in geosciences</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Zehner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Watanabe</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Kolditz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Geosciences</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1268" to="1275" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
