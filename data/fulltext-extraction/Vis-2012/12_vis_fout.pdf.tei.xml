<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T15:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Adaptive Prediction-Based Approach to Lossless Compression of Floating-Point Volume Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Nathaniel</forename>
								<surname>Fout</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Kwan-Liu</forename>
								<surname>Ma</surname>
								<roleName>Fellow, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">An Adaptive Prediction-Based Approach to Lossless Compression of Floating-Point Volume Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Volume compression</term>
					<term>lossless compression</term>
					<term>floating-point compression</term>
				</keywords>
			</textClass>
			<abstract>
				<p>—In this work, we address the problem of lossless compression of scientific and medical floating-point volume data. We propose two prediction-based compression methods that share a common framework, which consists of a switched prediction scheme wherein the best predictor out of a preset group of linear predictors is selected. Such a scheme is able to adapt to different datasets as well as to varying statistics within the data. The first method, called APE (Adaptive Polynomial Encoder), uses a family of structured interpolating polynomials for prediction, while the second method, which we refer to as ACE (Adaptive Combined Encoder), combines predictors from previous work with the polynomial predictors to yield a more flexible, powerful encoder that is able to effectively decorrelate a wide range of data. In addition, in order to facilitate efficient visualization of compressed data, our scheme provides an option to partition floating-point values in such a way as to provide a progressive representation. We compare our two compressors to existing state-of-the-art lossless floating-point compressors for scientific data, with our data suite including both computer simulations and observational measurements. The results demonstrate that our polynomial predictor, APE, is comparable to previous approaches in terms of speed but achieves better compression rates on average. ACE, our combined predictor, while somewhat slower, is able to achieve the best compression rate on all datasets, with significantly better rates on most of the datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p> In the scientific visualization community, floating-point data resulting from observational measurements or computer simulations is being generated at an ever-increasing rate. In particular, large volumetric datasets with sizes on the order of terabytes or even petabytes are now commonplace, and such datasets consume massive resources in terms of both bandwidth and storage. Such datasets are typically highdimensional , being high-precision, high-resolution, multi-variate, and time-varying with thousands of time steps. Consider a 1000 time-step data output from a state-of-the-art direct numerical simulation, where each time step is a 1024 3 volume consisting of five variables stored in floating-point format. In this case, each time step consumes 20 gigabytes, with the entire dataset requiring 20 terabytes. Difficulties related to dataset size inundate the entire data processing pipeline, including generation, transfer, storage, and analysis. Analysis often consists of volume visualization, but most real-time volume visualization is accomplished using graphics hardware, which has limited memory. Even main memory is insufficient for many datasets, with one time step often exceeding the available RAM on current machines. Similarly, in the medical imaging community massive amounts of volumetric data are produced by CT, MRI, PET and other scanning modalities. There may be multiple studies for each patient, with many thousands of patients in a typical large hospital. The resolution of the scanners continues to improve, as well as the precision of the measurements . This results in many petabytes of data, which must be acquired and archived indefinitely. Furthermore, this data must be immediately available for evaluation by health professionals; such evaluation typically consists of slice browsing, although 3D volume visualization is becoming more popular in certain applications. Thus in medical imaging, archiving of such massive amounts of data and fast transfer to local workstations are critical components of this data management context. An effective solution to such large data management problems is to reduce the size of the data using compression techniques. However , most data compression methods target integer data of limited @BULLET Nathaniel Fout is with UC Davis, e-mail: natefout@gmail.com. @BULLET Kwan-Liu Ma is with UC Davis, e-mail: klma@ucdavis.edu. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. dynamic range, with only a few works focusing on floating-point formats . Floating-point compression is more challenging than integer compression, as the least significant bits of the mantissa tend to be very poorly correlated. This is less of an issue for lossy compression, but makes lossless compression significantly more difficult. Although lossy compression is acceptable in many applications, the effort and expense involved in the acquisition of scientific data usually warrants lossless compression. Likewise, for both practical and legal reasons, medical imaging data is almost always compressed losslessly. For visualization , however, a lossy version may be acceptable, so that the optimal configuration would be a lossy-to-lossless compressed format. Our solution to this problem is to use an adaptive prediction-based lossless compression scheme, similar to those used in lossless audio compression. This is accomplished by using a switched predictor, in which the best predictor out of a small set of candidate predictors is selected for encoding at any given time. Such an approach is able to adapt to varying data statistics by selecting the predictor that best models the local characteristics of the data. Furthermore, such a scheme is very extensible, in that it is able to incorporate existing predictors as well as predictors developed in the future without modifying the basic framework. While our primary goal is to improve the compression rate, we also describe a unique feature of our system that supports progressive analysis of the compressed data, called Progressive Precision. Progressive Precision allows up to three levels of data access, thereby providing a lossy, lower-resolution option appropriate for visualization preview. To summarize, we believe the salient contributions of our work to be: @BULLET An adaptive switched prediction framework for lossless floatingpoint volume compression, allowing consolidation of existing prediction methods into a single highly-adaptive compressor. @BULLET A fast method for entropy coding of residual leading zeros using a rank table and universal codes. @BULLET A method providing progressive transmission, thereby supporting efficient visualization. </p><p> We apply our approach to several large volume datasets, demonstrating lossless compression rates consistently in the 30-50% range. Our approach is able to provide better rates than existing approaches, while offering similar compression/decompression times. This allows fast, efficient compression of volume data at acquisition time, so that the benefit of compression is conferred to data transfer, storage, and subsequent visualization. Our novel progressive scheme is able to provide lower-precision, lossy representations of the datasets, thereby facilitating real-time visualization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Many approaches for scientific data compression focus primarily on combining compression with data synthesis in order to increase throughput and conserve storage. Engelson et al. <ref type="bibr" coords="2,211.05,127.74,10.45,8.02" target="#b12">[5] </ref> compress sequences of double-precision floating-point values resulting from simulations based on ordinary differential equations. In their approach, the numbers are treated as integers and then compressed using predictive coding, with residuals being explicitly stored in the case of lossless coding or truncated for lossy coding. Ratanaworabhan et al. <ref type="bibr" coords="2,34.28,187.52,14.94,8.02" target="#b22">[16] </ref>propose a lossless prediction-based compression method for double-precision floating-point scientific data using a DFCM (Differential Finite Context Method) value predictor, which is based on pattern matching using a hash table holding the recent encoding context. The bitwise residual is then computed using the XOR operator, with the compressed representation consisting of the number of leading zeroes and the remaining residual bits. Lindstrom and Isenburg <ref type="bibr" coords="2,241.48,247.29,14.94,8.02" target="#b19">[13] </ref>also focus on the goal of fast throughput for online encoding, achieving lossless or lossy compression of floating-point sequences by prediction and entropy coding of residuals. A Lorenzo predictor is used with computation of the predicted value by floating-point arithmetic, with the option of bit-wise integer arithmetic. The residual is computed by transforming the actual and predicted values to unsigned integers and taking the integer difference, which is subsequently encoded using a two-level scheme based on fast entropy coding of the sign and leading zero bits, with explicit transmission of the remaining residual bits. Burtscher and Ratanaworabhan <ref type="bibr" coords="2,159.15,346.92,10.45,8.02">[1] </ref><ref type="bibr" coords="2,172.53,346.92,10.45,8.02" target="#b9">[2] </ref> propose a lossless compression method for double-precision scientific data with the goal of achieving fast encoding/decoding for high throughput environments. This method employs two prediction methods, FCM (Finite Context Method) and DFCM, with the method producing the smallest residual being chosen. The difference is computed using XOR and the number of leading zero bytes is recorded along with the nonzero residual. Xie and Qin <ref type="bibr" coords="2,54.45,416.66,14.94,8.02" target="#b36">[31] </ref> describe a method for compression of seismic floatingpoint data wherein a differential predictor is used, followed by separate context-based arithmetic coding of each of the sign, exponent, and mantissa fields of the residual, where the exponent switches between contexts. Tomari et al. <ref type="bibr" coords="2,132.98,456.51,14.94,8.02" target="#b29">[24] </ref>propose a simple lossless scheme for double-precision scientific data designed with fast hardware decompression in mind. They compress only the exponent by keeping a small table of recent exponents into which they index. Finally, Schendel et al. <ref type="bibr" coords="2,59.27,496.36,14.94,8.02" target="#b25">[19] </ref>propose to preprocess floating-point data in order to selectively encode only bytes that are amenable to compression by existing byte-based lossless compressors. On the other hand, some researchers attempt to compress in a way that facilitates post-processing and analysis, especially visualization . Tao and Moorhead <ref type="bibr" coords="2,115.88,546.65,14.94,8.02" target="#b28">[22] </ref><ref type="bibr" coords="2,133.83,546.65,14.94,8.02">[23] </ref>compress scientific floating-point data by using a biorthogonal wavelet transform followed by entropy coding, with the goal of providing progressive transmission. Trott et al. <ref type="bibr" coords="2,44.24,576.54,14.94,8.02" target="#b30">[25] </ref><ref type="bibr" coords="2,62.46,576.54,14.94,8.02" target="#b31">[26] </ref>compress scientific floating-point data in curvilinear grids by using a Haar wavelet transform followed by entropy coding . Lossless coding is achieved by converting the single-precision floating-point numbers to double precision prior to the transform, with the double-precision coefficients being compressed directly via bytewise application of Huffman coding. Du and Moorhead <ref type="bibr" coords="2,224.13,626.35,10.45,8.02" target="#b11">[4] </ref>describe a similar approach using Haar or first-order B-spline wavelet transforms preceded by conversion to double precision. The double-precision coefficients are encoded by separate entropy coding of the exponent and mantissa, with run-length encoding of the least significant mantissa bits. Most high-quality audio coding approaches are built around a PCM (pulse code modulation) lossless coding scheme. Ghido <ref type="bibr" coords="2,227.96,696.57,10.45,8.02" target="#b14">[7] </ref>proposes a method for lossless compression of single-precision audio data by defining a transform from floating-point to integer numbers that is portable and renders the integers amenable to efficient compression. The transform yields an auxiliary stream with encoding parameters and an integer stream, which is then compressed using a PCM lossless entropy coder. Yang et al. <ref type="bibr" coords="2,406.88,63.35,14.94,8.02" target="#b37">[32] </ref>encode single-precision audio data by truncating the floating-point numbers to integers and computing the bit-wise residual between the truncated integer version of the number and the original version. The integers are encoded using an established PCM lossless compression method, whereas the floatingpoint residuals are compressed byte-wise using the lossless coder gzip, excluding zero bytes. Liebchen et al. <ref type="bibr" coords="2,427.98,123.12,14.94,8.02" target="#b18">[12] </ref>describe compression of single-precision floating-point audio data in the MPEG-4 ALS specification . Floating-point values are decomposed into a truncated integer component, which is encoded using the prediction-based integer ALS scheme, a common multiplier, which is related to the local dynamic range of the signal, and a floating-point residual, which is compressed using a masked Lempel-Ziv dictionary coder. The problem of floating-point compression also arises in image, texture, and geometry encoding. <ref type="bibr" coords="2,380.47,223.48,13.74,8.02" target="#b35">[30]</ref> . Most floating-point texture compression methods are based on block truncation coding and are therefore lossy, being designed for hardware application. In the area of geometry compression most methods are lossy, but some lossless methods based on predictive coding have been described <ref type="bibr" coords="2,458.00,263.33,14.94,8.02" target="#b17">[11] </ref>[33] <ref type="bibr" coords="2,492.36,263.33,13.74,8.02" target="#b39">[34]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p> The primary source of floating-point volume data is scientific simulations , in which simulation code runs on massively parallel supercomputers in an attempt to model some natural or theoretical phenomenon. To begin the simulation, an initialization of the volume is defined, and then the code iteratively modifies the volume as the simulation progresses in order to explore the dynamic behavior of the system. Each iteration defines a time step in the simulation, and these iterations are of great interest to the scientists. A simulation can run for many thousands of time steps, yielding thousands of volumes for subsequent processing and analysis. Managing such large amounts of data continues to be a challenge for simulation centers. In order to meet this challenge, there has been increasing interest in compression strategies that ameliorate this problem. One such strategy is called in-situ compression, in which the data is compressed in conjunction with the simulation, as it proceeds. This approach is advantageous , because by reducing volume size early savings are gained in transfer of data to storage, storage requirements, and transfer of data to clients for data analysis. The cost of this approach is additional processing time up front to compress the data, as well as any additional cost to decompress the data on the client side. Therefore, in-situ compression methods should ideally maximize the compression rate with the constraint of modest encoding/decoding complexity. Modest decoding complexity allows better integration of decoding with volume visualization. In the following sections, we present a lossless compression method for floating-point volume data that meets these requirements. In particular , we describe a method that offers fast and efficient coding, thereby allowing in-situ compression. Decoding is even faster, which allows this approach to be integrated with visualization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PREDICTIVE CODING FRAMEWORK</head><p>Only a few approaches have been developed for lossless floating-point compression of volume data, most of which are based on predictive coding using various source models. We believe that in certain aspects the challenges of floating-point compression more closely resemble those of audio compression; in particular, audio data formats typically accommodate high dynamic ranges (e.g. 16-bit integer) and require efficient compression/decompression. Based on this observation, we attempt to combine ideas from audio and image coding in order to address the unique challenges of floating-point compression. The basic approach to lossless compression of correlated data is to first decorrelate the data and then apply entropy coding. There are two general approaches to decorrelation: prediction-based encoding and transform coding (most notably wavelet transform coding).  While wavelet coding is preferred for lossy compression, predictionbased methods generally achieve better rates for lossless compression. Furthermore, prediction-based methods are fast, efficient, and require only one pass over the data. For these reasons, prediction-based methods are popular for both lossless image and audio compression, and form the basis of our proposed solution as well. A prediction-based compression scheme uses a subset of previously encoded values, called the context, to attempt to predict the next value to be encoded. Then the difference between the actual value and its prediction, called the residual, is encoded instead of the value. If the prediction is accurate then the difference values will be decorrelated and small in magnitude, thereby allowing efficient compression with entropy coding. Central to this approach is the definition of the context and how to compute the prediction. Typically, the prediction calculation is based on knowledge of the structure of the data and is a linear combination of a few previously encoded values. Given a finite context , it is possible to compute the optimal linear coefficients in terms of minimum prediction error; however, in practice this is prohibitively expensive. An alternative is to use a set of coefficients, or equivalently a set of linear predictors, in order to approximate an optimal predictor . If one predictor out of the set is chosen to make the prediction then this is called a switched prediction scheme, and this arrangement forms the basis of our prediction framework. Such a scheme is able to adapt both to different datasets and to varying statistics within the data by choosing predictors that best model the local behavior of the data. This allows more effective and robust decorrelation, thereby yielding good compression rates over a wide range of data. Our approach operates as follows. We encode the data in blocks consisting of several thousand values (8K in our system), within which we subdivide the data into small frames consisting of only a few values , as shown in <ref type="figure" coords="3,91.53,625.80,30.85,8.02" target="#fig_2">Figure 1</ref>. Blocks are encoded independently to allow efficient access to subsets of the data. Blocks are processed frame by frame, and within each frame a prediction is computed for each value by each of the predictors based on its respective context. The total prediction error is computed for each prediction method and the predictor with the minimum error is selected to encode the frame. The predictor selection for each frame is sent to a dedicated entropy coder. The added complexity involved in the selection scheme is justified as producing a good prediction is of paramount importance, since all incorrectly predicted bits are transmitted uncompressed. Once a prediction is generated, the residual is computed. If the prediction is accurate then the predicted and actual values should be </p><p>(a) Axis-aligned prediction contexts (b) Diagonal prediction context <ref type="figure" coords="3,294.12,203.11,19.48,7.37">Fig. 2</ref>. Our prediction context consists of three sets of linear contexts in the X, Y, and Z orientations (a), as well as a diagonal predictor in the X-Y plane (b). Interpolating polynomials of orders one through four in each of these contexts yield a total of 16 predictions of V, the voxel to be encoded. The contexts used by each predictor configuration are given in <ref type="figure" coords="3,302.54,250.43,26.57,7.37" target="#tab_1">Table 1</ref>. </p><p>close, which allows a residual to be computed that consists of a string of leading zeros followed by a set of significant bits representing the error in the prediction. The leading zero count can be efficiently entropy coded, whereas the error bits are sent verbatim to allow lossless reconstruction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Polynomial Predictors</head><p>Our basic scheme, which we refer to as APE (Adaptive Polynomial Encoder), uses a set of interpolating polynomial predictors with integer coefficients, as first proposed for audio coding by Robinson <ref type="bibr" coords="3,529.56,370.50,14.94,8.02" target="#b23">[17] </ref>and adopted by Hans and Shafer in their AudioPaK coder <ref type="bibr" coords="3,517.96,380.46,9.52,8.02" target="#b15">[8]</ref>. A unique (k − 1)-order polynomial can be constructed from the last k data points and subsequently used to predict the next value, x n . We have found that the most useful polynomials for encoding floatingpoint data are orders zero through three, which give the following pre- dictions: </p><formula>P 1 (n) = x n−1 (1) P 2 (n) = 2x n−1 − x n−2 (2) P 3 (n) = 3x n−1 − 3x n−2 + x n−3 (3) P 4 (n) = 4x n−1 − 6x n−2 + 4x n−3 − x n−4 (4) </formula><p>where P i (n) is the (i − 1)th-order prediction of x n . The prediction residuals for each polynomial predictor can be computed directly using only subtractions based on the following recursive relationship: </p><formula>R 0 (n) = x n (5) R 1 (n) = R 0 (n) − R 0 (n − 1) (6) R 2 (n) = R 1 (n) − R 1 (n − 1) (7) R 3 (n) = R 2 (n) − R 2 (n − 1) (8) R 4 (n) = R 3 (n) − R 3 (n − 1) (9) </formula><p>We can also use these equations as a more efficient way to determine and compute the best prediction. These four linear polynomial predictors of orders zero through three form the basis of our APE predictor. We extend this 1D approach to 3D by computing parallel predictions in the three cardinal directions, as shown in <ref type="figure" coords="3,455.79,676.65,29.47,8.02">Figure 2</ref> . This results inrequiring additional encoding bits, but this would require more prediction calculations, which would impact the encoding efficiency. In the case of the X-oriented predictions only, wherein the contexts overlap from one step to the next, we can compute the residuals even more efficiently by reusing residuals from the previous prediction: </p><formula>R k (n) = R k−1 (n) − R k−1 (n − 1) n = 1 R k (n − 1) otherwise (10) </formula><p>where R denotes the new residuals to be computed. This amounts to one subtraction per residual. Because the prediction context includes the previous four voxels in the Z direction, the memory footprint for encoding must potentially include five slices of the volume. Since our block size is smaller than the size of a slice, blocks will depend on previously-encoded blocks for their Z context, and therefore cannot be coded independently . However, if we consider simulations in which a distributed processing model is used so that each node processes a small subvolume , then we can independently encode blocks as long as the slices of the subvolume are sufficiently small. In order to support lossless reconstruction, we must be careful that the predictions at the decoder exactly match the predictions made by the encoder. This can be accomplished in several ways. The most direct way, which we adopt in our system, is to use floating-point operations (of the same or greater precision) while using the same platform for the encoder and decoder. Even this is problematic if truly lossless compression is important, as differences in compilers, compiler options, runtime environments, and several other factors may affect floating-point operations. Therefore, the most robust approach is to base the prediction on integer operations only. One integer-based method is to map the values to integers and carry out the operations using integer arithmetic, with an associated reduction in prediction accuracy . A second integer-based method is to use a software implementation of floating-point arithmetic such as SoftFloat <ref type="bibr" coords="4,212.27,384.86,10.45,8.02">[9] </ref>that relies on integer operations, which preserves prediction accuracy but is significantly more expensive. It is possible to implicitly select the predictor, for instance by taking the median prediction or by examining the nearby context; however , we explicitly select the predictor with the best prediction (i.e. the smallest residual) and send the selection as auxiliary information. This choice is based on our observation that the overhead for transmitting the predictor selection is more than compensated by the better prediction accuracy. As we have 16 predictors, binary coding of the selection requires four bits. For our encoder we use frames of size eight, so that the cost of selection is 0.5 bpf (bits per float), since one predictor is chosen to encode the entire frame. For reference, <ref type="figure" coords="4,212.19,504.71,27.55,8.02" target="#tab_1">Table 1</ref>lists our encoder configurations based on the constituent predictors. The predictor selection tends to be highly skewed, with some predictors being selected much more frequently than others, allowing for further reduction in cost by entropy coding. In our system, within each block we record the predictor selection using binary codes while simultaneously collecting the predictor pmf (probability mass function). Once we reach the end of the block we compute a Huffman table based on the pmf and encode the selections, which have been recorded in the binary-coded buffer. This buffer serves a dual purpose. In the case of ineffectual Huffman coding, which might result from a more uniform selection pmf, the binary-coded buffer is transmitted instead, thereby guaranteeing at most 0.5 bpf. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Combined Predictors</head><p>Our second scheme combines the polynomial predictors with other types of predictors in order to diversify our framework, and is thus referred to as ACE (Adaptive Combined Encoder). Incorporating other predictors provides additional data models which may aid in achieving effective decorrelation. This not only allows us to build on previous work, but provides a convenient mechanism for extension should better predictors be developed in the future. Based on an analysis of previous work in floating-point prediction, we chose two predictors to incorporate in our system. The first is the Lorenzo predictor, which is a <ref type="figure" coords="4,285.12,59.66,24.20,7.37" target="#tab_1">Table 1</ref>. Configurations for the APE and ACE encoders. For each frame the best predictor from the given set is selected. The frame size is set so that the maximum rate for specifying the predictor selection is 0.5 bpf (bits/float). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scheme Predictors </head><formula>APE P 1 (x), P 2 (x), P 3 (x), P 4 (x) P 1 (y), P 2 (y), P 3 (y), P 4 (y) P 1 (d), P 2 (d), P 3 (d), P 4 (d) P 1 (z), P 2 (z), P 3 (z), P 4 (z) ACE P 1 (x), P 2 (x), P 3 (x), P 4 (x) P 1 (y), P 2 (y), P 3 (y), P 4 (y) P 1 (z), P 2 (z), P 3 (z), P 4 (z) </formula><p>Lorenzo, FCM, DFCM, Mean generalization of the parallelogram predictor, as introduced by Ibarria et al. <ref type="bibr" coords="4,305.84,223.78,13.74,8.02" target="#b16">[10]</ref>. The second is the FCM/DFCM hash-based predictor pair introduced by Burtscher and Ratanaworabhan <ref type="bibr" coords="4,452.61,233.74,9.52,8.02">[1]</ref> , which relies on locally repeating patterns in the data. Additionally, we compute a mean prediction as the average of all the constituent predictors. Thus we add a total of four additional predictors: Lorenzo, FCM, DFCM, and Mean. The ACE predictor configurations are listed in <ref type="figure" coords="4,467.87,283.73,25.96,8.02" target="#tab_1">Table 1</ref> , with relevant contexts for the polynomial predictors shown in <ref type="figure" coords="4,483.04,293.70,29.86,8.02">Figure 2</ref>. The ACE scheme includes the X, Y , and Z polynomial predictor sets, and the four additional predictors, Lorenzo, FCM, DFCM, and Mean. We use a frame size of eight in order to ensure a worst-case rate of 0.5 bpf. Inspection of the predictor selection histograms for ACE demonstrates that each predictor is selected a variable number of times, depending on the characteristics of the dataset. Again, in most cases the histogram is highly skewed, allowing efficient entropy coding. The strength of our approach is manifest, in that our encoder adapts to each dataset by selecting the predictors that best model the data. Although there is some amount of overhead in calculating several predictions and in the selection process, we show that this modest increase in complexity is offset by significantly better compression rates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PREDICTION RESIDUAL ENCODING</head><p>Prediction residuals for integer data are usually entropy coded based on a Laplacian model of the error signal. However, this approach is not pragmatic for floating-point residuals, for several reasons. First, consider the pmf of the residuals for a typical dataset, as shown in <ref type="figure" coords="4,520.55,475.61,14.95,8.02;4,285.12,485.57,11.45,8.02">Fig- ure</ref>3. The pmf shows the probability of occurrence for each possible residual, which aids in the development of a statistical model for encoding . Although from a distance the pmf appears to be Laplacian, when we inspect the distribution more closely we discover that there are discontinuities due to the sparse population of possible values, of which there are over four billion. This structure will be difficult to model without assigning codes to residuals that do not occur. Furthermore , as pointed out by Lindstrom and Isenburg, there are many possible residuals compared to the cardinality of the data to be compressed, so that entropy coding will be inefficient and likely ineffectual. An alternative approach, first proposed by Sayood and Anderson <ref type="bibr" coords="4,505.46,585.20,14.94,8.02" target="#b24">[18] </ref>and used in variant forms by both Burtscher and Ratanaworabhan <ref type="bibr" coords="4,509.58,595.16,10.45,8.02">[1] </ref>and Lindstrom and Isenburg <ref type="bibr" coords="4,373.66,605.13,13.74,8.02" target="#b19">[13]</ref>, is to use entropy coding for the number of leading zeros in the residual while simply transmitting the remaining error bits verbatim. Our framework also uses this method, but with a novel fast entropy coding scheme for the leading zero count. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Computation of the Prediction Residual</head><p>There are two efficient methods of computing the residual using only integer operations. The first approach, used by Burtscher and Ratana- worabhan <ref type="bibr" coords="4,323.32,686.61,9.52,8.02">[1]</ref>, is to simply XOR the two values, since floating-point numbers in close proximity will often have identical bit patterns in their most significant bits. The second approach, used by Lindstrom and Isenburg <ref type="bibr" coords="4,333.86,716.50,13.74,8.02" target="#b19">[13]</ref>, is to map the floating-point numbers into unsigned integers and compute the absolute integer difference, while keeping track of the residual sign explicitly. Regardless of the method, the end <ref type="figure" coords="5,31.50,164.63,18.64,7.37">Fig. 3</ref>. Bottom: The pmf of the prediction residual for a typical floating-point dataset appears to be Laplacian. Top: Magnification demonstrates how sparsely the pmf is populated; these discontinuities in the pmf, along with the exceptionally large number of possible residuals compared to actual residuals, precludes conventional entropy coding of the residual. <ref type="figure" coords="5,31.50,359.42,19.02,7.37">Fig. 4</ref> . A rank table keeps a sorted list of symbol frequencies. As symbols are encountered, the frequency array is updated by moving entries up or down in order to maintain a decreasing frequency order. As entries are moved, the pointers to and from the rank array are modified accordingly. This data structure allows the use of Universal codes for fast entropy coding of the LZC symbols. result is a residual which consists of some number of leading zeros, followed by a number of significant error bits. The error bits are transmitted uncompressed, so in general the more leading zeros we can produce the better the compression rate. The XOR method of computing the residual is faster than the second method (which we call UINT/SUB), but produces fewer leading zeros when the values are close but separated by an exponent boundary. We offer both methods in our framework, but the default is UINT/SUB. One key observation to make is that the most significant bit of the residual error bits is always 1, and therefore does not need to be transmitted. This small modification improves the compression rate by 1 bpf. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Entropy Coding of Leading Zero Count</head><p>For single-precision floats, the LZC (Leading Zero Count) can range from 0 to 32. In order to more efficiently represent this quantity we compact it into 32 values by the following mapping: </p><formula>LZC = 0 LZC = 0, 1 LZC − 1 otherwise (11) </formula><p>which maps a LZC of 0 and 1 to the same value. This means that for both LZC = 0 and LZC = 1, 32 bits will need to be transmitted, since the value of the first bit is unknown. Since values of 0 and 1 tend to occur very infrequently, this has negligible effect on the encoding process. LZC , in the range <ref type="bibr" coords="5,129.91,685.79,9.46,7.96">[0,</ref><ref type="bibr" coords="5,140.35,685.94,10.27,8.02" target="#b36"> 31]</ref>, can be encoded using a binary code of 5 bits. Examining the histogram of LZC in some typical datasets (see <ref type="figure" coords="5,266.93,706.53,14.95,8.02;5,31.50,716.50,16.97,8.02">Fig- ure 5</ref> ) demonstrates that further reduction is possible with entropy coding . In our system we offer adaptive Huffman coding for this purpose, as well as range coding, the arithmetic coding variant described by Lindstrom and Isenburg <ref type="bibr" coords="5,385.27,214.82,13.74,8.02" target="#b19">[13]</ref> . In addition, we introduce a third option that is faster than either of these and achieves nearly the same rate. This method uses Universal codes with an associated rank table . Universal codes are variable-length, self-delimiting integer codes that encode a set of integers where smaller values are more likely than larger ones. Codes for the integers can be computed on-the-fly or precomputed and stored if the range of possible values is given a priori, as in the case of the LZC integers. Using precomputed codes is extremely fast, since only a single table look-up from a very small table is required. The problem with using Universal codes for encoding the LZC is that the assumption of lower values having a greater probability of occurrence does not hold in general; in fact, this assumption only holds for early-peaking LZC pmfs, and even then not perfectly. In order to address this problem, we propose the use of the rank table in conjunction with Universal codes. A rank table is a data structure that is used in some adaptive Huffman coders <ref type="bibr" coords="5,486.37,364.51,14.94,8.02" target="#b21">[15] </ref>in order to determine when a revised Huffman table should be recomputed. The basic structure of a rank table is shown in <ref type="figure" coords="5,446.67,384.43,29.20,8.02">Figure 4</ref>. It consists of two arrays, one for the symbol ranks, which is indexed by the symbols, and another for the symbol frequencies. The frequency array keeps a sorted list of the symbol frequencies, along with a reverse pointer which indicates to which symbol each frequency belongs. The symbol rank table is indexed by the symbol value and contains a forward pointer to the corresponding frequency in the frequency array. The rank table operates by updating the symbol frequencies as each symbol is encountered and maintaining them in sorted order by manipulating table pointers. We can use the same structure as a way to transform the symbols into another set of sorted symbols (i.e. the rank symbols) in which the probabilities are strictly non-increasing, thereby allowing proper use of Universal codes; that is, we encode the rank of the LZC instead of the LZC itself. As long as the decoder constructs and uses the same rank table then we can unambiguously decode the rank sequence and use the reverse pointer to obtain the LZC for each rank. <ref type="figure" coords="5,513.14,544.09,31.36,8.02">Figure 5</ref>shows how each type of LZC pmf is transformed to a corresponding rank pmf having the necessary Universal coding property. The only remaining issue to resolve is the choice of Universal code. There are several types of Universal codes, of which we tested the Gamma code, the FK1 code, Rice codes, Golomb codes, and Split-sample codes. Rice codes, Golomb codes, and Split-sample codes are more flexible, as they each have a parameter that can be optimized in order to tailor the codes to a particular sequence. Based on our experiments, Golomb codes with the parameter adaptively determined give the best overall coding rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ADAPTIVE PROGRESSIVE PRECISION</head><p>The least significant bits of floating-point numbers in scientific datasets often appear to be statistically random. Although it is impossible to determine whether they are truly random or not, we do know that for many datasets some of the least significant bits of the mantissa represent noise resulting from data acquisition (random variation in measurements or model parameters) or from data processing (e.g. quantization noise from floating-point computation/storage). As (a) Early-peaking LZC pmf. </p><p>(b) Mid-peaking LZC pmf. (c) Late-peaking LZC pmf. <ref type="figure" coords="6,22.50,197.58,18.90,7.37">Fig. 5</ref>. In practice we encounter three types of LZC pmfs (probability mass functions): (a) early-peaking, (b) mid-peaking, and (c) late-peaking. By using a rank table structure, all three types are converted into a non-increasing pmf appropriate for Universal codes. a result of the apparent randomness in these last bits, lossless compression of floating-point values is constrained, and we often end up explicitly storing the least significant bits of the residual for all values . However, as these bits may not contain useful information at all, it would be nice to offer a more compact representation of the data in which these bits are initially omitted, in order to facilitate data analysis and/or visualization. We describe such an approach, which we call Progressive Precision (PP). PP provides a lossy preview option for our lossless compression algorithm; however, this method is not intended to compete with more conventional lossy methods. If lossy compression is acceptable, then other methods are more appropriate, such as those based on wavelets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Variable-precision Prediction</head><p>At first glance, it may appear that in order to create a representation supporting progressive precision we can simply partition the error bits in the residual and store them separately. However, prediction-based encoding relies on a context that must be the same for the encoder and decoder, and this requires all residual bits. Instead, we can partition the bits in the original data, with the most significant bits participating in predictive coding and the least significant bits left unencoded. The least significant bits can then be stored separately and accessed as necessary. As shown in <ref type="figure" coords="6,78.25,467.44,28.66,8.02" target="#fig_4">Figure 6</ref>, partitioning on byte boundaries results in two options for progressive encoding, based on encoding the first two bytes or first three bytes. This is to be compared to the standard encoding of all four bytes. Ideally we would like to partition the data as much as possible in order to provide greater flexibility in accessing the data; however, partitioning will negatively impact the compression rate if the unencoded bits are not random and are able to be predicted. This means that in practice the efficacy of this approach will depend on the characteristics of the dataset. This method is appropriate in cases where a lossless version of the dataset must be maintained, but yet a more compact, lossy preview of the dataset is desirable for certain types of analysis (i.e. visualization). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Statistical Modeling of Mantissa</head><p>Evaluation of scientific data often involves computation of derived properties, statistical analysis, and/or visualization, especially for 2D/3D data. These tasks may be sensitive to the effects of quantization when using the lossy progressive approximations offered by PP. For instance, visualization of quantized properties may exhibit banding , a distracting artifact that is due to sensitivity of the human visual system to sudden visual discontinuities. In order to address this issue, we propose to keep some statistical information regarding the unencoded bytes along with the encoded data. Specifically, as we process the blocks of data we compute the pmf of the third and fourth bytes of the mantissa and store them with the encoded data if the bytes are partitioned into separate files. This allows us to model the statistical behavior of the last bytes even if they are not entirely uniformly distributed . We find in practice that these pmfs possess varying degrees of nonuniformity; they are not so nonuniform as to allow entropy coding, but they are usually not completely uniform either. If they do happen to be uniform, which sometimes happens with the least significant byte, then we can simply use a uniform random variable to reconstruct bytes. Otherwise, when analyzing the data we reconstruct these last bytes using the inversion method <ref type="bibr" coords="6,406.34,457.79,9.52,8.02" target="#b10">[3]</ref>, whereby the inverse cumulative mass function is sampled with a uniform random variable. This effectively achieves a non-deterministic, full-precision lossy reconstruction of the data that reduces the deleterious effects of quantization on subsequent evaluation. Shown in <ref type="figure" coords="6,397.26,497.64,30.87,8.02">Figure 8</ref>is a synthetic sphere volume rendered using Level 0 with and without statistical modeling. Comparison to rendering of the original full-precision volume indicates that this approach is able to reduce some but not all of the artifacts from limiting the precision. </p><p>(a) Level 0 (b) Level 0 w/ modeling (c) Levels 0-2 (Original) <ref type="figure" coords="6,285.12,674.82,18.71,7.37">Fig. 8</ref>. A synthetic sphere dataset is compressed using our Progressive Precision ACE encoder. Images show volume rendering based on: (a) reconstruction using only Level 0; (b) non-deterministic reconstruction at original precision using statistical models of the least significant bytes; (c) original image reconstruction using all levels. Statistical modeling is able to reduce some but not all of the artifacts of limiting the precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p> In this section we evaluate our proposed technique and compare the results to other lossless compression methods for floating-point volume data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Floating-point Datasets</head><p>In order to evaluate our proposed compression scheme, we collected a variety of datasets with various dimensions and from various sources. We use four numerical simulations and two measurement datasets, all in single-precision floating-point format. The numerical simulations , shown in <ref type="figure" coords="7,100.53,387.75,30.85,8.02" target="#fig_6">Figure 7</ref> , include a jet flame turbulent combustion (Sci Comb), a CFD simulation of turbulent vortex structures (Sci Vortex), a thermal fluid simulation (Sci Ohm), and a supernova core collapse (Sci Super). The dimensions of these datasets are noted in the figure caption. For the jet flame combustion we list results for two of the five variables: the scalar dissipation rate (Sci Comb Dr) and the stoichiometric mixture fraction (Sci Comb Mx). We also choose two of the five supernova variables, namely density (Sci Super D) and pressure (Sci Super P). Our measurement datasets, shown in <ref type="figure" coords="7,182.56,477.41,30.41,8.02">Figure 9</ref>, include the head MRI from the Visible Human Female, but instead of using the original 12-bit dataset we use a registered floating-point version of this data as generated by Muraki et al. <ref type="bibr" coords="7,156.80,507.30,13.74,8.02" target="#b20">[14]</ref>. The registration method they employ is based on numerical solution of a nonlinear minimization problem, such that the resulting transformed data is expanded into floating-point precision. This dataset consists of three variables: T1- weighted image (Med MRI T1), T2-weighted image (Med MRI T2), and proton-density-weighted image (Med MRI PD). The other dataset is a Diffusion Tensor MRI (DT-MRI) scan of a healthy person, which consists of the tensor volume (Med DTMRI T) as well as the computed anisotropy field (Med DTMRI A). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Implementation</head><p>As already mentioned, our encoder processes the data block by block, with each block being independently coded. In our system we use blocks of size 8K, with a frame size of eight, in order to ensure at most 0.5 bpf overhead for predictor selection. Our default APE/ACE encoders use floating-point operations for prediction, compute residuals using the UINT/SUB method, and use Golomb codes for entropy coding the LZC . The residual sign, produced by the UINT/SUB method, is stored directly, as efforts directed at more efficient encoding were generally unsuccessful. All tests were conducted using a singlethreaded program running on a machine equipped with a 2.7GHz Intel i7 processor and 12GB of memory. We compare our method to two established floating-point compressors for scientific data. The first is FPC <ref type="bibr" coords="7,202.01,736.42,9.52,8.02">[1]</ref>, which uses FCM (a) MRI Dataset (b) DT-MRI Dataset <ref type="figure" coords="7,294.12,446.55,19.37,7.37">Fig. 9</ref>. Our data suite includes two medical datasets: (a) A registered MRI scan (256 × 256 × 187, 3 channels); (b) A Diffusion Tensor MRI scan (256 × 256 × 64, 9 channels), along with the computed anisotropy field. and DFCM for prediction. Both are essentially hash tables, with the hash value for FCM based on the previous value and the hash value for DFCM based on the previous stride. In their original form these methods operate on double-precision data, so we modified them in a straightforward way to work on single-precision data as well. Our single-precision implementation follows FPC closely except that we compute the stride using floating-point operations. In particular, we compute residuals using the XOR method and explicitly store both the predictor selection (FCM or DFCM) and number of leading zero bytes (0-3) for each value. We also experimented with different hashing parameters in order to find the best overall settings. The second method we use for comparison is the Lorenzo predictor as described by Lindstrom and Isenburg <ref type="bibr" coords="7,445.51,616.30,13.74,8.02" target="#b19">[13]</ref> . Predictions are computed in floating-point arithmetic and residuals are computed based on the UINT/SUB method. Entropy coding of the LZC is accomplished using a C translation of the C++ range coder implementation provided by the authors. Our implementation differs only in that the residual sign is stored separately, instead of being folded in with the LZC as in the original implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Compression Analysis</head><p>Of paramount importance in achieving good compression rates for lossless floating-point compression is computing accurate predictions. In order to evaluate the different prediction methods we can analyze the predictor selection histograms for ACE, since this <ref type="figure" coords="8,22.50,228.90,23.24,7.37" target="#fig_2">Fig. 10</ref>. We apply the PP scheme to scientific datasets using the ACE encoder, with compression of two bytes (2B), three bytes (3B), or all four bytes (4B). The results indicate that providing a lossless, progressive version of certain datasets does not greatly impact the compression rate. Regardless, the L0 level of the 3B and 2B options provides a compact lossy version of the dataset for preview. tion is based on minimum prediction error with all the candidate predictors participating. We observe that the polynomial predictors and the Lorenzo predictors are almost exclusively selected, with the Lorenzo predictor being selected much less frequently, except in the Sci Super D dataset. However, we point out that the Lorenzo predictor generally incurs less prediction error than any one polynomial predictor. The efficiency of entropy coding the predictor selection is related to the relative frequency of selection for all predictors, which varies greatly from dataset to dataset. For most datasets, entropy coding of the predictor selection using adaptive Huffman coding results in a rate of 0.2-0.3 bpf. Experiments with entropy coding of the LZC are shown in <ref type="figure" coords="8,72.25,431.34,25.94,8.02" target="#tab_2">Table 2</ref>, where our rank-based Golomb coding scheme is compared to adaptive Huffman coding and range coding. The proposed method is able to offer faster encoding, typically twice as fast as Huffman coding and four times as fast as range coding, with slightly lower compression factors. This explains why encoding times for the Lorenzo method and APE are very close; the switched predictor in APE is slower than the Lorenzo predictor, but some of this cost is offset by using a faster entropy coder. Overall, we believe that this method offers a good compromise between encoding speed and coding quality. The results of applying our PP (Progressive Precision) scheme to the datasets are shown in <ref type="figure" coords="8,117.82,542.39,38.83,8.02" target="#fig_2">Figures 10</ref>and 11. Compression rates are <ref type="figure" coords="8,285.12,228.22,23.46,7.37" target="#fig_2">Fig. 11</ref>. We apply the PP scheme to medical datasets using the ACE encoder, with compression of two bytes (2B), three bytes (3B), or all four bytes (4B). Although compression using the 3B version results in a small to moderate increase in compression rate, the 2B version requires a significantly higher rate. However, the L0 level of the 2B option offers a lossy version of the dataset at about 10% compression rate. </p><p>shown for encoding the first two bytes (2B) and the first three bytes (3B), with comparisons provided for conventional encoding of all four bytes (4B). For the 3B and 2B options, a breakdown of the space required by each of the constituent levels (L0, L1, L2) is shown in stacked form. For some of the datasets, the 3B and/or 2B options can provide a lossless progressive version of the data with minimal increase in compression rate. However, for several other datasets, these options significantly increase the rate. The primary advantage of using PP in these datasets is the relatively small size of the L0 level, which can provide a very compact, lossy version of the dataset, often at a rate of about 10% for the 2B option. Tables 3, 4, and 5 list the main results of this work, in which our proposed methods are compared against the two previously published methods for our given data suite. Results are reported for a single representative time step, with other time steps giving similar results. Compression is lossless, so the results are reported in terms of the compressed data size relative to the original data size. We provide two metrics of compression performance, namely the compression rate (compressed size / original size) and the compression throughput, reported as mean bpf (bits per float). Based on these tests, we conclude that the best encoder is ACE, which achieves the best rate on all datasets, regardless of dimensionality or source (simulation or observation ). These higher compression rates come at a cost, however, as ACE is also the slowest of the methods, although objectively it is not slow. Our other approach, APE, achieves the second best rate on all datasets except for the Supernova dataset, in which the Lorenzo method is second best. Of particular interest are the remarkable rates that APE/ACE is able to achieve on the 3D medical datasets. By examining the predictor selection histogram we are able to attribute this phenomenon to the success of the 4th order Z-oriented polynomial predictor, which is an excellent model for these particular datasets. By examining the compression rates in conjunction with the predictor selection histograms, we can make some inferences about the characteristics of the datasets themselves. We can think of each predictor as a model for the data, so that if a particular prediction method achieves a low rate then that model is valid. From this we can infer that when the predictor selection pmf shows significant contributions from multiple predictors then there are multiple regions within the data. This heterogeneity can be exploited by the selection mechanism of ACE, and so we expect the ACE rate to be significantly better than the best single method. On the other hand, if the predictor selection pmf is dominated by one predictor, then we can infer that the dataset is homogenous , in which case the ACE method will only do marginally better than the best individual predictor. In general these observa-tions are borne out in the data. In terms of dataset modeling, we see that APE models the Sci Comb and Med MRI datasets well, whereas Lorenzo works well for Sci Super. Results for the other datasets are fairly mixed. The conclusion we make is that no one model works well for all types of data, thereby justifying our switched prediction framework (in particular ACE). In regards to compression speed (shown in <ref type="figure" coords="9,194.00,301.81,24.80,8.02">Table 4</ref>), FCM/DFCM is easily the fastest method on all datasets. The Lorenzo and APE methods lie between FCM/DFCM and ACE in terms of rates and speed. Lorenzo is faster than APE but with lower compression rates in general . The slower speed of ACE compared to APE is primarily due to the computational cost of the Lorenzo predictor and to the additional overhead related to FCM/DFCM caching. Results for decompression speed (shown in <ref type="figure" coords="9,125.68,371.55,26.12,8.02">Table 5</ref>) mirror those of compression, with FCM/DFCM being the fastest decoder. Again, Lorenzo and APE offer similar speeds. The decoding times demonstrate the same trend as encoding times, but are shorter than encoding times, mostly due to fast entropy decoding. <ref type="figure" coords="9,31.50,442.10,25.51,7.37">Table 4</ref>. Compression timing results for encoding of single-precision datasets by previously published methods (Lorenzo and FCM/DFCM) and the proposed methods (APE and ACE), reported in seconds. The best time for each dataset is highlighted in bold. FCM/DFCM is shown to be the fastest method on all datasets, with Lorenzo and APE marginally slower. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this work, we introduced a switched prediction lossless coding scheme for compressing scientific and medical floating-point volume data. Our basic approach, APE, uses a set of interpolating polynomials , whereas ACE, our combined approach, uses the Lorenzo and FCM/DFCM predictors along with polynomials. The end result is an encoding framework that is robust, adapting to the data as it encodes, performs extremely well, and is easily extensible. We demonstrate on a variety volume datasets that our proposed approach achieves significantly better compression rates than existing methods at a modest <ref type="figure" coords="9,294.12,247.98,24.68,7.37">Table 5</ref>. Decompression timing results for decoding of single-precision datasets by previously published methods (Lorenzo and FCM/DFCM) and the proposed methods (APE and ACE), reported in seconds. The best time for each dataset is highlighted in bold. As with encoding, FCM/DFCM is the fastest method on all datasets, but the differences between the decompression times for the various methods are less than with encoding. Med DTMRI A 1.2 0.8 1.9 2.4 increase in encoding time. We believe this additional cost is justified , since our method is still relatively fast and the data is compressed only once, whereas the cost due to increased storage/transmission is incurred continuously/frequently. An important future investigation is the use of only integer operations in the prediction calculation. As mentioned previously, this is important when encoding and decoding on multiple platforms, and could be accomplished without loss of accuracy by using an integerbased floating-point implementation. Furthermore, an obvious direction for future work is the development of better prediction methods for volume data. These predictors could be incorporated directly into our ACE encoder, perhaps replacing predictors like FCM/DFCM that aren't often used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>This research is sponsored in part by the National Science Foundation through grants OCI 0905008, OCI 0850566, and OCI 0749227, and the Department of Energy through grants DE-FC02-06ER25777 and DE-FC02-12ER26072, program manager Lucy Nowell. The Sci Ohm dataset was generated at the National Center for Atmospheric Research (NCAR) in Boulder, Colorado, and the supernova dataset was obtained from Dr. John Blondin of North Caroline State University. The vortex dataset is from the VIZLAB of CAIP at Rutgers University , and the combustion datasets are from Dr. Jackie Chen of Sandia National Laboratories. The Visible Human Female MRI datasets are from the Visible Human Project of the National Library of Medicine. Floating-point versions of these datasets were obtained from Shigeru Muraki. Adam W. Anderson of Vanderbilt University provided us with </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,33.49,699.32,244.40,6.86;1,31.50,708.78,144.33,6.86"><head></head><figDesc>Manuscript received 31 March 2012; accepted 1 August 2012; posted online 14 October 2012; mailed on 5 October 2012. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,407.39,203.55,128.11,8.02;2,285.12,213.51,250.38,8.02;2,285.12,223.48,92.61,8.02"><head></head><figDesc>Most floating-point image methods are based on JPEG-2000. Lossless variants are described by several authors [6] [27] [28] [29] </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,31.50,210.81,250.38,7.37;3,31.50,220.28,250.38,7.37;3,31.50,232.12,250.38,7.37;3,31.50,245.77,250.38,7.37;3,31.50,259.55,250.38,8.02;3,31.50,274.27,250.38,7.37;3,31.50,283.73,250.38,7.37;3,31.50,293.20,41.65,7.37"><head>Fig. 1. </head><figDesc> Fig. 1. Datasets are encoded block by block, with each block further subdivided into many frames. Our proposed switched prediction scheme selects the best prediction from a small set of predictors P with given context C for each frame and encodes the residual, which is the difference between the predicted value S P and the actual value S . The residual R is divided into the Leading Zero Count (LZC), which is further encoded, and some number of error bits, which are transmitted unencoded. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,285.12,327.55,250.38,7.37;6,285.12,337.01,250.38,7.37;6,285.12,346.47,250.38,7.37;6,285.12,355.94,250.38,7.37;6,285.12,365.40,250.38,7.37;6,285.12,374.87,203.91,7.37"><head>Fig. 6. </head><figDesc>Fig. 6. The Progressive Precision method offers two progressive options for the ACE encoder, by applying the ACE method to the first three bytes and first two bytes of each floating-point value. The remaining bytes are transmitted unencoded. Multiple levels of the data, denoted as L0-L2, are stored separately. In the case of the two-byte and three-byte options, the L0 level provides a compact, lossy version of the data. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,31.50,234.54,513.00,7.70;7,31.50,243.03,513.00,8.64;7,31.50,252.50,433.04,8.67"><head>Fig. 7. </head><figDesc>Fig. 7. Our data suite includes four simulation datasets: (a) A jet flame turbulent combustion (1200 × 600 × 270, 122 time steps, 5 variables); (b) A CFD simulation of turbulent vortex structures (128 3 , 100 time steps); (c) A thermal starting plume descending through an adiabatically-stratified fluid (256 2 × 1024, 400 time steps, 5 variables); (d) A supernova core collapse simulation (864 3 , 105 time steps, 5 variables). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true" coords="8,22.50,583.42,250.38,156.63"><figDesc coords="8,22.50,583.42,250.38,7.37;8,22.50,592.89,250.38,7.37;8,22.50,602.35,250.38,7.37;8,22.50,611.81,250.38,7.37;8,22.50,621.28,250.38,7.37;8,22.50,630.74,250.38,7.37;8,22.50,640.21,157.80,7.37">Table 2. The proposed Universal coding scheme using Golomb codes is compared to adaptive Huffman coding and Range coding for representative datasets. Results are reported as compression factor, followed by relative encoding time, expressed in terms of the time required for Universal coding. The results show that the proposed method is about four times faster than Range coding and about twice as fast as Huffman coding, at the cost of some coding efficiency.</figDesc><table coords="8,28.48,649.84,240.07,90.22">Universal 
Huffman 
Range 
Coding 
Coding 
Coding 
(Compression (Compression (Compression 
Factor / 
Factor / 
Factor / 
Dataset 
Coding Time) Coding Time) Coding Time) 
Sci Comb Dr 
1.5 / 1.0 
1.6 / 1.7 
1.8 / 4.1 
Sci Ohm 
1.2 / 1.0 
1.3 / 2.2 
1.4 / 5.0 
Sci Super D 
1.5 / 1.0 
1.6 / 1.9 
1.8 / 4.3 
Med MRI PD 
1.2 / 1.0 
1.5 / 2.0 
1.6 / 4.5 

</table></figure>

			<note place="foot" n="12"> predictors (4 X-oriented, 4 Y -oriented, and 4 Z-oriented), which requires a four-bit binary code. To take advantage of the full range of the four bits (which selects for 16 predictors), we also use an X −Y diagonal context. This often complements the X and Y predictors well, especially when the inter-plane distance in Z is larger than the resolution in the X − Y plane. We could potentially add more predictors,</note>

			<note place="foot">FOUT AND MA: AN ADAPTIVE PREDICTION-BASED APPROACH TO LOSSLESS COMPRESSION OF FLOATING-POINT…</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="8,153.02,26.18,383.23,7.50"  xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,31.50,59.61,513.00,7.37;9,31.50,69.07,513.00,7.37;9,31.50,78.54,513.00,7.37;9,31.50,88.00,104.11,7.37"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Table 3 Results for lossless compression of single-precision floating-point datasets, reported as compression rate in percent with mean bits per float in parenthesis. Results using four compression methods are given: Lorenzo and FCM/DFCM refer to previously published methods, with APE and ACE being the proposed methods. For each dataset the best compression rate is highlighted in bold</title>
	</analytic>
	<monogr>
		<title level="m">As indicated in the table, ACE achieves the best rates for all datasets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,148.05,95.99,28.88,8.06;9,218.26,95.99,31.71,8.06;9,271.80,95.99,49.80,8.06;9,344.89,95.99,17.93,8.06;9,396.19,95.99,18.43,8.06;9,148.05,108.45,49.30,8.02;9,218.26,108.45,35.11,8.02;9,277.41,108.45,39.60,8.02;9,336.80,108.45,35.11,8.02;9,388.35,108.35,35.11,8.06"  xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Dataset</forename>
				<surname>Lorenzo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Fcm</forename>
				<surname>Dfcm</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Sci</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,148.05,168.63,49.79,8.02;9,218.26,168.63,39.60,8.02;9,277.41,168.63,39.60,8.02;9,334.56,168.52,91.15,8.12"  xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Mri</forename>
				<surname>Med</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>T1</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,148.05,178.59,49.79,8.02;9,218.26,178.59,39.60,8.02;9,277.41,178.59,39.60,8.02;9,334.56,178.48,91.15,8.12"  xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Mri</forename>
				<surname>Med</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>T2</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,148.05,188.55,51.29,8.02;9,218.26,188.55,39.60,8.02;9,277.41,188.55,39.60,8.02;9,334.56,188.45,91.15,8.12"  xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Mri</forename>
				<surname>Med</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Pd</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,148.05,198.52,109.80,8.02;9,277.41,198.52,39.60,8.02;9,334.56,198.41,91.15,8.12"  xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Dtmri</forename>
				<forename type="middle">T</forename>
				<surname>Med</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="9,148.05,208.48,109.80,8.02;9,277.41,208.48,39.60,8.02;9,334.56,208.37,88.90,8.12"  xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Dtmri</forename>
				<forename type="middle">A</forename>
				<surname>Med</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,22.72,124.83,54.31,8.42;10,26.49,140.83,246.40,7.13;10,40.76,150.30,232.12,7.13;10,40.76,159.76,126.84,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">High throughput compression of double-precision floating-point data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Burtscher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ratanaworabhan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conf. &apos;07, DCC &apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,169.23,232.12,7.13;10,40.76,178.69,232.12,7.13;10,40.76,188.16,89.21,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">FPC: A high-speed compressor for double-precision floating-point data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Burtscher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ratanaworabhan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Computers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,197.62,232.12,7.13;10,40.76,207.08,17.93,7.13"  xml:id="b10">
	<monogr>
		<title level="m" type="main">Non-Uniform Random Variate Generation</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Devroye</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,216.55,232.12,7.13;10,40.76,226.01,232.12,7.13;10,40.76,235.48,107.21,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiresolutional visualization of evolving distributed simulations using wavelets and MPI</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Du</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Moorhead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization Conference</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,244.94,232.12,7.13;10,40.76,254.41,232.12,7.13;10,40.76,263.87,179.02,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Lossless compression of highvolume numerical data from simulations</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Engelson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Fritzson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Fritzson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Data Compression &apos;00, DCC &apos;00</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page">574</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,273.34,232.12,7.13;10,40.76,282.80,232.12,7.13;10,40.76,292.26,77.26,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Lossless coding of floating point data with JPEG 2000 part 10</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">N</forename>
				<surname>Gamito</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S</forename>
				<surname>Dias</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications of Digital Image Processing XXVII</title>
		<imprint>
			<biblScope unit="volume">5558</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="276" to="287" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,301.73,232.12,7.13;10,40.76,311.19,232.12,7.13;10,40.76,320.66,17.93,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">An efficient algorithm for lossless compression of IEEE float audio</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Ghido</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conf</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="429" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,330.12,232.12,7.13;10,40.76,339.59,171.27,7.13;10,26.49,349.05,56.60,7.13;10,121.53,349.05,31.67,7.13;10,169.09,349.05,17.93,7.13;10,225.46,349.05,47.42,7.13;10,40.76,358.52,154.47,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Lossless compression of digital audio</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hans</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Schafer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine IEEE</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,367.98,232.12,7.13;10,40.76,377.45,232.12,7.13;10,40.76,386.91,159.54,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Out-of-core compression and decompression of large n-dimensional scalar fields</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Ibarria</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Lindstrom</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Rossignac</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Szymczak</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="343" to="348" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,396.37,232.12,7.13;10,40.76,405.84,232.12,7.13;10,40.76,415.30,232.12,7.13;10,40.76,424.77,53.35,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Lossless compression of predicted floating-point geometry</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Isenburg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Lindstrom</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Snoeyink</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Issue: Modelling and Geometry Representations for CAD</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,434.23,232.12,7.13;10,40.76,443.70,232.12,7.13;10,40.76,453.16,230.63,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">The MPEG-4 audio lossless coding (ALS) standard -technology and applications</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Liebchen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Moriya</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Harada</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kamamoto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Reznik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">119th Convention of Audio Engineering Society</title>
		<imprint>
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,462.63,232.12,7.13;10,40.76,472.09,232.12,7.13;10,40.76,481.55,110.47,7.13"  xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast and efficient compression of floatingpoint data</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Lindstrom</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Isenburg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1245" to="1250" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,491.02,232.12,7.13;10,40.76,500.48,232.12,7.13;10,40.76,509.95,147.99,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">An attempt for coloring multichannel MR imaging data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Muraki</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Nakai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kita</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Tsuda</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="274" />
			<date type="published" when="2001-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,519.41,232.12,7.13;10,40.76,528.88,232.12,7.13;10,40.76,538.34,114.23,7.13"  xml:id="b21">
	<monogr>
		<title level="m" type="main">Lossless Compression Handbook, chapter Huffman Coding . Number ISBN 0-12-620861-1 in Communications, Networking, and Multimedia</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Pigeon</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,547.81,232.12,7.13;10,40.76,557.27,232.12,7.13;10,40.76,566.74,75.71,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast lossless compression of scientific floating-point data</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ratanaworabhan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Burtscher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conf. &apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,576.20,232.12,7.13;10,40.76,585.66,232.12,7.13;10,40.76,595.13,57.78,7.13"  xml:id="b23">
	<monogr>
		<title level="m" type="main">SHORTEN: Simple lossless and near-lossless waveform compression</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Robinson</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Cambridge , UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,604.59,232.12,7.13;10,40.76,614.06,224.44,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">A differential lossless image compression scheme</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Sayood</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Anderson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="236" to="241" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,623.52,232.12,7.13;10,40.76,632.99,232.12,7.13;10,40.76,642.45,232.12,7.13;10,40.76,651.92,92.53,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">ISOBAR preconditioner for effective and high-throughput lossless data compression</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Schendel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Jin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Shah</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">S</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S.-H</forename>
				<surname>Ku</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ethier</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Klasky</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Latham</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">B</forename>
				<surname>Ross</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">F</forename>
				<surname>Samatova</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="138" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,661.38,232.12,7.13;10,40.76,670.84,232.12,7.13;10,40.76,680.31,39.75,7.13"  xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive coarsening: simple, effective floating-point compression</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">R</forename>
				<surname>Schroeder</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Conference on Supercomputing, SC &apos;06</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,689.77,232.12,7.13;10,40.76,699.24,232.12,7.13;10,40.76,708.70,183.29,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">A method of adaptive coarsening for compressing scientific datasets</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">M</forename>
				<surname>Shafaat</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">B</forename>
				<surname>Baden</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l. Conf. on Applied Parallel Computing , PARA&apos;06</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="774" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,718.17,232.12,7.13;10,40.76,727.63,232.12,7.13;10,40.76,737.10,134.10,7.13;10,285.12,54.06,250.38,7.13;10,303.38,63.52,232.12,7.13;10,303.38,72.99,176.43,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Lossless progressive transmission of scientific data using biorthogonal wavelet transform Progressive transmission of scientific data using biorthogonal wavelet transform</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Tao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Moorhead ] H</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Tao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Moorhead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Image Processing &apos;94 Visualization Conf. &apos;94, VIS &apos;94</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="373" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,82.45,232.12,7.13;10,303.38,91.92,232.12,7.13;10,303.38,101.38,174.92,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">Compressing floating-point number stream for numerical applications</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Tomari</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Inaba</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Hiraki</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l. Conf. on Networking and Computing &apos;10, ICNC &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,110.85,232.12,7.13;10,303.38,120.31,232.12,7.13;10,303.38,129.78,232.12,7.13;10,303.38,139.24,110.68,7.13"  xml:id="b30">
	<analytic>
		<title level="a" type="main">The application of wavelets to lossless compression and progressive transmission of floating point data in 3-d curvilinear grids</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Trott</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Moorhead</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mcginley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Data Compression Conference &apos;96, DCC &apos;96</title>
		<meeting>. of Data Compression Conference &apos;96, DCC &apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page">458</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,148.70,232.12,7.13;10,303.38,158.17,232.12,7.13;10,303.38,167.63,232.12,7.13;10,303.38,177.10,117.77,7.13"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Wavelets applied to lossless compression and progressive transmission of floating point data in 3-d curvilinear grids</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Trott</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Moorhead</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mcginley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization Conf. &apos;96, VIS &apos;96</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="385" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,186.56,232.12,7.13;10,303.38,196.03,232.12,7.13;10,303.38,205.49,63.68,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">JPEG2000 compliant lossless coding of floating point data</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Usevitch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conf. &apos;05, DCC &apos;05</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="484" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,214.96,232.12,7.13;10,303.38,224.42,203.01,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">JPEG2000 compatible lossless coding of floating-point data</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Usevitch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,233.88,232.12,7.13;10,303.38,243.35,232.12,7.13;10,303.38,252.81,79.18,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">JPEG2000 extensions for bit plane coding of floating point data</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">E</forename>
				<surname>Usevitch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conf. &apos;03, DCC &apos;03</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page">451</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,262.28,232.12,7.13;10,303.38,271.74,184.96,7.13"  xml:id="b35">
	<monogr>
		<title level="m" type="main">JPEG 2000 part 10: Floating point coding</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Wohlberg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Brislawn</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,281.21,232.12,7.13;10,303.38,290.67,232.12,7.13;10,303.38,300.14,163.70,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast lossless compression of seismic floating-point data</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Xie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Qin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l. Forum on Information Technology and Applications &apos;09 of IFITA &apos;09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="235" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,309.60,232.12,7.13;10,303.38,319.07,232.12,7.13;10,303.38,328.53,232.12,7.13;10,303.38,337.99,73.50,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">A lossless audio compression scheme with random access property</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Moriya</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Liebchen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Acoustics, Speech, and Signal Processing &apos;04</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1016" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,347.46,232.12,7.13;10,303.38,356.92,232.12,7.13;10,303.38,366.39,75.49,7.13"  xml:id="b38">
	<analytic>
		<title level="a" type="main">Haptic data transmission based on the prediction and compression</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>You</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">Y</forename>
				<surname>Sung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Communications &apos;08, ICC &apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1824" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,375.85,232.12,7.13;10,303.38,385.32,232.12,7.13;10,303.38,394.78,232.12,7.13;10,303.38,404.25,17.93,7.13"  xml:id="b39">
	<analytic>
		<title level="a" type="main">An entropy coding method for floating-point texture coordinates of 3D mesh</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Cai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Teng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l. Symp. on Circuits and Systems &apos;10, ISCAS &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1835" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
