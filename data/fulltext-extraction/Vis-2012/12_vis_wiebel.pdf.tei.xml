<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T15:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WYSIWYP: What You See Is What You Pick</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Alexander</forename>
								<surname>Wiebel</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Frans</forename>
								<forename type="middle">M</forename>
								<surname>Vos</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">David</forename>
								<surname>Foerster</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Hans-Christian</forename>
								<surname>Hege</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">WYSIWYP: What You See Is What You Pick</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Picking</term>
					<term>volume rendering</term>
					<term>WYSIWYG</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. Visibility-based picking in volume rendering: The user clicks on a visible structure in the volume rendering (left image), a slice crossing this structure and oriented according to the viewing direction is automatically selected and displayed (middle). With an optional second click the volume rendering is switched off to provide an unobstructed view of the slice (right). The slice can then be moved using standard navigation mechanisms. The data set used in this example is from abdominal magnetic resonance imaging. Abstract—Scientists, engineers and physicians are used to analyze 3D data with slice-based visualizations. Radiologists for example are trained to read slices of medical imaging data. Despite the numerous examples of sophisticated 3D rendering techniques, domain experts, who still prefer slice-based visualization do not consider these to be very useful. Since 3D renderings have the advantage of providing an overview at a glance, while 2D depictions better serve detailed analyses, it is of general interest to better combine these methods. Recently there have been attempts to bridge this gap between 2D and 3D renderings. These attempts include specialized techniques for volume picking in medical imaging data that result in repositioning slices. In this paper, we present a new volume picking technique called WYSIWYP (&quot; what you see is what you pick &quot;) that, in contrast to previous work, does not require pre-segmented data or metadata and thus is more generally applicable. The positions picked by our method are solely based on the data itself, the transfer function, and the way the volumetric rendering is perceived by the user. To demonstrate the utility of the proposed method, we apply it to automated positioning of slices in volumetric scalar fields from various application areas. Finally, we present results of a user study in which 3D locations selected by users are compared to those resulting from WYSIWYP. The user study confirms our claim that the resulting positions correlate well with those perceived by the user.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Direct volume rendering (DVR) <ref type="bibr" coords="1,148.46,484.49,14.94,8.33" target="#b27">[30] </ref> is the state-of-the-art for the display of volumetric data from medicine, engineering and natural sciences . As a flexible and versatile tool, it is adaptable to virtually all application problems dealing with 3D scalar fields. The latest hardware developments allow DVR to be used interactively even on consumer type systems. Although this makes it available for the analysis and inspection of volumetric data, physicians, scientists and engineers still rely mainly on the examination of slice-like depictions (including multi-planar reformatting, MPR). Motivated by this fact previous work has already addressed the combination of DVR and MPR repre- sentations <ref type="bibr" coords="1,70.11,584.12,13.75,8.33" target="#b12">[13]</ref>, <ref type="bibr" coords="1,90.16,584.12,13.75,8.33" target="#b13">[14]</ref>, <ref type="bibr" coords="1,110.21,584.12,13.75,8.33" target="#b14">[15]</ref>, <ref type="bibr" coords="1,130.27,584.12,13.75,8.33" target="#b34">[37]</ref>. By providing interaction techniques (commonly called volume picking, point picking or volume pinpointing ) that allow users to pick in the volumetric rendering to adjust a slice, and vice versa to pick on the slice to reorient the DVR, it is possible to integrate DVR in the daily routine of physicians, engineers @BULLET <ref type="bibr" coords="1,43.70,656.49,142.90,7.09"> Alexander Wiebel, David Foerster, and </ref>and scientists dealing with three-dimensional scalar data. DVR can serve as an overview, while the slices are still used for the detailed examinations. With this background in mind, the motivation for the technique presented in this paper is to overcome the three limitations of current methods, which either require metadata, are designed for medical data only, or provide only very basic picking techniques like first-hit or opacity-threshold. The basic assumption of this paper is that the user wants to examine structures that can be made visible with DVR and suitable transfer functions. We aim to introduce a picking technique which takes a visibility-based view and overcomes the mentioned limitations . Picking is probably the most intuitive interaction technique, as it is the technical equivalent of one of the most natural actions in the real world: pointing at something that we see. We present a method, called WYSIWYP ( " what you see is what you pick " ), which enables users to intuitively select spatial positions in volumetric renderings. Target users of WYSIWYP are all users of DVR from all backgrounds. In particular, the main contributions of this paper are @BULLET a new technique allowing users to pick 3D structures visible in their direct volume rendering images, @BULLET the technique's independence of any information apart from the volume data and the transfer function of the direct volume ren- dering, <ref type="figure" coords="2,22.50,252.31,19.17,7.64">Fig. 2</ref> . Problem of first-hit method with zero threshold and " foggy " rendered image. The resulting position of picking the location marked with the crosshairs will be on the bounding box of the dataset instead of on the kidneys because every position in the volume has non-zero opacity. </p><p>@BULLET application of the technique to renderings for any volumetric scalar dataset and all types of transfer functions (e.g. " foggy " looking images), @BULLET its usefulness for navigating (e.g. selecting slices) in the resulting visualizations, @BULLET a comparison of WYSIWYP to other techniques, @BULLET and a user study that supports our claims by comparing user selected 3D locations to locations provided by WYSIWYP. </p><p>We do not aim to replace well established picking techniques, but rather see WYSIWYP as a complementary technique in cases where users would like to pick what they actually see in the rendering instead of what is known to be in the data. We therefore intend to pave the way for further application of DVR in application areas that are still reluctant to adopt this fundamental visualization technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section we review the previous work on picking in volumetric renderings and the combination of direct volume rendering with slices. There has been a lot of work on perception and visibility in visualization research <ref type="bibr" coords="2,71.31,535.89,13.75,8.33" target="#b11">[12]</ref>, <ref type="bibr" coords="2,90.98,535.89,13.75,8.33" target="#b32">[35]</ref>, however, this work is mainly concerned with designing interaction for visualization according to perception principles , rather than analyzing how volumetric visualizations are per- ceived. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Picking</head><p>Direct volume rendering has been around for over twenty years now <ref type="bibr" coords="2,39.81,606.61,14.94,8.33" target="#b27">[30] </ref> and over time has developed into an interactively usable rendering technique <ref type="bibr" coords="2,84.59,616.57,9.52,8.33" target="#b0">[1]</ref>, <ref type="bibr" coords="2,99.45,616.57,13.45,8.33" target="#b15">[16]</ref>), which has resulted in research that aims at facilitating the interaction with volumetric depictions. Volume picking , the interaction technique that is the focus of this article, has been adapted from its well-known predecessor which is used for picking real geometry like surfaces. Accordingly the first volume picking techniques mimicked the surface picking by searching for the first surfacelike structure along the viewing ray passing through the picked screen position. Gobbetti et al. <ref type="bibr" coords="2,114.34,686.31,14.94,8.33" target="#b10">[11] </ref> introduced the most widely used technique . It searches along the ray using the usual compositing scheme (described in Section 3) and stops as soon as the accumulated opacity exceeds a user defined threshold. This means a surface is assumed to be at locations where the opacity threshold is exceeded. The endpoint of the search is returned as a 3D position resulting from the picking. <ref type="figure" coords="2,301.01,233.44,3.32,7.64">3</ref>. Problem of threshold picking method with relatively transparent regions. Example regions (vessel, terminal ileum) are marked with arrows . Due to a high threshold such regions might be missed although they are clearly visible. The cast ray will reach the end of the dataset without the accumulated opacity exceeding the threshold. Thus the ray's exit position will be picked instead of the clearly visible features. </p><p>A simplified version of this approach sets the threshold to zero. This results in selecting the first position in the volume which is not completely transparent (first-hit). Both variants may produce undesirable results. Using a zero threshold will return positions in regions surrounding the features in " foggy " looking renderings (see <ref type="figure" coords="2,491.02,354.06,28.99,8.33">Figure 2</ref>). If on the other hand the threshold is non-zero some relatively transparent, but still visible, regions may be missed (see <ref type="figure" coords="2,443.45,373.98,28.89,8.33">Figure 3</ref>). Another widely used method selects the largest data value along the ray. While yielding perfect results in conjunction with maximum intensity projection renderings, this technique is not suitable for DVR in general. For common DVR it can result in selecting positions that are completely transparent, i.e. deliberately not shown, due to the selected transfer function. Toennies and Derz <ref type="bibr" coords="2,447.32,434.92,14.94,8.33" target="#b31">[34] </ref>present a technique that searches for user-defined data values or user-defined properties of metadata along the ray. In our setting, it suffers from the same problem as the previously described method. Bruckner et al. <ref type="bibr" coords="2,469.79,464.81,10.46,8.33" target="#b6">[7] </ref> select the position along the ray that contributes most to the final pixel. They report that it works well with the special volume rendering technique they used in their BrainGazer system. As the sample contributing most to the final pixel does not necessarily belong to the most visible object , i.e. the group of samples contributing most to the final pixel, their method can yield undesirable results. Furthermore, as it considers only one sample, the method can result in positions that lie at some arbitrary location in the perceivable structures instead of at the front or center of the perceived structure. The following list of visualization tools and their volume picking techniques gives an impression of the use of volumetric picking techniques: MeVisLab <ref type="bibr" coords="2,355.35,585.52,14.94,8.33" target="#b22">[24] </ref>provides a technique selecting the maximum data value as well as opacity threshold-based picking. Voreen <ref type="bibr" coords="2,505.63,595.48,14.95,8.33" target="#b20">[22] </ref>and Avizo <ref type="bibr" coords="2,308.46,605.44,10.46,8.33" target="#b2">[3] </ref>use the first-hit approach. VTK <ref type="bibr" coords="2,434.92,605.44,14.94,8.33" target="#b28">[31] </ref>and thus ParaView <ref type="bibr" coords="2,520.56,605.44,14.94,8.33" target="#b29">[32] </ref>employ the opacity threshold method <ref type="bibr" coords="2,420.76,615.41,13.75,8.33">[25]</ref>. Kohlmann et al. <ref type="bibr" coords="2,355.45,626.53,14.94,8.33" target="#b14">[15] </ref>employ a more sophisticated picking method called contextual picking that is especially tailored to medical data in DICOM <ref type="bibr" coords="2,317.21,646.46,10.46,8.33" target="#b3">[4] </ref>format. It uses the meta information given in the DICOM files to deduce which anatomical parts of the volumetric image the user intends to pick (e.g. angiography→vessels). Very few, initially user specified, ray profile samples are matched against the data curve along the viewing ray to find the intended structures. As the matching identifies the approximate extent of the picked structure, Kohlmann et al. are able to provide picking positions either on the front of the structure or in its center. Malik et al. <ref type="bibr" coords="2,386.90,716.20,14.94,8.33">[18] </ref> use ray-profiles similarly in a different context, the division of the data into different peelable layers. Like our method, they use the derivatives of a ray-profile to find " features " along the ray. However, they search for features in the data whereas our method searches for features in the visible rendering (profile of data vs. profile of accumulated opacity along the ray). Additionally, in contrast to our method their transition points are extrema and are thus easily detected as zeros of the first derivative. Another peeling technique somewhat related to the present work is the so-called opacity peeling by Rezk-Salama and Kolb <ref type="bibr" coords="3,158.03,112.87,13.75,8.33" target="#b26">[29]</ref>. Opacity peeling uses several rounds of opacity accumulation (each up to some threshold) to render layers originally occluded behind other rendered layers of the data. Users can interactively select the layer they would like to see. In contrast to most techniques discussed above, which only select one position in 3D, Owada et al. <ref type="bibr" coords="3,152.29,163.10,14.94,8.33" target="#b23">[26] </ref>present a method that manages to locate the intended location of a 2D stroke in 3D space. Their technique uses the determined 3D line as the basis for segmenting a 3D object marked by the stroke and thus the 3D line. For this task the stroke has to follow the border or contour of the object visible in the rendering. Determining the location of the 3D line translates to the task of finding the 3D line that is a projection of the 2D stroke and fits contours in the data best (dot product of normal vector of extruded stroke and gradient in data). We consider their aim and method to be different from ours for two reasons: Firstly it only works at contours of objects, whereas our method is applicable to any position in the rendering . Secondly, their method segments objects whose contour best fits the stroke. Such objects are not necessarily those that we aim at, i.e. the most visible ones. In the same spirit as Owada's technique for DVR, Yu et al. <ref type="bibr" coords="3,85.34,302.58,14.94,8.33" target="#b37">[40] </ref>describe a lasso-based technique to spatially select 3D subsets in massive particle cloud datasets. One of the most recent papers on picking in volumetric rendering we are aware of is by Peng et al. <ref type="bibr" coords="3,155.97,332.89,13.75,8.33" target="#b24">[27]</ref> . They use two different techniques: (1) a one-click method restricted to their data with a blob-like structure where it is easy to guess the desired position as the center of the blob hit by the viewing ray; (2) a two-click method where the user clicks on the desired location from two different viewing directions . The picked position is then the (fuzzy) intersection of the two viewing rays. This method works for arbitrary data, but the desired location has to be visible from both viewing directions. The method we present is superior to Peng's method in so far as it provides picking in renderings of arbitrary data with only one mouse click (or a similar pointing action) from one viewing direction. Picking is also a very important interaction technique in virtual environments . Argelaguet and Andujar <ref type="bibr" coords="3,164.35,452.86,10.46,8.33" target="#b1">[2] </ref>describe one method and give a good overview of other techniques with references in their paper. The main difference between all the techniques mentioned by Argelaguet and Andujar and our work is that the picking is targeted to real geometric objects and not volume renderings. An extensive list of different three-dimensional rendering techniques and how picking can be implemented in their context can be found in a recent technical report by Wiebel et al. <ref type="bibr" coords="3,210.71,523.03,37.74,8.33">[39, Sec.3]</ref>. The list also contains comments on how exact the respective picking methods are. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Combining Slices and DVR</head><p>Many techniques dealing with isosurfaces in volumetric data provide picking on slices for reorienting the isosurface to a view point offering good visibility of the selected position. Picking on the surface is often used to change the position of a slice in the data. Recently this has been combined with picking in volume renderings by Kohlmann et al. <ref type="bibr" coords="3,43.52,626.11,13.75,8.33" target="#b12">[13]</ref>, <ref type="bibr" coords="3,64.29,626.11,14.94,8.33" target="#b13">[14] </ref>and others <ref type="bibr" coords="3,124.03,626.11,9.52,8.33" target="#b5">[6]</ref>, <ref type="bibr" coords="3,140.30,626.11,13.75,8.33" target="#b34">[37]</ref>. In their work picking on a slice results in a reorientation of the volume rendering and a local adaption of the opacity (transfer function) such that the view on the selected position is improved. For the reverse direction, i.e. for picking in the volume and adapting the position of the slice, they use either the firsthit or their contextual picking approach <ref type="bibr" coords="3,175.91,675.92,14.94,8.33" target="#b14">[15] </ref>that we described above. In their framework the selected position can also be used for placing labels. In addition to placing slices according to a certain user interaction, there are also methods which allow the automatic placement of cross sections at interesting locations in the 3D data. See for example the method by Mori et al. <ref type="bibr" coords="3,111.70,736.12,13.75,8.33" target="#b21">[23]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metadata</head><p>Medical datasets often come with metadata that can help to infer the position a user intends to pick. If metadata are available, it makes sense to exploit the information provided by them, and, in fact, some of the above described methods do so <ref type="bibr" coords="3,440.28,96.16,13.75,8.33" target="#b14">[15]</ref>, <ref type="bibr" coords="3,461.34,96.16,13.75,8.33" target="#b31">[34]</ref>. We nevertheless consider WYSIWYP's independence of metadata to be one of its important features because many datasets, particularly from outside the medical domain, come without metadata. In such cases the metadatabased approaches can not be applied. Our method, in contrast, is applicable to any data. Examples of fields where the data often lacks meta information are flow simulations as well as any types of derived density fields e.g. from a 3D scatter plot. For medical data with suitable meta information we suggest allowing the users to interactively choose whether they would like to pick only positions with an a-priori known meaning (metadata) or if they would like to pick the features they see in the actual rendering (WYSI- WYP). Their choice will vary with the task at hand. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>To understand the proposed picking approach, a basic understanding of the volume rendering procedure is necessary. We therefore give a summary of the most relevant aspects and in this way introduce the notation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Volume Rendering Integral</head><p>As DVR tries to make volumetric data directly visible to the user, its most natural implementation is casting rays along the viewing direction through the volume and accumulating color information for the values of the volumetric data along these rays. The density of the rays and the samples along the rays are chosen to cover the volume sufficiently . The color information for the data values is determined by the transfer function. The mentioned accumulation can be formalized mathematically by the volume rendering integral <ref type="bibr" coords="3,469.61,376.20,13.75,8.33" target="#b9">[10]</ref>, <ref type="bibr" coords="3,488.76,376.20,13.75,8.33" target="#b19">[21]</ref>, <ref type="bibr" coords="3,507.92,376.20,13.75,8.33" target="#b25">[28]</ref>, <ref type="bibr" coords="3,527.07,376.20,13.95,8.33" target="#b27">[30]</ref></p><formula>: I(r max ) = I 0 e rmax r 0 τ(t)dt + r max r 0 Q(s)e rmax s τ(t)dt ds </formula><p>In this equation, I is the intensity in a color channel resulting from accumulating the color for a certain distance along the ray. [r 0 , r max ] is an interval along the ray, with r max being at the eye point and r 0 at the back end of the volume; s is a parameter in this interval; τ is the attenuation coefficient and Q the source term describing emission for a certain sample. For a numerical approximation the volume rendering integral has to be discretized: compositing (accumulation) is performed for a finite number of samples along the ray. The iterative computation of the discretized version in a front-to-back fashion can be denoted as follows <ref type="bibr" coords="3,323.04,519.68,9.52,8.33" target="#b8">[9]</ref>, <ref type="bibr" coords="3,337.98,519.68,13.95,8.33" target="#b9">[10]</ref>: </p><formula>c acc n+1 = c acc n + (1 − α acc n )c src n (1) α acc n+1 = α acc n + (1 − α acc n )α src n (2) </formula><p>Here, c denotes color, α denotes opacity, n denotes the step number, acc indicates the accumulated values and src indicates values of the transfer function for the data found at the current sample position. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compositing</head><p>Equation 2 describes the steps that have to be performed to compute the opacity at a certain sample on the ray. This opacity is accumulated along the ray up to that position. It determines how much the final pixel value is influenced by the values of the samples on the ray that lie behind the current sample. Later in this paper we will discuss how α acc varies along the ray. Therefore it is worth noting three important properties of α acc that can be easily deduced from Equation 2 by mathematical induction: first, the accumulated opacity will never be larger than one. Second, α acc is monotonically increasing along the ray, and, third, α acc n ∈ <ref type="bibr" coords="3,374.86,705.89,9.46,8.97">[0,</ref><ref type="bibr" coords="3,385.33,706.24,6.14,8.33" target="#b0"> 1]</ref>. Another important fact about the compositing in Equation 2 is that with a change of the sampling density along the ray the series of accumulated opacity changes. More samples result in a faster increasing The gray areas are the intervals used in equation 3. The parameters i 0 and i max in the same equation correspond to the left and right borders of the gray regions respectively. Note that the jump denoted c is steeper than jump b, but that b is higher than c. The increase in interval d represents the feature with largest extent while contributing only a small amount to the overall opacity. opacity compared to the location of the samples along the ray. This can be compensated by scaling α src with respect to the sample density (opacity correction) <ref type="bibr" coords="4,96.45,269.74,13.75,8.33" target="#b9">[10]</ref>, <ref type="bibr" coords="4,115.92,269.74,13.75,8.33" target="#b33">[36]</ref>. Opacity correction is also necessary if non-equidistant samples are used. For sake of simplicity we restrict all explanations to equidistant samples throughout this paper. All presented methods are easily extendable to the general case. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WYSIWYP</head><p> In this section we give a detailed description of the new visibilityoriented picking technique WYSIWYP. A comparison with previous techniques which emphasizes its advantages is provided in Section 6. The overall procedure of all picking techniques is similar. First, the user clicks on a position in the screen. This position and the user's viewing direction are transformed from screen coordinates into world coordinates. The result is then used to cast a ray through the scene (see <ref type="figure" coords="4,39.30,403.52,29.01,8.33">Figure 6</ref>). Along this ray a number of samples are used to gather information about the volume data. Finally certain criteria are applied to the gathered data to determine the position resulting from the pick. This last step is the one that the new method focuses on. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visibility-Oriented Picking Criterion</head><p>At the heart of the new technique are the characteristics of the values of α acc along the viewing ray, i.e. the discretized version of the opacity accumulation described by the volume rendering integral. Previous work on volume rendering already noted that opacity (resp. accumulated opacity) along the ray is strongly correlated with the visibility of positions (resp. regions or features) in the volume <ref type="bibr" coords="4,216.54,516.20,40.46,8.33">[5, Sec.3.2]</ref>, <ref type="bibr" coords="4,263.17,516.20,9.71,8.33;4,22.50,526.16,21.34,8.33">[8, Sec.3]</ref>, <ref type="bibr" coords="4,50.04,526.16,38.18,8.33">[38, Sec.3]</ref>. Consequently, we hypothesize (confirmed by our experiments, Section 5) that the user usually perceives those features at a screen position which contribute the highest amount of opacity or in other words, the highest jump of α acc along the ray (<ref type="figure" coords="4,221.58,556.05,28.10,8.33" target="#fig_1">Figure 4</ref>). The amount of opacity contribution of a spatial feature determines its influence on the final color of the pixel and thus defines which feature is perceived. This means that an object's visibility does not only depend on the optical properties of a single location but on the properties of a number of consecutive locations. Furthermore as all our examples confirm, it does not seem to depend on the steepness of the increasing opacity, but on how much the opacity increases in an interval of consecutive samples, i.e. on how large the contribution of the interval to the final is. An example of this effect can be seen in <ref type="figure" coords="4,220.08,645.71,33.80,8.33">Figure 12</ref> . Finally , it is sensible to consider the changes of opacity for selecting the picked position because high opacity is usually assigned to important features during transfer function design, in short: opacity correlates to importance. <ref type="figure" coords="4,32.46,695.90,31.55,8.33" target="#fig_1">Figure 4</ref>illustrates the criterion. Here, the largest jump can be found in region b, while the the steepest jump, appears in region c. Consequently, the region used to determine the picking position is b. To determine the highest jump, the first task is to define the regions of <ref type="figure" coords="4,54.97,736.12,30.68,8.33" target="#fig_1">Figure 4</ref></p><formula>as intervals I = [i 0 , i max ] ⊂ [r 0 , r max ] </formula><p>along the ray. <ref type="figure" coords="4,285.12,201.94,19.22,7.64">Fig. 5</ref>. Detection of borders of jumps via first and second derivative of α acc along the ray (parameter s). The blue curve represents the accumulated opacity, the red curve its first derivative β acc and the green curve its second derivative γ acc . The dashed gray lines mark the detected borders . The curves are only sketched for illustration purposes and thus are only qualitatively correct. Thereafter, the difference between α acc at the start and the end of the interval, i.e. the jump j as </p><formula>j = α acc (i 0 ) − α acc (i max ), </formula><formula>(3) </formula><p>has to be computed. Extracting the boundaries i 0 and i max of the jumps is similar to the task of edge detection <ref type="bibr" coords="4,425.04,345.31,14.94,8.33" target="#b17">[19] </ref> in one dimension. Consequently , our method for detecting the boundaries is inspired by computer vision methods <ref type="bibr" coords="4,365.54,365.24,14.94,8.33" target="#b18">[20] </ref>and incorporates the second derivative of α acc . We denote the first derivative of α acc as β acc and the second derivative as γ acc . <ref type="figure" coords="4,353.08,385.16,30.39,8.33">Figure 5</ref>illustrates the idea behind our method for extracting the interval boundaries. In principle, the boundaries are the positions where the second derivative γ acc crosses zero from below, i.e. from negative to positive values. This criterion, however, is only reliable if α acc is strictly increasing. As α acc has plateau like regions and thus γ acc has extended regions where it is constantly zero, the criterion is adapted as follows. The lower bounds i 0 of such intervals are the positions where accumulated opacity starts to grow stronger, that is where γ acc becomes positive after being negative or zero. The criterion for the upper bounds i max is that α acc stops decreasing again. For γ acc this means that it becomes zero or positive after being negative. After having determined the interval boundaries and having computed all jumps j one simply selects the interval with the largest jump j. This is the interval dominantly perceived at the picked screen posi- tion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Front vs. Center of Perceived Feature</head><p>The criterion described above does not directly yield a position. It only yields the interval seen most prominently along the viewing ray through the picked screen position. This is not a problem but rather an advantage of the criterion because it allows to choose the final position according to the task at hand by application of further criteria. For labeling features in the volume rendering the front most position of the feature is of interest, whereas for repositioning slices to display most of the picked feature, the center of the feature is of interest. This has also been noted by Kohlmann et al. <ref type="bibr" coords="4,424.63,644.10,14.94,8.33" target="#b14">[15] </ref>and has been implemented for their contextual picking. For WYSIWYP determining the center and the front position is straightforward because the front and back positions are implicitly computed as the start and end of the jump interval. A feature's front is simply the first position i 0 of the interval corresponding to the largest jump. A feature's approximate center is the center i c of the interval, i.e. i c = 1 2 (i 0 + i max ). Of course,other task and data specific criteria are conceivable. However, in our applications the described methods proved to be sufficient. <ref type="figure" coords="5,31.50,121.74,19.26,7.64">Fig. 6</ref>. Ray casting for WYSIWYP in DVR. The dashed line is the part of the ray that is determined by common object picking of the proxy geometry. Samples on the solid part of the ray are used for WYSIWYP. The dotted line is outside the bounding cube of the dataset. Stepping along the ray will be stopped before reaching it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p> In our implementation, casting the ray through the volume is realized by a combination of usual surface picking and straightforward ray casting on the CPU. We draw a transparent bounding cube (proxy geometry) around the volume rendered data in the scene. The standard geometry picking mechanism of the scene graph is then used to determine the position where the viewing ray intersects the proxy geometry and enters the data volume (see <ref type="figure" coords="5,173.24,264.00,28.78,8.33">Figure 6</ref>). The direction of the ray is computed as the difference between the intersection point and the camera position or eye point. With this information we can step through the volume and gather the desired information. As soon as a step gets outside the dataset's bounding box we stop gathering information . The information obtained for each step is the data value d(x) at the position and the result of applying the transfer function to this data value, i.e. color C src n and opacity α src n . Using Equation 2, these values are accumulated to provide the values of α acc along the ray. At this point it is worth noting that the parameters of the DVR implementation and of the procedures described above need to be coordinated . This is because usual compositing (Equation 2 and, e.g. refer- ence <ref type="bibr" coords="5,50.44,383.62,14.34,8.33" target="#b9">[10]</ref>) does not consider the distance of the samples. Thus, if the sample distances of DVR and picking are not equal, the accumulated opacity may vary differently along the viewing ray. The following example demonstrates the possible issues. Consider the DVR using half the step size, i.e. twice as many samples, as the ray for the picking. The easiest way to achieve consistency between DVR and picking is to use the same number of steps and the same step size. If this cannot be achieved, then the previously mentioned opacity correction needs to be applied during compositing. <ref type="figure" coords="5,31.50,494.19,34.14,8.33">Figure 12</ref>, an example that will be described in detail in Section 6, suggests that WYSIWYP for transparent polygonal surfaces can be implemented analogous to the method for DVR. One simply traces a ray through the volume, accumulates or composits the opacities of the different polygons that are hit by the ray, and finally selects the intersection of the ray with the polygon that has the largest opacity contribution after compositing. Computing derivatives of α acc is not necessary in this case because the intervals are the intersections of the ray with the surfaces, and thus infinitesimally small. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transparent Surfaces</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We conducted a user study to show the suitability and appropriateness of the presented picking method and to further support the claim that the parts contributing the most opacity are observed by the user. In this section, we first describe the experimental setup of the study that led to the results we present afterwards. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p> We set up the user study to determine the 3D position that the participants perceive at a certain 2D screen position. The positions recorded throughout the study can then be compared to the position the presented picking method selects automatically. The general set-up of the test cases consists of a volume rendered image, small crosshairs indicating the currently considered position and a method to let the participants specify the actual 3D position they <ref type="figure" coords="5,294.12,183.99,19.92,7.64">Fig. 7</ref>. Two images (taken directly from the user study) showing the steps of the evaluation. In the first image a scene showing capillary blood vessels in the brain with a position marked by crosshairs is given. The dataset is courtesy of MPI f ¨ ur Biologische Kybernetik T ¨ ubingen, AG Logothetis (Bruno Weber). The second image shows the situation after a participant has moved a slice to a position he/she perceives to intersect with the position marked with the crosshairs. In order to provide the best view of the marked features for the participant, the crosshairs are intentionally very thin. If the crosshairs are not visible to you please zoom in using an electronic version of the paper. perceive. The latter is a slice that can be moved through the rendering by manipulating a slider until the slice intersects the 3D position perceived at the 2D location marked by the crosshairs. The setup is exemplified in <ref type="figure" coords="5,348.53,330.84,29.46,8.33">Figure 7</ref>. The left image shows the initial state where only the rendering and the crosshairs are visible. The second image also contains the slice that has been placed by a participant. The slice appears as soon as the participant moves the slider the first time for a certain test case. Usually participants do not place the slice by a single slider movement, but move it back and forth in order to select the visible position in an exploratory process. In this process the part of the volume lying behind the slice is not visible anymore. This simplifies the task of position selection, as the participant can move the slice to the position where the observed feature just disappears. Please note that this does not influence the position which is actually marked as being perceived. Furthermore, simplifying the specification of positions does not influence the study because it is not our objective to judge the depth perception of the participants. Instead the intention is to get positions that can be compared with positions that have been automatically selected by picking. Excluding the pretests, 20 people participated in the study. Their ages ranged from 18 to 55 years (mean=30.95, median=32). Most participants were familiar with volumetric rendering at least superficially (14 professional, 3 student, 3 non-professional) and they had no known vision deficiencies or wore appropriate glasses or contact lenses. 7 females and 13 males participated in the study. We chose nine datasets from different domains for the user study. Two datasets stem from abdominal MRI measurements (<ref type="figure" coords="5,501.53,560.56,26.15,8.33">Figures</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to Picking Method</head><p>As mentioned before, the aim of the user study was to find out how close the position chosen by WYSIWYP is to the position of structures that humans perceive as essential. To quantify the answers to this question we performed some basic statistical analysis for each test case. As the selected positions of the participants naturally vary to some degree we computed the average µ of the positions chosen by the participants. This is simple and appropriate as all positions lie on the viewing ray. The average position µ is then compared to the position selected by WYSIWYP by computing their distance. The quality of the WYSIWYP position is then established by comparing its distance to the standard deviation σ of the distances of the positions picked by the participants. We consider WYSIWYP to perform well if the picked position lies in an interval of </p><formula>[µ − σ , µ + σ ] around </formula><p>the average position µ. The actual analysis of the recorded experimental data and the positions picked by WYSIWYP yielded a correspondence of WYSIWYP and the perceived positions: For 89% (32 of 36) of the test cases the picked position lay inside the desired interval I of one standard deviation (see <ref type="figure" coords="6,60.52,514.56,30.72,8.33" target="#fig_4">Figure 9</ref> for an example). In four cases where the participants' choices varied strongly compared to the dataset size, the picking also did not perform well. See <ref type="figure" coords="6,148.53,534.49,30.30,8.33" target="#fig_3">Figure 8</ref>for an example where the choice of the participants varied strongly. The exact numbers for the experiments can be found in a table in the supplemental material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS, COMPARISON AND DISCUSSION</head><p>This paper introduces a new picking technique that does not need any metadata, can be applied to volumetric scalar fields from all application domains and nevertheless picks the visible 3D location corresponding to a selected 2D position. To demonstrate these characteristics we applied the method to a selection of very different volumetric scalar fields. As the most sophisticated previous picking techniques come from the area of medical visualization, an abdominal MRI scan with intravenous contrast is our first example. <ref type="figure" coords="6,192.88,656.42,30.57,8.33">Figure 1</ref>shows how a position in a DVR image (DVRI) is picked, how a slice with the appropriate orientation is positioned so that it cuts the picked vessel, and how the slice can be subsequently used to examine the vessel in detail. The DVR is hidden in the final image to provide a completely free view of the slice. <ref type="figure" coords="6,91.14,706.24,57.96,8.33">Figures 2 and 3</ref>show DVRIs of the same dataset with different transfer functions. As adumbrated before, thresholdbased picking fails for the DVRI in <ref type="figure" coords="6,150.82,726.16,30.14,8.33">Figure 3</ref>if the threshold is chosen too high and for the DVRI in <ref type="figure" coords="6,125.31,736.13,29.68,8.33">Figure 2</ref>if the threshold is chosen too low (e.g. zero, first-hit). In the first case the cast ray would go through the volume without identifying any position as picked. In the second case the ray tracing would stop as soon as it reaches the bounding box of the dataset because opacity can be found everywhere. WYSIWYP can handle the DVRs of all three transfer functions correctly. Please see the accompanying video for another comparison of the techniques using the fuel dataset which is described later in this section. The video also compares WYSIWYP to picking the position with the highest contribution to the final pixel. The comparison shows that the latter method (known from Bruckner et al. <ref type="bibr" coords="6,423.30,393.95,10.08,8.33" target="#b6">[7]</ref>) has problems in situations where WYSIWYP performs well. The second example dataset comes from a numerical simulation of flow around an ellipsoidal body. The images in <ref type="figure" coords="6,458.39,425.56,34.87,8.33">Figure 11</ref>show DVR of the vorticity ∇ × v of the velocity vector field v. The images have been rotated for illustration purposes so that the flow comes from below . Like for the MRI dataset, the steps of WYSIWYP are shown. Additionally, the curves of the accumulated opacity α acc illustrate the interval selection. While approaches employing metadata, like contextual picking <ref type="bibr" coords="6,330.19,485.34,13.75,8.33" target="#b14">[15]</ref>, are possibly applicable to the MRI dataset, they are not applicable for the flow field as there are no clearly defined structures that can be named and matched for detection. Vortices might be considered as such structures but there is still no vortex definition commonly agreed upon (see e.g. Lugt <ref type="bibr" coords="6,423.07,525.19,13.45,8.33" target="#b16">[17]</ref>). A synthesized scalar function increasing from two locations provides data for the third example. The transfer function used for rendering produces two balls that are visible in the DVRIs of <ref type="figure" coords="6,498.25,556.80,33.51,8.33">Figure 12</ref>. This synthetic example has been included because the shapes of the α acc , β acc and γ acc curves are clearly discernable. The principle of choosing the highest jump is nicely visible in the lower right image of <ref type="figure" coords="6,285.12,596.65,33.67,8.33">Figure 12</ref>. The first, last and central location of the selected interval are marked by gray bars. As our picking criterion suggests, the marks coincide with zero crossings of γ acc . Although the two jumps corresponding to the first two peaks of β acc are steeper, the criterion selects the marked interval because it exhibits the highest jump. The result is that a position in the shell of the ball in the background is picked through two transparent shell areas of the ball in front of it. This example also shows that material boundaries parallel to the viewer, which are usually well perceived, are easily picked because they are represented by a long and strong increase in opacity and thus a high jump of accumulated opacity. Finally, this rendering is another example where choosing the position with the highest contribution to the final pixel fails. It will select the front-most shell. The steepness of the first jump in <ref type="figure" coords="6,348.00,726.16,35.52,8.33">Figure 12</ref>confirms that single samples in this area have a high contribution to the final pixel.  The last dataset we present stems from a simulation of fuel injection into a combustion chamber. It contains density values of gas: the higher the density value, the less air. We use this dataset for comparing different picking techniques in <ref type="figure" coords="7,157.87,384.67,33.29,8.33" target="#fig_5">Figure 10</ref> . The figure shows snapshots from the video accompanying this paper. <ref type="figure" coords="7,200.19,394.63,33.05,8.33" target="#fig_5">Figure 10</ref>(a) shows the initial state of the rendering and the mouse pointer over the position that will be used for picking with the different techniques in the fol- lowing. <ref type="figure" coords="7,63.72,424.52,34.25,8.33" target="#fig_5">Figure 10</ref>(b) shows the situation directly after picking with the first-hit strategy. The slice is situated where nothing interesting is visible because the picked position is on the border of the dataset (see <ref type="figure" coords="7,49.05,454.41,34.28,8.33" target="#fig_5">Figure 10</ref>(b) for another perspective). This is the case because there is opacity in a large area of the DVR. In <ref type="figure" coords="7,203.19,464.37,34.00,8.33" target="#fig_5">Figure 10</ref>(d) the slice is on the front of the dataset because no position could be found by the threshold-based picking. The last two images (10(e), 10(f)) show the picking with WYSIWYP. The slice in <ref type="figure" coords="7,189.56,494.26,34.24,8.33" target="#fig_5">Figure 10</ref>(e) cuts exactly through the intended position. Finally, <ref type="figure" coords="7,172.18,504.23,32.96,8.33" target="#fig_5">Figure 10</ref> (f) shows the rendering from a different position in order to demonstrate that the small gray sphere indicating the picked position is located at the desired position. It becomes clear that only WYSIWYP can reliably yield the desired position in this dataset for which no metadata are available. The presented technique is intrinsically interactive and threedimensional , and thus hard to demonstrate in static 2D images. Therefore , a video with a live demonstration using some of the described and several additional datasets accompanies this paper. </p><formula>(a) (b) (c) (d) (e) (f) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations</head><p>As may be deduced from the images throughout this paper, the proposed method deals with volume rendering using the standard emission-absorption model. This does not impose any constraint on the type of transfer function (e.g. one-dimensional vs. multidimensional ). However, we did not investigate how the method deals with images in which local illumination has been applied after evaluating the transfer function. Perception theory <ref type="bibr" coords="7,190.53,676.35,14.94,8.33" target="#b17">[19] </ref>tells us that lighting, color and context influence the perception of transparency. Therefore we expect that the method will have to be extended to correctly handle volume rendering using local illumination. It is probable that complex computer vision methods are not required because more information than only the resulting image is available. The data and the transfer function are highly valuable information for the picking task. </p><p>Our current implementation does not ensure that close positions in screen space also result in close 3D locations. In noisy data sets this might be desirable. A solution one could imagine in conjunction with the handling of local illumination is the following: The 3D locations corresponding to positions lying next to (probably on pixel base) the picked position can be taken into account. In other words, one could cast additional rays for pixels around the picked position. An outlier filtering for the resulting 3D locations could then avoid rapid changes in the selected depth. Overall, this problem will only become relevant if we allow the user to drag the mouse while continuously updating the picked position and thus the slice. However, this is a quite unusual scenario for picking. Furthermore, it somehow contradicts the idea of picking what is visible. Nevertheless, we plan to incorporate the presented technique into a methods that allows to trace visible structures by dragging the mouse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p> We have presented a method to allow users to pick positions in volumetric renderings of three-dimensional data in a WYSIWYG type of interaction. Users can select, in an intuitive manner, the 3D position of structures that they really perceive in the rendering. In contrast to previous methods, the described approach is rendering-centered and is thus applicable for any type of volume rendered data. It only uses the transfer function of the volume rendering together with the data itself to determine the opacity and thus the visible structures along the viewing direction. Observable structures are characterized by large jumps in the accumulated opacity; the picked structure corresponds to the largest jump of the accumulated opacity. We emphasized the fact that no metadata is needed by demonstrating the method with data from a flow simulation where no metadata is available. The usefulness of the proposed technique for medical data has been shown by its application to an abdominal MRI scan, and the claims are supported by a user study. The application to flow and other data shows that the method is useful far beyond the medical domain. As mentioned before, WYSIWYP has been developed for volume rendering without local illumination; research into picking in illuminated direct volume rendering is one of the next steps. Furthermore, we are already working on incorporating information from rays in the vicinity of the ray through the picked position. <ref type="figure" coords="8,22.50,296.40,23.00,7.64">Fig. 11</ref>. Picking in DVR of vorticity field of flow around an ellipsoid. The images in the upper row show the picking process (picked position as red dot). The lower left image shows the volume rendered data from a different perspective. The lower right shows the accumulated opacity and its derivatives along the ray. Curves of derivatives are scaled by a factor of ten, but changes in second derivative are still hardly visible. The data is courtesy of Markus R ¨ utten, DLR G ¨ ottingen. <ref type="figure" coords="8,22.50,686.09,23.45,7.64">Fig. 12</ref>. The images in the upper row show the picking process (red dot=picked position) in a synthetic dataset consisting of two spheres. The picked 3D position can be inferred from the position and the gray-scale map of the slice. It shows that a position on the border of the sphere in the back is picked. This fits with what can be seen in the upper left image where the border of the sphere in the back shines through the second sphere in front of it. The lower left image shows the DVR from a different perspective to show the spatial relation of the spheres. The lower right shows the accumulated opacity and its derivatives along the ray. Curves of derivatives are scaled by a factor of ten. It is clear that the highest jump corresponding to the border of the sphere in the back is chosen from the intervals determined by the second derivative. </p><p>We believe that the proposed technique can help to pave the way for further application of DVR in application areas that are still reluctant to adopt this fundamental visualization technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>The research leading to these results has received funding from the European Community's Seventh Framework Programme (FP7/2007- 2013) of the VIGOR++ Project under grant agreement nr. 270379. The information presented is provided as is and no guarantee or warranty is given that the information is fit for any particular purpose. The user thereof uses the information at its sole risk and liability. The opinions expressed in the document are of the authors only and in no way reflect the European Commission's opinions. All visualizations were produced with Amira <ref type="bibr" coords="9,112.07,189.00,13.75,8.33" target="#b30">[33]</ref>. The authors are grateful to the reviewers for their valuable comments that helped to improve the paper. Many thanks go to Jesica Makanyanga for proofreading. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,285.12,233.44,250.36,7.64;2,285.12,242.90,250.38,7.64;2,285.12,252.37,250.37,7.64;2,285.12,261.84,250.36,7.64;2,285.12,271.29,250.38,7.64;2,285.12,280.76,223.61,7.64"><head>Fig. </head><figDesc>Fig. 3. Problem of threshold picking method with relatively transparent regions. Example regions (vessel, terminal ileum) are marked with arrows . Due to a high threshold such regions might be missed although they are clearly visible. The cast ray will reach the end of the dataset without the accumulated opacity exceeding the threshold. Thus the ray's exit position will be picked instead of the clearly visible features. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,22.50,160.08,250.37,8.77;4,22.50,170.61,249.88,8.47;4,22.50,180.08,250.37,8.34;4,22.50,189.55,250.37,7.64;4,22.50,199.01,250.37,7.64;4,22.50,208.47,250.39,7.64;4,22.50,217.93,102.90,7.64"><head>Fig. 4. </head><figDesc>Fig. 4. Jumps in accumulated opacity α acc along the ray (parameter s). The gray areas are the intervals used in equation 3. The parameters i 0 and i max in the same equation correspond to the left and right borders of the gray regions respectively. Note that the jump denoted c is steeper than jump b, but that b is higher than c. The increase in interval d represents the feature with largest extent while contributing only a small amount to the overall opacity. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,530.42,560.56,14.08,8.33;5,294.12,570.52,250.38,8.33;5,294.12,578.65,250.38,10.16;5,294.12,590.45,250.37,8.33;5,294.12,600.41,250.37,8.33;5,294.12,608.54,250.38,10.16;5,294.12,620.34,250.38,8.33;5,294.12,628.46,250.38,10.17;5,294.12,640.26,250.38,8.33;5,294.12,650.23,250.37,8.33;5,294.12,660.19,250.37,8.33;5,294.12,670.15,250.38,8.33;5,294.12,680.12,250.37,8.33;5,294.12,690.08,250.37,8.33;5,294.12,699.69,250.37,8.97"><head></head><figDesc>1, 2 and 3) conducted in the VIGOR++ project. They were used for four resp. three test cases. Three datasets from the volvis repository 1 , i.e. Bonsai (see video), Hydrogen Atom and Nucleon (Figure 8), were used for ten test cases. From the downloadable example datasets of the osirix viewer 2 we used FELIX (cerebral aneurysm) for five test cases. Six more test cases use the mouseCTdata that comes with free trial versions of amira 3 . The dataset shown in Figure 7 was used in six test cases. The last dataset, used for two test cases, comes from a CT capturing bone structure. In total, we used 36 test cases. For each test case we defined a transfer function, a viewing direction and a position of interest. We mostly varied the viewing direction and the position of interest. For some datasets the transfer functions varied between test cases. All participants performed the same 36 tests. It took them an average of 18.01 minutes (σ =6.49 minutes, min=9.20 </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,22.50,233.26,250.37,7.64;6,22.50,242.73,250.38,7.64;6,22.50,252.20,25.01,7.64"><head>Fig. 8. </head><figDesc>Fig. 8. The location of the position marked by the crosshairs is vague. Participants of the study marked very different locations along the viewing ray. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,285.12,250.80,250.37,7.64;6,285.12,260.27,250.37,7.64;6,285.12,269.73,246.21,7.64"><head>Fig. 9. </head><figDesc>Fig. 9. Histogram of the distances from the positions selected by the participants to the average selected position in test case number 11. The blue bar shows the distance of the position chosen by WYSIWYP. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,31.50,315.76,513.00,7.64;7,31.50,325.23,50.50,7.64"><head>Fig. 10. </head><figDesc>Fig. 10. Snapshots from the video accompanying this paper show a comparison of picking techniques in the fuel dataset. For a detailed discussion see Section 6. </figDesc></figure>

			<note place="foot" n="1"> http://www.volvis.org 2 http://www.osirix-viewer.com/datasets/ 3 http://www.amira.com/downloads/trial.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,49.76,244.68,232.11,7.40;9,49.76,254.15,232.12,7.40;9,49.76,263.61,232.14,7.40;9,49.76,273.08,33.88,7.40"  xml:id="b0">
	<analytic>
		<title level="a" type="main">GPU accelerated direct volume rendering on an interactive light field display</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Agus</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gobbetti</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Iglesias Guitián</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Marton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Pintore</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,282.54,232.12,7.40;9,49.76,292.00,232.11,7.40;9,49.76,301.47,78.40,7.40"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient 3d pointing selection in cluttered virtual environments</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Argelaguet</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Andujar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="34" to="43" />
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,310.93,232.11,7.40;9,49.76,320.40,202.27,7.40"  xml:id="b2">
	<monogr>
		<title level="m" type="main">Avizo -the 3D analysis software for scientific and industrial data</title>
		<imprint>
			<biblScope unit="page" from="2011" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,329.87,232.11,7.40;9,49.76,339.32,232.12,7.40;9,49.76,348.79,141.24,7.40"  xml:id="b3">
	<monogr>
		<title level="m" type="main">Introduction to the ACR-NEMA DICOM standard. Radiographics a review publication of the</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">D</forename>
				<surname>Bidgood</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C</forename>
				<surname>Horii</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Radiological Society of North America Inc</publisher>
			<biblScope unit="page" from="345" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,358.26,232.12,7.40;9,49.76,367.72,232.12,7.40"  xml:id="b4">
	<analytic>
		<title level="a" type="main">View selection for volume rendering</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">D</forename>
				<surname>Bordoloi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-W</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization 2005</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,377.19,232.12,7.40;9,49.76,386.65,232.11,7.40;9,49.76,396.34,232.11,7.09;9,49.76,405.58,138.89,7.40"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating volume visualization techniques into medical applications</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kohlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kanitsar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<meeting>5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro</meeting>
		<imprint>
			<date type="published" when="2008-03" />
			<biblScope unit="page" from="820" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,413.20,57.35,9.25;9,103.56,415.04,178.32,7.40;9,49.76,424.51,232.12,7.40;9,49.76,433.97,232.11,7.40;9,49.76,443.44,94.34,7.40"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Braingazer -visual queries for neurobiology research</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Soltészová</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hladůvka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Bühler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Dickson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1497" to="1504" />
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,452.90,232.12,7.40;9,49.76,462.36,232.13,7.40;9,49.76,471.83,232.13,7.40;9,49.76,481.29,43.74,7.40"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Visibility-driven transfer functions</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">D</forename>
				<surname>Correa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Pacific Visualization Symposium, PACI- FICVIS &apos;09</title>
		<meeting>the 2009 IEEE Pacific Visualization Symposium, PACI- FICVIS &apos;09<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,490.76,232.12,7.40;9,49.76,500.23,161.93,7.40"  xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
				<surname>Drebin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Carpenter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Hanrahan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIG- GRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1988-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,509.68,232.11,7.40;9,49.76,519.15,149.52,7.40"  xml:id="b9">
	<monogr>
		<title level="m" type="main">Real-Time Volume Graphics</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Engel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hadwiger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kniss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rezk-Salama</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>AK Peters</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,528.62,232.12,7.40;9,49.76,538.08,232.12,7.40;9,49.76,547.55,231.78,7.40"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive virtual angioscopy</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gobbetti</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pili</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zorcolo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Tuveri</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the conference on Visualization &apos;98, VIS &apos;98</title>
		<meeting>. of the conference on Visualization &apos;98, VIS &apos;98<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="435" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,557.01,232.12,7.40;9,49.76,566.48,232.13,7.40;9,49.76,575.94,144.67,7.40"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention and visual memory in visualization and computer graphics</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G</forename>
				<surname>Healey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T</forename>
				<surname>Enns</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1170" to="1188" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,585.40,232.12,7.40;9,49.76,594.87,232.11,7.40;9,49.76,604.33,227.55,7.40"  xml:id="b12">
	<analytic>
		<title level="a" type="main">LiveSync: Deformed viewing spheres for knowledge-based navigation</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kohlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kanitsar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1544" to="1551" />
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,613.80,232.11,7.40;9,49.76,623.27,232.11,7.40;9,49.76,632.72,232.10,7.40;9,49.76,642.19,34.54,7.40"  xml:id="b13">
	<analytic>
		<title level="a" type="main">LiveSync++: Enhancements of an interaction metaphor</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kohlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kanitsar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface</title>
		<meeting>Graphics Interface</meeting>
		<imprint>
			<publisher>Canadian Information Processing Society</publisher>
			<date type="published" when="2008-03" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,651.65,232.13,7.40;9,49.76,661.12,232.11,7.40;9,49.76,670.59,232.12,7.40;9,49.76,680.05,167.07,7.40"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Contextual picking of volumetric structures</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kohlmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kanitsar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Pacific Visualization Symposium</title>
		<editor>H.-W. S. Peter Eades, Thomas Ertl</editor>
		<meeting>the IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009-03" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,689.51,232.11,7.40;9,49.76,698.98,232.13,7.40;9,49.76,708.44,232.12,7.40;9,49.76,717.91,59.24,7.40"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Acceleration techniques for GPU-based volume rendering</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Krüger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Westermann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th IEEE Visualization 2003 (VIS&apos;03), VIS &apos;03</title>
		<meeting>the 14th IEEE Visualization 2003 (VIS&apos;03), VIS &apos;03<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,49.76,727.37,232.12,7.40;9,49.76,736.84,76.38,7.40;9,294.12,53.80,250.38,7.40;9,312.38,63.27,229.00,7.40"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction to Vortex Theory Feature peeling</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">J</forename>
				<surname>Malik</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Graphics Interface</title>
		<meeting>Graphics Interface<address><addrLine>Potomac , Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Vortex Flow Press, Inc. A K Peters Ltd</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,72.73,232.11,7.40;9,312.38,82.19,225.45,7.40"  xml:id="b17">
	<monogr>
		<title level="m" type="main">Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Marr</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Freeman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,91.65,232.12,7.40;9,312.38,101.12,232.13,7.40;9,312.38,110.59,33.88,7.40"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Theory of edge detection</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Marr</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Hildreth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London. Series B. Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="187" to="217" />
			<date type="published" when="1167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,120.05,232.11,7.40;9,312.38,129.52,186.83,7.40"  xml:id="b19">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Max</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,138.98,232.11,7.40;9,312.38,148.44,232.12,7.40;9,312.38,157.91,232.13,7.40;9,312.38,167.37,52.05,7.40"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Voreen: A rapid-prototyping environment for ray-casting-based volume visualizations</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Meyer-Spradow</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ropinski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mensmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">H</forename>
				<surname>Hinrichs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="6" to="13" />
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,176.84,232.12,7.40;9,312.38,186.31,232.12,7.40;9,312.38,195.77,232.11,7.40;9,312.38,205.23,232.13,7.40;9,312.38,214.69,112.76,7.40"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic cross-sectioning based on topological volume skeletonization</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Mori</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Takahashi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Igarashi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Takeshima</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Fujishiro</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smart Graphics</title>
		<editor>A. Butz, B. Fisher, A. Krüger, and P. Olivier</editor>
		<meeting><address><addrLine>Berlin ; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="924" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,224.16,232.12,7.40;9,312.38,233.63,200.65,7.40;9,294.12,243.09,51.69,7.40;9,364.57,243.09,33.56,7.40;9,416.89,243.09,2.65,7.40;9,438.30,243.09,9.30,7.40;9,466.35,243.09,34.84,7.40;9,519.93,243.09,24.56,7.40;9,312.38,252.55,229.56,7.41"  xml:id="b22">
	<monogr>
		<title level="m" type="main">MeVisLab -development environment for medical image processing and visualization. http://www.mevislab.de, last visited 2011-09-21. [25] VTK/New CellPicker - for vtkVolume objects</title>
		<imprint>
			<biblScope unit="page" from="2011" to="2020" />
			<pubPlace>New CellPicker</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,262.01,232.11,7.40;9,312.38,271.48,232.12,7.40;9,312.38,280.95,162.49,7.40"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Volume catcher</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Owada</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Nielsen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Igarashi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 symposium on Interactive 3D graphics and games, I3D &apos;05</title>
		<meeting>the 2005 symposium on Interactive 3D graphics and games, I3D &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,290.41,232.12,7.40;9,312.38,299.87,232.11,7.40;9,312.38,309.33,198.45,7.40"  xml:id="b24">
	<analytic>
		<title level="a" type="main">V3D enables real-time 3D visualization and quantitative analysis of large-scale biological image data sets</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Peng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Ruan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Long</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">H</forename>
				<surname>Myers</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">W</forename>
				<surname>Myers</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Biotech</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="348" to="353" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,318.80,232.11,7.40;9,312.38,328.27,17.94,7.40"  xml:id="b25">
	<monogr>
		<title level="m" type="main">Visualization in Medicine</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Preim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Bartz</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Morgan Kaufman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,337.73,232.12,7.40;9,312.38,347.20,174.59,7.40"  xml:id="b26">
	<analytic>
		<title level="a" type="main">Opacity peeling for direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rezk-Salama</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kolb</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="597" to="606" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,356.66,232.12,7.40;9,312.38,366.12,152.64,7.40"  xml:id="b27">
	<analytic>
		<title level="a" type="main">A rendering algorithm for visualizing 3D scalar fields</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Sabella</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIG- GRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="1988-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,375.59,232.11,7.40;9,312.38,385.05,103.83,7.40"  xml:id="b28">
	<monogr>
		<title level="m" type="main">The Visualization Toolkit</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Schroeder</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Martin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Lorensen</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
	<note>2nd. edition</note>
</biblStruct>

<biblStruct coords="9,312.38,394.52,223.57,7.40"  xml:id="b29">
	<monogr>
		<title level="m" type="main">The ParaView Guide</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Squillacote</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Kitware, Inc</publisher>
		</imprint>
	</monogr>
	<note>3rd. edition</note>
</biblStruct>

<biblStruct coords="9,312.38,403.99,232.11,7.40;9,312.38,413.45,232.12,7.40;9,312.38,422.91,219.84,7.40"  xml:id="b30">
	<analytic>
		<title level="a" type="main">Amira: A highly interactive system for visual data analysis</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Stalling</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Westerhoff</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-C</forename>
				<surname>Hege</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Visualization Handbook</title>
		<editor>C. D. Hansen and C. R. Johnson</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="749" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,432.37,232.12,7.40;9,312.38,441.84,232.12,7.40;9,312.38,451.31,89.89,7.40"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Volume rendering for interactive 3-d segmentation</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">D</forename>
				<surname>Toennies</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Derz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE</title>
		<meeting>the SPIE</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="602" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,460.77,232.12,7.40;9,312.38,470.24,232.12,7.40;9,312.38,479.69,34.10,7.40"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Human factors in visualization research</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Tory</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transctions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="84" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,489.16,232.12,7.40;9,312.38,498.63,232.12,7.40;9,312.38,508.09,117.46,7.40"  xml:id="b33">
	<analytic>
		<title level="a" type="main">Image quality improvements in volume rendering</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Van Scheltinga</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Bosma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Smit</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lobregt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in Biomedical Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,517.56,232.12,7.40;9,312.38,527.03,232.12,7.40;9,312.38,536.48,48.94,7.40"  xml:id="b34">
	<monogr>
		<title level="m" type="main">Picking on fused 3D volume rendered images and updating corresponding views according to a picking action</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Vlietinck</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,545.95,232.12,7.40;9,312.38,555.41,232.12,7.40;9,312.38,564.88,232.14,7.40;9,312.38,574.35,17.94,7.40"  xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient opacity specification based on feature visibilities in direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Chi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="302117" to="2126" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Pacific. Graphics 2011</note>
</biblStruct>

<biblStruct coords="9,312.38,583.81,232.12,7.40;9,312.38,593.27,232.12,7.40;9,312.38,602.73,98.99,7.40"  xml:id="b36">
	<monogr>
		<title level="m" type="main">Perception-oriented picking of structures in direct volumetric renderings</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Wiebel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">M</forename>
				<surname>Vos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-C</forename>
				<surname>Hege</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ZIB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,612.20,232.12,7.40;9,312.38,621.67,232.11,7.40;9,312.38,631.13,232.11,7.40;9,312.38,640.82,232.13,7.09;9,312.38,650.06,102.91,7.40"  xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient structureaware selection techniques for 3D point cloud visualizations with 2DOF input</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Yu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Efstathiou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Isenberg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Isenberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Scientific Visualization / Information Visualization 2012)</title>
		<meeting>Scientific Visualization / Information Visualization 2012)</meeting>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
	<note>In. this issue</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
