<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Volume Rendering on a Distributed Memory Parallel Computer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Todd</forename><surname>Elvins</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Box</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Nadeau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netv</forename><forename type="middle">:</forename><surname>An</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Westover</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">San Diego Supercomputer Center Advanced Scientific Visualization Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>92, 186-9784</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">PhD. Dissertation</orgName>
								<orgName type="institution">University of North</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Volume Rendering on a Distributed Memory Parallel Computer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Volume rendering ideally produces high-quality images at interactive rates. High CPU and memory requirements make this goal unreachable with current off-the-shelf technology. Exploiting highly parallel computers is one way that future systems may approach acceptable speeds. This paper discusses the adaptation of a known volume rendering algorithm to a new commercially available distributed memory MIMD parallel architecture.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scientific Visualization uses computer graphics techniques to give scientists greater insight into theiir scientific data One sub-field of scientific visualization is volume visualization, the process of projecting a multidimensional dataset onto a twwbensional image plane for the purpose of studying the structures contained within the volumetric data To be useful, volume visualization techniques must allow scientific users to change viewing, lighting, and data classification parameters and to see the resultant image in a short amount of time. Scientists need these techniques not only for their own insight, but also to share their results with their colleagues, the institutions that support the scientist's research, and the general public.</p><p>Most volume visualization software packages operate in dedicated-mode. Dedicated-mode software runs all of its components on the workstation-class machine in front of the user regardless of the capabilities of the machine. The workstation-class machines that most scientists have access to, however, are not sufkiently powerful to quickly generate highquality volume renderings, and some datasets are so large that they cannot be imaged on even the largest workstation. One advantage of the dedicated-mode approach is that partial solutions can be stonxi in memory along the volume-rendering pipeline so that the incremental images can be generated with fewer calculations. This advantage will be discussed in more detail in the next section.</p><p>An altanative volume visualization approach is to use the scientist's workstation-class machine as a visualization client and run the compute-intensive components on a remote specialized visualization server. Images are tendered and displayed on the scientist's workstation in a much shorter time than if they were created by the worksration itself. This makes the scientist's workstation appear more powerful than it really is. The San Diego Supercomputer Centet (SDSC) has adopted this client/server approach in its NetV Dishibuted Volume Visualization package <ref type="bibr">[ElvBl]</ref>. An Alliant FX/2800 is the primary NetV volume rendering server. When the demand for the Alliant becomes too great, NetV will be revamped to distribute volume rendering jobs to altmative volume rendering servers, such as the nCUBE 2, based on the characteristics of the jobs.</p><p>In volume visualization images are typically mated in one of two ways; using an image-order traversal of the pixels in the image plane or an object-order traversal of the volume elements (voxels) in the volumetric dataset. See @%eb881 or [Elvi92] for a more thorough overview of volume visualization techniques.</p><p>An image-order traversal usually casts one or more rays from each pixel through the dataset, then resamples each pixel [Levo88]. The advantage of this approach is that all of the rays can be cast independently [Bad0901. However, it is diffhlt to implement for a distributed memory architecture because either the entire dataset has to be stored at every processor, or some method of passing rays or samples between pfocess~s has to be implemented. Since the numbex of rays and the numbex of samples are extremely large, this is not a practical solution.</p><p>Object-order traversals operate by calculating the projection and contribution that &amp; voxel makes to the pixels in the image plane. This approach sometimes uses the data at intervals along the fay to calculate the color of intermediate surface primitives to approximate iso-value COntoIlr surfaces in the volumetric dataset Wyvi861Dre871. Surface-fitting is a powerful technique for some types of data but sutFers from several problems such as occasional false positive and negative surf= pieces, poor handling of amorpbous data, and incofiect handling of small features and branches in the data. 'Itvo other object-order algorithms include the V-BufFer method <ref type="bibr">[upso88]</ref> and the more recent splatting method West901, both of which perform a front-to-back object-order h a v d of the data volume and composite voxel contributions into an image accumulation buf&amp; in image-order. Both the V-Bu&amp; and splatting algorithms were reporwl by Wit inventors to be readily parallelizable.</p><p>'Ihis paper desaibes a prototype implementation of a splatting volume rendem (SVR) on a commercially available distributed memory MJhfD parallel pracessor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Theplatform</head><p>The purpose of this project was to investigate the possibility of using an nCUBE 2 parallel computer as an alternative NetV volume rendering server.</p><p>nCUl3E Corporation builds distributed memory MIMD machines based on a proprietary processor. There are more than 250 nCUBE systems installed world-wide with a maximum instalied nCUBE configuration of 1024 processors. Tbe SDSC nCUBE 2 has 128 processors. 64 of these processors have 4MB of local memory, and the remaining processors have 16MB of local memory.</p><p>There is no shared memory on the nCUBE. 'Ibe peak speed of one processor is 3.3MFlops and the peak processor-to-processor communication rate is 2MB/sec. nCUBE computers are hosted-by and accessed-from a stand-alone computa such as a Sun SPARCStation.</p><p>The nCUBE compiler includes command-line directives to control heap, stack, and communication b u f k (comm-space) size. Tbe sum of the sizes of the three must fit in the processor's local physical memory <ref type="bibr">[nCUBW]</ref>. The nCUBE processors do not have virtual memory. Heap is the area where program memory is dynamically allocated and deallocated. Stack is where the program and fixed-size variables are kept. Comm-space is the area where processor-p.ocesor and processor-bost communication messages are staged.</p><p>Since the architecture has no shared memory, message-passing is necessary to get data resident on one processor's memory to another processor's memory. The comm-space must be declared at compile time to be large enough tohold the sum of all messages passed to an individual processor. Using a message-passing protocol that limits the number of messages in a processors readqueue allows a small CO"-space to be used. Dedicating a small portion of memory to comm-space leaves more memory free for heap space which is desirable for most computer graphics programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Splatting</head><p>The procedure is called splatting because it can be h e d to throwing a snowball (voxel) at a glass plate. Tbe snow contribution at the center of impact will be high and the contribution will drop off further away from the center of impact.</p><p>A serial version of an SVR was developed as a basis for the future parallel vexsion of the code. An SVR pmceeds in s e v d steps. The first step determines in nates of the corners of the volume through the viewing matrix, the face of the volume and comer of the face closest to the image plane can be found. Voxels are splatted according to their distance from the image plane, with the closest voxel being splatted fitst. The voxels in one slice are all splatted before the next slice is started. This h a v d ensures that voxels close to the viewpoint obscure voxels far from the viewer.</p><p>lbcb voxel in each slice is visited. The value at the voxel is checked in user-specified classification tables to determine what substance is contained in the voxel. 'Ibe substance's color and opacity values are used to determine a voxel's contribution to the image. For example, the user may have specified that values in the range 200-255 are bone, are colored white, and have an opacity of 0.9 (nearly opaque). Next the voxel is shaded. The gradient at the voxel is calculated using a central ditfietence formula and is used to approximate the normal for the voxel in the shading formula. The 6nal shading value is used to attenuate the voxel's color tuple before it is splatted into the image plane.</p><p>The next step projects the voxel into image-space to find how the voxel's RGB tuple, and associated opacity, contribute to an image bu-.</p><p>A round filter called a reconsrructwn keml is used to 6nd the pixel extent of this contribution. Fot orthogonal viewing, a circular kernel (usually a Gaussian) can be calculated o m and used for all of the voxels. However, for perspective viewing, a new oblique kernel must be calculated for every voxel. The projection of the kernel into the image bu&amp; is called a footprint. Tbe size of the footprint is made proportional to the size of the volume and tbe size of the image to be generated, so that a small volume can fill a large image. Once the footprint is calculated, its center is placed at the center of the voxel's projee4.b in the image buffer. Note that there is not necessarily going to be a pixel center at this location.</p><p>The This has the e f k t of making voxel contributions higher when neat the center of the projection and lower when far from the center. Afm all of the voxels have been splatted into the image b u m , the image is complete. When the opacity at a pixel in the image b u f h reaches unity, then no further splats w i l l have an e&amp;t on it.</p><p>Although the splatting technique operates much dillkrently than either surface-fitting or ray-casting, it quickly produces highquality images that are similar to the images produced by other algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Parallel Implementation</head><p>It is best to execute and control the programs running on the nCUBE from a program running on the nCUBE's host computer. Each nCUBE processor may be running a unique nCUBE executable or, as in this case, the processors may all be running the same executable. The host program will not attempt to start the nCUBE if the dataset is too big. Additionally, the host will allocate only those processops required for tbe rendew to run. Since the number of processors to be used must be specified at compile time, a number of host programs are maintained for varying nCUBE configurations. The host program calculates approximately the sum of memory that will be needed to hold dataset slices and image contribution bufgers. The host program then allocates a subcube of the processors with 4MB of memory, if this sum is less than 4MB, and a subcube of processors with 16MB of memory otherwise.</p><p>Tbe 6nal image is passed from the master nCUBE proc e s s~~ to the host in RGB tuple floating-point. Tbe host compum puts the RGB values into a virtual frame-bufh made911 and invokes a library routine that writes the pixelmap to an image file.</p><p>Tbe initial nCUBE version of the SVR uses one nCUBE processor as a master processor. The master is responsible for reading the dataset from disk, distributing slices of data to a group of slave processors. and then collecting image contributions from the slaves afm they finish splatting the slice data. lllemasterprocesscwactuallydistributesthreeslicesof data to each slave processor. This is because the slice in-hnt-of the slice to be splatted and the slice behind the slice to be splaued are needed to calculate gradients. After splatling the middle slice, each slave retums an image-size image contribution b u t k to the master p m cessor. If there are more slices than processors, then some slaves are given a second packet of three slices sors than slices, then some processors are idle. 'Ibe first and last s l i in the data volume are not splatted because their gradients cannot be estimated with any certainty. It is important to note that the master must collect and composite image contributions in front-@back order to arrive atacorrectimage.</p><p>Since each data-slice has to be accompanied by its two neighbors, the initial nCUBE implementation requires distributing each data slice to tbtee processors (the exception to this is the first and last slice are only distrislices are only distributed to two processors). Splatting is into a 16-byte deep (one float for each of red, green, blue, opacity) image contribution bu!%x. Splatting each slice of 8-bit per voxel data into its own 16-byte per pixel image buftir, then passing the whole b u f b to anotlm processor, is clearly not a good final solution because the message-passing overhead is too high. The run-times of the initial implementation are shown in wallclock time on l i one in <ref type="table">Table 1.</ref> aftet the first packet is hished. If there are more procesbuted t o m m aad the Second and SecOnd-tO-laSt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Optimizations</head><p>Several optimizations have been implemented to cut down on message-passing requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Exploiting image coherence</head><p>Tbe first optimization uses image coherence to reduce the size of the image Contribution packets that have to be passed from the slave processors to the master processor. Often only a small, coherent portion of an image contribution bufkx contains non-black, non-transparent pixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>l k r e is overhead associated with starting up processors even i f they are not used</head><p>For many datasets, cutting the rectangular bounding-box containing the non-black pixels out of the image b&amp;, and passing this, along with the location it was cut from, results in a speed-up in message-passing. Ibe compositing step on the master processor is also faster because a smaller number of pixels is being composited. Line two in <ref type="table">Table 1</ref>. shows tbe timings for a volume containing all non-black, semi-opaque voxels. Althougb this particular dataset does not exhibit coherent image properties, an ovefall speed-up of approximately 15% is realized for coherent images, such as those generated from CT data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Splitting the master's tasks</head><p>The second optimization splits the datadistribution and image-compositing tasks among two master processors. The reasons for this are twofold. The first reason is that the master processor runs out of physical memory for large data volumes. Tbe entire dataset has to be in memory at one time so the volume can be sliced along any of the tbree mes. Fof this reason, the single master procesor scheme has to maintain the entire dataset, an image contribution buffer, and a final image accumulation buffet inmemory all at the same time.</p><p>The second reason for splitting the master processor's tasks is that distributing data and collecting image contributions is too much work for one pnx;essot. Since the image contributions must be received and composited in a particular Order, each slave processor waits until it receives a ready token from the master processor before it sends its image contribution. lhis wait is sometimes unreasonably long if the master is in the middle of distributing data.</p><p>The speed-up realized from splitting the master's responsibilities between two ptocessors is fairly dramatic optimization are shown on line three of <ref type="table">Table 1.</ref> f o r l e s s t h a n 6 4~s s o r s . Therenderingtimesafterthis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="53.">Distributing groups of slices</head><p>The next optimization attempts to hutbea reduce message passing requirements. Instead of sending one slice of data with its two adjacent slices, then gathering a number of image contributions equal to the number of slices (minus two), a group of slices is sent to each processot so that fewer data messages are sent and fewer image contribution messages are received.</p><p>In this version of the software, the slave p " each get one packet of n data slices, where ? I is the total number of slices divided by the number of slave processors. For example, if them are two slave processors and nine slices of data, the first pFocessor gets the first six contiguous slices of data and the second processor gets the remaining five slices of data (the extra slices are for gradient calculations). Note again that the first and last slices are not splatred. A slave processof splats all of its data slices into one image contribution bufBr and then passes this buik back to a master processor. This scheme unchdres the image contribution messagepassing bottleneck. The numbex of image contributions is reduced fnwn the number of slices to the number of pro-cess~s. The times for this optimization are shown on line four of <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Unbdlbred mesrage-passing</head><p>?he 6nal opthidon uses unbuffiered message passing instead of b M messagepassing. This r e q h s additional messagepassing coordination between processors but does not require copying each message into comm-space before sending it. Receiving processors must be waiting for the message. Tbe speedup using unbuffiered messagepassing is shown on line five of m l e 1.</p><p>Tbe nCUBE C compiler has four levels of compiler Optimization. Level 0 is deemed very safe and level 3 is called experimental and is not guaranteed to work at all. All four levels of compiler optimization weze tried with little speed-up gained. The program continued to function properly at every level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>W l e 2 shows nCUBE, Sun, SGI. Alliant, and Cray rendering times for five datasets. A serial version of the SVR was run on one processor on the Sun. SGI, Alliant, and Cray. The times in table 2 are for a relatively small (U)0x200 pixel) image. Not all datasets could be run on all nCUBE configurations due to physical memory limitations on the individual processoss. Note that the 32 proc e s s~~ configuration is faster than the other architectures on the dolphin dataset and on the brain dataset. Although the nCUBErendehg times ace approximately the same as the tendering times on the more traditional acchitectures in most cases, these initial timings do demonstrate that parallel volume rendering ofFers potential. Future parallel volume rendering implementations will probably be most useful for imaging larger datasets where the benefit of having additional rendering processors best justifies the cost of passing data to and from those proces-'Ibe timings in <ref type="table">Table 1</ref> show that each ncuBE processor must be given about eight slices of data to overcome the messagepassing overhead the processor-starting overhead, and to keep the psocessors from from being data-starved. If a p " is given fewer than eight s l i of data to qla4 tbe processor will be abk to render tbe slices faster than it will be able to send its image amtribution butfir to a master processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMS.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">hture work</head><p>Lee Westovex made a correction to his original splatring algorithm in his mxmt thesis lWesSl1. It deals with how image contribution buffiers are accumulated. This correction should be incorpofated in the SDSC SVR.</p><p>?he "um gradient magnituk should be found and broadcast to all processors so that normahd * gradient magnitudes can be used to attenuate Splats. This has the efFect of detecting iso-surfaces in the volume.</p><p>Instead of performing all of the image contribution compositing on the master pcocewx, images should be ampited in a hierarchical fashion as image contribution bufkrs are passed up a tree of processors.</p><p>An inmesting experiment would be to try running the volume fendefer in dedicated-mode. This method leaves all of the data on the processors and just changes rendering parameters such as the viewing, lighting, or data classification. In this way, the p r " are only started once, tbe data is only distributed once, and the gradients are only calculated once. The SVR currently runs in a batch made because that fits best with the SDSC NetV distributed volume visualization scheme.</p><p>Resrricting the view to axis-aligned parallel projections would ensure that a hofizontal slice of data peapendicular to the image plane would project into a few tows of pixels. Tbe.numbex of rows of pixels would be the same as the Might of the splat kernel. Passing a few tows of pixels between processors would consume a small fraction of the messagepassing time that is spent passing entire image pixel blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>Some relatively good rendering times are achieved witb the nCUBE SVR. Message-passing bottlenecks occuf when large numbers of flc~iting-point values have to be collected from every processar for every picture. For large images this is a severe limitation. This initial implementation of a SVR on a distributed memory parallel computer demonstrates the need for parallel amputers with high-bandwidth amnections between processors and also for new pgtallelizable volume rendering algorithms.</p><p>An increasing numbex of CPU-intensive visdzathn tasks will be handled in tbe future by adapting existing vi s-n algorithms to take advantage of powerful new parallel oonputers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">References [Bad&amp;]</head><p>Badouel, D., Bouatoucb, K., and Riol, T., "Ray-tzacing on Distributed Memory paral- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>4. group slices 5. UnbUfBered I/o area covered</head><label></label><figDesc>shaded color tuple and opacity values are blended into the image b u m at every pixel that falls within the what order to m~a tbe volume. By passing the ooordi-by the circular Gaussian footprint. Before a tuple is blended with a pixel, it is attenuated by the value of the Gaussian footprint at that particular pixel center.</figDesc><table><row><cell>Opthimion</cell></row><row><cell>1. none</cell></row><row><cell>2.imageboundingbox</cell></row><row><cell>3.twomasterprocessors</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Wall-clock execution time given in minutes and seconds</figDesc><table><row><cell></cell><cell>Blob 15x15~15</cell><cell>Geophysics 51x33~58</cell><cell></cell><cell cols="2">NPiP 64X64X64</cell><cell>Dolphin 2OOx18Ox91</cell><cell cols="2">Brain 256X256X90</cell></row><row><cell></cell><cell>byte data</cell><cell>byte data</cell><cell></cell><cell cols="2">float data</cell><cell>byte data</cell><cell cols="2">sbortdata</cell></row><row><cell></cell><cell>3KB</cell><cell>98KB</cell><cell></cell><cell>1MB</cell><cell></cell><cell>3 . m</cell><cell cols="2">12MB</cell></row><row><cell>SUn46PARC I</cell><cell>021</cell><cell>1:33</cell><cell></cell><cell>1 :49</cell><cell></cell><cell>TooBig</cell><cell cols="2">TooBig</cell></row><row><cell>SGI 4DB20VGX</cell><cell>006</cell><cell>017</cell><cell></cell><cell>0:12</cell><cell></cell><cell>8:13</cell><cell>15:40</cell><cell></cell></row><row><cell>AUiantFx/2800</cell><cell>007</cell><cell>018</cell><cell></cell><cell>0 1 0</cell><cell></cell><cell>240</cell><cell>245</cell><cell></cell></row><row><cell>Cray yMp8/864</cell><cell>005</cell><cell>0:21</cell><cell></cell><cell>0:23</cell><cell></cell><cell>213#</cell><cell cols="2">TooBig</cell></row><row><cell>nCUBE 4 p r w</cell><cell>0:18</cell><cell>0 5 1</cell><cell></cell><cell>0:39</cell><cell></cell><cell>TooBig</cell><cell cols="2">TooBig</cell></row><row><cell>nCUBE8pnx:</cell><cell>018</cell><cell>025</cell><cell></cell><cell>0:23</cell><cell></cell><cell>TooBig</cell><cell cols="2">TmBig</cell></row><row><cell>nCUBE 16 proc</cell><cell>027*</cell><cell>026</cell><cell></cell><cell>028</cell><cell></cell><cell>1 :48</cell><cell cols="2">TooBig</cell></row><row><cell>nCUBE 32 p r w</cell><cell>048*</cell><cell>0:32</cell><cell></cell><cell>053</cell><cell></cell><cell>1:34</cell><cell>2 14</cell><cell></cell></row><row><cell>nCUBE 64 proc</cell><cell>1:37* ~~~~~~</cell><cell>1:24*</cell><cell>~~~</cell><cell>155</cell><cell>~~~~</cell><cell>2:05</cell><cell>253</cell><cell>-</cell></row></table><note>(#) Time shown is time to execute. The job was submitted via the large-job batch queue.(*) Since groups of &amp;a slices are distributed to processors, attempting to use more processors than there are data slices leaves many processors completely idle.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>de Recbercbe en Infcamatque eâ‚¬ Systemes Aleatoices (IRISA), January 1990. Reprinted in S I G G W H '90 Course Notes #28.</head><label></label><figDesc>lel Computers," Technical R e p t 508,</figDesc><table><row><cell>Institut</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(See color plates, p.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M I</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hamahan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
