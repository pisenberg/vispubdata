<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Visualization of Large Scalar Voxel Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Sakas</surname></persName>
							<email>gsakas@igd.fhg.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><forename type="middle">Hartig</forename><surname>Technische</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hochschule</forename><surname>Darmstadt</surname></persName>
						</author>
						<title level="a" type="main">Interactive Visualization of Large Scalar Voxel Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>W e present a technique allowing interactive visualization of large, scalar, discrete volume fields as semitransparent clouds &quot;on the fly&quot;, i.e., without preprocessing. Interactivity is not restricted to geometric transformations, but includes all possible methods of processing the data (thresholds, filtering, clipping, illumination, etc.). Our system flexibly trades-off quality for performance at any desirable level. In particular, by using a scanline based method and a DDA-based traversing scheme instead of ray-tracing we achieve real-time processing during previewing. B y means of the &quot;pyramidal volume&quot; traversing technique we achieve highquality, constant-time filtering independent of the data resolution. Several filters help to detect &quot;fuzzy&quot;, obscured hot spots, even within noisy data. Our visualization pipeline allows the application of filters at four different stages, maximizing thereby flexibility. Four diflerent illumination models have been implemented. Our method can be used in numerous areas, including 3-D medical imaging, geologic, technical, and environmental applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation and Previous Work</head><p>The term "volume rendering" refers to techniques for direct visualizing measured or simulated, discrete or continuous, scalar or vector data distributed over 3-D space, i.e., without mapping the data on intermediate representations. 3-D discrete scalar data fields are common in a wide range of technical and scientific applications, e.g., CT, MRI and 3-D ultrasound, distributions of temperature, velocity, pressure, and density over 3-D space, geological, chemical, atmospherical, astronomical or environmental data, etc.</p><p>The two most frequently used presentation techniques for rendering volume data focus either on the reconstruction of the appearance of surfaces, used widely for the visualization of medical data, like C T or MRI ([8], <ref type="bibr" target="#b3">[15]</ref>), or in the visualization as semi-transparent els having frequently the appearance of clouds ( <ref type="bibr">[13], h</ref> ) . E ach of these representations possesses advantages when used with particular types of data. Surfaceoriented techniques work unsatisfactorily either when the interior of the examined data is of interest rather than the surface, or in cases where the "hot spot" is not necessarily separated by rl sharp, or well-defined, boundary from the surrounding data (e.g., geological, atmospherical, meteorological and environmental data, pressure, temperature or density distributions, data from non-destructive material testing, medical imaging of soft tissues, etc.). The presence of noise makes the use of a surface-oriented approach even more dubious. For data of the latter type, a visualization as clouds can be more advantageous .</p><p>Unfortunately, information presented as a cloud is hard to understand and to interpret from a still picture. Instead, users need interactive techniques like animated sequences, multiple views, interactive isolation of regions of interest via clipping or positioning of cutting planes, adjustment of filter values, etc. However, a serious drawback of existing volume renderin techniques is the required processing time. The usuafly emplo ed raycasting <ref type="bibr">[lo]</ref> or coherent projecting technique 16frequire interactive systems. A different approach [2] uses a very expensive preprocessing step before executing geometric transformations in real time. This approach is also very restrictive and not applicable with truly interactive systems: The use of, for instance, different filters or cutting planes requires a new preprocessing. Generally speaking, the time requirements for volume rendering have as result a limited acceptance of the method until interactive, or even real-time processing, is possible. several minutes for each frame, which is pro h ibitive for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Aim of the Work</head><p>The aim of this work is to present a technique allowing for the interactive, or even real-time, processing of scalar, discrete volume data as semi-transparent clouds. Users are supported in trading-off quality for speed. Thus, interactivity is not restricted to geometric transformations, but includes all possible stages of the visualization pipeline; we calculate "on the fly" color and transparency of the volume. Data are traversed in a front-to-back, image-order manner involving parallel or perspective projection; alternative object-order methods have also been reported ( 151, 61). We use sing scheme instead of ray-tracing, 2) "cloudy" illumination models avoiding the calculation of local gradients (normals), 3) a user adjustable level of detail allowing real-time processing during previewing and "pyramidal volume" traversing for high-quality rendering, and 4) multi-processing provided by modern workstations, to achieve a significant speed-up in comparison to other existing techniques. Several filters help to detect fuzzy hot spots, even within noisy data. Our visualization pipeline allows the application of filters at four different stages in order to maximize flexibility. In particular, the processing time of pyramidal volume traversing de-1) a scanline based method and a 6 b DAased traver-pends only on the image resolution and is independent of the size of the data. This permits the high-quality</p><formula xml:id="formula_0">( 3-D data )</formula><p>visualization of huge data sets in TV-resolution within minutes. In addition to geometric transformations one</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I)---.</head><p>may scan the data from any direction by cutting a slice of arbitrary thickness at any location along the eye-ray. A hybrid rendering is trivially achieved using the same pipeline.</p><p>Transformation</p><formula xml:id="formula_1">2 T h e Visualization Pipeline I 1 ,, Geometric</formula><p>The visualization pipeline consists of three stages, as presented in <ref type="figure" target="#fig_2">Fig. 2</ref>. The 3-D data are initially represented as a cube. During the "geometric transformation" stage the data are transformed, (perspectively) projected, and scan-converted using usual scanline techniques before the visible portion is clipped. Three different types of clipping are possible: 1) the "classic" method clip along the planes of the viewing pyramid, 2) additional clipping along the axes of the surrounding cube, and 3) a slice of any thickness may be cut perpendicular to the viewing direction (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Additional used-defined cutting planes are also possible. Thus, a particular portion of the data may be isolated for better examination. The results from the first stage are stored in the "geometry buffer". This buffer is similar to a z-buffer and contains for every pixel the u,v, and w texture coordinates as well as the z-depth of the front and back planes delimiting the volume data portion visible through the pixel. This pipeline stage may be implemented either in software or by the hardware of the geometry engineif any available. During "traversing" the volume data are traversed in an image-space, front-to-back order by means of either a DDA or a pyramidal volume traversing scheme (see Section 2.2), according to the information stored in the geometry buffer. During traversing, which takes place in texture coordinates, the densities along the eye-ray are properly weighted and accumulated. Several "intermediate" filters may be applied here (all filters are presented in detail in Section 2.3). The result of the accumulation, together with the length between the start and end point, is stored for each pixel in the pixel buffer.</p><p>During "mapping" an illumination of the data is performed. Prior to illumination, the accumulated data in the pixel buffer may be filtered again ("post-filtering"). The result of this stage is an RGB-color and transparency stored in the frame buffer. Before the picture is displayed, 2-D image processing ("imaging") may be applied on the frame buffer.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Illumination Models</head><p>Illumination takes place within the "mapping" stage. From the four different implemented illumination models we present only the physics-based one in more details; the other three apply similar. We use first-order multiple scattering similar to [I] (please refer to [14] for a more detailed discussion): the light of the source is scattered only once toward the eye; second-and higherorder reflections are ignored. The resulting equation is (see also <ref type="figure">Fig. 3)</ref>:</p><formula xml:id="formula_2">d = f 3 - K p ( 5 ) d? I, = 2 7' I,,, e 3=i U P ( . ) "l;.=zo - s = f o ==+I @($) e *=f di + la e ; ==O -J n p ( 2 ) dP -J n p ( F ) G<label>(1)</label></formula><p>whereby U and K are the scattering and extinction coefficients, respectively, w = U / K is the albedo, and a($)</p><p>is the phase function. In order to simplify things further, we assume parallel light and isotropic scattering: (3)</p><formula xml:id="formula_3">CP $) = 1. Still,</formula><p>where C is the color and T the transparency of the volume. As a result, when self-shadowing is omitted all intermediate terms are eliminated in the final sum except the first and the very last one, so that the calculation of the color and transparency along a view ray requires only the calculation of the optical depth 7 , or the average density B, and one exponeniial function per pixel. The resulting picture is similar to an X-ray image of the volume. The optical depth is calculated in the "traversing" stage, see <ref type="figure" target="#fig_2">Fig. 2</ref> and Section 2.2. A very important observation is that the order of accumulation is irrelevant, since only the sum influences the final result. Illumination equation ( 2 ) is equivalent to Levoy's</p><p>[8] if the latter is expressed for clouds, with the difference that in our case ihe calculation of a color and a transparency for each voxel during a pre-processing step is not necessary, but is calculated at the end of traversing for the complete line of sight. This is an important difference from existing cloud models which include self-shadowing. In addition, this "on the fly" calculation results in reducing both, storage space as well as pre-processing time, so that the turn-around time for a new image is also reduced. The second illumination model maps directly the o p tical depth 7 on transparency and color (C*D model in</p><formula xml:id="formula_4">[16l): 1 x ( T --1 -L T L x 255 c = - L T 2 T = 1 -r<label>(4)</label></formula><p>L controls the intensity and LT the contrast of the image. The third model maps the mean density i j on color using a similar equation. The fourth model extracts the maximum value along a given ray; such models are commonly used with MRI data. Generally speaking, any other model able to map a function of density onto color and opacity may be implemented as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Traversing and Sampling</head><p>The evaluation of the volume opacity along an eyeray requires the calculation of the optical depth T eq.</p><p>line. For such a depth-traversing and density evaluation (i.e., weighting and accumulation) along an eyeray, ray-casting is usually employed. Two alternatives are proposed in <ref type="bibr">[12]</ref> for equally spaced, discrete fields. The first one, called "distance sampling", is a faster ray-sampling technique, which evaluates the geometry buffer ( <ref type="figure" target="#fig_2">Fig. 2 )</ref> in a row-by-row manner, reading for each pixel the corresponding 3-D texture coordinates of starting and end faces delimiting the visible volume segment. Recall that all these values are already calculated during the "transformation" stage and stored in the geometry buffer. This visible volume segment is traversed by using a very fast DDA (or 3-D Bresenham) algorithm, which can also be implemented in software or hardware (see <ref type="bibr">[12]</ref> for pseudo-code). The speed improvements compared to usual ray-casting result from exploiting data coherence (scanline) during the calculation of the coordinates of start-and endpoint stored in the geometry bufferalso using possible hardware capabilities usual in geometric engines supporting such a calculationas well as from using the faster DDA or 3-D Bresenham algorithm for the depth traversing.</p><p>The second proposed method, called "pyramidal volume" sampling, is an improved volume sampling method developed for both reducing aliasing and minimizing the number of traversing steps. Ray-sampling methods are essentially point-sampling techniques. Problems associated with point sampling volume fields, are caused by two inherent drawbacks: first, such a point sampling uses a "wrong", usually heuristically estimated, sampling frequency, and second, they sample along a line instead of considering the complete volume visible through a pixel (see Fi . 4 a). A lower than necessary sampling frequency resufts in aliasing, like loss of details,</p><p>(3)), mean density p or peak density pm along t 6 at flicker during movement, etc., while a higher one in wasted computing time. The pyramidal volume method samples the complete volume defined by the surfaces of the front and back faces as seen through the pixel by covering it with a chain of sampling cubes, each of an increasing size (see <ref type="figure" target="#fig_4">Fig. 4 b)</ref>. Since the level of distinguishable details is determined by the actual resolution, i.e., image pixel size, volume data details (=voxels) within a sampling cube may be averaged, thereby preventing aliasing. If such an averaging is performed fast, computing time is saved when the data resolution is higher than the image resolution (compression area of <ref type="figure" target="#fig_4">Fig.  4 a)</ref>. A significant part of this averaging is performed by a simple and rather fast preprocessing step: we extended Williams' [17] "mip-map" texturing method into 3-D, averaging eight voxels into one and creating a pyramid of different levels of detail. With this pyramid we may calculate a cube of any size by using two inter-level tri-linear and one intra-level linear interpolation. If the desired size is smaller than the voxel size of the highest resolution, only one tri-linear interpolation is performed on the upper pyramid level. Thus, this approach averages data within the compression area and interpolates intermediate values in the magnification area, thereby smoothing the voxel boundaries. A similar idea, but wi- Since our calculations are performed using floating-point representation, we bypass all interpolation, shading, and "blocky appearance" problems reported in <ref type="bibr" target="#b4">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T-Volume</head><p>In addition to avoiding aliasing in both the magnification and the compression area, pyramidal volume sampling also allows 1) the calculation of the "correct" sampling frequency, thus minimizing the number of steps through the volume, 2) adapting the volume representation detail to the actual display resolution, 3) the incremental calculation of the sampling cubes chain, and 4) rendering time to depend only on the image resolution, not the dataset size. This means that for a given image size the rendering time is the same for datasets of any size (see [la] for derivation and more detailed discussion). The last point reflects the fact that the employed image resolution determines the sampling frequency, and, thus, the highest displayable level of detail. If the database resolution is larger than the imager resolution, high-frequency details must be averaged, since their information can not be displayed and may cause aliasing. This fact has been of little importance so far, because most employed datasets have a resolution of 643, 1283 or 2563 which is usually less than the common sampling resolution of 512', but we expect this to change in the future (e.g., geological or environmental applications).</p><p>Distance sampling is the simplest and pyramidal volume sampling the most complex solution to the volume sampling problem. In between these two extrema we implemented four additional traversing techniques of intermediate complexity and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is the "correct sampling resolution"?</head><p>The question of interpolating data is a crucial one in scientific visualization. Within the compression area of <ref type="figure" target="#fig_4">Fig. 4</ref>, an averaging of voxels seems to be plausible, but what about interpolating intermediate values from the existing ones within the magnification area? How many "rays" are realy needed to sample a field of, e.g., 643 voxels by an image resolution of 3002? We think that interpolation does not really add information, but just expands the dataset by smoothly filling intermediate pixels for improved visual presentation. We decided therefore to avoid such an interpolation and to declare that resolution, which employs one ray, or pyramid, per projected voxel column to be the "correct sampling resolzdion", since it samples each voxel once. In order to smooth results for improving visual presentation, we interpolate pixel colors on the display using the hardware engine instead of interpolating intermediate values in object space. Thus, we distinguish between data, sampling and display resolutions: the second is found as a function of the first, while the pixel values of the third are interpolated from the entries of the second. This may possibly give poor results when surfaces are extracted, though it works well with "cloudy" data representations up to a magnification factor of three or four. The reason is that clouds usually do not contain high frequency details typical for surfaces, like sharp boundaries or highlights, hence an image-space approximation may work better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Filtering</head><p>A substantial benefit of the proposed visualization pipeline is that filtering is possible at four different processing stages: 1) on the original data, 2) in the traversing stage 3) in the mapping stage, and 4) on the 2-D image values (see <ref type="figure" target="#fig_2">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p = z=o</head><p>We implemented three different filters to operate on the original 3-D data (pre-filtering and intermediate fil-</p><formula xml:id="formula_5">I I</formula><p>tering). The first one is the well-known threshold filter, which sets densities lying within a certain range to a different value, e.g., to zero:</p><p>n(x) with a mean value pn overlaps the original signal, the integration along any line results in: </p><formula xml:id="formula_6">I C M . 1 + 4 2 ) ) li<label>(</label></formula><formula xml:id="formula_7">P = { p : p &lt; min, V p &gt; matp</formula><p>Exponential biasing [13] is another useful filter, which controls visibility by adjusting the exponent -y and maps all voxels to some non-zero value, so that suppressed values remain visible and contribute to the final image:</p><formula xml:id="formula_8">i = pr, i E [O, 11<label>(7)</label></formula><p>A "3-D pseudo-color" filter supports the detection of hot spots or other inhomogeneities hidden within the data interior and obscured by the layers lying above them. Detection of "internal spots" has applications in medical imaging, geology, environmental applications, non-destructive testing, etc. When searching for an unknown or suspected hot spot in unknown data sets, thresholding is not always a good filter because it may eliminate the range of values containing the spot. Therefore, after isolating clipping) a location of interest and amplifyinga specific 6 ata range by, e.g., biasing, we map densities on color values in order to enhance visual discrimination. In the following example, possible hot spots shine red through the white material surrounding them:</p><p>In this case accumulation is provided for each color separately. The same filter may also be used to map density values to continuous colors. Still, this method of coding densities as colors is not a pseudo-color filter in the usual sense, because pseudc+color map 2-D pixel luminosity onto color; instead, this filter 1) operates on values before illumination takes place, 2) alters the material properties (i.e., color), and 3) works in 3-D instead of 2-D. Since intensity depends on density times length, a long distance of moderate density and a short distance of hiqh density may produce the same intensity in the final image. Hot spot coloring allows the discrimination of both structures, because the latter will appear red and the first white. Post-filters, illumination and imaging all operate on the 2-D data stored either in the pixel buffer or in the frame buffer. Such filters operate very fast since they do not require re-traversing of the volume. Our noise-reduction filter belongs to the post-filtering category and is based on the fact that the integral of noise along a path is equal to its mean value. Such an integration takes place when the average density 7 or optical depth 7 is calculated. If an uncorrelated noise signal The original signal may be derived by subtracting p , from p after integration. The larger the database is, the more the summed value approximates the theoretical integration result. Unlike thresholding, our n o b reduction filter removes signal "offset" offer integration instead of before. The filter operates on the data stored in the pixel buffer, thus allowing the adjustment of in an interactive, quasi-real-time manner. K n o w l e g of the correct distribution or mean value of the noise is not necessary: the user interactively adjusts the value of pn until a satisfactory result is gained. The filter works very well if the noise is equally distributed throughout the database.</p><p>Luminocity and contrast of the final picture may be adjusted by changin the parameters appropriate for each illumination moiel. Such adjustments operate also during the evaluation of the pixel buffer and are therefore performed in quasi-real-time. Note that when the absorption model is used one can even choose a negative K value, further increasing the picture contrast. After illumination, all known image-processing filters may be applied to the pixel values stored in the frame buffer imagin ). Due to the pipeline structure of the system,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A P</head><p>lters o different stages may be applied successively in a modular way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation and Results</head><p>We implemented our InViVo (Interactive Visualizer of Volume data) system on a Silicon Graphics Indigo and on a 4D/380 VGX machine with 8 CPU units. In addition to the software solution we used the available graphics engine for the "geometric transformation" stage of the pipeline, as well as for smoothing and zooming the image on the display. The cube geometry surrounding the texture data is scan-transformed, clipped and stored in the z-buffer by using the graphics hardware. For generating the front and back faces using hardware, the cube is scanned two times: once for the near faces, i.e., faces of the cube bounding the volume data with normals directed toward the eye, and once for the remote faces. After each scanning, the contents of the z-buffer is copied into the geometry buffer in the main memory. If "foreign" polygons are present in the database (other objects, cutting planes, etc.), they are simply included in the geometry buffer. Volume object sections are delimited by two entries in this buffer, specifyin the geometric and texture coordinates of the start a n i end position. Thereby it makes little difference if a delimiting polygon originates from the volume bounding cube or from a "foreign" object intersecting the volume data: both volume and polygonal objects are transformed through the same single pipeline, but are shaded by different illumination models. This unified hybrid scanline rendering approach has been used by us ( 41, <ref type="bibr" target="#b2">[14]</ref>) for fast rendering and anti-aliasing aniwith moving polygonal objects. One should be wary of truncation errors due to the 8-bit depth of the frame buffer when using the Gouraud interpolation hardware.</p><p>A complete visualization usually employs several phases: during the preview phase interactive rates with short response time are required. For previewing we switch to a small resolution (60' for the Indigo, 100' for the 4D/380) and perform distance sampling on the corresponding pyramid level. During the inspection phase, performed usually with reduced (clipped) dataset, we use the "correct sampling resolution" with a zooming factor of 2-5 for display, and perform distance, intermediate, or even full pyramidal sampling. For generating highest quality pictures, needed for presentation purposes, we apply pyramidal sampling on a very high sampling resolution (e.g., 1024').</p><p>Parallelization can further speed-up the calculation. One is able to parallelize all calculation parts of the pipeline. The bottleneck of our shared memory machine is the access to the data stored in the main memory. By construction, the bandwidth of the 4D/380 memory bus limits the number of efficiently working processors to mostly four. Using more processors may in several cases even worsen the calculation times, especially with very large datasets (&gt; 2563): when scanning along particular disadvantageous directions (e.g., perpendicular to the "storage direction" of the data), the speed-up almost vanishes due to the continuous cache-faults and reloading of data from the main memory. This last problem can partly be solved with a better cache exploitation resulting from an oct-tree memory organization instead of a linear one. Even with larger caches and wider memory bus, we feel that the number of effectively working processors in parallel may not exceed sixteen for a shared memory machine. A better, massive parallel future solution should rather be based on asynchronously working, distributed memory systems. The invariance of accumulation order provided by "cloudy" models like ours may be advantageous for such asynchronous, MIMD implementations.</p><p>In the following table the runtimes for several data sets are given for different numbers of processors. The times in <ref type="table" target="#tab_1">Table 1</ref> for "distance sampling" are given for a sampling frequency equal to the texture resolution. On the other hand, when "pyramidal volume sampling" is used, computation times depend only on the image resolution and not on the size of the actual data set (see <ref type="table" target="#tab_2">Table 2</ref>). A hardware support by the calculation of the tri-linear interpolations will significantly decrease these times. Several examples demonstrate the methods presented here. <ref type="figure">Fig. 5</ref> shows the user interface of InViVo. <ref type="figure">Fig. 6</ref> shows a comparison of the maximum density, mean density, C*D, and absorption illumination models for four different datasets.' Each model amplifies different aspects of the data, and, thus, can be advantageous in one case and disadvantageous in another.   The data resolution is arbitrary 7 shows a mechanical part having a T-shaped drilling' with internal cracks around the location where the horizontal and the perpendicular drillings meet. These defects are represented in an ultrasonic examination as a cloud <ref type="figure">(Fig. 7 a)</ref>. By varying the threshold value from 0% to 30%, this "cloud" becomes visible <ref type="figure">(Fig.7 b)</ref>. Note the noise surrounding the defect: noisy pictures like this are common when ultrasonic methods are used. <ref type="figure">Fig. 7</ref> c &amp; d show how obscured information may become visible by noise filtering and biasing the original data. Note that noise may be significantly reduced without altering the relevant information. In <ref type="figure">Fig.7</ref> d bias and noise filters have been used. In <ref type="figure">Fig. 7</ref> e the hot-spot filter has been added to the previous ones. <ref type="figure" target="#fig_11">Fig. 8</ref> shows how medical data may be viewed from an arbitrary direction and clipped perpendicularly to the viewing direction; the data resolution is 2562 x 109, the slice thickness 3 cm. <ref type="figure" target="#fig_12">Fig. 9</ref> shows clipping of high and low density regions for the same head. <ref type="figure" target="#fig_0">Fig. 10</ref> shows internal structures of a bug.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented an interactive, front-to-back, image-space approach for visualizing scalar, discrete volume data as clouds. A scanline based fast traversing method using a DDA algorithm is employed for previewing. For high-quality rendering pyramidal volume traversing is used to minimize the number of traversing steps and to eliminate most aliasing without oversampling. In the latter case computation times depend 81) may also be used to further increase the processing speed. In addition, volume data and poly onal objects used in the same scene are visualized and anti-aliased with the same renderer hybrid <ref type="figure">rendering, [3]</ref>). Additional tools (stereo, dials, I oops, "virtual video recorder", parallel/perspective projection, densities/colors inversion, etc.) increase the flexibility of the system. We regard our approach as a complement to the other three existing approaches, i.e. thresholding/marching cubes, Levoy's surface extracting and splatting, particularly suitable for interactively "X-raying" large, scalar, fuzzy, noisy datasets.</p><p>Between our technique and the techniques presented by Levoy and others there is a significant difference. In our approach, color and transparency are not assigned to each voxel or each traversing step, but are calculated once for the complete ray after the optical depth accumulation has been carried out. Since in the simplest case this accumulation requires only a simple addition for each voxel (which is significantly faster than computing an exponential function, or a local gradient), runtime is minimized. In addition, due to the buffering of intermediate result only the affected pipeline stages must be repeated after a parameter change.</p><p>Ultrasonic imaging and geological data are ideal application examples for our method. An ultrasonic examination of the human body always appears "noisy" due to the reflection of the waves by many small inhomogeneities and "soft" tissues. Surface-based methods do not work properly with such databases. On the other hand, the visualization of "soft" tissues, spots, defects, inhomogeneities, etc., as clouds looks much more natural, since sharp surfaces are not present anyway. The rather small preprocessing work required (some seconds for the calculation of the pyramid) allows the system to be used with dynamic applications, in which data acquisition and visualization take place in a continuous manner. As an example, 3-D medical sonography requires only about four to ten seconds of data acquisition for each patient. It is advantageous if data visualization takes place in a comparable amount of time, and not after several minutes, or even hours, of processing. Similar results are achieved in the field of non-destructive material testing, where unknown, arbitrarily formed shapes (contrary to medical applications, which deal with wellknown shapes, e.g. the head) have to be inspected. In this case interactivity allows inspecting and understanding the examined shape within a short time.       <ref type="figure" target="#fig_4">, p. CP-4.)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Clipping of a slice perpendicular t o the viewing direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The visualization pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Several models for volume data have been presented in the last years in the fields of realistic imaging [l [ 5 , [ll], [14]) as well as scientific visualization ([I&amp;, klb]].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Sampling geometry for volume objects. thout details, has been mentioned by Levoy in [7]. The chain of cubes forms a geometric series with increment d, the number of steps along a ray segment is: log (1A11 e + 1) do is the edge-size of the starting cube and lAll is the segment length; all sizes are expressed in normalized texture coordinates. The calculation of the mean density 7 or optical depth T of eq. (3) becomes a multiplication of an average density pi within a sampling cube times the cube-edge 1,.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>0 :</head><label>0</label><figDesc>minp &lt; p &lt; m a x , , p~[ O , l ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. mated L lms containing 3-D clouds or steam intersecting image resolution loo2 ~ '3-D ultrasonic data (2562 x loo), showing the head and the left hand of an unborn baby, provided by Dr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>'</head><label></label><figDesc>SD ultrasonic data (643) provided by Prof. Langenberg, Uni-3MRI data (12S3) provided by Prof. Haase and Dr. Syha, versity of Kassel University of Wurzburg only on the image resolution, not on the data resolution (constant-time filtering). The user is able to choose between three clipping types, four illumination models, and six traversing modes trading-off quality for speed. The pipeline structure of the system allows filtering on four different stages. Two new filters (noise, hot-spot) have been used; other filters may be easily integrated. Processing can approach quasi-real-time speeds via parallelisation. Other known speed-up techniques like roressive refinement, oct-tree traversing, etc. ([9], LO],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Blinn, J. F.: Light Reflection Functions for Simulation of Clouds and Dusty Surfaces, ACM Computer Graphics, [2] Foley, T., Lane, D., Nielson, G.: Towards Animating Ray-traces Volume Visualization, Visualization and Computer Animation, Vol. 1, No. 1, pp. 2-8, August 1990 [3] Fri?hauf, M.: Combining Volume Rendering with Line and Surface Rendering, Proceedings EUROGRAPHICS'SI, Vienna-Austria, North-Holland Publishers, pp. 21-32, September 1991 [4] Haas, S., Sakas, G.: Methods for EBcient Sampling of Arbitrary Distributed Volume Densities, K. Bouatouch, C. Bouville (Eds.), 'Photorealism in Computer Graphics', Springer Verlag, 1991 [5] Kajiya, J. T., von Herzen, B.: Ray Tracing Volume Densities, ACM Computer Graphics, SIGGRAPH-84, Vol. 18, [6] Laur, D., Hanrahan, P.: Hierarchical Splatting: A Progressive Refinement Algorithm for Volume Rendering, ACM Computer Graphics, SIGGRAPH-91, Vol. 25, No. 4, pp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>[ 7 ]Figure 5 :</head><label>75</label><figDesc>Levoy, M . , Whitaker, R.: Gaze-Directed Volume Rendering, Proceedings of the 1990 Utah Symposium on Interactive 3D Graphics, 1990 [8] Levoy, M.: Display of Surfaces from Volume Data, IEEE Computer Graphics and Applications, Vol. 8, No. 3, pp. 29-37, May 1988 [9] Levoy, M.: Volume Rendering by Adaptive Refinement, The Visual Computer, Vol. 6, No. 1, pp. 2-7, January 1990 [lo] Levoy, M.: EBcient Ray-Tmcing of Volume Data, ACM Transactions on Graphics, Vol. 9, No. 3, July 1990 [ l l ] Nishita, T., Miyawaki, Y., Nakamae, E.: A ShadingModel for Atmospheric Scattering Considering Luminous Intensity Distribution of Light Sources, ACM Computer Graphics, The user Interface of InViVo.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Comparison of the four illumination models, Ultrasonic data form nondestructive material testing. Upper row: a, b. c, Lower row: d, e. f.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Cllpplng of medical MRI data perpendicular to an arbitrary axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of medical MRI data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Structures in the interior of a bug. (See colorplates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Geometry of the light propagation within a volume object.</head><label></label><figDesc></figDesc><table><row><cell>I,</cell></row><row><cell>Figure 3: heless, if self-shadowing the attenuation of the source</cell></row><row><cell>( 1 ) may be solved for arbitrarily distributed, discrete,</cell></row><row><cell>scalar 3-D fields as follows: each voxel with a ray length</cell></row><row><cell>of li and a density of pi will be illuminated by the same source intensity I (the position of the source is thereby</cell></row><row><cell>irrelevant), its transparency is ti = e-npi'i and its color</cell></row><row><cell>The total color, or cumulative intensity along the ray</cell></row><row><cell>results from the accumulated contributions of all vo-</cell></row><row><cell>xels multiplied by the corresponding cumulative trans-</cell></row><row><cell>p arency</cell></row><row><cell>this equation can not be analytically so r ved for arbitrary density distributions p ( z ) . Nevert-</cell></row></table><note>ci = IS;* npie-fipizdz = 1w(1 -e-Ep8') = 1w(1 -t i ) .: light within the volume a \ ong 123) is ignored, equationn n .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Runtimes</figDesc><table><row><cell>in CPU seconds for visualizing data</cell></row><row><cell>of varying resolution using "distance sampling" for Si-</cell></row><row><cell>licon Graphics Indigo (1. column) and 4D/380 VGX.</cell></row><row><cell>The sampling resolution is equal to the data resolution</cell></row><row><cell>200' 600' I 400' 1 325 164 87 60 i; 1 78 40 21.8 13.5 606 340 150 121 100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Runtimes in CPU seconds</figDesc><table><row><cell>for varying display</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors wish to express their thanks to Prof.</p><p>EncarnaGik for his suport and to R. Kautz for proof reading. This research has been partially sponsored by the " Deutsche Forschungsgemeinschaft" (DFG) , grant</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sampling and Anti-Aliasing Discrete 3-D Volume Density Teztures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUROGRAPHICS&apos;SI Award Paper</title>
		<imprint>
			<publisher>Pergamon Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Rendering Algorithm for Visualizing 9-D Scalar Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sabella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast Rendering of Arbitrarily Distributed Volume Densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EUROGRAPHICS&apos; 90</title>
		<meeting>EUROGRAPHICS&apos; 90<address><addrLine>Montreux-Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>North-Holland Publishers</publisher>
			<date type="published" when="1990-09" />
			<biblScope unit="page" from="519" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Footprint Evaluation for Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Graphics, SIGGRAPH-90</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Coherent Projection Approach for Direct Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilhelms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Graphics, SIGGRAPH-91</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="284" />
			<date type="published" when="1991-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramidal Parametrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Graphics, SIGGRAPH-83</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1983-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m">SIGGRAPH-82</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m">SIGGRAPH-87</title>
		<imprint>
			<date type="published" when="1987-07" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
