<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Poco</surname></persName>
							<email>jpocom@ucsp.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Mayhua</surname></persName>
							<email>angela.mayhua@ucsp.edu.pe</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
							<email>jheer@uw.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Universidad Católica San Pablo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Católica San Pablo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2017.2744320</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visualization</term>
					<term>color</term>
					<term>chart understanding</term>
					<term>information extraction</term>
					<term>redesign</term>
					<term>computer vision</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. Automatic extraction and redesign of color mappings for a geographic heatmap. The bitmap image on the left uses a questionable rainbow color palette. Our methods automatically recover the color mapping, enabling applications such as automatic recoloring. The generated image on the right replaces the original color palette with a perceptually-motivated diverging color scheme.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Color mappings are commonly used to encode data and enhance visualization aesthetics. Distinct hues can be used to effectively convey category values, while changes in luminance or saturation can encode ordinal or quantitative differences. Designing effective color encodings, however, can prove difficult. Designers must balance issues including perceptual discriminability, cultural conventions, and aesthetic preferences <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. Poorly designed palettes can lead to imprecise and inaccurate readings of the data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>. Even when well-designed, color encodings often afford less precise comparisons than alternative visual channels such as position or size, and may suffer from issues such as limited discriminability or simultaneous contrast.</p><p>Given the complexities of color design, analysts often rely on the default palettes provided by visualization software packages. In many cases these palettes -including ubiquitous rainbow palettes <ref type="bibr" target="#b1">[2]</ref> -have not been subjected to perceptual design and analysis. As a result, viewers could benefit from tools for automatic recoloring and interactive querying of visualizations with color encodings.</p><p>However, many visualizations "in the wild" are available only as static images in a bitmap or vector format, including charts found in media such as web sites, presentation slides, textbooks, and academic papers. While these static images are useful for viewing by people, their content is largely inaccessible to computers, limiting our ability to perform automated analysis and retargeting.</p><p>In response, some recent projects explore the use of computer vision and machine learning techniques to (semi-)automatically interpret chart images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. These systems can successfully identify chart types, recover spatial encodings, and read off data values for basic plots such as bar, pie, or line charts. However, these systems do not address the task of automatically recovering color mappings.</p><p>We contribute a method to semi-automatically extract color encodings from a bitmap visualization image. Given an image and legend location, we classify the legend as describing either a discrete or continuous color encoding, identify the colors used, and extract legend text using OCR methods. We then combine color and text information to recover the color mapping. We evaluate our techniques using a corpus of images extracted from scientific papers and demonstrate accurate automatic inference of color mappings across a variety of chart types. We also present a legend annotation interface that can be used to label images and correct interpretation errors.</p><p>Extracted color mappings can be used to aid visualization indexing, search, and redesign. We present two user-facing applications of our color encoding extraction methods. Our first application performs automatic recoloring: given bitmap images as input, we produce new images that use effective, perceptually-motivated color encodings. Inspired by Kong &amp; Agrawala <ref type="bibr" target="#b15">[16]</ref>, our second application adds interactive overlays to a static image, enabling data querying and highlighting. Users can brush a color legend to select corresponding data values, hover over data points to highlight matching legend values, and select regions to view automatically-produced statistical summaries. Many of these features require accurate extraction of legend color values only (not OCR text), and are applicable to a variety of visualization images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bar Charts Line Charts</head><p>Scatter Plots Pie Charts Heatmaps Geographical Maps <ref type="figure">Fig. 2</ref>. Example visualization images from our corpus, each including an explicit color legend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work on color mapping extraction draws on two streams of prior work: color design for visualization and automatic chart interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Color Design for Visualization</head><p>Colors play a central role in data visualization, as they are commonly used to visually encode data. However, there is often a disconnect between visualization research and visualization practice "in the wild". For example, Dasgupta et al. <ref type="bibr" target="#b7">[8]</ref> report mismatches with the climate research community based on a two year collaboration with domain scientists. Despite the availability of decades-old design guidelines <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref> in the visualization literature, rainbow color maps are still ubiquitous. Such design choices can have serious consequences: Borkin et al. <ref type="bibr" target="#b0">[1]</ref> found that changing the color encoding used for visualizations of arterial stress (switching from a rainbow scale to a perceptually-motivated palette) led to marked improvements in doctors' diagnostic accuracy. Unfortunately, a number of existing commercial visualization tools still provide default color palettes of questionable quality. Meanwhile, a number of visualization projects have sought to provide both stock palettes and palette-generation tools to promote more effective mappings between data values and colors. Cynthia Brewer's color-use guidelines <ref type="bibr" target="#b3">[4]</ref> and popular ColorBrewer palettes are widely used for coloring both maps and more general information visualizations. Heer &amp; Stone <ref type="bibr" target="#b12">[13]</ref> demonstrate how models of color naming can be used to assess and improve color palettes. Lin et al. <ref type="bibr" target="#b17">[18]</ref> contribute an algorithm for generating color palettes that respect "semantically resonant" color-concept associations. More recently, Gramazio et al.'s Colorgorical <ref type="bibr" target="#b10">[11]</ref> tool allows designers to interactively balance concerns such as discriminability, naming similarity, and aesthetic preferences.</p><p>In this work, we contribute techniques for extracting color encodings from visualization images, as well as applications for automatic recoloring and interactive overlays. To be clear, we do not contribute methods for color design. For example, our recoloring application assumes an appropriate target color scheme is provided as an input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automatic Chart Interpretation &amp; Retargeting</head><p>A growing body of work focuses on the "inverse problem" of data visualization: given a visualization, can one recover the underlying encodings and data values? Solutions to this problem can enable automated analysis, indexing and redesign of published visualizations -in some cases without recourse to an original specification or dataset.</p><p>Harper and Agrawala <ref type="bibr" target="#b11">[12]</ref> introduce a system to deconstruct D3 <ref type="bibr" target="#b2">[3]</ref> visualizations within a web browser. By exploiting the live data binding between vector graphics and backing data elements, they extract data, marks, and mappings between them. This information can then be used for tasks such as redesign and style transfer.</p><p>A number of projects focus on the more difficult problem of interpreting visualizations available only as static images. Savva et al. <ref type="bibr" target="#b24">[25]</ref> introduce ReVision, a system to classify bitmap chart images by chart type, automatically extract data from pie charts and bar charts, and generate redesigns using the extracted data table. Siegel et al. <ref type="bibr" target="#b25">[26]</ref> and Choudhury et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> present techniques to extract data from line charts using bitmap and vectorial images, respectively.</p><p>Other projects -including ChartSense <ref type="bibr" target="#b14">[15]</ref> and iVoLVER <ref type="bibr" target="#b20">[21]</ref> focus on semi-automated approaches, leveraging interactive annotation to perform accurate chart interpretation. We similarly provide an annotation interface that we use for corpus annotation; this interface can also be applied to correct automated extraction errors.</p><p>While reading data from a chart image is one potential goal of automatic interpretation, one might also wish to infer the program that generated the visualization. Extracting encoding specifications can be valuable for indexing and can often be performed even when occlusion and visual clutter render precise data extraction impossible. Poco &amp; Heer <ref type="bibr" target="#b21">[22]</ref> present a pipeline that specializes in accurate text localization and recognition within bitmap chart images, and classifies recovered text labels according to their role (e.g., x-axis label, y-axis label, legend label, etc). Using these components they are able to perform accurate extraction of spatial encodings. In this paper we follow a similar aim, but focus squarely on the problem of color mapping extraction. We adopt methods from Poco &amp; Heer <ref type="bibr" target="#b21">[22]</ref> to recognize the text content of legend labels. Siegel et al. <ref type="bibr" target="#b25">[26]</ref> also contribute a classifier to assign semantic roles to each text element, and use inferred legend labels to identify legend symbols. However, they assume that text information is given a priori, focus only on discrete color legends, and do not recover the color mapping. To the best of our knowledge, no prior work has proposed a technique to semi-automatically recover color mappings from a static visualization image.</p><p>In addition to indexing and redesign applications, automatic chart interpretation can enable new interactions with previously static media. Kong &amp; Agrawala <ref type="bibr" target="#b15">[16]</ref> demonstrate how to add interactivity to static pie and bar charts. They use the ReVision system <ref type="bibr" target="#b24">[25]</ref> to interpret the chart and generate graphical overlays to highlight marks and provide guideline annotations to assist chart reading. Inspired by this application, we introduce a novel application that leverages extracted color mappings to support interaction with either plotted data or color legends to highlight and summarize values of interest. Elmqvist et al. propose Color Lens <ref type="bibr" target="#b8">[9]</ref>, a technique that dynamically optimizes color scales based on a set of sampling lenses. Using this technique we can emulate some of the features of interactive overlays; however, Color Lens has a different goal and does not recover color mappings from bitmap images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA COLLECTION AND ANNOTATION</head><p>To develop techniques for color mapping extraction, we compiled an annotated corpus of visualization images using color encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Collection</head><p>We collected visualization images from both academic papers and the web sites of scientific institutions (e.g., NASA, universities). First we downloaded 16 million images from papers indexed by the Semantic Scholar 1 search engine. These chart images come from academic publications in Computer Science. In order to reduce the number of images and promote design variability, we selected figures from the areas of visualization, human computer interaction, computer vision, machine learning, and natural language processing. From an initial  collection of 330,000 figures, we manually selected 1,000 chart images containing explicit color legends. We initially selected a random subset of 200 images per category. We then removed figures without a color legend and randomly selected more figures to replace them; after some iteration, we arrived at a set of 1,000 figures. We then downloaded 275 papers from three journals influential in earth sciences: Nature, the Journal of Climate, and Geophysical Research Letters. To extract figures from journal PDF files, we used the pdffigures <ref type="bibr" target="#b6">[7]</ref> utility. We extracted 994 images and manually selected 500 chart images with color legends, using the same process for manual selection described previously. To further augment our collection, we crawled scientific websites such as NASA and university departments (e.g., climatology &amp; oceanography), netting 300 additional images with color legends. In sum, our corpus contains 1,800 visualization images with color legends. We manually classified each image as representing discrete or continuous data, and selected 800 charts uniformly at random for each type. We use these 1,600 chart images for training and testing throughout this paper. <ref type="figure">Figure 2</ref> shows examples from our corpus, including line charts, scatter plots, bar charts, area charts, pie charts, heat maps, and geographic maps. Our only constraint for selecting chart images was the inclusion of a color legend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Legend Annotation</head><p>For each visualization image, we classified the color legend as either discrete or continuous, and then manually labeled the legend elements using a custom graphical interface <ref type="figure" target="#fig_0">(Figure 3</ref>). Two annotators performed these tasks, requiring roughly 20 hours to complete using our graphical interface.</p><p>We define discrete color legends as consisting of a set of symbol elements l disc = {e 1 , e 2 , ..., e n }. Each symbol element is a 2-tuple e i = (c i ,t i ), where c i is a color and t i is the text associated with the color. Using our annotation interface, one can click a legend symbol to extract the representative color c i and its location p i . Additionally, to annotate the text t i associated with c i , one can draw a rectangle to cover the whole text element. Given the text bounding box, we attempt to recover the text content using the Tesseract OCR engine <ref type="bibr" target="#b26">[27]</ref> and manually correct the text as needed. We store both the text content and bounding box information. In <ref type="figure" target="#fig_1">Figure 4</ref>(a) the yellow circles represent the selected pixels and the red boxes represent the text elements.</p><p>We model continuous color legends as spatially contiguous gradients parameterized by the minimum and maximum extents. Thus,</p><formula xml:id="formula_0">l cont = {(c i ,t i ), p min ≤ p i ≤ p max },</formula><p>where p i is the i-th pixel position. In other words, a continuous color legend is represented by all the colors along the line between the minimum and maximum positions. To annotate continuous color legends within our interface, one can simply click the minimum and maximum positions. In <ref type="figure" target="#fig_1">Figure 4</ref>(b), these points are represented by the yellow circles. To recover the colors, we scan from p min to p max (for example, along the red line in <ref type="figure" target="#fig_1">Figure 4</ref>(b)). One can also draw rectangles next to p min and p max to cover the legend labels (red boxes in <ref type="figure" target="#fig_1">Figure 4</ref>(b)). We then apply OCR and (as needed) manual correction to recover the text content of the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COLOR MAPPING EXTRACTION METHOD</head><p>Our approach comprises 5 steps: (1) color legend identification, (2) legend classification, (3) color extraction, (4) legend text extraction, and (5) color mapping recovery. These steps are illustrated in <ref type="figure">Figure 5</ref>. In step 1, we automatically detect legends that occur outside the main plotting area; otherwise, users can manually indicate the legend region. In step 2, we classify the legend as discrete, continuous, or other (to capture images not supported by our technique). In steps 3 and 4, we automatically identify and process legend regions to extract colors and associated text, using different algorithms for the discrete and continuous cases. Finally, in step 5 we combine color and text information to recover color mappings. Across each step, we model colors using the CIE LAB color space. All the techniques described in this section are trained and validated using the annotated corpus of visualization images described in §3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Color Legend Identification</head><p>Automatic legend identification is a difficult problem, as there is significant variation in both placement and arrangement (i.e., vertical, horizontal, grid). While legends are sometimes placed outside the plotting area, it is also common to place them inside the plot area, particularly in academic papers where the amount of space is limited. In prior work, both Siegel et. al. <ref type="bibr" target="#b25">[26]</ref> and Poco &amp; Heer <ref type="bibr" target="#b21">[22]</ref> contribute classifiers that assign semantic roles to each text element (i.e., legend label, axis title, axis label, etc.). Legend regions could then be inferred based on recognized legend label and legend title elements. A shortcoming of Siegel et al.'s method is that they assume that text information is given a priori, which is not true for bitmap images. Poco &amp; Heer do not make this assumption and propose text localization and recognition methods; however, these steps can still produce errors that propagate forward.</p><p>Given the difficulty of accurate automatic legend extraction, and the relative ease with which users can simply indicate a legend region with a rectangular brush, our color extraction pipeline expects the legend region to be given as an input. Nevertheless, in this section we present a simple automatic method that does not require labeled text elements and is applicable when the legend is outside the plotting area. This method can be used to initialize a candidate legend region that can be interactively adjusted as needed.</p><p>Method: Detection of continuous color legends is typically straightforward, as the color gradient is commonly placed outside the plotting area. The intuition behind our method is that the plotting area will typically be the largest connected component in the chart, and the second largest component will be the color gradient. For example, see the heatmaps and geographic maps in <ref type="figure">Figure 2</ref>. We first binarize an input image, flood fill holes, and run the connected components algorithm. We then sort the connected components by total area and remove the largest one (i.e., plotting area). Next, we search through the other components in order. If the next largest component fits a rectangle, we return it as the color gradient. <ref type="figure">Figure 6</ref>(a) shows the largest component in orange, and the automatically recognized legend area in green. Discrete color legends are often placed inside the plotting area. However, if the legend has a rectangular border and is placed outside a well-delimited plotting area, we can apply the identical method described above for continuous legends. <ref type="figure">Figure 6</ref>(b) shows the identified plotting area and discrete legend for a line chart.</p><p>Obviously, our simple method will fail for cases that violate our assumptions. Our approach could be further augmented using the methods of Poco &amp; Heer <ref type="bibr" target="#b21">[22]</ref>, identifying legends based on text element classification. Within user-facing applications, one can also manually indicate a legend position by simply dragging a rectangle, as in our annotation interface.</p><p>Validation: Across our visualization image corpus, our simple legend identification technique correctly identifies 50% (400/800) of continuous legends and 10% (77/800) of discrete legends. As expected, the high prevalence in our corpus of legends placed within the plotting area contributes to poor performance, particularly in the discrete case. Other error cases include graphics that do not have a single dedicated plotting area (e.g., heatmaps over multiple 3D physical models) or the presence of variable background colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Color Legend Classification</head><p>Once we have identified the legend region in an image, we seek to classify the legend type in order to apply appropriate extraction methods. We trained a classifier that takes a legend region sub-image as input and classifies the image into one of three color legend types: discrete, continuous and other. This last class is included to recognize legend images that are not supported by our approach. In our current version, the other class includes random sub-images from chart images. We decided to train our classifier using these sorts of images to ensure the classifier results are useful for seeking user intervention if automated legend identification fails (i.e., it returns an incorrect legend region).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method:</head><p>We use a Convolutional Neural Network (CNN) model for classification. CNNs achieve state-of-the-art performance for many computer vision tasks and have been successfully used to classify the chart type (bar, line, scatter, etc.) of visualization images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. Training a CNN from scratch requires a large amount of labeled training data, yet we have only a few thousand images in our corpus. A common solution to this mismatch is to fine-tune a pre-trained network via backpropagation with additional images. Here, we fine-tune using the Caffe <ref type="bibr" target="#b13">[14]</ref> implementation of AlexNet <ref type="bibr" target="#b16">[17]</ref>, pre-trained on the millions of images contained in the ImageNet dataset. To train our CNN, we first extract the image regions (x l , y l , w l , h l ) for each discrete and continuous color legend to use as examples of those classes. To provide examples of the other class, for each chart image we additionally extract an image region (</p><formula xml:id="formula_1">x o , y o , w o , h o ), where (x o , y o )</formula><p>is a random coordinate and w o = r × w l , h o = r × w l , where r is a random (uniform) number between [0.9, 1.1]. We resize each extracted sub-image to a 256 × 256 pixel square, preserving aspect ratio and filling empty space with a white color.</p><p>Validation: We evaluate our classification approach using 2,400 images across 3 categories. To maintain parity among classes, for the other class we sample 800 of the 1,600 randomized image regions generated above. We then randomly split the data into training (80%) and test (20%) sets. <ref type="table">Table 1</ref> shows the resulting precision, recall and F1-scores for test set classification. Across all classes, we find that our classifier exhibits an average F1-score of 96%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Color Extraction</head><p>Given the color legend region and type we can proceed to extract colors, using different algorithms for the discrete and continuous cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Discrete Legends</head><p>As described in Section 3, we represent a discrete color legend as a set of elements l disc = {e 1 , e 2 , ..., e n }, where e i = (c i ,t i ). In this section, we present a technique to extract the colors c i . As shown in <ref type="figure" target="#fig_3">Figure 7</ref>, we first mask a legend image to isolate colored legend symbols and then use clustering to extract representative colors. Method: We apply two masks to the legend image to filter extraneous pixels: a background mask and a grayscale mask. We infer the background color c bg by computing a color histogram for all pixels in the legend image and selecting the most frequent color. In most cases the background color is white. As pixel colors may be noisy, we consider a pixel part of the background if its color c i is in the range c bg ± T bg , where we set T bg = 5 (or approximately two JNDs <ref type="bibr" target="#b19">[20]</ref>). We consider a color c i to be grayscale if both its a and b LAB components lie in the range 0 ± T gray (again, setting T gray = 5). The grayscale mask is useful for removing grid lines and text; however, it can also remove pixels that belong to a symbol encoded with a gray color. We describe how we recover those pixels later in this section.</p><p>Applying these masks leaves the pixels for the legend colors. <ref type="figure" target="#fig_3">Figure 7(b)</ref> shows the combined mask, and <ref type="figure" target="#fig_3">Figure 7</ref>(c) shows the resulting colored pixels. Note that the colors for each symbol might not be homogeneous, especially along the symbol edges. As the color values are noisy and also potentially discontinuous (see inline <ref type="figure">figure)</ref>, we can not use simple color binning or connected components analysis. We need a more robust approach for grouping pixels with similar color.</p><p>We apply a clustering algorithm to identify the legend colors. We represent each colored pixel using a five-dimensional feature vector consisting of the three LAB color channel values plus the x and y image pixel coordinates: <ref type="figure">(L, a, b, x, y)</ref>. In most cases, color information is enough to groups the pixels. However, some legends have colors with the same hue but varying luminance (see inline <ref type="figure">figure)</ref>. In such cases our clustering algorithm could fail if the colors are near each other in the CIELAB space. Pixel coordinates help to separate these cases.</p><p>We then use the DBSCAN <ref type="bibr" target="#b9">[10]</ref> clustering algorithm. DBSCAN does not require that the number of clusters be chosen a priori, and accepts parameters that are well defined for our setting. The ε parameter sets the maximum distance for two points to be considered in the same neighborhood; we use ε = 5. The minPoints parameter sets the minimum number of points in a cluster. We assume that a color legend has at most 20 symbols and so minPoints = |color pixels|/20. For each resulting cluster g i , we then select the most common color to be the representative legend color c i . <ref type="figure" target="#fig_3">Figure 7(d)</ref> shows the locations of pixels with representative colors (yellow circles).</p><p>As mentioned earlier, our grayscale mask may erroneously remove pixels that belong to gray-colored legend symbols. To recover such pixels, we run the connected components algorithm within the grayscale mask, and retrieve components with an area (i.e., number of pixels) similar to the area of the discovered clusters g i .</p><p>Validation: We evaluate our discrete color extraction method against our visualization corpus. We use precision, recall and F1-score metrics typically used for comparing inferred bounding boxes in the context of text localization <ref type="bibr" target="#b18">[19]</ref>. However, in our case, we compare two sets of colors (estimated and ground truth) instead of bounding boxes. For a color c we find the best match in a set of colors S such that:</p><formula xml:id="formula_2">m(c, S) = min c ∈S dist color (c, c )<label>(1)</label></formula><p>where dist color (c i , c j ) is the CIEDE2000 color difference. We apply this best matching to align the ground truth legend colors T and estimated colors E. We define δ (c i , c j ) = dist(c i , c j ) &lt; 5. Note that δ is 1 if two colors are within 5 CIE LAB units, 0 otherwise. We noticed that comparing colors (c i , c j ) in the representative pixels (p i , p j ) can be very sensitive, specially when legend symbol is a segment line. This problem arises specially in annotated data. To address this issue, we use patch regions centered at representative pixels (p i ) and width 3. Thus, dist(c i , c j ) returns the minimum color difference between patches centered at p i and p j .</p><p>We define precision, recall and F1-score as follows:</p><formula xml:id="formula_3">p = ∑ e∈E δ (e, m(e, T )) |E| , r = ∑ t∈T δ (t, m(t, E)) |T | , F1 = 2 p • r p + r<label>(2)</label></formula><p>Our technique has a precision of 92%, recall of 89%, and F1-score of 90% in our 800 discrete color legends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Continuous Legends</head><p>In this section, we describe how to extract colors for a continuous legend. To characterize a continuous color legend, we must identify the color gradient end points p min and p max . We record the pixel locations for each end point and extract colors by scanning the line between the two. <ref type="figure" target="#fig_4">Figure 8</ref> illustrates each step in this technique.</p><p>Method: Akin to the discrete case, we first infer the background color and remove background pixels. Next, we binarize the legend image using a global threshold approach. Note that the bottom part of the color gradient in <ref type="figure" target="#fig_4">Figure 8(b)</ref> is incomplete. To recover such regions, we use a flood fill algorithm. <ref type="figure" target="#fig_4">Figure 8(c)</ref> shows the completed region.</p><p>We noted that in some charts the text elements are very close to the colorbar (see inline <ref type="figure">figure)</ref>. To separate them, we apply a morphological erosion operation <ref type="figure" target="#fig_4">(Figure 8(d)</ref>). Then we run the connected components algorithm <ref type="figure" target="#fig_4">(Figure 8</ref>(e)). Our intuition is that the color gradient should be the largest connected component and all the text characters should be smaller. Thus, we select the largest component <ref type="figure" target="#fig_4">(Figure 8(f)</ref>) and fit a rectangle that covers these pixels. We note if the rectangle has a vertical or horizontal orientation based on its aspect ratio. If vertical, we determine p min and p max by selecting two aligned points, one at the top and one at the bottom side of the rectangle, each horizontally centered. Scanning a line that join these two points, we recover the colors belonging to this legend (See <ref type="figure" target="#fig_4">Figure 8(g)</ref>). An analogous approach applies for the horizontal case.</p><p>Validation: We evaluate continuous legend color extraction on the 800 continuous examples in our corpus. For each image, we estimate p min and p max using the method above and compare these two points with the ground truth data p min and p max . For a vertical rectangle, we deem two points similar if |p y − p y | &lt; 3 (i.e., the y-coordinates are at most 3 pixels away). If both p min and p max lie near the ground truth values, we consider the color gradient to have been successfully identified. The horizontal case is treated similarly.</p><p>Our method achieves accurate extraction for 83% (662/800) of the continuous legends in our corpus. Recurring error cases involve confusion between the color gradient and background colors (i.e., when a color gradient is placed within the plotting area) and confusion due to text labels placed so close to the gradient that they become part of the same connected component. Perhaps ironically, this first error case is most prevalent in the visualization literature, as other disciplines tend to place the color gradient outside the plotting area.</p><p>Quantized Continuous Domains: It is common to use discretized version of continuous legends <ref type="figure" target="#fig_5">(Figure 9(a)</ref>). Our color legend classifier classifies these legends as continuous, and we process them using our continuous color extraction methods (i.e., determining p min and p max ). However, we can then perform a post-processing step to properly handle this legend type.</p><p>Consider the horizontal color gradient in <ref type="figure" target="#fig_5">Figure 9</ref>(a). First we compute the horizontal derivative to identify strong changes along the x-axis. We apply a Sobel filter with a kernel of size 3. <ref type="figure" target="#fig_5">Figure 9(b)</ref> shows the absolute values of this derivative (computed as the sum of r, g, and b color channel derivatives). We next convolve the data with a wavelet of size 10 (this value is the expected range that should cover the peaks of interesest) and extract the peaks. Given k identified peaks, we extract a set of k + 1 colors by picking the colors at the midpoints between peaks, or between the legend boundary and nearest peak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Legend Text Extraction</head><p>The previous sections describe how we extract colors for discrete and continuous legends. We now present a method for recovering text elements from a legend image <ref type="figure">(Figure 5(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method:</head><p>We first apply the text localization method of Poco &amp; Heer <ref type="bibr" target="#b21">[22]</ref>. This approach consists of three stages: (1) word detection, (2) optical character recognition, and (3) word merging. In stage 1, a CNN trained to classify text vs. non-text pixels is used to mask non-text pixels. Then, standard region-based text localization methods are applied to output bounding boxes for individual words. In stage 2,  we apply optical character recognition to each candidate word using the open source Tesseract <ref type="bibr" target="#b26">[27]</ref> engine. Finally, in stage 3, words are merged into phrases based on their orientation and geometric relationships. Before submitting legend images to the text localization pipeline, we remove all pixels that belong to either discrete legend symbols or continuous color gradients. This preprocessing step cleans the input images in order to improve text localization performance.</p><p>Validation: To evaluate the text extraction technique, we use the definition of best matching depicted in Section 4.3.1 (Equation 1), but in our case, we compare two sets of boxes (estimated and ground truth) instead of colors. For a box b, we find the best match in a set of boxes S such that:</p><formula xml:id="formula_4">m(b, S) = max b ∈B dist box (b, b )<label>(3)</label></formula><p>Where</p><formula xml:id="formula_5">, dist box (b i , b j ) = 2 area(b i ∩b j )</formula><p>area(b i )+area(b j ) . Note that dist box is 1 for equal boxes and 0 for boxes without intersection. We then apply this matching to our bounding boxes T (ground truth boxes) and E (estimated boxes) in a legend image. We define precision, recall and F1-score as follows:</p><formula xml:id="formula_6">p = ∑ e∈E m(e, T ) |E| , r = ∑ t∈T m(t, E) |T | , F1 = 2 p • r p + r<label>(4)</label></formula><p>Using only the discrete color legends we obtain a precision of 82%, recall 91% and F1-score 86%. If we do not remove the colored pixels beforehand the F1-score is 83%; the OCR results are meaningfully improved by this filter. In addition, these performance results are very sensitive to the bounding boxes. For example, if we shrink or expand the ground truth boxes by 2 pixels, the F1-scores reduce to values between 82% and 85%. The main errors arise when legend symbols contain multiple colors, in which case our preprocessing can fail to remove those pixels.</p><p>Using the continuous color legends we obtain a precision of 78%, recall 79% and F1-score 78%. The main issues arise with tick marks. A tick may be confused with the '-' character, extending the text bounding box. In some cases ticks connect the text with the color gradient, causing the text localization step to throw out text along with the color gradient. Across both legend types, we obtain an overall precision of 80%, recall 85% and F1-score 82%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Recovering Color Mapping</head><p>Given the color information and the text extracted in the last two sections, we can recover the full color mapping. To do so, we need to associate the text values with the color information ( <ref type="figure">Figure 5</ref>(e)).</p><p>Discrete color legends: Here, we connect each text bounding box with a legend symbol -represented by a pixel inside the legend symbol. For each text bounding box, we calculate the closest legend symbol and associate this text with the color. This strategy also works for legends with multi-line text <ref type="figure" target="#fig_6">(Figure 10(a)</ref>). However, for horizontally-aligned elements or multiple columns, this strategy can fail. In <ref type="figure" target="#fig_6">Figure 10(b)</ref>, the legend text "Roc-SVM" might be considered closest to the brown legend symbol for "CR-EM". To address this issue, we must determine if text lies to the left or right of the legend symbols. We first sort by x-coordinate of the representative pixels for each legend symbol. Next we check if the left-most and right-most pixel are closer to the left or right side of the legend bounding box, respectively. If the left-most pixel is closer, then text lies to the right (See <ref type="figure" target="#fig_6">Figure 10(b)</ref>), otherwise text lies to the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous color legends:</head><p>We first identify if the legend gradient is vertically or horizontally oriented. We do so by checking the alignment of the p min and p max coordinates. For the horizontal case, we sort the text bounding boxes by the x-coordinate of their centers. We denote the center of the left-most and right-most text bounding boxes with p le f t and p right respectively (see <ref type="figure" target="#fig_7">Figure 11)</ref>. We map the value represented by the text in both bounding boxes to p le f t and p right , respectively. Finally, we extrapolate the values out to p min and p max . An analogous approach applies for the vertical case.</p><p>In order to extract the values represented by the text, we first attempt to parse the label text as number, including scientific notation (e.g., '-10', '15', '4.5', '3e-4'). Next, we verify if there is a modifier, such as characters indicating units. We check if the text is using International System units (e.g., '1M, '1k, '10M) and parse these as numbers (e.g., '1k' is converted to 1,000). We leave text parsing of other cases, such as physical units of measurement, as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">END-TO-END PERFORMANCE</head><p>At first blush our pipeline might look complex, given the multiple processing stages. An alternative approach might be to analyze a color histogram for each image and skip the color legend identification step. However, we found that the unbalanced number of pixels for each color makes subsequent color extraction less accurate. Moreover, to automatically infer a color mapping one must identify the legend region. This step could increase the complexity of our pipeline and suffer from error propagation (see discussion by Poco &amp; Heer <ref type="bibr" target="#b21">[22]</ref>). Instead, we assume the legend regions are provided as input, as it is relatively easy for a user to drag a rectangle over them.</p><p>The validations presented in §4 assume that output from a previous step is perfect. However, without user intervention, an error generated in an earlier step can propagate through the pipeline, reducing the final accuracy. As mentioned above, we present our results without interdependence of the pipeline steps. Though users can manually fix interpretation errors using our annotation interface, for some applications (e.g., analysis of visualizations practices "in the wild") a fully automatic method is required.</p><p>For an end-to-end analysis, we assume legend regions are given as input to our pipeline. Our color and text extraction components then depend on the output of legend type classification. However, there is no interdependence between color and text extraction: both steps can be performed in parallel as shown in <ref type="figure">Figure 5</ref>. For discrete color legends, we achieve local F1-scores of 96% in the legend classification step, 90% in the color extraction step, and 87% in the legend text extraction step. If we propagate errors, we have final F1-scores of 82% for color extraction and 80% for the legend text extraction. For continuous color legends, we have local F1-scores of 96% in the legend classification step, 82% in the color extraction step, and 80% in the legend text extraction step. If we propagate errors, we have final F1-scores of 79% for color extraction and 77% for the legend text extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICATIONS OF COLOR MAPPING EXTRACTION</head><p>Our extraction methods can be used to support a variety of visualization applications. For example, our work can be used to extend existing visualization reverse-engineering tools <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> with support for color encodings, improving tasks such as indexing for visualization search engines <ref type="bibr" target="#b4">[5]</ref>. Here, we contribute two novel user-facing applications: automatic recoloring and interactive overlays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Automatic Recoloring</head><p>Our first application performs automatic recoloring: given bitmap images as input, we produce new images that use perceptually-motivated color encodings. This application works for both discrete and continuous color legends. <ref type="figure" target="#fig_8">Figure 12</ref> shows examples for four chart images: the first row contains original visualizations and the second row shows output images with new color encodings.</p><p>Discrete color legends. Given representative colors C = {c 1 , ..., c n } inferred by our extraction methods and a target color scheme T = {t 1 , ...,t n }, we create a transfer function t i = f disc (c i ) that simply maps from one indexed color to another. For each pixel p i in the image, we find the nearest color in C such that dist color (c i , c s ) &lt; 2.5 and then set p i to the color f disc (c i ). The distance constraint helps avoid recoloring pixels that do not belong to the color legend (e.g., grid lines or text). The bar chart in <ref type="figure" target="#fig_8">Figure 12</ref>(a) uses a sequential color encoding where a categorical encoding might be more appropriate. In <ref type="figure" target="#fig_8">Figure 12</ref>(e), our tool replaces the original scheme with a ColorBrewer palette. The line chart in <ref type="figure" target="#fig_8">Figure 12</ref>(b) uses a discrete rainbow color map. Some legend items (e.g., "RNN1" and "RNN4") are difficult to discriminate. In <ref type="figure" target="#fig_8">Figure 12</ref>(f) we retarget the color encoding to a more perceptually effective scheme.</p><p>Continuous color legends. For continuous legends, we begin with the extracted colors spanning the minimum and maximum points in the color gradient ({c i | p min &lt; p i &lt; p max }), and a target continuous color palette T parameterized on the interval [0, 1]. We define a transfer function t i = f cont (c i ), such that c i and t i occur at the same relative index in their respective color ramps. For each pixel p i in the image, we search for the nearest color in C such that dist color (c i , c s ) &lt; 2.5 and recolor the pixel with f cont (c i ).</p><p>The map in <ref type="figure" target="#fig_8">Figure 12</ref>(c) uses a multi-hue color gradient across a domain spanning both negative and positive values. In response, we recolor the image using a diverging color scheme as shown in <ref type="figure">Figure</ref> 12(g). <ref type="figure" target="#fig_8">Figure 12</ref>(d) uses a rainbow color scheme; in <ref type="figure" target="#fig_8">Figure 12</ref>(h) we replace the rainbow with a perceptually-informed sequential scheme.</p><p>For these recoloring tasks, we need only extract the color information provided in the legend. The actual legend text is not necessary, making our tool robust to issues such as OCR error. However, as our next application demonstrates, recovering the legend values can enable more sophisticated interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Interactive Overlays</head><p>Our second application adds interactive overlays <ref type="bibr" target="#b15">[16]</ref> to static images to support data querying and highlighting. Our web application generates an interactive visualization from a static image input. First, the system must locate the color legend. If automated identification ( §4.1) fails, a user can simply draw a rectangle to isolate the legend. Next we recover legend color and text information using our extraction techniques. If a user notes any extraction errors, they can use our annotation interface ( §3.2) to correct them manually. The application then generates an interactive visualization that supports two-way interactions between the legend and plotting area. Users can brush in either region to see corresponding information highlighted in the other component. <ref type="figure" target="#fig_0">Figure 13</ref> shows examples of such interactions across four different chart types.</p><p>From legend to plot area. We provide two types of interactions: point and range selection. These interactions are designed to highlight selected data values, as illustrated in the second row of <ref type="figure" target="#fig_0">Figure 13</ref>.</p><p>Point selection is supported for both discrete and continuous color legends. For discrete legends, users can click either a legend symbol or the text associated with the symbol. We then identify the associated representative color c sel . Next, we overlay the plotting area with a translucent white layer, using full transparency for pixels p i that satisfy dist color (c sel , c i ) &lt; 2.5. If the legend is inside the plot area (as in <ref type="figure" target="#fig_0">Figure 13</ref>(e)), we make that region transparent as well. For continuous color legends, a user can click anywhere within the color gradient to select the color c sel . Then, we similarly add a translucent overlay, but with full transparency for pixels p i that satisfy c i = c sel .</p><p>Range selection is often more appropriate for continuous color legends. When a user draws a rectangle (R) within the color gradient, we select the set of colors C sel = {c i | p i ∈ R}. As with point selection, we then add a translucent overlay with full transparency for pixels that satisfy c i = c s , ∀c s ∈ C sel .</p><p>From plot area to legend. For interactions with the plot area, we similarly support both point and range selections. These interactions are depicted in the third row of <ref type="figure" target="#fig_0">Figure 13</ref>.</p><p>Point selections can be performed visualizations with either discrete or continuous color legends. For discrete legends, users can click a data-encoding mark in the plot area. We then select the color c sel at the click position p sel , and use it to identify the legend element e sel with representative color nearest to c sel . Next, we overlay the legend region with a translucent white layer. We use the information in e sel (i.e., text and color) to make the layer transparent over pixels that lie inside the text bounding box or satisfy c sel = color(e sel ). We also add a layer over the plot area, with full transparency for pixels that satisfy dist color (c sel , c i ) &lt; 2.5. In this second layer highlights all marks associated with the same e sel (for instance, the multiple bars highlighted in <ref type="figure" target="#fig_0">Figure 13</ref>(i)). For continuous legends, when a user clicks in the plot area we select the color c sel and search for the nearest color in the legend. We proceed as before to highlight pixels in the plot area. However in the legend region, we highlight only the nearest color in the color gradient.</p><p>Our tool also supports range selections for continuous encodings. A user can draw a rectangle (R) in the plot area, for which we retrieve all contained colors C sel = {c i | p i ∈ R}. Then, for all colors in C sel , we find the nearest colors in the legend gradient and highlight them <ref type="figure" target="#fig_0">(Figure 13(k)</ref>). In this case, we do not add an overlay to the plot area.</p><p>The interactions described above require only color information from the legend. However, we can use the full mapping (i.e., inferred data values) to enable additional features. Our tool calculates and displays descriptive statistics for the data values indicated by the selected pixels. For example, <ref type="figure" target="#fig_0">Figure 13</ref>(m) uses the data values associated with the colors in C sel to create a histogram for the selected region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LIMITATIONS</head><p>Our color legend classifier includes the category other, which we train using random sub-images sampled from our corpus. An improvement would be to collect real legend images for other encoding channels (e.g., size, shape) and train our classifier to discriminate those.</p><p>Given the variability of color legend styles, we constrained the color legend types supported. For example, we do not support grayscale legends. Our color extractor for discrete color legends uses a grayscale mask to remove text and grid elements from the legend. As a result, all legend symbols in grayscale are also removed. However, if not all the legend symbols are grayscale, we can recover them as explained in § 4.3.1. We also do not support bi/trivariate color maps, which are uncommon in the figures we collected. In the case of legend symbols with repeated colors (e.g., "red" symbol with a solid line and "red" symbol with a dashed line), our color extractor might fail because we are using LAB coordinates in the clustering, and all the red pixels will be near each other in the CIELAB space.</p><p>In the legend text recognition step, we use the techniques of Poco &amp; Heer <ref type="bibr" target="#b21">[22]</ref>. However, OCR errors remain an issue in some cases. Recurring challenges for text localization include the resolution of the legend labels, spacing between labels and color elements, and mathematical notation. Moreover, in this work we are not using the "units" information included in some continuous color legends. For applications such as indexing figures (e.g. for a scientific database), accurately identifying units may be important.</p><p>We currently do not support accurate data value interpolation for non-linear color gradients. To do so, our method must also (a) extract intermediate value labels (not only minimum and maximum), (b) correctly associate the labels with positions in the color gradient, and (c) infer the the type of scale function (e.g., log, square root) based on the label values and spacing.</p><p>Step (c) is relatively straightforward and supported in prior work <ref type="bibr" target="#b21">[22]</ref>. However, step (a) depends heavily on OCR accuracy.</p><p>For our recoloring application, errors arise when the legend includes colors that also occur in other chart components (e.g., black for text and grid lines, white for background). In <ref type="figure" target="#fig_8">Figure 12</ref>(g), the chart title was affected by our transfer function because the initial color gradient contains black as well.</p><p>In addition, we found that some bar charts contain elements with variable colors (e.g., gradient fills). This can lead to the same color occurring across marks associated with different legend elements. For instance, in the inline figure, the colors inside the two yellow rectangles are the same, even though the two bars correspond to different legend entries. As a result, our interactive overlay application does not highlight the bars correctly. A more sophisticated color matching model is needed to handle cases such as these shading gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper we presented methods for recovering color mappings from static visualization images. To evaluate our approach we compiled an annotated corpus of images and color legend information for 1,600 visualizations from academic documents, and demonstrated accurate extraction of both discrete and continuous color encodings. While we focused on the general case of bitmap images, components of our system are directly applicable to structured image formats such as vector PDF or SVG documents, where objects and text content might be trivially extracted.</p><p>We also presented applications of our method for automatic recoloring and generating interactive visualizations from static images. Conveniently, many of the features of these applications do not require the full mapping from color to data value, for example supporting recoloring and highlighting based on extracted colors alone. However, as demonstrated by interactive overlay application, access to the full color mapping can provide even richer interactive capabilities.</p><p>To the best of our knowledge, this research contributes the first approach to recover full color mappings from visualization images. Going forward, we hope to improve our methods and explore additional applications enabled by our approach.</p><p>An immediate future work item is to further improve the performance of each stage in our pipeline, for example by addressing the limitations enumerated above. Looking further out, we might also extend our techniques to recover mappings for other non-spatial channels, such as size, shape, and texture encodings. We might also extend our work to support bi/trivariate color maps. Combining legend analysis with reverse-engineering pipelines for spatial encodings <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> might enable rich indexing of visualizations, in turn supporting new visualization search and retrieval applications.</p><p>Another area for future work is to perform a more thorough comparison with previous approaches. The closest work is Siegel et al.'s approach <ref type="bibr" target="#b25">[26]</ref> to extract legend symbols from discrete color legends. An appropriate comparison would be to run our pipeline on Siegel et al.'s annotated image dataset (1,000 annotated charts). For that, we will need to parse the figure metadata and infer the legend regions using the bounding boxes of the legend text and legend symbols. However, comparing text elements would be unfair because Siegel et al. assume that the text information is given as input. Also, in Siegel et al.'s dataset the legend symbols are annotated using bounding boxes; to compare with our color extraction method, we will need to process each bounding box region and extract the colors used.</p><p>We might also use our color extraction techniques to perform a large scale analysis of the use of colors "in the wild". We can analyze a large collection of visualization images from academic documents across different communities, identify common color schemes, and assess how they may have evolved or disseminated over time.</p><p>To help enable such future applications, both our annotated visualization image corpus and the source code for our color mapping extraction methods are freely available at https://github.com/uwdata/rev.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Graphical user interface for legend annotation. The bottom panel shows chart images in our corpus. Outline colors represent the color legend type: orange for continuous and blue for discrete.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Annotations on color legends. Note that we extract different information for (a) discrete and (b) continuous legend types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>o r E x t r a c t i o n T e x t E x t r a c t io n discrete c o n t in u o u s Steps to infer color mappings from a chart image. (a) Localize the legend region in the chart images. (b) Use a CNN to classify the color legend as discrete, continuous, or other. (c) Depending on the legend type, process the legend to extract colors. (d) Use OCR to recover legend text. (e) Merge color and text information to recover the color mapping. Automatically identified plotting areas (orange) and legends (green) for both (a) a continuous legend and (b) a discrete legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Extracting color from discrete color legends. (a) Original legend image. (b) Mask combining background and grayscale masks. (c) Color pixels after applying the mask. (d) Yellow circles indicate pixels with representative colors found via cluster analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Extracting color from continuous color legends. (a) Original legend image. (b) Binarized image. (c) Flood fill to recover color gradient. (d) Morphological erosion to separate text from legends. (e) Connected components. (f) Color gradient is the largest connected component. (g) Yellow circles indicate positions of minimum and maximum values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Analysis of quantized legends for continuous data. (a) Some legends discretize a quantitative domain. (b) Breaks between colors correspond to peaks in the absolute derivative of pixel color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Recovering color-value mappings for discrete legends. In (a), we associate text with the nearest legend symbol. For multi-column legends (b), we first identify if the text is on the right or left of the legend symbols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Recovering color-value mappings for continuous color legends. Values in bounding boxes b le f t and b right are mapped to positions p le f t and p right , respectively. We then extrapolate values for p min and p max .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Automatic Recoloring: Given a chart image and target color scheme, we generate a recolored image with an alternative color encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Interactive Overlays. The first row contains input chart images. The second row shows interactive highlights of the data in response to selecting legend values. The third row illustrates highlights in response to selecting marks in the plotting area. Sub-figure (m) shows a value histogram for the selected pixels, generated by inverting the extracted color mapping.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.semanticscholar.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by a Paul G. Allen Family Foundation Distinguished Investigator Award and the Moore Foundation Data-Driven Discovery Investigator program. The second author gratefully acknowledges CONCYTEC for a scholarship in support of graduate studies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of artery visualizations for heart disease diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gajos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mitsouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melchionna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rybicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2479" to="2488" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rainbow color map (still) considered harmful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14" to="17" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">D3: Data-Driven Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Visualization &amp; Comp. Graphics (Proc. InfoVis)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Color use guidelines for data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Section on Statistical Graphics</title>
		<meeting>the Section on Statistical Graphics</meeting>
		<imprint>
			<publisher>American Statistical Association</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DiagramFlyer: A search engine for data-driven diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web, WWW &apos;15 Companion</title>
		<meeting>the 24th International Conference on World Wide Web, WWW &apos;15 Companion<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable algorithms for scholarly figure mining and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Big Data, SBD &apos;16</title>
		<meeting>the International Workshop on Semantic Big Data, SBD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pdffigures 2.0: Mining figures from research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, JCDL &apos;16</title>
		<meeting>the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, JCDL &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bridging theory with practice: An exploratory study of visualization use and design for climate model comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="996" to="1014" />
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Color lens: Adaptive color scale optimization for visual exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="795" to="807" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 2nd International Conference on Knowledge Discovery and</title>
		<meeting>of 2nd International Conference on Knowledge Discovery and</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Colorgorical: Creating discriminable and preferable color palettes for information visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Gramazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deconstructing and restyling d3 visualizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology, UIST &apos;14</title>
		<meeting>the 27th Annual ACM Symposium on User Interface Software and Technology, UIST &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Color naming models for color selection, image editing and palette design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ChartSense: Interactive data extraction from chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-I</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graphical overlays: Using layered elements to aid chart reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2631" to="2638" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selecting semantically-resonant colors for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (Proc. EuroVis)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ICDAR 2005 text locating competition results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Document Analysis and Recognition (IC-DAR&apos;05)</title>
		<imprint>
			<date type="published" when="2005-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="80" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation of uniform color spaces developed after the adoption of cielab and cieluv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Eycken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oosterlinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Color Res. Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="121" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">iVoLVER: Interactive visual language for visualization extraction and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Méndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nacenta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenheste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI &apos;16</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems, CHI &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4073" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reverse-engineering visualizations: Recovering visual encodings from chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (Proc. EuroVis)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="353" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Curve separation for line graphs in scholarly documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ray</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, JCDL &apos;16</title>
		<meeting>the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, JCDL &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="277" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How not to lie with visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Rogowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Treinish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bryson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Physics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="273" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ReVision: Automated classification, analysis and redesign of chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chhajta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology, UIST &apos;11</title>
		<meeting>the 24th Annual ACM Symposium on User Interface Software and Technology, UIST &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FigureSeer: Parsing result-figures in research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision, ECCV &apos;16</title>
		<meeting>the European Conference on Computer Vision, ECCV &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An overview of the tesseract ocr engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Document Analysis and Recognition</title>
		<meeting>the Ninth International Conference on Document Analysis and Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page" from="629" to="633" />
		</imprint>
	</monogr>
	<note>ICDAR &apos;07</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
