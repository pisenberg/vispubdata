<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Exploration of Semantic Relationships in Neural Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Peer-Timo</roleName><forename type="first">Shusen</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayaraman</forename><forename type="middle">J</forename><surname>Bremer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Thiagarajan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Srikumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarden</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Livnat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascucci</surname></persName>
						</author>
						<title level="a" type="main">Visual Exploration of Semantic Relationships in Neural Word Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2017.2745141</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP). However, despite their widespread application, little is known about the structure and properties of these spaces. To gain insights into the relationship between words, the NLP community has begun to adapt high-dimensional visualization techniques. In particular, researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (e.g., word analogies), respectively. Unfortunately, these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings. Here, we introduce new embedding techniques for visualizing semantic and syntactic analogies, and the corresponding tests to determine whether the resulting views capture salient structures. Additionally, we introduce two novel views for a comprehensive study of analogy relationships. Finally, we augment t-SNE embeddings to convey uncertainty information in order to allow a reliable interpretation. Combined, the different views address a number of domain-specific tasks difficult to solve with existing tools.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Natural language processing (NLP) is one of the key components in today's digital world responsible for everything from web search to document classification and from machine translation to speech recognition. A crucial breakthrough that led to the recent surge of AI research in NLP is the concept of neural word embeddings, such as word2vec <ref type="bibr" target="#b26">[27]</ref> or Glove <ref type="bibr" target="#b32">[33]</ref>. These systems utilize a large corpus of training articles to determine the co-occurrence statistics between pairs of words within a given context, and employ a neural network to infer a vector space for embedding words. Interestingly, the position and difference vectors between words appear to encode semantic relationships (see <ref type="figure" target="#fig_0">Fig. 2</ref>). One of the most striking examples is analogy pairs such as (king, queen) and (man, woman). In the word embedding space, one finds that (woman + kingman) ≈ queen <ref type="bibr" target="#b28">[29]</ref>. Broadly speaking, encoding words or even sentences into intermediate vector representations provides the foundation for a number of NLP applications, such as sentiment analysis <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref> or document ranking <ref type="bibr" target="#b15">[16]</ref>. However, despite its central importance and wide-scale adoption, the word embedding space remains a rather abstract and unintuitive concept to most NLP researchers.</p><p>To encode a large number of semantic relationships between words from a large corpus of text, the embedding dimension is chosen to be reasonably high (∼300). Reasoning in such spaces is difficult and thus some NLP researchers have turned to visualization for more intuitive interpretations of the word embedding space. In particular, nonlinear dimension reduction strategies, most notably t-distributed stochastic neighbor embeddings (t-SNE) <ref type="bibr" target="#b41">[42]</ref>, are used to provide a high-level overview of the embedding space. Although such an embedding can reveal some interesting separation between word groups, i.e., countries, nouns, verbs, etc., they inherently distort the linear (semantic) relationships most interesting to researchers. Consequently, to preserve such relationships, linear projections are preferred. The most common approach is to use principal component analysis (PCA) restricted to carefully chosen subsets of words, i.e., countries and capitals, nouns and their plurals, etc. Unfortunately, both the linear (PCA) and nonlinear (t-SNE) approaches, which are now the de facto standard in NLP research, are fairly limited and often misleading. For example, t-SNE embeddings are often used to validate (or discredit) various intuitions on the nature of the embedding space without any consideration for the inherent distortions in the projection itself. Given the complex nature of the high-dimensional space, any two-dimensional embedding will exhibit significant distortions and thus any given feature may in fact be an artifact. Similarly, the PCA embeddings rely on the fact that the semantic direction of interest, i.e., the vector (man -king), has more variation than other directions. As demonstrated in this paper, such an assumption is sometimes true, in which case PCA embeddings work reasonably well. However, in other cases the variation within one word group, i.e., countries, can be greater than the distance to a related group, i.e., currencies and the PCA embedding fails to provide the expected alignment. In addition, for a given analogy type, binary labels for words are known. However, such important information is not utilized in the PCA. In general, we find that the embeddings used in NLP research are not necessarily ideal. Furthermore, in a number cases, embeddings are provided as is, with little information on how they were created or how reliable they might be. This lack of information invariably leads to misuse or misleading interpretations of the visualization results.</p><p>As part of a long-standing collaboration with domain scientists, we present a system aimed at addressing some of these problems. The goal is twofold: First, to develop new tools specifically designed to answer such questions as how well a given semantic relationship is approximated by a single direction or how different semantic concepts are related (tasks the proposed tool aims to address are characterized in <ref type="table">Table 1</ref>); and second, to provide users with more information about how to interpret the visualization results. In particular, we enhance the global view (computed by t-SNE) by incorporating per-word distortion metrics as well as an interactive display of neighboring words in the highdimensional space. This augmentation provides an intuitive illustration of which apparent features in the data are trustworthy and a quick way to explore the embedding in detail. Furthermore, we introduce new approaches to compute linear embeddings of semantic relationships (by utilizing the binary label information in an analogy type) that simultaneously maximize the separation of the two concepts, i.e., male vs. female, and minimize the differences between semantic directions, i.e., the vectors (man -king) vs. (woman -queen). Finally, to verify that the resulting projections indeed capture true semantics rather that an accidental alignment, we extend the notion of hypothesis testing to the embedding and provide users with the equivalent of a p-value for a given result. Our contributions in detail are:</p><p>• A characterization of domain-specific tasks for exploring semantic relationships in neural word embeddings (see <ref type="table">Table 1</ref>);</p><p>• An interactive word embedding visualization tool specifically designed to support NLP research;</p><p>• A generalization of hypothesis testing for embeddings to evaluate the saliency of the apparent structure; and</p><p>• A case study demonstrating new insights into a well-known semantic analysis dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Visualization has been used to tackle several challenges in text analysis and NLP, such as topic modeling and sentiment analysis. In the existing literature, several visualization systems, including the Termite <ref type="bibr" target="#b7">[8]</ref>, the Hiérarchie <ref type="bibr" target="#b37">[38]</ref>, and the concurrent words and topics visualization <ref type="bibr" target="#b36">[37]</ref>, have attempted to include humans in the analysis loop for identifying topics through visual encodings. Sentiment analysis applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45]</ref> find and track the sentiment toward topics or summarize the evolution of topics in social media feeds and other mass text sources. These techniques are useful, but they focus on more abstract concepts and are not well suited to understand low-level details present in word embeddings such as analogy relationships. Furthermore, understanding neural models as well as their training process has also attracted interests. Interactive tools, such as LAMVI <ref type="bibr" target="#b34">[35]</ref>, are designed to help domain experts understand the effects of training parameters and debug the training process. The recent work of Li et al. <ref type="bibr" target="#b18">[19]</ref> focuses on visualizing the compositionality (build sentence meaning from the meanings of words and phrases) of vector-based models. Gladkova et al. <ref type="bibr" target="#b11">[12]</ref> questioned the existing intrinsic (semantics) evaluation approach for word embeddings that relying on abstract ratings and argued the importance of exploratory evaluation that characterizes embeddings' strengths and weaknesses. Compared to techniques often seen in the visualization community (where novel visual encoding plays a key role), the focus of these works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> is on carefully designed experiments that stem from an in-depth understanding of the model. The visualization (e.g., heatmap) mostly plays a supplementary role in aiding the interpretation of the experiment results. There are generic dimension reduction tools that can be applied to word embeddings. For example, the Embedding Projector <ref type="bibr" target="#b35">[36]</ref> is a new embedding visualization tool released by Google as part of the Tensor-Flow framework <ref type="bibr" target="#b1">[2]</ref>. In addition, a number of openly available toolkits such as scikit-learn <ref type="bibr" target="#b31">[32]</ref> implement several dimension reduction approaches. Currently, the t-SNE embedding <ref type="bibr" target="#b41">[42]</ref> is the most commonly adopted approach for visualizing word embeddings. Compared to other common nonlinear dimension reduction techniques, t-SNE is optimized for 2D visualization and is more likely to reveal inherent clusters in the data. Consequently, it is often used to provide a quick overview of the overall structure and to highlight separation between word categories. However, since t-SNE creates nonlinear embeddings, the linear relationships most interesting to researchers are invariably lost. As a result, in practice, PCA of known subsets of words is used to visualize linear relationships. Since the vector values in the embedding space have no explicit meaning, many popular high-dimensional visualization techniques, such as parallel coordinates <ref type="bibr" target="#b13">[14]</ref> and scatter plots matrices <ref type="bibr" target="#b6">[7]</ref>, are less appropriate. A related challenge regarding any 2D projection of the word embedding space is the error (uncertainty) invariably introduced during the dimension reduction process. However, these challenges are rarely considered or visualized explicitly in the NLP community, which presents a risk for gross misinterpretation of the data. Here, we adopt the embedding quality measures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref> previously explored in the visualization community <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref> to aid in the interpretation of uncertainty in the t-SNE embeddings. For more information on uncertainty visualization techniques, we refer the reader to the surveys in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Nevertheless, none of the techniques discussed above address the specific needs of NLP researchers nor do they provide capabilities beyond what is currently state of the art in the NLP community. After an extensive literature search, to the best of our knowledge, a dedicated tool that helps NLP researchers understand and explore high-dimensional word embeddings does not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND: NEURAL WORD EMBEDDINGS</head><p>From web search to voice recognition, the advances in NLP have shaped how we interact with the digital world. At the core of several modern NLP systems is the concept of neural word embeddings, such as word2vec <ref type="bibr" target="#b26">[27]</ref> and Glove <ref type="bibr" target="#b32">[33]</ref>, which provide a powerful, distributed representation for words <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>. In particular, the embeddings support any number of higher level analysis tasks, such as sentiment analysis <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>, machine translation <ref type="bibr" target="#b45">[46]</ref>, and document modeling <ref type="bibr" target="#b15">[16]</ref>.</p><p>The general idea behind word embeddings can be described as follows (see <ref type="figure">Fig. 1</ref>): Let us assume we have a vocabulary of n words {w 1 ,...,w n } extracted from a large text corpus (e.g., Wikipedia). We first create the co-occurrence statistics matrix M (e.g., pairwise mutual information) in which each entry M(i, j) encodes how strongly w i and w j are related. Loosely speaking, one might interpret M(i, j) to encode the probability for words w i and w j to appear together within the same context. Subsequently, we can employ a variety of optimization strategies, such as Glove or skip-gram with negative sampling, to obtain a metric space for words that preserve the relationships encoded in M. Interestingly, it was showed in <ref type="bibr" target="#b17">[18]</ref> that the word embedding optimization is equivalent to performing matrix decomposition (symmetric SVD) on the matrix M directly.  <ref type="figure">Fig. 1</ref>. An illustration of the word embedding process: The input of the algorithm is a large corpus of text, which is summarized in a n × n matrix M that encodes the relationships between n unique words. Typically, M(i, j) records statistical relationships, such as the probability of joint occurrence between word i and word j . Subsequently, M is factorized and the coordinates in the d n most significant components define the vector representation of words.</p><p>In order to obtain a quantitative understanding of the word embeddings, it is common to analyze the vector difference between word vectors. More specifically, word positions and difference vectors encode crucial semantic and syntactic information. One of the surprising findings is that in the learned vector space, words support simple, algebraic manipulations. A prototypical example is the study of analogy pairs -king:queen, man:woman -where kingman + woman is approximately equal to queen (see <ref type="figure" target="#fig_0">Fig. 2</ref>). The analogy relationships have proven so useful that they are now routinely used to evaluate how well an embedding is capturing the semantic and syntactic characteristics <ref type="bibr" target="#b28">[29]</ref>. Nevertheless, due to the high-dimensional nature of the vector space, researchers still have only a limited understanding of the true relationships between words. <ref type="table">Table 1</ref>. A list of prominent NLP tasks pertinent to word embeddings, gathered from NLP experts. We identify which of these tasks can be performed using existing solutions (adopted in NLP community) and highlight the gaps bridged by the proposed approach .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks</head><p>Existing Proposed 1 What is the overall distribution of words or clusters?</p><p>[42] 2 Can we evaluate the quality of neighborhood preservation in a 2D embedding? 3 How can we view high-dimensional neighborhood information in a 2D embedding?</p><p>[36] 4 Can we find the most dominant linear structure for a given analogy relationship?</p><p>[15] 5 Can we find a linear projection that highlights analogy relationships? 6 Are certain analogy relationships observable only in subspace? 7 Can we identify the dimensions directly corresponding to semantic concepts (e.g., masculine → feminine)? 8 Are there subtrends within an analogy relationship? 9 How can we visually compare different word embeddings?</p><p>Finally, we define a few NLP-related terms used in the paper for clarity. An analogy pair is used to indicate a pair of words exhibiting a specific analogy relationship (e.g., man:woman). An analogy group is a set of analogy pairs sharing the same analogy concept and an analogy direction is used to denote the difference vector, manwoman. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN PROCESS</head><p>The project started from an incidental demonstration of a highdimensional visualization tool to an NLP researcher -one of the coauthors. The NLP expert remarked that such a system had the potential to be very helpful in his line of research and began introducing us to the challenges of word embeddings. We subsequently recruited additional NLP experts collaborating with us on other projects and jointly started to discuss the specific visualization needs of the NLP community. Through an iterative process of design, development, and evaluation, the team specialized the tool for NLP problems. Some of the important milestones in this process were the realization that most questions of interest involve only a small subset of words, i.e., a single analogy category, and that the subjective notion of which embedding is the most informative is not necessarily connected to any of the commonly used metrics of distortion. Furthermore, visualization practices in some of the well-known NLP publications seemed problematic from our perspective as visualization researchers. In particular, projection results are typically presented as is without assessing the impact of potential errors/uncertainty in the visual representation. In general, there is a lack of dedicated tools for answering the specific questions of greatest interest to the domain expert. We believe the NLP community could benefit from the introduction of better visualization tools tailored specifically for this domain. Throughout the design process, we have assembled a list of specific visualization tasks (provided in <ref type="table">Table 1</ref>), along with an initial assessment of whether these tasks can be addressed using tools that have already been adopted in the NLP community. As mentioned above, creating an embedding for all words (T1) addresses only a small aspect of the problem as it is clear that such a projection will inherently create severe clutter and large distortions. However, these embeddings are used to provide an overall context and our collaborators suggested that augmenting such embeddings with distortion measurements (T2) and the ability to explore neighborhoods of words (T3) would be useful. Most of the remaining tasks focus on using subsets of words. In partic-ular, T4-T8 are all designed to explore analogy relationships as they are critical to understanding the semantics in word embeddings. The final task (T9) constitutes a generic goal to understand differences between the word embedding spaces produced by various methods, such as word2vec <ref type="bibr" target="#b27">[28]</ref> and Glove <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONSTRUCTING ANALOGY PROJECTIONS</head><p>The study of analogy pairs forms one of the basic building blocks in understanding word embeddings. In particular, researchers are interested in the separation between the concepts, i.e., splitting male vs. female terms, as well as in the analogy directions, i.e., the vector kingqueen.</p><p>Since the latter implies a linear relationship, nonlinear projections are not appropriate in this context. Instead, the NLP community is utilizing PCA in an attempt to highlight the prominent linear structure. However, PCA treats all words as individual points -not as analogy pairs -and simply captures the direction of the largest variation among all words. In many cases, the intergroup variance can be larger than the variance between the two concepts, resulting in an embedding that does not preserve the analogy relationship. For example, <ref type="figure" target="#fig_2">Fig. 4(a)</ref> shows the PCA for words with singular vs. plural analogy, which highlights word categories rather than the desired analogy direction. In general, a popular hypothesis among NLP researchers is that analogy pairs are linearly related only within certain (linear) subspaces. If this hypothesis holds, then unless the corresponding subspace represents the directions of dominant variation, PCA cannot provide a useful visualization. Furthermore, PCA, like most other common embedding techniques, ignores the separation between the two labeled concepts, which can lead to a poor analogy separation and unintuitive projections. From discussion with our collaborators, two key objectives for an informative embedding emerged (illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>): First, the two concepts (male vs. female) should be well separated along one axis of the projection (here we choose the x-axis); and second, the different pairs, i.e., king:queen, man:woman, should preserve their relative distances in the orthogonal direction (in the y-axis of the projection). The first objective directly corresponds to a typical loss function in supervised classification, e.g., linear support vector machines <ref type="bibr" target="#b38">[39]</ref> (SVMs), which attempts to find the hyperplane that best separates two classes of data. Consequently, we use the normal to the estimated hyperplane as the x-axis of our 2D embedding (see <ref type="figure" target="#fig_1">Fig. 3</ref>). We discuss how to generate the y-axis, which addresses the second objective, in the next paragraph. Since an analogy relationship has only a clearly defined binary label, linear discriminant analysis (LDA), despite being the default option for supervised 2D projection, does not apply directly. (For k-class dataset, LDA can produce only a k-1 dimensional embedding.) In addition, LDA assumes each class follows a normal distribution, whereas linear SVM does not make any assumption about the class distribution.</p><p>Here, we introduce two methods to optimize the y-coordinate of words (see <ref type="figure" target="#fig_1">Fig. 3</ref>). The first approach is designed to best align the analogy pairs, whereas the second focuses on preserving interclass distances. The former optimizes the y-axis direction to minimize the pairwise angles (in the 2D plane) between pairs. Given a set of analogy pairs {(x i , y i )} T i=1 (x i is a word vector), the x-axis of the projection already captures the variation between the two concepts in the analogy relationship. Let w be the y-axis basis vector. To decrease the pairwise angles, we optimize w to make each pair as horizontal as possible, i.e., (|w T x i − w T y i | = 0). However, without additional regularization, the resulting embedding often collapses all words in each concept to a small area, which results in parallel lines indicating the analogy relationship, but does not produce an informative visualization. We resolved this issue by adopting an additional term in the objective function to require the projection to preserve the distances between words in the same concept, x i − x j . We pose this as a regression problem to predict the distances using the difference vectors. More specifically, we adopt the 2 regularized, ridge regression formulation:</p><formula xml:id="formula_0">min w ∑ P p=1 d p − w T v p 2 2 + λ w 2 ,</formula><p>where P denotes the total of pairs used for fitting the regressor, d p = 0 if the difference vector v p is constructed using words from an analogy pair and d p is the actual Euclidean distance if v p is constructed using two words from the same concept. Finally, the y-axis w is orthogonalized to the x-axis computed from linear SVM. In the rest of this paper, we will refer to this technique as SVM+REG. The results obtained using this approach for the singular vs. plural example are shown in <ref type="figure" target="#fig_2">Fig. 4(b)</ref>. Compared to the PCA, the concepts are well separated (the result of the SVM) and the lines are roughly parallel.</p><p>The second approach, which is designed to better preserve intra-class distances, replaces the optimization of the first approach with a 1D PCA of word vectors from one concept. Since it better preserves distances within the same concept, this approach (referred to as SVM+PCA) often produces more intuitive arrangements and subjectively appears to better preserve interesting relationships. For example, <ref type="figure" target="#fig_2">Fig. 4(c)</ref> shows the results of the singular vs. plural embedding. Analogies are well separated and roughly parallel but again form multiple subgroups, which is meaningful with the left cluster representing fruits, the middle animals and the right cluster other words. The SVM+PCA approach, unlike the SVM+REG approach, does not explicitly optimize for parallel patterns among analogy pairs, which, in some cases, results in less consistency in creating a 2D visualization that maximally reveals the analogy relationships.</p><p>The interpretation of the projection is twofold. First, the proposed projection approaches made the assumption that the analogy relationship exists; therefore, it is crucial to verify whether the projection indeed captures the salient structure of the data instead of noise. In order to address such a challenge, we extend the concept of hypothesis testing to linear projection, where we test against the likelihood of finding a similar pattern in randomized data (where we are sure the analogy relationship does not exist). This test and the corresponding visual representation are an integral part of using the proposed projection method, as discussed in detail in Section 6. The second aspect of interpreting the plots is the kind of patterns we should look for in these projections. Since the goal of these projections is to highlight analogy relationships, the parallelism among lines connecting two words in an analogy is a good indicator of how strong the analogy relationship is, i.e., see <ref type="figure" target="#fig_3">Fig. 6(c)(d)</ref>, where the same projection method is applied to two types of analogies. From the projections, we can see that (d) has a stronger analogy relationship than (c).</p><p>Finally, through extensive experiments and discussions with domain experts, it has become clear that a single projection cannot provide the user with all the important information to address all the tasks listed in <ref type="table">Table 1</ref>. Therefore, instead of relying only on projections, we introduce two additional views (see Section 7.2) to provide a more comprehensive picture of the analogy relationships and to help address all tasks discussed in Section 8. In addition, the proposed system allows animation transitions (each frame is an in-between linear projection) among these different projections (PCA, SVM+PCA, SVM+REG), which provide additional structural insight into the word embedding space via exploratory analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">A HYPOTHESIS TEST FOR PROJECTION SALIENCE</head><p>The analogy projection of the previous section aims to find the linear subspace that optimally aligns the analogy relationship. However, given that we typically project only around 30 pairs (60 words) from 300D to 2D, it is possible that the large number of degrees of freedom produces artificially well-aligned vectors even for unrelated word pairs. Therefore, it is crucial to evaluate whether a well-aligned pair projection captures a truthful/salience analogy relationship or should be considered a false positive. In other words, we need to understand how reliable analogy projections represent the high-dimensional structure.</p><p>To guard against false positives, we adapt a standard hypothesis testing <ref type="bibr" target="#b2">[3]</ref>. In particular, we test how likely it is that a certain projection result comes from random data. More specifically, our null hypothesis is that the current set of analogies has no correlation, i.e., the words are  selected randomly from the word embedding. As a preprocessing step, we assemble a large number of random pairs of words, treat each of them as an analogy pair and embed them in 2D using analogy-projection techniques. For each 2D projection, we compute its analogy-direction error, defined as the sum of pairwise angles, an indicator of how well aligned all directions are in 2D. We then use these errors to estimate the distribution (the blue distribution in <ref type="figure">Fig. 5(a)</ref>) of the null hypothesis, which encodes how likely it is to see a given analogy-direction error from random data. Note that this distribution depends on the projection technique used, the dimension of the vector space and the number or pairs that are projected. In practice, we pre-compute distributions for a range of sizes.</p><p>Given a projection for word pairs, we compare the resulting analogydirection error to the distribution of the error under the null hypothesis obtained with the same projection method, see <ref type="figure" target="#fig_3">Fig. 6</ref>. Such a plot provides a visual indication of the statistical significance (p-value <ref type="bibr" target="#b2">[3]</ref>) for a given projection result, i.e., the likelihood for the strong correlations to be artifacts. As shown in <ref type="figure" target="#fig_3">Fig. 6(a)</ref>, when the SVM+PCA projection optimized for the singular-plural analogy pairs is used with random words, the resulting analogy-direction error (red dotted line) overlaps with the null hypothesis distribution, thereby indicating the correlation pattern observed in the projection is artificial. On the other hand, for the actual analogy words <ref type="figure" target="#fig_3">(Fig. 6(b)</ref>), there is no overlap between the distribution and the analogy-direction error, which confirms the presence of salient structure. The test can also distinguish differences in confidence regarding the analogy relationship captured in the projection. As shown in <ref type="figure" target="#fig_3">Fig. 6(c)(d)</ref>, based on the hypothesis-testing plots, we can see countries:capitals has a stronger analogy relationship compared to adjs:antonyms-adjs, which can also be observed from the projection.</p><p>Note that our null hypothesis is somewhat optimistic since random word pairs represent the worst-case behavior. As such, many of the projections of reasonably aligned pairs are significantly below the lower end of the null hypothesis distribution. Nevertheless, the relative distance still conveys the saliency of a given result in an intuitive manner, and we have encountered several cases in which optimized projections are well within the range of the null hypothesis. If needed, one could easily create additional hypothesis tests by, for example, selecting pairs between two randomly selected word categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">WORD EMBEDDING VISUAL EXPLORER</head><p>Word Embedding Visual Explorer (see <ref type="figure" target="#fig_4">Fig. 7</ref>) is a web-based visualization tool for exploring the word embedding space. In this section, we describe the design choices, functionality, and the implementation of the system. To start the exploration, users can either select from a number of widely used pretrained word embeddings (e.g., Glove <ref type="bibr" target="#b32">[33]</ref>, Word2Vec <ref type="bibr" target="#b26">[27]</ref>) or upload their own. Typically, users then load word groups of interest, for example, different analogy pairs or individual word categories. However, the exploration is not limited to the local scope among the words of interest. For certain tasks, the region of interest includes the entire word embedding space, in which case large numbers of word vectors will be fetched in the background and seamlessly passed to the analysis pipeline. The system consists of two major visualization components, the Global t-SNE view and the Analogy Relationships view. Both views are meant to replace the standard t-SNE and PCA with their enhanced counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Global t-SNE View</head><p>The global t-SNE view is designed to provide an overview of the arrangement of a large number of words. With respect to <ref type="table">Table 1</ref>, the view aims to address tasks T1-T3 in the list. What differentiates this visualization from the standard t-SNE embedding is the ability to use visual encodings to aid the understanding of uncertainties and distortions in the 2D embeddings. According to a domain expert, users of t-SNE in NLP will often interpret inconsistency in the embedding, for example, a word far away from its expected neighborhood, as potential noise in the embedding, before considering the possibility of inaccuracies in the visualization. However, in our experience misplaced words are more likely the result of distortion in the dimension reduction. Since the intrinsic dimension of the words is expected to be 2, such artifacts are unavoidable and should be taken into account before considering more complex explanations.</p><p>We adopt the concept of distortion error, in particular, per-point distortions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>, to help illustrate the various levels of accuracy and reliability within the same embedding. To provide the per-point estimation, we compare how much a given point's neighborhood relationship changes in the 2D projection with respect to the original highdimensional space. In this work, a ranking-based approach is adopted; the implementation details can be found in <ref type="bibr" target="#b16">[17]</ref>. As illustrated in <ref type="figure" target="#fig_4">Fig. 7(a)</ref>, points are colored based on how well their high-dimensional neighbors are preserved in the 2D space, allowing users to determine how much one should trust different regions of the same embedding. However, the distortion values provide the information only about the magnitude of the error. We still do not know exactly which neighbor's information is lost. To provide this information, we allow users to select a word and show its closest neighbors in the high-dimensional space as a link between the corresponding points. As an additional visual cue, we encode the distance in high-dimensional space as the thickness (thick-close, thin-far) of the line. Finally, we support multiple types of clustering algorithms to allow an interactive exploration of both the high-dimensional neighbors and clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Analogy Relationships View</head><p>In the word embedding space, analogy pairs exhibit interesting algebraic relationships (see <ref type="figure" target="#fig_0">Fig. 2</ref>) that are often used to evaluate the quality of word embeddings <ref type="bibr" target="#b28">[29]</ref>. Consequently, obtaining an in-depth understanding of analogy pairs' behaviors is essential for exploring the word embeddings. The analogy relationships view is designed to address tasks T4-T8, and to some extent T9 in the task list. As illustrated in <ref type="figure" target="#fig_4">Fig. 7</ref>, there are three panels in the analogy relationship view. Analogy Pairs Projection. The analogy pair projection panel <ref type="figure" target="#fig_4">(Fig.7(b)</ref>) supports different linear projections: PCA, SVM+REG and SVM+PCA. The orange and blue colors correspond to the two concepts of a given analogy group. The link connects the words belonging to the same analogy. Furthermore, each projection displays its error relative to the corresponding null hypothesis (top left corner) to indicate the salience of the observed structure. Both SVM-based approaches capture hidden subspaces (T5, T6), which are typically lost in a PCA embedding. Each of the methods emphasizes a different aspect of the data. As illustrated in our case study (Section 8), by examining their relationships, the user can obtain a multifaceted understanding of analogy pairs' structure.</p><p>The system also provides an animated transition between any pair of linear projections (e.g., from PCA to SVM+PCA). Compared to showing different projections sequentially, the dynamic transitions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> allow users to maintain the visual context and track the correspondence between individual words. In addition, each frame in the animation is always a linear projection (i.e., a generalization of 3D rotation in high-dimensional space). As illustrated in the case study, the transition provides the user with an intuitive understanding of how one subspace is related to another in a geometric sense, akin to how a 3D rotation helps convey geometric relationships.</p><p>In addition, this panel provides the option of applying a variety of clustering algorithms. It includes not only widely used hierarchical, k-means++ and spectral clustering, but also advanced methods such as subspace clustering <ref type="bibr" target="#b10">[11]</ref> that are designed to reveal the low-dimensional subspaces shared by subsets of data. As demonstrated in Section 8.1, subspace clustering is ideal for identifying subtrends and other intricate linear relationships within an analogy group. Pairwise Cosine Distance Histogram. Linear projections of an analogy relationship inform the user whether the pair directions are coherent in a given (2D) subspace. However, knowing the actual angles between the pair directions in the high-dimensional vector space is also important. As illustrated in <ref type="figure" target="#fig_4">Fig. 7(c)</ref>, a histogram of all pairwise cosine distances in an analogy group conveys quantitatively how coherent the analogy relationship is. Combined with the test statistic of the projections, the distribution of distances represents an intuitive way to judge how confident one should be in interpreting the projections. In the histogram, the horizontal axis corresponds to the cosine distance (0.0-1.0). Semantic Axis. According to one of the domain experts, researchers in NLP suspect that there exist correlations between certain dimensions (factors) and a specific concept or meaning. As demonstrated in Section 5, we can find the general direction of the analogy from the SVM direction. However, the projection and the histogram panel alone do not readily address global inquiries such as the task T7, where the hypothesis needs to be evaluated in the global word embedding space.</p><p>The semantic axis panel is designed to identify which words in the global embedding space have the largest or smallest values along the given axis defined by a vector direction (a factor of the word embedding dimensions), for example, an analogy pair, an analogy group, or a concept simply defined by two words (e.g., the "royalty":man-king). If the given axis corresponds to a distinct concept, such as masculine and feminine, one expects to find masculine and feminine words at the opposite ends of the axis, which we refer to as the semantic axis.</p><p>As shown in <ref type="figure" target="#fig_4">Fig. 7(d)</ref>, the horizontal axis corresponds to values of the words on the semantic axis, and the vertical axis corresponding to the ranking order based on the same value. Note that we show only the k top-and bottom-most words along an axis to keep the list size manageable and focus on the most important words. <ref type="figure" target="#fig_4">Fig. 7(d)</ref> shows are identified in each of the analogy groups. In (a), closely related analogy pairs are grouped into the same cluster. In (b), currency:country, the two concepts in the analogy are distinct, so the subspace clustering focused on the linear trends within the two groups, instead of among the analogy pairs. In (c), the singular :plural analogy group contains words that have very different concepts (animal, fruit, etc.); therefore, analogy pairs are grouped into clusters not necessarily because they have a similar orientation, but because the larger distances between these different concepts have a stronger influence. The histograms in (d), (e), (f) confirm the observation.</p><p>the 20 top-and bottom-most words along the male-female direction among a list of 10k most frequent words. Conceptually, these words represent what the embedding defines as most masculine vs. feminine. Unsurprisingly, the results show male/female names and gender stereotypical jobs. By examining these words, we can verify the authenticity of the concept obtained using a limited number of words in the global embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Implementation</head><p>As illustrated in <ref type="figure">Fig. 9</ref>, The system is split into server and client modules. The server handles complex computation tasks and the client manages the user interface. The client is web-based, allowing us to continuously share the latest improvements with our collaborators, which has been crucial for a tight design, implementation, and feedback circle. The communication between the web client and the server is accomplished through a set of RESTful APIs.  <ref type="figure">Fig. 9</ref>. An overview of the system architecture. The entire system is split into server and client modules, where the server handles complex computation tasks and the client handles user interaction and display.</p><p>To achieve a good trade-off between implementation complexity and performance, the server is implemented in Python, and the computation methods are handled by efficient libraries (e.g., scikit-learn <ref type="bibr" target="#b31">[32]</ref>) or python bindings of native C++ code. The client graphical interface is implemented in JavaScript and HTML with d3.js <ref type="bibr" target="#b3">[4]</ref> to handle the graphical components. The widely used pretrained datasets usually contain large numbers of words and phrases (i.e., word2vec's google-News dataset contains 3 million words and phrases). To manage the pretrained and user-provided data and achieve efficient storage and query operations, we use the MongoDB <ref type="bibr" target="#b0">[1]</ref> database on the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CASE STUDY: ANALOGY TASKS</head><p>In this section, we showcase how the domain scientists have used our tool to study semantic and syntactic relationships in word embeddings and gained new insights. The analogy task dataset (originally used in <ref type="bibr" target="#b26">[27]</ref>) examined in this study is one of the most widely used analogy datasets for evaluating the quality of word embeddings. It contains 14 analogy groups, some of which are semantic (male:female) in nature, whereas the others are syntactic (singular:plural). For each analogy group, a set of analogy pairs (e.g., man:woman) is provided. This dataset is often used to compute the error in analogy prediction (i.e., is manwoman + queen close to king?), which is considered as a key indicator of how well the word embedding captures the intricate relationships among words. Despite being a common practice, little is known regarding the characteristics of these analogy relationships, and how they compare to each other within the same or different embeddings (e.g., Glove vs. word2vec). The goal of this study is to address the often-neglected and challenging questions (T5-T9) about analogy relationships. In the following visualizations, the 300D version of pretrained word embeddings (Glove or word2vec) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Are They Really Parallel?</head><p>The assumption of analogy relationship (as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>) suggests that the pair orientations (e.g., manwomen) within each analogy group should be similar to each other. Particularly, 2D PCA projections, in which the pair directions are parallel to each other, are typically used to illustrate the coherency of the analogy relationship in the NLP community. However, projections can easily destroy existing correlations between directions (i.e., by projecting along the analogy direction) or create a false alignment. As a result, without additional information, PCA projections may be misleading.</p><p>In our tool, we use the histogram of the pairwise analogy directions to provide a direct estimate of the parallelism among the orientations in high-dimensional space. As illustrated in <ref type="figure" target="#fig_5">Fig. 8(a),(d)</ref> for the country:nationality analogy, even though in the PCA projection the direc- tions appear similar, the histogram reveals that most pairs are not as well aligned (with cosine distance value 0.3-0.5) as they appear. One of the domain experts was surprised by numerous such examples when exploring the different analogy groups using the tool. It is unexpected because country:nationality is one of the analogy groups that has the most coherent-looking (parallelism among the lines) PCA embedding. Furthermore, when examining all the 14 analogy pairs, the scientist noticed that different types of analogy groups can have extremely different behaviors (see <ref type="figure" target="#fig_5">Fig 8)</ref>.</p><p>To help make sense of these huge variations among the analogy relationships, the scientist could utilize the different projection techniques and subspace clustering <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44]</ref> (group the data as belonging to different low-dimensional subspaces, discussed in Section 7.2) to shed light on the structure of words in each analogy group. In <ref type="figure" target="#fig_5">Fig. 8(a)</ref>, for the case of country:nationality, closely related analogy pairs are grouped into the same cluster (e.g., Scandinavian countries and capitals, highlighted by the dotted circle at the top of the embedding). Whereas, as seen in <ref type="figure" target="#fig_5">Fig. 8(b)</ref>, the two concepts in the currency:country analogy are distinct, so when applying subspace clustering, we see currencies and countries forming their own subspaces. In other words, subspace clustering identifies the stronger linear trends within currencies and countries, instead of analogy pairs. Finally, in <ref type="figure" target="#fig_5">Fig. 8(c)</ref>, the singular:plural analogy group contains words that have very different concepts (animal:animals vs. fruit:fruits), and therefore, when applying subspace clustering, animal and fruit words are grouped into different subspaces, not necessarily because the pairs have very similar directions, but because the differences between concepts have a stronger influence on the subspace distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Do Analogy Directions Really Capture Semantics?</head><p>Based on the variations observed among pair orientations in each analogy group, the domain scientist naturally started questioning the widely accepted assumption that an analogy direction corresponds to a particular semantic idea. If the variation is so large within an analogy such as the singular:plural case, does the analogy direction still encode the actual syntactic relationship?</p><p>To further investigate this conundrum, the domain scientist could use the proposed analogy projection schemes to emphasize the apparent word relationships. As described earlier, the projection techniques attempt to find a subspace that maximally reveals the semantic and syntactic relationships. As illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>, even for the least coherent analogy group (singular:plural), as determined by PCA, such a subspace exists <ref type="figure" target="#fig_2">(Fig. 4(c)</ref>). Furthermore, using the new hypothesis-testing procedure, one can be fairly confident in attributing this result to the inherent structure in the high-dimensional space. The interesting results of our analogy projection enabled the domain expert to hypothesize that the word embedding attempts to preserve the analogy relationship in the high-dimensional space, while simultaneously trying to capture other conflicting relationships. By viewing the dynamic projection transition between PCA and the SVM+PCA projection, the domain scientist could easily track how each analogy pair changes. The animation also revealed an interesting rotational movement, which enabled an intuitive understanding of the geometric relationship between the two projections (see the supplemental video for the animation).</p><p>Despite its usefulness for exploratory analysis, the analogy projection focuses only on local relationships. The next natural question of the expert was if the semantic/syntactic relationship observed locally will hold in a global context. And more importantly, is there an apparent relationship between a latent factor (a linear combination of original dimensions) and the corresponding semantic or syntactic concepts? The semantic axis panel provides the domain scientist with the capability to answer these questions. As described in the previous sections, the overall analogy direction of an analogy group can be obtained via the normal of the linear SVM. <ref type="figure" target="#fig_6">Fig. 10(b)</ref> shows the most extreme words among the 10k most frequent ones along the analogy direction (the semantic axis). Interestingly, one finds words that strongly represent the concept defined by the analogy although they do not actually exist in the original analogy group.</p><p>The optimized analogy projections and the semantic axis plots can also aid in the comparison of the different word embedding approaches (T9). As illustrated in <ref type="figure" target="#fig_0">Fig. 10, word2Vec</ref> is doing better at capturing syntactic relationships than Glove, which explains the superiority of word2Vec in syntactic analogy prediction tasks commonly observed by NLP researchers <ref type="bibr" target="#b32">[33]</ref>.</p><p>For the domain expert, the new insights from the proposed analysis raised additional questions on the validity and possible limitations of how word embedding quality is evaluated. Using just the algebraic vector relationships in high-dimensional space as the quality measure (as suggested in <ref type="bibr" target="#b28">[29]</ref>) might result in the large variance being ignored among different kinds of analogies. Furthermore, as a global error measure, the algebraic relationships do not account for the existence of subspaces. In particular, there can be analogies with rather poor global alignment that nevertheless are highly correlated in some linear subspace. Potentially, this should be taken into account when creating and using word embeddings, which might lead to better results overall. Even though critics (e.g., <ref type="bibr" target="#b19">[20]</ref>) from the NLP community have recently drawn attention to many potential problems with the analogy-based evaluation approaches for word embeddings, our visualization still provides a unique perspective for the domain experts on this pressing topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">What Does the Global Relationship Tell Us?</head><p>The analogy data include more than 900 unique words. Studying how they are distributed in the word embedding space may shed light on the differences among analogy groups. The per-point distortion error <ref type="bibr" target="#b16">[17]</ref> (discussed in Section 7.1) estimates how well high-dimensional local structure is preserved in the 2D embedding. As illustrated in <ref type="figure" target="#fig_7">Fig. 11(a)</ref>, we can see there is a region in the t-SNE embedding, indicated by the dotted circle, with high distortion errors. By checking the corresponding words, this region corresponds to a broad range of verbs, adverbs, and adjectives. We also notice the different types of nouns correspond to more compact groups. To highlight these groups, we show the spectral clustering label computed from high-dimensional word vectors, which separate the semantically distinct words well. However, other techniques (or word labels) that capture semantic separation can be applied here as well. As showed in <ref type="figure" target="#fig_7">Fig. 11(b)</ref>, various type of nouns (clusters distributed around the perimeter of the embedding) seem to form easily separable regions, but it is much harder to distinguish the differences among verbs, adverbs, and adjectives (indicated by the dotted circle). The domain scientists found these observations very interesting and postulated that this behavior likely indicates how the vectors are trained. Word2vec's vectors, for example, are created by factorizing the cooccurrence statistics matrix between words (discussed in Section 3). Since nouns (animals, cities, etc) co-occur within fewer contexts than verbs and adjectives, their vector representations will be more sparse, which might lead to tighter clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">FEEDBACK AND EVALUATION</head><p>We redesigned and improved the system over multiple iterations based on constant feedback from domain scientists. In the beginning of the study, we focused on the effective communication of uncertainty in the t-SNE embedding. Through our subsequent discussions, the importance of analogy relationships became more apparent. Feedback and discussions such as these shaped the goal and features of the tool.</p><p>For the evaluation of the final version of the tool, we solicited feedback from three domain experts, who are familiar with the capability of the tool via the usage example and their own experiences from the web-based system, on the following inquiries: 1) Does the tool address the questions summarized in the task list? 2) Does the tool make the user more aware of the uncertainty in the visualization? 3) What is the most useful feature of the tool? 4) Does the new projection better capture analogy relationships? 5) Does the hypothesis testing for salient structure aid in the interpretation of the projection? 6) Does the dynamic transition help explain the relationship between projections? 7) What can be improved? 8) What are the related topics you would like to explore?</p><p>A summary of the anecdotal evaluation is as follows: The domain experts agreed that the tool provides a number of unique approaches to address domain-specific inquiries that were not possible before. In particular, one of the experts argued that the most interesting part of the tool is that it helps raise questions he never considered previously (as discussed in the case study). Due to familiarity with t-SNE, all three domain experts were easily drawn to the high-dimensional neighborhood lookup and distortion visualization and found them to be very useful for their everyday workflow. One domain expert pointed out that the concept of hypothesis testing and salient structure estimation can be hard to grasp at first. However, with a little experimentation, he found the information it provides to be invaluable for interpreting the embedding result. Further, he believes that the general idea of hypothesis testing can be valuable for other types of visualization. The domain experts concurred that the addition of the error/uncertainty estimation features made them more aware of the potential pitfall of visualization, which is ignored in a numnber of well known NLP publications. One expert even suggested that he will start promoting the concept of "error in visualization" to the NLP community. During our discussions, we realized that the notion of error in the t-SNE projection is often wrongly interpreted by NLP researchers as an error in the word embedding space and not in the visualization, and our tool helped to clarify this difference to all three domain experts. Finally, in regard to the dynamic transition between linear projections, one domain expert commented that the effect is helpful in tracking the changes between projections. However, he found it to be very challenging to interpret the geometry in high-dimensional spaces since our intuition is built around rotations in 3D. On potential improvements and extensions, one domain expert wonders how could the tool be extended to handle other types of embeddings, such as sentence embeddings <ref type="bibr" target="#b30">[31]</ref> instead of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION AND FUTURE WORK</head><p>Through a long-term collaboration with NLP domain experts, we introduce the first dedicated visualization tool for exploring word embedding spaces. We have developed a myriad of specialized techniques and visualizations, including linear projection techniques for highlighting analogy relationships, a novel concept of hypothesis testing for salient structure, and various visual encodings for domain-specific tasks. By utilizing the new tool, domain experts are able to gain insights and even form new hypotheses pertinent to language modeling. The concept of word embeddings has recently been extended to phrases or even short sentences <ref type="bibr" target="#b30">[31]</ref>. For future work, we plan to extend the scope of the tool to handle these new types of embeddings. On a more general note, we plan to further develop this work into an open-source package in the near future to attract more users and enhance the exposure of our tool in the NLP community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>In the word embedding space, the analogy pairs exhibit interesting algebraic relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>An illustration of the intuition behind the projection-finding schemes for an analogy relationship. In a 2D projection, the x-axis captures variation between the two concepts in an analogy relationship. The y-axis captures the variation among pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The domain-specific projection-finding scheme. PCA (a) captures the largest variance in the data, which corresponds to the difference in the meaning of the nouns, whereas the proposed projection schemes (b) (c) capture the singular:plural analogy relationship. In this experiment, the official pretrained Glove vectors are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of the hypothesis testing results between random data (a) and regular input (b), as well as between a strong analogy relationship (d) and a weak analogy relationship (c), using the same projection technique (SVM+PCA). The analogy-direction error plots tell us how likely it is the analogy relationship exists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Word Embedding Visual Explorer. (a) t-SNE embedding panel shows the overall structure of the words of interest. (b) Analogy projection panel enables the exploration among linear projections that highlight an analogy relationship. (c) Pairwise cosine distance histogram panel captures the overall parallelism of the analogy vector orientations. (d) Semantic axis panel shows the words at the extreme ends of an axis that capture a distinct concept.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Different types of analogies can correspond to very different structures. In (a), (b), (c), the subspace clusters (highlighted by dotted circles)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Comparison of Glove and word2Vec word embeddings: (a) Analogy projection (SVM+PCA)-based comparison shows that word2Vec produces a more apparently aligned directions for both syntactic relationships. Here we utilize the hypothesis-testing plot to verify whether the projection captures the truthful structure of the data. As we can see in all four plots, the current error (red triangle) falls outside the possible range of error from random data, which indicates high confidence in the presence of salience structures. (b) The semantic axis plots reveal that the average analogy direction from word2Vec embedding better identifies the true syntactic concept (verb-ing:verb-ed) word2Vec embedding. The mistakes in the case of Glove are marked with red arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Examining the high distortion regions in the t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Shusen Liu, Peer-Timo Bremer, Jayaraman J. Thiagarajan is with LawrenceLivermore National Laboratory. E-mail:{liu42,bremer5,jayaramanthi1}@llnl.gov.</figDesc><table /><note>• Bei Wang, Yarden Livnat and Valerio Pascucci is with SCI Institute, University of Utah. E-mail:{beiwang,yarden,pascucci}@sci.utah.edu.• Vivek Srikumar is with School of Computing, University of Utah. E-mail: svivek@cs.utah.edu</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mongodb: cross-platform document-oriented database</title>
		<ptr target="https://www.mongodb.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">P value and the theory of hypothesis testing: an explanation for new researchers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Biau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Jolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Porcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Orthopaedics and Related Research</title>
		<imprint>
			<biblScope unit="volume">468</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">D 3 data-driven documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2301" to="2309" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A review of uncertainty in data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brodlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Osorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expanding the frontiers of visual analytics and visualization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="81" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data visualization with multidimensional scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Swayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="444" to="472" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scatterplot matrix techniques for large n</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Littlefield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Littlefield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">398</biblScope>
			<biblScope unit="page" from="424" to="436" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Termite: Visualization techniques for assessing textual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Working Conference on Advanced Visual Interfaces</title>
		<meeting>the International Working Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="74" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grand tour and projection pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="155" to="172" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic-and time-oriented visual text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="13" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2790" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intrinsic evaluations of word embeddings: What can we do better?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gladkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drozd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>1st Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Principal manifolds and graphs in practice: from molecular biology to dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zinovyev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="232" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parallel coordinates: a tool for visualizing multi-dimensional geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inselberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dimsdale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st conference on Visualization&apos;90</title>
		<meeting>the 1st conference on Visualization&apos;90</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="361" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quality assessment of dimensionality reduction: Rank-based criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1431" to="1443" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Issues in evaluating semantic spaces using word analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>1st Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distortion-guided structure-driven interactive exploration of high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual exploration of high-dimensional data through subspace analysis and dynamic projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="280" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Storyflow: Tracking the evolution of stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2436" to="2445" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online visual analytics of text streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2451" to="2466" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual analysis of dimensionality reduction quality for parameterized projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Coimbra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="26" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing the quality of dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mokbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lueks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gisbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="109" to="123" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="694" to="707" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From quantification to visualization: A taxonomy of uncertainty visualization approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty Quantification in Scientific Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="226" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual tools for debugging neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Visualization for Deep Learning</title>
		<meeting>ICML Workshop on Visualization for Deep Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05469</idno>
		<title level="m">Embedding projector: Interactive visualization and interpretation of embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Concurrent visualization of relationships between words and topics in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Findlater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces</title>
		<meeting>the Workshop on Interactive Language Learning, Visualization, and Interfaces</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">7982</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hiérarchie: Interactive visualization for hierarchical topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hawes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces</title>
		<meeting>the Workshop on Interactive Language Learning, Visualization, and Interfaces</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Least squares support vector machine classifiers. Neural processing letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning sentimentspecific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multidimensional scaling: I. theory and method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Torgerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="401" to="419" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">85</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Information retrieval perspective to nonlinear dimensionality reduction for data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Venna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nybo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aidos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="451" to="490" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A tutorial on subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sentiview: Sentiment analysis and visualization for internet popular topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="620" to="630" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
