<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2017.2744158</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>allows the user to see patterns that lead to further refinement of the selection hypothesis. Navigation aids provide convenience (i1, i2).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>i2 t <ref type="figure">Fig. 1</ref>. The LSTMVIS user interface. The user interactively selects a range of text specifying a hypothesis about the model in the Select View (a). This range is then used to match similar hidden state patterns displayed in the Match View (b). The selection is made by specifying a start-stop range in the text (c) and an activation threshold (t) which leads to a selection of hidden states (blue lines). The start-stop range can be further constrained using the pattern plot (d). The meta-tracks below depict extra information per word position like POS (e1) or the top K predictions (e2). The tool can then match this selection with similar hidden state patterns in the data set of varying lengths (f), providing insight into the representations learned by the model. The match view additionally includes user-defined meta-data encoded as heatmaps (g1,g2). The color of one heatmap (g2) can be mapped (h) to the word matrix (f) which</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, deep neural networks have become a central modeling tool for many artificial cognition tasks, such as image recognition, speech recognition, and text classification. These models all share a common property in that they utilize a hidden feature representation of their input, not pre-specified by the user, which is learned for the task at hand. These hidden representations have proven to be very effective for classification. However, the black-box nature of these learned representations make the models themselves difficult to interpret. So while it is possible for users to produce high-performing systems, it is difficult for them to analyze what the system has learned. While all deep neural networks utilize hidden features, different model structures have shown to be effective for different tasks. Standard deep neural networks (DNNs) learn fixed-size features, whereas convolutional neural networks (CNNs), dominant in image recognition, will learn a task-specific filter-bank to produce spatial feature maps. In this work, we focus on deep neural network architectures known as x t-2</p><p>x t-1</p><p>x t h t-2 h t-1 h t p t w t-1 w t w t-2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.,w t ).</head><p>At each time step, a word w t is converted to a word vector x t , which is then used to update the hidden state h t ← RNN(x t , h t−1 ). This hidden state vector can be used for prediction. In language modeling (shown) it is used to define the probability of the next word, p(w t+1 |w 1 ,...,w t ) = softmax(Wh t + b).</p><p>recurrent neural networks (RNNs) that produce a time-series of hidden feature-state representations.</p><p>RNNs <ref type="bibr" target="#b7">[7]</ref> have proven to be an effective general-purpose approach for capturing representation in sequence-modeling applications, such as text processing. Recent strong empirical results indicate that internal representations learn to capture complex relationships between the words within a sentence or document. These improved representation have led directly to end applications in machine translation <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b29">28]</ref>, speech recognition <ref type="bibr" target="#b1">[2]</ref>, music generation <ref type="bibr" target="#b3">[4]</ref>, and text classification <ref type="bibr" target="#b6">[6]</ref>, among a variety of other applications.</p><p>While RNNs have shown clear improvements for sequence modeling, it has proven very difficult to interpret their feature representation. As such, it remains unclear exactly how a particular model is representing long-distance relationships within a sequence. Typically, RNNs contain millions of parameters and utilize repeated transformations of large hidden representations under time-varying conditions. These factors make the model inter-dependencies challenging to interpret without sophisticated mathematical tools. How do we enable users to explore complex network interactions in an RNN and directly connect these abstract representations to human understandable inputs?</p><p>In this work, we focus on the visual analysis of hidden features in RNNs. We have developed LSTMVIS, a tool to allow advanced user groups to explore and form hypotheses about RNN hidden state dynamics. We analyzed neural network users and identified three major user roles, each with a different set of requirements: architects who develop novel deep learning structures, trainers who develop new data sets to train existing models, and end users who apply deep models to new data. LSTMVis is focused on architects and trainers, for which we performed a goal and task analysis to develop effective visual encodings and interactions. LSTMVis combines a time-series based select interface with an interactive match tool to search for similar hidden state patterns in a large dataset. A live system can be accessed via lstm.seas.harvard.edu and the source code is provided. We present use cases applying our technique to identify and explore patterns in RNNs trained on large real-world datasets for text, speech recognition, biological sequence analysis and other domains. We discuss our release-before-publish strategy used to develop and improve the tool based on user feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND: RECURRENT NEURAL NETWORKS</head><p>RNNs are a type of deep neural network architecture that has proven effective for sequence modeling tasks such as text processing. A major challenge of working with variable-length text sequences is producing features that capture or summarize long-distance relations in the text. These relationships are particularly important for tasks that require processing and generating sequences such as machine translation. RNNbased models effectively learn hidden representations at each time-step which are then used for decision making. We refer to the change in these representations over time as the hidden state dynamics produced by the model. Throughout this work, we will assume that we are given a sequence of words w 1 ,...,w T for time 1 to T . These might consist of English words that we want to translate or a sentence whose sentiment we would like to detect, or even some other symbolic input such as musical notes or code. Additionally, we will assume that we have a mapping from each word into vector representation x 1 ,...,x T . This representation can either be a standard fixed mapping, such as word2vec <ref type="bibr" target="#b23">[23]</ref>, or can be learned with the rest of the model.</p><p>Formally, RNNs are a class of neural networks that sequentially map input word vectors x 1 ...x T to a sequence of hidden feature-state representations h 1 ,...,h T . This is achieved by learning the weights of a neural network RNN, which is applied recursively at each time-step t ∈ 1 ...T :</p><formula xml:id="formula_0">h t ← RNN(x t , h t−1 )</formula><p>This function takes input vector x t and a hidden state vector h t−1 and gives a new hidden state vector h t . Each hidden state vector h t is in R D . These vectors, and particularly how they change over time, will be the main focus of this work. We are interested in each c ∈ {1 ...D} and in particular the change of a single hidden state h t,c as t varies.</p><p>The model learns these hidden states to represent the features of the input words. As such, they can be learned for any modeling tasks utilizing discrete sequential input. One notable application is RNN language modeling <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b34">33]</ref>, a core task in natural language processing. In language modeling, at time t the prefix of words w 1 ,...,w t is taken as input and the goal is to model the distribution over the next word p(w t+1 |w 1 ,...,w t ). An RNN is used to produce this distribution by applying multi-class classification based on hidden feature-state vector h t . Formally we define this as p(w t+1 |w 1 ,...,w t ) = softmax(Wh t +b), where W, b are parameters. The full computation of an RNN language model is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>It has been widely observed that the hidden states are able to capture important information about the structure of the input sentence necessary to perform this prediction. However, it has been difficult to trace how this is captured and what exactly is learned. For instance, it has been shown that RNNs can count parentheses or match quotes, but is unclear whether RNNs naturally discover aspects of language such as phrases, grammar, or topics. In this work, we focus particularly on exploring this question by examining the dynamics of the hidden states through time.</p><p>Our use cases will mainly focus on long short-term memory networks (LSTM) <ref type="bibr" target="#b11">[11]</ref>. LSTMs define a variant of RNN that has a modified hidden state update which can more effectively learn long-term interactions. As such these models are widely used in practice. In addition, LSTMs and RNNs can be stacked in layers to produce multiple hidden state vectors at each time step, which further improves performance. While our results mainly use stacked LSTMs, our visualization only requires access to some time evolving abstract vector representation, and therefore can be used for any layer of a wide variety of models.</p><p>Note that LSTMs maintain both a cell state vector and a hidden state vector at each time step. Our system can be used to analyze either or both of these vectors (or even the LSTM gates), and in our experiments we found that the cell states are easier to work with. For simplicity, however, we refer to these vectors generically as hidden states throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Understanding RNNs through Visualization Our core contribution, visualizing the state dynamics of RNNs in a structured way, is inspired by previous work on convolutional neural networks in vision applications <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b35">34]</ref>. In linguistic tasks, visualizations have shown to be useful tool for understanding certain aspects of RNNs.  <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref> show that RNNs specifically learn lexical categories and grammatical functions that carry semantic information, partially by modifying the inputs fed to the model. While inspired by these techniques, our approach tries to extend beyond single examples and provide a general interactive visualization approach of the raw data for exploratory analysis.</p><p>Extending RNN Models for Interpretability Recent work has also developed methods for extending RNNs for certain problems to make them easier to interpret (along with improving the models). One popular technique has been to use a neural attention mechanism to allow the model to focus in on a particular aspect of the input. Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref> use attention for soft alignment in machine translation. Xu et al. <ref type="bibr" target="#b33">[32]</ref> use attention to identify important aspects of an image for captioning, whereas Hermann et al. <ref type="bibr" target="#b10">[10]</ref> use attention to find important aspects of a document for an extraction task. These approaches have the side benefit that they visualize the aspect of the model they are using. This approach differs from our work in that it requires changing the underlying model structure, whereas we attempt to interpret the hidden states of a fixed model directly.</p><p>Interactive Visualization of Neural Networks There has been some work on interactive visualization for interpreting machine learning models. Tzeng et al.</p><p>[30] present a visualization system for feedforward neural networks with the goal of interpretation, and Kapoor et al. <ref type="bibr" target="#b15">[15]</ref> give a user-interface for tuning the learning itself. The Prospector system <ref type="bibr" target="#b17">[17]</ref> provides a general-purpose tool for practitioners to better understand their machine learning model and its predictions.</p><p>Recent work also describes systems that focus on analysis of hidden states for convolutional neural networks. Liu et al. <ref type="bibr" target="#b20">[20]</ref> utilize a DAG metaphor to show neurons, their connections, and learned features. Rauber et al. <ref type="bibr" target="#b25">[25]</ref> use projections to explore relationships between neurons and learned observations. Other work has focused on user interfaces for constructing models, such as TensorBoard <ref type="bibr" target="#b0">[1]</ref> and the related playground for convolutional neural models at playground. tensorflow.org/. Our work is most similar in spirit to the work by Tzeng et al. <ref type="bibr" target="#b31">[30]</ref>, Liu et al. <ref type="bibr" target="#b20">[20]</ref>, and Rauber et al. <ref type="bibr" target="#b25">[25]</ref>, in that we are concerned with interpreting the hidden states of neural network models. However, our specific goals focus on RNNs and the needs of specific users, and the resulting visual design is significantly different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">USER ANALYSIS AND GOALS</head><p>Deep neural networks are now widely employed both in the research and industrial setting by a diverse set of users with different needs.</p><p>Before developing our visual analysis goals we first laid out a set of prototypical stakeholders who might benefit from improved analysis. In meetings with members of a natural language processing group and a computational biology group, we identified three user roles, their incentives, and their view on neural network models. <ref type="figure">Figure 3</ref> summarizes the following roles and which aspect of a model they might hope to master:</p><p>• Architects are looking to develop new deep learning methodologies or to modify existing deep architectures for new domains. An architect is interested in training many variant network structures and comparing how the models capture the features of their domain. We assume that the architects are deeply knowledgeable about machine learning, neural networks, and the internal structure of the system. Their direct goal is comparing the performance of variant models and understanding the learned properties of the system.</p><p>• Trainers are those users interested in applying known architectures to tasks for which they are a domain experts. Trainers utilize RNNs as a tool and understand the key concepts of network optimization. However, their main focus is on the application domain and utilizing effective methods to solve known problems. Their goal is to use a known network architecture and to observe how it learns a novel model. Examples of trainers include a bioinformatician or an applied machine learning engineer.</p><p>• End Users make up the most prevalent role of network users. End users utilize general-purpose pretrained networks for various tasks. These users may not need to understand the training process at all, only how to apply the networks as an algorithm to new data. Their main desire is to explain the results and locate what is happening when something goes wrong. Examples of end-users include data scientists or product engineers using ML.</p><p>These user roles are general to the neural network domain and we believe they can help to describe and understand stakeholders in this space. For our analysis, we decided to focus on the more advanced end of this spectrum, particularly on the user role of architects. We aimed to provide these users with greater visibility into the internals of the system. User feedback from our first prototype further motivated us to include the trainer role as users with a focus on the predictions of the model. In future work, we want to develop systems to engage the end users of RNN and other deep neural networks more. With these roles in mind, we aimed to help trainers and architects better understand the high-level question: "What information does an RNN capture in its hidden feature-states?". Addressing this question is the main goal of our project. Based on a series of dicussions with deep learning experts we identified the following domain goals:</p><p>• G1 -Formulate a hypothesis about properties that the hidden states might learn to capture for a specific model. This hypothesis requires an initial understanding of hidden state values over time and a close read of the original input.</p><p>• G2 -Refine the hypothesis based on insights about learned textual similarities based on patterns in the dynamics of the hidden states. Refining a hypothesis may also mean rejecting it.</p><p>• G3 -Compare models and datasets to allow early generalization about the insights the representations provide, and to observe how task and domain may alter the patterns in the hidden states.</p><p>During the design phase, we developed the following list of tasks for visual data analysis from the three domain goals (G1-G3). The mapping of these tasks to goals is indicated by square brackets:</p><p>• T1 -Visualize hidden states over time to allow exploration of the hidden state dynamics in their raw form.</p><p>[G1]</p><p>• T2 -Filter hidden states by using discrete textual selection along with continuous thresholding. These selections methods allow the user to form hypotheses and to separate visual signal from noise.</p><p>[G1,G2]</p><p>• T3 -Match selections to similar examples based on hidden state activation pattern. A matched phrase should have intuitively similar characteristics as the selection to support or reject a hypothesis.</p><p>[G2]</p><p>• T4 -Align textual annotations visually to matched phrases. These annotations allow the user to compare the learned representation with alternative structural hypotheses such as partof-speech tags or known grammars. The set of annotation data should be easily extensible. [G2,G3]</p><p>• T5 -Provide a general interface that can be used with any RNN model and text-like dataset. It should make it easy to generate crowd knowledge and trigger discussions on similarities and differences between a wide variety of models.</p><p>[G3]</p><p>This list of tasks provided a guideline for the design of LSTMVIS. In addition, tasks (T1-T4) define the core interaction mechanisms for discovery with LSTMVIS: Visualize (Section 5.1) -Filter &amp; Select (Section 5.2) -Match &amp; Align (Section 5.3). We will first describe the implementation of these interactions and later demonstrate their application to multiple use cases in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN OF LSTMVIS</head><p>LSTMVIS is composed of two major visual analysis components. The Select View (Section 5.2) supports the formulation of a hypothesis (T2, G1) by using a novel visual encoding for hidden state changes (T1, Section 5.1). The Match View (Section 5.</p><p>3) allows refinement of a hypothesis (T3, T4, G2) while remaining agnostic to the underlying data or model (T5). Our design decisions are the outcome of a long iterative process. First, we developed several interactive, low-fidelity prototypes of varying complexity with our expert users to highlight different aspects of the data <ref type="figure">(Figure 4</ref>). After developing a complete system, we released it to a broader audience online to collect long-term feedback (Section 7). Based on the feedback from this release we developed the current version several months later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visualization of Hidden State Changes</head><p>Visualizing the progression of hidden state vectors h 1 ,...,h T along a sequence of words (time-steps) is at the core of LSTMVIS. In the following, we refer to hidden state as one dimension of the Ddimensional hidden state vector.</p><p>In the related literature, it is common to encode hidden state vectors as a heatmap along the time-axis <ref type="figure">(Figure 4(a)</ref>). This style has been a b <ref type="figure">Fig. 4</ref>. Two early-stage prototypes of the system. (a) Hidden state vectors are encoded as heatmaps over time. This style places emphasis on the relationships between neighboring (vertically adjacent) states, which has no particular meaning for this model. (b) A selection prototype utilizing parallel coordinates. This prototype emphasized selections based on small movements of state values directly on the plot, which made it difficult to specify connections between hidden state values and source text.</p><p>favored as a view of the complete hidden state vectors h 1 ,...,h T . However, this approach has several drawbacks in an interactive visualization. Foremost, the heatmaps do not scale well with increasing dimensionality D of hidden state vectors. They use a non-effective encoding for the most important information, i.e. hidden state values by color hue. Additionally, they emphasize the order of hidden states in each vector, but this relative order of abstract hidden states is not actually used by the model itself.</p><p>We decided to consider each hidden state as a data item and timesteps as dimensions for each data item in a parallel coordinates plot. Doing so, we encode the hidden state value using the more effective visual variable position. <ref type="figure">Figure 4(b)</ref> shows the first iteration on using a parallel coordinates plot. The number of data points along the plot is additionally encoded by a heatmap in the background to emphasize dense (e.g., around the zero value) and sparse regions.</p><p>However, it was very cumbersome to formulate a hypothesis for a longer range by adjusting many y-axis brush selectors at a fine granularity. Allowing the user to perform selection by directly manipulating the hidden state values felt decoupled from the original source of information-the text. The key idea to facilitate this selection process was to allow the user to easily discretize the data based on a threshold and select on and off ranges directly on top of the words (Section 5.3). <ref type="figure">Figure 6</ref> shows the first complete prototype that we put online to collect long-term user feedback (Section 7). Several user comments led us to believe that the redundant encoding of hidden states that are "on" in the Select View <ref type="figure">(Figure 6d</ref>,e) was not well understood. In the final design shown on the top in <ref type="figure">Figure 1</ref> we omitted this redundant encoding for the sake of clarity and to highlight wider regions of text. The xaxis is labeled with the word inputs w 1 ,...,w T for the corresponding time-step. If words do not fit into the fixed width for time steps they are distorted. <ref type="figure">Figure 1</ref> shows the movement of a hidden state vector through an example sequence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Select View</head><p>The Select View, shown in the top half of <ref type="figure">Figure 1</ref>, is centered around the parallel coordinates plot for hidden state dynamics. The full plot can be difficult to comprehend directly. Therefore, LSTMVIS allows the user to formulate a hypothesis (G1) about the semantics of a subset of hidden states by selecting a range of words that may express an interesting property. For instance, the user may select a range within a shared nesting levels in tree-structured text (Section 6.1), a representative noun phrase in a text corpus (Section 6.2), or a chord progression in a musical corpus(Section 6.4).</p><p>To select, the user brushes over a range of words that form the pattern of interest <ref type="figure">(Figure 1c)</ref>. In this process, she implicitly selects hidden states that are "on" in the selected range. The dashed red line on the parallel coordinates plot <ref type="figure">(Figure 1t</ref>) indicates a user-defined threshold value, , that partitions the hidden states into "on" (timesteps ≥ ) and "off" (timesteps &lt; ) within this range.</p><p>In addition to selecting a range, the user can modify the pattern plot below <ref type="figure">(Figure 1d</ref>) to define that hidden states must also be "off" immediately before or after the selected range. <ref type="figure">Figure 5</ref> shows different combinations of pattern plot configurations and the corresponding hidden state selections. Using these selection criteria the user creates a set of selected hidden states S 1 ⊂ {1 ...D} that follow the specified on/off pattern w.r.t. the defined threshold.</p><p>To assist with the selection of ranges, the user can make use of a heatmap underlying the word sequence which depicts how many of the selected hidden states are "on" for each word <ref type="figure">(Figure 1c</ref>). This indicates repetitions of hidden state patterns in the close local neighborhood.</p><p>Additionally, a user can add aligned tracks of textual annotations (meta tracks) to the selection view. E.g., she can visualize part-ofspeech (POS) annotations or named entity recognition (NER) results. The feature of meta tracks is the result of feedback from multiple online users asking us to include the tracks. Some users that used the tool for training also wanted to see the top K predictions (outcomes) for each word. <ref type="figure">Figure 1</ref> shows examples for POS (e1) and topK (e2).</p><p>LSTMVIS also provides several convenience methods to navigate to specific time steps. Buttons on the timeline can be used to move forward and backward <ref type="figure" target="#fig_0">(Figure 1-i2</ref>). LSTMVIS also offers search functionality to find specific phrases. The selection panel on the top can be used to efficiently switch between the different layers (T5). The word-width can be decreased and increased <ref type="figure">(Figure 1-i1)</ref> to provide support for different average word lengths (e.g., character-based models vs. word-based models). The option to turn heatmaps on and off in Match View <ref type="figure" target="#fig_0">(Figure 1g1 and g2)</ref> provides convenience when working with many mate tracks.</p><p>These interaction methods allow the user to define a hypothesis as word range which results in the selection of a subset of hidden states following a specified pattern w.r.t. a defined threshold (T2, G1) that only relies on the hidden state vectors themselves (T5). To refine or reject the hypothesis the user can then make use of the Match View.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Match View</head><p>The Match View <ref type="figure">(Figure 1b)</ref> provides evidence for or against the selected hypothesis. The view provides a set of relevant matched phrases that have similar hidden state patterns as the phrase selected by the user.</p><p>With the goal of maintaining an intuitive match interface, we define the matches to be ranges in the data set that would have lead to a similar set of on hidden states under the selection criteria (threshold, on/off pattern). Formally, assume that the user has selected a threshold with hidden states S 1 and has not limited the selection to the right or left further, as shown in <ref type="figure">Figures 5a and 5b</ref>. We rank all possible candidate ranges in the dataset starting at time a and ending at time b with a two step process 1. Collect the set of all hidden states that are "on" for the range,  For instance, if there is a limit on the left, we only include state indices c in S 2 in that also satisfy h a−1,c &lt; . For efficiency, we do not score at all possible candidate ranges (datasets typically have T &gt; 1 million). We limit the candidate set by filtering to ranges with a minimum number of hidden states from S 1 over the threshold . These candidate sets can be computed efficiently using run-length encoding.</p><p>From the matching algorithm, we retrieve the top 50 results which are shown, one per row each, in a word matrix (e.g., <ref type="figure">Figure 1f</ref>) located in the Match View. Each cell in the word matrix is linked to a cell in corresponding heatmaps. These heatmaps encode additional information about each word in the matching results. The always available match count heatmap <ref type="figure">(Figure 1-g1</ref>) encodes the number of overlapping states |S 1 ∩ S 2 | for each timestep.</p><p>The user can use additional annotations, similar to meta tracks in the Selection View, as heatmaps (T4). We imagine these annotations act as ground truth data, e.g., part-of-speech tags for a text corpora <ref type="figure" target="#fig_0">(Figure 1-g2</ref>), or as further information to help calibrate the hypotheses, e.g., nesting depth of tree-structured text. <ref type="figure">Figure 1</ref> shows how hovering over "little" (mouse pointer) leads to highlights in the match count heatmap (g1) indicating seven overlapping states between match and selection for this position. The POS heatmap (g2) indicates that the word at this position acts as adjective (ADJ).</p><p>The heatmap colors can be mapped directly to the background of the matches <ref type="figure">(Figure 1h</ref>) as a simple method to reveal pattern across results <ref type="figure">(Figure 1f</ref>). Based on human identifiable patterns or alignments, the matching and mapping methods can lead to further data analysis or a refinement of the current hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Design Considerations and Limitations</head><p>LSTMVis operates around as a bottom-up approach starting from a seed hypothesis and searching for similarities across the whole dataset. During the project, we experimented with top-down approaches following the Shneiderman mantra (overview first -zoom and filter later). We found that global overviews, such as projections of hidden state values or summary statistics, did little to reveal interpretable patterns on large datasets. LSTMVis differs from related work, in that instead of working with the prediction output, it operates directly on the hidden states to identify relationships. This design decision addresses our target group of architects and trainers who are familiar with model internals.</p><p>Another goal of our tool is to use a representation that is invariant to the ordering of hidden states in the hidden state vector while remaining scalable w.r.t. size of the vector. As addressed in Section 5.1, those two conditions are fulfilled by plotting hidden states individually onto parallel coordinates. However, even with this compact representation, applications with &gt; 500 hidden states become challenging to analyze as a whole. Approaches to address the visual noise are to either filter the hidden states using our interactive methods or to use algorithmic preprocessing, like dimensionality reduction techniques or pruning methods. After testing the latter automated filtering approaches, we decided against using them within the application. We found that if the tool loses its inherent connection to the data, results were less interpretable to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Technical Design and Implementation</head><p>LSTMVIS consists of two modules, the visualization system and the RNN modeling component. The source code with documentation and some example models are available at lstm.seas.harvard.edu.</p><p>The visualization is a client-server system that uses Javascript and D3 on client side and Python, Flask, Connexion, h5py, and numpy on server side. Timeseries data (RNN hidden states and input) is loaded dynamically through HDF5 files. Optional annotation files can be specified to map categorical data to labels (T4). New data sets can be added easily by a declarative YAML configuration file. The RNN modeling system is completely separated from the visualization to allow compatibility with any deep learning framework (T5). For our experiments we use the Torch framework. We trained our models separately and exported results to the visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USE CASES</head><p>In experimenting with the system we trained and explored many different RNN models, datasets and tasks, including word and character language models, neural machine translation systems, auto-encoders, summarization systems, and classifiers. Additionally, we also experimented with other types of real and synthetic input data.</p><p>In this section we highlight three use cases that demonstrate the general applicability of LSTMVIS for the analysis of hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Proof-of-Concept: Parenthesis Language</head><p>As proof of concept we trained an LSTM as language model on synthetic data generated from a very simple counting language with a parenthesis and letter alphabet Σ = {( ) 0 1 2 3 4 }. The language is constrained to match parentheses, and nesting is limited to at most 4 levels deep. Each opening parenthesis increases and each closing parenthesis decreases the nesting level, respectively. Numbers are generated randomly, but are constrained to indicate the nesting level at their position. For example, a string in the language might look like:</p><p>Blue lines indicates ranges of nesting level ≥1, orange lines indicate nesting level ≥2, and green lines indicate nesting level ≥3.</p><p>To analyze this language, we view the states in LSTMVIS. An example of the cell states of a multi-layer 2x300 LSTM modelis is shown in <ref type="figure" target="#fig_3">Figure 7(a)</ref>. Here even the initial parallel coordinates plot shows a strong regularity, as hidden state changes occur predominately at parentheses.</p><p>Our hypothesis is that the hidden states mainly reflex the nesting level. To test this, we select a range spanning nesting level four by selecting the phrase ( 4. We immediately see that several hidden states seem to cover this pattern and that in the local neighborhood several other occurrences of our hypothesis are covered as well, e.g., the empty parenthesis and the full sequence ( 4 4 4 . This observation nicely confirms earlier work that demonstrates simple context-free models in RNNs and LSTMs <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b32">31]</ref>. Next we consider the case of a real-world natural language model from the perspective of an architect interested in the structure of the internal states and how they relate to underlying properties. For this experiment we trained a 2-layer LSTM language model with 650 hidden states on the Penn Treebank <ref type="bibr" target="#b21">[21]</ref> following the medium-sized model of <ref type="bibr" target="#b34">[33]</ref>. While the model is trained for language modeling (predict the next word), we were interested in seeing if it additionally learned properties about the underlying language structure. To test this, we additionally include linguistic annotations in the visual analysis from the Penn Treebank. We experimented with including part-of-speech tags, named entities, and parse structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Phrase Separation in Language</head><p>Here we focus on the case of phrase chunking. We annotated the dataset with the gold-standard phrase chunks provided by the CoNLL 2003 shared task <ref type="bibr" target="#b30">[29]</ref> for a subset of the treebank <ref type="figure" target="#fig_4">(Sections 15-18</ref>). These include annotations for noun phrases and verb phrases, along with prepositions and several other less common phrase types.</p><p>While running experimental analysis, we found a strong pattern that selecting noun phrases as hypotheses leads to almost entirely noun phrase matches. Additionally, we found that selecting verb phrase prefixes would lead to primarily verb phrase matches. In <ref type="figure">Figure 6</ref>.2 we show two examples of these selections and matches.</p><p>The visualization hints that the model has implicitly learned a representation for language modeling that can differentiate between the two types of phrases. Of course the tool itself cannot confirm or deny this type of hypothesis, but the aim is to provide clues for further analysis. We can check, outside of the tool, if the model is clearly differentiating between the classes in the phrase dataset. To do this we compute the set S 1 for every noun and verb phrase in the shared task. We then run PCA on the vector representation for each set. The results are shown in <ref type="figure">Figure 6</ref>.2, which shows that indeed these on-off patterns are enough to partition the noun phrases and verb phrases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Biological Sequence Analysis</head><p>RNN models are now commonly used for time-series analysis outside of the space of text processing. One area of interest is in biological sequence analysis for genomics where deep neural networks are directly applied to sequences of DNA for tasks such as classification and sequence labeling. We consider models trained for regulatory marker prediction, where neural networks are used to predict binding sites aligned to sequences of the genome, a task that has been recently explored with both CNN <ref type="bibr" target="#b36">[35]</ref> and RNN models <ref type="bibr" target="#b24">[24]</ref>. For this case study, we collaborated with a domain expert trainer who approached us after becoming aware of the release of our tool. He had employed a mixed RNN/CNN model that was trained over a genomic dataset made up of a 2.3 billion base pair long nucleotide sequence for this problem of regulatory marker prediction.</p><p>Notably this problem differs in several crucial ways from our previous use case: the granularity of the input is much smaller (base pairs as opposed to words), the training objective is different (0/1 binding site labels as opposed to the next word), and the problem is known to exhibit much longer term dependencies due to its latent 3D structure. Conversations around these issues led us to introduce several iterative features to our tool. These include the (a) ability to zoom in and out of the timeseries representation to adjust for granularity, (b) use of auto-scaling axes ranges to allow for switching between data of different activation ranges, such as CNN outputs, and most notably (c) ability to add/remove arbitrarily many annotation label along the word sequence timeline, to allow domain experts to view both the predictions and ground truth as an annotation track associated with the features at each time step. <ref type="figure">Figure 6</ref>.3 shows an example of the last feature which shows the predict binding locations for several different proteins along a region of DNA.</p><p>For this application the tool has so far been used primarily as a debugging assistant, providing a way for an expert in an ongoing research project to analyze the output of a trained model and make adjustments based on mistakes of the model. For instance, in early use of the tool, the researcher noticed that one layer of the network was using very few of the available states, and another had significant artifacts from a poorly aligned convolutional layer. These observations have helped provide feedback for subsequent experimental design. Furthermore, recent work indicates an increasing interest in LSTMVis in the biomedical domain and for genomics (see Section 7). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Musical Chord Progressions</head><p>Past work on LSTM structure has emphasized cases where single hidden states are semantically interpretable. For text and biological data sets, we found that with a few exceptions (quotes, brackets, and commas) this was rarely the case. However, for datasets with more regular long-term structure, single states could be quite meaningful.</p><p>As a simple example, we collected a large set of songs with annotated chords for rock and pop songs to use as a training data set, 219k chords in total. We then trained an LSTM language model to predict the next chord w t+1 in the sequence, conditioned on previous chord symbols (chords are left in their raw format).</p><p>When we viewed the results in LSTMVIS we found that the regular repeating structure of the chord progressions is strongly reflected in the hidden states. Certain states will turn on at the beginning of a standard progression, and remain on though variant-length patterns until a resolution is reached. In <ref type="figure">Figure 6</ref>.4, we examine three very common general chord progressions in rock and pop music. We select a prototypical instance of the progression and show a single state that captures the pattern, i.e., remains on when the progression begins and turns off upon resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LONG-TERM CASE STUDY</head><p>Shneiderman and Plaisant <ref type="bibr" target="#b27">[26]</ref> propose strategies for multi-dimensional in-depth long-term case (MILC) studies to evaluate information visualization tools. They observe that "scaling up by an order of magnitude in terms of number of users and duration of the observations is clearly beneficial." We decided to adopt their core ideas and report on qualitative feedback and quantitative success indicators after the open-source release of LSTMVIS in June 2016.</p><p>We created a webpage and a video that introduces the core ideas of LSTMVIS at lstm.seas.harvard.edu. The webpage provides an opportunity for users to leave comments. To advertise the tool online we followed a social media strategy that included collecting followers on Twitter, using broadcast media such as Reddit and Hackernews, and inviting editors of popular machine learning blogs to write guest articles.</p><p>The webpage contains a demo system with example datasets that allows us to acquire a large set of logging data. We noticed that users often try to reproduce the scenarios that are explained in the online video. To allow users to share insights from their exploratory analysis, we ensured that our URL parameter string contains all necessary information to replay and share a scenario.</p><p>We collected logging information about our webpage using Google Analytics. Within the first 7 days, the web page received ∼5,600 unique users, with 49% of traffic coming through social media and 39% arriving directly. The social media traffic was dominated by channels that we used to advertise the tool (Twitter 40%, Reddit 26%). To our surprise we also observed substantial traffic from channels that we did not contact directly (e.g., Sina Weibo 11%, Google+ 9%). After 300 days we recorded ∼19,500 page views with shrinking user traffic from social media (31%). At that point most users used search (22%) or accessed the webpage directly (39%). Only a small percentage (10%) of users tried the online demo. Most of these users used the datasets shown in the explanation video and did not explore further.</p><p>Our open source code release was stable yet simple enough to "...ensure that the tool has a reasonable level of reliability and support for basic features" <ref type="bibr" target="#b27">[26]</ref>. For easy adoption, we provide user documentation that explains what data can be used and how to use the tool. We also provide convenience tools to prepare the data for import (Section 5.5). We asked students to act as testers for our source code. Based on their feedback we made several improvements to the installation process. For example, our prototype required NodeJS to resolve client-side dependencies. By providing the required libraries within our repository we removed the cumbersome step of installing NodeJS for our target audience. We observe that around 400 programmers liked the project (stars on GitHub) and over 95 practitioners copied (forked) the project to make custom modifications. We think that this demonstrate a reasonable interest in the system after only 300 days considering its highly specialized target audience.</p><p>Furthermore, we observe adoption of the tool for several documented use cases. Evermann et.al. <ref type="bibr" target="#b8">[8]</ref> describe the application of LSTMVis to understand a trained model for a business process intelligence dataset. Liu et.al. <ref type="bibr" target="#b19">[19]</ref> use LSTMVis in experiments to investigate language variants and vagueness in website privacy policies. We see an increasing interest to apply our tool for biomedical and genomic data. This is indicated by the use case we described in Section 6.3 or, e.g., in Ching et.al. <ref type="bibr" target="#b4">[5]</ref> Besides the quantitative observations we also collected qualitative feedback from comments on the webpage, GitHub tickets (feature requests), and in-person presentations of the prototype. This qualitative feedback led us to make several changes to our system that were discussed in Section 5.</p><p>In retrospective, conducting a long-term user study benefited the project at multiple stages. Preparing the release of the prototype required us to focus strongly on simplicity, usability, and robustness of our tool. This lead to many small improvements to an internal prototype. The design iterations we inferred from user feedback strengthen the tool further. We think, that planning a successful long-term study requires four core ingredients: (1) reach out to your target audience by describing your approach (webpage, video) and inform them using social media, email, etc. (2) allow users to play with your tool by setting up a demo server, (3) allow engagement and experimentation with your tool by providing sufficiently documented, easily adoptable source code, and (4) make it as simple as possible for users to give you feedback via discussion forums, reported issues, or in person. During the study, we found it to be crucial to continuously engage with users and quickly take action based on their feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>LSTMVIS provides an interactive visualization to facilitate data analysis of RNN hidden states. The tool is based on direct inference where a user selects a range of text to represent a hypothesis and the tool then matches this selection to other examples in the data set. The tool easily allows for external annotations to verify or reject hypothesizes. It only requires a time-series of hidden states, which makes it easy to adopt for a wide range of visual analyses of different data sets and models, and even different tasks (language modeling, translation etc.).</p><p>To demonstrate the use of the model we presented several case studies of applying the tool to different data sets. Releasing the tool and source code online allows us to collect long-term user feedback that has already led us to make several improvements. In future work, we will explore how the wide variety of application cases can be adoptedbeyond our imagined use cases and user groups. As example for such a use case, we got contacted by a highschool student using LSTMVis to learn about RNN methods. While we did not optimize for a learning scenario, we are now thinking about datasets and a blog publication that focus on learning. Another interesting future work is to support the user role of end users with simplified views on internals of RNN to help explaining specific model behavior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>A recurrent neural network language model being used to compute p(w t+1 |w 1 ,..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>The hypothesis selection process. (a) The selection covers a little prince and has a threshold = 0.3. Blue highlighted hidden states are selected. (b) The threshold is raised to 0.6. (c) The pattern plot in the bottom is extended left, eliminating hidden states with values above after reading of (one word to the left). (d) The pattern plot is additionally extended right, removing hidden states above the threshold after reading "." (one word to the right). I.e., only hidden states are selected with the following pattern: below threshold for one word before -above during a little prince -below for one word after. Snippet of the first complete prototype. While the list of selected cells (a) and the brushing method (c) remained in the final version, we modified the pattern plot (b) and omitted the redundant encodings (d) and (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>S 2 =</head><label>2</label><figDesc>{c ∈ {1 ...D} : h t,c ≥ for all a ≤ t ≤ b} 2. Rank the candidates by the number of overlapping states |S 1 ∩ S 2 | using the inverse of the number of additional "on" cells −|S 1 ∪ S 2 | and candidate length b − a as tiebreaks.If the original selection is limited on either side (as, e.g., inFigures 5c or 5d), we modify step (2) to take this into account for the candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Plot of a phrase from the parenthesis synthetic language. (a) The full set of hidden states. Note the strong movement of states at parenthesis boundaries. (b) A selection is made at the start of the fourth level of nesting. Even in the select view it is clear that several hidden states represent a four-level nesting count. In both plots the meta-track indicates the nesting level as ground truth. (c) The result of matching the selected states indicates that they seem to capture nesting level 4 phrases of variable length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Phrase selections and match annotations in the Wall Street Journal. (a) The user selects the phrase a very marked improvement (turning off when improvement is seen). The matches found are entirely other noun phrases, and start with different words. Note that here groundtruth noun phrases are indicated with a sequence of colors: cyan (DET), blue (ADV), violet (ADJ), red (NOUN). (b) We select a range starting with has invited. The results are various open verb phrases as sequence of colors orange (VERB) and blue (ADV). Note that for both examples the model can return matches of varying lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>PCA projection of the hidden state patterns (S 1 ) of all multi-word phrasal chunks in the Penn Treebank, as numerical follow-up to the phrase chunking hypothesis. Red points indicate noun phrases, blue points indicate verb phrases, other colors indicate remaining phrase types. While trained for language modeling, the model separates out these two phrase classes in its hidden states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Biological Sequence Analysis. A selected region of the genome with seven different aligned annotation tracks. Each track indicates the 0/1 prediction of protein binding sites at each time step. Tracks can be activated/deactivated as part of the debugging process to compare predictions, ground truth, input properties, and other user-specified annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Three examples of single state patterns in the guitar chord dataset. (a) We see several permutation of the very common I -V -vi -IV progression (informally, the "Don't Stop Believing" progression). (b) We see several patterns ending in a variant of the I-vi-IV-V (the 50's progression). (c) We see two variants of I -V -vi -iii -IV -I (beginning of the Pachelbel's Canon progression). Chord progression patterns are based on http://openmusictheory.com/.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Manuscript received 31 Mar. 2017; accepted 1 Aug. 2017. Date of publication 28 Aug. 2017; date of current version 1 Oct. 2017. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2017.2744158 LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, and Alexander M. Rush</figDesc><table><row><cell></cell><cell>i1</cell><cell></cell></row><row><cell>a</cell><cell>c</cell><cell></cell></row><row><cell>d</cell><cell>e1</cell><cell>e2</cell></row><row><cell></cell><cell></cell><cell>g1</cell><cell>h</cell><cell>g2</cell></row><row><cell>b</cell><cell></cell><cell>f</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• HS is with Harvard SEAS -hendrik@strobelt.com • SG, HP, AR are with Harvard SEAS -{gehrmann,pfister,rush}@seas.harvard.edu.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig. 3. Views on neural network models for different user roles. The architect analyzes and modifies all components of the system. The trainer abstracts the model to the main components and parameters and focuses on training on different data sets. The end user has the most abstract view on the model and considers whether the output is coherent for a given input.</figDesc><table><row><cell>F</cell><cell>X</cell><cell>Z</cell><cell>I X X +</cell><cell>O</cell><cell>X</cell><cell>Input</cell><cell>Layer 1 Layer 2 word2vec …</cell><cell>Output</cell><cell>Input</cell><cell>mostly black box</cell><cell>Output + Reliability</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Parameters</cell><cell></cell><cell></cell><cell></cell><cell>Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>architect</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>trainer</cell><cell></cell><cell></cell><cell>end-user</cell><cell></cell><cell></cell></row><row><cell cols="7">al. [16] use static visualization techniques to help understand hidden</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">states in language models. Their work demonstrates that selected cells</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">can model clear events such as open parentheses and the start of URLs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Li et al. [18] present additional techniques, particularly the use of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">gradient-based saliency to find important words. Their work also looks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">at several different models and datasets including text classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">and auto-encoders. Kadar et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Karpathy et</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the Air Force Research Laboratory, DARPA grant FA8750-12-C-0300, Oracle Research grant, and NIH grant U01CA198935.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seetapun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1512.02595</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning, ICML 2012</title>
		<meeting>the 29th International Conference on Machine Learning, ICML 2012<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012-07-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Himmelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Beaulieu-Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Agapow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Lengerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Israeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woloszynek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Cofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decaprio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H S</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gitter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Opportunities and obstacles for deep learning in biology and medicine. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greene</surname></persName>
		</author>
		<idno type="DOI">10.1101/142760</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Predicting process behaviour using deep learning. Decision Support Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Rehse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fettke</surname></persName>
		</author>
		<idno>doi: 10.1016/j .dss.2017.04.003</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lstm recurrent networks learn simple context-free and context-sensitive languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1333" to="1340" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Lingusitic analysis of multi-modal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alishahi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Representation of linguistic form and function in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Á</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alishahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08952</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">413</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive optimization for steering machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1343" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="681" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling language vagueness in privacy policies using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Fella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Fall Symposium Series</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Danq: a hybrid convolutional and recurrent deep neural network for quantifying the function of dna sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="page">226</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Falco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
		<idno>doi: 10</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Tvcg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Strategies for evaluating information visualization tools: multi-dimensional in-depth long-term case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<idno type="DOI">10.1145/1168149.1168158</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 AVI Workshop on BEyond time and errors: novel evaluation methods for information visualization</title>
		<meeting>the 2006 AVI Workshop on BEyond time and errors: novel evaluation methods for information visualization<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05-23" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Tjong Kim Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F. De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Opening the black box-data driven visualization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIS 05. IEEE Visualization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent neural networks can learn to implement symbolsensitive counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R J</forename><surname>Wiles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>F. R. Bach and D. M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent Neural Network Regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Predicting effects of noncoding variants with deep learning-based sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="931" to="934" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
