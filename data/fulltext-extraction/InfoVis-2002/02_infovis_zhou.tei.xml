<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T18:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Example-based graphics generation systems automatically create new information visualizations by learning from existing graphic examples. As part of the effort on developing a general-purpose example-based generation system, we are building a visual database of graphic examples. In this paper, we address two main issues involved in constructing such a database: example selection and example modeling. As a result, our work offers three unique contributions: First, we build a visual database that contains a diverse collection of well-designed examples. Second, we develop a feature-based scheme to model all examples uniformly and accurately. Third, our visual database brings several important implications to the area of information visualization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automated graphics generation systems promise to simplify developers' tasks by automatically designing visualizations based on data characteristics and user tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. To automatically create proper visualizations, most existing systems use a rule-based approach <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. Nonetheless, hand-crafting design rules and managing an increasingly larger rule base could be very challenging.</p><p>Alternatively, we are exploring an example-based approach to graphics generation. Our approach uses a casebased machine learning method <ref type="bibr" target="#b6">[7]</ref> to create new graphics from a database of existing graphics (examples). Upon a user's request (e.g., comparing two data sets), our approach first uses a quantitative similarity measuring model to retrieve the top-matched examples from the database <ref type="bibr" target="#b22">[23]</ref>. The top-matched examples are then directly reused for or be adapted to the new situation (e.g., new data).</p><p>As the success of case-based learning largely depends on the design of the case database <ref type="bibr" target="#b6">[7]</ref>, part of our effort is building a visual database of graphic examples. To the best of our knowledge, the challenges and implications of constructing such a database have not been addressed before. In this paper we address two main issues involved: example selection and example modeling.</p><p>Example selection is concerned with the quality and coverage of the database, which directly impact the capability of the generation system and the quality of the results. For example, if our database included only examples designed for a specific application/domain, the system would then not be able to synthesize effective graphics for other applications/domains. Therefore, we have formulated a set of criteria to guide our example selection.</p><p>Example modeling addresses how we express a diverse collection of examples accurately and uniformly to facilitate case-based learning. Specifically, we must capture various characteristics of graphic examples for case retrieval <ref type="bibr" target="#b22">[23]</ref>. We thus develop a feature-based XML annotation scheme to describe each example. This step is necessary, since it remains a challenge for today's technologies to automatically obtain higher-level image properties (e.g., what data is encoded or what visual technique is used).</p><p>By addressing the two issues mentioned above, our work presents three unique contributions: 1) We build the first visualization database that contains over 300 graphic examples from a wide variety of sources. These examples range from 2D business diagrams to 3D molecular visualization.</p><p>2 <ref type="bibr">)</ref> We develop a comprehensive model to express a graphic as a collection of integrated, hierarchical data and visual features. Our model enables us to capture richer and finer characteristics of graphics and to perform quantitative visual analysis using machine learning.</p><p>3) Our database also fills a long-standing need for a benchmark to test and compare different visualization techniques, and the need for a knowledge repertoire to learn visual design techniques and principles.</p><p>We organize the rest of our paper as follows. After a brief review of related work, we describe our visual database construction and discuss its potential applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the area of multimedia retrieval, researchers have built various visual databases of images/videos <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16]</ref>. These images/videos come with minimal semantic annotations. In contrast, we construct a database of information visualizations with fine-grained annotations to facilitate semantics-based visual analysis and retrieval.</p><p>To characterize information graphics, researchers have worked on data characterization <ref type="bibr" target="#b20">[21]</ref> and different visual design taxonomies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref>. While our annotation scheme is based in part on these previous efforts, we have extended them to express a wider variety of properties of a graphic, and at multiple levels of abstraction. As described in Section 3.2, our current model also offers several significant improvements over our initial version <ref type="bibr" target="#b22">[23]</ref>. There are also related efforts on example-based graphics generation. A notable system, Sage/SageBook <ref type="bibr" target="#b1">[2]</ref>, reuses examples created by its own rule engine. Thus it does not touch upon the issue of building a general visual database. In contrast, we are building a typical example-based generation system that must exploit examples from different sources. Moreover, SageBook describes the data and visual features of its examples separately. We, on the other hand, use an integrated scheme to express data features, visual features, and the corresponding data-visual mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building a Visual Database for Example-based Graphics Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Constructing Visual database</head><p>In this section, we describe how to choose proper examples using a set of criteria and how to model a graphic example as a set of data and visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Example Selection</head><p>As a knowledge source for designing new graphics, our examples directly impact the capability/quality of the generation system. Thus, we formulate a set of quality and coverage criteria to aid our example selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Criteria</head><p>We measure the quality of examples from three aspects.</p><p>Suitability. Almost every graphic is created with a purpose (e.g., illustrating information vs. decoration <ref type="bibr" target="#b7">[8]</ref>). It would not be effective to create new graphics by learning from examples that were designed for a different purpose. Accordingly, suitability measures how closely candidate examples are related to our overall system goal. As our current goal is to create information visualizations, we choose only graphics that serve the purpose of communicating information and helping users achieve their informationseeking goals. By this criterion, for example, <ref type="figure" target="#fig_3">Figure 1</ref> Modelability. To allow computers to perform accurate analysis (e.g., similarity measuring), we must model the characteristics of each example comprehensively, including both its data and visual properties. Modelability judges whether a candidate graphic can be accurately described by a defined model. For the purpose of graphics generation, currently we select only examples that can be expressed using our feature-based model described later.</p><p>It is worth mentioning that not every graphic can be accurately modeled due to missing information or the complexity of information. For example, we cannot model <ref type="figure">Figure</ref> 1(c) accurately, since its source <ref type="bibr" target="#b4">[5]</ref> did not explain what exact data the graphic intends to convey. Although all the information needed to model <ref type="figure" target="#fig_3">Figure 1</ref>(d) is available, it is difficult to describe each penalty pose 1 precisely, which happens to be the key characteristic of this example.</p><p>Design quality. To create useful new graphics, we use design quality to assess how effectively candidate examples can convey the intended information. This seems the most obvious criterion but the most difficult one to follow due to the subjectivity involved in measuring the effectiveness of a visualization <ref type="bibr" target="#b10">[11]</ref>. Currently, we qualitatively evaluate the design quality of an example by judging whether we can digest the intended information and how quickly we can.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coverage Criteria</head><p>To handle heterogeneous data using an assortment of visual techniques, our system relies on having a diverse set of examples. Moreover, the diversity prevents our system from learning biased designs (e.g., learning a visual technique only meant for a particular type of data/domain). In our work, we use data and visual coverage to systematically evaluate the diversity of examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data coverage. Data coverage measures the number of different visualization situations covered by our examples.</head><p>Here a visualization situation is characterized by the data to be conveyed, user's information-seeking goals, and the environment settings (e.g., display devices used). Accordingly, we measure data coverage by evaluating the heterogeneity of these three factors. For example, <ref type="figure" target="#fig_3">Figure 1</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Using the criteria defined above, we have collected over 300 examples from a wide variety of sources, including newspapers (e.g., NY Times), design books (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref>), and automated graphics generation systems (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>). Not only does our collection cover a rich assortment of data, but it also represents a wide range of visual forms and styles.</p><p>We have also learned a couple of valuable lessons during the selection process. First, we find newspapers to be one of the better sources for finding desired examples, since the accompanying articles give us many clues on what the illustrations aim to convey. Another excellent source is automated graphics generation systems, which produce effective graphics using design rules. Second, we have difficulty in finding examples designated for different environment settings (e.g., devices). This may be attributed to the current domination of the desktop computing.</p><p>Although our selection criteria are established based on our graphics generation needs, they can still be used as general guidelines for constructing a general graphic database. We can easily modify and augment these criteria to suit other application purposes. Furthermore, some of our criteria may become obsolete with the advance of relevant computer technologies. For example, the modelability criterion, would be unnecessary if future technologies can capture all characteristics of a graphic example automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Example Modeling</head><p>Each collected example by itself is just a set of pixels, which carry little meanings. We must model the examples before using them to design new graphics. Our model is based on the theory that a visualization expresses a complex mapping between a set of data elements and visual elements <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. To express such a data-visual mapping, we describe the data elements, visual elements, and their correspondences. In particular, we characterize data/visual elements using a set of symbolic/numeric features.</p><p>Compared to our previous model <ref type="bibr" target="#b22">[23]</ref>, our current one presents two significant improvements. 1) We have modified and extended our previous feature sets to allow a more accurate and comprehensive description of an example. 2) We have augmented the previous one-to-one data-visual mapping structure to capture m-to-n mappings (see below).</p><p>Since we have explained some of our features and their rationale in <ref type="bibr" target="#b22">[23]</ref>, here we present only the enhancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Elements and Visual Features.</head><p>The most basic building blocks of a visual depiction are visual elements <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. A visual element is a meaningful visual pattern with its unique syntax and semantics. It may be recursively made up of other visual elements <ref type="bibr" target="#b19">[20]</ref>. For example, the top-level visual element of <ref type="figure" target="#fig_2">Figure 2</ref> is composed of several lower-level elements, including stacked bars and the labels connecting the bars. Each stacked bar consists of two subelements, the vertical position of the bar and rectangular stacks. Finally, a stack is made up of two primitive elements (elements with no sub-components): a length and a color.</p><p>To express the characteristics of each visual element, we use 7 features <ref type="table" target="#tab_1">(Table 1)</ref>, including three newly added syntactic features: Technique, Parameter, and Dimension. Technique captures the particular visual style used in a visual element, and Parameter describes specific details of a Technique or a visual form (expressed by Category). For example, Parameter may record the actual color used in highlighting an object (a Technique) or a layout constraint used in composing a diagram (a visual form). Capturing visual details is useful, since a generation system may directly reuse these Parameters when realizing a new graphic (see Section 4.1).</p><p>Dimension indicates the dimensionality of the rendering space (e.g., 2D vs. 3D).</p><p>All three new features allow our generation system to better cope with different user visual preferences. Unlike our previous model, which let users express only their preferred visual formalism (e.g., the Category is scatter plot or table), users can now specify their favored visual style (Technique), visual details (Parameter), and rendering space  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path</head><p>The purpose of a visual object, a path in our visual task ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>String</head><p>One of the 10 roles which a visual element can play in a visual composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition</head><p>Structure*</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph</head><p>Represents a visual composition, including the sub-components and the relations among them.</p><p>(Dimension). By taking into account different user preferences, our system could then synthesize customized graphics from the desired examples. We are not claiming that we can accommodate every possible user preference using our current visual features, since building a complete model of user visual preferences is a challenging task itself. Nevertheless, we have made the first attempt to enable users to specify their preferences flexibly at multiple levels.</p><p>To describe complex visual compositions, we extend our previous tree-based Structure feature to be a graph-based feature. Unlike our previous Structure, which only expresses the visual relations between a visual element and its children, the current one also captures the relations among the children. <ref type="figure" target="#fig_7">Figure 3</ref>(a) (right) depicts the overall visual composition of <ref type="figure" target="#fig_2">Figure 2</ref>. For example, the top-level element V0 is made up of V1, V2, and V3; V1 and V2 are juxtaposed together, and V3 connects V2. Our current Structure can easily describe these relations as a graph of four elements V0-V3. In contrast, such complex relationships could not be specified by our previous tree-based <ref type="bibr">Structure.</ref> Data Elements and Data Features. Similarly, we use data elements and their compositions to express the data encoded in a visualization. Specifically, a data element is a meaningful information unit that can be comprehended by users through its visual encodings. In <ref type="figure" target="#fig_2">Figure 2</ref>, for example, the revenue produced by each division is a data element. <ref type="figure" target="#fig_7">Figure 3</ref>(a) (left) shows the key data elements and their compositions encoded by <ref type="figure" target="#fig_2">Figure 2</ref>. Here three types of data relations appear in this composition: index/identify (one element is used to index or identify the other), and sum (one element is the summation of others).</p><p>Our model uses 16 features to characterize a data element ( <ref type="table" target="#tab_3">Table 2)</ref>. Building on our previous model <ref type="bibr" target="#b22">[23]</ref>, we modify the definitions of two features, Type and Form, to describe the make of a data element more accurately. For example, we can now easily distinguish between a set of car prices (Type: entity; Form: array) and a complex patient record (Type: compound; Form: singleton). It is however impossible for us to differentiate such data sets previously.</p><p>In a visualization process, a data element may directly come from a database or may be derived from other data elements. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, for example, the total revenue per year may be computed from the division revenues stored in a database. We add a new feature Source that uses a binary value (original vs. derived) to indicate whether a data element is originally in the database or is derived from others. This feature benefits us from the following two aspects.</p><p>First, Source enables us to define different data similarity metrics (e.g., comparing only data with the same Source values). Second, it helps us learn the data needed to complete a graphic design. Usually a user request provides only the data to be visualized but not the data for generating visual references, such as coordinate axes and legends. To create those visual references, our system must derive the needed data. Suppose that we need to design a graph like <ref type="figure" target="#fig_3">Figure 1(a)</ref>. To create the coordinate axes, our system must derive the ranges of the longitudes and latitudes from the original location data. Such a data derivation can be learned based on the Source feature specified in the examples.</p><p>Similar to our modification made to the visual Structure, we also extend our data Structure to capture complex data compositions. Using a tree structure, for example, our previous model simply cannot express the data relations among elements D3, D4, and D6 shown in <ref type="figure" target="#fig_7">Figure 3</ref>(a), since both D3 and D4 are the parents of D6.</p><p>In summary, we use a total of 23 features to describe the characteristics of data and visual elements. By no means this feature set is complete for capturing all properties of a visualization. Nevertheless, we find that the current feature set is adequate for us to capture various subtle semantic and syntactic differences between examples. In addition, our feature-based scheme is flexible, and we can easily add/ remove/modify a feature to suit different analysis needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-based Indexing</head><p>Using our feature sets defined above, we can now describe the data and visual characteristics of a graphic example. To efficiently access/retrieve relevant examples, we must index our cases by its unique features or a unique structure of the features <ref type="bibr" target="#b6">[7]</ref>. Due to the complex make of a graphic, it is difficult to find a single feature or a subset of features to uniquely identify a case. Resorting to the most intrinsic property of a visualization, we decide to use a data-visual mapping structure to index our examples, since each example expresses a unique data-visual mapping <ref type="bibr" target="#b20">[21]</ref>.</p><p>To capture such a mapping, we link a visual element to its corresponding data element and vice versa <ref type="figure" target="#fig_4">(Figure 3a)</ref>. For the sake of simplicity, our previous model assumed that   such a data-visual binding is always one-to-one. However, the simplified modeling is incapable of describing a complex example. Thus our current model supports the most general form of m-to-n data-visual mappings. For example in <ref type="figure" target="#fig_7">Figure 3</ref>(a), data element D6 (a division's revenue amount) is mapped to two visual elements, V6 (the text used to label the amount) and V8 (the length of the bar).</p><p>The m-to-n data-visual mappings imply that within one example, data elements and visual elements could form different hierarchical structures of their own. For example, <ref type="figure" target="#fig_4">Figure 3(a)</ref> shows the two different data and visual hierarchies encoded by <ref type="figure" target="#fig_2">Figure 2</ref>. To create a unique structural index, we must combine the two hierarchies while preserving all the features. To handle this problem, we introduce a two-way graph-based indexing mechanism: one by data and the other by visual.</p><p>In particular, a PDGraph links together mapping pairs (e.g., <ref type="bibr">D0</ref> and V0 in <ref type="figure" target="#fig_4">Figure 3a</ref>) by the structure of the data elements (e.g., D0). <ref type="figure" target="#fig_7">Figure 3(b)</ref> shows a PDGraph, built on the data composition structure in <ref type="figure" target="#fig_4">Figure 3(a)</ref>. Likewise, a PVGraph organizes data-visual mapping pairs following the structure of the visual elements. <ref type="figure" target="#fig_7">Figure 3</ref>(c) depicts a PVGraph, formed based on the visual structure in <ref type="figure" target="#fig_7">Figure  3</ref>(a). Here symbol "/" represents a non-existing element. Note that a mapping pair may contain more than one visual or data elements (e.g., the shaded pair in <ref type="figure" target="#fig_7">Figure 3b</ref>), when the data-visual mapping is not one-to-one. Using PVGraph and PDGraph together, we can now uniquely index all examples in the database. Consequently, we can access and manage these graphic examples by their PDGraphs and PVGraphs. For example, when retrieving an example according to a user request, we compare their PDGraphs and PVGraphs (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XML-based Annotation Schema</head><p>To encode all the features for an example, we develop an XML annotation schema <ref type="bibr" target="#b16">[17]</ref>. Using this schema, we create an XML document for each collected example, which now has two parts: an annotation in the form of an XML document, and the image itself in JPEG format. <ref type="figure" target="#fig_8">Figure 4</ref> presents a fragment of the XML annotation created for <ref type="figure" target="#fig_2">Figure 2</ref>. As shown here, our XML annotation defines a visual root (the top-level visual element), a data root (the top-level data element), a set of visual elements, and a set of data elements. Each visual/data element (including the root) is uniformly described using our visual/ data features defined in <ref type="table" target="#tab_1">Table 1 and 2.</ref> Currently, each XML document is created by hand, since existing image understanding technology cannot automatically extract key characteristics that we need (e.g., data encoded) except certain low-level visual features (e.g., color or texture histograms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Use of Visual Database</head><p>Using our visual database as a knowledge base, our generation system can create new graphics by learning from existing examples. In addition to graphics generation, we have also explored several other uses of the database. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path</head><p>The data application/domain. It is a path in our application/domain ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path</head><p>The semantic category to which a data element belongs. It is a path in our semantic ontology.</p><p>Type*</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>String</head><p>One of the 4 meta data types: {entity, relation, ellipses, compound}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta</head><p>Form*</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>String</head><p>One of the 2 meta data formations: {singleton, array}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>String</head><p>One of the 4 ways a data element is ordered and measured: {nominal, ordinal, interval, ratio}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>String</head><p>The unit of measure used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>String</head><p>Whether data are continuous or discrete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resolution Integer</head><p>The number of distinct values of a data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Volume</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integer</head><p>The total number of children in a data element Cardinality Integer</p><p>The number of elements in a data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integer</head><p>The number of elements in a data relation.</p><p>Source*</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>String</head><p>Whether the data is originally in the database or derived from others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pres. related</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>String</head><p>One of 10 presentation-related roles that a data element plays in a data composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path</head><p>The presentation goal of visualizing a data element. It is a path in our user task ontology.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Example-based graphics generation</head><p>Our example-based graphics generation consists of two main phases: graphic sketch generation and sketch realization. Here a sketch is an intermediate representation that outlines the basic structure and elements but omits the lowlevel visual details. For example, a sketch of a bar chart specifies the number of axes and bar elements to be created but not the specifics of the axes and bars, such as their exact scales and positions. Since describing the details of the entire generation process is out of the scope of this paper, here we highlight the key ideas.</p><p>The input to our system is a user request, represented by an XML document similar to the XML annotations made for our examples. However, a user request is often partially specified, since users may not know every aspect of the data or the desired visual encodings. As a preprocessing, our system parses each XML document (including the request and examples in the database) and automatically builds the corresponding PDGraphs and PVGraphs. <ref type="figure">Figure 5</ref> outlines our three-step sketch generation algorithm. Given a user request, our algorithm first retrieves the top-K (e.g., K = 3) matched examples based on the distances between the request and existing graphics (lines 1-7). It then evaluates whether top-matched examples are adequate for creating a new graphic (lines 8-12). If the evaluation fails, the current request is decomposed into a set of request fragments (line 15), and the sketch generation is recursively called to create sketches for the fragments (lines 17-20). Next we briefly discuss the three main routines involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sketch Generation</head><p>Distance. Our distance function calculates the distance between a user request and an example by computing a weighted sum of distances of their corresponding PDGraphs and PVGraphs. The distance between two graphs is a weighted sum of distances of their elements. Finally, the distance between two data/visual elements equals to a weighted sum of the distances of their features using our quantitative similarity model defined in <ref type="bibr" target="#b22">[23]</ref>.</p><p>Evaluate. Our evaluation function uses two heuristics to determine whether we can use a matched example to synthesize a new graphic. 1) The matching score (similarity distance) for every data element of the request should be below a certain threshold. 2) Every primitive data element of the request acquires a visual mapping. Note that we do not require to find direct mappings for intermediate data elements, since their mappings can be composed from those of their children.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compose.</head><p>A sketch can be directly constructed based on a matched example (line 13), or it can be composed from a set of sketches (line 21). Since a sketch may be made up of sketches created from different examples, our system needs to ensure that such a composition is valid. Currently we compare the proposed composition with existing visual compositions in the examples. If a similar composition could not be found, our composition function returns null, and the overall sketch generation fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sketch Realization</head><p>At this stage, our system transforms the sketch into the actual graphic on the screen. To fill in all visual details (e.g., the exact scale of a visual element), our system may learn different settings and constraints from the Parameter features of the matched elements. We then determine the final layout using a numerical constraint solver <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implications to Information Visualization</head><p>Although our original motivation of building a visual database is to support example-based graphics generation, our further exploration shows that our database also brings important implications to information visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark</head><p>Offering a diverse collection of graphic examples, our database fills the long-standing need for a benchmark to test and compare different visual techniques. Like other benchmarks (e.g., document or video databases used by TREC (http://trec.nist.gov)), our database can be used in two ways: graphics retrieval and graphics organization.</p><p>Graphics retrieval. Given a new visualization, graphics retrieval attempts to find all similar visualizations in the database. Researchers can use our database as a testbed to experiment different retrieval algorithms using different quantitative similarity metrics. In particular, we have investigated using different similarity metrics (e.g., by assigning different feature weights) for graphics retrieval. This pro- cess is very similar to the example retrieval step described in <ref type="figure">Figure 5</ref> (lines 1-7) except that now a user request is a fully specified graphic example. Graphics retrieval is especially useful when evaluating a new visualization, since we can accurately assess how this new visualization is related to existing visualizations (e.g., how new it is) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Assume that <ref type="figure" target="#fig_2">Figure 2</ref> is a new example. We would like to learn to which examples in the database it is closely related. As a result, our algorithm retrieves examples 2 and 1 in <ref type="figure" target="#fig_10">Figure 6</ref> as the top two matched examples, with the respective distances of 0.2647 and 0.2891. By examining the matching results, we can explain that <ref type="figure" target="#fig_2">Figure 2</ref> is similar to <ref type="figure" target="#fig_10">Figure 6</ref>(2) because both are variations of bar charts. Despite their visual differences, <ref type="figure" target="#fig_2">Figure 2</ref> is also close to <ref type="figure" target="#fig_10">Figure 6</ref>(1) since both depict a whole-part data relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graphics organization.</head><p>As the number of examples grows, a certain type of organization is necessary for us to easily identify the overall trend among all examples and the local patterns within a subset. Moreover, when a new example is encountered, we would like to classify it to dynamically update the existing organization. Graphics organization studies the general structure and patterns of graphic examples in a database. From this standpoint, our database also provides researchers a testbed to apply different pattern-finding methods, including classification and clustering, to study the organization of the examples.</p><p>In one of our experiments, we have used a hierarchical clustering algorithm <ref type="bibr" target="#b2">[3]</ref> to arrange 7 examples ( <ref type="figure" target="#fig_10">Figure 6</ref>) based on their pair-wise distances computed by our similarity model. By setting a distance threshold, our algorithm produces several clusters. For example, if the threshold is 0.2, we obtain 3 clusters <ref type="figure" target="#fig_4">(Figure 6a</ref>): (6, 7), (1), and <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4)</ref>. Here, example 1 is collected from a newspaper to depict the sales made by a business. Examples 2 and 5 are from <ref type="bibr" target="#b13">[14]</ref> to illustrate the relations of various car attributes (e.g., prices and mileages). Examples 3 and 4 are generated by APT <ref type="bibr" target="#b10">[11]</ref> and SAGE <ref type="bibr" target="#b14">[15]</ref>, respectively. The last two examples 6 and 7 are created by a design professional for a real-estate application. Using the same clustering algorithm, we can also classify <ref type="figure" target="#fig_2">Figure 2</ref>, which belongs to the same cluster where examples 1-5 reside <ref type="figure" target="#fig_11">(Figure 7)</ref>.</p><p>Properly organizing examples not only helps us understand the intrinsic relations among different examples, but also facilitates graphics generation. For example, our example retrieval process ( <ref type="figure">Figure 5</ref>) may use the organization (clusters) of the examples to optimize its search, i.e., first checking all the examples within the same cluster of a promising example that has been identified.</p><p>In both graphics retrieval and graphics organization, our fine-grained model of graphic examples plays a critical role. Unlike other work on visual retrieval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, which uses limited semantic information, our model allows researchers to define more accurate similarity metrics by taking into account both syntactic and semantic properties of graphics. An accurate similarity metric in turn helps produce better retrieval results and visual analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>new</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Visual Design Principles</head><p>Besides using our database as a benchmark for visual analysis, we could also use our database as a knowledge repertoire to learn different visual design principles. Recall the process of composing a new sketch. To verify whether a new visual composition is valid, we can take another approach. From our examples, we may use decision-treebased machine learning technique <ref type="bibr" target="#b12">[13]</ref> to induce a set of classification rules for visual composition. We can then use the derived rules to cross-validate whether the proposed new composition is appropriate.</p><p>Likewise, we can use machine learning methods to study or generalize a number of visual design principles or techniques from our examples. For example, we may study the correlations between specific domain data and corresponding visual techniques used, or discover the rules of using a particular visual technique (e.g., <ref type="bibr">Highlighting)</ref> with specific parameter settings (e.g., style of Highlighting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions And Future Work</head><p>We build a database of graphic examples to support an example-based graphics generation system, which aims to create new visualizations directly from existing visualizations (examples). In the process of constructing our database, we have addressed two issues involved: example selection and example modeling. First, we have defined a set of example selection criteria to assure the coverage and quality of our examples. Second, we have presented a feature-based annotation model to describe each graphic comprehensively and accurately. Furthermore, we have discussed the important implications that our database may bring to the area of information visualization.</p><p>As we continue collecting graphic examples to enrich our database, we are developing a user interface to ease the example annotation. To annotate each example, currently we use XmlSpy IDE (http://www.xmlspy.com) to create an XML document by hand. Our new interface should be able to take a graph structure directly as an input (e.g, the data or visual hierarchy shown in <ref type="figure" target="#fig_4">Figure 3a)</ref>, and then automatically generates an XML document. To further simplify the laborious hand annotation process, we are also exploring how to automatically extract salient features using advanced computer vision and graphics techniques (e.g., extracting 3D shape features).</p><p>So far we have only collected positive (good) examples in our database, we would also like to augment the database by adding negative examples to help the example adaptation process. In other words, we can better judge whether a proposed adaptation (e.g., a new visual composition) is acceptable using both positive and negative examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) is suitable, but Figure 1(b) is not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) and Figure 2 differ greatly in their data and information-seeking goals. Specifically, Figure 1(a) illustrates the Napoleon's long march, including the army's locations (longitudes and latitudes) and sizes, to help users learn the overall trend of the event.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 depicts</head><label>2</label><figDesc>IBM revenues before and after Gerstner took office to help users draw a comparison. Visual coverage. Visual coverage evaluates the number of (a) Napoleon's long march (b) A teapot image (c) Statistics Map (d) Hockey penalty rules</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 .</head><label>1</label><figDesc>Graphics samples: (a) qualified example (b-d) unqualified examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )</head><label>a</label><figDesc>Generated by Sage, reprinted by permission from S. Roth. (b) Copyright 1993 IEEE, reprinted by permission [6]. (c) reprinted by permission [5]. (d) Copyright 1998 TUBE Graphics, reprinted by permission [18].different visual forms and styles exhibited by our examples. In particular, the variety of visual forms measures the number of high-level visual organization formalisms. For example,Figure 1(a)andFigure 2present two different visual forms: a line graph and a bar chart. The variety of visual styles, on the other hand, counts the number of different visual techniques (e.g., highlighting an important data by changing its color vs. by drawing a bounding box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Comparing IBM revenues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )</head><label>a</label><figDesc>Data-visual mappings (b) PDGraph (c) PVGraph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Data-visual mappings, PDGraph, and PVGraph for the graphic shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>An XML annotation fragment for Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Sketchif success break endfor 3 .Figure 5 .</head><label>35</label><figDesc>GenerateSketch (Request req, DB db, integer k) Examples candidateSet ← empty set for each example e ∈ db do d ← distance (req, e) insert e to candidateSet in ascending order of d if sizeOf (candidateSet) &gt; k then remove the last element from candidateSet endif endfor 2. Example evaluation boolean success ← false for each candidate c ∈ candidateSet do success ← evaluate (req, c) Query decomposition and sketch composition if success then sketch ← compose (req, e) else Requests requestSet ← decompose (req) Sketches sketchSet ← empty set for each sub-request p ∈ requestSet do newSketch ← GenerateSketch (p, db, k) add newSketch to sketchSet endfor sketch ← compose (req, sketchSet) endif return sketch Outline of example-based sketch generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Existing examples (1-7) and their computed similarity clusters (a).(2) &amp; (5) Copyright 1994 ACM, reprinted by permission from B. Myers [14]. (3) Copyright 1986 ACM, reprinted by permission from J. Mackinlay [11]. (4) Copyright 1997 AAAI, reprinted by permission from S. Roth [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Clusters after adding a new example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 . Visual features (* new or modified). Feature Value Definition Syntactic</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Category</cell><cell>Path</cell><cell>Syntactic category of a visual element, a path in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>our visual hierarchy.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Technique* String</cell><cell>One of the 21 visual techniques for visualizing</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>data.</cell></row><row><cell>'01</cell><cell>35.0</cell><cell>33.4</cell><cell>13.0</cell><cell>$4.6</cell><cell>$85.9</cell><cell cols="2">Parameter* Pair Dimension* Integer</cell><cell>Parameters of visual techniques, in the form of attribute-value pairs. Dimension of Euclidean space (1, 2, or 3),</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>where the visual element is rendered.</cell></row><row><cell cols="2">SERVICES</cell><cell>HARDWARE</cell><cell>SOFTWARE OTHER</cell><cell cols="2">Total Revenue In billions</cell><cell></cell><cell></cell></row><row><cell></cell><cell>15.0</cell><cell>33.8</cell><cell>11.1 $4.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>'92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>$64.5</cell><cell></cell><cell></cell></row></table><note>pr</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Data features (* new or modified).</head><label>2</label><figDesc></figDesc><table /><note>Feature Value Definition (see attached for a full definition)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here we stress the difficulty of expressing graphic compositions as a set of symbols/numbers for the purpose of machine learning.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Joonhwan Lee from CMU for designing examples 6 and 7 in <ref type="figure">Figure 6</ref>. We also thank all parties noted in the paper for letting us use their figures.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using semantic contents and wordnet in image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aslandogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Their</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rishe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR &apos;97</title>
		<meeting>ACM SIGIR &apos;97</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sketching, searching, and customizing visualizations: A content-based approach to design retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chuah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kerpedjiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Multimedia Information Retrieval</title>
		<editor>M. Maybury</editor>
		<imprint>
			<publisher>AAAI Press/The MIT Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="83" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern Classification and Scene Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Differential Approach to Graphical Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="15213" to="3891" />
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Information Graphics: A Comprehensive Illustrated Reference. Management Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Visual Cues: Practical Data Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Keller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>IEEE Computer Society Press and IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Case-based Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolodner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On empirically validating functions of pictures in prose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Carney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Psychology of Illustration: Basic Research</title>
		<editor>D. Willows and H. Houghton</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1987" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="51" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A classification of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lohse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Biolsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rueter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="36" to="49" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A texture thesaurus for browsing large aerial photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="633" to="648" />
			<date type="published" when="1998-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automating the design of graphical presentations of relational information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="141" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A formal specification scheme for network diagrams that facilitates automated design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="414" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Creating charts by demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;94</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="106" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automating the presentation of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mattis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on AI Applications</title>
		<meeting>IEEE Conf. on AI Applications</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Support vector machine active learning for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM &apos;01</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<ptr target="www.w3.org/TR/2001.XMLSchema" />
	</analytic>
	<monogr>
		<title level="j">W3C</title>
		<imprint>
			<date type="published" when="2001-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Information Graphics: Innovative Solutions in Contemporary Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wildbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Thames and Hudson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Information Architects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wurman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Graphics Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual planning: A practical approach to automated visual presentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI &apos;99</title>
		<meeting>IJCAI &apos;99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="634" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data characterization for automatically visualizing heterogeneous information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE InfoVis &apos;96</title>
		<meeting>IEEE InfoVis &apos;96</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual task characterization for automated visual discourse synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CHI &apos;98</title>
		<meeting>ACM CHI &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Representing and retrieving visual presentations for example-based graphics generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Intl. Syp. on Smart Graphics</title>
		<meeting>1st Intl. Syp. on Smart Graphics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
