<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Evaluation for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yijie Hou</orgName>
								<address>
									<addrLine>Chengshun Wang, Junhong Wang, Xiangyang Xue, Jun Zhu, Dongliang Wang</addrLine>
									<settlement>Xiaolong (Luke) Zhang, Siming Chen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Evaluation for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DBF36FCEC5C105BE0112B249CCB3DEB7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1: System User Interface: (a) Spatial-temporal view to display a simulation scenario of autonomous driving; (b) Radar view to present the evaluation scores of five modules of autonomous driving and an overall evaluation score at a given time; (c) Timeline view to show the scores over time; (d) Parallel coordinates view to display both the score (d-1) and factor value (d-3) of each factor from five modules, with customizable ranking settings for different factor priorities (d-2); (e) Visualization of the autonomous driving states on speed, acceleration, wheel turning angle of autonomous driving vehicles (e-1), as well as the type and priority distributions of obstacles in the scenario (e-2).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>• Y. Hou, C. Wang, J. <ref type="bibr">Wang</ref> Autonomous driving technologies have advanced rapidly in recent years. More and more car manufacturers consider the installation of autonomous driving systems in their cars, in particular those electronic models. By letting people stay away from the steering wheel, autonomous driving can reduce the stress associated with driving. However, delegating decision-making in driving to algorithm-driven automatic systems also raises many concerns, such as driving safety and algorithm reliability <ref type="bibr" target="#b0">[1]</ref>. Autonomous driving model developers and researchers have investigated methods to evaluate autonomous driving technologies to identify design deficiencies and then fix them. Recently, various evaluation criteria for autonomous driving have been proposed. For example, the Society of Automotive Engineers (SAE) released a classification to categorize autonomous driving technology into six levels, from L0 to L5 <ref type="bibr" target="#b1">[2]</ref>. China also published a white paper with its own classification, from T1 to T5 <ref type="bibr" target="#b2">[3]</ref>. These classifications grade autonomous driving vehicles based on their performances in multiple driving tasks in various scenarios. Although these qualitative evaluation criteria provide a clear understanding of what an autonomous driving system can achieve, they cannot tell what factors may contribute to success or failure in a test. For example, when an evaluation method can downgrade a system due to its failure in a test of turning at an intersection (e.g., failing to yield to a vehicle or a pedestrian), recognizing the causes of the failure requires more advanced techniques to understand the interior of the system. Some research has explored quantitative evaluation methods to reveal more details of autonomous driving systems. However, some of these methods evaluate autonomous driving from the perspectives of task complexity and environmental complexity without considering objective metrics, such as starting acceleration and following distance <ref type="bibr" target="#b3">[4]</ref>, while some rely solely on driving experience or conventional autonomous driving scenarios for evaluation model selection without the integration of official evaluation guidance or important criteria provided by autonomous driving module developers <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Many large technology companies have developed autonomous driving visualization tools, such as Apollo Dreamview <ref type="bibr" target="#b7">[8]</ref>, etc. However, they merely visualize data from autonomous driving modules, not evaluate them.</p><p>We propose a visual analytics approach for the evaluation of autonomous driving systems. Our goal is to let module developers understand the performance of the autonomous driving system. We aim to provide module developers with tools to investigate and explain various evaluation scores of an autonomous driving system through an intuitive and interactive interface. We implement a visual analytics system to support the user-driven computation, exploration, and evaluation of the performance score for each factor in each module of the whole autonomous driving process. Through various visualization designs, module developers can examine the overall performance of the system, the performance of each involved module, and the impacts of individual data factors. Our research contribution is as follows:</p><p>• We develop a visual evaluation workflow from overview to details that helps autonomous driving module developers explore the performance of each module and related contributing factors. • We build a visual evaluation system to support visual and interactive evaluation of the whole process of autonomous driving. • We develop application scenarios for autonomous driving evaluation with cases evaluated by the domain experts.</p><p>The paper is structured as follows. Sect. 2 reviews related work. Then, we briefly describe the workflow of autonomous driving and derive the evaluation requirement in Sect. 3. Our visual evaluation method is presented in Sect. 4. To demonstrate the capability of our approach, we introduce the applications of our method in the simulation driving datasets in Sect. 5 and our evaluation work with domain experts in Sect. 6. Finally, we discuss the limitations and future work in Sect. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Autonomous Driving and Simulation</head><p>The success of Boss, the champion vehicle the DARPA Urban Challenge <ref type="bibr" target="#b8">[9]</ref>, has ignited autonomous driving and also popularized the software structure of its autonomous driving system, which include such major subsystems as perception, motion planning, and decisionmaking. Nowadays, various types of autonomous driving systems have been developed. A recent comprehensive summary on the development of autonomous driving by Yurtsever et al. <ref type="bibr" target="#b9">[10]</ref> discusses the challenges in autonomous driving and presents available databases and tools during the development of the autonomous driving system. In addition to technical challenges, there are also many social concerns in autonomous driving that have been widely discussed. Litman <ref type="bibr" target="#b10">[11]</ref> explores the benefits and costs of autonomous driving as well as its development in transport planning, transport safety, energy-saving, and emission reduction, and other social issues in the future. An autonomous driving system must go through rigorous tests before being put on the road. Considering the potential high costs of testing on public roads, research on simulation environments before road tests has become an important topic. Many corporations or organizations provide open-source autonomous driving frameworks such as Autoware <ref type="bibr" target="#b11">[12]</ref> and Apollo <ref type="bibr" target="#b12">[13]</ref> and continuously develop and improve autonomous driving simulation tools. Various simulation tools have been developed. CARLA <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, an open-source autonomous driving simulation tool, supports training, prototype design, perception, control, and other autonomous driving model verification, and provides available signals for driving strategy training and a variety of environmental conditions specified. LGSVL <ref type="bibr" target="#b15">[16]</ref>, a high fidelity simulation tool for the autonomous driving, supports customizable simulations by allowing the creation of new controllable objects and the replacement of simulation modules. In addition, corporations are also active in developing their own testing methods. For example, Baidu has proposed its simulation test platform, Dreamland, within its autonomous driving platform Apollo <ref type="bibr" target="#b16">[17]</ref>. Dreamland provides various simulation scenarios of autonomous driving performances (e.g., collision detection).</p><p>These tools and platforms largely focus on data generation with a visible scene and the presentation of evaluation results, and do not support user-driven in-depth evaluation of an autonomous driving systems. However, they can be used as the foundations for interactive visual analytics of the performances of the system and individual modules. The data generated by LGSVL can be used to gain the insight into the behaviors of autonomous driving system. Apollo Dreamview <ref type="bibr" target="#b7">[8]</ref>, as a visualization platform for autonomous driving data, can be used to develop our visual analytics evaluation method of the performance scores and contributing factors. To our best knowledge, our method is the first of its kind to use a visual evaluation workflow to analyze the key components of a whole autonomous driving process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation methods for Autonomous Driving</head><p>Currently, the evaluation of autonomous driving is mainly divided into qualitative evaluation and quantitative evaluation. Qualitative evaluation methods can directly show the quality of autonomous driving technology but face challenges in explaining the rationale and evaluation criteria of autonomous driving decisions. For example, while we can easily tell that an autonomous driving system fails to avoid a pedestrian in a test, finding the true cause of the failure (e.g., a high speed or an insufficient deceleration distance) is a non-trivial task.</p><p>Quantitative evaluation is important to the evaluation and analysis of autonomous driving but currently lacks unified standards. Researchers often use their own methods <ref type="bibr" target="#b17">[18]</ref>. Some have chosen objective indicators based on their experience and expert opinions. Even so, the parameters they choose are quite different. Meng et al. <ref type="bibr" target="#b4">[5]</ref> divide autonomous driving data into three parts, intersection behavior, objective avoiding behavior, and car-following behavior. They evaluate the autonomous system based on parameters generated from these three parts. Dong et al. <ref type="bibr" target="#b5">[6]</ref> select such parameters as driving time, detection of signs and lines, velocity variance in evaluation. Evaluation models include Grey Relational Analysis, AHP, TOPSIS, or just by subjective experience <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Different choices on parameters lead to different mathematical modeling methods (e.g., information-theory-based vs. entropy-based), so the models of these projects cannot be directly compared or transferred. Most of evaluation methods discussed above are mathematical and lack interactive or visualization support.</p><p>Visual analytics approaches are argued to be suitable for complex situation awareness <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> and for explanation of machine learning models <ref type="bibr" target="#b20">[21]</ref>. However, research on visual analytics in autonomous driving is rare. One exception is VATLD <ref type="bibr" target="#b21">[22]</ref>, but its focus is on a specific component of an autonomous driving system, traffic light detection, rather than the whole process. More research is needed in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">High-dimensional and Spatial-temporal Visualization</head><p>In autonomous driving evaluation, data of interest is usually highdimensional spatial-temporal information. Thus, we review relevant literature on the visualization of such data, which has been an important topic in visualization research <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>.</p><p>High-dimensional data visualization mainly include dimension reduction algorithm, subspace partition, interactive visualization customization, parallel coordinates, radar view, etc. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Dimension reduction (DR) methods are mainly divided into linear mapping and nonlinear mapping. The general DR methods are PCA, LDA <ref type="bibr" target="#b27">[28]</ref>, and the kernel-based methods KPCA, KFDA <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, and flow learning such as Isomap, LE, and LLE <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. However, because the DR method lacks explainability, we mainly adopt the visualization methods like parallel coordinates and radar view to visualize the highdimensional data in autonomous driving evaluation.</p><p>Autonomous driving is a good application scenario for spatialtemporal visual analytics. Regarding the visualization of spatialtemporal data, Andrienko et al. surveyed available software tools <ref type="bibr" target="#b32">[33]</ref>, and also proposed an approach to reveal patterns and trends of mass mobility through spatial and temporal abstraction of movement data <ref type="bibr" target="#b33">[34]</ref>. Ferreira et al. <ref type="bibr" target="#b34">[35]</ref> supported spatio-temporal queries for taxi data and enabled the examination of mobility across the city. Chen et al. <ref type="bibr" target="#b35">[36]</ref> proposed an visual analytical workflow using context information for real-world vehicle trajectory analysis to identify dangerous driving behaviors. In our work, the high-dimensional and spatial-temporal data comes from the various components of an autonomous driving system, and the characteristics and patterns of such system data differs from those of data generated from human behaviors seen in those systems mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>We collaborate with a group of experts from a car company who focus on the development of autonomous driving systems. We have in-depth collaboration in the requirement specification, visualization justification and expert evaluation. Our general goal is to support the visual evaluation of autonomous driving systems. In this section, we first summarize the components in an autonomous driving system and derive the suitable data for evaluation. Then, we describe the design requirements for a visualization-based evaluation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Autonomous Driving Framework</head><p>Generally speaking, an autonomous driving system include three major components: perception, decision-making, and control. The Perception system concerns the use of various sensors that collects basic data on a vehicle and its surrounding environment. Currently, involved sensors often include GPS, IMU, ultrasonic radar, millimeter-wave radar, and cameras. Information collected from these sensors is diverse and can include essential information about the vehicle (e.g., its position, orientation, and driving mode), signal information (e.g., traffic lights), surrounding obstacles (e.g., obstacle category, position), etc.</p><p>The results of the perception component are the inputs to the decision-making component. Combined with non-environmental factors such as traffic rules and driver experience, the decision-making component predicts the changes of the driving environment and completes the judgment on the behaviors of the traffic participants and the computation of their potential travel trajectories.</p><p>In the autonomous driving mode, the results of the decision-making component are used to control the actions of the vehicle. First, a travel path is calculated based on the results, and then a corresponding driving action plan that includes a series of instructions is produced. To execute the action plan, the autonomous driving system transmits the instructions to the vehicle body to control various systems of the vehicle, including braking, steering, engine, and signals. During this process, the autonomous driving system continuously monitors the status of the vehicle and the surrounding environment, and if necessary, adjusts the action plan.</p><p>Based on the above autonomous driving process and design requirements from module developers we collaborate with, we classify autonomous driving data into five modules: perception, planning, prediction, control, and comfort. Our design to support the evaluation of autonomous driving systems targets the evaluation of these modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data for Evaluation</head><p>Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification <ref type="bibr" target="#b2">[3]</ref> and the inputs from our collaborators, we divide our data into five modules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The measurement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving system needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, autonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual position, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table <ref type="table" target="#tab_1">1</ref> summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design Requirements</head><p>Our design requirements for the visual evaluation system are developed based on the real-world application needs from our industrial collaborators. Our visual evaluation system is designed to provide the module developers of autonomous driving systems with a perceptible and interactive evaluation system so that they can better understand and optimize the overall performance of an autonomous driving system. More specifically, the requirements can be summarized as the following:</p><p>• R1: Module developers should be able to use the system to assess the overall performance of an autonomous driving system with a score as general feedback. • R2: The system should allow module developers to evaluate the performance of a system in different time periods of the whole driving process and to identify the time periods with bad performance. • R3: The system should provide methods for the evaluation of the five modules (perception, prediction, planning, control and comfort) so that module developers can tell the performances of individual modules. • R4: The system should allow module developers to observe the performances of individual module at any time period and to identify those factors that contribute to the observed performances. • R5: The system should allow module developers to customize the ranking of importance of each module based on their different goals and contexts in analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUAL EVALUATION APPROACH</head><p>This section describes our method. Observing autonomous driving data, we can determine the factors for autonomous driving evaluation based on the previous evaluation criteria and driving experience. With the help of AHP <ref type="bibr" target="#b36">[37]</ref> and TOPSIS <ref type="bibr" target="#b37">[38]</ref>, two mathematical evaluation models, we evaluate and analyze each factor, and the evaluation result is obtained by weighted calculation. Finally, we introduce the visual interface design. The difference between positions information planned and actual positions information at the corresponding time point</p><p>The smaller the absolute value, the better; the ideal value is 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Velocity Error</head><p>The difference between velocity information planned and actual velocity information at the corresponding time point The smaller the absolute value, the better; the ideal value is 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perception Object Distance</head><p>The shortest distance between autonomous driving vehicle and obstacles</p><p>Combined with vehicle speed, the larger the value taken, the better. If speed is less than 10m/s, the optimal value is greater than 20; if speed is greater than 10m/s. the optimal value is greater than speed-10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Probability</head><p>The accuracy between the predicted and the actual trajectory of obstacles The bigger the value, the better; the optimal value is 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mathematical Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Analytic Hierarchy Process (AHP)</head><p>The basic idea of AHP is to stratify the evaluation decision problem.</p><p>According to the design requirements and description, the problem is decomposed into different component factors. Based on the comparison of these factors and their affiliations, the factors are cohesively combined at different levels to form a multi-level analysis structure model. Finally, the objectives in the problem are compared and ranked according to their performance in the model. AHP uses the consistent matrix method to construct the judgment matrix. Let r i j denote the importance of factor i relative to j, which ranges from 1 (factor i is as important as j) to 9 (factor i is much more important than j); then the importance of factor j relative to i is r ji = 1 r i j . Fill the scale value r i j into the matrix R to get the judgment matrix. When the judgment matrix R satisfies the consistency test, the eigenvector corresponding to its maximum eigenvalue</p><formula xml:id="formula_0">λ max is ω = (ω 1 , ω 2 , •••, ω n ) T .</formula><p>According to the properties of consistency matrix, we have r i j = w i w j . Based on the construction of the judgment matrix, ω i and ω j can be taken as the absolute importance of factor i and factor j. Finally, the weights of the factors are obtained by normalizing ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS)</head><p>The basic principle of the TOPSIS method is to rank the evaluation objects from the positive ideal solution to the negative ideal solution by calculating the distances between them. If the evaluation object is close to the positive ideal solution and far away from the negative ideal solution, it is optimal. The factor values of the positive ideal solution and the negative ideal solution represent the positive ideal value and the negative ideal value of each evaluation factor, respectively.</p><p>In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined evaluation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given evaluation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation results for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP.</p><p>Assuming that there are n factors for evaluation, the matrix A m×n = {a i j } represents the autonomous driving data for n factors at m time points, and the evaluation criterion is r = (r 1 , r 2 , •••, r n ). Based on TOPSIS, the positive ideal solution A + and the negative ideal solution A − can be determined. The criteria for determining the positive and the negative ideal solutions vary by factors, as shown in Table1. For each object, its distances to positive ideal solution D + and negative ideal solution D − are calculated separately. The relative closeness can be calculated by B = D − D + +D − . The higher the relative closeness is, the better the evaluation result will be. The overall evaluation result matrix S = (B 1 , B 2 , •••, B n ) can be obtained. We can then calculate the final weighted evaluation matrix Z = Sdiag(ω) with normalized evaluation weights ω. In our case, we set the importance of each module equal at the beginning. Module developers can determine the relative importance of factors and rank them to generate new criteria r * and judgment matrix R * . Repeating these steps leads to a new evaluation result Z * , which satisfies the user's evaluation needs.</p><p>Due to our computing design, there must be at least one negative ideal solution for each factor, i.e., each evaluation factor has at least one object that takes the value of 0 even though this evaluation factor may have an excellent overall evaluation result. This design amplifies the evaluation difference between values of the factor, which may lead to misjudgment in the evaluation. Therefore, we make adjustments in TOPSIS with the assumption that the difference between the positive ideal solution and the negative ideal solution of the evaluation factor is minimal. In such case, we can add a regulation factor γ so that the shortest distance between each object of the factor and the negative ideal solution is γ. This will produce a better evaluation result. We visualize the evaluation results in the visual interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual Interface</head><p>The visual interface, as shown in Fig. <ref type="figure">1</ref>, mainly includes the components for score and state visualization. The score visualization component contains four parts to show the total score (R1, Fig. <ref type="figure">1-b</ref>), overall scores over time (R2, Fig. <ref type="figure">1-c</ref>), scores for five modules at each time (R3, Fig. <ref type="figure">1-b</ref>. Left) and the score of factors for each module in an autonomous driving process (R4, Fig. <ref type="figure">1-d.1</ref>). The state visualization component mainly contains the spatial-temporal simulation scene of the whole process (Fig. <ref type="figure">1-a</ref>) to show the overview of the autonomous driving scene, which includes a map, lane lines, traffic signals, various obstacles, predicted paths of obstacles, and planned paths of autonomous vehicles. The state visualization also presents the states of Fig. <ref type="figure">2:</ref> The visual evaluation workflow. Our visualization system is mainly divided into into two parts, the score visualization part and the state visualization part. From overview to detail, autonomous driving module developers as users can investigate the total score, score distribution over time, module score at a specific time period and scores of the factor from the module. To understand why the score is good or not, users can interactively explore the autonomous driving state including the factor values and spatial-temporal scene.</p><p>the autonomous driving vehicle (Fig. <ref type="figure">1-e</ref>.1) and obstacles during the process (Fig. <ref type="figure">1</ref>-e.2), and the values of each factor involved (Fig. <ref type="figure" target="#fig_0">1-d.3</ref>). Module developers can explore the scores from overview to details and then identify the state of each module and the spatial-temporal scenario to better understand the performance (Fig. <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Score Visualization</head><p>With the input of the autonomous driving data record, we first calculate the scores for each factor based on our model described in Sect. 4.1. The total score, module score and factor score range from 0 to 1. The score of each module is averaged by the scores of all the factors from this module at each time period. The score of each time period is averaged by the scores of all the modules at this time period. The total score is calculated by the average of all the scores in every time step. Module developers first read the total score in Fig. <ref type="figure">1-b (R1</ref>) and examine the timeline visualization (R2, Fig. <ref type="figure">1-c</ref>) to check the trend of the scores along the time. They can identify the outlier moments with the low scores and check details in the radar view (Fig. <ref type="figure">1-b</ref>). We choose the radar chart because we have five modules which is a suitable number of axes for choosing the radar chart. Users can easily identify the good and bad performance scores in different modules.</p><p>From the radar view, the score distributions of five modules, including perception, planning, prediction, control, and comfort are visualized (R3). We can compare and analyze each module to observe the performance of different autonomous driving evaluation modules. The radar view shows the overall performance of the five modules at the initial moment and the module scores at specific moments. It can be dynamically updated along with the timeline. We can find the time period with poor or unbalanced evaluation scores of each module.</p><p>Each module contains several factors for evaluation. For example, the comfort module has Jerk, HeadingChange, and centrifugation (large centrifugal means that the car goes through a small radius curve at high speed and in this situation, the passengers are prone to dizziness and nausea.) of the vehicle. The specific evaluation modules and factors are detailed in Table <ref type="table" target="#tab_1">1</ref>.</p><p>There are four design alternatives for visualizing the scores and factor values of the five modules, including scatter plot matrix, radar chart, projection method, and parallel coordinates. Scatterplot matrix is not suitable because it is not a multivariate solution but a multiple bivariate solution. Radar chart has already been used in the module visualization (Fig. <ref type="figure">1-b</ref>) and may be cluttered with larger amount of factors while the projection methods like PCA, t-SNE lack interpretability. Parallel coordinates provide a good display of high-dimensional data and help users perceive both scores and values of all factors. The axes of the parallel coordinates are the corresponding factors of the five modules, arranged based on the order of these modules. This can help to alleviate the order issue in parallel coordinates and to better reflect data patterns. Moreover, the space availability in the user interface can accommodate a view of parallel coordinates, and our collaborators agree on it.</p><p>Therefore, a view of parallel coordinates (Fig. <ref type="figure">1-d</ref>) is used to show the situation of each factor involved in evaluation (R4). As shown in the figure, the top axis of parallel coordinates is the score axis, which shows the evaluation result of each time point. The polylines in parallel coordinates are color encoded by the total score at a specific moment. Under the score axis are factor axes, which indicate the evaluation result of each factor at each time point (Fig. <ref type="figure">1-d.1</ref>). With the help of parallel coordinates, users can easily see the connection of between overall scores and the values of individual factor, and observe the influence of the evaluation factors on the modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">State Visualization</head><p>The spatial-temporal view (Fig. <ref type="figure">1-a</ref>) shows the information of the environment and other traffic participants in an autonomous driving process. Its design goal is to provide the overall understanding of the spatial-temporal scenarios with animation. Users can examine the behaviors of the vehicle in this view when they try to identify the low scores at a specific time period. The green lines in the spatial-temporal view show the predicted movements of the obstacles. The thicker green line indicates the planned trajectories of ego-vehicle. The fences reflect planning decisions made by the planning module. Each type of decision is presented in different color. A red fence means that the detected obstacle stops moving forward while a purple fence means that the egovehicle is yielding. With the fence and predicted route visualization, users can easily understand the planned and perceived behaviors of the ego-vehicle and traffic participants. By dragging the progress bar in the spatial-temporal, user can review the state data and score data at any point of time.</p><p>To help users understand both the evaluation result (score) and relevant contributing factors, we use parallel coordinates to visualize the factor values (Fig1-d.3). Thus, it is consistent that users can investigate the score distribution and the factor value distribution in the same visual form. The top of the chart is still the score axis, and the factor axes below show the actual values of individual factor at each time point. Among them, Jerk, Headingchange, Centrifugal belong to the comfort module; Brake and Steering are about the control module; PoseError and VelocityError concern the planning module; ObjectDistance represents the perception module; and Probability is related to the prediction module. Thus, users not only understand how well a module performs, but also know how individual factors in each module contribute to it.</p><p>In addition, the state component visualizes the standard attributes of the autonomous vehicle and the obstacles during the autonomous driving process (Fig. <ref type="figure">1-e</ref>), including the speed, acceleration, and wheel steering of the autonomous vehicle. This can help users directly understand the autonomous driving process. The obstacle attribute view (Fig. <ref type="figure">1-e2</ref>) displays the information of the obstacles surrounding the vehicle during the autonomous driving process. It visualizes the type and priority strategy information of obstacles in the form of a stacked histogram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Interactive Visual Evaluation</head><p>After the system evaluation is completed, users can review an autonomous driving process by clicking a button to animate the process, or by dragging the timeline to watch the animation. Users can also observe the time view below, drag the timeline, and explore the factor distribution of the five modules. After that, users can select and observe the factors in the view of parallel coordinates, find the factors with low scores, and examine the characteristics of the assessed factor values to understand the causes of failure. Finally, users can do interactive filtering and exploration on the parallel coordinates view, swipe a segment of the evaluation factors of interest to take values, and highlight the distribution of each evaluation factor at the corresponding time. Fig. <ref type="figure" target="#fig_0">3</ref> shows a scenario in which a user brushes the objects with scores less than 0.8 on the score axis of the factor score chart, view the evaluation results of the factors that influence such scores, switch to a view to display the same objects on the factor value chart, and examine the values of the factors that influence the evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Interactively Customized Ranking</head><p>The weights of all evaluation factors are equally distributed in the initial state. Because module developers with different background and interest may view different factors with different priorities. Users </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation</head><p>We use LGSVL Simulator <ref type="bibr" target="#b15">[16]</ref> and Apollo <ref type="bibr" target="#b12">[13]</ref> to implement our system. LGSVL Simulator is an end-to-end autonomous vehicle simulator that can be integrated with autonomous driving software and are compatible with various autonomous driving platforms. Apollo, an open-source autonomous driving platform that incorporates such modules as location, perception, control, prediction, and planning, has a component, Dreamview, to support the visualization of data from each module.</p><p>We use the Dreamview in Apollo as the basic framework to build our visual analysis system. Through the in-depth exploration of each module, we have refined a new set of visual evaluation methods. The visual evaluation system creates a perceptible environment for model developers through a series of real-time line graphs, radar view, stacked bar graphs, and parallel coordinates. It helps module developers better understand the performance of each module as the car moves. The system also supports user-driven definition of the importance of the evaluation index with interactive tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY</head><p>We evaluate our system with three case studies, including an overall evaluation process on a car accident, a case to verify the specific autonomous driving module evaluation, and a case on the customized ranking in evaluation. All data involved is generated by LGSVL simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual Evaluation for A Car Accident</head><p>This case is about a collision accident at an intersection without traffic lights where both vehicles make a left turn. This case used the Bor-regasAve map, with cars and pedestrians randomly generated by the simulator, to test the autonomous driving algorithms of Apollo. The collision, a severe accident, should lead to a low overall score at the time when the accident happens, which serves as a warning. We study whether the system would score the relevant modules reasonably because of the accident. We also evaluate the hundred-millisecond-level planning and prediction capabilities, which are essential metrics for assessing the capabilities of autonomous driving algorithms.</p><p>First, we get the total score and its distribution from the timeline view. As shown in Fig. <ref type="figure" target="#fig_2">5</ref>, the total score is 89.06%, with a low score of 45% near the time 16:49:57.871. In the radar view, the scores of some modules are also very lower during this time period. Second, we continue to explore parallel coordinates and examine several factors with low scores, such as HeadingChange, Centrifugal, Steering, PoseError, VelocityError, and ObjectDistance. Third in the exploration, we replay the scene around the accident time and find that the vehicle under autonomous driving was turning left and colliding with another car on the right in the spatial temporal view. The distance between two vehicles was 0, so the perception module scores 0. During the collision, the vehicle was impacted and deflected to the extent that HeadingChange, Steering, PoseError, and other factors scores were affected. Apparently, the system correctly identifies the collision and gives a low score for driving status at the time when the accident happened. Further exploring the visualized data, we can find some flaws in the autonomous driving behaviors. We explore the data around the time 16:49:57 (Fig. <ref type="figure" target="#fig_4">6-a</ref>) to observe the scores of individual factors and behaviors of the vehicle. At 16:49:56.274, the Apollo autonomous driving algorithm had not yet predicted that the other vehicle was about to make a left turn. At 16:49:56.372, a purple decision fence appeared in front of the ego-vehicle, indicating a decision by the planning module to slow down the vehicle and to make space for the other vehicle on the right. However, after 0.4s at 16:49:56.773, the planning module changed its plan and decided to continue the movement. At 16:49:56.976, the autonomous driving system stopped the vehicle. From the radar view and parallel coordinates during this period (Fig. <ref type="figure" target="#fig_4">6-b</ref>), we can see that the planning module and the prediction module have low scores. As shown in Fig. <ref type="figure" target="#fig_4">6</ref>-b, Apollo can predict vehicle trajectories and plan vehicle routes within a time frame of a hundred milliseconds. Thus, the prediction module functions appropriately and can determine the trajectories of objects in complex road conditions. The prediction score is 0.65, indicating that there is still room for improvement in this module. If the algorithm could predict the trajectory of the other vehicle better, it might stop the ego-vehicle earlier, outside the course of the other vehicle. This case suggests that algorithm developers need to improve algorithms to ensure that sudden changes of the trajectories of objects around the vehicle do not create a new risk of collision within a time frame of a hundred milliseconds. The planning module, on the other hand, performed poorly during this period of time. After the prediction module predicted the trajectory of the other vehicle, the planning module still planned to continue the movement at 16:49:56.773. This error also reflects the inconsistency of the prediction and planning module, which should be investigated further by the developers.</p><p>Overall, this case shows that the evaluation system correctly identifies the time of the collision accident and gives an appropriate score based on the performance of each module. Moreover, through interactive exploration, users can understand the correlation between the accident and the performance of individual modules.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Autonomous Driving Module Evaluation</head><p>In this case, we aim at examining the evaluation functionality for each module in autonomous driving. The initial state is shown in Fig. <ref type="figure">1</ref>.</p><p>From the radar view (Fig. <ref type="figure">1-b</ref>), we can clearly see that the overall score of this autonomous driving data segment is 91.94%. The initial state of the radar view shows the overall evaluation of each module. The timeline view (Fig. <ref type="figure">1-c</ref>) shows the evaluation results over time. We focus on the period when a vehicle in autonomous driving scores low when making turns. During this period, we learn from the radar view that the comfort and planning modules perform poorly. Seeing the score of each factor through parallel coordinates, we find the factors that cause the low scores of the above two modules: HeadingChange, PoseError, and VelocityError. To explore other factors with bad performance, we brush the low score range on the left part (&lt; 80) of the parallel axes (Fig. <ref type="figure">7</ref>). There are two main periods with low score, as shown in the timeline view, the second half of the turn and a short period before the turn. For these two scenarios, we explore each module.</p><p>For the comfort module, we analyze the evaluation results of Head-ingChange through several steps. First, we use parallel coordinates to select the region where the score is below 0.6 and query the time points Fig. <ref type="figure">7</ref>: Exploration of overall poor evaluation results. There are some poor evaluation results in the second half of the turn. a, b, c, and d shows time points with poor evaluation results in this turning process of low scores. Second, we observe the distribution of low scores in the timeline view. These low scores are distributed in the second half of the turn. Third, we drag the progress bar of the spatial-temporal view to see the state information in specific scenarios. From the autonomous driving state data, we find that the WheelPanel changes frequently. Finally, we see from the radar view that the comfort module scores are low during the turning process. Observing the path of the vehicle, we find that it is slightly away from the intersection center during the turn. Apparently, the vehicle turned early and deviated from the optimal path (Fig. <ref type="figure" target="#fig_5">8-a</ref>).</p><p>Similarly, we focus on the Steering factor of the control module. Brushing the area on the Steering axis where evaluation results are smaller than 0.6, we can see that the points mainly fall in the second half and the end of the vehicle turning process. Considering the fact that the vehicle deviates from the optimal path and the steering wheel rotation changes rapidly, we believe what is happening is that the control module is trying to bring the vehicle back on its planned course by making a series of adjustments to the wheel orientation (Fig. <ref type="figure" target="#fig_5">8-b</ref>).</p><p>When analyzing the planning module, we find that PoseError and VelocityError tend to get low scores simultaneously. The scenario where both scores are below 0.4 occurs before the vehicle makes the turn. Dragging the progress bar and observing the scenario, we find that the planning score quickly picks up when the vehicle starts to move forward and turn. We speculate this is because the planning module needs information about the current state of the vehicle. When the vehicle moves, the information of its speed, acceleration, and other parameters are used as the reference for planning. However, when the vehicle is stationary, available information is insufficient for good planning. Therefore the accuracy of planning is relatively low (Fig. <ref type="figure" target="#fig_5">8-c</ref>).</p><p>While exploring the ObjectProbability factor of the prediction module, we find some evaluation results with scores close to 0. We explore the corresponding scenarios by brushing the low-scoring area of this factor in parallel coordinates. The system highlights these scenarios in the timeline view. By dragging the progress bar to these time points and analyzing the obstacle view on the left, we find that the autonomous driving system identified an obstacle with a priority level of CAUTION. Through the spatial-temporal view, we can easily find that this obstacle is a car on the right side of the ego-vehicle. We consider that obstacles marked as CAUTION need special attention. Therefore, in our evaluation method, the evaluation of ObjectProbability relies heavily on the performance of the obstacles marked as CAUTION. At that moment, since the ObjectProbability value of the obstacle is 0, the system gives the score of the ObjectProbability factor as 0. In other words, the evaluation system considers that the prediction module has completely misjudged the trajectory of this obstacle (Fig. <ref type="figure" target="#fig_5">8-d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Customized Factors Ranking</head><p>From the user's point of view, the importance of individual modules varies in different situations. For object detection algorithm developers, they care more about the accuracy of their algorithms, so value the performance of the perception and prediction modules most. However, for control algorithm developers, their interests are in the performance of the control module. When product managers evaluate an autonomous driving system, they probably focus on how comfortable the passengers of a vehicle or even the driver may feel in autonomous driving. Thus, evaluation methods should be flexible enough to accommodate different priorities by different stakeholders. In this case, we demonstrate how our system allows users to change the evaluation algorithm interactively.</p><p>We sort the factors according to their level of importance (Fig. <ref type="figure" target="#fig_6">9</ref>). Assuming that users have greater interest in passenger comfort and vehicle planning route capability, they can elevate the comfort and planning modules in the list of sorted factors .</p><p>As shown in (Fig. <ref type="figure" target="#fig_6">9</ref>), after the change of the order of factors, there is a clear tendency for the score to drop and then rise during the turn. Turning too fast can cause discomfort, so the scores of comfort factors drop accordingly. The values of Jerk, HeadingChange, and Centrifugal should be kept within a reasonable range, which algorithm developers should be concerned with. On the other hand, the low planning score indicates that the planning algorithm performs poorly when the vehicle is turning, as reflected by the bias in estimating vehicle pose and velocity. In this case, the algorithm developer should consider how to improve the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERT EVALUATION</head><p>In this research, we worked closely with domain experts in autonomous driving from a large car manufacturer. Five experts were involved in the collaboration from the definition of design requirements to the evaluation of the system.</p><p>To collect feedback on the system from experts, we organized a 90-minute workshop and invited three domain experts participated to evaluate system usability and to provide suggestions for improvement. The workshop started with a 15-minute tutorial, followed by the introduction of two cases that we presented in Sect. 5.1 and Sect. 5.2. The entire case study process took 20 minutes, including included a Q&amp;A session. The experts then spent 40 minutes using the system for free exploration. After evaluating the cases and using the system, they filled out a questionnaire.</p><p>The questionnaire has 12 questions from three perspectives: 1) the correctness and intuitiveness of the visual evaluation method and interactive workflow; 2) the comprehension of the visualization designs in the system; 3) and suggestions for future improvement.</p><p>The overall feedback for our system and evaluation method is positive. In the workshop, all experts could follow the evaluation workflow and use our system to go through different cases. One expert believed that "The system can visually demonstrate when, where, and how autonomous driving systems perform poorly." They also confirmed that our case study is interesting and inspiring, as one expert said "I'm convinced that this system can help experts quickly and efficiently identify the problems in autonomous driving."</p><p>The experts agreed that the visualization tools provided by our system greatly help the evaluation process. For example, one expert praised the radar view by saying "it helps users clearly identify which modules perform well or bad, supports good comparison, and also serves a good entrance for detailed analysis." Most experts also liked the view of parallel coordinates and the interactive brushing function for linked view analysis. One expert said "The interface design and interactive performance of the system are impressive. The correspondence between factor values and scores can be easily seen in parallel coordinates."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSION</head><p>As autonomous driving systems are installed in more and more cars, it is very important to test their effectiveness and reliability before their deployment. The evaluation system of autonomous driving systems are needed to investigate the performances of various algorithms in different driving scenarios and to provide evidences for decisions on improvement and deployment.</p><p>There are some limitations in our method. The mathematical modeling approach we used, TOPSIS, is a method that amplifies the gaps between factors involved in evaluation. Although this treatment facilitates the identification of those subjects with poorer assessment results, it may overestimate the internal disparity of assessed factors. In the context of the evaluation of autonomous driving systems, even if driving data shows excellent performance at a certain time period, this method still tries to find some points in the process where the evaluation result is 0 on each factor. Consequently, autopilot data with good performance may still get poor assessment results somewhere. Although we consider adding a moderator γ when the extreme differences of the scoring data are minor, its values and the moderating criteria lack sound theoretical support.</p><p>In addition, although our visual evaluation system can identify most of the anomalous results in system testing, sometimes it misses the corresponding anomalies for the low scoring results identified by the system. This situation may be due to the fact that the system is sensitive to the changes of data inputs. Therefore, scores may fluctuate with the change of data. Some weakness of our system is reflected at the data level. First, so far we only used a limited amount of simulation data without considering real-world data, only evaluated autopilot performances in normal weather conditions without including more challenging conditions (e.g., fog, rain), and only examined common autopilot behaviors without looking into more dangerous behaviors such as u-turns in busy traffics. To make our system more reliable and effective, we need to test it with more diverse driving scenarios that include broader driving behaviors based on both simulated and real-world data. In addition, we have not fully explored all driving data factors yet, and only used a few factors to evaluate the module performances. Considering the complexity of autonomous driving systems, we need to integrate more factors into the evaluation algorithm. Finally, although our evaluation methodology can be applied to simulation and live vehicle exercises, no road test evaluation have been conducted yet.</p><p>For the scalability of our system, to support batch analysis of multiple driving scenarios, we need to provide users with more flexibility in analytical procedure. Our current analytical procedure includes three main steps: 1) checking the distribution of the score timeline, 2) selecting module(s) with a low score to inspect the scores of related factors, and 3) examining the state showing factor values and spatial-temporal visualization. This procedure functions well for single-case analysis, but may be insufficient for batch analysis. It is needed to enrich the workflows and analytical procedures in our system.</p><p>We expect that our research can offer some new ideas for the evaluation of autonomous driving systems. In this work, we propose a visual evaluation system to assist the evaluation of an autonomous driving process, modules involved in autonomous driving, and the factors that contribute to the autonomous driving process. We constructed an evaluation model by combining mathematical methods of AHP and TOPSIS, and built a visualization system to support interactive evaluation of the results and state of autonomous driving data. The system can help the designers of autonomous driving systems to locate poorly performing moments, view the corresponding autonomous driving scenarios, and analyze and explain the reasons for low autonomous driving evaluation results. The outcomes of our system can help them better design and improve autonomous driving systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Parallel Coordinate Brush: the evaluation scores (left) and the factor values (right) of each factor at the same time point.</figDesc><graphic coords="6,44.75,460.01,215.18,159.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Interactively customized ranking. The left figure shows the original evaluation scores with all the equal weight of modules. Users can drag the evaluation factor bars to reorder the importance of the factors, the new evaluation score results are generated on right figure.</figDesc><graphic coords="6,307.43,49.25,143.42,144.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: A case of traffic accident. The yellow circle in the timeline view marks the lowest score. The spatial-temporal view shows the scene of the accident. The yellow ticks in parallel coordinates indicate the factors with poor performance at that moment.</figDesc><graphic coords="7,196.91,213.53,107.66,128.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) From the spatial-temporal view, we can see that the autonomous driving algorithm successfully determines the trajectory change of the obstacle within 100 milliseconds and makes the decision (the purple decision fence) to stop and avoid it. However, the planning module contradict with the prediction module and still plans the vehicle's forward path. (b) Module Performance. Parallel coordinates show that the low scores of PoseError, VelocityError and ObjectProbability lead to the poor performance of the planning module and medium-poor performance of the prediction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig.6: Exploring the correlation between the accident and module performance by reviewing the scene before the accident.</figDesc><graphic coords="7,53.75,213.53,143.54,128.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Exploration of overall worse evaluation results.</figDesc><graphic coords="8,309.71,537.89,215.06,76.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Changing the order of factors leads to the change of the score line graph significantly.</figDesc><graphic coords="9,53.75,181.37,250.82,66.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, X. Xue and S. Chen are with School of Data</figDesc><table /><note>Science, Fudan University. Y. Hou and C. Wang contribute equally. S. Chen is the corresponding author. E-mail: {yijiehou, chengshunwang, junhongwang, xyxue, simingchen}@fudan.edu.cn. • X. Zhang is with Pennsylvania State University. E-mail: lzhang@ist.psu.edu. • J. Zhu and D. Wang are with China FAW (Nanjing) Technology Development Co., Ltd. E-mail:{zhujun18, wangdongliang}@faw.com.cn. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation factor with its description and criteria</figDesc><table><row><cell>Module</cell><cell>Factor</cell><cell>Description</cell><cell>Evaluation Criteria</cell></row><row><cell>Comfort</cell><cell>Jerk Heading Change</cell><cell>Acceleration change rate Orientation change rate</cell><cell>The smaller the absolute value, the better; less than 1 is the positive ideal value The smaller the absolute value, the better; less than 1 is the positive ideal value</cell></row><row><cell></cell><cell>Centrifugal</cell><cell>Centrifugal fore, calculated as speed 2 × kappa</cell><cell>The smaller the absolute value, the better; less than 1 is the positive ideal value</cell></row><row><cell>Control</cell><cell>Brake</cell><cell>Brake strength change rate</cell><cell>The smaller the absolute value, the better; less than 1 is the positive ideal value</cell></row><row><cell></cell><cell>Steering</cell><cell>Steering strength change rate</cell><cell>The smaller the absolute value, the better; less than 1 is the positive ideal value</cell></row><row><cell>Planning</cell><cell>Pose Error</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank Lei Peng, Siqi Shen, Aolin Zhang, Weide Zhang and anonymous reviewers for their valuable suggestions. This work is supported by Shanghai Municipal Science and Technology Major Project (No.2018SHZDZX01), ZJ Lab, and Shanghai Center for Brain Science and Brain-Inspired Technology. This work is also supported by Shanghai Science and Technology Commission (Grant No. 21ZR1403300) and Shanghai Sailing Program No.21YF1402900.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Autonomous vehicle safety: An interdisciplinary challenge</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="96" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles</title>
		<author>
			<persName><surname>Sae International</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://jtgl.beijing.gov.cn/jgj/jgxx/gsgg/jttg/588465/683743/index.html" />
		<title level="m">Contents and methods of field test capability assessment for automated vehicle</title>
				<imprint>
			<date type="published" when="2018-02-25">2018. February 25, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Specifying autonomy levels for unmanned systems: interim report</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Albus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>English</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Unmanned Ground Vehicle Technology VI</title>
				<editor>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Gerhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Shoemaker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Gage</surname></persName>
		</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">5422</biblScope>
			<biblScope unit="page" from="386" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Evaluation of the Intelligent Behaviors of Unmanned Ground Vehicles Based on Information Theory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Application of gray correlation and improved ahp to evaluation on intelligent u-turn behavior of unmanned vehicles</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 8th International Symposium on Computational Intelligence and Design (ISCID)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multiple attribute-based decision making model for autonomous vehicle in urban environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Intelligent Vehicles Symposium Proceedings</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="480" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apollo</forename><surname>Dreamview</surname></persName>
		</author>
		<ptr target="https://github.com/ApolloAuto/apollo/tree/266afbf68d83fa6fac7a812ff8a950223f5ab2c0/modules/dreamview.lastaccessedon" />
		<imprint>
			<date type="published" when="2021-06-30">June 30, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autonomous driving in urban environments: Boss and the urban challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Urmson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anhalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Galatali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gittleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harbaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Likhachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Field Robotics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="425" to="466" />
			<date type="published" when="2008-01">01 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of autonomous driving: Common practices and emerging technologies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yurtsever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carballo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="58443" to="58469" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Autonomous vehicle implementation predictions: Implications for transport planning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Litman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Autoware</forename></persName>
		</author>
		<ptr target="https://www.autoware.org/.lastaccessedon" />
		<imprint>
			<date type="published" when="2021-03-02">March 2, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apollo</forename></persName>
		</author>
		<ptr target="https://apollo.auto/index_cn.html.lastaccessedon" />
		<imprint>
			<date type="published" when="2021-02-25">February 25, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><surname>Carla</surname></persName>
		</author>
		<title level="m">An open urban driving simulator</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Osiński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miłoś</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakubowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ziecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martyniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Homoceanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<title level="m">Carla real traffic scenarios -novel training ground and benchmark for autonomous driving</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lgsvl simulator: A high fidelity simulator for autonomous driving</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tabatabaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Možeiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Uhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agafonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sterner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ushiroda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zelenkovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apollo</forename><surname>Dreamland</surname></persName>
		</author>
		<ptr target="https://bce.apollo.auto/.lastaccessedon" />
		<imprint>
			<date type="published" when="2020-12-23">December 23, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluation methods for the autonomy of unmanned systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Science Bulletin</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="3409" to="3418" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual analytics for electromagnetic situation awareness in radio monitoring and management</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="590" to="600" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cloud-edge based lightweight temporal convolutional networks for remaining useful life prediction in iiot</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Deen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of visual analytics techniques for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Visual Media</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vatld: A visual analytics system to assess, understand and improve traffic light detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Shekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Managing, modeling, and visualizing high-dimensional spatio-temporal data in an integrated system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Christoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GeoInformatica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="59" to="77" />
			<date type="published" when="1998">03 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tripvista: Triple perspective visual trajectory analytics and its application on microscopic traffic data at a road intersection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Pacific Visualization Symposium</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interactive visual discovering of movement patterns from sparsely sampled geo-tagged social media data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="270" to="279" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interactive high-dimensional data visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Swayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="99" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Principles of high-dimensional data visualization in astronomy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pca versus lda</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kpca plus lda: a complete kernel fisher discriminant framework for feature extraction and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="244" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving kernel fisher discriminant analysis for face recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised nonlinear dimensionality reduction for visualization and classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1098" to="1107" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Localization in wireless sensor network using lle-isomap algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kasinayal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TENCON 2017 -2017 IEEE Region 10 Conference</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="393" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploratory spatio-temporal visualization: an analytical review</title>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Galalsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="503" to="541" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revealing patterns and trends of mass mobility through spatial and temporal abstraction of origin-destination movement data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2120" to="2136" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual exploration of big spatio-temporal urban data: A study of new york city taxi trips</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2149" to="2158" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contextualized Analysis of Movement Events</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doulkeridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koumparos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroVis Workshop on Visual Analytics (EuroVA). The Eurographics Association</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Tatiana von Landesberger and Cagatay Turkay</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A scaling method for priorities in hierarchical structures</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Saaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="234" to="281" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-Criteria Decision Making Methods</title>
		<author>
			<persName><forename type="first">E</forename><surname>Triantaphyllou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="5" to="21" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
