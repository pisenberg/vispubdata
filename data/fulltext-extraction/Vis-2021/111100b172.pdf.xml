<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sibyl: Understanding and Addressing the Usability Challenges of Machine Learning In High-Stakes Decision Making</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexandra</forename><surname>Zytek</surname></persName>
							<email>zyteka@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Auckland University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongyu</forename><surname>Liu</surname></persName>
							<email>dongyu@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Auckland University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rhema</forename><surname>Vaithianathan</surname></persName>
							<email>rhema.vaithianathan@aut.ac.nz</email>
							<affiliation key="aff0">
								<orgName type="institution">Auckland University of Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Auckland University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kalyan</forename><surname>Veeramachaneni</surname></persName>
							<email>kalyan@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Auckland University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sibyl: Understanding and Addressing the Usability Challenges of Machine Learning In High-Stakes Decision Making</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0DE8FB8939005323C3AF0D57749CDC82</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>XAI</term>
					<term>Usability</term>
					<term>child welfare</term>
					<term>visualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning (ML) is being applied to a diverse and ever-growing set of domains. In many cases, domain experts -who often have no expertise in ML or data science -are asked to use ML predictions to make high-stakes decisions. Multiple ML usability challenges can appear as result, such as lack of user trust in the model, inability to reconcile human-ML disagreement, and ethical concerns about oversimplification of complex problems to a single algorithm output. In this paper, we investigate the ML usability challenges that present in the domain of child welfare screening through a series of collaborations with child welfare screeners. Following the iterative design process between the ML scientists, visualization researchers, and domain experts (child screeners), we first identified four key ML challenges and honed in on one promising explainable ML technique to address them (local factor contributions). Then we implemented and evaluated our visual analytics tool, SIBYL, to increase the interpretability and interactivity of local factor contributions. The effectiveness of our tool is demonstrated by two formal user studies with 12 non-expert participants and 13 expert participants respectively. Valuable feedback was collected, from which we composed a list of design implications as a useful guideline for researchers who aim to develop an interpretable and interactive visualization tool for ML prediction models deployed for child welfare screeners and other similar domain experts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Thanks to innovations in machine learning (ML), computers can now help with many tasks previously performed by humans alone, often improving both speed and precision. However, in many domains, human decision-makers provide essential insights that cannot be replaced by existing algorithms. In such cases, decision-making outcomes are improved when ML output is used to augment human decision-making, rather than replace it.</p><p>ML models often output only a single number or classification, such as the risk score seen in the upper right corner of Figure <ref type="figure" target="#fig_0">2</ref>. This can make it difficult for human decision-makers to incorporate the model into their decision making. As a result, many ML algorithms lack usability, or the attribute of being able to be efficiently used by humans to make better decisions.</p><p>The machine learning, data science, and data visualization communities have offered a multitude of algorithms and tools to augment ML predictions and address these usability challenges -we refer to these as ML augmentation tools. These tools, when chosen carefully for the domain, have the ability to greatly improve the usability of ML models for decision making. Examples of such tools include data visualizations, global and local explanations <ref type="bibr" target="#b1">[2]</ref>, cost-benefit analysis <ref type="bibr" target="#b8">[9]</ref>, performance metrics, and information about historic usage and results of the ML model. However, research aimed at augmenting ML predictions often focuses on an audience of ML/data experts <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b7">[8]</ref> or domain experts in more technical or data-driven fields such as medicine <ref type="bibr" target="#b14">[15]</ref>  <ref type="bibr" target="#b11">[12]</ref>. For example, Zhang et. al. <ref type="bibr" target="#b26">[27]</ref> developed a framework for helping data scientists and ML experts interpret and debug ML models, and Lundberg et. al. <ref type="bibr" target="#b14">[15]</ref> developed an interface for helping anaesthesiologists prevent hypoxaemia during surgery through detailed data visualization. In contrast, many fields are more qualitative in nature, with decisions following discussion more than data crunching. In this paper, we focus on these more qualitative fields, and the usability challenges they face.</p><p>To concretely assess usability challenges, we investigated the need for additional auxiliary information alongside ML predictions through a comprehensive literature review. We first selected a number of papers that had ML and explainability as topics. We selected and reviewed 55 papers covering ML applications and explainability. We identified challenges in human-ML interactions described in these papers. Three of us then began codifying a set of factors that decrease usability in ML models. Table <ref type="table" target="#tab_0">1</ref> summarizes a set of usability challenges we codified that are relevant when a model is actively used for decision-making. Some challenges stem from not understanding where a model's predictions come from, making it difficult for human decision-makers to trust the model (TR), and to handle any disagreements between their opinions and the model's output (DIS). Others are caused by a lack of information about the real effects of a decision. A lone model prediction often does not explicitly indicate the expected results of a decision (CON), suggest accountability (ACC), or provide ethical assurances (ETH). Finally, challenges may arise when the output of the model is not a direct suggestion of a decision, but rather auxiliary information. In this case, the output may be confusing (CT) or entirely irrelevant (UT).</p><p>Determining which usability challenges exist, the best tools to address them, and the necessary design choices for these tools depends highly on specific aspects of the domain and the decision-makers involved. Through our literature review and the case study discussed in this paper, we identified a subset of context-dependent factors that should be considered when working to make an ML model more usable in a particular domain. Table <ref type="table">2</ref> lists some examples of these factors.</p><p>To investigate the problem of finding and mitigating usability challenges in more qualitative fields, we selected the domain of child welfare screening. In terms of the relevant context factors, child welfare screeners are domain experts without ML/data science expertise, making decisions using an ML model as an auxiliary tool, with about a few minutes per decision, in a high-risk field.</p><p>Addressing usability challenges is a non-trivial task that requires collaboration with end-users. In this paper, we engaged in three forms of collaboration: observations to understand their existing workflow and its possible usability challenges, interviews to gain additional insights into the desires of end-users, and user studies with possible ML augmentation techniques to get concrete feedback on design.</p><p>The main research questions, and our key findings with regards to these questions, are as follows. RQ3 What design choices must be made when building these tools to optimize them for use by child welfare screeners and other experts in similar domains? Based on our interviews and formal user study, we identified a list of design considerations that should be made when choosing model features and developing ML augmentation visualizations, described in Sections 6 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">STUDY CONTEXT: CHILD WELFARE SCREENING</head><p>In this section, we introduce the domain of child welfare screening Child Abuse in the U.S. Child abuse is an active issue affecting the health and well-being of communities. The Centers for Disease Control and Prevention (CDC) estimates at least 1 in 7 children have experienced child abuse and/or neglect in the past year <ref type="bibr" target="#b0">[1]</ref>. Child abuse victims can suffer physical and emotional injuries, and may experience trauma resulting in long-term mental health problems <ref type="bibr" target="#b0">[1]</ref>. More than one-third of American children are investigated as potential victims of abuse or neglect by age 18 <ref type="bibr" target="#b9">[10]</ref>. Still, in 2018, there were 1,770 reported fatalities resulting from child abuse and neglect <ref type="bibr" target="#b0">[1]</ref>.</p><p>Child Welfare Screening. In the U.S., regional Child Protective Services (CPS) agencies are tasked with handling child abuse and neglect referrals from concerned members of the community, including mandated reporters such as teachers, who are required by law to report any suspicion of abuse or neglect. These referrals are examined by child welfare specialists ("call screeners"), who decide whether to screen in or screen out each case. A screened-in case will be investigated further, while a screened-out case will be recorded but not investigated.</p><p>Both false negatives (real abuse cases that are screened out) and false positives (cases with no abuse that are screened in) can have heavy consequences. False negatives lead to prolonged child suffering and, in extreme cases, child fatality. False positives can lead to longterm emotional distress for parents, children, and other family and community members, as well as damaged financial, career and social prospects for parents and other caretakers <ref type="bibr" target="#b17">[18]</ref>.</p><p>In 2018, CPS agencies in the United States received 4.3 million referrals from concerned parties about potential child abuse <ref type="bibr" target="#b0">[1]</ref>. 56% of these referrals were screened in and investigated, but only 16.8% of the screened in cases were found to involve abuse or neglect <ref type="bibr" target="#b0">[1]</ref>.</p><p>ML for Child Welfare. One important motivation for computerized assistance in child welfare call screening is repeated cases of missed abuse. Fatal child abuse cases in which children were referred several times but were never screened in are tragic, and although such cases are rare, they are avoidable <ref type="bibr" target="#b6">[7]</ref>. An ML solution can quickly scan for red flags, such as repeated referrals, that busy human call screeners may miss in the overload of data.</p><p>In recent years, predictive risk modelling (PRM) has been deployed in child welfare contexts in multiple counties, with the goal of enabling more efficient and consistent decision-making and improving the overall health and safety of county residents <ref type="bibr" target="#b23">[24]</ref>. One example of such a model was deployed in Allegheny Country, PA by Vaithianathan et. al. in 2016 <ref type="bibr" target="#b23">[24]</ref>.</p><p>Currently, PRM is being introduced to our collaborating county in Colorado by Vaithianathan et. al. <ref type="bibr" target="#b22">[23]</ref>, through a LASSO regression model trained on 461 features, which include information such as the child and parents' age, past referrals and their outcome, and past court involvements. The model predicts the likelihood of removal from home in the next two years, translated to a 1 through 20 risk score where the higher the score, the higher the risk <ref type="bibr" target="#b23">[24]</ref>. This paper focuses on the usage of this model.</p><p>Study Participants. We collaborated over the course of a year with a pool of 19 social workers and supervisors working for the child welfare department in a collaborating county in Colorado. All participants regularly act as screeners in the county's child welfare screening decision making process. Our collaborations began in December 2019, with two days of in-person field observations (Section 4.1). Following this, we observed a simulated case review session via video conferencing (Section 4.2), and conducted several interviews also via video conferencing (Sections 4.3 and 5). Finally, our collaborating screeners participated in our user study digitally (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>In this section, we discuss related work in human-centered and explainable ML, as well as existing ML augmentation tools.</p><p>Human-Centered ML. Past literature has advocated for a humancentered perspective to ML <ref type="bibr" target="#b3">[4]</ref> -one that considers machines and algorithms as part of collaborative systems alongside humans. This perspective considers how humans use, interact with, adapt to, and evaluate ML applications <ref type="bibr" target="#b3">[4]</ref>. A truly human-centered ML approach acts end-to-end, beginning with human-in-the-loop training systems and ending with evaluation systems based on the metrics that end users are most interested in <ref type="bibr" target="#b3">[4]</ref>. In this paper, we take a deeper look at one step of this extensive pipeline: the use of ML algorithm predictions by humans for real-world decision making.</p><p>Explainable ML. A common usability challenge addressed by the literature is the black box nature of most ML algorithms. Humans struggle to use ML predictions because they do not understand where they came from. This usability challenge is addressed through the fields of interpretable or explainable ML. Doshi-Velez and Kim proposed that the need for ML interpretability stems from an "incompleteness in the problem formulation" <ref type="bibr" target="#b1">[2]</ref>, which prevents the system from being thoroughly evaluated or trusted. This incompleteness can take several forms, including a need for scientific understanding, concerns about Table <ref type="table">2</ref>. Domain context factors that may influence the usability of an ML model. The context factors relevant to child welfare screening are in bold. By technical expertise, we refer to the ML or data science expertise of the end-user. This is not meant to be a complete list -there are many other factors that could also be relevant. safety or ethics, or mismatched objectives between the model output and the human goal <ref type="bibr" target="#b1">[2]</ref>. Doshi-Velez and Kim <ref type="bibr" target="#b1">[2]</ref> also define three evaluation approaches for ML interpretability. In application-grounded approaches, domain experts work with explanations within a real application. This provides the most realistic quantification of explanation quality, but may require high time commitments from a potentially small pool of domain experts. In human-grounded approaches, researchers develop simpler problems for experimentation using non-expert subjects. Finally, in functionallygrounded evaluation, a formal definition of interpretability is used as a proxy to evaluate an explanation without using human subjects. This paper focuses on an example of such an application-grounded approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Factors</head><p>Wang et. al. <ref type="bibr" target="#b24">[25]</ref> developed a human-driven conceptual framework for building explainable AI systems. They found that decision-makers seek explanations to justify unexpected occurrences, monitor for important events, or facilitate learning. They created a taxonomy of AI techniques based on how they support human reasoning and represent information. Finally, the authors discuss how explainable AI can mitigate cognitive biases. Our work builds on this by finding cognitive biases that can be caused by explainable AI, as listed in Section 7.</p><p>ML Augmentation Tools. Spinner et. al. <ref type="bibr" target="#b20">[21]</ref> developed a conceptual framework for explainable AI, along with a corresponding implementation called EXPLAINER. This framework includes single-model explainers (which are the focus of this paper) as well as multi-model explainers, which can be used to compare and select between models. The authors also distinguish between different model audiences, including novices, model users, and developers -our work focuses on model users.</p><p>Krause et. al. <ref type="bibr" target="#b10">[11]</ref> developed PROSPECTOR, a comprehensive visualization system for data scientists. This system includes interactive functionality for both global and local feature-focused explanations.</p><p>Hohman et. al. <ref type="bibr" target="#b5">[6]</ref> developed a visual analytics system called GAMUT to investigate how machine learning practitioners and data scientists interact with machine learning. To develop this tool for use on GAMs, the authors interviewed technical experts to generate a list of common questions asked about predictions. In total, they identified six question types, which they address using three views. GAMUT was tested by having 12 data scientists use the tool while thinking out loud, followed by an interview.</p><p>Lundberg et. al. developed PRESCIENCE, an explanatory ML system focused on preventing hypoxaemia during surgeries <ref type="bibr" target="#b14">[15]</ref>. This tool predicts the risk of hypoxaemia in the next five minutes using a gradient-boosting algorithm trained on time series. It also includes several visualizations to explain the prediction, including SHAP feature contribution explanations.</p><p>Kwon et. al. <ref type="bibr" target="#b11">[12]</ref> developed RETAINVIS, a visualization tool for explaining recurrent neural networks (RNNs) applied to electronic medical records (EMRs). The tool was developed with active feedback from domain experts (medical practioners). RETAINVIS includes five different visualizations for looking into RNNs.</p><p>Our work is similar to these tools in that it relies on collaboration with end users to develop a tool that provides additional information alongside an ML prediction. However, our users are not expected to have any prior ML or data science expertise, nor are they used to working with data-heavy visualizations. This work is also the first to our knowledge that investigates through a complete case study the usability of machine learning for child welfare screeners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNDERSTANDING CONTEXT AND END-USER NEEDS</head><p>To address RQ1 (identifying ML usability challenges), we performed a series of field observations and interviews. In this section, we discuss our goals and the findings we made during these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Understanding Existing Workflows</head><p>To better understand the existing child welfare screening workflow, we travelled to our collaborating county in Colorado to observe screeners' decision-making on referrals without using the ML model. This process led to the following findings:</p><p>1. Our collaborating county uses the general procedure for child welfare screening shown in Figure <ref type="figure">1</ref>. In cases of immediate concern, a referral may be screened in immediately after it is received by CPS (this decision is made by a child welfare supervisor). In most cases, however, the decision as to whether to screen-in or screen-out a referral is made by a team of child welfare experts.</p><p>It is this team that receives the ML risk score prediction.</p><p>2. Five to ten minutes are spent on each case by this team. Most of this time is spent going over the details of the case. The screening decision is made after about one to two minutes of discussion.</p><p>3. A large portion of these five-to-ten minutes of screening time is dedicated to determining the factors that are associated with higher and lower likelihood of abuse -referred to as risk and protective factors, respectively -involved in a case, and weighing these against each other. The factors considered will vary based on the details of the case, but may include information such as child's age (very young children are more vulnerable), criminal Fig. <ref type="figure">1</ref>. The general child welfare screening process used by the CPS department of our collaborating county. The referral is first received by the CPS hotline, and then sent to a child welfare supervisor. In a minority of cases, the supervisor will deem the child or children involved to be in serious and immediate risk of danger, and will screen-in the case for immediate further investigation. In most cases, however, the case will be reviewed by a team of child welfare screeners the next day. This team will be given the ML risk score prediction (dark blue box). If this team decides to screen-in the case, it will be investigated further through home visits, interviews, or other means. Otherwise, the case will be recorded but not investigated unless re-referred. In the case of a screen-out, the screeners may elect to provide the family with additional family services.</p><p>record of adults involved, whether there are any trusted adults active in the child's life, and actions and statements made by adults involved (for example, a mother taking actions to separate an aggressive partner from her children).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Identifying ML Usability Challenges</head><p>To identify the particular ML usability challenges relevant to child welfare screening (RQ1), we observed a simulated case review session where social workers used an ML model. In this session, our 19 expert collaborators were split into three teams of typical size, which we will refer to as T1, T2, and T3. They were asked to make decisions about real referrals that the county had handled in the past. For each case, they had access to the information that screeners are typically given during decision-making -including the age of all involved parties and a written description of the potential abuse as given by the referring party -as well as a 1 through 20 risk score provided by the ML model. After receiving this information, the screeners went ahead with their usual process: discussing the case as a team for five to ten minutes and then making a screening decision. The teams were then interviewed and asked to reflect upon how they used each ML score, whether the scores aligned with their expectations, and how the scores impacted their decisions. Each team made decisions on seven to nine cases. During interview sessions, all three teams expressed reservations about using the ML model for decision-making. Based on their responses to our interview questions, we identified four key usability challenges:</p><p>1. Lack of Trust TR Screeners expressed a lack of trust when making decisions using the ML model, evidenced by their tendency to not consider the model prediction at all when it disagreed with their intuition. For example, when asked if the score caused them to reconsider their decision, T3 responded In addition to determining which usability challenges were relevant to child welfare screening, we also confirmed that some were not relevant, and therefore did not require consideration when designing SIBYL. For example, three of the example usability challenges from Table <ref type="table" target="#tab_0">1</ref> were not relevant. Lack of accountability ACC was not relevant, as accountability is always held by human decision-makers in this domain. Similarly, unclear consequences of actions CON was not relevant, because the domain experts have extensive training and understanding of the potential consequences of screening decisions, and do not intend to offload this understanding to an ML algorithm. Finally, unhelpful prediction target UT was not relevant, as the ML model has been developed specifically to provide relevant information.</p><formula xml:id="formula_0">"</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interviewing Screeners</head><p>To begin addressing RQ2 (what tools can be helpful in mitigating usability challenges), we interviewed the 19 screeners about what additional information they would be interested in receiving alongside ML predictions. The format was a semi-structured open-floor session. We began by asking the screeners whether they thought additional information would be useful, and what specific information they would be interested in. We also proposed possible augmentation information (e.g. the relative importance of different factors, answers to what-if questions, and comparisons with past cases) and asked if they might be helpful.</p><p>Our findings from this interview included:</p><p>1. Screeners were confident that they would want to know why the model made the predictions it made.</p><p>2. Screeners believed that understanding how important each factor was to the score prediction would be helpful.  (H) A button for switching between a single-table view and a side-by-side view, which splits factors that increase and decrease risk. (I) A sidebar for switching between different explanation types, as described Section 5.</p><p>5. Some screeners were interested in seeing similar cases they dealt with in the past. Others thought this would be too much information to digest in such a short period of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GETTING FEEDBACK ON POSSIBLE TOOLS</head><p>To address RQ2 (identifying helpful interfaces) and RQ3 (identifying important design choices), and based on the usability challenges identified (Section 4.2) and responses to our interview (Section 4.3), we engaged in a user-centered iterative design process <ref type="bibr" target="#b15">[16]</ref> to develop SIBYL, an ML augmentation tool. We began by designing high-fidelity mock-ups for five augmentation interfaces, each with a separate purpose and goal. Table <ref type="table">3</ref> summarizes the motivation behind each interface in terms of its theorized effect on addressing the usability challenges.</p><p>The full versions of the original high fidelity mock-ups can be found in Appendix B.</p><p>Early in the design process, we learned that the word "factor" is more familiar to screeners than "feature" when referring to pieces of information used when making decisions. For the purposes of consistency, we use the word factor throughout this paper when referring to data inputs used by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case-Specific Details: Factor Contributions</head><p>The Case-Specific Details interface (Figure <ref type="figure" target="#fig_0">2</ref>) provides a simple local explanation of where an individual model prediction comes from through factor contributions. The table assigns each factor a contribution (Figure <ref type="figure" target="#fig_0">2E</ref>), either positive (red) or negative (blue). The bar length indicates the magnitude of the contribution.</p><p>The phrases positive contribution and negative contribution may cause confusion. In the ML community, a positive contribution indicates that the model prediction will increase in value. In the screener community, however, an increased risk score is a negative occurrence. We decided to avoid using the terms "positive" and "negative" in the app or instructions. Instead, the factors are labelled as "risk factors" or "protective factors" to mirror the screeners' language. We utilize the bar's direction along with the two arrows (↑ and ↓) to suggest risk increasing (red bar pointing right) or decreasing (blue bar pointing left).</p><p>A local factor contribution explanation may reveal that the young age of a particular child (infant) has caused a significant increase in the risk score (Figure <ref type="figure" target="#fig_0">2</ref>-C1), while the low number of past referrals (2) compared to the average referred child resulted in a decrease compared to the average risk (Figure <ref type="figure" target="#fig_0">2-C2</ref>). The local factor contributions were found using the Shapely Additive Explanations (SHAP) algorithm <ref type="bibr" target="#b13">[14]</ref>. We chose to use SHAP because it is theoretically grounded in game theory and generates consistent and intuitive explanations for an ML model.</p><p>We decided on a local contribution interface as we theorized it may help with all identified usability challenges: the screeners' lack of trust TR by demonstrating that the model relies in part on similar factors as the human screeners in making decisions, difficulty reconciling disagreements DIS by highlighting differences in the human and model's logic, unclear prediction target CT by providing a concrete explanation of the scores' meaning, and concerns about ethics ETH by making critical thinking about relevant factors easier.</p><p>Feedback. Initial interview feedback suggested that screeners were most interested in the Case-Specific Details interface, so it was kept as the default option. Additionally, screeners said that in their usual workflow, they would list "Risk" and "Protective" factors side by side. To mirror this, the updated version of this interface has a split-view toggle (Figure <ref type="figure" target="#fig_0">2H</ref>) that shows negatively and positively contributing factors in two side-by-side tables).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sandbox: Investigating "What Ifs"</head><p>The Sandbox interface allows users to experiment with and see how the model prediction would change if factors differed. It has two parts:</p><p>(1) The Experiment with Changes box (Figure <ref type="figure" target="#fig_1">3A</ref>) allows users to Table <ref type="table">3</ref>. The proposed SIBYL interfaces (left column), the challenges they were theorized to address (middle column, using the codes from Table <ref type="table" target="#tab_0">1</ref>), and the reasons we expected these interfaces to address the given challenges (right column). TR: Lack of trust in the model. DIS: Difficulty reconciling disagreements. CT: Confusing prediction target. ETH: Ethical concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interface</head><p>Challenges Addressed</p><p>How does it address the challenge?  <ref type="figure" target="#fig_1">3B</ref>) shows the resulting prediction if each Boolean factor value was individually reversed (ie. true to false or false to true).</p><p>Following the consistent design principle, we adopted a similar tabular design and use the red/blue up/down arrows to highlight changes in risk scores. This interface was added based on feedback from the interviews described in Section 4.3. We theorized it may help with difficulty reconciling disagreements DIS by allowing screeners to test their theorized justifications, and with concerns about ethics ETH by making more detailed consideration about the model's output easier.</p><p>Feedback. Screeners expressed concern that the Sandbox interface may be misconstrued as suggesting specific actions or reflecting realworld causal structures. However, screeners also said that they saw value in this interface as a supervision tool, used to review model predictions and human decisions rather than being actively used during the decision-making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Similar Cases: Investigating Past Cases</head><p>The Similar Cases interface shows the complete history of child welfare involvement with past cases that had similar factor values. The similar cases are found using a Nearest Neighbors algorithm. For design purposes, the algorithm used weights all factors equally. This interface includes a timeline for the current case and each similar case, and highlights events such as referrals to child welfare services, investigations, and removals. To facilitate comparative analysis, cases are lined up row by row and share the same timeline.</p><p>The interface was added as we theorized it may help with screeners' lack of trust TR by demonstrating past performance, and concerns about ethics ETH by providing a deeper look into an individual case.</p><p>Feedback. Screeners were concerned that this interface may cause poor decision-making. They explained that basing decisions about a current case on past cases that seem similar is discouraged, as this can lead to biases or self-fulfilling prophecies. Therefore, it was decided that this interface would not be included in a decision-making tool. However, county officials pointed out that it could be used retroactively (outside of decision-making) to investigate unusual predictions made by the model for the purposes of model evaluation. Due to these concerns, we decided not to include this interface in our formal user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Global Factor Importances: Understanding the Model</head><p>The About Model interfaces offer information about the model's general logic, outside of the context of a individual prediction.</p><p>The first About Model interface is the Global Factor Importance explanation. This interface shows a global explanation in the form of the general, relative importance of each factor. It also provides a brief description of the model architecture and logic, as well as its performance metrics. A visualization for this interface can be found in the Appendix, Figure <ref type="figure">9</ref> The global factor importance rankings were found using the Permutation Importance algorithm <ref type="bibr" target="#b2">[3]</ref>. This algorithm computes the change in model performance if each factor is permuted individually. It therefore describes how closely each factor is linked to model performance.</p><p>This interface was added as we theorized it may help screeners build trust in the model TR by seeing how it generally makes predictions, and because it may clarify the meaning of the prediction target CT.</p><p>Feedback. Screeners said that the Factor Importance interface seemed intuitive, but may provide too much information to be practical during active decision-making. Instead, they said it may be useful for training and education.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Factor Distributions: Understanding Past Predictions</head><p>The second About Model interface is the Factor Distributions explanation, which gives a quick retrospective view of how the model performed in the past. This interface shows the distribution of factor values among past cases that were given a particular score (Figure <ref type="figure" target="#fig_2">4A</ref>), as well as the percentage of children with that score who were removed from the home (Figure <ref type="figure" target="#fig_2">4B</ref>).</p><p>Depending on the factor type (Figure <ref type="figure" target="#fig_2">4C</ref>), the Factor Distributions explanation uses one of three visualizations to show the value distribution of children who received the selected prediction score. For binary factors, a progress-bar like design is used. For numeric factors, a boxand-whiskers plot shows the global minimum and maximum values for a feature, and the minimum, first quartile, third quartile and maximum for the selected risk score. For categorical factors, segmented bars are used to encode a categorical value distribution; hovering over a segment provides more information about the categories and their corresponding percentages.</p><p>This interface was added as we theorized it may help screeners build trust in the model TR by seeing how it generally performs, and it may clarify the value of the prediction target CT by showing how it relates to a more tangible output of removals from the home.</p><p>Feedback. Like the Factor Importance interface, screeners expressed concern that the Factor Distributions interface shows too much information for use during active decision-making. However, they said it may be useful for training and finding gaps in provided services. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USER STUDIES</head><p>To evaluate SIBYL, we ran two formal user studies.</p><p>Our first study involved 12 data and/or social scientists. We chose to run a study with non-experts first in order to fix immediate usability problems and iterate on the UI/UX elements of the tool. Although these participants had no prior experience in child welfare screening, it was possible for them to understand the process intuitively enough to use the tool in roughly the same way as experts would.</p><p>In our second study, we engaged 13 collaborating child welfare screeners (experts), 2 of whom completed the task while video conferencing and screen-sharing with us. We discuss the results of both user studies in this section. Data used: For privacy reasons, we used only synthetically generated and deidentified data in this section. Data for different factors was generated using the CTGAN synthetic data generation algorithm <ref type="bibr" target="#b25">[26]</ref>, in which a generative model is learned from the real data and samples that resemble the real data are drawn from the model.</p><p>Case descriptions were real paragraph-form narratives provided by concerned parties during past referrals. Names were changed by a representative of the county for de-identification. For example:</p><p>"Caller (teacher) says Abby (age 5) came into school with bruise on arm. Caller says Abby often comes in bruised. Abby told teacher she fell off bike. Teacher asked Abby's mother about this and mother started acting aggressive..."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Study Procedure</head><p>Participants were first shown a short video explaining how to use SIBYL. Next, they were shown 7 case descriptions, accompanied by model predictions and SIBYL interfaces with simulated data. Participants were then asked to make a screen-in/screen-out decision, and to answer some reflection questions. These questions included 1) five-point Likert-scale style questions about how much participants trusted the model and how confident they felt in their decisions, 2) multiple choice questions about which SIBYL interfaces were helpful, and 3) free response questions about trust in the model and general feedback.</p><p>In total, experts completed 73 individual case analyses, and nonexperts completed 75. The procedure for this user study is summarized in Figure <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Study Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Helpful Interfaces</head><p>To address RQ2, we analyzed the self-reported helpfulness of each augmentation interface.</p><p>The Case-Specific Details interface was by a large margin considered the most helpful interface, by both experts and non-experts. It was labelled as being helpful by experts in 91.8% of case analyses, and by non-experts in 90.7% of case analyses. This was significantly higher than Sandbox (experts: 16.4%, non-experts: 22.6%) and Factor Distributions (experts: 20.5%, non-experts: 8.0%). Factor Importance was never listed as helpful by either group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Reliance on Sibyl</head><p>Unsurprisingly, non-experts were more likely to report listening to the model without considering the added information in SIBYL. One non-expert participant commented, "No idea what is going on in this case description -so completely defer to the model here." Another non-expert participant commented "I found the score useful -and used it as a justification for screening out without exploring in detail all the factors." Additionally, non-experts reported that they used the model "a lot" or "a great deal" on 46.4% of cases, while experts chose these options in 15.6% of cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Impact on Trust</head><p>The SIBYL interfaces were reported to both increase and decrease expert users' trust in the model for different reasons. To analyse why this might occur, we thematically analyzed the responses to the open question "What made you trust the model more or less?" We gathered 56 responses to this question and divided the answers into two categories, based on the corresponding answer to the 5-point Likert scale question "How much did you trust the model's prediction for this case?" 43 free-responses corresponded to trusting the model "a great deal," "a lot," or "a moderate amount," while 13 corresponded to trusting the model "a little" or "not at all." For all but three of these answers, the response corresponded with the degree of trust listed (ie., participants who reported trusting the model provided reasons why they trusted the model more). We do not include the three exceptions in our analysis.</p><p>To conduct the thematic analysis, we sorted according to which (if any) specific SIBYL interfaces were referenced, and further identified what specific elements of the interfaces were referenced. For answers that did not reference a specific interface, we identified themes in the responses, such as agreement with the model, references to general information provided by the model, or inaccuracies in the model. With these two coding mechanisms, we sorted the responses into 7 themes that corresponded with increasing trust, and 5 that corresponded with decreasing trust. Table <ref type="table">4</ref> lists these themes. We see that agreeing with the model's score increases trust of the model the most. Beyond this, the Case-Specific Details page was frequently cited as increasing model trust, either due to specific factors listed or more general elements of the page, such as the number of factors. Trust was reduced when there was confusion or inconsistencies in the presented information, or when the model did not consider important factors that participants knew about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Information Presentation</head><p>To address RQ3, we categorize and summarize the comments made by users regarding SIBYL design choices, as well as the steps we took to address them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Too many factors shown</head><p>The model was originally trained on over 400 factors, all of which were presented in the SIBYL interface, but many of these factors have zero or near-zero weight. For example, one participant commented: "Too many factors listed. I only want to see the material risk and protective factors."</p><p>Our updated version of SIBYL only shows 10 factors by default, with an option to show more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Confusion caused by correlated factors</head><p>The model uses some engineered factors, resulting in factors that have deterministic relationships. For example, there is a numeric factor called AGE OF CHILD, and then a set of binary factors referring to each age group: i.e. CHILD IS LESS THAN 1 YEAR OLD, CHILD IS BETWEEN THE AGES OF 1 AND 3, etc. These factors may cause confusion when shown directly to users. In addition to increasing the cognitive load on users without providing additional useful information, explanations using these factors may reveal seemingly contradictory or unusual relationships. For example, an age category may contribute greatly, while the numeric age factor does not.</p><p>Additionally, having these correlated factors causes confusion on the sandbox page, as it is possible to change one factor without changing all of the other deterministically-correlated factors in its set. One participant commented, "I'm not sure, in the sandbox, if I change one feature, other features will be changed automatically."</p><p>To solve these problems, we combined the correlated factors in the SIBYL interface, forming categorical factors out of binary one-hot encoded factors, and summing the additive contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Confusion caused by Boolean terminology</head><p>One source of confusion was the method of displaying Boolean factors. In our original design, we displayed the description of the factor, with a value of True or False. This is the most accurate way of representing the model's logic, but it is not the most intuitive way for our end-users. One participant said, "The 'true' and 'false' is hard to interpret... Would rather have a positive statement (e.g., no perpetrator named)" Therefore, our final version of SIBYL instead states only true statements about the child -including by negating descriptions of false factors. For example, the factor CHILD HAS SIBLINGS with a value of False will be displayed as CHILD DOES NOT HAVE SIBLINGS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND LIMITATIONS</head><p>In this section, we summarize general lessons learned regarding how augmentation tools can improve the usability of ML models in the child welfare domain. We speculate that these lessons may generalize to other domains with similar context factors -i.e. those where users with high domain expertise and low technical expertise make highimpact/high-risk decisions with the help of ML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Helpful Augmentation Tools</head><p>In Section 6.2.1, we noted that our user study participants did not frequently list our About Model or Sandbox interfaces as helpful, despite our hypothesis that these interfaces could address some usability challenges as noted in Table <ref type="table">3</ref>. While further user studies will be required to identify exactly why more interfaces were not used, a few potential reasons come to mind. First, screeners are used to making decisions fairly quickly, and may not have enough time to parse and consider case details while investigating more than one interface. Second, the About Model interfaces may not be very relevant to screeners in the midst of a specific case, and may be more helpful if presented to screeners prior to screening. Third, the experimentation functionality of the Sandbox page is only useful if screeners have a specific what-if question in mind, while the "flipped feature" aspect may be somewhat redundant -it shows similar information to the Case-Specific Details page (the relative importance of each factor), but it a way that appears to be less intuitive to users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Accuracy versus Fidelity</head><p>Robnik-Sikonja and Bohanec <ref type="bibr" target="#b18">[19]</ref> define the accuracy of an explanation as how well it generalizes to other unseen examples (i.e., how accurately these rules predict what happens in the real world), and fidelity as how well an explanation describes the model itself.</p><p>Our users were mostly interested in getting accurate explanations that provided information about the case at hand. As evidenced by all three findings about the interface design (Section 6.2.4), users wanted to receive information about the model in a language and format that mirrored their own, not the format used by the model itself. This is also evidenced by design requests like using the terms risk and protective factors, rather than the more ML-centric terms negative and positive features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">The Importance of Interpretable Factors</head><p>Simple models, such as regression, are often cited as being inherently interpretable <ref type="bibr" target="#b19">[20]</ref>. However, this case study suggested that even simple models may cause confusion in users, and lead to challenges when attempting to explain model predictions for decision making.</p><p>Instead, our work found that, for the purposes of making models usable for end-users, the interpretability of the model factors may be most important. In our study, the screeners were often confused when explanations used factors that did not have clear implications on risk. For example, in our user study, one participant said "... 2 parents have missing date-of-birth is shown as a significant blue bar which I can't imagine is protective." Additionally, as discussed in Section 6.2.4, one-hot-encoded factors were not interpretable, and many of the reasons screeners trusted the model more or less (Table <ref type="table">4</ref>) related to the specific factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Non-Applicable Usability Challenges</head><p>One interesting finding of our work was the usability challenges we did not see evidence of in this case.</p><p>For example, one possible use of explanations is to give humans the ability to actively correct errors in a model's logic. We did not see evidence of this behavior from our users, however. There are several possible reasons for this. First, our users are making decisions in a very limited time, and do not have additional time to review the model's quality. Second, our users are thoroughly analyzing every case on their own and were only using the model as an extra flag. Finally, the users already have some discomfort about the model, likely due to the high associated risk. As a result, our users tended to discount the model altogether if they did not believe it was correct about a particular case.</p><p>Additionally, users expressed almost no interest in learning about the model itself through explanations. A common explanation need addressed by the literature <ref type="bibr" target="#b12">[13]</ref> is to understand how the model works (model transparency), possibly for debugging. In our case study, however, only once (see Section 4.3, Item 3) did any screener express interest in understanding the details of how the model worked under the hood -and even then, they were mostly looking for a broad overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Cognitive Biases</head><p>Wang et. al. <ref type="bibr" target="#b24">[25]</ref> introduced a list of the cognitive biases explanations can help address. Our experience with child welfare screeners addition-  <ref type="figure">5</ref>. The procedure for our formal user study. Our participants were first shown the description of potential abuse from a child welfare referral, as well as the corresponding ML prediction risk score. Next, they were given the opportunity to interact with the SIBYL interfaces. Once they were ready, they were asked to make a screen-in or screen-out decision, and then asked a series of reflection questions. Table <ref type="table">4</ref>. Summary of answers to the question "what made you trust the model more/less". The top section lists reasons for having "a great deal", "a lot" or "a moderate amount" of trust in the model. The bottom section lists reasons for having "a little" or "not at all" trust in the model. The first column lists the general themes we found in the answers. The second column lists the number of answers that fell within each theme. The final column lists selected answers for each theme. "The past number of child welfare involvements (listed in the features listed)", "The risk factors involved, especially prior placements, benefits, and current CPS involvement" The number of factors listed (Details page) 5 "Very few risk factors", "The lack of protective factors" The score agreed with screener intuition 10 "...not residing with the alleged perpetrators which I would assume would reduce the risk score", "Model prediction makes sense" The explanation agreed with screener intuition 3 "Risk factors made sense for the model prediction number" General comments about sandbox page 1 "Details under sandbox of why the risk level was so high" General comments about the explanation providing more information or understanding 6</p><p>" Allowed for more understanding", "History clarification"</p><p>Factors that decreased trust Category # of answers Sample answers A specific, key factor was not considered by the model 3 "This is a case for law enforcement, not CPS", "...it may have been handled during the open case" The importance weighting of factors was off (Details page) 2 "Young, vulnerable children being left alone is still cause for concern, despite past involvement" The score disagreed with screener intuition 3 "Seems high, no health history, no real proof of any drug use, no proof child is at any risk of abuse" There was some confusion about information presented 2 "There were discrepancies between the info in the referral and the info provided by the tool" The screener wanted more information 2 "I would want to see other referrals for the family" ally suggested some of these cognitive biases could be encouraged by the explanations and other forms of further information. For example:</p><p>1. Representativeness Bias <ref type="bibr" target="#b24">[25]</ref>: Case-based explanations that offer similar examples to the case at hand (such as our Similar Cases page) risk encouraging users to make decisions based on similarities to another case.</p><p>2. Causation vs Correlation: Counterfactual-based explanations, which consider how the model prediction would change under different circumstances, made participants more likely to interpret the explanations as containing information about the causal structure of the world.</p><p>3. Availability Bias <ref type="bibr" target="#b24">[25]</ref>: A factor-contribution explanation that is sorted in ascending order (and therefore lists negative contributions first) may result in different decisions than one that is sorted in descending order (and therefore lists positive contributions first) due to availability bias, which causes humans to put too much importance on recent or memorable events or information.</p><p>Further work and user studies may better reveal the extent to which these biases are caused or exacerbated by ML augmentation tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Limitations</head><p>Our work has some limitations that may be addressed in future work.</p><p>Our analysis focused on the qualitative comments and self-reported confidence measures provided by participants, which we used to measure the usability and perceived usefulness of the different explanation interfaces. Because the data used in our formal user study was synthetic, and therefore not associated with real-world outcomes, we did not measure the efficacy of the screening decisions made by these users. Arguably, it would be possible to quantitatively measure certain effects of SIBYL -such as changes in the quality of decisions made, or differences in results depending on how the platform was deployed -or even to use SIBYL to identify biases in human decision-making. However, this was beyond the scope of this paper, which sought to explicitly augment human decision-making, rather than potentially cross the line into partially automating it.</p><p>The literature on ML usability would benefit from a complete, methodological investigation into the ML usability challenges present across domains, to extend our sample subset introduced in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In this work, we identified the ML usability challenges associated with the domain of child welfare screening. We found one promising tool (factor contributions, on the Case-Specific Details interface) for mitigating many of these challenges, and pinpointed important design decisions that must be made to maximize the usefulness of this tool. Future work should empirically investigate the effect this tool has on decision-making, quantitatively measure how well it mitigates existing usability challenges, and explore innovative visualization designs to better solve the remaining challenges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The final version of the "Case-Specific Details" interface after factoring in user study feedback. This interface shows how particular factors contribute to predictions made by ML models about child welfare. Labeled elements are as follows: (A) The risk score for the case (1-20). (B) Categories for each factor, such as demographics (DG) or referral history. (C) A short description of each factor. (D) The value of numeric or categorical factors. (E)The contribution of each factor (the table can be sorted in ascending or descending order of contribution). (F) UI components for searching by factor name or filtering by category, enabled when the full factor list is shown. (G) A button for switching between a view that shows only the top 10 most contributing factors and one that shows all factors. (H) A button for switching between a single-table view and a side-by-side view, which splits factors that increase and decrease risk. (I) A sidebar for switching between different explanation types, as described Section 5.</figDesc><graphic coords="5,316.31,373.73,250.94,71.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Sandbox visualizations. (A) Users can change up to four factor values at a time, and the new score will be displayed on the top right corner. (B) The table lists the resulting prediction if each Boolean value is individually reversed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Factor Distribution visualizations. Users specify a risk score of interest (A) and observe the percentage of children with that score who were removed from the home (B). Three kinds of visualizations (C) are proposed to show the value distribution of children with the selected prediction score.</figDesc><graphic coords="7,53.63,88.01,250.70,73.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Factors 8 "</head><label>8</label><figDesc>being shown protective and risk factors (Details page) The details and the risk and protective factors and the contribution they have" "Info in the details and risk factors" Specific factors listed as risk or protective (Details page) 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>List of challenges that could negatively impact the usability of an ML model.</figDesc><table><row><cell cols="2">Usability Challenge Code</cell><cell>Mitigating Tools</cell></row><row><cell cols="2">Lack of TRust TR</cell><cell>Global explanations, local explanations, performance met-</cell></row><row><cell></cell><cell></cell><cell>rics, historical predictions and results</cell></row><row><cell cols="2">Difficulty Reconciling human-ML Disagreements DIS</cell><cell>Local explanations</cell></row><row><cell cols="2">Unclear Consequences of actions CON</cell><cell>Cost-benefit analysis, historical predictions and results</cell></row><row><cell cols="2">Lack of ACcountability or protections from accountability ACC</cell><cell>Local explanations, performance metrics</cell></row><row><cell>Ethical Concerns (ex. possible bias, concerns about</cell><cell>ETH</cell><cell>Global explanations, local explanations, ML fairness met-</cell></row><row><cell>oversimplification)</cell><cell></cell><cell>rics, historical predictions and results</cell></row><row><cell>Confusing or unclear prediction Target (ie. the measure</cell><cell>CT</cell><cell>Cost-benefit analysis, further analysis of prediction target</cell></row><row><cell>predicted by the model has an unclear meaning or</cell><cell></cell><cell>impact</cell></row><row><cell>significance)</cell><cell></cell><cell></cell></row><row><cell>Unhelpful prediction Target (ie. the measure predicted by</cell><cell>UT</cell><cell>Retrain model with new prediction target</cell></row><row><cell>the model is not relevant to the required decision)</cell><cell></cell><cell></cell></row><row><cell cols="2">RQ2 What interfaces can be helpful in mitigating these ML usability</cell><cell></cell></row><row><cell cols="2">challenges? We designed and implemented five augmentation</cell><cell></cell></row><row><cell cols="2">interfaces, making up our SIBYL tool, described in Section 5.</cell><cell></cell></row><row><cell cols="2">Based on our interviews and a formal user study, described in</cell><cell></cell></row><row><cell cols="2">Section 6, we suggest that local factor contributions are most</cell><cell></cell></row><row><cell>useful for this domain.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Rhema Vaithianathan, Diana Benavides Prado, Megh Mayur, Athena Ning, and Larissa Lorimer for their guidance throughout the process of applying machine learning explainability to child welfare, for connecting us to the child welfare domain experts, and for providing us with their trained model and dataset. We thank the child welfare experts at Larimer County Department of Human Services for their invaluable insights and feedback on the tool. We thank Iulia Ionescu, Sergiu Ojoc, Ionut Radu, and Ionut Margarint for their work on developing the Sibyl application. We thank Arash Akhgari for his work on our diagrams and visualizations. We thank Michaela Henry for support in project management and insights. We thank the participants of both user studies for their time and feedback. Finally, we would like to thank our anonymous reviewers for their comments and suggestions. This work is supported in part by NSF Award 1761812.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX: MITIGATING TOOL DEFINITIONS</head><p>Here, we define the types of mitigating tools discussed in this paper and Table <ref type="table">1</ref>.</p><p>Global Explanation: An explanation of a model's general logic, achieved through methods such as quantifying the overall importance of features or visualizing the model boundary <ref type="bibr" target="#b1">[2]</ref>.</p><p>Local Explanation: An explanation as to why a model made an individual prediction, achieved through methods such as quantifying how much each feature contributed to this particular prediction <ref type="bibr" target="#b1">[2]</ref>.</p><p>Cost-Benefit Analysis: A measurement of the expected total reward from taking an action, defined by the expected benefits minus the costs <ref type="bibr" target="#b8">[9]</ref>. In the case of machine learning, this would involve providing information about the expected results of a prediction alongside the prediction itself.</p><p>ML Fairness Metrics: Mathematical approaches to measuring the level of bias present in models <ref type="bibr" target="#b4">[5]</ref> B APPENDIX: DESIGN MOCKUP REVIEWS  show the original design mockups that were presented to child welfare screeners, and describe the feedback and changes that were made as a result of this interview. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C APPENDIX: USER STUDY QUESTIONS ASKED</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Summary of Key Findings</title>
				<imprint>
			<date type="published" when="2018">2018. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08608</idno>
		<title level="m">Towards A Rigorous Science of Interpretable Machine Learning</title>
				<imprint>
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All Models are Wrong, but Many are Useful: Learning a Variable&apos;s Importance by Studying an Entire Class of Prediction Models Simultaneously</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dominici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="81" />
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human-Centred Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gillies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fiebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heloir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nunnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alessandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tilmanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caramiaux</surname></persName>
		</author>
		<idno type="DOI">10.1145/2851581.2856492</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</title>
				<meeting>the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems<address><addrLine>San Jose California USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<title level="m">Machine Learning Glossary: Fairness. Google Developers</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gamut: A Design Probe to Understand How Data Scientists Understand Machine Learning Models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300809</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems -CHI &apos;19</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems -CHI &apos;19<address><addrLine>Glasgow, Scotland Uk</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Can an Algorithm Tell When Kids Are in Danger</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hurley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
			<biblScope unit="page">30</biblScope>
			<pubPlace>New York Times</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<meeting><address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-04">Apr. 2017</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">How Cost-Benefit Analysis Process Is Performed. Investopedia</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kenton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lifetime Prevalence of Investigating Child Maltreatment Among US Children</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wildeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jonson-Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Drake</surname></persName>
		</author>
		<idno type="DOI">10.2105/AJPH.2016.303545</idno>
	</analytic>
	<monogr>
		<title level="j">American Journal of Public Health</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="280" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858529</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2016 CHI Conference on Human Factors in Computing Systems<address><addrLine>San Jose California USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2865027</idno>
		<title level="m">RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2019-01">Jan. 2019</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="299" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Mythos of Model Interpretability</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ICML Workshop on Human Interpretability in Machine Learning</title>
				<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>IMLS</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="31" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, California</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explainable machine-learning predictions for the prevention of hypoxaemia during surgery</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Vavilala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Eisses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Liston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-W</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41551-018-0304-0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="749" to="760" />
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Nested Model for Visualization Design and Validation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2009.111</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="921" to="928" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">National Highway Traffic Safety Administration (NHTSA)</title>
		<imprint>
			<date type="published" when="2017-09">Sept. 2017</date>
		</imprint>
	</monogr>
	<note>Automated Vehicles for Safety. NHTSA</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Effects of a False Allegation of Child Sexual Abuse on an Intact Middle Class Family. IPT</title>
		<author>
			<persName><forename type="first">D</forename><surname>Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="226" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Human and Machine Learning: Visible, Explainable, Trustworthy and Transparent, Human-Computer Interaction Series</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bohanec</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-90403-09</idno>
		<editor>J. Zhou and F. Chen</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="159" to="175" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Perturbation-Based Explanations of Prediction Models</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0048-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Spinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934629</idno>
		<title level="m">explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2020-01">Jan. 2020</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InfoVis</title>
				<meeting><address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Implementing a Child Welfare Decision Aide in Douglas County</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vaithianathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kithulgoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mayur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Benavides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Prado</surname></persName>
		</author>
		<author>
			<persName><surname>Putnam-Hornstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Developing Predictive Models to Support Child Maltreatment Hotline Screening Decisions: Allegheny County Methodology and Implementation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vaithianathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Putnam-Hornstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maloney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Designing Theory-Driven User-Centric Explainable AI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300831</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems -CHI &apos;19</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems -CHI &apos;19<address><addrLine>Glasgow, Scotland Uk</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling tabular data using conditional GAN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Skoularidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cuesta-Infante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864499</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
