<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hamza</forename><surname>Elhamdadi</surname></persName>
							<email>helhamdadi@umass.edu</email>
						</author>
						<author>
							<persName><forename type="first">Shaun</forename><surname>Canavan</surname></persName>
							<email>scanavan@usf.edu</email>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Rosen</surname></persName>
							<email>prosen@usf.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of South Florida</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of South Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AffectiveTDA: Using Topological Data Analysis to Improve Analysis and Explainability in Affective Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D92D8286AC2732BE9646C50706375D14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Affective computing</term>
					<term>topological data analysis</term>
					<term>explainability</term>
					<term>visualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Our affective computing visualization provides numerous options for comparing and contrasting the data. For example, the small multiples view (left) is comparing a male (left) subject to a female (right) subject considering different subsets of facial landmarks (columns), across emotions (rows) of anger , disgust, fear , happiness, sadness, and surprise. Each point represents one facial pose. The embedding graph (top right) compares all facial poses across all emotions, in this case showing the female full face using MDS on non-metric topology. The 3D landmarks (lower right) show a single facial pose per emotion. Settings are selected on the bottom.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Affective computing, computer-based detection of human affects, has applications that span education (e.g., judging learners' confidence), healthcare (e.g., judging pain), and product marketing (e.g., measuring consumers' response to products). Early work in measuring affect began in the late 1960's spearheaded by Ekman and Friesen <ref type="bibr" target="#b21">[23]</ref>. Their work culminated in a classification of six basic emotions: anger, disgust, fear, happiness, sadness, and surprise <ref type="bibr" target="#b22">[24]</ref>, which were later expanded <ref type="bibr" target="#b20">[22]</ref>. The field of affective computing has seen significant growth since the seminal work from Rosalind Picard <ref type="bibr" target="#b65">[65]</ref>. The vast majority of research in affective computing has been focused on machine learning algorithms trained on emotion data to classify affect. Like many machine learning solutions, these neural networks focused on classifying the input emotion and ignored data inspection and decisionmaking explainability.</p><p>To inspect the data, an effective visual representation of emotion data must address numerous challenges. First, the affect data are quite large, captured by multiple high-speed video cameras. Fortunately, previous affective computing research already partially addressed this issue by reducing the data to 83 landmark points tracked temporally. Nevertheless, the problem remains challenging because patterns in emotion occur over extended time periods, represented by a series of 83-landmark poses. Furthermore, patterns of interest may occur in different time sequences lasting for different lengths of time, making alignment and comparison non-trivial. Finally, changes in landmarks are simultaneously subtle and subject to noise from the extraction process, making them difficult to observe.</p><p>This paper presents a visual analytics approach utilizing Topological Data Analysis (TDA) to examine emotion data respective and irrespective of time. By using TDA to address this problem, our approach can capture and track the topological "shape" of facial landmarks over time in a manner robust to noise <ref type="bibr" target="#b18">[20]</ref>. After analysis, the data are presented for investigation using familiar visualizations, e.g., timelines (see Fig. <ref type="figure" target="#fig_5">5</ref> top) and scatterplots (see Fig. <ref type="figure">1</ref> top right), and through landmark-based representations (see Fig. <ref type="figure">1</ref> bottom right or Fig. <ref type="figure">10</ref>). These interfaces enable tracking facial movement, comparing emotions, and comparing individuals, while also providing the ability to derive precise explanations for features identified in the data.</p><p>A natural question at this point would be, why is TDA well-suited to this problem? Our approach utilizes one of the foundational tools of TDA, namely persistent homology. There are four main advantages to this tool. <ref type="bibr" target="#b0">(1)</ref> Persistent homology has a solid mathematical grounding, and its output is explainable. <ref type="bibr" target="#b1">(2)</ref> Persistent homology extracts homology groups, which in our context are (connected) components and tunnels/cycles. These fundamental shapes match well with the shapes of a face. <ref type="bibr" target="#b2">(3)</ref> The homology groups are extracted at multiple scales, without the need to specify any thresholds or other parameters. This means that persistent homology captures all of the topological structures without any user intervention 1 . (4) Finally, it classifies features by their importance with a measure called persistence, which automatically differentiates topological signal from noise <ref type="bibr" target="#b12">[13]</ref>. The specific contributions of this paper are:</p><p>1. a mapping of affective computing data to TDA (see Sect. 4), including a novel non-metric formulation of geometry for faster and more accurate topology extraction (see Sect. 4.3); 2. a visual analytics interface that enables analyzing, comparing, and contrasting multiple data configurations (see Sect. 6); 3. an evaluation that uses our methodology to explain features in data that were extracted by state-of-the-art emotion detection machine learning algorithms (see Sect. 7.2); and 4. an evaluation of the ability of TDA to differentiate emotions within the same individual (see Sect. <ref type="bibr" target="#b6">7</ref>.3) and differentiate multiple individuals showing the same emotion (see Sect. 7.4).</p><p>Perhaps most importantly, our approach opens the door to explainability in a way that may help to unlock open questions in the affective computing community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND IN AFFECTIVE COMPUTING</head><p>Affective computing has applications in fields as varied as medicine <ref type="bibr" target="#b85">[85]</ref>, entertainment <ref type="bibr" target="#b30">[31]</ref>, and security <ref type="bibr" target="#b44">[45]</ref>. Most notably, the expression recognition sub-field focuses on detecting subjects' affective states automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Expression Recognition</head><p>While successful 2D facial-expression image recognition exists <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">49]</ref>, the approaches suffer from weaknesses, such as occlusion from, e.g., a rotating head. We focus our discussion instead on a few representative 3D facial recognition approaches. Zhen et al. <ref type="bibr" target="#b91">[91]</ref> developed a model that localized points within each muscular region of the face and extracted features that include coordinate, normal, and shape index <ref type="bibr" target="#b45">[46]</ref>. The features were then used to train a Support Vector Machine (SVM) <ref type="bibr" target="#b77">[77]</ref> to recognize expressions. Xue et al. <ref type="bibr" target="#b82">[82]</ref> proposed a method for 4D (3D + time) expression recognition, which showed promise differentiating difficult emotions, such as anger and sadness. The method extracted local patch sequences from consecutive 3D video frames and represented them with a 3D discrete cosine transform. Then, a nearest-neighbor classifier was used to recognize the expressions. Hariri et al. <ref type="bibr" target="#b35">[36]</ref> proposed an approach to expression recognition using manifold-based classification. The approach sampled the face by extracting local geometry as covariance regions, which were used with an SVM to recognize expressions. Some recent techniques showed that not all regions of the face carry the same importance in emotion recognition. Hernandez-Matamoros et al. <ref type="bibr" target="#b36">[37]</ref> found that segmenting the face based on the eyes and mouth resulted in improved expression recognition. Fabiano et al. <ref type="bibr" target="#b27">[28]</ref> further illustrated that different areas of the face carry different levels of importance for emotions, e.g., one subject happiness had more important features on the right eye and eyebrow, while embarrassment had more on the left eye and eyebrow. We utilize this information in our visualization design by targeting specific subsets of facial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Affective Computing in Visualization</head><p>There has been limited work in the visualization community on affective computing; what exists has been primarily focused on visualizing affective states, i.e., considering valence and arousal, not inspecting the landmarks used as input to affective computing algorithms.</p><p>Early work on visualizing affective states concerns the glyph-based Self-Assessment Manikin (SAM), which measures pleasure, arousal, and dominance of a person's affective state <ref type="bibr" target="#b6">[7]</ref>. Cernea et al. <ref type="bibr" target="#b8">[9]</ref> later described guidelines for conveying the user emotion through the use of widgets that depict the affective states of valence and arousal. The widgets employed emotion scents, hue-varied colormaps representing either valance or arousal, e.g., red and green represent negative and positive valance, respectively. Emotion-prints was an early system to provided real-time feedback of valance and arousal to users using touch-displays <ref type="bibr" target="#b9">[10]</ref>. More recently, Kovacevik et al. <ref type="bibr" target="#b46">[47]</ref> employed ideas from SAM and emotion scents to create a glyph for simultaneous representation of valence and arousal. Their research focused on video game players' and developers' awareness of emotions elicited from a particular gaming experience. For visualizing affect over extended periods, AffectAura provided an interface that enabled users to visualize emotional states over time for the purpose of reflection <ref type="bibr" target="#b56">[56]</ref>.</p><p>There has also been some work visualizing the affective state of multiple individuals using, e.g., virtual agents in collaborative work <ref type="bibr" target="#b10">[11]</ref> or using a visual analytics interface to access the emotional state of students in a classroom <ref type="bibr" target="#b86">[86]</ref>. Qin et al. <ref type="bibr" target="#b66">[66]</ref> created HeartBees, which was an interface to demonstrate the affect of a crowd using physiological data. The interface used an abstract flocking behavior to demonstrate the collective emotional state.</p><p>In contrast to all of these prior approaches, our work focuses on using TDA and visualization to investigate the data used in classifying expression, i.e., the input data, not the emotional state itself. There has been some recent work that looked at the explainability of deep networks in expression recognition, e.g., <ref type="bibr" target="#b62">[62]</ref>. These approaches focus on visualizing heatmaps that highlight what parts of the image most influenced decision making, not necessarily why.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>To evaluate our approach, we use the BU4DFE 3D facial expression dataset <ref type="bibr" target="#b84">[84]</ref>, which has been extensively used for expression recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b74">74]</ref>, 3D shape reconstruction <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b53">53]</ref>, face tracking <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b64">64]</ref>, and face recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b72">72]</ref>. The dataset contains 101 subjects (58 female and 43 male) from multiple ethnicities, including Caucasian, African American, Asian, and Hispanic, with an age range of 18-45 years old. Each modality has the six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. For each sequence, the expression is the result of gradually building from neutral to low then high intensity and back again. Each of the video sequences is 3-4 seconds in length.</p><p>The data are captured using the Di3D dynamic face capturing system <ref type="bibr">[17]</ref>, which consists of three cameras, two to capture stereo and one to capture texture. Passive stereophotogrammetry is used on each pair of stereo images to create the 3D facial pose models with an RMS accuracy of 0.2 mm. Each 3D model contains 83 facial landmarks (see Fig. <ref type="figure" target="#fig_1">2</ref>), which correspond to the key areas of the face that include the mouth, eyes, eyebrows, nose, and jawline. The landmarks are the result of using an active appearance model <ref type="bibr" target="#b13">[14]</ref> that detects the landmarks on the 2D texture images, which are aligned and projected into the corresponding 3D models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW OF THE PIPELINE</head><p>TDA has received significant attention in the visualization community, e.g., <ref type="bibr" target="#b73">[73]</ref>. We utilize a foundational tool of TDA, persistent homology, which has been studied in graph analysis <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b71">71]</ref>, high-dimensional data analysis <ref type="bibr" target="#b78">[78]</ref>, and multivariate analysis <ref type="bibr" target="#b68">[68]</ref>. We utilize persistent homology to capture the topology of the landmarks of each facial pose into a structure known as a persistence diagram. We then compare the topology of different subsets of facial poses to reveal their relationships. Our processing pipeline contains three main stages, which are fed into the visualization (see Sect. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 1:</head><p>The first stage is extracting the topology of a single facial pose. We offer two variations, a Euclidean metric-based approach (see Sect. 4.1) and a novel non-metric-based approach (see Sect.  Stage 3: Finally, using the topological dissimilarity, we utilize a variety of dimension reduction techniques to highlight different aspects of the dissimilarity between groups of facial poses (see Sect. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TOPOLOGICAL DATA ANALYSIS OF FACIAL LANDMARKS</head><p>We consider two variations for extracting the topology of facial poses, a Euclidean metric approach, followed by a novel non-metric variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Euclidean Metric Persistent Homology on Landmarks</head><p>Homology deals with the topological features of a space. Given a topological space X, we are interested in extracting the H 0 (X) and H 1 (X) homology groups, which correspond to (connected) components and tunnels/cycles of X, respectively 2 . In practice, there may not exist a single scale that captures the topological structures of the data. Instead, we use a multi-scale notion of homology, called persistent homology, to describe the topological features of a space at different spatial resolutions. We briefly describe persistent homology in our limited context. Nevertheless, understanding persistent homology can be daunting for those who are unfamiliar with it. For a high-level overview, see <ref type="bibr" target="#b79">[79]</ref>, or for detailed background, see <ref type="bibr" target="#b17">[19]</ref>.</p><p>To calculate the persistent homology of a single facial pose, we first calculate the Euclidean distance between all 83 landmarks. We then apply a geometric construction, the Rips complex, R(r), on the point set. In brief, for a given distance, r, the Rips complex has all 0-simplicies, i.e., points, for all values of r. A 1-simplex, i.e., an edge, between two points is formed iff r is greater than or equal to their distance. A 2-simplex, i.e., a triangle, is formed among three points iff r is greater than or equal to every pairwise distance between the points.</p><p>To extract the persistent homology (see Fig. <ref type="figure" target="#fig_1">2</ref>(a)), we consider a finite sequence of increasing distances, 0</p><formula xml:id="formula_0">= r 0 ≤ r 1 ≤ ••• ≤ r m = ∞. A sequence of Rips complexes, known as a Rips filtration, is connected by inclusions, R(r 0 ) → R(r 1 ) → ••• → R(r m )</formula><p>, and the homology of each is calculated, tracking the homomorphisms induced by the inclusions,</p><formula xml:id="formula_1">H(R(r 0 )) → H(R(r 1 )) → ••• → H(R(r m )).</formula><p>As the distance increases, topological features, i.e., components and tunnels, appear and disappear. The appearance is known as a birth event, r b i , and the disappearance is known as a death event, r d i . The birth and death of all features are stored as a multi-set of points in the plane, (r b i , r d i ), known as the persistence diagram, which is often visualized in the scatterplot display (see Fig. <ref type="figure" target="#fig_1">2(b)</ref>). From the points, we devise an importance measure, called persistence, which helps to differentiate signal from noise. The persistence is simply the difference between the birth and death of a feature, i.e., r d i − r b i . Furthermore, in visualizations of the persistence diagram, such as Fig. <ref type="figure" target="#fig_1">2</ref>(b), distance from the diagonal dotted line represents the persistence of a feature.</p><p>In addition to considering all the topology of all landmarks, we provide the user the functionality to consider only related subsets of features. In particular, they have the option of including/excluding jawline, mouth, nose, left/right eyes, and left/right eyebrows in the calculation of the topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interpolating Known Geometry</head><p>Our computation using facial landmarks ignores an important aspect of the data, namely the known connectivity between landmarks. In other words, landmarks of, e.g., the mouth, have known connectivity to their neighboring landmarks. Fig. <ref type="figure" target="#fig_2">3</ref>(f) shows this connectivity. This raises two questions. First, does our failure to consider this connectivity impact the features we extract, and second, how do we efficiently consider the connectivity?</p><p>We first consider using interpolation of the connectivity to supersample additional landmarks. For our experiment, we take the known connectivity and interpolate across each edge, such that points are no further than a user-defined ε apart. Fig. <ref type="figure" target="#fig_2">3</ref>(b) through Fig. <ref type="figure" target="#fig_2">3(e)</ref> show four examples with ever-smaller ε values. As expected, as ε gets smaller, the data looks increasingly similar to the known connectivity in Fig. <ref type="figure" target="#fig_2">3(f)</ref>.</p><p>We now consider the impact of the connectivity by comparing the persistence diagrams of H 1 features in the original data in Fig. <ref type="figure" target="#fig_2">3</ref>(a) to the lowest ε data in Fig. <ref type="figure" target="#fig_2">3</ref>(e). The persistence diagrams are clearly different (the H 0 features are also different but more difficult to observe pictorially). The difference is exceedingly important because it means using the 83 landmark points alone is insufficient to capture the topological structure of the data.</p><p>To overcome this limitation, we considered using the supersampled landmarks for calculations. However, there are three interrelated problems to this approach. (1) The first is the challenge of selecting an appropriate ε value. The smaller the value, the closer the representation is to the geometric structure. For example, Fig. <ref type="figure" target="#fig_2">3</ref>(c) appears sufficient for this example, but it is unclear if this is sufficient for all of the data, leading one to perhaps select an even smaller ε. (2) However, the second challenge is that the smaller the ε, the longer the computation time for detecting the topological features. Fig. <ref type="figure">4</ref>(a) shows that as ε is divided in half, the compute time grows exponentially. (3) The third related challenge is that the smaller ε, the greater the number of topological features generated. Fig. <ref type="figure">4(b)</ref> shows this extreme growth. To make matters worse, the vast majority of these features are topological noise with very low persistence. In other words, they do not contribute to our understanding of the shape of the face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Non-metric Variant of Persistent Homology</head><p>We instead use a novel modification to the persistent homology calculation to utilize this connectivity as follows. Instead of considering 83 landmark points, we consider the relationship between 81 landmark edges formed by the known connectivity of the landmark points (see Fig. <ref type="figure" target="#fig_2">3(f)</ref>). We calculate a distance matrix representation of the landmark edges, where the distance is the shortest Euclidean distance between line segments. Finally, we run persistent homology calculations on this distance matrix.</p><p>One immediate question should be the appropriateness of this configuration for persistent homology calculations, particularly considering that this representation breaks two important axioms of a metric space, namely the identity of indiscernibles and the triangle inequality. Fortunately, persistent homology calculations themselves do not explicitly require a metric space-they have a weaker requirement of inclusion <ref type="bibr" target="#b17">[19]</ref>. In other words, as long as in the filtration R(r i ) ⊂ R(r i+1 ), the calculation can proceed. The challenge is that the Rips complex does require that the underlying space is metric. We define a new non-metric Rips complex that satisfies the inclusion property, where: 0-simplicies, representing landmark edges, are present for all values of r; 1-simplicies appear when r is strictly greater than the non-metric distance between a pair of 0-simplicies; and 2-simplices appear when r is strictly greater than all of the non-metric distances of the three related 1-simplicies. Fortunately, this definition is similar enough to the standard Rips complex that careful ordering of inclusions (i.e., observing the strictly greater than cases) in the filtration allows us to utilize conventional persistent homology tools on our non-metric distances.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref>(f) shows the landmark edges and the persistence diagram of the associated H 1 features. Our non-metric approach overcomes all three limitations of supersampling. (1) The result is very similar to the output of the supersampling in Fig. <ref type="figure" target="#fig_2">3</ref>(e) without the need for specifying any ε parameter. (2) Furthermore, Fig. <ref type="figure">4</ref>(a) shows that the compute time for our non-metric approach is approximately the same as that of the original 83 landmark points. (3) Finally, Fig. <ref type="figure">4(b)</ref> shows the number of topological features output is small (i.e., we avoid outputting extraneous topological noise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPARING FACIAL POSE TOPOLOGY</head><p>Thus far, we have introduced a method for extracting the topological features from a single facial pose. We now describe how we compare the topology of multiple facial poses. We start by describing the notion of topological distance between persistence diagrams, which serves as a pairwise dissimilarity between them (see Sect. 5.1). Next, we discuss how dimension reduction is used on all pairwise dissimilarities to cluster, compare, and summarize changes in topology (see Sect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dissimilarity Between Poses</head><p>Once persistence diagrams are calculated, we wish to explore the relationship between them by performing pairwise comparisons of features of the persistence diagrams. This type of pairwise comparison is commonly performed using bottleneck or Wasserstein distance <ref type="bibr" target="#b18">[20]</ref>.</p><p>Intuitively speaking, these measures find the best match between the features of two persistence diagrams and report the topological feature of the largest distortion, in the case of bottleneck distance, or the average topological distortion, in the case of Wasserstein distance.</p><p>Technically speaking, consider two persistence diagrams, X and Y , let η be a bijection, with all diagonal points, (x, x), added for infinite cardinality <ref type="bibr" target="#b43">[44]</ref>. The bottleneck distance is W ∞ (X,Y ) = inf η:X→Y sup x∈X x − η(x) ∞ , and the 1-Wasserstein distance, which we use, is W 1 (X,Y ) = inf η:X→Y Σ x∈X x − η(x) ∞ . Our implementation computes the bottleneck and 1-Wasserstein distance for H 0 and H 1 features separately, and combines the results. In other words, for bottleneck,</p><formula xml:id="formula_2">W ∞ (X,Y ) = max(W ∞ (X H 0 ,Y H 0 ),W ∞ (X H 1 ,Y H 1 )), and for 1- Wasserstein, W 1 (X,Y ) = W 1 (X H 0 ,Y H 0 ) +W 1 (X H 1 ,Y H 1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Summarizing Topological Dissimilarity</head><p>Once the set of all persistence diagrams is calculated, we explore the relationship between them by calculating all pairwise dissimilarities between poses, forming a dissimilarity matrix representing all of the topological variations between facial poses. However, a dissimilarity matrix, such as this, is difficult to explore directly. We investigated several options to represent and evaluate the relationship between different facial poses and emotions. Importantly, each technique preserves a different aspect of the dissimilarity matrix, providing different perspectives on the data.</p><p>The first approach we used is 1D relative distance. In this approach, a keyframe or focal pose is selected by the user. All other facial poses are positioned by their relative distance (i.e., pairwise distance) to that keyframe. Relative distance perfectly preserves the relationship between the keyframe and all other frames. It does not, however, provide information about the relationship between other pairs of frames.</p><p>Next, we consider two dimension reduction techniques, with each using the pairwise dissimilarity matrix directly. We first consider Multidimensional Scaling (MDS) <ref type="bibr" target="#b48">[48]</ref>, which tries to preserve pairwise distances between the topology of poses. Second, we use t-SNE <ref type="bibr" target="#b76">[76]</ref> and UMAP <ref type="bibr" target="#b57">[57]</ref>, which attempt to preserve the clustering structure by considering a local neighbor. Both t-SNE and UMAP contain hyperparameters that can impact the structures visible to the user. We have performed a structured evaluation of various hyperparameters and found that the structures visible in our results are, by-and-large, stable across a wide variety of parameter values (see our supplement for an example). Therefore, we use the default parameters in our evaluation. To measure the dimension reduction quality for all methods, we cal-culate the goodness-of-fit using the Spearman rank correlation of the Shepard diagram (denoted in the lower right of images as Rank).</p><p>Note that none of these approaches directly consider time. Nevertheless, the temporal components of the data are presented in the visualization when relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUALIZATION</head><p>To examine the topological structure of facial landmark data, we built a visualization (see Fig. <ref type="figure">1</ref> and Fig. <ref type="figure">10</ref>) with the following design criteria:</p><p>[D1] provide multiple ways to evaluate temporal and non-temporal aspects of the data (e.g., animated, static, and non-temporal visualizations);</p><p>[D2] provide multiple conditional perspectives (e.g., bottleneck vs. Wasserstein, MDS vs. t-SNE vs. UMAP, etc.) on the topology;</p><p>[D3] allow comparison of data between two or more emotions;</p><p>[D4] allow for investigating subsets of landmarks; and</p><p>[D5] provide direct explanations for the topological differences between facial poses.</p><p>Small Multiples (Fig. <ref type="figure">1</ref>  . Each small multiple is shown using the visualization modality chosen in the settings at the bottom.</p><p>Embedding Graph (Fig. <ref type="figure">1</ref> top right) The primary visualization tool in our approach is the embedding graph, which is either a line chart or scatterplot representation of time-varying topological data for the selected emotions [D1]. For the line chart, time is plotted horizontally, while the 1D relative distance is plotted vertically (see Fig. Persistence Diagrams (Fig. <ref type="figure">10</ref>) When additional details about a given facial pose are desired, the persistence diagram captured by persistent homology is represented by a scatterplot [D5]. The persistence diagram plots feature birth horizontally and death vertically. In this context, H 0 features are represented as solid squares, and H 1 features are represented as rings. The size of each element is proportional to its persistence (i.e., importance). Furthermore, the distance from the dashed diagonal to a feature is also a measure of persistence.</p><p>Representative Components and Cycles (Fig. <ref type="figure">10</ref>) A byproduct of the calculation of persistent homology is a structure known as generators, which are the landmark elements that generated a particular topological feature. For H 0 , the generators are the 0-simplices representing the joining of two components. For H 1 features, the data are output in the form of a representative cycle 3 . Each topological feature is associated with a generator that we use to identify what input data generated that topological feature, with a general focus on the high persistence features in the data [D5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION</head><p>We evaluate our approach by first performing a detailed evaluation of two individuals-one female ('F001') and one male ('M001') from the BU4DFE expression dataset <ref type="bibr" target="#b84">[84]</ref>. We then evaluate the ability of our approach to differentiate individuals using the entire dataset of 101 subjects. Each of these individuals has approximately 600 facial poses (6 emotions × ∼ 100 frames per emotion). Since the data provided are large and time-varying, our approach allows conditional observation of the topology of emotions based upon individuals, emotions, selected subset of facial features (full face, eyes+nose, mouth+nose, eyebrows+nose), topological dissimilarity, and dimension reduction technique. Our evaluation looks at how these conditional comparisons can be matched to known phenomena in affective computing. We note that one of the coauthors of this paper, Shaun Canavan, is a researcher in affective computing and provided detailed feedback at every stage of the design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Implementation and Performance</head><p>We implemented our approach using Python for data management, non-metric distance, and dimension reduction calculations, ripser <ref type="bibr" target="#b5">[6]</ref> for persistent homology calculations, Hera <ref type="bibr" target="#b43">[44]</ref> for topological distance, and D3.js for the user interface. Persistent homology and topological dissimilarity are pre-calculated for all combinations of landmark subsets. Dimension reduction is performed at run-time, taking at most a few seconds; as this data is calculated, it is also stored in a short-term cache to improve performance. The user interface is interactive. Our source code is available at https://github.com/ USFDataVisualization/AffectiveTDA.</p><p>We evaluated the computational performance of the persistent homology and bottleneck and Wasserstein dissimilarity matrix calculations for F001 and M001 in Table <ref type="table">1</ref>. The calculations were performed on a Linux workstation with a 3.40GHz Intel i7-6700 CPU and 48 GB of RAM. In this table, we compare the metric landmark point-based approach and our novel non-metric landmark edge-based approach. Comparing persistent homology calculations, our non-metric approach took approximately twice as long as the metric approach. This is entirely attributable to the extra cost of calculating segment-segment distance (instead of point-point distance). The performance benefit of the non-metric approach comes with the calculation of the dissimilarity matrices, which saw a 10x − 15x speedup over the metric approach. This is attributable to the reduced number of noise features created by the non-metric approach, as described in Sect. 4.3. Overall, our approach saw a speedup of ∼ 7.5x. Table <ref type="table">1</ref>. Computation time for metric (M) and non-metric (NM) approaches to extract persistent homology features and calculate the dissimilarity matrix for all frames from each subject, including subsets of facial landmarks (full face, eyes+nose, mouth+nose, eyebrows+nose). The number of landmarks input to each method is similar, 83 for full face metric and 81 for non-metric. In addition, we show the average number of H 0 and H 1 features generated per frame. </p><formula xml:id="formula_3">|H 0 | |H 1 | Persistent Topological</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Relative Distance Topology and Action Units (AUs)</head><p>In affective computing, there are various approaches for recognizing expressions, as detailed in Sect. 2.1. One promising approach is the use of action units (AUs) <ref type="bibr" target="#b24">[25]</ref>, which are facial muscle movements linked to expression. AUs are represented as an intensity from [0, 5], where 0 is byproduct of a process called boundary matrix reduction, is output instead <ref type="bibr" target="#b19">[21]</ref>.</p><p>inactive, and &gt; 0 is an active AU, with higher values representing more intense movement. Specific configurations of active AUs have been shown to be useful for recognizing facial expressions <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b81">81]</ref>. AUs are generally created in one of two ways. Either an expert manually annotates video frames, or a machine learning algorithm extracts them from the data. While the former is a slow and tedious process, the latter is fast but lacks any explainability in measuring the activity of AUs. We automatically detect 17 AUs (see Table <ref type="table" target="#tab_2">2</ref>) using the publicly available OpenFace toolkit <ref type="bibr" target="#b3">[4]</ref>, which is commonly used in affective computing literature <ref type="bibr" target="#b75">[75]</ref>. However, coming from a machine learning model, the extracted AUs lack specific explainability.</p><p>We now demonstrate how our topology-based system can explain certain AU features detected by OpenFace by comparing the output of each. Our approach is as follows, since all sequences begin with a neutral pose, we consider relative distance with respect to the first frame of the sequence. We hypothesized that we would observe similar signals in the AUs and their associated facial features using our topology-based approach. Fig. <ref type="figure" target="#fig_5">5</ref> shows two examples of this relationship. In Fig. <ref type="figure" target="#fig_5">5(a)</ref>, we compare the activity of the nose and eyes to AU45 (blink) for the F001 disgust emotion. The relative distance shows three clear spikes at the same frames as AU45 (approximately frames 28, 52, and 75). However, AU45 does not tell the entire story of the activity that the topology is capturing.</p><p>Instead, we hypothesize that the topology is a combination of multiple AUs. Fig. <ref type="figure" target="#fig_5">5(b)</ref> shows an example comparing the mouth+nose to AU14 (dimple) and AU25 (lips part) for the F001 fear emotion. In this case, a linear combination of both AUs seems to capture a more complete picture of the activity represented in the topology. A broader analysis of both subjects, multiple emotions, and multiple facial features, as seen in Fig. <ref type="figure">7</ref> and Fig. <ref type="figure" target="#fig_8">8</ref>, revealed these relationships are widely observable. This confirms our hypothesis of a strong similarity between topology features and AUs.</p><p>Nevertheless, there is still not a perfect one-to-one relationship between the topology and AUs. The AUs go through further contextual processing than the topology does, e.g., to separate the activity of AU7 (eyelid tightening) from AU45 (blinking) and other related movements. One challenge with the contextual processing in the state-of-the-art in affective computing is the lack of explainability, which our topologybased approach provides (see Sect. 8).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparing and Differentiating Expressions</head><p>Next, we consider whether the topological features are sufficient for differentiating the six different emotions present in the data. To perform this evaluation, we look at the full face topology for the female subject using t-SNE (Fig. <ref type="figure" target="#fig_6">6</ref>(a), Shepard fitness: 0.79) and MDS (Fig. <ref type="figure">1</ref>, Shepard fitness: 0.88), and male subject using t-SNE (Fig. <ref type="figure" target="#fig_6">6</ref>(b), Shepard fitness: 0.78) and MDS (Fig. <ref type="figure" target="#fig_6">6</ref>(c), Shepard fitness: 0.88). We begin by examining the female and male subjects using t-SNE, as seen in Fig. <ref type="figure" target="#fig_6">6</ref>(a) and Fig. <ref type="figure" target="#fig_6">6</ref>(b), respectively. We make three important observations about the resulting images. (1) First, for both subjects, the emotional states tend to form separate clusters, indicating that they are indeed differentiable. This is particularly important if the topology were to be used for predicting unknown emotional states. (2) Second, most of the emotions begin and end towards the centers of the plots. This colocation is caused by the neutral facial pose that subjects were asked to begin and end with each sequence. (3) The final observation is that the facial poses form temporally coherent 'strings.' This observation is particularly poignant, considering that nowhere in calculating the topological dissimilarity does it utilize temporal information.</p><p>We next consider the MDS projections for the female subject and male subject in Fig. <ref type="figure">1</ref> and Fig. <ref type="figure" target="#fig_6">6</ref>(c), respectively. With the female  subject, we observe that happiness, surprise, fear, and sadness largely cluster into separate regions of the plot with limited overlap or mixing.</p><p>The anger and disgust emotions, on the other hand, overlap significantly, which corresponds with the recent literature in the affective computing community that considers them to be similar expressions <ref type="bibr" target="#b59">[59]</ref>. Another interesting finding is that the emotional states of the male are less differentiated than those of the female. Interestingly, it is commonly accepted in affective computing that, in general, the expressiveness of females is more differentiable than that of males <ref type="bibr" target="#b14">[15]</ref>. Given that we are only observing two subjects, no broad gender-based conclusions can be made in our case. Nevertheless, this female's expressions are more differentiated than this male's expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Comparing and Differentiating Individuals</head><p>Finally, we consider how topological features allow the differentiation of each of the 101 subjects in the BU4DFE dataset.</p><p>To perform this evaluation, we look at the full face topology of a subset of 10 subjects (F001-F010) using t-SNE (Fig. <ref type="figure" target="#fig_9">9</ref>(a)-9(f)). We notice that, for all six emotions, all ten subjects form relatively independent clusters; this is particularly true of the anger (Fig. <ref type="figure" target="#fig_9">9(a)</ref>) and sadness (Fig. <ref type="figure" target="#fig_9">9</ref>(e)) emotions, while some minor overlap occurs for a few of the individuals in the other emotions.</p><p>We performed a similar evaluation for all 101 subjects (58 female and 43 male) (Fig. <ref type="figure" target="#fig_9">9</ref>(g)-9(l)). We can see that the clustering behavior seen with only 10 subjects scales to all the subjects of the dataset. To test the robustness of this t-SNE result to variations in hyperparameters, we ran the tests with four different perplexities <ref type="bibr">(30, 40, 50, and 100)</ref> and found that the clusters remained roughly constant throughout (see supplemental material). The clustering phenomenon present in the t-SNE dimension reduction images was also present when we used UMAP (see supplemental materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Contribution to Affective Computing</head><p>Due to the challenging nature of detecting AUs and determining emotion from expression, we hypothesize that our TDA-based approach can be used to provide new insights into these challenges. As it has been shown that temporal AU information can make recognizing emotions easier, our approach evaluates temporal facial expressions (i.e., AUs), which allows us to visualize a new representation of this data. As shown in Sect. 7, this representation shows the similarity between the topological signals and the AU signals over time, which provides the following insight, as validated by the coauthor on this paper, who is a researcher in affective computing.</p><p>Validation That AUs Are Correctly Detected A limitation of current machine learning AU detection models is their accuracy, as little improvement has been made compared to previous models <ref type="bibr" target="#b38">[39]</ref>. This is mainly due to the models detecting AUs that are not active, as well as not detecting AUs that are active. Our TDA-based approach can validate that the detected AUs are correctly capturing the muscle movement of the face. More specifically, the proposed approach will ensure that the AUs that have been detected are correct. As shown in Fig. <ref type="figure" target="#fig_5">5</ref>(a), AU45 has high-intensity values three times during the sequence. This correctly corresponds to the three blinks that occur in the data, which are also captured in the topological signal. If the blinks were not captured in the topological signal, then the spikes in the AU intensity signal could be attributed to mislabelling or noise. This could facilitate more intelligent active learning <ref type="bibr" target="#b0">[1]</ref> that would improve the machine learning detection models.</p><p>Relationship Between Which AUs Occur Together AU cooccurrences <ref type="bibr" target="#b70">[70]</ref> and patterns <ref type="bibr" target="#b38">[39]</ref> can have a significant impact on the accuracy of machine learning models. Our approach can also give insight into multiple AUs that occur together, including specific AUs that occur when an emotion is expressed. This is detailed in Fig. <ref type="figure" target="#fig_5">5(b)</ref>, where AU14 and AU25 are active at different intensities at different times during the sequence. The topological signal shows a combination of the two AU signals corresponding to the most intense segments of the active AUs. AU14 is a dimple, and AU25 is active when the lips part, which are common muscle movements that could occur during a wide smile. When a smile occurs, AU6 and AU12 are commonly found together, according to the Facial Action Coding System (FACS) <ref type="bibr" target="#b24">[25]</ref>. However, according to Barrett et al. <ref type="bibr" target="#b4">[5]</ref>, expressions vary across cultures and situations meaning, AU6 and AU12 may not be active in all smiles. The proposed approach can provide insight into this phenomenon, allowing investigations of the relationships of new AUs, over time, for different expressions.</p><p>Detecting Facial Expressions There are many successful approaches to detecting facial expressions in affective computing <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b83">83,</ref><ref type="bibr" target="#b87">87]</ref>. Considering this, the purpose of the proposed approach is not to detect facial expressions but to provide greater analysis and explainability of the data. The insight provided by our visualization will allow new insight not previously seen in affective computing, as we can analyze the movement of the face using the proposed TDA-based approach, which directly corresponds to AU movements. This will result in new, more accurate ways of building facial expression detection models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explainability of Machine Learning Models Machine learning has</head><p>given us many advancements in fields as diverse as medicine <ref type="bibr" target="#b80">[80]</ref>, security <ref type="bibr" target="#b1">[2]</ref>, and education <ref type="bibr" target="#b15">[16]</ref>. However, one of the main limitations is the lack of explainability <ref type="bibr" target="#b16">[18]</ref>. Considering this, one of the key advantages of TDA over machine learning is the explainability of the features identified in the process. We demonstrate this using an example of four facial poses from the female surprise data, as shown in Fig. <ref type="figure">10</ref>. These examples focus on the opening and closing of the eyes and mouth, which is commonly associated with a surprised expression. Given a machine learning model that successfully detected the AUs associated with this expression, with the long list of possible muscle movements (e.g., AU1, AU2, AU5, AU25, and AU26), it is difficult to understand why such a model detected them. This is especially true given the black-box nature of neural networks <ref type="bibr" target="#b61">[61]</ref>. In Fig. <ref type="figure">10</ref>, we can directly see the features that change the data. In the persistence diagrams, the number and persistence of the most important features are clearly different. Furthermore, when evaluating the representative cycles, we can further associate the landmark geometry of each high persistence feature in the data. This explainability will facilitate more accurate emotion recognition systems. This is due to the insight that the explainability will give affective computing researchers to better understand and tune their models, resulting in more accurate models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Topology Doesn't Capture Everything</head><p>Limits of Topology Some of the advantages of topology over geometry are also its biggest weaknesses. There are certain shapes of the data that may not be captured by topology alone. For example, smiles and frowns may have the same topological shape. That said, the relationship between the smile, nose, and jawline may be sufficient enough to disambiguate between smiles and frowns. Furthermore, smiles and frowns will also be associated with other changes in facial features, e.g., changes in eye or eyebrow shape. At the very least, our evaluation showed that smile emotions, e.g., happiness, and frown emotions, e.g., sadness, were differentiable in Sect. 7.3.</p><p>Differences Between TDA and Machine Learning Topology does not capture all features of AUs. The AU extraction may utilize other data, nonlinearities, knowledge of physiological relationships of AUs, etc., to determine the extent of the activation of the AUs. That said, it is also important to understand that the AU information is not ground truth. It is the output of a machine learning technique, and it may, in fact, not be showing genuine AU activation. On the other hand, all of the topological features we observed are in the data, and for any matching feature, TDA provides evidence as to why the machine learning algorithm classified AU activation as it did.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Challenges and Limitations of Automatic AU Detection</head><p>The use of AUs for expression recognition is a promising approach. However, there are significant challenges in the detection of them <ref type="bibr" target="#b25">[26]</ref>. Many works that have developed approaches for automatically detecting AUs using machine learning have focused on learning single AUs. However, it has been shown that patterns of AUs can have a significant impact on detection <ref type="bibr" target="#b38">[39]</ref>. This leads to a bigger limitation of current machine learning approaches to AU detection, namely the data. Stateof-the-art machine learning models require a large amount of good data to be accurate. Current models are trained on data that has biases in ethnicity <ref type="bibr" target="#b55">[55]</ref>, as well as significant imbalances in the distribution of AUs <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b89">89]</ref>. Along with these data biases and imbalances, the ground truth data is often manually annotated, which is subjective and can lead to errors <ref type="bibr" target="#b60">[60]</ref>. This results in machine learning models that are not learning how to represent an AU but learning the distribution of data <ref type="bibr" target="#b38">[39]</ref>. In machine learning, many times, the solution to the problem is to collect more data. In the case of AUs, it has the additional challenge that multiple AUs occur simultaneously <ref type="bibr" target="#b70">[70]</ref>, resulting in an unresolvable imbalance of data for the AUs that occur more often (e.g., AU6 or AU12) <ref type="bibr" target="#b38">[39]</ref>. These challenges lead to machine learning models that often fail to recognize AUs that are active, as well as recognize AUs that are not active <ref type="bibr" target="#b90">[90]</ref>.</p><p>Along with challenges in detecting AUs, there is a larger discussion of how humans learn and express emotions <ref type="bibr" target="#b40">[41]</ref>. Broadly, this discussion can be categorized into two hypotheses. The first hypothesis states that emotions can be recognized from facial expressions (AUs) <ref type="bibr" target="#b22">[24]</ref>. This hypothesis is the basis for the Facial Action Coding System (FACS) AUs <ref type="bibr" target="#b24">[25]</ref> and is a significant motivation for the field of affective computing. The second hypothesis contradicts that and states that expressions vary across cultures, situations, and people in the same situation. Because of this, emotions cannot be recognized from facial expressions alone <ref type="bibr" target="#b4">[5]</ref>. Instead, other factors such as context, physiology, age, and gender should be considered. While these two hypotheses contradict one another, recent work has shown validity in both hypotheses. More specifically, while it is difficult to determine emotion from AUs given a single facial image when temporal AU information and context are considered, the task becomes easier <ref type="bibr" target="#b37">[38]</ref>.</p><p>Along with this, it has also been shown that the fusion of physiological signals, such as heart rate and respiration, along with AUs can be used to accurately recognize pain in subjects <ref type="bibr" target="#b39">[40]</ref>. Although it is a challenging problem to automatically detect AUs using machine learning, these works show that AUs are still a promising approach to solve the challenging problem of emotion from expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS</head><p>In this paper, we have demonstrated that TDA can be used to discern and better understand patterns that exist between emotions. Paired with machine learning approaches to affective computing, TDA provides a means to evaluate particular aspects of the data to discern what parts of the face may be causing the machine learning algorithm to recognize the data as a particular emotion or explain the shortcomings or misdiagnoses that the machine learning algorithm provides, e.g., if the algorithm recognizes a happiness emotion as anger, TDA may help to discern what led to this misdiagnosis. The next phase of this work is to begin evaluating these affective computing hypotheses, particularly those discussed in Sect. 8, using our TDA-based analysis and interface.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>4.3).Stage 2: Once the topologies of individual poses are extracted, we compare the topology to that of other poses to determine their pairwise dissimilarity (see Sect. 5.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of persistent homology on the 83 facial landmarks on the female subject F001. (a) The persistent homology is calculated by forming a Rips filtration and tracking/extracting the associated homology groups. Starting at r = 0, if the pairwise distance between any two or three points is less than r, an edge or triangle is formed, respectively. As r increases, components merge, and tunnels form and disappear. (b) The topology is visualized with a persistence diagram. Square points are H 0 components (the triangle indicates a single infinite H 0 component), and the hollow circles are the H 1 tunnels. The horizontal position of points is their birth r b i and their vertical position is their death r d i . Distance from the dotted diagonal, as well as object size, is proportional to its persistence.</figDesc><graphic coords="3,89.39,49.25,320.06,77.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of supersampling and non-metric persistent homology shows the data and the persistence diagram of H 1 features. (a) The original 83 landmarks from a single pose on the female subject F001. (b-e) Supersampling the landmarks with different ε values shows a significant difference in the persistence diagram between the (a) original and (e) supersampled data. (f) Our non-metric representation of the data requires significantly less data and produces a persistence diagram similar to that of (e) supersampling.</figDesc><graphic coords="4,63.95,49.25,237.74,154.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 4. Plots of (a) the compute time and (b) the number of topological features generated for the original 83 landmarks, six levels of supersampling (ε = 8, 4, 2, 1, 0.5, and 0.25), and our non-metric representation. The regression line in (a) only considers the supersampling data points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>5 top) [D2]. Each selected emotion is overlaid for time-dependent comparison [D3]. The keyframe is user-selectable, and the visualization updates as the keyframe is modified. The scatterplot representation uses 2D dimension reduction for both the horizontal and vertical axes. The choice of MDS, t-SNE, or UMAP is provided for the user. The data are either shown as points or connected via a path (see Fig. 6(a)) if the user wants a temporal context [D1].The plot is also interactive-selecting a point updates the time-index used in other visualizations, e.g., the 3D landmarks.3D Landmarks (Fig. 1 lower right) The 3D landmarks represent the data of the current time-index for the respective emotion [D1]. The faces are placed side-by-side for comparison [D3][D5]. The sliders beneath each can be used to animate or adjust their time-index [D1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A comparison of relative distance on non-metric topology (top) to Action Units (AUs) (bottom) on F001. The results demonstrate the similarity between the features extracted by the topology and AUs, which are commonly used in affective computing.</figDesc><graphic coords="6,309.35,102.29,119.14,56.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Evaluation using (a-b) t-SNE and (c) MDS to determine how effective our non-metric topology-based approach is at differentiating emotions. F001 MDS can be found in Fig.1.</figDesc><graphic coords="6,372.95,590.33,59.78,88.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )Fig. 7 .</head><label>a7</label><figDesc>Fig. 7. Comparison of F001 non-metric topology (top) and AUs (bottom) shows a similarity between eyes+nose (column 1), mouth+nose (column 2), and eyebrows+nose (column 3) and AUs associated with those facial regions.</figDesc><graphic coords="7,62.15,365.33,110.56,55.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison of M001 non-metric topology (top) and AUs (bottom) shows a similarity between eyes+nose (column 1), mouth+nose (column 2), and eyebrows+nose (column 3) and AUs associated with those facial regions.</figDesc><graphic coords="8,67.55,263.93,141.62,55.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Clustering of 10 subjects (F001-F010) on the top and all 101 subjects on the bottom. For 10 subjects, each subject is colored differently. For 101 subject tests, 12 colors were mapped to 101 by creating roughly 10 shades per color. Each plot includes the associated Shepard fitness (SF).</figDesc><graphic coords="9,64.55,153.89,77.53,76.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Fig.10. Illustration of the explainability of our TDA-based approach using four poses from F001 surprise. For each, the persistence diagrams are shown (right), along with the highest persistence representative cycles (left). The number and persistence of features explains the difference between poses, while the representative cycles explain which landmarks generated those topological features.</figDesc><graphic coords="9,56.87,525.41,208.82,136.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>left) Our interface features a small multiples display for comparing different data conditions. The interface features a comparison between two conditions, including comparing two subjects, bottleneck vs. Wasserstein distance, metric vs. non-metric topology, t-SNE vs. MDS, etc. [D2]. The interface further divides each column into comparisons of different subsets of facial features, including full face,</figDesc><table /><note>eyes+nose, mouth+nose, and eyebrows+nose [D4]. Finally, each row represents one of the six main emotions, anger, disgust, fear, happiness, sadness, and surprise. The user selects the data for the embedding by selecting a column and enabling/disabling specific emotions of interest by selecting rows [D3]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Action Units (AUs) and their corresponding facial muscle movements as used in our evaluation.</figDesc><table><row><cell>Action Unit</cell><cell>Facial Muscles</cell><cell>Description</cell></row><row><cell>AU1</cell><cell>Frontalis, pars medialis</cell><cell>Inner eyebrow raise</cell></row><row><cell>AU2</cell><cell>Frontalis, pars lateralis</cell><cell>Outer eyebrow raise</cell></row><row><cell></cell><cell>Depressor Glabellae,</cell><cell></cell></row><row><cell>AU4</cell><cell>Depressor Supercilli,</cell><cell>Eyebrow lower</cell></row><row><cell></cell><cell>Currugator</cell><cell></cell></row><row><cell>AU5</cell><cell>Levator palpebrae superioris</cell><cell>Upper eyelid raise</cell></row><row><cell>AU6</cell><cell>Orbicularis oculi, pars orbitalis</cell><cell>Cheek raise</cell></row><row><cell>AU7</cell><cell>Orbicularis oculi, pars palpebralis</cell><cell>Eyelid tighten</cell></row><row><cell>AU9</cell><cell>Levator labii superioris alaquae nasi</cell><cell>Nose wrinkle</cell></row><row><cell>AU10</cell><cell>Levator Labii Superioris, Caput infraorbitalis</cell><cell>Upper lip raise</cell></row><row><cell>AU12</cell><cell>Zygomatic Major</cell><cell>Lip corner pull</cell></row><row><cell>AU14</cell><cell>Buccinator</cell><cell>Dimple</cell></row><row><cell>AU15</cell><cell>Depressor anguli oris</cell><cell>Lip corner depress</cell></row><row><cell>AU17</cell><cell>Mentalis</cell><cell>Chin raise</cell></row><row><cell>AU20</cell><cell>Risorius</cell><cell>Lip stretch</cell></row><row><cell>AU23</cell><cell>Orbicularis oris</cell><cell>Lip tighten</cell></row><row><cell></cell><cell>Depressor Labii,</cell><cell></cell></row><row><cell>AU25</cell><cell>Relaxation of Mentalis,</cell><cell>Lips part</cell></row><row><cell></cell><cell>Orbicularis Oris</cell><cell></cell></row><row><cell>AU26</cell><cell>Masetter, Temporal/Internal Pterygoid</cell><cell>Jaw drop</cell></row><row><cell></cell><cell>Levator Palpebrae,</cell><cell></cell></row><row><cell>AU45</cell><cell>Orbicularis Oculi,</cell><cell>Blink</cell></row><row><cell></cell><cell>Pars Palpebralis</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that while persistent homology itself has no parameters, our pipeline does have some options available to users.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We do not consider H 2 (voids) because despite our data being 3D, it is nearly flat. Thus, voids rarely occur, and when they do, they have low persistence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For reasons outside the scope of this paper, there is no single generator for a cycle but instead a class of generators. A representative cycle, which is a</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their feedback. This project was supported in part by the National Science Foundation (IIS-1845204).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wild facial expression recognition based on incremental active learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Hyeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Bashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="212" to="222" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of gender inequality in face recognition accuracy</title>
		<author>
			<persName><forename type="first">V</forename><surname>Albiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vangara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision Workshops</title>
				<meeting>the IEEE Winter Conference on Applications of Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="81" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D Deformation Signature for Dynamic Face Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cherenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2138" to="2142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Openface: an open source facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science in the Public Interest</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ripser: Efficient Computation of Vietoris-Rips Persistence Barcodes</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02518</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring Emotion: The Self-Assessment Manikin and the Semantic Differential</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavior Therapy and Experimental Psychiatry</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Landmark Localization on 3D/4D Range Data Using a Shape Index-Based Statistical Shape Model with Global and Local Constraints. Computer Vision and Image Understanding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="136" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Emotion Scents -A Method of Representing User Emotions on GUI Widgets. SPIE Digital Library</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cernea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emotion-prints: Interactiondriven emotion visualization on multi-touch interfaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cernea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">p. 93970A. International Society for Optics and Photonics</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">9397</biblScope>
		</imprint>
	</monogr>
	<note>Visualization and Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Group affective tone awareness and regulation through virtual agents</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cernea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IVA 2014 Workshop on Affective Agents</title>
				<meeting>eeding of IVA 2014 Workshop on Affective Agents<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08">August. 2014</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mobileface: 3D Face Reconstruction with Efficient CNN Regression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chinaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stability of persistence diagrams</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; computational geometry</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active Appearance Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gender differences in emotional response: Inconsistency between experience and expressivity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">e0158666</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emotiw 2020: Driver gaze, group emotion, student engagement and physiological signal based challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence: A survey</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Došilović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brčić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hlupić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="210" to="0215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Persistent Homology -A Survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contemporary Mathematics</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="257" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Computational Topology: An Introduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Topological persistence and simplification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Letscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomorodian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 41st Annual Symposium on Foundations of Computer Science</title>
				<meeting>41st Annual Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Basic Emotions. Handbook of Cognition and Emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Repertoire of Nonverbal Behavior: Categories</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Origins, Usage, and Coding. Semiotica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="98" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Diacoyanni-Tarlatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lecompte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitcairn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ricci-Bitti</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universals and Cultural Differences in the Judgments of Facial Expressions of Emotion</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">712</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crossing domains for au coding: Perspectives, approaches, and measures</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Ertugrul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics, Behavior, and Identity Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="158" to="171" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable Synthesis Model for Emotion Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fabiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Fabiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaishanker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08341</idno>
		<title level="m">Impact of Multiple Modalities on Emotion Recognition: Investigation into 3d Facial Landmarks, Action Units, and Physiological Data</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition with Deeply-Supervised Attention Network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D and 4D Face Recognition: A Comprehensive Review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Recent Patents on Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Physiological-Based Affect Event Detector for Entertainment Video Applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fleureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guillotel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huynh-Thu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="385" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D Face Reconstruction From Volumes of Videos Using a MapReduce Framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="165559" to="165570" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multilinear Modelling of Faces and Expressions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grasshof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and scalable complex network descriptor using pagerank and persistent homology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hajij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Munch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Intelligent Data Science Technologies and Applications (IDSTA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="110" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual Detection of Structural Changes in Time-Varying Graphs Using Persistent Homology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hajij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific Visualization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3D Facial Expression Recognition Using Kernel Methods on Riemannian Manifold</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benouareth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Declercq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Facial Expression Recognition with Automatic Segmentation of Face Regions Using a Fuzzy-Based Classification Approach. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hernandez-Matamoros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Escamilla-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakano-Miyatake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Perez-Meana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing context using facial expression dynamics from action unit patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinduja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aathreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing (Under Review</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Impact of action unit occurrence patterns on detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinduja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aathreya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal fusion of physiological signals and facial action units for pain recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hinduja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="387" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Developing an understanding of emotion categories: Lessons from objects</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hoemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lobue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Oakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Subject Identification across Large Expression Variations Using 3D Facial Landmarks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jannat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fabiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08339</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition Using Local Composition Pattern</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jashem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jabid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer and Communications Management</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="63" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Geometry Helps to Compare Persistence Diagrams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nigmetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Algorithmics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CONTVERB: Continuous Virtual Emotion Recognition Using Replaceable Barriers for Intelligent Emotion-based IoT Services and Applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ben-Othman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mokdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Surface Shape and Curvature Scales</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="557" to="564" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Kovacevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wampfler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Günther</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Glyph-Based Visualization of Affective States</title>
	</analytic>
	<monogr>
		<title level="j">EuroVis: Short Papers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multidimensional Scaling by Optimizing Goodness of Fit to a Non-Metric Hypothesis</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep Facial Expression Recognition: A Survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Occlusion aware facial expression recognition using cnn with attention mechanism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2439" to="2450" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automated facial expression recognition based on facs action units</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="390" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Saanet: Siamese action-units attention network for improving dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">413</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3D Face Modeling from Diverse Raw Scan Data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9408" to="9418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automatically detecting pain in video through facial action units</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="664" to="674" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Affectaura: an intelligent system for emotional memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roseway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep-emotion: Facial expression recognition using attentional convolutional network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdolrashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3046</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Disgust and anger relate to different aggressive responses to moral violations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Molho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tybur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balliet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="609" to="619" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Promoting reproducible research for characterizing nonmedical use of medications through data annotation: Description of a twitter corpus and guidelines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">e15861</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Towards reverse-engineering blackbox neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="121" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial expressions based on deep covariance trajectories</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otberdout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kacem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3892" to="3905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Emotion Recognition from 3D Videos Using Optical Flow Method</title>
		<author>
			<persName><forename type="first">G</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference On Smart Technologies For Smart Nation (SmartTechCon)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="825" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Robust Real-Time Performance-Driven 3D Face Tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jen Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition (ICPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1851" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Affective Computing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Constantinides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quercia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07209</idno>
		<title level="m">Heartbees: Visualizing crowd affects</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Clique Community Persistence: A Topological Visual Analysis Approach for Complex Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fugacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lukasczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leitte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="822" to="831" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multivariate Data Analysis Using Persistence-Based Filtering and Topological Signatures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leitte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2382" to="2391" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A novel machine vision-based 3d facial action unit identification for fatigue detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sikander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Exploiting sparsity and co-occurrence structure for action unit recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vasisht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Persistent Homology Guided Force-Directed Graph Layouts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hajij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="697" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">3D Spatio-Temporal Face Recognition Using Dynamic Range Model Sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The Topology Toolkit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tierny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Favelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gueunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="832" to="842" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">3D Facial Action Units and Expression Recognition Using a Crisp Logic</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tornincasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vezzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Violante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marcolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dagnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Tregnaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Aided Design and Applications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="256" to="268" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Quantified facial temporal-expressiveness dynamics for affect analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visualizing Data Using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer science &amp; business media</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Branching and Circular Features in High Dimensional Data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Summa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vejdemo-Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1902" to="1911" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">What is... persistent homology? Notices of the</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>AMS</publisher>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="36" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Preparing medical imaging data for machine learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Willemink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Koszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleischmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Folio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Exploring multidimensional measurements for pain evaluation using facial action units</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="559" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Automatic 4D Facial Expression Recognition Using DCT Features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Facial expression recognition by deexpression residue learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2168" to="2177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A High-Resolution 3D Dynamic Facial Expression Database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">An Approach for Automated Multimodal Analysis of Infants&apos; Pain</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zamzmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldgof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ashmeade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition (ICPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Emotioncues: Emotion-oriented visual summarization of classroom videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Facial expression recognition via learning deep sparse autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dobaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="page" from="643" to="649" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Region of interest based graph convolution: A heatmap regression approach for action unit detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2890" to="2898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Muscular Movement Model-Based Automatic 3D/4D Facial Expression Recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1438" to="1450" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
