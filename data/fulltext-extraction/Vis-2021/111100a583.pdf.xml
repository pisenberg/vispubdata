<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Occlusion Culling using Confidence Maps for High-Quality Rendering of Large Particle Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Ibrahim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Rautek</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guido</forename><surname>Reina</surname></persName>
							<email>guido.reina@visus.uni-stuttgart.de</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Agus</surname></persName>
							<email>magus@hbku.edu.qa</email>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Hadwiger</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">King Abdullah University of Science and Technology (KAUST)</orgName>
								<orgName type="institution" key="instit2">Visual Computing Center</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Science and Engineering</orgName>
								<orgName type="institution">Hamad Bin Khalifa University</orgName>
								<address>
									<addrLine>Qatar Foundation</addrLine>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Occlusion Culling using Confidence Maps for High-Quality Rendering of Large Particle Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7763149B68820CCF11CE80DF0B88D68F</idno>
					<note type="submission">received 31 Mar. 2021; accepted xx xxx. 201x.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large-scale particle data</term>
					<term>sub-pixel occlusion culling</term>
					<term>super-sampling</term>
					<term>anti-aliasing</term>
					<term>coverage</term>
					<term>probabilistic methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RENDERING visible/culled particles visible/culled density 75.5% CULLED OCCLUSION Fig. 1. Rendering a synthesized SARS-CoV-2 virus with atomistic resolution (∼40M particles). (Left) Main view rendered with 75.5% of particles culled. (Right) We estimate occlusion probabilities using particle density functions sampled from a coarse particle density volume. From front to back (left to right), both the number of particles and the volume density decrease due to detected occlusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The steadily growing size of simulation data leads to significant novel challenges. One area where this is particularly apparent is in molecular dynamics (MD) simulations. Such data sets comprise millions to billions of particles, and practitioners increasingly require high rendering quality and accuracy to capture important data features, in addition to high performance. However, the particles comprising large-scale particle simulations are very small, and therefore the number of particles per pixel in visualizations is typically much larger than for many other types of data. In this context, the standard approach of sampling pixels with only a few samples (rays) thus often leads to significant aliasing and missing features, because the data are severely undersampled.</p><p>Many approaches therefore employ pre-computed levels of detail, or abstractions. This typically means skipping groups of particles that are small, or aggregating particles into simplified proxy geometries. This not only removes high-frequency detail from the visualization, but can also bias the analyses of domain scientists toward data that is simplified for the current scale of exploration. This poses a problem for accurate exploratory analysis, as it limits the detail level at which data can be understood, and the assessment where to continue exploring. Moreover, for data with similar features on different scales, simplification makes it unnecessarily hard to understand which scale is currently shown.</p><p>Particles in large-scale data sets are often distributed heterogeneously in space, e.g., depending on the type of matter being simulated. In molecular dynamics, particles in a solid/crystalline state tend to be tightly packed, as do liquid particles. In contrast, particles in a gas phase often lead to sparsely populated regions, e.g., in the left-hand side of Fig. <ref type="figure">2</ref>. In such regions, the particles require accurate super-sampling without overestimating their potential for occluding, which would lead to visible particles not being sampled. The right-hand side, however, is densely packed, and most particles there in fact do not contribute to the visualization, providing significant opportunities for occlusion culling. (See Table <ref type="table" target="#tab_0">1</ref> for an overview of the terminology that we use in this paper.) Efficient and accurate occlusion culling must take these properties into account, but particles that are smaller than a pixel pose a major challenge for dynamic (on-the-fly) occlusion culling. If stateof-the-art occlusion culling is employed for such data, small particles lead to overestimation of potential occlusion: the depth of a sample is not representative for an entire pixel. This holds unless occlusion is also super-sampled on top of super-sampling for rendering.</p><p>The major kinds of state-of-the-art interactive molecular visualization approaches and their drawbacks are: <ref type="bibr" target="#b0">(1)</ref> Reducing individual cells of an acceleration structure, or even entire molecule bounding boxes, to a single pixel when their projection is small enough <ref type="bibr" target="#b6">[7]</ref>. This skips details of large parts of the scene. <ref type="bibr" target="#b1">(2)</ref> Clustering and simplifying the ~0.5 million ~47.5 million Fig. <ref type="figure">2</ref>. Heterogeneous particle distributions. Often, only a small part of a data set, e.g., the ∼ 0.5 million particles in the crown (left), and the faces of the aluminum block (right), contribute to the actual visualization.</p><p>data to fit the current zoom level <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref>, filtering out finer details. In terms of performance and perception, all of these approaches scale well, but at the cost of lost accuracy. (3) Progressive super-sampling achieves high accuracy after convergence, but at a significant cost <ref type="bibr" target="#b36">[37]</ref>. For fast re-lighting, the result of costly super-sampling can be cached in normal distribution functions <ref type="bibr" target="#b21">[22]</ref>. This captures details well, but super-sampling needs to be re-computed for every new rotated view.</p><p>Main goals. We build on the basic premise that accurate, highquality visualization of the intricate details in large-scale particle data (Fig. <ref type="figure" target="#fig_7">1</ref> and Fig. <ref type="figure">2</ref>) is impossible without extensive super-sampling per pixel. However, doing this for all particles in large data sets was so far prohibitive at interactive rates. We therefore target improving performance by concentrating most of the sampling effort on the visible parts of the data only. We propose a novel architecture for dynamic occlusion culling that is probabilistic, uses progressive refinement, and also takes current GPU architectures into account for high performance. Our approach performs both occlusion culling and high-quality rendering via progressive sampling, adding more and more samples from frame to frame, without requiring any data simplification or approximations.</p><p>Contributions. The major contributions of this paper are:</p><p>(1) A novel probabilistic occlusion culling architecture (Fig. <ref type="figure">4</ref>) that goes hand in hand with progressive super-sampling of particle data containing intricate detail. While we start progressively super-sampling the scene, we likewise progressively build up visibility information that enables culling more and more parts of the scene that become known to be occluded with high confidence. We combine fine culling granularity with fast rendering via a two-level object space subdivision, grouping particles into nodes and meshlets, and using recent GPU features.</p><p>(2) Instead of computing sub-pixel coverage via explicit discretization, as in Fig. <ref type="figure" target="#fig_0">3</ref> (left), we track a representative pixel depth (d), and probabilistically estimate sub-pixel coverage, as in Fig. <ref type="figure" target="#fig_0">3</ref> (right), to determine confidence (C) in the current value of d. We track depth and confidence information (d,C) in a multi-resolution image pyramid, the depth confidence map, enabling probabilistic culling.</p><p>(3) To enable the accurate estimation of depth and confidence updates, in addition to the particle data we maintain and progressively update a coarse 3D particle density volume, from which we extract particle density functions along view rays on-the-fly via ray casting. This enables a novel probabilistic computation of visibility estimates.</p><p>We show that our probabilistic culling converges quickly, enabling extensive super-sampling of visible particles at real time rates, while significantly reducing the overall number of particles that are sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Super-sampling for high-quality rendering is a standard technique in graphics and visualization. Many approaches subdivide pixels into a sub-pixel grid, and perform sampling on this finer grid (see Fig. <ref type="figure" target="#fig_0">3 (left)</ref>). An early seminal work is the A-buffer <ref type="bibr" target="#b2">[3]</ref>. The accumulation buffer <ref type="bibr" target="#b18">[19]</ref> sums color samples per pixel in multiple rendering passes, reusing the standard depth buffer for correct visibility without sub-pixel depth storage overhead. However, this accumulation buffer cannot be used for sub-pixel-accurate culling. We also do not store or use an explicit sub-pixel grid (see Fig. <ref type="figure" target="#fig_0">3</ref> (right)), but overall our approach is very different and focuses on probabilistic estimates that enable sub-pixelaccurate culling. We also use an "accumulation buffer," but this is not related to the concept just described <ref type="bibr" target="#b18">[19]</ref>. Our accumulation buffer performs a probabilistic accumulation of confidence in depth values. Occlusion culling. We perform hierarchical occlusion testing, inspired by hierarchical occlusion maps (HOMs) <ref type="bibr" target="#b40">[41]</ref> and the hierarchical z buffer <ref type="bibr" target="#b12">[13]</ref>. However, we emphasize that instead of aggregating depth information of pixels or sub-pixels, our "finest resolution" occlusion information is fully probabilistic and not an aggregation of discretized sub-pixel data. Bittner et al. <ref type="bibr" target="#b1">[2]</ref> and Cohen-Or et al. <ref type="bibr" target="#b4">[5]</ref> provide comprehensive overviews of occlusion culling methods. Many approaches exploit GPU hardware occlusion queries, e.g., in OpenGL <ref type="bibr" target="#b3">[4]</ref>, which are challenging to use because they are asynchronous. Various methods reduce stalls in the occlusion pipeline, such as coherent hierarchical culling <ref type="bibr" target="#b0">[1]</ref>, and later improvements <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. On top of these methods, fast hierarchical culling can be built <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>. To achieve scalability to large scenes, hybrid image/object-space methods use spatial scene partitionings and hierarchical data structures, amortizing occlusion query and culling cost over many primitives <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>. Proximity information of occluding triangles can also be used to infer good occluders from neighboring triangles <ref type="bibr" target="#b39">[40]</ref>. A more recent CPU-based approach for early occlusion culling was presented by Hasselgren et al. <ref type="bibr" target="#b19">[20]</ref>.</p><p>Particle rendering. A common approach to render particle data is to use GPU-computed glyphs <ref type="bibr" target="#b16">[17]</ref>, where the particle geometry is replaced by implicit descriptions that are rendered as billboards and then filled in, for example by computing ray-sphere intersections for each pixel in the billboard's screen space projection. We render particles based on the same principle. Large-scale particle data can also be rendered via volume rendering, e.g., via RBF volume ray casting <ref type="bibr" target="#b22">[23]</ref>, or ray tracing for molecular visualization of ball-and-stick models <ref type="bibr" target="#b23">[24]</ref>, also in ultra-resolution immersive environments <ref type="bibr" target="#b32">[33]</ref>. Other approaches for molecular visualization that exploit a grid and instancing for large particle data have been proposed. Lindow et al. <ref type="bibr" target="#b27">[28]</ref> traverse a grid in layers to just require a single reference per atom even when it spans multiple cells, additionally optimizing normal computation to reduce aliasing. Falk et al. <ref type="bibr" target="#b6">[7]</ref> use a grid containing local grids for entire molecules to further optimize intersection testing depending on screenspace size. Grottel et al. <ref type="bibr" target="#b15">[16]</ref> perform occlusion culling of particle scenes on two levels, in both cases against a (hierarchical) depth buffer of the previous frame. On the coarse level, they check whether the bounding boxes of large chunks of data are occluded. On the fine level, they do per-glyph occlusion culling against the appropriate depth hierarchy level in the vertex shader, that is, after the data is streamed (chunk-wise) to the GPU. This produces good results in terms of occlusion and frame rates. However, this technique always streams the data probabilistic occlusion culling Fig. <ref type="figure">4</ref>. Probabilistic occlusion culling architecture. To facilitate culling, particles are grouped together in a two-level object space hierarchy comprising nodes and meshlets (top left). For probability estimates, we also compute a coarse particle density volume (bottom left). For each new view, we sample subsets of particles not (yet) known to be occluded, and update a novel image space hierarchy for culling: depth confidence maps. Confidence values are updated probabilistically, requiring particle density estimates along rays through the density volume, determining an occlusion class for each meshlet, from visible to occluded. Rendering is performed by super-sampling only the meshlets that are not known to be occluded, with culling as well as super-sampling performed progressively. After culling has converged, we continue super-sampling for higher rendering quality.</p><p>to the GPU and is bandwidth-limited for large data: To use occlusion queries, the respective data has to be uploaded to the GPU. The authors have implemented a caching mechanism such that relevant batches of particles are kept on the GPU for rendering, but for each batch with unclear visibility uploading is required. Wang et al. <ref type="bibr" target="#b38">[39]</ref> introduce a particle-specific metric to determine occlusion on the CPU.</p><p>Ray tracing frameworks. A recent CPU ray tracing framework is OSPRay <ref type="bibr" target="#b35">[36]</ref>, built on Embree <ref type="bibr" target="#b37">[38]</ref>, using a standard bounding volume hierarchy (BVH). The Particle Kd-Tree (P-k-d) is an extension for particle data <ref type="bibr" target="#b36">[37]</ref>. Similar approaches are available for GPUs, for example the NVIDIA RTX hardware ray tracing capabilities. This approach also requires an acceleration structure to be efficient, which can only have two levels, of which the bottom one has bounding information <ref type="bibr" target="#b34">[35]</ref>. A more recent approach investigates nesting of P-k-ds inside a standard BVH to balance memory requirements and resulting performance <ref type="bibr" target="#b11">[12]</ref> and benchmarks implementations using OptiX, OSPRay and OpenGL.</p><p>Point-based rendering. Particle rendering and point-based rendering share several of the basic approaches, and they manage occlusion in similar ways. For example, layered point clouds <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> are organized top-down in chunks through a hierarchical data structure, exploiting the chunks together with frame-to frame coherence for culling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OCCLUSION CULLING ARCHITECTURE OVERVIEW</head><p>Fig. <ref type="figure">4</ref> depicts an overview of the major components of the pipeline of our probabilistic occlusion culling architecture. Table <ref type="table" target="#tab_0">1</ref> gives an overview of our terminology. In the following, we first summarize the major ideas that our probabilistic approach is built on, and then describe each of the pipeline stages depicted in Fig. <ref type="figure">4</ref> in more detail.</p><p>All input particles are spatially grouped into a simple two-level hierarchy (Sect. 3.1), comprising small groups of particles stored as meshlets, and meshlets further grouped into nodes (Fig. <ref type="figure">4</ref>, top left). Moreover, we compute an auxiliary 3D particle density volume to facilitate accurate probability estimates (Fig. <ref type="figure">4</ref>, bottom left).</p><p>The main idea is to sample the scene progressively, over several iterations of the right-hand side of our pipeline, with fully dynamic occlusion culling, consisting of (1) dynamic sampling of occlusion (Sect. 3.2), (2) probabilistic updates of depth confidence maps (Sect. 3.3), and (3) probabilistic occlusion culling using the depth confidence to classify groups of particles (meshlets) into one of four occlusion classes; culling the particles in the class O (occluded). These particles are known to be occluded with high confidence. After each iteration of determining occlusion classes, we dynamically update the particle density volume by removing the contribution of particles that were put into O. These updates significantly improve our probabilistic confidence estimates.</p><p>Our approach converges quickly, separating occluded (O) particles from potentially visible (P) particles , which enables extensive supersampling of only those remaining particles at real time rates (Sect. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Processing and Object Space Data Structures</head><p>Object space particle hierarchy. For efficient hierarchical culling, and to exploit recent GPU architectures, we do not perform occlusion tests at the granularity of single particles. We group particles into two hierarchy levels in a simple view-independent pre-processing step: Small groups (e.g., <ref type="bibr" target="#b15">16</ref>) of nearby particles are grouped into meshlets (corresponding to GPU mesh shaders), and nearby meshlets are grouped further into nodes. While this significantly increases efficiency, the small size of meshlets preserves fine culling granularity (Fig. <ref type="figure">5</ref>).</p><p>Particle density volume. We generate a low-resolution 3D particle density volume that is used to determine a per-pixel particle density function for each view ray via ray casting. It is needed to allow accurate estimation of occlusion probabilities for each sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Sampling of Occlusion</head><p>We sample and improve occlusion estimates over multiple iterations, until sufficient convergence is reached (Fig. <ref type="figure">4</ref>, red cycle). In each iteration, we perform multiple passes of choosing a subset of potential occluders (meshlets) to sample occlusion (Fig. <ref type="figure">4</ref>, top center). In each pass, we generate one sub-pixel depth sample per pixel via rasterization.</p><p>A major motivation for progressively sampling occlusion is that, a priori, the best selection of particles for the purposes of occlusion is not known. In the simple case without sub-pixel accuracy, farther depths will be occluded by closer depths, which can be resolved by simple depth testing. However, with sub-pixel accuracy any sampled depth will in general not be representative for a whole pixel. For this reason, samples with closer depth should not always lead to more occlusion: Overall occlusion depends on the sub-pixel coverage corresponding to that depth. Therefore, if the closest depth is retained, within pixels a suboptimal choice of depth sampling will often occur. To solve this problem, our probabilistic approach (Sect. 4) dynamically estimates the probability of a new sample improving occlusion, and chooses the depth samples to retain according to this probability (Sect. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Depth Confidence Maps</head><p>To avoid the overhead and discretization of explicit sub-pixels (Fig. <ref type="figure" target="#fig_0">3</ref>  For each pixel, we maintain a pair (d,C), with d its representative depth, and C its associated depth confidence. The details of this approach are described in Sect. 4.1 and Sect. 4.2. In fact, to progressively improve both confidence (higher is better) as well as representative depth (closer is better), we maintain two pairs (d,C) per pixel: One in a cull buffer, which at any time is used for probabilistic culling, and another one in an accumulation buffer (Sect. 4.3). The latter is used to progressively build up confidence for a closer depth value with better potential occlusion (see Sect. 4.4 and Sect. 4.5). For hierarchical occlusion testing, we further maintain the cull buffer as a multi-resolution pyramid, similar to mipmaps or HOMs <ref type="bibr" target="#b40">[41]</ref>, as described in Sect. 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Probabilistic Occlusion Culling</head><p>Occlusion classes. Our architecture maintains and progressively updates a classification of meshlets into one of four occlusion classes. All meshlets start in the occlusion class P (potentially visible). As the confidence in the occlusion of a meshlet increases, it "moves up" in confidence, toward occlusion class O (occluded). See Sect. 5.</p><p>Confidence-based culling. In every iteration, the depth and confidence values (d cull ,C cull ) stored in the cull buffer are used to test meshlets that are not yet in occlusion class O. This gives a new overall confidence value for each meshlet, which is its probability of being occluded. This in turn triggers an update of its occlusion class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Super-Sampled Rendering</head><p>Occlusion culling, in each iteration of Fig. <ref type="figure">4</ref>, results in a potentially visible set (PVS) of particles, which we define as all particles whose meshlets are not in occlusion class O, with the PVS becoming smaller from iteration to iteration. Actual rendering is performed by progressively super-sampling all particles in all the meshlets in the PVS. The most important property of our pipeline is that this super-sampling effort is only spent on particles that are not occluded with sufficient confidence, as determined by culling against the depth confidence map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEPTH CONFIDENCE MAPS</head><p>Considering sub-pixel occlusion (Fig. <ref type="figure" target="#fig_0">3</ref>), the depth value of a pixel only corresponds to the sub-pixel samples computed so far. Future samples may have larger (farther) depth values. For each pixel we therefore estimate the confidence in the depth value currently associated with it, which we call its representative depth, with that associated confidence.</p><p>We estimate, improve, and use depth confidence probabilistically over multiple iterations (Fig. <ref type="figure">6</ref>). We iterate over a sequence of samples of particles mapping to a given pixel, and for each sample we: (1) Estimate the sub-pixel area corresponding to all samples so far, for which the depth value is representative (Sect. 4.1 and 4.2). ( <ref type="formula" target="#formula_2">2</ref>) Estimate whether the new sample's depth would increase particle occlusionwhich also requires sufficient accumulation of confidence-using a novel probabilistic strategy for accepting the incoming depth (Sect. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Probabilistic Sub-Pixel Coverage</head><p>We want to derive an estimate of how much of the sub-pixel area of a given pixel is covered so far, in an iteration over a sequence of particle samples. For this, we consider a given pixel of screen space size A, and perform the estimation of sub-pixel coverage iteratively, iterating from particle contribution to particle contribution (see Fig. <ref type="figure" target="#fig_0">3 (right)</ref>).</p><p>Representative depth. A crucial concept in our method is the meaning of the depth value associated with a given pixel. Although we perform extensive super-sampling, we only store one depth value d for every pixel, not one depth value per sub-pixel sample. We call this depth d the representative depth, which is computed over the nonempty regions of the pixel (Fig. <ref type="figure" target="#fig_0">3</ref> (right)). The farthest visible depth anywhere marked in orange is d. In contrast, all regions shown in white have unknown depth, and (future) samples taken there can potentially hit particles with any depth. Thus, d for a given pixel is a representative conservative depth for sub-pixel regions already known to be covered.</p><p>Particle sampling without replacement. We first consider iterating over all particles mapping to the pixel, with 0 &lt; a n ≤ A the size of particle n within the pixel footprint of area A. We denote the total pixel area that is covered jointly by n particles, after n iterations, by A n . The fraction covered inside the pixel's area after n iterations therefore is A n /A. However, because particles often overlap in screen space, A n cannot be determined as ∑ n a n . In order to estimate how much the next particle's area will contribute to the area A n of the pixel covered in iteration n, we estimate that the fraction of its area a n , that will map to a sub-pixel area that was not already covered before, is given by the ratio of the sub-pixel area not yet covered, to the total area of the pixel, i.e., (A − A n−1 )/A. We can view this estimate as computing the expected value of the newly covered area over a long series of trials of placing the same particle at different sub-pixel locations. This expected value will be ((A − A n−1 )/A) a n . Thus, an iterative estimation of sub-pixel coverage A n , starting with n = 0, is given by the recurrence relation</p><formula xml:id="formula_0">A 0 = 0, A n = A n−1 + A − A n−1 A a n . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>We can simplify this by normalizing the pixel area, setting A = 1 (and thus 0 &lt; a n ≤ 1). If all particle sizes a n are equal (a n = a, for all n), the closed-form solution of this recurrence for n ≥ 0 is the function</p><formula xml:id="formula_2">A n = 1 − (1 − a) n . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Since 0 &lt; a ≤ 1, lim n→∞ A n = 1. If the a n are not equal, the equation above simply has to multiply n different (1 − a n ) terms accordingly.</p><p>Particle super-sampling with replacement. However, because our architecture employs extensive super-sampling of particles, this means that we are in fact sampling many particles multiple times, i.e., we sample with replacement. Thus, n above must be the number of unique particles hit by k samples, where n ≤ k. Keeping track of individual sampled particles is too costly, so we cannot compute the exact number n(k) of unique particles hit over k iterations. However, for any k ≥ 0, we can compute the expected value E n(k) , via the recurrence relation</p><formula xml:id="formula_4">E n(0) = 0,<label>(3)</label></formula><formula xml:id="formula_5">E n(k) = E n(k − 1) + N − E n(k − 1) N . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Here, N is the number of particles out of which we are choosing the k samples, which in our case must be the total number of particles with a depth ≤ d (i.e., closer than the representative depth d for which we are accumulating coverage). For any given pixel, we obtain the corresponding view-dependent number N by performing volume rendering into a per-pixel particle density histogram, as described in Sect. 4.4. The closed-form solution of this recurrence relation, for k ≥ 0, is</p><formula xml:id="formula_7">E n(k) = 1 − M k 1 − M , with M := 1 − 1 N . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>We note that E n(k) ≤ k, for any k ≥ 0, and lim N→∞ E n(k) = k, as expected. </p><formula xml:id="formula_9">Δ E n(k) := E n(k) − E n(k − 1) = 1 − E n(k − 1) N . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>Particle sizes. In the derivations above, we have considered particles of size a n . However, for particles that we cannot consider to be extremely small compared to the pixel size A, we would first have to clip the particle against the pixel footprint in order to determine the actual size within the footprint. See Fig. <ref type="figure" target="#fig_0">3</ref> (right). In practice, we only do this probabilistically, by using effective particle sizes a n that are precomputed as the expected values of the area of the particle inside the pixel, instead of the full particle size. We have also implemented exact clipping for comparison, and the differences in practice are negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Depth Confidence</head><p>To obtain an interpretable probabilistic notion of depth confidence associated with the current representative depth of a given pixel, we interpret (1 − A n ) as the probability that a randomly cast ray in the pixel's footprint, with its location determined by a uniform (spatial) distribution, hits a new particle at a sub-pixel location where no particle has been hit before. The confidence for not hitting such a particle that could have a farther depth than the representative depth is therefore A n .</p><p>(1) Assuming constant particle size a, we can define the depth confidence C(k) after the k'th sample using Eq. 2 for the estimated sub-pixel coverage A n , with n given by the expected value E n(k) , as</p><formula xml:id="formula_11">C(k) := A n(k) ≈ 1 − (1 − a) n , with n := E n(k) ,<label>(7)</label></formula><p>with E n(k) given by Eq. 5.</p><p>(2) For non-constant particle sizes a n , using Eq. 1 (with A = 1) we iteratively estimate the depth confidence</p><formula xml:id="formula_12">C(k) := A n(k) ≈ 1 − k ∏ i=1 (1 − a i ) n i , with n i := Δ E n(i) , (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>with Δ E n(i) given by Eq. 6. We note that in addition to Eq. 8 being only an expected value, it is also an approximation because the expected increments E Δ n(i) for a particle of size a n get split over different particle sizes indiscriminately. However, Eq. 8 works well in practice. Fig. <ref type="figure" target="#fig_3">8</ref> illustrates depth confidence curves for different particle sizes a, sample counts k, and different N. Fig. <ref type="figure">6</ref> depicts a real example.</p><p>Sample count. To be able to update the per-pixel C(k) incrementally from sample to sample, in addition to the accumulated value C we also need to store and update a per-pixel sample count k. See algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Double-Buffered Depth Confidence Updates</head><p>The representative depth and the associated depth confidence value for each pixel try to achieve a trade-off between two opposing goals:</p><p>In order to obtain the maximum possible amount of culling, we want an as small as possible representative depth for each pixel. However, this will only lead to sufficient culling if the associated confidence is also high enough: A small representative depth is not useful if its associated confidence is too low. Likewise, a high confidence is not useful if the depth is too large, because large depth values will usually occlude only a smaller number of particles. We quantify these considerations jointly, by estimating the number of occluded particles OP(d,C) (see Eq. 12) for a given depth and confidence pair (d,C).</p><p>Depth confidence is accumulated over occlusion sampling passes, which increase sub-pixel coverage and thus depth confidence. However, for a new representative depth value, the associated confidence needs a sufficient number of samples to build up. While this is happening, for some time the confidence value is low, and therefore OP(d,C) is small. We therefore maintain double-buffered depth and confidence maps, i.e., we maintain two buffers with one depth and confidence map each:</p><p>Cull buffer. The depth/confidence pair in the cull buffer, denoted by (d cull ,C cull ), is the one that so far has the largest occlusion value OP(d cull ,C cull ). Therefore, this is the buffer that we use for culling.</p><p>Accumulation buffer. The depth/confidence pair in the accumulation buffer, denoted by (d,C), is used to try and accumulate confidence for a smaller, i.e., better, representative depth d than the current d cull . However, while confidence C is accumulating due to new samples coming in progressively, the pair (d cull ,C cull ) will still be used for culling. Only when the accumulated number of occluded particles OP(d,C) surpasses the number of occluded particles OP(d cull ,C cull ), we perform the assignment (d cull ,C cull ) = (d,C), overwriting the previous values in the cull buffer. The accumulation buffer is not changed after having been copied. Accumulation and double-buffering simply continue in the same way, to try to improve OP(d,C) even further (see Sect. 4.5), in order to potentially improve OP(d cull ,C cull ) again. See algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Per-Pixel Particle Density Functions</head><p>Several of our probability computations require an estimate for the number of particles that are in front of or behind a certain depth. To be able to estimate these numbers, we maintain a coarse 3D density volume D on a regular grid that stores a 3D scalar density function of particles, i.e., (x, y, z) → D(x, y, z) gives a number of particles per unit volume. For each pixel, we then extract a 1D particle density function D(t) along a ray t through that pixel via volume ray casting, gathering particle densities integrated over the 2D footprint A of the pixel, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D(t) :=</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A D x(t), y(t), z(t) dA,</head><p>A ⊥ to the ray t.   Dynamic volume updates. For correct estimates, the volume D must correspond to particles still being considered for sampling. Thus, we dynamically update D when a meshlet reaches the occlusion class O. This is efficient, because D is low-resolution, and we pre-compute a (static) list of voxels affected by any meshlet becoming occluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Estimating and Improving Particle Occlusion</head><p>From its per-pixel particle density function D(t), we can estimate the number of particles behind a given pixel with representative depth d as</p><formula xml:id="formula_14">B(d) := D d, d max . (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>To get the number of occluded particles, we need to take into account the sub-pixel coverage of the pixel, which is given by the confidence C. We therefore get an estimate for the number of occluded particles as</p><formula xml:id="formula_16">OP(d,C) := B(d)C. (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>We now want to compare two depths d s and d in terms of numbers of occluded particles, with d s &lt; d to consider depth improvements. For this, we compute the unknown confidence C s to be associated with d s , such that OP(d s ,C s ) &gt; OP(d,C), i.e., that more particles are occluded. The minimum threshold for the unknown confidence C s is therefore</p><formula xml:id="formula_18">C s &gt; B(d) B(d s ) C. (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>Here, we always have C s &lt; C, because we only consider d s &lt; d, and thus B(d s ) &gt; B(d). Now, we want the minimum number n of unique particles, such that the confidence C s above is reached as</p><formula xml:id="formula_20">C s = C s (n).</formula><p>This n can be determined by inverting the pixel coverage relationship</p><formula xml:id="formula_21">C s (n) = 1 − (1 − a) n . (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>The number n depends on d s , and we therefore now write n(d s ). Combining the above two equations, its minimum threshold is given by</p><formula xml:id="formula_23">n(d s ) &gt; log (1−a) 1 − B(d) B(d s ) C . (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>The base (1 − a) of the logarithm here is less than one. Thus, n(d s ) &gt; 0. Now, we additionally have to take into account that we are actually sampling with replacement. We can solve this problem by computing the required minimum k such that the expected value of the corresponding number of unique samples is at least n(d s ). Using Eq. 5, we get</p><formula xml:id="formula_25">k(d s ) = log M 1 − (1 − M) E n(k(d s )) . (<label>16</label></formula><formula xml:id="formula_26">)</formula><p>In this context, we already know that we want E n(k(d s )) ≥ n(d s ), with n(d s ) as given by Eq. 15. We can therefore obtain the minimum number of necessary samples k(d s ), which, in expectation, will reach sufficient coverage C s (n) as derived in Eq. 13, Eq. 14, and Eq. 15, as</p><formula xml:id="formula_27">k(d s ) &gt; log M 1 − (1 − M) n(d s ) . (<label>17</label></formula><formula xml:id="formula_28">)</formula><p>The expected value computation, which is now only implicit in Eq. 17, corresponds to a maximum number of particles N, from which we are sampling (N in Eq. 5). Here, this must be N := D d min , d s , because our coverage only counted samples closer than d s . Thus, M must be</p><formula xml:id="formula_29">M = 1 − 1 D d min , d s . (<label>18</label></formula><formula xml:id="formula_30">)</formula><p>As above, the base of the logarithm is less than one, because 0 &lt; M &lt; 1.</p><p>In our framework, we use the minimum threshold k(d s ) computed as</p><formula xml:id="formula_31">k(d s ) = log M 1 − n max D d min , d s , (<label>19</label></formula><formula xml:id="formula_32">)</formula><p>where we have defined the clamped value n max as</p><formula xml:id="formula_33">n max := min n(d s ), η • D d min , d s , with 0 &lt; η &lt; 1. (<label>20</label></formula><formula xml:id="formula_34">)</formula><p>We compute n(d s ) as the right-hand side of Eq. 15. The reason for clamping is to prevent k(d s ) to increase without bound, due to the logarithm. In our implementation, we use a fixed percentage of η = 0.98. Moreover, in practice we in fact check explicitly whether n(d s ) would be clamped by Eq. 20. If that is the case, we treat this sample with a symbolic k(d s ) = ∞, from which we know without further computation that the sample acceptance probability below (Eq. 21) will be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Probability of fast enough culling improvement</head><p>We However, if we assign d = d s as the new representative depth, the current depth confidence C associated with d becomes invalid, because then the sub-pixel region known to have depth ≤ d s is only the area a s of this particle. That is, if we update d we essentially have to restart accumulating confidence from scratch. Because we are sampling particles progressively, the crucial question of the latter point is whether the confidence C s of depth d s would accumulate fast enough. We define fast enough in terms of a number of samples N budget . That is, if we are willing to wait for N budget future samples, will we then have reached a confidence C s such that OP(d s ,C s ) &gt; OP(d,C)?</p><p>In order to answer this, we compute the probability of this happening.</p><p>Eq. 19 tells us how many samples k(d s ) with a depth ≤ d s we need, in expectation, to <ref type="figure">reach OP(d s ,C s ) &gt; OP(d,C</ref>). However, our pipeline samples particles with uniform probability over the entire depth range [d min , d max ]. We thus need the probability of seeing a large enough subset of particles with depth ≤ d s , within a budget of N budget samples. A sample's depth being ≤ d s or not constitutes a Bernoulli trial. Thus, we can compute the probability of getting at least k := k(d s ) samples with depth ≤ d s , in N = N budget samples, from the binomial distribution</p><formula xml:id="formula_35">P d s := P k ≥ k(d s ) = N ∑ k=k(d s ) N k p k (1 − p) N−k . (<label>21</label></formula><formula xml:id="formula_36">)</formula><p>The probability p is given by the particle density function D(t) as</p><formula xml:id="formula_37">p = p(d s ) := D d min , d s D d min , d max . (<label>22</label></formula><formula xml:id="formula_38">)</formula><p>Note that if the minimum threshold k(d s ) exceeds the maximum allowed N budget , the probability P d s will be zero, as should be expected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Depth Confidence Pyramid</head><p>We maintain the depth confidence maps in the cull buffer in a multiresolution pyramid, like a mipmap, to facilitate hierarchical occlusion tests similar to hierarchical occlusion maps <ref type="bibr" target="#b40">[41]</ref>, see Fig. <ref type="figure">4</ref> (top right). The strategies described above are used to update the highest-resolution pyramid level LOD 0 . All other, coarser-resolution levels LOD i , with i &gt; 0, are updated with the iterative update rule</p><formula xml:id="formula_39">(d,C) LOD i = max p∈N d p , 1 n ∑ p∈N C p LOD i−1 . (<label>23</label></formula><formula xml:id="formula_40">)</formula><p>This rule combines all pixels p in a pixel neighborhood N , comprising n pixels, in resolution level LOD i−1 , into a single pixel in resolution level LOD i . As in standard mipmaps, we use neighborhoods of 2 × 2 pixels (n = 4). The maximum operator for depth values guarantees that the combined depth is representative for the whole coarser-resolution pixel, and averaging the confidence gives the correct total confidence, corresponding to the actual combined normalized sub-pixel coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OCCLUSION CULLING</head><p>In every iteration, the current cull buffer is used for confidence-based occlusion culling. We perform hierarchical culling, first testing nodes, and then testing the meshlets contained inside each node if necessary.</p><p>Confidence-based culling. We test the axis-aligned bounding box of each meshlet/node for occlusion in two ways: (1) The bounding box is rasterized, and the standard GPU depth test determines whether the depth test failed (occluded with respect to the depth buffer) or passed (not occluded). The depth buffer used for this test is the representative depth d cull stored in the cull buffer. (2) Considering sub-pixel occlusion, it is important to take into account that particles in a node/meshlet that failed the depth test can still be visible. This depends on the associated confidence values C cull . We aggregate all confidence values C cull in the set of pixels in the screen space projection of the bounding box that is tested, resulting in an overall confidence for the tested geometry. Only if this confidence value is high enough (and the depth test failed), is a meshlet/node considered to be occluded (in occlusion class O, below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Occlusion Classes</head><p>At any time, meshlets belong to one of four occlusion classes (Fig. <ref type="figure" target="#fig_7">10</ref>), according to the criteria below. After each iteration, the occlusion class must be updated according to new cull buffer contents. The classes are</p><p>• O (Occluded): Meshlets considered definitely occluded with high confidence. They have failed the depth test with a high confidence of occlusion. We define this as confidence greater than a confidence threshold C occ , e.g., C occ = 0.95 (see Fig. <ref type="figure" target="#fig_6">11</ref>). Meshlets in this class will not be sampled again (unless we restart for a new view).</p><p>• M (Medium): Meshlets with medium confidence of occlusion. They have failed the depth test, but with medium confidence, so they cannot be culled yet. However, the probability of occlusion is too high to consider them as good potential occluders themselves.</p><p>• L (Low): Meshlets with low confidence of occlusion. They have failed the depth test. They could be occluded, but confidence in occlusion is low. They will be sampled as potential occluders, but only when there are not enough meshlets in P to increase occlusion. • P (Potentially visible): Potentially visible meshlets. They have passed the depth test, so they cannot be culled, independent of confidence. They are the main candidates for good potential occluders.</p><p>Initially, and every time the view has changed, all meshlets are put into class P, because nothing is known yet regarding their visibility. Then, in each iteration, a set of potential occluders is selected (see below) and sampled into the cull and accumulation buffers. Afterward, all meshlets in any class except O are culled against the cull buffer to update their occlusion class. The overall goal of iterating is to move as many meshlets as possible into class O. Meshlets are moved between classes according to their current overall confidence values and may only move in the "upward" direction, i.e., increase in confidence from iteration to iteration, because confidence values can never decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Occluder Selection</head><p>To select potential occluders in each iteration, we pull meshlets from P until we reach a pre-specified budget per iteration. If the budget is not yet reached after all meshlets in P have been consumed, we continue by sampling meshlets selected from L. We do not prevent the same meshlets from being selected as potential occluders in multiple iterations. This is crucial for our approach: It allows confidence values to increase due to super-sampling occlusion, iteratively attaining more known sub-pixel coverage and thereby occlusion.</p><p>Avoiding starvation. In the common case when there are more meshlets in P and L together than the specified budget for potential occluders per iteration, it is important to not select the same subset of potential occluders in every subsequent iteration, to avoid "starving" other good potential occluder candidates. We therefore randomize the selection of potential occluders by shuffling meshlets in P and L. This can be implemented very efficiently on GPUs using task shaders, Vulkan subgroups <ref type="bibr" target="#b20">[21]</ref>, and a random permutation buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Occlusion Convergence</head><p>After sampling nodes/meshlets in order to determine the confidence of occlusion, we need to decide whether or not occlusion computations have converged. We discuss several criteria for convergence below.</p><p>Rendering budget. We can check whether a small enough number of meshlets remains after all other meshlets have reached class O. This criterion is often too crude, unless the number of remaining meshlets is very small, because there could still be room for improving occlusion.</p><p>Overall confidence. A potential test could be to check the root node of the confidence map pyramid to determine that the total confidence in all depth values is high enough. However, this just means that the known depth values-which could be unnecessarily large-are accurate enough. Because high coverage does not directly correspond to small depth values, it does not necessarily correspond to high occlusion. For this reason, we do not use this criterion for determining convergence.</p><p>Active changes of occlusion class membership. We use the following criterion to determine convergence: If over multiple iterations no meshlet changes its occlusion class, we stop iterating and assume that we have converged to a sufficient amount of occlusion. At the same time, this implies that we have already reached high confidence for the occlusion decisions that we have performed, i.e., we are confident (with threshold C occ ) about occlusion of all meshlets in occlusion class O. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION</head><p>We exploit recent advancements in GPU architecture introduced with the NVIDIA Turing RTX GPU. We employ a new graphics pipeline that comprises task and mesh shaders, instead of the traditional pipeline.</p><p>The task/mesh shader pipeline operates on small groups of triangles called meshlets, which target vertex re-use to reduce the data to fetch in parallel rendering. However, particle data sets are not typical meshes with high vertex re-use. Rather, we are dealing with a soup of disjoint billboards with two triangles per particle. However, our occlusion culling architecture exploits the parallel processing of small groups of particles packed into meshlets for fast occlusion testing.</p><p>Meshlet generation uses the Point Cloud Library (PCL) <ref type="bibr" target="#b33">[34]</ref> in a pre-processing step to subdivide the input data into small neighborhoods of particles, which then become meshlets. We use a neighborhood size of 16 particles, because this requires 64 vertices to be drawn: Two triangles and four vertices per billboard, and 64 is currently the maximum number of emitted vertices recommended per meshlet <ref type="bibr" target="#b25">[26]</ref>.</p><p>Nodes. For the purpose of hierarchical culling of particle data sets, we group neighborhoods of meshlets together into larger nodes. Each node fits a fixed maximum number of particles (we are using 10, 000).</p><p>Mesh shaders. We use mesh shaders to generate the actual particle billboard geometry for potentially visible meshlets. Each invocation of a mesh shader operates on a meshlet, and generates the corresponding geometric primitives. These primitives are then rasterized to produce the fragments that are then shaded by a traditional fragment shader.</p><p>Task shaders. These are the most important feature of the Turing geometry pipeline for our framework. Task shaders are compute shaders that dynamically enable/disable emittance of mesh shader workgroups. We use them to efficiently determine whether whole meshlets are occluded, and issue workgroups only for potentially visible meshlets.</p><p>Vulkan. We have implemented our framework in C++ using the Vulkan API, which offers more low-level control over GPU operation than APIs such as OpenGL. We make use of Vulkan's subgroups <ref type="bibr" target="#b20">[21]</ref> to determine which meshlets to sample, and perform culling for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>We evaluate rendering quality and performance, and compare to previous work. Table <ref type="table" target="#tab_4">2</ref> lists the data sets we have used for the evaluation. A more extensive evaluation is given in the supplemental App. B.  × copper/silver mixture 232,000,000 14,420,300 meshlets even when not all of their particles are already sufficiently covered, leading to artifacts. However, Fig. <ref type="figure" target="#fig_6">11</ref> demonstrates that our probabilistic approach is quite robust. In practice, we use C occ = 0.95. Fig. <ref type="figure" target="#fig_7">12</ref> illustrates the importance of super-sampling, by comparing the quality of our method and the method of Grottel et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>We also employ screen-space ambient occlusion as a visual cue, since the understanding of spatial structure is essential for molecular dynamics simulations <ref type="bibr" target="#b24">[25]</ref>. Since we already maintain a low-resolution particle density volume (see Sect. 4.4), we could easily also extend this to the object-space technique presented by Grottel et al. <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Performance</head><p>We evaluate culling performance in terms of percentage of particles culled, time taken to cull, and the rendering speed (frames per second/fps) during ("dc") and after ("ac") culling. Performance was measured on a dual Intel Xeon X5550 2.67 GHz, Geforce RTX Titan (24 GB), at 1920 × 1080 resolution. Table <ref type="table" target="#tab_5">3</ref> gives performance results for culling and rendering, for three different views. (See full tables and more data sets in the supplemental material.) Our method results in a significant speed up after culling is finished ("ac"). This is more apparent for large data sets, such as the 16 × copper/silver mixture data set, where the speed up is up to 10×. We note that the fps during culling ("dc") is usually faster than the fps without culling ("w/o c"). This is because our method determines meshlets in class O progressively, and therefore will place more and more meshlets there as confidence builds up to greater or equal the confidence threshold C occ . This in turn will result in a progressive decrease in the number of meshlets to render until the occlusion computations converge, leading to higher fps.</p><p>Table <ref type="table" target="#tab_6">4</ref> shows percentages of culled meshlets, and number of samples required until convergence (full results are in the supplemental material). To evaluate our probabilistic acceptance strategy (Sect. 4.5), we also report numbers where instead we simply accept incoming sam-Fig. <ref type="figure" target="#fig_7">12</ref>. Top: Our method successfully captures interesting features and removes noise due to super-sampling. Bottom: The same views using a single sample per pixel (with Grottel et al. <ref type="bibr" target="#b15">[16]</ref>) exhibit undersampling. ples randomly with a probability of 50% (a coin flip). Our probabilistic approach achieves much higher culling percentages than random acceptance (about 2-8×). We note that the bigger the data set, the smaller the particles' projection is on screen, and therefore more samples are required in order to achieve confidence in the depth used for culling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison to Previous Work</head><p>Our approach is conceptually orthogonal to the actual sampling method for the final rendering: Both rasterization and ray tracing are valid approaches. In our implementation, we perform ray casting of the particle density volume, but sample particles via rasterization <ref type="bibr" target="#b16">[17]</ref>. Given that ray tracers exploit spatial data structures to accelerate ray-geometry intersections, integration of our culling approach with these data structures and the corresponding (dynamic) updates go far beyond the scope of this work and will require more in-depth investigation. We therefore compare our results to the free and open-source particle renderers available in MegaMol, given that they perform better than some other available implementations <ref type="bibr" target="#b13">[14]</ref>. We use both the baseline brute-force SSBO-based renderer, and the multilevel culling variant by Grottel et al. <ref type="bibr" target="#b15">[16]</ref> (with all culling and caching enabled). Both of these renderers, however, do not support multisampling, so their output will always suffer from aliasing. The performance of these approaches is still useful for interpreting the performance of our approach: The brute-force renderer represents a low-overhead, straightforward implementation without any acceleration structure. The impact of large data sets on performance is mitigated by the low number of fragments generated for each sphere (just one in the limit case), but this renderer cannot scale to arbitrary data sizes, it is both limited by available GPU RAM and shader performance. The occlusion culling approach uses both occlusion queries and culling against a hierarchical z buffer in the vertex stage, and thus serves as a reference for a basic culling technique.</p><p>In principle, it scales to arbitrary data sizes because only occluders are cached on the GPU. It progressively updates the GPU cache, but renders and tests the whole data set, thus streaming the uncached data to the GPU. Interactivity is limited by upload bandwidth and, to a lesser degree, rendering performance. Our approach is completely progressive to guarantee interactivity. Since on current GPUs 8 GB RAM or more are common, keeping all data in GPU memory is not a problem.</p><p>Rendering quality is significantly improved over brute-force and smoothing alike (see also App. D). Our method captures interesting features better (see dashed ellipses in Fig. <ref type="figure" target="#fig_7">12</ref>). Sub-pixel details allow the discovery of features without bias from smoothing or LOD.</p><p>Performance. The MegaMol brute-force renderer is consistently faster than our approach with culling disabled ("w/o c"): As long as the view does not change, our approach always uses the averaging pass over new samples and the previous results to reduce aliasing, while the MegaMol renderers do not. Also, the brute-force renderer in MegaMol is probably better optimized than our sampling. Despite this, the converged culling achieves much better performance than the culling-assisted MegaMol rendering, especially for large data.</p><p>Culling. Our culling percentage is similar to the approach by Grottel et al., with a few notable exceptions: Since our meshlets have much finer granularity than the bricks used by Grottel et al., we can remove particle groups from medium-density gas phases present in these data sets. Their approach can only accomplish this for each sphere separately in the hierarchical z test and thus has higher geometry processing load.</p><p>Ray tracing. We compare our method against GPU ray tracing using P-k-d trees <ref type="bibr" target="#b11">[12]</ref> in App. D. While P-k-d trees are faster per sample, increasing the number of samples per pixel to allow for sub-pixel detail degrades performance linearly, as expected. Interactively sampling tens to hundreds of times per pixel is only feasible with P-k-d trees if the ray tracer accumulates samples as long as no user interaction takes place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Storage Requirements</head><p>We consider the memory usage of our approach in two categories:</p><p>Data-dependent storage. We maintain density volumes D of very small resolutions, from 700k to 1M voxels (in total, not per dimension), depending on the size of the data set. We store a (node/meshlet)/voxel correspondence vector that for each node/meshlet stores the indices of the voxels in D to which they correspond. For each node/meshlet we store: (1) An axis-aligned bounding box in four float (32 bit) values.</p><p>(2) An unsigned int (8 bit) for its occlusion class. (3) An index to the voxel in the (node/meshlet)/voxel correspondence vector (32 bit).</p><p>Resolution-dependent storage. Per pixel, we store the representative depth d, the depth confidence C(k), and the sample count k. We store d and C(k) in 32 bit floats, and k in an unsigned 32 bit int. Therefore, for the highest resolution pyramid level LOD 0 , we store 3 × 32 = 96 bits/pixel. For the lower-resolution pyramid levels we store only 2 × 32 = 64 bits/pixel. Finally, for each pixel in LOD 0 , we store the discretized function D(t) in a maximum number of 64 bins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>Our culling architecture is probabilistic in many respects, allowing us to target high-quality rendering with extensive super-sampling of finely detailed particle data with high performance. All our probabilistic estimates are based on expected value derivations, and therefore our pipeline operates with the idea of expected occlusion. Due to the large number of samples and particles in the data we are targeting, this turns out to be a very good strategy. We probabilistically estimate both subpixel coverage and probabilities of hitting samples in front or behind a certain depth, using a particle density estimate from a coarse density volume. Building on the latter capability, we can efficiently estimate an acceptance probability that determines whether an incoming depth sample is likely to improve occlusion and should therefore be accepted or not. Our results have shown that this strategy works very well in practice and significantly improves the percentage of culled particles.</p><p>For future work, we would like to investigate how progressive confidence can be exploited for even larger data sets or time-dependent data, where not all particles fit into GPU memory. This approach could also be combined with partial and differential updates to a GPU-resident cache that reflects the state of the current confidence class distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Sub-pixel coverage. We do not discretize or store sub-pixel samples on a grid (left), but instead probabilistically estimate the accumulated sub-pixel coverage A n(k) (right) after k samples, out of which n(k) are from unique particles (here, n = 3), where n is also probabilistically estimated. The confidence C(k) for the pixel's representative depth d then corresponds to the covered area, for which depth ≤ d is guaranteed.</figDesc><graphic coords="2,43.43,49.64,229.02,72.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(left)), we estimate confidence values probabilistically, conceptually corresponding to continuous sub-pixel coverage (Fig. 3 (right)). For the depth value d of each pixel in screen space, we estimate an associated confidence value C, as a probability 0 ≤ C ≤ 1 that a randomly chosen, new sample in this pixel would hit a particle with a depth ≤ d. (Smaller d meaning closer to the camera.) We call this d the representative depth of this pixel, with associated depth confidence C. particles nodes meshlets Fig. 5. Meshlets and nodes. For efficiency, we use two hierarchy levels to group particles. Meshlets (center) comprise a few particles (e.g., 16) each, and facilitate fine-grained culling. Meshlets are further grouped into nodes (right), to test a group of meshlets for occlusion in one step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Confidence accumulation over sampling iterations for the Laser Ablation data set of about 48 million particles. From left to right, confidence accumulates from iteration to iteration (k = 4 to 32 samples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Depth confidence, for the representative depth of a pixel, for various sample counts k, particle sizes a, and closer particle counts N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 9 )Algorithm 1 :</head><label>91</label><figDesc>Given D(t), for any depth interval [d a , d b ] along the ray t we then get the corresponding particle count between depth d a and depth d b as D d a , d b := Per-pixel particle density histograms. While conceptually D(t) is a continuous function, for each pixel our ray caster traverses the particle Double-Buffered Depth Confidence Updates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Per-pixel particle density functions. We maintain a 3D number of particles/unit volume density function in a low-resolution volume D(x, y, z), with dynamic updates for fully occluded meshlets. From this, we compute 1D per-pixel particle density histograms D(t) via volume ray casting. density volume D and computes a discretized D(t) in the form of a particle density histogram. That is, we store particle counts D d i , d i+1 in discrete bins [d i , d i+1 ], from depth d min to d max . See Fig. 9 (right).Dynamic volume updates. For correct estimates, the volume D must correspond to particles still being considered for sampling. Thus, we dynamically update D when a meshlet reaches the occlusion class O. This is efficient, because D is low-resolution, and we pre-compute a (static) list of voxels affected by any meshlet becoming occluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Varying confidence threshold C occ for class O. Ground truth renderings (left) use 1,024 samples per pixel, without any culling. We compare two different particle sizes, large at the top, and small at the bottom. From left to the right, we decrease the confidence threshold C occ that determines when meshlets are moved into occlusion class O. (Top rows) Renderings. (Bottom rows) Depth difference images highlighting pixels for which the representative depth is closer than the ground truth with confidence ≥ C occ . Too low C occ increases culling, but result in artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>7. 1</head><label>1</label><figDesc>Rendering Quality In Fig. 11, we analyze the effects of different confidence thresholds C occ for occlusion class O. Too low values for C occ lead to culling of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Terminology and main concepts used in this paper.</head><label>1</label><figDesc></figDesc><table><row><cell>term</cell><cell>explanation</cell></row><row><cell>occlusion culling</cell><cell>determining occluded subsets of</cell></row><row><cell></cell><cell>the data. here: to exclude groups</cell></row><row><cell></cell><cell>of particles from super-sampling</cell></row><row><cell>sub-pixel super-sampling</cell><cell>more than one ray per pixel</cell></row><row><cell>sub-pixel coverage</cell><cell>percentage of occluded pixel area</cell></row><row><cell>depth confidence; C</cell><cell>sub-pixel area with depth ≤ d</cell></row><row><cell>representative depth; d</cell><cell>depth valid for sub-pixel area C</cell></row><row><cell>depth confidence map</cell><cell>(d,C) tracked in image pyramid</cell></row><row><cell>particle density volume; D</cell><cell>3D volume of local particle density</cell></row><row><cell cols="2">particle density function; D(t) density along ray; extracted from D</cell></row><row><cell>meshlet</cell><cell>a small group of particles</cell></row><row><cell></cell><cell>(used in Vulkan mesh shaders)</cell></row><row><cell>node</cell><cell>a group of meshlets</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Bringing everything together, we use the probability P d s determined by Eq. 21 as the sample acceptance probability in algorithm 1: Every time a new sample with depth d s comes in, we compute the minimum sample number k(d s ) using Eq. 19, which requires computing Eq. 15, Eq. 18, and Eq. 20 first. We then compute Eq. 21 for the new sample. In practice, however, evaluating Eq. 21 directly is inefficient. We therefore use a much faster standard approximation, which is very accurate. The details are described in App. A. Finally, in order to determine whether the new sample with depth d s will be accepted or not, we compute a uniform random number u ∈ [0, 1]. If the random number u ≤ P(d s ), we accept the sample with depth d s . Otherwise, we do not.</figDesc><table><row><cell>Depth Test</cell><cell>Confidence</cell><cell>Occlusion Classes</cell></row><row><cell></cell><cell>High</cell><cell>OCCLUDED</cell></row><row><cell></cell><cell>C occ</cell><cell></cell></row><row><cell>Fail</cell><cell>Medium</cell><cell>MEDIUM</cell></row><row><cell></cell><cell></cell><cell>High</cell></row><row><cell></cell><cell>Low</cell><cell>LOW</cell></row><row><cell>Pass</cell><cell>Any</cell><cell>PVISIBLE</cell></row><row><cell cols="3">Fig. 10. Occlusion classes of meshlets are determined during occlusion</cell></row><row><cell cols="3">testing from computed confidence values and from passing or failing the</cell></row><row><cell cols="3">depth test. Only class O (occluded) will be excluded from rendering.</cell></row><row><cell cols="3">4.5.2 Sample acceptance probability and strategy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Data sets used for evaluation. (See the appendixes for more.)</figDesc><table><row><cell>data set</cell><cell># particles</cell><cell># meshlets</cell></row><row><cell>covid-19</cell><cell>40,048,645</cell><cell>2,733,504</cell></row><row><cell>large laser ablation</cell><cell cols="2">199,940,704 12,513,900</cell></row><row><cell>16</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Rendering performance. Average rendering speed for different data set/view (v) combinations: averaged over the whole culling process (dc); after occlusion convergence (ac); without occlusion culling (w/o c). Confidence threshold C occ = 0.95; rendering budget b = 2 meshlets/node.</figDesc><table><row><cell>data set</cell><cell>v</cell><cell>dc [fps]</cell><cell>ac [fps]</cell><cell>w/o c [fps]</cell><cell>MegaMol [fps]</cell><cell>Grottel [fps]</cell></row><row><cell></cell><cell>0</cell><cell>22</cell><cell>56</cell><cell>19</cell><cell>32</cell><cell>18</cell></row><row><cell>covid-19</cell><cell>1</cell><cell>25</cell><cell>63</cell><cell>28</cell><cell>38</cell><cell>17</cell></row><row><cell></cell><cell>2</cell><cell>24</cell><cell>59</cell><cell>27</cell><cell>50</cell><cell>20</cell></row><row><cell>large laser ablation</cell><cell>0 1 2</cell><cell>13 18 14</cell><cell>35 42 36</cell><cell>16 15 16</cell><cell>14 9 13</cell><cell>59 28 40</cell></row><row><cell>16 × copper/</cell><cell>0</cell><cell>8</cell><cell>34</cell><cell>3</cell><cell>7</cell><cell>35</cell></row><row><cell>silver</cell><cell>1</cell><cell>12</cell><cell>41</cell><cell>6</cell><cell>13</cell><cell>12</cell></row><row><cell>mixture</cell><cell>2</cell><cell>9</cell><cell>37</cell><cell>5</cell><cell>8</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Culling efficiency. Percentage of culled meshlets (c) and number of samples (smp) required for culling convergence. (Column "random" uses 50:50 sample acceptance for comparison only.) Confidence threshold C occ = 0.95; rendering budget b = 2 meshlets per node.</figDesc><table><row><cell>data set</cell><cell>view</cell><cell cols="4">probabilistic c [%] smp c [%] smp random</cell><cell>Grottel c [%]</cell></row><row><cell></cell><cell>0</cell><cell>75</cell><cell>112</cell><cell>42</cell><cell>200</cell><cell>56</cell></row><row><cell>covid-19</cell><cell>1</cell><cell>58</cell><cell>120</cell><cell>29</cell><cell>128</cell><cell>32</cell></row><row><cell></cell><cell>2</cell><cell>60</cell><cell>120</cell><cell>25</cell><cell>152</cell><cell>24</cell></row><row><cell>large laser ablation</cell><cell>0 1 2</cell><cell>68 79 70</cell><cell>208 136 176</cell><cell>8 36 32</cell><cell>64 96 272</cell><cell>89 79 83</cell></row><row><cell>16 × copper/</cell><cell>0</cell><cell>89</cell><cell>88</cell><cell>35</cell><cell>104</cell><cell>93</cell></row><row><cell>silver</cell><cell>1</cell><cell>92</cell><cell>72</cell><cell>59</cell><cell>104</cell><cell>82</cell></row><row><cell>mixture</cell><cell>2</cell><cell>90</cell><cell>88</cell><cell>44</cell><cell>112</cell><cell>87</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">(d cull ,C cull ) = (d,C); k cull = k;</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coherent Hierarchical Culling: Hardware Occlusion Queries Made Useful</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Purgathofer</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2004.00793.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="615" to="624" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Visibility in computer graphics. Environment and Planning B: Planning and Design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno>doi: 10. 1068/b2957</idno>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">2003</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The a-buffer, an antialiased hidden surface method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;84</title>
				<meeting>the 11th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;84<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ARB occlusion query2 OpenGL extension</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Licea-Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Everitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ribble</surname></persName>
		</author>
		<ptr target="https://www.khronos.org/registry/OpenGL/extensions/ARB/ARB_occlusion_query2.txt.Accessed" />
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of visibility for walkthrough applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2003.1207447</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="412" to="431" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Probability and Statistics for Engineering and the Sciences</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Devore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-01">January 2011</date>
			<pubPlace>Brooks/Cole</pubPlace>
		</imprint>
	</monogr>
	<note>8th ed.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Atomistic Visualization of Mesoscopic Whole-Cell Simulations Using Ray-Casted Instancing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12197</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the normal approximation to the binomial distribution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177731058</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1945</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Layered point clouds: a simple and efficient multiresolution structure for distributing and rendering gigantic pointsampled models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marton</surname></persName>
		</author>
		<idno>doi: 10. 1016/j.cag.2004.08.010</idno>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="815" to="826" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Far Voxels -a multiresolution framework for interactive rendering of huge complex 3d models on commodity graphics platforms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH</title>
				<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005-08">August 2005. 2005</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="878" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High Quality Interactive Rendering of Massive Point Models Using Multi-way kd-Trees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pajarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
		<idno type="DOI">10.1109/PacificGraphics.2010.20</idno>
	</analytic>
	<monogr>
		<title level="m">2010 18th Pacific Conference on Computer Graphics and Applications</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010-09">sep 2010</date>
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial partitioning strategies for memory-efficient ray tracing of particles</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gralka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno type="DOI">10.1109/LDAV51489.2020.00012</idno>
	</analytic>
	<monogr>
		<title level="m">10th IEEE Symposium on Large Data Analysis and Visualization (LDAV)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="42" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical z-buffer visibility</title>
		<author>
			<persName><forename type="first">N</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;93</title>
				<meeting>the 20th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;93<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Megamol -a prototyping framework for particle-based visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2350479</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="214" />
			<date type="published" when="2015-02">Feb 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object-Space Ambient Occlusion for Molecular Dynamics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scharnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno>doi: 10. 1109/PacificVis.2012.6183593</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Pacific Visualization Symposium 2012</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-02">Feb. 2012</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coherent culling and shading for large molecular dynamics visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dachsbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2009.01698.x</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of Eurovis</title>
				<meeting>of Eurovis</meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="953" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Splatting Illuminated Ellipsoids with Depth Correction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision, Modeling, and Visualization</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="245" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Near optimal hierarchical culling: Performance driven use of hardware occlusion queries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><surname>Balázs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="207" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The accumulation buffer: Hardware support for high-quality rendering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Haeberli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Akeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;90</title>
				<meeting>the 17th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;90<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Masked software occlusion culling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akenine-Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of High Performance Graphics, HPG &apos;16</title>
				<meeting>High Performance Graphics, HPG &apos;16<address><addrLine>Aire-la-Ville, Switzerland, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="23" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vulkan subgroup tutorial</title>
		<author>
			<persName><forename type="first">N</forename><surname>Henning</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Screen-Space Normal Distribution Function Caching for Consistent Multi-Resolution Rendering of Large Particle Data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wickenhäuser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rautek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2017.2743979</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="944" to="953" />
			<date type="published" when="2018-01">jan 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rbf volume ray casting on multicore and manycore cpus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gaither</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ray tracing and volume rendering large molecular data on multi-core and many-core architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Navratil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gaither</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UltraVis &apos;13: Proceedings of the 8th International Workshop on Ultrascale Visualization</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualization of Biomolecular Structures: State of the Art Revisited</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kozlíková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lindow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parulek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13072</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="178" to="204" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Introduction to turing mesh shaders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kubisch</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">cellVIEW: a Tool for Illustrative and Multi-Scale Rendering of Large Biomolecular Datasets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Le</forename><surname>Muzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Autin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parulek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<idno type="DOI">10.2312/vcbm.20151209</idno>
	</analytic>
	<monogr>
		<title level="j">Eurographics Workshop on Visual Computing for Biomedicine</title>
		<editor>K. Bühler, L. Linsen, and N. W. John</editor>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive Rendering of Materials and Biological Structures on Atomic and Nanoscopic Scale</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lindow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2012.03128.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1325" to="1334" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chc+ rt: Coherent hierarchical culling for ray tracing</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mattausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaspe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pajarola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="537" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chc++: Coherent hierarchical culling revisited</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mattausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pantaray: Fast ray-traced occlusion caching of massive scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pantaleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fascione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1145/1778765.1778774</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Continuous Levels-of-Detail and Visual Abstraction for Seamless Molecular Visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Parulek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jönsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12349</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="276" to="287" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing large-scale atomistic simulations in ultra-resolution immersive environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nomura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Large Data Analysis and Visualization (LDAV)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="59" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d is here: Point cloud library (pcl)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to Real-Time Ray Tracing with Vulkan</title>
		<author>
			<persName><forename type="first">N</forename><surname>Subtil</surname></persName>
		</author>
		<ptr target="https://devblogs.nvidia.com/vulkan-raytracing/.Accessed" />
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ospray -a cpu ray tracing framework for scientific visualization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Amstutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brownlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeffers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gunther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Navratil</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2599041</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="931" to="940" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CPU Ray Tracing Large Particle Data with Balanced P-k-d Trees</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Papka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Embree: A kernel framework for efficient cpu ray tracing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Woop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Benthin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
		<idno type="DOI">10.1145/2601097.2601199</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014-07">July 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visibility-culling-based geometric rendering of large-scale particle data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICVRV.2016.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2016 International Conference on Virtual Reality and Visualization, ICVRV 2016</title>
				<meeting>-2016 International Conference on Virtual Reality and Visualization, ICVRV 2016</meeting>
		<imprint>
			<date type="published" when="2017-09">sep 2017</date>
			<biblScope unit="page" from="197" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Guided visibility sampling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maierhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hesina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reshetov</surname></persName>
		</author>
		<idno type="DOI">10.1145/1141911.1141914</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="494" to="502" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visibility culling using hierarchical occlusion maps</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.1145/258734.258781</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;97</title>
				<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;97<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
