<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking the Ranks of Visual Channels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Caitlyn</forename><forename type="middle">M</forename><surname>Mccoleman</surname></persName>
							<email>caitlyn.mccoleman@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Fumeng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
							<email>timbrady@ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Franconeri</surname></persName>
							<email>franconeri@northwestern.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking the Ranks of Visual Channels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D8BAF406D20041979BAD4BC7045782FB</idno>
					<idno type="DOI">10.17605/OSF.IO/3E2QT</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>What percentage is the smaller value compared to the larger?</p><p>Reproduce the chart briefly after viewing it, for 2, 4, or 8 marks. Length and angle were the best.</p><p>Position was the best.</p><p>Fig. <ref type="figure">1</ref>. One core guideline for data visualization design is that some visual channels offer better perceptual precision than others, drawing those precision estimates from two-value ratio judgment tasks <ref type="bibr" target="#b16">[17]</ref>. (a) The figure depicts typical data (from <ref type="bibr" target="#b32">[33]</ref>, 50 participants) showing these judgments are more precise for position (e.g., bar graphs) than for area (e.g., bubble charts). We tested whether that ranking generalizes to the new task of reproducing 2 to 8 previously seen values, and analyzed reproduction bias, precision, and error using a Bayesian modeling approach. (b) The figure shows our modeled results (49 participants). The ranking did not hold, and other factors besides channel choice-like the number of values in the series-had an order of magnitude more influence on performance.</p><p>Abstract-Data can be visually represented using visual channels like position, length or luminance. An existing ranking of these visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is surprisingly little existing work that tests this assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs, across displays that almost universally depict more than two values. To simulate the information extracted from a glance at a visualization, we instead asked participants to immediately reproduce a set of values from memory after they were shown the visualization. These values could be shown in a bar graph (position (bar)), line graph (position (line)), heat map (luminance), bubble chart (area), misaligned bar graph (length), or 'wind map' (angle). With a Bayesian multilevel modeling approach, we show how the rank positions of visual channels shift across different numbers of marks (2, 4 or 8) and for bias, precision, and error measures. The ranking did not hold, even for reproductions of only 2 marks, and the new probabilistic ranking was highly inconsistent for reproductions of different numbers of marks. Other factors besides channel choice had an order of magnitude more influence on performance, such as the number of values in the series (e.g., more marks led to larger errors), or the value of each mark (e.g., small values were systematically overestimated). Every visual channel was worse for displays with 8 marks than 4, consistent with established limits on visual memory. These results point to the need for a body of empirical studies that move beyond two-value ratio judgments as a baseline for reliably ranking the quality of a visual channel, including testing new tasks (detection of trends or motifs), timescales (immediate computation, or later comparison), and the number of values (from a handful, to thousands).</p><p>Index Terms-DataType Agnostic; Human-Subjects Quantitative Studies; Perception &amp; Cognition; Charts, Diagrams, and Plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Metric values can be efficiently transmitted to the human visual system across a set of channels, including position, length, or intensity <ref type="bibr" target="#b5">[6]</ref> (see <ref type="bibr" target="#b56">[56]</ref> for review). When creating a visualization, designers face a choice of which channel to depict metric values, with a major constraint being a ranking of putative perceptual precision of that channel. This ranking is organized by either expert judgment <ref type="bibr" target="#b50">[50]</ref> or operationalized by a particular task. The most referenced operationalization is the precision of making ratio judgments between two values <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b78">77]</ref>.</p><p>For example, in Fig. <ref type="figure" target="#fig_1">2a</ref>, the viewer might use the position channel to estimate that the value for A is 85% of the value for B, close to the correct answer of 80%. The typical (log) error for this judgment is shown in Fig. <ref type="figure">1a</ref>. It is typically the lowest error of any channel. Making the same judgment in Fig. <ref type="figure" target="#fig_1">2b</ref> between A and B (now separated vertically) is a bit tougher, as reflected by the larger error value for ratio judgments of length in Fig. <ref type="figure">1a</ref>. Finally, Fig. <ref type="figure" target="#fig_1">2c</ref> shows the same data plotted as luminances. While we do not know of empirical measurements of ratio judgment error for this channel, expert judgment <ref type="bibr" target="#b16">[17]</ref> (as well as ours) suggests that the error would be quite high <ref type="bibr" target="#b50">[50]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Beyond a ranking based on two-value ratio precision</head><p>The channel ranking derived from error measurements of two-value ratio judgments likely deserves its role as a key factor that determines the choice of a channel for depicting metric values. But there is an implicit assumption that it should extrapolate broadly across the types of lowerlevel visual tasks that viewers execute in real-world visualizations and visual analytics. This is a bold assumption, because visualizations require that we extract, remember, and compare sets of statistics, trends and motifs, across visualizations that almost universally depict more than two values, leading to increasing unease about the dominance of this method of ranking channels <ref type="bibr" target="#b6">[7]</ref>. A taxonomy of such operations presents ten low-level perceptual tasks used in analyzing a set of datasets <ref type="bibr" target="#b1">[2]</ref>. Interestingly, computing ratios does not appear as a task. 'Retrieving a value' is present, and it is plausible that this task is the foundation of a two-value ratio judgment for charts <ref type="bibr" target="#b16">[17]</ref>. A more recent survey includes 'computing derived value' but also reveals concerns about task-dependent effectiveness <ref type="bibr" target="#b66">[66]</ref>. In Fig. <ref type="figure" target="#fig_1">2a</ref>, if the viewer knew that the maximum value of the y-axis were 10, computing a ratio between any bar and that number would allow the viewer to extract the value of a single bar from an unlabeled (or sparsely-labeled) axis. In Fig. <ref type="figure" target="#fig_1">2</ref>, for each of the three visualizations, where are the three highest, or lowest, values for A or B? Where are the largest (or average) differences for each value pairing across the series? There are dozens of such critical comparisons that all involve more than two points (see <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b57">57]</ref> for review), and there is insufficient empirical work that evaluates whether the ranking of channels extracted from two-value ratio tasks also applies to them (see Sec. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The present study: reproduction as a proxy for various comparison tasks</head><p>How might one compare performance for each channel across such a long list of potential comparison tasks? We start with the assumption that many of these comparison tasks require that one set of values be held in visual memory, and that memory is compared to a subsequently perceived set. For example, in any panel of Fig. <ref type="figure" target="#fig_1">2</ref>, computing a twovalue ratio might not feel like it requires a heavy memory component. But comparing the global shape of series A versus series B feels far more capacity-limited <ref type="bibr" target="#b79">[78]</ref> and memory-intensive <ref type="bibr" target="#b20">[21]</ref>. Indeed, evidence from the visual memory and attention literatures suggest that for such more complex comparisons, one must first inspect A, hold the set in memory, and then compare that memory to set B <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b85">84]</ref>. At the very least, comparisons that are not 'within the eyespan' <ref type="bibr" target="#b80">[79]</ref>, requiring an eye movement or turn of a page, certainly require, and will be limited by, visual memory. Visual memory is highly capacity-limited <ref type="bibr" target="#b74">[73]</ref>. As we attempt to remember more information, precision plummets, bias quickly increases, and storage capacity hits ceiling limits (see Sec. 2.3). Therefore, we would expect the number of data values involved in comparison tasks to predict whether the viewer is successful. Because memory serves as a critical gateway for performance in comparison tasks, the present study measures how a viewer's memory precision, bias, and overall error is affected by the channel used to encode a dataset, and how those measures are affected by the number of data values that the viewer is asked to process and remember.</p><p>The present study measures memory using a reproduction task, under the assumption that this measurement will generalize to a variety of comparison tasks. If we had instead used a more specific comparison task, which would we pick? Comparisons of data distributions? A search for the longest set of relatively low values? Ask for the differences in the global shape across the two series? If so, what type of difference, and how would it be reported? And how different should the two data series be, and in what ways? The present reproduction task allows a first look at how channel and number of marks affects reproduction performance, without the need to consider these more specific operationalizations of the various types of visual comparisons. We hope that after this initial exploration, the field can begin to ask more targeted empirical questions for particular comparison tasks.</p><p>We asked participants to immediately reproduce a set of values seen moments earlier across six channels and three numbers of marks {2, 4, 8}. Our results from a Bayesian multilevel model show that the previous ranking <ref type="bibr" target="#b17">[18]</ref> does not hold, even for reproducing only 2 marks. The new probabilistic ranking also varies with the number of marks. Other factors besides channel choice have an order of magnitude more influence on performance, such as the number of marks in the series, or the value of each mark. Across every visual channel, performance drops precipitously when more than just a few marks have to be stored, consistent with the known limits on visual memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Contributions</head><p>This work challenges the assumption that the ranking derived from the precision of judging a ratio between two visual marks will extrapolate to new tasks, especially those that involve more than two marks. Our primary contributions are as follows.</p><p>• Experimental study results on the effects of six typical encoding channels, and the number of marks {2, 4, 8}, on a task of reproducing a set of visualized data, leading to a reassessment of the value of rankings based on two-value ratio tasks. • A contextual, probabilistic ranking of the six visual channels on three statistical measures: bias, precision, and error. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Here we surveyed work in visual perception, information visualization, and visual working memory to gather considerations for factors that may impact visual reproduction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context and bias effects on visual judgements</head><p>While Cleveland and McGill <ref type="bibr" target="#b17">[18]</ref> tested the precision of ratio judgements with only two relevant values for the judgment itself, they also showed decreased precision for displays where those values were crowded by adding other values in the display <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>. More recent work <ref type="bibr" target="#b78">[77,</ref><ref type="bibr" target="#b88">87]</ref> identified similar impairments. In other reproduction tasks, like the one used in the present study, surrounding values in a display created memory biases, such that recollections of a single relevant value were repulsed from the 0, .5, and 1.0 proportion of a second larger reference bar <ref type="bibr" target="#b52">[52]</ref>. Memory bias has been shown even for values presented alone, such that tall bars with a high height:width ratio were underestimated, and wide bars with a low height:width ratio were overestimated <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluations beyond two-value ratio precision</head><p>After one study showed that correlation judgments follow a systematically measurable profile of perceptual precision for scatterplots using the position channel <ref type="bibr" target="#b69">[69]</ref>, a later study ranked the relative precision of correlation judgments across other visualizations, finding that positionbased scatterplots offered the highest precision, but position-based line charts offered the lowest precision <ref type="bibr" target="#b30">[31]</ref>. Angle, a channel with low precision on a two-value ratio task, showed the second-highest precision <ref type="bibr" target="#b30">[31]</ref>. Though in this case, the correlation judgment may not have been perceptually extracted by angle per se, but emergent shapes created by the angles for high negative correlations. With judgments of aggregate properties of mean, average, or spread, the typical ranking can reverse, such that typically low-ranked values like luminance (in this case, a ramp combining luminance and color saturation) can actually lead to the best performance in those tasks <ref type="bibr" target="#b0">[1]</ref> (see <ref type="bibr" target="#b77">[76]</ref> for review). Judgments of minimum, maximum, or range were still best for visualizations that used position channels. Another study asked participants to complete four tasks-read value, compare values, find maximum, and compare averages-across visualizations that relied on position, size, or color (similar to the luminance ramp used here). They found similar results, where extracting one value, or comparing two single values, was fast and accurate for position, but for aggregate properties like comparing averages, the color condition showed equal performance <ref type="bibr" target="#b44">[44]</ref>. Another study, similar in spirit, tested In reproduction, participants clicked on the screen or dragged the mouse to redraw the previously-seen marks. In all conditions, the visual channel changed as a linear function of the vertical position of the mouse cursor, such that even angle and area were changed by dragging the mouse up-and-down. For area channel, participants adjusted based on area not radius. More details are available in Appendix A.</p><p>the speed, accuracy, and preference for ten data visualization tasks across scatterplots, bar charts, pie charts, and line charts <ref type="bibr" target="#b73">[72]</ref>. They found, for example, advantages for bar charts in finding value clusters, or that scatterplots show advantages for anomaly detection, but not for cluster detection.</p><p>Others evaluated the visual channels for comparison (measured by staircasing threshold differences that could be detected in a limited time <ref type="bibr" target="#b86">[85]</ref>) across two tasks, finding the maximum difference among two paired values in a display similar to the left bar chart in Fig. <ref type="figure" target="#fig_1">2</ref>, or the stronger correlation between two such pairings of values. The study included bar, line, and donut charts, was focused on comparing value arrangements within each chart type (e.g., juxtaposed vs. interleaved values). Those charts-and their underlying channels-could in theory be compared in their effectiveness for supporting those comparison tasks, but differences in the methods between chart types make that comparison difficult <ref type="bibr" target="#b59">[59]</ref>.</p><p>Similar to the cited studies, the present study relies on a single task, but we regard reproduction as a starting point for more generalizable results, compared to two-value ratio precision or a single visual comparison task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual memory</head><p>Working memory is the ability to hold information actively in mind, and to manipulate that information to perform a wide variety of cognitive tasks <ref type="bibr" target="#b2">[3]</ref>. For visual memory in particular, when asked to remember visual information across eye movements (e.g., for comparisons) or across interruptions <ref type="bibr" target="#b35">[35]</ref>, studies typically claim a capacity limit of only '3-4 items' (e.g., <ref type="bibr" target="#b18">[19]</ref>). Even for fewer than 3-4 items, when participants recall the sizes, colors, or angles, of previously seen objects, they are notably less accurate in recalling 2 items than 1 item (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b87">86]</ref>).</p><p>Remembering more complex conjunctions of visual channels (e.g., both the color and orientation of a mark) is extremely difficult when more than 1-2 objects must be remembered <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">58]</ref>. The performance cost of increasing memory load from just 1 item held in mind at once to 2 items is larger than the cost of increasing the load from 4 to 8 items (e.g., <ref type="bibr" target="#b74">[73]</ref>). Thus, the profile of memory performance for tasks that involve only 1 or 2 items at a time may not predict the profile for more complex visual displays <ref type="bibr" target="#b10">[11]</ref>. There are also strong contextual dependency effects where values are stored in compressed ways, as relative to other values <ref type="bibr" target="#b9">[10]</ref>. In a visualization, increasing the number of memorized values will lead to performance changes that are hard to predict. Since nearly all data visualizations include more than 1 or 2 marks, it is critical to study these cases directly rather than assume the lessons drawn from studies of 1 or 2 marks will generalize to these larger value sets.</p><p>In the present study, participants were asked to reproduce data displays that fall within (2 marks), at (4 marks), or beyond working memory capacity (8 marks) to gather data from qualitatively different memory loads. Participants in this task rely on reproduction of values, as opposed to semantic recall of the main message of a visualization <ref type="bibr" target="#b47">[47]</ref> or whether they have encountered an entire image before <ref type="bibr" target="#b7">[8]</ref>. This task is an analogy to typical visual working memory tasks, acting as a proxy for how one retains values of marks across eye movements and delays (as when reading the text associated with the visualization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>This section presents and justifies our design decisions, along with the description of the stimuli generation process, the experimental design and procedure, and the data collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual channels</head><p>As introduced above, we chose six visual channels (denoted by Visu-alChannel) to cover a wide range of the original ranks by Cleveland and McGill <ref type="bibr" target="#b17">[18]</ref>: position (bar) (bar chart), position (line) (line chart), luminance (heatmap), area (bubble chart), length (misaligned bar chart), and angle (wind map). We show an example of each of the six visual channels in Fig. <ref type="figure" target="#fig_2">3a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The number of marks</head><p>We tested three different numbers of marks (denoted by NumMark): 2, 4, and 8 (Fig. <ref type="figure" target="#fig_3">4a</ref>). The 2-mark condition requires that the viewer extract the value of two data visualization marks, replicating the earlier studies based on two-value ratio judgments (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b78">77]</ref>). The difference in our task is the nature of that extraction, in that participants must redraw it rather than reporting a ratio value. The 4-mark condition aligns with the boundary of typical working memory capacity, and the 8-mark condition exceeds even the most optimistic estimates for human visual working memory. These three conditions have categorically different loads for working memory, allowing us to infer how the working memory limits affect reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental design</head><p>We split the visual channels into two experiments based on whether the channel uses a common baseline. This split was decided to align participants' mental models and to keep the experiment duration approximately 30 minutes to avoid severe fatigue effects. The first experiment tested position (bar), position (line), and luminance. The second experiment tested length, area, and angle. Each experiment tested all three numbers of marks {2, 4, 8}. Each participant did the task with 3 visual channels and all 3 numbers of marks, but with different channels for different experiments.</p><p>Each pair of VisualChannel × NumMark was a block with a series of trials. The first experiment used 13 trials per block. The second experiment used 15 trials per block; this is because, in the pilot study, we found that the second experiment was more difficult: the mapping between the vertical mouse click and the visual change was challenging, and responses were noisier. Thus, we included the additional two trials to offset this additional noise. Within each of the two experiments, the order of visual channels was counterbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generating stimuli</head><p>All the values were in the numeric range of [0.01, 1.0] and encoded to the visual channels as follows (see Appendix A for more details). The dimensions of marks were decided to maximize the varying range but to avoid overlapping. The background was set to rgb(.75,.75,.75) (light grey) to control visual contrast effects. As a result, position (bar) has the height of each bar ranging from 3.9 pixels to 390 pixels. Position (line) has the height of each line end ranging from 3.9 pixels to 390 pixels. Luminance has the color of each square ranging from rgb(.5,.5,.5) (grey) to rgb(1.0,1.0,1.0)(white) such that its middle point was the same as the background color. Area has the area of each circle ranging from π(5 + 1.19 pixels) 2 to π(5 + 37.5 pixels) 2 ; the 5 pixels offset was to ensure that all the circles were visible all the time. Length has the height of each bar ranging from 3.75 pixels to 375 pixels. Lastly, angle has each segment rotated counter-clockwise in the range of 1.8 • to 180 • . For area, length, and angle, the vertical position of the marks were randomly generated in the range of the y-axis, spanning .0 of its height (i.e., the bottom of the axis range) to .9 of its height.</p><p>All datasets were pre-generated, and the same datasets were repeated within the same experiment for different VisualChannel × NumMark blocks. Each dataset consisted of 8 numeric values, and each value was randomly and uniformly sampled from the standardized values of {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. When the NumMark was 8, participants saw all 8 values in a trial. When the number of marks was 2 (or 4), participants saw the middle two (or four) values; the remaining values were not displayed (Fig. <ref type="figure" target="#fig_3">4b</ref>). Each participant viewed different datasets within a VisualChannel × NumMark block, and repeated the same datasets across different blocks. The order of the datasets and the values within a dataset were otherwise randomized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Procedure</head><p>The experimenter first collected informed consent from the participants and then shared an instruction presentation displaying the format, structure and response modality for all trial conditions. The experimenter was present for training and answered clarifying questions the participant had about how to make their response. Trial As discussed above, in each trial, participants performed a reproduction task. They first saw the stimuli visualization for .75 seconds. The stimuli were then replaced with a blank screen for .25 seconds. Immediately after this, participants were asked to reproduce each visual element (e.g., a bar) as they clicked and/or dragged the mouse to change the pre-marked visual elements on the screen (Fig. <ref type="figure" target="#fig_2">3c</ref>). The stimuli visualization was randomly placed in one of the four quadrants (Fig. <ref type="figure" target="#fig_2">3b</ref>) and redrawn in the diagonal quadrant. For example, if participants saw the stimuli in the upper left, they redraw the stimuli in the bottom right.</p><p>The short duration exposure, along with unlabeled axes, prevent participants from recoding stimuli into other forms <ref type="bibr" target="#b51">[51]</ref> and suppress top-down effects like prior knowledge. The duration is adequate for testing visual working memory <ref type="bibr" target="#b51">[51]</ref> and provides ample time for the vision system to encode information (e.g., comparing correlation in scatterplots <ref type="bibr" target="#b67">[67]</ref>, estimating two-value ratio in bar charts <ref type="bibr" target="#b52">[52]</ref>, etc.). The inclusion of a blank screen as a mask and a different redrawing location together eliminated visual aftereffects. Participants Thirty and twenty-nine participants were recruited for the two experiments, respectively. They were undergraduate students from the same institution, enrolled in introductory psychology classes, for which they earned partial credit in exchange for their time. Participants were between 18 and 23 years old (μ = 19.02 years, σ = 0.96; 22 female, 34 male, 3 unspecified), all with normal, or corrected-to-normal vision. The same author and experimenter proctored all the experiment sessions and finished them before the COVID-19 pandemic. Apparatus The experimental system was implemented using Psychophysics Toolbox <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">46]</ref> and MATLAB 2018a, running on a Mac Mini (OS 10.10.5). Stimuli were displayed on a 23" monitor with a resolution set to 1280 × 800 pixels and a 60 Hz refresh rate. Participants were sat approximately 18.5" from the display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Response data</head><p>All the raw data from all the participants were considered for analysis with two exceptions. First, 3 and 7 participants from the two experiments, respectively, contributed to the pilot study or were unable to finish the experiment; they were excluded for the purpose of balancing learning and fatigue effects. Second, in the angle condition, when showing a maximum value 1.0 (180 • ) as the reference, 45.79% of the responses were the same default value of 0.001 (∼0 • ), resulting in a very large error (100% error). Because both 0 • and 180 • were a flat segment (see Fig. <ref type="figure" target="#fig_2">3</ref>), we think, if not all, the majority of the participants misinterpreted 180 • as 0 • . To ensure the comparability of our results, we transferred the reference value (1.0) to 0.0 (180 • to 0 • ) for angle.</p><p>We recorded the reproduced value of each mark, the order of visual marks, the reference values shown on the screen, the reaction time, VisualChannel, NumMark, and the trial index. We collected 6,129 trials = 3 VisualChannels × 3 NumMarks × (13 trials × 27 participants + 15 trials × 22 participants). Together we analyzed 28,602 responses = 3 VisualChannels × (2 + 4 + 8) marks × (13 trials × 27 participants + 15 trials × 22 participants).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ANALYSES</head><p>To analyze the response data, we first decided the measures to quantify the effects, followed by a description of the modeling approach and the model to support the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Measures</head><p>We follow the literature on visual memory and used three statistical measures to compare participants' responses: bias bias, precision precision, and individual response level error error <ref type="bibr" target="#b9">[10]</ref> (see Fig. <ref type="figure" target="#fig_4">5</ref>).</p><p>Among these, bias bias is how the mean of the responses deviates from the actual value presented as the reference the reference. Think of bias as systematic error or the tendency to make mistakes in a certain direction, such as exhibiting a bias to overestimate wide bars <ref type="bibr" target="#b13">[14]</ref>. Precision Precision is the consistency of participants' responses; they may consistently report the same value, regardless of the reference the reference value. Bias Bias and precision precision are different facets for the same set of responses. Participants could be precise but consistently underestimate (or overestimate) the value <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">52]</ref>. They could be imprecise but generally right on average. Alternatively, error error measures how each response deviates from the reference value. These three measures are different facets for the same distribution of the responses, capturing variations in visual error and reproduction performance through different lenses.</p><p>Here we used a student t(μ μ,σ σ ,ν) distribution for a more robust understanding. Bias Bias, the mean of responses, is described by the location . Bias, precision, and error. Bias Bias and precision precision describe the average properties of a set of responses, while error error is a measure for a single response. In this work, error error is defined as the deviation from the reference, mean of errors is defined as bias bias, and standard deviation of errors is defined as precision precision.</p><p>parameter μ μ; and precision precision, the consistency of responses, is described by the dispersion parameter σ σ 1 . The errors errors of individual responses combine both bias (μ) bias (μ) and precision (σ ) precision (σ ) of the responses into one measure. If we fit the distribution with the response data collected, then knowing μ μ and σ σ , we are able to draw samples from the distribution and calculate error error of each draw.</p><p>It is important to note that bias bias and precision precision describe the average properties of a set of responses (e.g., responses from one or more experimental conditions, one or more participants). However, error error is a measure for a single response, combining variance from bias bias and precision precision; hence it is with more uncertainty than bias bias and precision precision.</p><p>Because each of the three measures is associated with a reference, in the remainder of this paper, we subtract the reference value from each response and transform all the raw responses to errors errors (i.e., relative responses = raw responses − reference values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bayesian multilevel (hierarchical) modeling</head><p>We adopted a Bayesian modeling approach to estimate the error distribution. The mean and standard deviation parameters of this distribution, as described above, are considered bias and precision of the responses.</p><p>We followed a process of model expansion with regularization <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b65">65]</ref>. It allowed us to understand how each predictor affects the model, to capture more variance in the data while reducing overfitting, and to explore the effects of secondary variables. We started with a minimal model, which contained only experimental variables, and a list of potential predictors, ordered by their importance in our subjective beliefs. We then progressively added the predictors and evaluated each intermediate model by inspecting their posterior predictions and posterior distributions of the coefficients. We compared each intermediate model to the last model using WAIC (widely applicable information criterion) and LOO (Leave-One-Out Cross-Validation) for out-of-sample prediction accuracy, and examined their Akaike weights (the probabilities of the differences in these predictions) <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b53">53]</ref>. We also started with weakly informative priors and gradually regularized the priors as the model expanded <ref type="bibr" target="#b53">[53]</ref>. We chose the final model which was the best at addressing our research questions, describing the current data, and predicting future observations. We implemented the modeling processes using R packages brms <ref type="bibr" target="#b12">[13]</ref>, CmdStanR <ref type="bibr" target="#b23">[24]</ref>, bayesplot <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, ggdist <ref type="bibr" target="#b41">[41]</ref>, and tidybayes <ref type="bibr" target="#b42">[42]</ref>. We provide the analysis script and the resulting model files as supplementary materials (the analysis.Rmd|html and *.rds files). 1 Strictly, the σ parameter (standard deviation) describes imprecision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model specification</head><p>Formula Using a syntax similar to brms's <ref type="bibr" target="#b12">[13]</ref> extended Wilkinson-Rogers-Pinheiro-Bates notation <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b81">80]</ref>, our final model is We treat all the responses as arising from a mixture of two distributions: a student t distribution for all the genuine reproduction responses, and a normal distribution for those made without an intention to reproduce a value, termed the 'default' distribution. This is because sometimes participants did not move the mouse to make a response, resulting in a cluster of likely irrelevant responses at a small (known) value. The mixture model separates these two sorts of responses; a mixture model like this is ubiquitous in the visual memory literature <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b87">86]</ref> for modeling responses. In the model, the mixture parameter θ 1 θ 1 , the mean (μ 1 ; bias μ 1 ; bias), and standard deviation (σ 1 ; precision σ 1 ; precision) of the student t distribution vary with the experimental variables. The mean (μ 2 μ 2 ) of the normal distribution captures the default responses (see line 11 below). We assumed that the ν 1 parameter of the student t distribution and the standard deviation (σ 2 ) of the normal distribution do not vary. We also left censored the responses to reduce the impact of erroneous responses. line 2 This line describes the probability of a response coming from the genuine reproduction (cf. default) distribution. This probability could be affected by if the mark was changed (1 or 0), the number of marks, the visual channel used, and the reference value.</p><p>The mean (μ 1 ; bias μ 1 ; bias) of the reproduction distribution is a joint function of a set of linear predictors with varying intercepts and slopes: line 3 The experimental variables NumMark and VisualChannel are of the most importance. ReferenceValue acknowledges that perceptual errors are likely to be affected by the magnitude of stimuli (e.g., Weber-Fechner's <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>, Stevens's power <ref type="bibr" target="#b75">[74]</ref>, and Guilford's laws <ref type="bibr" target="#b27">[28]</ref>) without making a strong assumption about this relationship is the same for different numbers of marks and visual channels; this aligns with the observations that Weber's law appears not to hold for extreme values <ref type="bibr" target="#b24">[25]</ref> nor perception of area and angle <ref type="bibr" target="#b75">[74]</ref> (see Appendix B for more discussion). The interaction between these variables further generalize this relationship. line 4 ExperimentalTrial captures learning and fatigue effects over the course of the experiment such that we can later divest these effects by conditioning on the median trial. line 5 DataMean is the average of the shown data in a trial. It approximates the context of a response. If the reference value is small but the data mean is large, it may indicate that this response was made in the presence of other large values, and vice versa. The interaction with VisualChannel is motivated by the speculation that participants may use perceptual proxies for mean <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b60">60]</ref>, and the proxies may be different for different visual channels <ref type="bibr" target="#b30">[31]</ref>. line 6 The group-level effects ("random intercepts and slopes") capture the correlation within a participant and also allow each participant to vary for different experiments and experimental conditions. lines 7-10 The same predictors were used for bias (μ 1 ) bias (μ 1 ) and precision (σ 1 ) precision (σ 1 ) to ensure compatibility. line <ref type="bibr" target="#b10">11</ref> The responses from the default distribution, when participants may not be trying to reproduce the value, are always near a small, known value (denoted by DefaultError), specified via the informative priors for the mean (μ 2 μ 2 and standard deviation (σ 2 ) . The number of marks (6 is the interpolation of the model)  The slope shows the effects of the number of marks (2 marks vs. 8 marks).</p><p>The span on y-axis shows the effects of the reference value (a 108° (0.6) angle vs. a 162° (0.9) angle).</p><p>A ribbon shows the estimated bias and the associated uncertainty for a reference value.</p><p>This figure shows the subtractions and the chain for 4 marks, which becomes the basis of Fig. <ref type="figure" target="#fig_3">8i (4 marks</ref>).</p><p>The subtractions and the chain for precision. This is the basis of Fig. <ref type="figure">8ii</ref> (4 marks).</p><p>The subtractions and the chain for error. This is the basis of Fig. <ref type="figure" target="#fig_3">8iii (4 marks</ref>).</p><p>These figures show how bias and precision vary with the number of marks and the reference value. Remember that bias (mean; μ) and precision (standard deviation; σ) are the aggregated properties of a set of responses; they are distributional parameters of the modeled responses (error).</p><p>The slope shows the effects from the number of marks, and the distance between two "ribbons" shows the effects from different reference values. Each ribbon averages across all participants and conditions on the median trial and an average case when data mean is equal to the reference value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>medians of posterior predictions</head><p>Mean and standard deviation jointly define the response distributions, and samples from the distribution express the predictions of a future response as the posterior predicted error.</p><p>Unlike aggregated properties (bias and precision), errors describe individual responses, in which randomness dominates the differences among reference values and visual channels. Thus, the ribbons overlap with each other.</p><p>is how the reponses systematically deviate from the truth.</p><p>is how consistent the reponses are.</p><p>expresses the prediction of a future response. We predict 95% of future observations will fall into this interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position</head><p>A ranking list of visual channels with uncertainty is considered a chain on which a previous node (a visual channel) is more likely (&gt;50%) to be better than any of its later nodes.</p><p>Such probabilities are found by subtracting the posterior samples of two channels. Negative values mean better (e.g., less bias). The proportion of negative values over the entire distribution is the probability. To understand the differences in visual channels for the reproduction task, we report various effects on each of the precision, bias, and error measures. We then derive ranks for the visual channels. We base our inference on the first distribution of the mixture model and the posterior distributions (marginal, conditional, and predictive distributions). Marginal posterior distributions summarize all the known information for one parameter; conditional posterior distributions tell us the expected value of one parameter in a specific situation; and posterior predictive distributions provide unobserved data conditioning on the observed data and the fitted model. Fig. <ref type="figure">7</ref>. The exmaples of quantified primary effects of the number of marks and reference values. We take subtraction and calculate the marginal probabilities of being better (see Fig. <ref type="figure" target="#fig_8">6b</ref>), averaging across visual channels and reference values (or different numbers of marks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Primary effects</head><p>The model suggests that the two experimental variables-the number of marks (NumMark) and the reference value (ReferenceValue)-both have very strong effects on the reproduction responses across the three measures. To show these effects, we take an average participant (to eliminate individual differences), conditional on the median trial (to rid learning/fatigue effects) and on the case where data mean is equal to the reference value (to remove the effects of the other marks in the same trial). Fig. <ref type="figure" target="#fig_8">6a</ref> shows all of the modeled effects, including the tendencies and the interactions between variables. Fig. <ref type="figure">7</ref> provides examples of quantified primary effects by showing how likely an average participant's responses are better (less biased/more precise/smaller errors).</p><p>i. Bias (Figs. <ref type="figure" target="#fig_8">6i and 7i</ref>)</p><formula xml:id="formula_0">Number of marks.</formula><p>Number of marks. The average participant is very likely to be less biased in the reproduction, when the number of marks is small. For an average visual channel and an average reference value, the estimated probability that the average participant is less biased in a chart with 2 marks than with 8 marks is .92 .92. That is, for the same reference value, we expect 92% of responses with 2 marks to exhibit less bias than the responses with 8 marks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference value.</head><p>Reference value. The average participant is very likely to overestimate a small reference value and seriously underestimate a large reference value, and are least biased with a reference value around .4 or .5 (median). For an average visual channel and an average number of marks, the estimated probability that the participant is less biased in a chart with the median reference value (.5) than the minimum value (.1) is .93 (this is 1-.07). Similarly, the estimated probability that an average participant is less biased in a chart with the median reference value (.5) than the maximum value (1.0) is .97 .97.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction effects.</head><p>Interaction effects. The effects of NumMark and ReferenceValue interact, and each interacts with VisualChannel. For most of the visual channels but position (line), response bias increases when the number of marks is large and a reference value deviates from the median further. Overall, angle is the visual channel where response bias is most sensitive to either a change in the number of marks or the reference value; position (line) is where bias is sensitive to the reference value, but robust to the number of marks for large reference values.</p><p>ii. Precision (Figs. <ref type="figure" target="#fig_8">6ii and 7ii</ref>)</p><formula xml:id="formula_1">Number of marks.</formula><p>Number of marks. The average participant is very likely to be more precise (more consistent) when the number of marks is small. For an average visual channel and an average reference value, the estimated probability that the participant is more precise with a chart of 2 marks than a chart of 8 marks is .99 .99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference value.</head><p>Reference value. The average participant is more precise with reproducing a small reference value and much less precise with reproducing a large reference value. For an average visual channel and an average number of marks, the estimated probability that the participant is more precise with the minimum reference value (.1) than the median or maximum reference value (.5 or 1.0) is 1.00 1.00 (nearly deterministic).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction effects.</head><p>Interaction effects. The effects of these two variables on precision interact with each other and further with visual channels. Response precision is more affected by the number of marks when the reference value is smaller, except angle, where precision is more affected by the number of marks when the reference value is large. Similarly, precision is more affected by the reference value with fewer marks, except angle, where precision is more affected by the reference value with more marks. Overall, luminance is the visual channel where precision is least sensitive to the reference value, and position (line) is where precision is most sensitive to the reference value.</p><p>iii. Error of individual response (Figs. <ref type="figure" target="#fig_8">6iii and 7iii</ref>) The samples drawn from the posterior distributions provide an estimation of errors in individual responses; for the convenience of comparison, we took the absolute values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of marks.</head><p>Number of marks. The average participant is likely to make smaller errors with fewer marks. For an average visual channel and an average reference value, the probability that a single future response exhibits a smaller error with 2 marks than with 8 marks is .71 .71.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference value.</head><p>Reference value. The average participant is likely to make smaller errors with a smaller reference value. The estimated probability that a single future response will have a smaller error for the minimum reference value (.1) than the maximum (1.0) is .74 .74.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction effects.</head><p>Interaction Reproduction error is affected the number of marks slightly more larger reference values for area and angle, less for position (bar) and position (line), and similarly across different reference values for luminance and length. These interactions effects are milder than those observed for bias and precision, owing in part to increased uncertainty in this measure relative to the aggregated properties described by bias and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Secondary effects</head><p>The model also suggests several moderate effects. To show the learning/fatigue effect, we condition on the average case where both reference value and the associated data mean are at their median (.5, .55, respectively). To show the effect of data properties (e.g., the mean of all the data values in a trial), we condition on the average case where reference value is at its median (.5) in the median trial, and sampled all the possible values of data mean. We also marginalize out the number of marks and visual channels and use an average participant.</p><p>i. Bias The participant appears to underestimate reference values at the beginning of the experiment. In later trials, the participant generally increases the reproduced values and becomes less biased. In reproducing an average value, the participant seems not affected by other small reference values, but is likely to underestimate the median value when other reference values are large.</p><p>ii. Precision The participant appears to become slightly less precise as the experiment goes on, possibly due to the fatigue effect. In reproducing an average value, the average participant seems less precise when other larger values are present in the same trial; these larger values possibly distract the participant's judgment and reproduction.</p><p>iii. Error of individual response It appears that learning or fatigue effects do not strongly affect response error. In reproducing an average value, the participant is likely to make smaller errors when other reference values are small, and to make larger errors when other values are large. The error of a response seems to largely increase when data mean is above .25, half of the median. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Deriving probabilistic ranks</head><p>One primary goal of this work is to derive ranks for the visual channels based on the reproduction task and compare them to those from twovalue ratio judgment tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>. A ranking list may provide a summary of effects for others to digest the results (e.g., <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b71">70]</ref>). However, a rank list may cause dichotomous thinking (e.g., "A is always better than B."), which belies the nuances in the ranking. In the spirit of rethinking the previous ranks and in the context of Bayesian statistics, we will derive probabilistic ranks that acknowledge the uncertainty in the observations and the modeling processes.</p><p>A rank in the probabilistic domain may mean that one is more likely to be better than another. Hence we start by calculating these probabilities for visual channels. We marginalize all reference values and condition on the median trial with the median data mean. We subtract the absolute values of each measure; if A is better (less biased/more precise/smaller errors) than B, we expect negative values after subtraction (see Fig. <ref type="figure" target="#fig_8">6b</ref>). The estimated probability of A being better than B is the proportion of the negative values over the entire subtracted distribution. When this probability is larger than 50% (larger than chance or other thresholds), we say A is more likely to be better than B, and A "wins."</p><p>We derive the probabilistic rank lists by pairwise subtraction and then build a chain of visual channels where any previous node on this chain always "wins" any comparison to the later nodes for a given measure. This is essentially a constraint satisfaction problem <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b55">55]</ref>, and there are many methods to find a solution <ref type="bibr" target="#b48">[48]</ref>. For the scale of our problem, we can build the chain by hand or apply heuristics (e.g., sorting by how many times a visual channel "wins"). We construct a chain for each measure and visualize them in Fig. <ref type="figure">8</ref>, augmented with the associated probabilities to convey uncertainty.</p><p>The ranking produced by the precision of two-value ratio judgments does not hold for each of the three measures nor any of the modeled numbers of marks. The ranks changes with different numbers of marks across different measures, which suggests that the previous channel rank is likely not generalize to other visual comparison tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The context of a visualization</head><p>We find that showing more marks adds substantial noise to memory representations, and has an order of magnitude more influence on performance than the choice of channel. Squeezing more data into one visualization may cause viewers to increasingly remember (and compare) data as statistics or rough global shapes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>, rather than precise representations of individual values. Memory for each value likely also depends on its relation to the distribution of other values, as in work on neighborhood effects <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b88">87]</ref> and distractor effects <ref type="bibr" target="#b78">[77]</ref>.</p><p>We also find that the value of a mark (the reference value) has a more powerful effect on reproduction than the channel chosen: tall bars are more biased than small areas, even though position (bar) is one of the least biased channels overall. The context of the reference value also shows strong bias (e.g., angle), similar to past work where participants tend to be more biased and less precise with a value further from the ends of the range <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b52">52]</ref> (e.g., "edge effects"); it also aligns with psychophysical observations <ref type="bibr" target="#b25">[26]</ref>, where low and high ends of the data range can serve as perceptual anchors (e.g., "the angle is 10 • from 90 • is perceptually congruent). Alternatively, for a channel like position (bar), participants perform better around the median value, possibly because they resort to near-mean estimations when their memory falters, which would also be consistent with better performance around the mean of   the possible range of values <ref type="bibr" target="#b37">[37]</ref>.</p><p>The reproduction task may also have provided a context that influenced the pattern of results. For example, while line charts relied on the position channel, they were underestimated (redrawn lower than the original reference values). The connected nature of a line graphs' points might have further induced viewers to perceive them as a single complex shape, or set of contrasting slopes, for the purposes of redrawing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implications and future directions</head><p>The channel ranking based on two-value ratio judgments is attractive as a rule. That task feels like a visual comparison distilled down to an atomic unit, which may lead to an assumption that a ranking based on that task should extrapolate to new ones. However, the present results question this assumption (Fig. <ref type="figure">8</ref>), producing a different ranking for the two-value condition using a reproduction task. Designers should be increasingly skeptical of the channel ranking produced by two-value ratio tasks <ref type="bibr" target="#b17">[18]</ref>, which may not generalize as much as has been presumed. The present results also suggest that the number of marks, the reference values, and other secondary factors such as data mean may strongly, progressively, and interactively affect reproduction, which we argue may serve as an important proxy for data perception across a suite of realistic perceptual tasks.</p><p>How might the rankings produced by the present model, once refined and expanded, be used by a designer? While we caution the reader that these results are a start, and more work is needed to replicate these rankings in more realistic and concrete perceptual tasks, we offer a glimpse of guidelines that might eventually follow. One might imagine that as length and position (bar) generally lead to less bias and smaller errors, these encodings might be desirable when bias must be minimized. Area is surprisingly more precise but could lead to more bias and larger error, so it might be preferred for improving precision. Luminance shows relatively stable performance across different numbers of marks and measures, which might make it more suitable when the number of marks vary. Angle drops rapidly in all ranks as the number of marks increases, making it less suitable for realistically complex data. Position (line) was surprisingly ineffective in this reproduction task, but may reduce bias for a larger dataset. Knowing one's risk appetite for the misperception of bias or precision will inform the choice of visual channels. Moving forward, parameterizing the influence of data properties (number of marks, values) and the designers' desire to optimize for lower bias, higher precision or lower error may help to inform visualization designers' decisions.</p><p>Concurrently considering multiple factors and their conflicts and interactions would likely be difficult for designers, such that an automated model might be important for weighting their complexities. We were inspired by the recent modeling work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b76">75]</ref> and appealed to psychophysical laws <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b83">82]</ref>, entropy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b71">70,</ref><ref type="bibr" target="#b72">71]</ref>, perceptual proxies <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b58">58]</ref>, serial-position and ordering effects <ref type="bibr" target="#b36">[36]</ref>, visual memory (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b54">54]</ref>), neighborhood effects <ref type="bibr" target="#b88">[87]</ref>, and distractors <ref type="bibr" target="#b78">[77]</ref>. We are capable of providing (very) preliminary recommendations given the inputs (see Fig. <ref type="figure" target="#fig_11">9</ref>). Thus, a modeling approach like the present one may harmonize different factors and provide design candidates.</p><p>It would be premature to derive other firm guidelines based on the present study; additional studies will be needed or establish whether there exist broadly applicable guidelines. Our task relies on a purposely abstract reproduction task as a first step toward inspiring future work using more concrete comparison tasks. Those studies will need to expand how the visualization research community operationalizes different visualizations tasks (e.g., detecting trends and motifs, immediate or later comparison, and viewing a visualization with thousands of data points), and what 'good performance' means in a task (e.g., precision, bias, error, speed <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b73">72]</ref>, etc.). In addition, other factors such as topdown effects like prior knowledge <ref type="bibr" target="#b82">[81]</ref> and expectations <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b63">63]</ref> may impact reproduction task performance, and individual differences <ref type="bibr" target="#b62">[62]</ref> and spatial ability <ref type="bibr" target="#b61">[61]</ref> may affect strategies that subsequently impact task performance, which could be promising directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Limitations</head><p>For the sake of comparability, we treated all visual channels equally in designing the experiment and analyzing the data. This makes the channels easier to compare, potentially at the cost of the usability for some. The redrawing method may add noise and bias to data, as it might not be equally intuitive for all the visual channels. For example, for angle, we mapped the y-coordinate of the cursor to the degrees of the angle, which might be more difficult to draw than others (e.g., position (bar) that maps the cursor to the height of the bar). Similarly, always dragging up from the zero value might result in a bias towards smaller values for the two position channels, possibly explaining the underestimation in position (line) noted above. These response methods likely have an impact on the result, and a comparison of different response methods might be important to generalization of these results. Similarly, the model also always assumes linearity between errors of the responses and all the variables. While most of the data meet this assumption, visual channels like angle display moderate non-linearity across different reference values (Fig. <ref type="figure" target="#fig_12">10</ref>), likely affecting the estimation of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We revisited the ranking of visual channels <ref type="bibr" target="#b16">[17]</ref> using a visual reproduction task as a proxy of various visual comparison tasks. We tested participants' reproduction performance with six visual channels: position (bar), position (line), luminance, area, length, and angle across different numbers of marks and data values. With a Bayesian multilevel model, we show that both the number of marks and the reference value strongly affect the bias and precision in a set of responses, as well as errors of individual responses; the number of marks gradually dominates the differences in visual channels and reference values, reflecting a strong limit on working memory, that likely serves to limit most comparison tasks in data visualization. We further derive probabilistic rankings from the model for each measure and show that the previous ranking <ref type="bibr" target="#b17">[18]</ref> does not hold. This work demonstrates the limitations of the previous ranking <ref type="bibr" target="#b16">[17]</ref>, offers the preliminary new rankings based on a reproduction task, and presents a Bayesian modeling approach to rank visual channels, all as bases for future work to continue exploring this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of visualization designs that use three different visual channels. (a) This bar chart relies on the position channel for comparison, (b) this bar chart relies on the length channel for vertical comparisons between A and B, and (c) this heatmap relies on the luminance channel. A two-value ratio judgment is precise in (a), and progressively less precise from (b) to (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual channels and the reproduction task. (a) Examples of visual channels for 4 visual marks. (b) Each trial followed a "show-removereproduce" procedure to indicate their responses. (c)In reproduction, participants clicked on the screen or dragged the mouse to redraw the previously-seen marks. In all conditions, the visual channel changed as a linear function of the vertical position of the mouse cursor, such that even angle and area were changed by dragging the mouse up-and-down. For area channel, participants adjusted based on area not radius. More details are available in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Different numbers of visual marks. We used the same pregenerated datasets across different NumMark and VisualChannel and removed the side values when showing 2 or 4 marks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Fig.5. Bias, precision, and error. Bias Bias and precision precision describe the average properties of a set of responses, while error error is a measure for a single response. In this work, error error is defined as the deviation from the reference, mean of errors is defined as bias bias, and standard deviation of errors is defined as precision precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The number of marks (6 is the interpolation of the model)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) The primary effects modeled from the experimental observations; and (b) how we compare two visual channels, calculate the probabilities of being better, and finally derive the probabilistic ranks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. An example of how the model could support design decision making via predicting a viewer's responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The moderate non-linearity and symmetry in responses for (b) luminance and (c) angle, compared to (a) position (bar).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Satoru Suzuki and members of the Visual Thinking Laboratory at Northwestern University for their suggestions during the experimental design. The authors also thank the anonymous reviewers for their feedback. This work was supported in part by grants BCS-1653457 and IIS-1901485 from the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task-driven evaluation of aggregation in time series visualization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFVIS.2005.1532136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE Symposium on Information Visualization, INFO VIS</title>
				<meeting>-IEEE Symposium on Information Visualization, INFO VIS</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">15334406</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The concept of episodic memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baddeley</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2001.0957</idno>
	</analytic>
	<monogr>
		<title level="m">Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</title>
				<imprint>
			<date type="published" when="1413">1413. 2001</date>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="page" from="1345" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactions between visual working memory representations</title>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-017-1404-8</idno>
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2376" to="2395" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The precision of visual working memory is set by allocation of a shared resource</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Catalao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Husain</surname></persName>
		</author>
		<idno type="DOI">10.1167/9.10.7</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semiology of graphics: diagrams, networks, maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wainer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>University of Wisconsin press Madison</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Why shouldn&apos;t all charts be scatter plots? Beyond precision-driven visualizations. CoRR</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.11310" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond memorability: Visualization recognition and recall</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467732</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical encoding in visual working memory: Ensemble statistics bias memory for individual items</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<idno>doi:10.1177/ 0956797610397956</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="384" to="392" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual effects in visual working memory reveal hierarchically structured memory representations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="DOI">10.1167/15.15.6</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="6" to="6" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">No evidence for a fixed object limit in working memory: Spatial ensemble representations inflate estimates of working memory capacity for complex objects</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000075</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">921</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The psychophysics toolbox</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vision</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">brms: An R package for Bayesian multilevel models using Stan</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v080.i01</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Truth or square: Aspect ratio biases recall of position encodings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ceja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccoleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno>doi:10. 1109/TVCG.2020.3030422</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1054" to="1062" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An information-theoretic framework for visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jäenicke</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2010.132</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1215" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adaptive memory distortion in visual working memory</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chunharas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serences</surname></persName>
		</author>
		<idno>doi:10.31234/ osf.io/e3m5a</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graphical methods graphical perception: Theory , experimentation , and application to the development of graphical methods</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">387</biblScope>
			<biblScope unit="page" from="531" to="554" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An experiment in graphical perception</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcgill</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0020-7373(86)80019-0</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The magical number 4 in short-term memory: A reconsideration of mental storage capacity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0140525X01003922</idno>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="114" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Uncertainty displays using quantile dotplots or cdfs improve transit decision-making</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173718</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The nature and status of visual resources</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1093/oxfordhb/9780195376746.013.0010</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page" from="147" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">bayesplot: Plotting for bayesian models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mahr</surname></persName>
		</author>
		<ptr target="https://mc-stan.org/bayesplot/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualization in bayesian workflow</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssa.12378</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="389" to="402" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CmdStanR: the R interface to CmdStan</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Češnovar</surname></persName>
		</author>
		<ptr target="https://mc-stan.org/users/interfaces/cmdstan" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sensitivity in the judgment of size by finger-span</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Gaydos</surname></persName>
		</author>
		<idno>doi:10.2307/ 1420251</idno>
	</analytic>
	<monogr>
		<title level="j">The American journal of psychology</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="557" to="562" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Psychophysics: the fundamentals</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Gescheider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual comparison for information visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jusufi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
		<idno>doi:10.1177/ 1473871611416549</idno>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A generalized psychophysical law</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guilford</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0070969</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="1932">1932</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Remembering complex objects in visual working memory: Do capacity limits restrict objects or features</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Hardman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cowan</surname></persName>
		</author>
		<idno type="DOI">10.1037/xlm0000031</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">325</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Influencing visual judgment through affective priming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Skau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2481410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2949" to="2958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ranking visualizations of correlation using weber&apos;s law</title>
		<author>
			<persName><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346979</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1943" to="1952" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The visual discrimination of intensity and the weber-fechner law</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of general physiology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="267" />
			<date type="published" when="1924">1924</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crowdsourcing graphical perception: Using mechanical turk to assess visualization design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th</title>
				<meeting>the 28th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<idno type="DOI">10.1145/1753326.1753357</idno>
		<title level="m">Annual CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bias in proportion judgments: the cyclical power model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hollands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Dyre</surname></persName>
		</author>
		<idno>doi:10.1037/ 0033-295X.107.3.500</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">500</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding the function of visual short-term memory: transsaccadic memory, object correspondence, and gaze correction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hollingworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<idno type="DOI">10.1037/0096-3445.137.1.163</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">163</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Shaping the Scientific Hypothesis Generation Process through the Design of Visual Analysis Tools</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<idno type="DOI">10.26300/vyf3-qw80</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD dissertation</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distinguishing target biases and strategic guesses in visual working memory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13414-019-01913-2</idno>
	</analytic>
	<monogr>
		<title level="j">Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<date type="published" when="1258">1258-1270, 2019</date>
		</imprint>
	</monogr>
	<note>Attention</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A boolean map theory of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pashler</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.114.3.599</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">599</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The perceptual proxies of visual comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jardine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ondov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934786</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1012" to="1021" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual reasoning strategies for effect size judgments and decisions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030335</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="272" to="282" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3879620</idno>
		<title level="m">ggdist: Visualizations of Distributions and Uncertainty</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>R package version 2.4.0.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">tidybayes: Tidy Data and Geoms for Bayesian Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1308151</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond weber&apos;s law: A second look at ranking visualizations of correlation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467671</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Assessing effects of task and data distribution on the effectiveness of visual encodings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno>doi:10.1111/ cgf.13409</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="157" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explaining the gap: Visualizing one&apos;s predictions improves recall and comprehension of data</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025592</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1375" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">What&apos;s new in psychtoolbox-3</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ingling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Broussard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Trust and recall of information across varying degrees of title-visualization misalignment</title>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300576</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Algorithms for constraint-satisfaction problems: A survey. AI magazine</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1609/aimag.v13i1.976</idno>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="32" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A student&apos;s guide to Bayesian statistics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lambert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Sage</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Automating the design of graphical presentations of relational information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<idno type="DOI">10.1145/22949.22950</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Tranactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="141" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Short-term memory</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Warrington</surname></persName>
		</author>
		<idno type="DOI">10.1016/B978-0-12-481845-3.50016-3</idno>
	</analytic>
	<monogr>
		<title level="m">Cognitive Neuropsychology</title>
				<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="275" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">No mark is an island: Precision and category repulsion biases in data reproductions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mccoleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030345</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1063" to="1072" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Statistical rethinking: A Bayesian course with examples in R and Stan</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcelreath</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781315372495</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The magical number seven, plus or minus two: Some limits on our capacity for processing information</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1525/9780520318267-011</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Formalizing visualization design knowledge as constraints: Actionable and extensible models in draco</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno>doi:10.1109/ TVCG.2018.2865240</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="438" to="448" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Visualization analysis and design</title>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Measures of the benefit of direct encoding of data deltas for data pair relation perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nothelfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno>doi:10. 1109/TVCG.2019.2934801</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="311" to="320" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visual working memory declines when more features must be remembered for each object</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oberauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eichenberger</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13421-013-0333-6</idno>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1212" to="1227" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Face to face: Evaluating visual comparison</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ondov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jardine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno>doi:10.1109/ TVCG.2018.2864884</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="861" to="871" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Revealing perceptual proxies with adversarial examples</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ondov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030429</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving bayesian reasoning: The effects of phrasing, visualization, and spatial ability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Afergan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ziemkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467758</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="529" to="538" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Personality as a predictor of user strategy: How locus of control affects search strategies on tree visualizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702590</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3251" to="3254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Using fnirs brain sensing to evaluate information visualization interfaces</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2470723</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;13</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;13</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Debroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heisterkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Willigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maintainer</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=nlme" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The garden of forking paths in visualization: A design space for reliable exploratory visual analytics: Position paper</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<idno type="DOI">10.1109/BELIV.2018.8634103</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Evaluation and Beyond-Methodological Approaches for Visualization (BELIV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A survey of perception-based visualization studies by task</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Quadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3098240</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On the prospects for a science of visualization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-7485-2_6</idno>
	</analytic>
	<monogr>
		<title level="m">Handbook of human centric visualization</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="147" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">An entropy theory of correlation perception</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
		<idno type="DOI">10.1167/16.12.811</idno>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The perception of correlation in scatterplots</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1203" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<idno type="DOI">10.1111/j.1467-8659.2009.01694.x</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Wiley Online Library</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Linesmooth: An analytical framework for evaluating the effectiveness of smoothing techniques on line charts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Quadri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030421</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1536" to="1546" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">At a glance: Pixel approximate entropy as a measure of line chart complexity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<idno>doi:10. 1109/TVCG.2018.2865264</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="872" to="881" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Task-based effectiveness of basic visualizations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2829750</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2505" to="2512" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Psychophysical scaling reveals a unified theory of visual memory strength</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Schurgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wixted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Brady</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41562-020-00938-0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature human behaviour</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1156" to="1172" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On the psychophysical law</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0046162</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="181" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Modeling color difference for visualization design</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2744359</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="392" to="401" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Four types of ensemble coding in data visualizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1167/16.5.11</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="11" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Four experiments on the perception of bar charts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346320</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2152" to="2160" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Clusters precede shapes in perceptual organization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Trick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Enns</surname></persName>
		</author>
		<idno>doi:10.1111/j. 1467-9280.1997.tb00694.x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="129" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Envisioning information</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Tufte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Goeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Graphics press Cheshire</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>CT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Symbolic description of factorial models for analysis of variance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="DOI">10.2307/2346786</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="392" to="399" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The curse of knowledge in visual data communication</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Weelden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2917689</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3051" to="3062" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Correlation judgment and visualization features: A comparative study</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2810918</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1474" to="1488" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Gestalt similarity groupings are not constructed in parallel</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<idno>doi:10.1016/j. cognition.2018.08.006</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="8" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Similarity grouping as feature-based selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Bemis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797618822798</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Perceptual proxies for extracting averages in data visualizations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13423-018-1525-7</idno>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="669" to="676" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Discrete fixed-resolution representations in visual working memory</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luck</surname></persName>
		</author>
		<idno>doi:10. 1038/nature06860</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="issue">7192</biblScope>
			<biblScope unit="page" from="233" to="235" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Neighborhood perception in bar charts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300462</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
