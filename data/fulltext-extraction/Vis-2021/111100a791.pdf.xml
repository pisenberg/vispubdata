<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenge</forename><surname>Zhao</surname></persName>
							<email>zhengezhao@email.arizona.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Bosch Research North America</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Bosch Research North America</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Panpan</forename><surname>Xu</surname></persName>
							<email>xupanpan@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bosch Research North America</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Bosch Research North America</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carlos</forename><surname>Scheidegger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bosch Research North America</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Bosch Research North America</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liu</forename><surname>Ren</surname></persName>
							<email>liu.ren@us.bosch.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bosch Research North America</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ren</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bosch Research North America</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">602D73EF1FC9F415BC07EFA5B8149B76</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Data Exploration</term>
					<term>Deep Neural Network</term>
					<term>Model Interpretation</term>
					<term>Explainable AI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. ConceptExtract supports human-in-the-loop visual concept learning and uses the extracted visual concepts for fine-grained model interpretation, diagnostics and comparison. (A) an image patch view displays small patches or super-pixels (Fig. 4) segmented from the original images with informative overlays to facilitate interactive visual concept extraction. It displays visually similar patches in close proximity without overlap. Here, the overlay shows pixel-wise prediction accuracy for an image segmentation model for road scene understanding, and the border color of each image patch encodes a concept confidence score; (B) a cross-filter panel enable filtering patches based on the concept confidence scores and other statistics; (C) an active learning labeler panel allows easy specification of positive and negative samples for training models; (D) a model analysis panel using the learned visual concepts (shadow, cloud) for fine-grained model interpretation, diagnostics and comparison. (E) a model summary panel showing a summary of the target model performance. This figure shows ConceptExtract being used to analyze an image segmentation model for road scene understanding in autonomous driving scenarios. Please refer to Section 7.2 for more detail.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks have achieved state-of-the-art performance in many challenging computer vision tasks and are being widely adopted in many real-world application scenarios such as autonomous driving. As a result, recent emphasis on deep learning models has moved from model accuracy alone towards issues such as model interpretability. The machine learning community has realized the necessity of making the models more understandable, especially since these models can easily have hundreds of millions of parameters with highly non-linear transformations. First of all, the model/application developers might want to scrutinize the decisions made by machine learning models and use them more responsibly <ref type="bibr" target="#b20">[21]</ref>. If the model developers can understand the weaknesses of their AI models, they could minimize the potential errors or biases of training data in real-world applications. This is orthogonal to model accuracy: a recent study by <ref type="bibr">Bansal et</ref> al. <ref type="bibr" target="#b3">[4]</ref> shows that increasing AI accuracy may not bring the same improvements for performance if the human cannot develop insights into the AI system. Secondly, improving model interpretability can facilitate model refinement.</p><p>For instance, when designing AIs for autonomous driving, the detection of unexpected road hazards such as lost cargo <ref type="bibr" target="#b32">[33]</ref> is a typical image segmentation task in computer vision. When model developers train a neural network like Fully Convolutional Network (FCN) <ref type="bibr" target="#b26">[27]</ref> or DeepLabV3 <ref type="bibr" target="#b8">[9]</ref> for lost-cargo detection, the accuracy is relatively low and the developers have difficulties in finding potential root causes <ref type="bibr" target="#b24">[25]</ref>, which could be the lighting conditions on the road, the visual features of the lost-cargo objects themselves or others. Identifying such potential root causes can help develop mitigation strategies (e.g. applying appropriate data augmentations) to further improve the model, and model interpretation is the key to discover such root causes.</p><p>To tackle the issue of interpretability in neural networks, many techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref> have been proposed to help people understand model predictions. TCAV (Testing with Concept Activation Vectors) and the follow-up work ACE aim to understand what signals the model uses for predicting different image labels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. They generate a measure of importance of a visual concept (e.g. wheel, glass) for a prediction (e.g. predicted as a car) in a trained model. However, the concepts generated by automatic clustering methods may not match human concepts.</p><p>In other words, such methods cannot guarantee that image patches which are relatively close and gathered in a latent space are semantically meaningful to humans as a concept. This mismatch provides the inspiration for our work. We propose a visual analytics framework to integrate human knowledge in the visual concept extraction process and use the identified concepts to analyze potential causes of model errors and develop mitigation strategies. Specifically, we propose a novel combination of an active learning process with a user interface expressly designed for fast labeling of images to train a concept extractor network that identifies patches containing a common concept. Our system ConceptExtract enables users to explore image patches, control the active learning process and use the resulting concepts for model comparison and diagnosis. We present example usage scenarios for different datasets and machine learning tasks, including image classification for ImageNet <ref type="bibr" target="#b10">[11]</ref> and image segmentation for the lost cargo challenge <ref type="bibr" target="#b32">[33]</ref>. We analyze a variety of neural network architectures, including ResNet <ref type="bibr" target="#b14">[15]</ref>, VGG <ref type="bibr" target="#b39">[40]</ref>, FCN <ref type="bibr" target="#b26">[27]</ref> and DeepLabV3 <ref type="bibr" target="#b8">[9]</ref>, demonstrating the generality of our proposed approach. Using ConceptExtract, users can extract semantically meaningful concepts, provide concept-based explanations for different machine learning models and compare them. Our quantitative evaluation (presented in <ref type="bibr">Section 8)</ref> shows this approach produces concept extractors accurately and more efficiently than random labeling or traditional active learning approaches. Furthermore, we show the validity of the concepts we extract by following up the concept extraction procedure with an associated data augmentation strategy that improves the performance of model under analysis.</p><p>In summary, we contribute:</p><p>â€¢ A novel visual analytics framework supporting a human-in-theloop, active learning based approach to extract visual concepts for model interpretation, as well as identifying visual concepts that negatively affect model performance (Section 4);</p><p>â€¢ A prototype system implementing our proposed human-in-theloop workflow, featuring scalable image patch exploration, visual cues and interactive filters for active learning and a rich set of model diagnostics and comparative analysis visualizations (Section 5);</p><p>â€¢ Two case studies and quantitative experiments demonstrating the value of using ConceptExtract for diverse machine learning tasks and datasets (Section 7).</p><p>â€¢ Quantitative experiments that show ConceptExtract produces concepts faster than traditional active learning, and that these con-cepts can help develop data augmentation strategies for model performance improvement (Section 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Neural Networks</head><p>In this paper, we use neural networks whose first layer has as many units as there are pixels in the input image. To exploit spatial locality, current deep neural networks (DNNs) use convolutional layers, typically followed by nonlinear activation functions. After a sequence of such layers, a fully-connected layer is usually present before the model output. This basic setup can be used in various tasks by assembling different layers; each potential configuration is called an architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Embeddings</head><p>To obtain image patches that potentially contain the same concept, we need some approach to measure image similarity. However, direct pixel difference measurements fail to take into account misalignment, distortions, lighting changes, and so on. To solve this problem, we use deep embeddings as a representation of the image patches. As an image is passed as an input through a DNN model, the output after each hidden layer is an embedding in that latent space. These deep embeddings provide hints for the model to distinguish different images.</p><p>Previous work shows that euclidean distance 1 in the latent space is an effective perceptual similarity metric <ref type="bibr" target="#b53">[54]</ref>. In this paper, we resize inputs to match the architecture's first layer and choose the embeddings from a low-dimensional layer as the latent representation. (In this paper, we use deep embedding interchangeably with latent representation.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Active Learning</head><p>Active learning is a semi-supervised machine learning method where the learning algorithm can interactively query a user for labeling instances. Instead of manually labeling all the unlabeled instances, active learning makes a priority to label the data that have the highest impact on training the model. This method is widely used in training neural networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>. Commonly used prioritizing methods include model confidence, margin sampling, and entropy <ref type="bibr" target="#b37">[38]</ref>. Once an approach has been chosen to prioritize the labeling, this process can be iteratively repeated: a small subset of data with the highest prioritization scores will be presented to the user to assign labels. After that, the DNN can be trained on the manually labeled data. Once the model has been trained, the unlabeled data points can be run through the model to update their prioritization scores, which significantly reduces the overall labeling burden. In this paper, we show that allowing a user to pick from a set of carefully laid-out images produces a more efficient sequence of training models than is possible with pure sequential active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Concept Annotations</head><p>In image classification, all possible categories are assumed to be known to the model, and images are typically assumed to belong to a single class. However, an image may be complex (for example, it can contain various objects and visual patterns). We refer to these "potential" labels of these objects as Concept Annotations. They are different from the classification labels, and an image may admit multiple concept annotations. Concept annotations are not used in training the network for the task, but they can provide the grounding necessary for model explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>The goal of providing a human level of understanding of a deep learning model now drives an entire subfield of machine learning research. While some work focuses on building inherently explainable models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> to achieve interpretability, we in this project focus on post-hoc explanations, interpreting models that were trained without any consideration to interpretability.</p><p>Post-hoc Explanations Saliency methods form a popular class of tools that provide localized explanations for each data sample by calculating the importance of each input feature (typically pixels). Backpropagation methods like Gradients <ref type="bibr" target="#b38">[39]</ref>, DeconvNets <ref type="bibr" target="#b50">[51]</ref> and Guided Backpropagation <ref type="bibr" target="#b42">[43]</ref> compute the gradient of the network's prediction with respect to the input. LRP <ref type="bibr" target="#b2">[3]</ref> redistributes the relevance of each neuron through additive functions that conserve a measure of total importance from layer to layer. VisualBackProp <ref type="bibr" target="#b4">[5]</ref> uses deconvolutions to arrive at a pixel-based importance metric. While saliency methods can illustrate the attention of deep learning models, the resulting heatmap itself can be hard to interpret and assess. The study by Adebayo et al. <ref type="bibr" target="#b0">[1]</ref> shows that some of these popular methods fail the sanity check. Data perturbation methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">51]</ref> use small prediction changes to generate interpretations. In particular, LIME <ref type="bibr" target="#b33">[34]</ref> and SHAP <ref type="bibr" target="#b27">[28]</ref> change the input in a controlled fashion and observe the effect on the output. Slack et al. demonstrate that LIME and SHAP can be easily fooled by crafting adversarial classifiers, showing a potential general weakness of perturbation methods <ref type="bibr" target="#b40">[41]</ref>. While most saliency methods focus on a single data sample, our system adopted concept-based explanations to provide a higher level interpretation that aligns better with human knowledge.</p><p>Inherently interpretable models One way to achieve interpretability is by building models which are inherently explainable. Mimic learning <ref type="bibr" target="#b1">[2]</ref> replaces the deep neural networks with models that are easier to explain. Choi et al. <ref type="bibr" target="#b9">[10]</ref> propose RETAIN, a sequence model with an attention mechanism for highlighting the most meaningful visits of the patients for their diagnosis results. Zhang et al. <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> propose methods that enforce the interpretability of high-level image filters through disentangled representations. ProtoPNet <ref type="bibr" target="#b7">[8]</ref> is a deep network architecture that picks out essential patterns in the image and generates a prediction by comparing those patterns to typical classes it has seen before. Ming et al. <ref type="bibr" target="#b28">[29]</ref> combines prototype learning with deep sequence models to achieve interpretability. TED <ref type="bibr" target="#b16">[17]</ref> suggests that while training a model, the objectives should combine both the explanations and the labels. However, currently how to combine them with popular architectures remains unsolved, and training such models are often time-consuming. Our approach focuses on diagnosing trained model and doesn't need to retrain the original model during the analysis.</p><p>Visualizing latent spaces A latent space in deep learning is a reduced-dimensionality vector space of a hidden layer. The neural network compresses the input and forms a new low-dimensional representation with interesting properties <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref>. Here, we employ latent space techniques to build concept extraction models and provide a good spatial arrangement for users to select images to label. Recently, some interactive visual systems have emerged to facilitate the exploration of the latent space. Spinner et al. <ref type="bibr" target="#b41">[42]</ref> propose an interactive visualization for comparing two different models by exploring their latent spaces. Liu et al. <ref type="bibr" target="#b25">[26]</ref> propose LSC (Latent Space Cartography), a comprehensive system for mapping and comparing meaningful semantic dimensions within latent space. SMILY <ref type="bibr" target="#b6">[7]</ref> is an interactive system to help pathologists search for similar medical images of patients based on the users' preferences. We use latent spaces indirectly as the input for our concept extraction networks and use latent spaces to drive the layout of the image patch view. More than that, by utilizing different visualization techniques, ConceptExtract can help the users easily explore the dataset and summarize their findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TASKS AND WORKFLOW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Analysis</head><p>Model developers encounter different problems while diagnosing their model to make improvements. Developers want to understand predictions and find the leading causes of a specific result. For example, if a classification model predicts an image as a "fish", is it because it recognizes the fish body, or is it using contextual cues in the image such as a human holding it or a container carrying the fish? This same question has been studied in Summit <ref type="bibr" target="#b19">[20]</ref> as well. Another main concern is the identification of systematic causes of misclassification. When developing a semantic segmentation model for analyzing road scenes in an autonomous driving application, developers find that a dog is incorrectly detected under a tree shadow. Is this just a coincidence or a common phenomenon happening across the entire dataset? Answering these questions not only help the developers better understand and anticipate the model behavior, but also helps them develop effective strategies to refine the model and improve its performance.</p><p>With ConceptExtract, we seek to give model developers a visual analytic system so they can interpret, diagnose and compare deep learning models with human-friendly, concept-based explanations. Concretely, based on previous discussions in visual analytics for deep learning <ref type="bibr" target="#b18">[19]</ref> and the requisite expertise of the co-authors from past experience, we start by identifying a set of analytic tasks to be supported in the system. T1: Summarize model behavior. The system should provide a summary of the model to the developers to start with. Deep learning models can have different performance metrics depending on the task, e.g. precision in image classification model and IoU (Intersection over Union) accuracy in semantic segmentation models; prompt access to these measures is a requirement.</p><p>T2: Browse and explore image patches/super-pixels. It is challenging for users even to know what visual concepts exist in the data. Since each dataset potentially contains many concepts, it is important for the user to be able to extract visual concepts that highly influence model decision. The system, therefore, needs to provide an overview of the image patches with a good layout strategy, as well as also provide a set of filters to help users quickly identify interesting data samples and decide which image patches to study first.</p><p>T3: Train and evaluate concept extraction models. Since no ground truth labels exist for visual concepts, and it is infeasible for users to manually label a large number of images, we propose using a separate concept extraction active learning loop to efficiently derive a set of image patches containing a visual concept. The system should involve users' human knowledge and give them the flexibility to choose and customize any potential concept they recognize in the image patches. It should also provide methods for the user to evaluate whether the model has sufficiently learnt the visual concept.</p><p>T4: Analyze how visual concepts affect model decisions. After extracting human-friendly visual concepts, the system should support using them to understand model behavior. The system should help users systematically analyze how important the visual concepts are for predicting different classes and analyze how the presence of different visual concepts in images affects model performance (e.g. shadow prevents detection of objects on the road).</p><p>T5: Compare different models. In addition to investigating the target model, the system should further support using the visual concepts extracted for fine-grained model comparison, esp. how the performance of the models differ on images containing different visual concepts. This helps reveal the strength and weaknesses of different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Workflow</head><p>To integrate the tasks described above, we present a workflow to help guide the user through the analysis steps in ConceptExtract. We will refer to Fig. <ref type="figure" target="#fig_0">2</ref> throughout the section. For the specific settings of the workflow in real-world applications, please refer to Section 7.1 and Section 7.2 for more details.</p><p>The workflow starts with a preprocessing stage (left portion) with the available image data and the target model. In the preprocessing stage, the original images are segmented into patches using fixed window sizes or super-pixel segmentation algorithms <ref type="bibr" target="#b46">[47]</ref>. The image patches/super-pixels are then resized to the input scale and fed to the target model. Their latent representations are extracted at a selected layer in the target model for visual concept learning. The visual concept learning stage (center portion) uses concept extractor networks on top of the latent representations to learn human-understandable concepts and retrieve image patches containing those concepts for model analysis (T3). Individual networks with the same architecture but different weights are trained to recognize different visual concepts through an active learning process. To help users create meaningful novel visual concepts, the system provides an overview of the image patches and projects them in a way such that visually similar image patches are close to each other. The user can also interactively overlay a variety of information on top of the image patches such as accuracy, ground-truth and predicted labels to prioritize looking for visual concepts that affect model performance (T2).</p><p>To support effective novel visual concept learning and reduce user labeling effort in the active learning process, we propose a hybrid approach that tightly couples visualization and computational techniques (T3). For each image patch, the concept extractor network produces a concept confidence score. The concept confidence score ranges from 0 to 1, where 0 is for confidently negative (the image patch does not contain the visual concept), 1 is for confidently positive (the image patch must contain the visual concept), and 0.5 is for not sure. The system visualizes the concept confidence score and supports interactive data filtering based on it to help the users prioritize labeling more informative examples for model training. In particular, we found that labeling hard negative samples <ref type="bibr" target="#b36">[37]</ref>, which are image patches confidently but wrongly classified, can greatly facilitate the training process. The user can also filter the image patches with the most confident predictions to verify if the concept extractor has been sufficiently trained to recognize visual concepts that align with human-knowledge. To further reduce user effort and recognize novel visual concepts with very few labeled examples provided by the user, we also use a data augmentation strategy <ref type="bibr" target="#b17">[18]</ref> which has been proven to be effective in similar scenarios such as few-shot learning or zero-shot learning <ref type="bibr" target="#b23">[24]</ref>. The data augmentation method selects each labeled image patch, randomly applies two categories of augmentation policies: (1) shape policies like shearing, flipping, rotating, and (2) color policies like gray-scaling and blurring.</p><p>After obtaining a set of visual concepts and the corresponding image patches, the user can move to the model analysis stage (right portion) and perform model interpretation, diagnostics and comparison (T4 and T5) using TCAV scores and confusion matrices. The visualization shows fine-grained analysis, including how each visual concept affects the model's and how the model performances differ on images containing different visual concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYSTEM DESCRIPTION</head><p>As shown in Fig. <ref type="figure">1</ref>, the visual interface consists of a set of visualization modules to display information like model summary (T1), visual concept images, and a series of interactions to support image patch explorations and the active learning process for training the concept extractor network. We will discuss the main components in ConceptExtract and how the these components together support the workflow ( Fig. <ref type="figure" target="#fig_0">2</ref> ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Image Patch View</head><p>The image patch view ( Fig. <ref type="figure">1 (A)</ref>) provides an overview of the image patches to help the user quickly explore the data collections and identify interesting visual concepts (T2). We apply t-SNE <ref type="bibr" target="#b45">[46]</ref> to the image patches' latent representations to get a 2D layout. Since directly plotting the image patches according to the projected coordinates will result in severe visual clutter, we use a de-cluttering algorithm to layout the image patches in non-overlapping grids while still keep visually similar image patches close to each other. Specifically, we partition the canvas area into grids with identical size rectangles. Then we randomize the image patch sequence. For each image patch, we find the grid cell containing the 2D coordinates. If the grid is empty, we plot the image patch on the grid. If the grid is already occupied, the layout algorithm will search for the nearest neighbor grids to fill. When no empty grid is available on the screen, the image patch will be hidden temporarily. Navigation operations like zooming in will increase the number of grid cells available. When a different scale is reached, we replot the image patch view to allow more image patches to be displayed on the screen. We bring similar image patches as close as possible through this layout while reducing visual clutter due to overdraw.</p><p>A control panel on top of the image patch view allows users to overlay additional information on the image patches as well as filter the data. When the users first explore the data, it is challenging for them even to know where to start their study. The "cluster" filter ( Fig. <ref type="figure">1 (b)</ref>) gives the user the option to plot only image patches in the selected clusters precomputed using algorithms such as k-means. Users can also choose color overlays or border highlights on the image patches to show information such as ground-truth, model predictions and model accuracy ( Fig. <ref type="figure">1 (a)</ref>). For example in Fig. <ref type="figure">1(A)</ref>, for an image segmentation model, the visualization displays pixel-wise image segmentation accuracy where red indicates the wrong prediction and blue indicates the right prediction. In another example shown in Fig. <ref type="figure" target="#fig_2">4</ref>(a), the visualization uses border color to indicate whether the source image of a super-pixel is correctly classified in an image classification model. With different overlays, users can focus on particular image patches to extract the relevant visual concept. For example, the user is usually interested in image patches related to wrong predictions ( Fig. <ref type="figure">1 (d)</ref>) and extracting visual concepts from those image patches could better benefit model diagnostics.</p><p>As a crucial component in the active learning process, the control panel also has a range slider ( Fig. <ref type="figure">1 (c</ref>)) to help users efficiently filter the data based on the concept confidence score of each image patch for the concept currently being trained. The user can also draw the concept confidence score as the border color of the image patches in a diverging color scheme, as shown in Fig. <ref type="figure">1</ref>.  <ref type="bibr" target="#b14">[15]</ref> trained on ImageNet (B) a Fully Convolutional Network (FCN) <ref type="bibr" target="#b26">[27]</ref> with a DenseNet encoder <ref type="bibr" target="#b21">[22]</ref> and a Pyramid Scene Parsing (PSP) <ref type="bibr" target="#b54">[55]</ref> structure trained for road image segmentation. We extract deep embeddings from the smallest hidden layers available. The chosen layers are highlighted in the architecture diagrams. The concept extractor network used for each architecture is shown on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Training View</head><p>The training view ( Fig. <ref type="figure">1 (C</ref>)) provides a frontend to control the active learning process (T3). It contains two parts: a patch details &amp; interaction area (( Fig. <ref type="figure">1 (e)</ref>) for the user to assign concept labels and a training samples list ( Fig. <ref type="figure">1 (f</ref>)) for showing selected images and their training status. The selected image patch from the image patch view will be magnified, and the related information will be presented such as the source of the image patch. The user can directly add any patch that doesn't contain the concept into the negative training set by selecting on the context menu. To add positive samples, the user either crops a rectangle on the image that contains the concept and discards the rest of the pixels, or directly selects the whole image patch/super-pixel as a positive sample. All the selected positive and negative samples will be displayed in the training samples list ( Fig. <ref type="figure">1 (f)</ref>). Concepts can be named and saved for use in future sessions. While the active learning network is trained, the user can continue adding different image patches into the training set, or end that training stage, save the concept extractor network and the retrieved images containing the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Model Analysis View Using Visual Concepts</head><p>The Model Analysis View uses the learned visual concepts to support fine-grained model interpretation, diagnostics and comparison ( Fig. <ref type="figure">1(D)</ref> ). After a user completes a new concept extractor's training process, ConceptExtract shows the record of this concept in this area, including the concept name and the image patches with the highest confidence scores. A barchart shows TCAV scores for each visual concept, and the length of each bar indicates the importance of this concept for predicting a specific class (T4). To gauge a potential weakness of the model being analyzed with respect to the concepts, we choose for each concept the top 50 image patches based on the concept confidence score, find the original images of these image patches and compare predictions of our target model with the ground-truth using a confusion matrix (T4). Each row in the confusion matrix represents the ground truth class, and each column represents the predicted class. The values on the matrix diagonal show the proportion of the data samples correctly classified in each class. We use a sequential colormap to encode the proportion ranging from 0 to 1. With the confusion matrices, the user can analyze whether the presence of a certain visual concept in the image leads to more model errors. An example is shown in Fig. <ref type="figure">1(g)</ref>, where the model being analyzed has worse performance on images containing the shadow concept.</p><p>We can also use the learned visual concepts to to compare different models visually. For the two selected models from a list, we compute their confusion matrices for each of the visual concepts and then directly calculate the difference between them. The differences are displayed using a diverging colormap, where red indicates negative values and blue indicates positive values in the matrix. If a second model has better performance than the first one, the diagonal entries should show more positive values (blues) in the matrix and vice versa. For example, in Fig. <ref type="figure" target="#fig_4">6</ref>(b) we compare DenseNet to ResNet on images containing the visual concept sky. Since there are more red colored entries on the diagonal, we can conclude that DenseNet has worse performance on this set of images. Such comparison reveals the strength and weaknesses of each model and helps identify opportunities to use model ensembles to improve prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Other Views</head><p>The model summary view ( Fig. <ref type="figure">1 (E</ref>)) shows basic information like the datasets and the model types. We use both bar charts and confusion matrices to show model performance on different classes (T1). A cross-filter view ( Fig. <ref type="figure">1 (B</ref>)) shows the distribution of image patches based on different features, supporting quick retrieval and comparisons (T4). In this view, each image patch could be treated as a multivariate data sample, including variables like prediction accuracy and concept confidence scores for the existing concept extractors. A barchart is displayed for each of these variables. To help the user quickly identify an interesting target and generate new facts, the crossfilter view is also connected with the image patch view. Only the selected image patches in the crossfilter will be plotted in the image patch view. These concept filters can help the user quickly identify confident or confused image patches for different concepts. It is particularly useful when the user has trained multiple visual concepts and would like to study how the learned concepts correlate with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SYSTEM IMPLEMENTATION</head><p>Our system design separates the frontend for data visualization and the backend for data storage and active learning. For the backend of the system, we use Pytorch <ref type="bibr" target="#b30">[31]</ref> to implement the target machine learning models including DenseNet-FCN (We use the implementation in this repository: https://github.com/sagieppel/Fully-convolutionalneural-network-FCN-for-semantic-segmentation-with-pytorch.) and ResNet-101 <ref type="bibr" target="#b15">[16]</ref>, as well as other models for comparison including DeepLabV3+ <ref type="bibr" target="#b8">[9]</ref>, DenseNet <ref type="bibr" target="#b21">[22]</ref>, VGG <ref type="bibr" target="#b39">[40]</ref>. We also use Pytorch to implement and train the concept extractor networks. To extract visual concepts, all the images are segmented into small image patches or super-pixels of different sizes. We use scikit-image (https://scikitimage.org/) for super-pixel extraction. The image patches or superpixels are then scaled to the same size as the input of the target model. By running them through the target model, we extract and save the latent representation of these image patches (or super-pixels) at the selected layer. All image patches, along with their latent representations, ground-truth labels, predicted labels and (per-pixel) accuracy are stored in the backend system as binary files in the file system. The application web server is implemented with Flask <ref type="bibr" target="#b13">[14]</ref>. For the frontend design, we mainly rely on two JavaScript libraries, React and D3 <ref type="bibr" target="#b5">[6]</ref> and draw on both SVG and HTML5 Canvas for better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXAMPLE USAGE SCENARIOS</head><p>We demonstrate ConceptExtract in example usage scenarios on two different perception models for image classification and semantic segmentation tasks. By utilizing our system, interesting visual concepts are revealed in the models and the datasets. We further demonstrate how to use these concepts to help model interpretation, model comparison, and developing mitigation strategies for model performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">ResNet-101 for Image Classification</head><p>In this section, we demonstrate the application of ConceptExtract in analyzing an image classification model trained on ImageNet data. We show that the system can help extract semantically meaningful visual concepts. ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) is a famous competition which has been held annually since 2010. ILSVRC uses a subset of ImageNet <ref type="bibr" target="#b10">[11]</ref> with roughly 1.4 million images belonging to 1000 categories. For simplicity, we choose a subset of ImageNet with 10 classes which are the 10 worst performing classes of the model (tench, tabby, tiger cat, tiger, barracouta, balloon, castle, church, parachute, vault). Also, most of the wrong predictions of these 10 classes made by the model are among the same 10 classes, for example, tench is often misclassified as another type of fish, tabby. In each class, we randomly sampled 200 images for analysis, which we think can be processed quickly by our system while generating enough concept patches. We analyze the pre-trained ResNet-101 <ref type="bibr" target="#b15">[16]</ref> model from PyTorch with top-1 error 22.63% and top-5 error 6.44%. To extract concepts of different resolutions from these classes, we use the Quickshift algorithm <ref type="bibr" target="#b46">[47]</ref> to compute a hierarchical segmentation of the images on multiple scales. For each image, Quickshift generates about 12 super-pixels. After that, we resize these super-pixels to the original input size of the model and pad the empty pixels with neutral gray. We choose the final convolutional layer ( Fig. <ref type="figure" target="#fig_1">3(A)</ref> ) with dimensions 2048 Ã— 7 Ã— 7 to extract the latent representation of each super-pixel.</p><p>Concept Extractor Network Setup Our concept extractor network ( Fig. <ref type="figure" target="#fig_1">3(A)</ref> ) contains only two layers on top of the latent representation extracted from ResNet-101: one convolutional layer and one max pooling layer. A sigmoid function is applied after the max pooling layer to obtain a concept confidence score between 0 and 1 to predict whether the super-pixel contains the specified visual concept or not. This simple architecture is accurate enough for identifying concept images (see Section 8 for details), and training such a network will not take the user a lot of waiting time. For each stage, the neural net is trained until the validation loss does not decrease. We observe that in Extracting Visual Concepts The system initially displays all the super-pixels in a compact layout, generated from a t-SNE projection of the latent representation. The layout places semantically-similar superpixels in clusters that can have associated concepts such as grass and sky. To prioritize finding concepts that affect model performance, the user can overlay predictive accuracy for each super-pixel. For example, one can observe a cluster of super-pixels showing orange and black strip patterns are often misclassified ( Fig. <ref type="figure" target="#fig_2">4(a)</ref> ). The user therefore starts creating a new visual concept "orange and black stripe" by adding new labeled examples. After specified 4 to 5 positive and the same number of negative samples, the user can click on the "Train" button in the Labeler to start the first training stage for this concept extractor. The training time depends on the dimension of the latent representation and the GPU configuration. On a machine with a GTX 1070Ti GPU, it typically takes about 50 seconds to train one stage. Based on the returned concept confidence score, the user can use the filter to select more informative examples to label, esp. hard negative samples which are confidently but wrongly classified by the concept extractor. After several iterations, the user finds that almost all the super-pixels filtered with a range of high concept confidence score (e.g. 0.75 âˆ’ 1.0) contains orange and black stripe patterns ( Fig. <ref type="figure" target="#fig_2">4(b)</ref> ) and all the super-pixels filtered with low and medium concept confidence scores (e.g. 0.0 âˆ’ 0.5) do not contain the stripe patterns. Therefore the user can consider the concept extractor network has successfully learned the orange and black strip concept and use it for model analysis and comparison.</p><p>The user can continue exploring the image patch viewer and create new visual concepts following a similar process. For each new visual concept, an individual concept extractor network is created and trained. They all have the same architecture as described in Fig. <ref type="figure" target="#fig_1">3(A)</ref>. For example, she can train four separate concept extractor networks to identify visual concepts, including human face, fish, stripes, and sky.</p><p>Model Analysis with TCAV Scores and Confusion Matrix From the TCAV scores, we can identify that the human face concept is highly relevant for predicting the class tench ( Fig. <ref type="figure" target="#fig_3">5(a)</ref> ), a type of fish. This result aligns with the findings in Summit <ref type="bibr" target="#b19">[20]</ref>, a previous paper that also analyzes deep neural networks for ImageNet classification. They also found that predicting the class tench relies heavily on person related features. Notice that we use ResNet-101, not InceptionV1 <ref type="bibr" target="#b43">[44]</ref> as in Summit. This leads to the hypothesis that the data distribution, in- stead of the model architecture, is causing the problem. Since the training data contains a lot of images of person holding tench, both models automatically make use of such visual concept to perform classification.</p><p>Based on the TCAV scores we can also observe that the three frequency confused classes tiger cat, tiger and tabby cat all uses stripes as a visual concept to perform classification ( Fig. <ref type="figure" target="#fig_3">5(b)</ref> ). The confusion matrix shows that on images containing stripes, the model often make mistakes among the three types of feline animals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compare Different Models</head><p>The visual concepts extracted can be reused to obtain a fine-grained comparison between different models, which goes beyond simple benchmarks such as overall model accuracy.</p><p>In particular, we can analyze which model is better at classifying images containing a certain concept. In this example, the user loads another state-of-the-art model DenseNet <ref type="bibr" target="#b21">[22]</ref> to compare it with ResNet-101. Based on the confusion matrix the user observes that while DenseNet performs better than ResNet-101 on images containing visual concepts like human-face, it makes more mistakes on images containing the sky concept ( Fig. <ref type="figure" target="#fig_4">6</ref>(a)(b) ). Based on such observation, the user hypothesizes that combining DenseNet and ResNet-101 may result in a stronger model. To verify such a hypothesis, we construct a simple ensemble model which takes the prediction (in the form of class probability) from both DenseNet and ResNet-101 and average the results to obtain the final class prediction. We further compare the ensemble model with DenseNet and ResNet-101 and observe that it indeed corrects the miss-classification of both models ( Fig. <ref type="figure" target="#fig_4">6(c)(d)</ref> ). We further verify the results by comparing the overall accuracy on the ten classes and found that the ensemble model achieves 81.8% accuracy that outperforms both DenseNet (80.0%) and ResNet-101 (80.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">FCN and Semantic Segmentation</head><p>In this section, we will focus on presenting the insights discovered by ConceptExtract when analyzing an image semantic segmentation model for detecting unexpected objects on the road, usually lost cargos. The model is trained and tested on the public lost cargo dataset <ref type="bibr" target="#b32">[33]</ref>. By utilizing our approach, we show that the model designers can obtain concepts that are both customized and human understandable. They can further utilize the insights generated from the concept to diagnose the model and improve model performance.</p><p>The lost cargo challenge addresses the problem of detecting unexpected small obstacles on the road often caused by lost cargo. To achieve this goal, a Fully Convolutional Network (FCN) <ref type="bibr" target="#b26">[27]</ref> with a DenseNet Encoder <ref type="bibr" target="#b21">[22]</ref> and Pyramid Scene Parsing <ref type="bibr" target="#b54">[55]</ref> ( Fig. <ref type="figure" target="#fig_1">3(B)</ref> ) is trained. We denote the model as DenseNet-FCN in our study.</p><p>DenseNet-FCN performs semantic image segmentation by predicting a label for each pixel in the image. In this case, each pixel could belong to three different classes, including lost-cargo (obstacles), road, and background. As shown in Fig. <ref type="figure" target="#fig_1">3(B)</ref>, to extract the latent representations for concept learning, we chose the layer at the beginning of the decoder (dimension: 512 Ã— 32 Ã— 64) for two reasons: (i) the layer encodes both local and global information, (ii) the layer has the most compact size, which will benefit future computation and storage.</p><p>For this task, since the model designers want to keep the context of potential concepts, we use rectangle boxes with three different sizes to obtain image patches for extracting concepts instead of segmenting the image into super-pixels. Since there are a large number of image patches (over 4 million), we sampled a subset of them for analysis. Furthermore, since the main task is to detect the lost cargo on the road, we chose all the image patches containing lost cargo (roughly 1000) and sampled around 1000 image patches containing the other two labels: road and background. In all, we have 2533 image patches for concept extraction and visualization.</p><p>The lost cargo has two types of pixel annotations: the coarse ones including lost cargo (obstacle), road, and background; the fine ones for distinguishing specific lost cargo objects/obstacles in the images like boxes, balls, and so on. The coarse annotations are used by DenseNet-FCN for training and prediction. To quantitatively evaluate our concept extraction model, we use the fine annotations as groundtruth visual concepts. We pick a concept -dogs and trained the concept classifier for 4 iterations. Ten positive and ten negative images are selected for the initial stage, and for each of the rest stages, four positive and four negative images are added. The results are presented in Fig. <ref type="figure" target="#fig_5">7</ref>. The figure plots the precision of the concept extractor when retrieving top-k image patches according to the concept confidence score. For each active learning stage, we can see a significant improvement in the precision of the predictions after the active learning process, especially for the top 50 image patches based on the concept score.</p><p>As shown in Fig. <ref type="figure">1</ref>(A), to prioritize the visual concepts that affect model performance, the user overlays the pixel accuracy of the model prediction on each image patch. While exploring these image patches, the user identifies that sometimes the lost cargo cannot be correctly detected when it is under a tree shadow ( Fig. <ref type="figure">1(d)</ref>). Is this just a coincidence, or is it happening across the entire dataset? To answer this question, the user creates the visual concept named "shadow". She also starts specifying positive and negative samples for the concept extractor to learn to retrieve similar image patches also containing shadow. The training process also utilizes the data augmentation strategy described in Section 4.2. The data augmentation process generates 200 images for both positive and negative training sets.</p><p>The model analysis result of "shadow" is displayed in Fig. <ref type="figure">1</ref>(g), together with some other concepts. From the confusion matrix, the user can verify that indeed the DenseNet-FCN model performs worse on the images containing "shadow" images compared to images containing other concepts such as standard objects. Meanwhile, the TCAV score indicates that the "shadow" pattern influences the prediction of all three segmentation labels (in the image segmentation model, we consider each pixel as an individual data sample to compute the TCAV score <ref type="bibr" target="#b22">[23]</ref>). To validate this hypothesis, we augmented the training set with artificially-generated shadows ( Fig. <ref type="figure" target="#fig_6">8</ref> ). We randomly draw a boundary line across the lost cargo's bounding box. On a random side of the line, we apply a brightness reduction. To make the shadow more realistic, we gradually change the darkness around the boundary with Gaussian blur. As shown in Table <ref type="table" target="#tab_0">1</ref>, the fine-tuned model after augmentation is more accurate. To further verify this strategy's scalability, we also apply the shadow augmentation to another state-of-art model, DeepLabV3+ <ref type="bibr" target="#b8">[9]</ref> and we see an improvement for IoU accuracy as well.</p><p>For this particular usage scenario, we interviewed and gathered feedback from an industrial expert with 15+ years of experience in computer vision research and has been heavily involved in the development of autonomous driving software. We introduced the main idea of using visual concept learning for model diagnostics and presented a walk-through of the system through a remote video call. He immediately identified that the system has great potential for collecting similar edge cases (like object under shadows) where the model frequently makes mistakes. The visual concepts collected provide a good way to cluster the edge cases, reason about them, and develop corresponding mitigation strategies (such as adding artificial shadow augmentation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTAL VALIDATION</head><p>In an active learning process, the performance of the model varies with the training strategies. For example, even if two models are identical in structure, different methods for selecting the labeling candidate can generate significantly different model states. Another similar option is how much data should be labeled in each stage of the active learning process. In order to understand the impact of these variations, we carried out a series of experiments to evaluate the effectiveness of our active learning model in identifying these concepts under different settings. In the experiments, we mainly consider two main factors: sampling strategy and latent representation. The first one mainly refers to how we select the samples for the user to label at each stage. We compared concept extractors trained using different sampling methods and explained what sampling strategy we choose for ConceptExtract. For the second one, to investigate the influence of different latent presentations, besides the current DenseNet-FCN model, we used another well-trained model-VGG-16 to extract the latent representations of the same data and trained the active learning models for the same concepts. We compared the accuracy of these concept extractors and demonstrated the concepts these extractors generate. Finally, we also make justifications about other choices in ConceptExtract including model architecture, cutout methods, and prioritizing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Concept Quality vs. Sampling Strategy</head><p>One of the main features of ConceptExtract is that it can include human knowledge in the training process of the active learning model. The users are able to choose which image patches to add to the training set for each stage. Another feature to involve human knowledge is that the user can brush on the image patches to mark the pixels containing the concept of their interest. To study if these two kinds of human knowledge can actually lead to a better concept extractor, we set up a baseline model.  An important choice to have to make during the active learning process is how many positive and negative samples should be labeled by the user at each stage. We seek a sampling strategy that makes the concept extraction network as accurate as possible while not requiring too many manually labeled samples. We tried a number of combinations in labeled images from each stage.</p><p>In Fig. <ref type="figure" target="#fig_8">9</ref>, we demonstrate the precision curves of the concept extractors for "standard objects" and "bobby cars" under different sampling methods. The five models are: This result illustrates that including human knowledge can improve the quality of the concept extractor. As we increase the number of training points for each stage, we see obvious precision improvements for the concept extractors as well. This suggests that we should include more image patches in the training process, especially for the first stage. Comparing these two concepts, "standard objects" are relatively easier to identify; using only about 3% of the data, we obtain a good concept extractor (at 80% accuracy for the top 100 selections). Although performance is not as good as the upper bound model as we expand the selections, the concept extractor still has a stable precision value. On the other hand, "bobby cars" is a more difficult concept to extract. Even using all the data, the top selection after 50 is lower than 0.5. Nevertheless, using only 7% of the positive data, we have already obtained a concept extractor approaching near upper bound performance, especially within the most confident predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Concept Quality vs. Latent Representation</head><p>In Section 8.1, we have seen different degrees of difficulty for training different concept extractors. We also find that the overall performance for the concept extractor network of "standard objects" is better than the one of "bobby cars." We mainly have two possible hypotheses. One assumption is about the model architecture. Since we are using a shallow network for short training time, is the active learning model complex enough to identify different concepts? Another assumption is about the latent representation. Since the extractors are using the deep embeddings extracted from the task model as input instead of the image's pixels, will the deep embeddings be capable of capturing these concept features in the latent space? To investigate how different deep embeddings will affect the quality of the extractors, we carried out an experiment, where we obtained a pre-trained VGG-16 model from Pytorch, sent the image patches through this model and extracted a new set of deep embeddings. After that, we trained the extractors for the same concepts using these new deep embeddings. Comparing to the image segmentation model (FCN) we originally used, this VGG-16 model is well-trained and the performance for the classification task is fairly good.</p><p>In Fig. <ref type="figure" target="#fig_9">10</ref>, we show results for "standard objects", "bobby cars", and "dogs", concepts trained with two different deep embeddings. For all concepts, extractors using embeddings of VGG-16 perform better than those of FCN. For "bobby cars" and "dogs", the difference is substantial: precision values are roughly 0.7 and 0.9 for the top 50 selections, compared to those using the FCN model, which is about 0.4 and 0.5.</p><p>This experiment also verifies that even though the architecture of our concept extraction network is very simple, it can still extract concepts from the image patches. It also indicates that we can relate the extraction network's inability to extract a concept to the quality of the task model representations in the latent space. For an image segmentation model like FCN, since the model is trained on the specific task to differentiate lost cargo, road and backgrounds, it is not able to distinguish different type of cargoes like bobby cars and dogs, or telling humans and trees from the background very well. In contrast, a high-quality large image classifier like VGG-16 readily separates these features, providing evidence that such model understands the concepts being extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Other Considerations</head><p>How complex must the concept learning model be? We ran an ablation study where we increased and decreased model complications a little by adding or removing hidden layers. Additional layers generally make convergence harder, and removing layers lowers the model accuracy significantly (see supplemental materials). How should we prioritize the display of active learning candidate images? Instead of the common practice of letting users focus on images where the active learning model is least confident, we instead focus on identifying confident but wrong images from the perspective of the base model. This choice maximizes the performance of the concept model in its most confident predictions, which matches the goal of ConceptExtract better: extracting human understandable concepts via exemplar images.</p><p>How should users localize class information in images? There are two natural candidate UI techniques: rectangular brushing or lassoing <ref type="bibr" target="#b29">[30]</ref>. We hypothesized that lassoing a detailed outline of the concept instead of a rectangle would increase the accuracy of the concept extractor. After running an experiment on FCN and VGG-16, we saw no significant difference in model performance for these two methods. Thus we choose the simpler UI: brushing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">LIMITATIONS, FUTURE WORK, AND CONCLUSION</head><p>Although we have shown that our prototype system can outperform standard active learning approaches (the baseline model), there are still a number of limitations to be addressed in the future. Most importantly, we would like to understand specifically what parts of the interface provide the most benefit for the human-in-the-loop approach? For example, how important is the exploratory clustering we provide during the active learning process? How do different modules affect the choice the user makes in selecting image patches? Would a different image patch layout help? These questions also will probe our understanding of the role of human expertise in the process.</p><p>Secondly, the current system supports a limited number of training data and image patches, and the user interface choices we made would not be effective under a larger number of classes and image samples. Although it is always possible to sample a small number of images from the training set to present to a user, a full study of the impact of choosing to show more (or fewer) images remains necessary.</p><p>Finally, a full evaluation in real-world application scenarios remains needed to complement our preliminary expert evaluation. For future machine learning interpretability practice, we would like to study more examples about how machine learning experts use these concept-based explanations to improve and understand their models.</p><p>In conclusion, we presented a novel approach for extracting concepts to help interpret neural networks, and contributed the use of a visualization-assisted active learning loop to extract interpretable concepts. We integrate the full pipeline into an interactive visualization system, ConceptExtract. Through case studies and experimental validations, we show our approach can extract concepts that are both human understandable and customizable based on the user's interest.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The workflow of ConceptExtract. In the preprocessing stage, the images are segmented into image patches or super-pixels. The latent representations of these image patches/super-pixels are extracted from a selected layer in the target model. The visualization interface layout the image patches such that similar patches are spatially close. Users can easily identify and create new visual concepts and overlay data such as target model misclassifications to focus on problematic cases. In the visual concept learning stage, we utilize concept extractor networks to retrieve image patches containing the same concept (sky and shadow in the figure). The concept extractor networks take the latent presentations of the image patches as inputs and output concept confidence scores in [0, 1]. We employ a visualization assisted active learning process to train the concept extractor networks. The learned visual concepts are used in the model analysis stage for model interpretation and comparison with visualizations such as TCAV scores charts and confusion matrices.</figDesc><graphic coords="4,69.35,102.53,107.06,62.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. We apply ConceptExtract to analyze image classification and segmentation DNNs: (A) ResNet-101<ref type="bibr" target="#b14">[15]</ref> trained on ImageNet (B) a Fully Convolutional Network (FCN)<ref type="bibr" target="#b26">[27]</ref> with a DenseNet encoder<ref type="bibr" target="#b21">[22]</ref> and a Pyramid Scene Parsing (PSP)<ref type="bibr" target="#b54">[55]</ref> structure trained for road image segmentation. We extract deep embeddings from the smallest hidden layers available. The chosen layers are highlighted in the architecture diagrams. The concept extractor network used for each architecture is shown on the right.</figDesc><graphic coords="5,53.75,137.33,239.90,59.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) The user identifies that many stripe patterns are associated with erroneous predictions (b) Through active learning the concept extractor network is able to accurately retrieve large amount of super-pixels containing the strip patterns. The stripe concept will be used for further model analysis.</figDesc><graphic coords="6,56.27,49.37,225.70,208.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) TCAV score shows that the human face is surprisingly important for identifying the class tench, a type of fish. (b) stripes is an important visual concept for identifying tiger cat and tiger and is also the main reason that these two classes are often confused with each other. most cases the training stops at around 10 epochs. Note that only the weights in the concept extractor network are updated (and not the task model under analysis). The concept confidence scores are sent to the frontend after the training is done. They are displayed in the image patch view and can also be used to filter the image patches to find the most informative examples to label. To obtain more training data, we generate 200 images for both positive and negative training sets by data augmentation. The 200 images are generated by first randomly sampling from the available training data and then applying the data augmentation strategy.Extracting Visual Concepts The system initially displays all the super-pixels in a compact layout, generated from a t-SNE projection of the latent representation. The layout places semantically-similar superpixels in clusters that can have associated concepts such as grass and sky. To prioritize finding concepts that affect model performance, the user can overlay predictive accuracy for each super-pixel. For example, one can observe a cluster of super-pixels showing orange and black strip patterns are often misclassified ( Fig.4(a)). The user therefore starts creating a new visual concept "orange and black stripe" by adding new labeled examples. After specified 4 to 5 positive and the same number of negative samples, the user can click on the "Train" button in the Labeler to start the first training stage for this concept extractor.The training time depends on the dimension of the latent representation and the GPU configuration. On a machine with a GTX 1070Ti GPU, it typically takes about 50 seconds to train one stage. Based on the returned concept confidence score, the user can use the filter to select more informative examples to label, esp. hard negative samples which are confidently but wrongly classified by the concept extractor. After several iterations, the user finds that almost all the super-pixels filtered with a range of high concept confidence score (e.g. 0.75 âˆ’ 1.0) contains orange and black stripe patterns ( Fig.4(b)) and all the super-pixels filtered with low and medium concept confidence scores (e.g. 0.0 âˆ’ 0.5) do not contain the stripe patterns. Therefore the user can consider the concept extractor network has successfully learned the orange and black strip concept and use it for model analysis and comparison.The user can continue exploring the image patch viewer and create new visual concepts following a similar process. For each new visual concept, an individual concept extractor network is created and trained. They all have the same architecture as described in Fig.3(A). For example, she can train four separate concept extractor networks to identify visual concepts, including human face, fish, stripes, and sky.Model Analysis with TCAV Scores and Confusion Matrix From the TCAV scores, we can identify that the human face concept is highly relevant for predicting the class tench ( Fig.5(a)), a type of fish. This result aligns with the findings in Summit<ref type="bibr" target="#b19">[20]</ref>, a previous paper that also analyzes deep neural networks for ImageNet classification. They also found that predicting the class tench relies heavily on person related features. Notice that we use ResNet-101, not InceptionV1<ref type="bibr" target="#b43">[44]</ref> as in Summit. This leads to the hypothesis that the data distribution, in-</figDesc><graphic coords="6,318.83,142.01,161.42,62.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The confusion matrices show pairwise model comparison, with fine-grained information about which model performs better on images containing a given concept. DenseNet performs better than ResNet on images containing human faces (a) but worse on images containing sky (b). The two models show complementary strength, suggesting that a model ensemble averaging their predictions outperforms both (c and d).</figDesc><graphic coords="7,53.87,139.37,107.55,90.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The precision curves of the "dogs" concept extractor network in the lost cargo challenge. The fine annotations available in the dataset are used as the groundtruth. The top k selections are made based on the concept confidence score. From the initial stage 0 to the final stage 3, we observe a significant improvement in the precision value especially for the top selections, validating the effectiveness of the active learning process and the usability of the concept extractor network.</figDesc><graphic coords="8,49.79,35.81,238.46,136.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. This figure shows sample images from augmenting the training set with artificially-generated shadows (Section 7.2). The correlation of the shadow concept with misclassifications of the FCN model led to us augmenting the training distribution. The fine-tuned FCN model after augmentation is more accurate, validating the importance of shadow.Even though the shadow we generated is not realistic, fine-tuning still provides a substantial performance improvement.architecture as the active learning model and it is also trained in 4 stages. For each stage, 10,4,4,4 positive and negative images are needed respectively (For convenience, we use "x-x-x-x" to refer to the number of samples added to each training stage). All the processes like the data augmentation and the training epochs are the same except that instead of using the images chosen and brushed by users, the baseline model mimics a standard active learning approach where the most confused images will be labeled and added to the next training stage. To model a fully-labeled setting, we also create an upper bound model, using all image patches as training data and training the model in a single stage.An important choice to have to make during the active learning process is how many positive and negative samples should be labeled by the user at each stage. We seek a sampling strategy that makes the concept extraction network as accurate as possible while not requiring too many manually labeled samples. We tried a number of combinations in labeled images from each stage.In Fig.9, we demonstrate the precision curves of the concept extractors for "standard objects" and "bobby cars" under different sampling methods. The five models are:(i) baseline model, the model trained without human knowledge; (ii) 2-2-2-2, for each stage, two positive and negative images are added to the training set; (iii) 6-4-4-4, for the initialization, six positive and negative images are added; for the other three stages, four positive and four negative images are added each time; (iv) 10-4-4-4, which is also the current model used in ConceptExtract; (v) upper bound model, trained using all the data in one stage. As shown in the figure, all active learning models perform better than the baseline model.This result illustrates that including human knowledge can improve the quality of the concept extractor. As we increase the number of training points for each stage, we see obvious precision improvements for the concept extractors as well. This suggests that we should include more image patches in the training process, especially for the first stage. Comparing these two concepts, "standard objects" are relatively easier to identify; using only about 3% of the data, we obtain a good concept extractor (at 80% accuracy for the top 100 selections). Although performance is not as good as the upper bound model as we expand the selections, the concept extractor still has a stable precision value. On the other hand, "bobby cars" is a more difficult concept to extract. Even using all the data, the top selection after 50 is lower than 0.5. Nevertheless, using only 7% of the positive data, we have already obtained a concept extractor approaching near upper bound performance, especially within the most confident predictions.</figDesc><graphic coords="8,318.83,44.81,225.74,112.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(i) baseline model, the model trained without human knowledge; (ii) 2-2-2-2, for each stage, two positive and negative images are added to the training set; (iii) 6-4-4-4, for the initialization, six positive and negative images are added; for the other three stages, four positive and four negative images are added each time; (iv) 10-4-4-4, which is also the current model used in ConceptExtract; (v) upper bound model, trained using all the data in one stage. As shown in the figure, all active learning models perform better than the baseline model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The precision curves (plotted based on top-k selections based on the concept confidence score) for two concept extractors: bobby cars and standard objects under five sampling strategies. "baseline" is the model trained without human knowledge; "2-2-2-2" means for each stage, two positive images, and two negative images are added to training set; similar to "6-4-4-4"; the current model is the one used in the system, using a 10-4-4-4 sampling strategy; the upper bound model trains all the data in one stage. The curves show that the quality of the extractors for different concepts can vary significantly. This is mainly caused by the latent representations we use, which we discuss in Section 8.2.</figDesc><graphic coords="9,65.15,49.25,193.70,210.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The precision curves for three concept extractors (standard objects, bobby cars, dogs) trained with two different deep embeddings: VGG-16 and FCN. The VGG-16 model is better trained than the FCN model and the concept extractors using deep embeddings from VGG-16 (dotted lines) outperform the ones from the FCN model (solid lines).</figDesc><graphic coords="9,316.31,49.25,250.94,154.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The baseline model shares the same network Lost cargo segmentation performance after training on data with shadow augmentation. *-B are baseline models trained without shadow augmentation and *-SA are models trained with shadow augmentations. Numbers in parentheses are standard deviations. The table shows IoU accuracy for each semantic segmentation class.</figDesc><table><row><cell>Model</cell><cell>road(%)</cell><cell cols="2">lost cargo(%) others(%)</cell></row><row><cell>DenseNet-FCN-B</cell><cell>75.8</cell><cell>50.6</cell><cell>95.4</cell></row><row><cell cols="3">DenseNet-FCN-SA 83.6 (0.1) 53.1 (0.5)</cell><cell>96.8 (0.0)</cell></row><row><cell>DeepLabV3+-B</cell><cell>82.8</cell><cell>57.4</cell><cell>96.8</cell></row><row><cell>DeepLabV3+-SA</cell><cell cols="2">82.8 (0.5) 58.6 (0.3)</cell><cell>96.8 (0.1)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The other common metric option could be cosine similarity.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
				<meeting>the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9525" to="9536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0130140</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2015-07">07 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Updates in human-ai teams: Understanding and addressing the performance/compatibility tradeoff</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33012429</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2429" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visualbackprop: Efficient visualization of cnns for autonomous driving</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Ackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zieba</surname></persName>
		</author>
		<idno>doi: 10.1109/ ICRA.2018.8461053</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4701" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">D3 data-driven documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.185</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2301" to="2309" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Human-centered tools for coping with imperfect algorithms during medical decision-making. CoRR, abs</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>ViÃ©gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902">1902.02960, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">This looks like that: deep learning for interpretable image recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<idno>abs/1806.10574</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retain: An interpretable predictive model for healthcare using reverse time attention mechanism</title>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>doi: 10. 1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Interpretable explanations of black boxes by meaningful perturbation. CoRR, abs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1704">1704.03296, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards automatic concept-based explanations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9273" to="9282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Flask web development: developing web applications with python</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grinberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ted: Teaching ai to explain its decisions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C F</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>MojsiloviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Natesan</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306618.3314273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;19</title>
				<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="123" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Population based augmentation: Efficient learning of augmentation policy schedules</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905">1905.05393, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual analytics in deep learning: An interrogative survey for the next frontiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno>doi: 10 .1109/TVCG.2018.2843369</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934659</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1096" to="1106" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Holstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>DaumÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1" to="16" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>doi: 10. 1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>ViÃ©gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.324</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Latent space cartography: Visual analysis of vector space embeddings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13672</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>doi: 10. 1109/CVPR.2015.7298965</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1705.07874</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpretable and steerable sequence learning via prototypes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330908</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;19</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="903" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">RISE: randomized input sampling for explanation of black-box models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petsiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1806.07421</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2016.7759186</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
				<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizations of Deep Neural Networks in Computer Vision: A Survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-54024-56</idno>
	</analytic>
	<monogr>
		<title level="m">Studies in Big Data</title>
				<meeting><address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="123" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-07">07 2010</date>
			<biblScope unit="page">52</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin, Madison</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<idno>doi: 10.5555/ 1613715.1613855</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1312.6034</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>1409.1556</idno>
		<imprint>
			<date type="published" when="2014-09">09 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">How can we fool lime and shap? adversarial attacks on post hoc explanation methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Slack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hilgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards an interpretable latent space : an intuitive comparison of autoencoders with variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Spinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>KÃ¶rner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>GÃ¶rtler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Visualization for AI Explainability 2018 (VISxAI)</title>
				<meeting>the Workshop on Visualization for AI Explainability 2018 (VISxAI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net. CoRR, abs/1412</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Latent space purification via neural density operators</title>
		<author>
			<persName><forename type="first">G</forename><surname>Torlai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.120.240503</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">240503</biblScope>
			<date type="published" when="2018-06">Jun 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quick shift and kernel methods for mode seeking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88693-852</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">10 2008</date>
			<biblScope unit="volume">5305</biblScope>
			<biblScope unit="page" from="705" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cost-effective active learning for deep image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2016.2589879</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
				<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
				<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning loss for active learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>doi: 10. 1007/978-3-319-10590-1 53</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, ECCV 2014 -13th European Conference, Proceedings</title>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interpretable convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00920</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Interpreting cnns via decision trees</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00642</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6254" to="6263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00068</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR 2018, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-12">Dec. 2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
	<note>Proceedings -2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.660</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
