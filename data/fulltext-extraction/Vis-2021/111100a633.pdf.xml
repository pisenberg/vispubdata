<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinqiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jaemin</forename><surname>Jo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yunhai</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D4800AAF9EAAEC897FDC977CFB4018FB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High-dimensional data</term>
					<term>projection</term>
					<term>embedding</term>
					<term>t-stochastic neighbor embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Joint t-Stochastic Neighbor Embedding (Joint t-SNE), a technique to generate comparable projections of multiple high-dimensional datasets. Although t-SNE has been widely employed to visualize high-dimensional datasets from various domains, it is limited to projecting a single dataset. When a series of high-dimensional datasets, such as datasets changing over time, is projected independently using t-SNE, misaligned layouts are obtained. Even items with identical features across datasets are projected to different locations, making the technique unsuitable for comparison tasks. To tackle this problem, we introduce edge similarity, which captures the similarities between two adjacent time frames based on the Graphlet Frequency Distribution (GFD). We then integrate a novel loss term into the t-SNE loss function, which we call vector constraints, to preserve the vectors between projected points across the projections, allowing these points to serve as visual landmarks for direct comparisons between projections. Using synthetic datasets whose ground-truth structures are known, we show that Joint t-SNE outperforms existing techniques, including Dynamic t-SNE, in terms of local coherence error, Kullback-Leibler divergence, and neighborhood preservation. We also showcase a real-world use case to visualize and compare the activation of different layers of a neural network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We aim to generate comparable t-Stochastic Neighbor Embedding (t-SNE) <ref type="bibr" target="#b27">[26]</ref> projections of a series of multidimensional datasets. Although multidimensional projection (MDP) techniques, such as t-SNE, play an essential role in high-dimensional data analysis by giving the users the ability to inspect high-dimensional data with lowerdimensional (e.g., 2D) representations, most of them are designed for visualizing a single high-dimensional dataset. Yet, a common data analysis task is to explore the evolution of a high-dimensional dataset, which requires comparing the projections between adjacent frames. One real-world example is a dataset consisting of the activation in each layer of a deep neural network. Given n input images, we record the internal representation of each image on layer i having f i dimensions, which yields a dataset D i consisting of n • f i scalar values. One may project each dataset D i and compare the projections to understand how each layer transforms the internal representations. Note that in this example, the number of items in datasets (i.e., n) does not change, but the number of dimensions (i.e., f i ) can vary.</p><p>Since most MDP techniques are designed to visualize one dataset, there have been a few workarounds to apply them to multiple datasets and use the results for comparison. The simplest workaround is to start with the same initial layout when optimizing the projection for each dataset. However, this straightforward method does not consider the relationship between adjacent time frames, and as the optimization process usually resorts to a stochastic process, the alignment between projections is not guaranteed. Thus, even the items that are invariant across the datasets can be placed in different locations, making the projections lack visual landmarks that can be used to facilitate comparison tasks <ref type="bibr" target="#b16">[15]</ref>.</p><p>Another workaround common in bioinformatics <ref type="bibr" target="#b15">[14]</ref> is to concatenate all datasets into a single dataset and compute the projection of the combined dataset. Then, the projection can be visualized as a conventional scatterplot with an additional visual encoding for the source of data points (i.e., the dataset from which a data point comes), for example, by adopting color-coding <ref type="bibr" target="#b0">[1]</ref> or by connecting the data points for the same item <ref type="bibr" target="#b3">[3]</ref>. However, this workaround does not scale well visually nor computationally since the number of data points to be shown increases by a factor of the number of the datasets, which leads to visual clutters and long computation time.</p><p>The most relevant technique to ours is Dynamic t-SNE <ref type="bibr" target="#b36">[35]</ref>. In this technique, the projections for multiple time frames are optimized together with an extra loss term that penalizes the length of the locus of each data point across projections. The additional loss term makes each data point stays in a similar position across projections. However, we identified three major drawbacks of Dynamic t-SNE as follows:</p><p>1. Smoothing effects: Dynamic t-SNE tends to keep projections too rigid, which leads to inaccurate projections when there are abrupt changes. For example, assume a point p lies inside a cluster A at time t but moves to another cluster B at t + 1. Although the local structure around p has been completely changed at t + 1, the extra loss term pulls back p to cluster A to minimize the locus of p between time t and t + 1, which gives users an illusion that p is gradually moving from cluster A to cluster B.</p><p>2. Long-range Interference: Since multiple projections are optimized together, the projection at t even reflects future changes that will happen after t and past changes happened before t, leading to a less faithful projection.</p><p>3. Monolithic: All datasets must be available when Dynamic t-SNE is performed, and this makes the technique unsuitable for datasets generated periodically. To compute the projection P t+1 for a new dataset at t + 1, all the previous projections (P 1 •••P t ) should be updated, which can invalidate the findings made previously and prevent incremental analysis.</p><p>To address these limitations, we present Joint t-SNE, a novel multidimensional projection technique that generates coherent projections of multiple high-dimensional datasets. The main idea of Joint t-SNE is to preserve the topology between projections selectively based on their topological similarity. To this end, we first capture the topological characteristics around each point by employing the Graphlet Frequency Distribution (GFD) <ref type="bibr" target="#b35">[34]</ref>. We then introduce a novel loss term, vector constraints, that guides the optimization process to preserve edge vectors between projected points across two-time frames. In Joint t-SNE, a projection at t + 1 is joined by the previous projection at t to preserve the edge vectors weighted by their topological similarity.</p><p>Through benchmarks on synthetic and real-world datasets, we found that Joint t-SNE can generate more consistent projections compared with the existing workarounds and Dynamic t-SNE. We also found that, compared with Dynamic t-SNE projections, Joint t-SNE projections are not only more faithful in terms of quality metrics, such as local coherence error or kNN preservation but also robust to distortion, such as the smoothing effect. Finally, Joint t-SNE breaks the global dependency between projections in the optimization process and naturally lends itself to dynamic scenarios where datasets arrive over time.</p><p>In summary, our contributions are:</p><p>• We introduce and implement Joint t-SNE that can generate consistent and faithful projections of multiple datasets by introducing kernel-based similarity measures and a novel loss term, vector constraints.</p><p>• We quantitatively and qualitatively evaluate Joint t-SNE and show that Joint t-SNE outperforms the existing practices and techniques in terms of visual consistency and projection fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we discuss previous studies targeting comparable projections and review the related work of a key technical module in Joint t-SNE, comparing graph similarity using Graphlet Frequency Distributions (GFD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Comparable Projections</head><p>A projection <ref type="bibr" target="#b31">[30]</ref> is a mapping that projects or embeds highdimensional data into a perceptible lower-dimensional space (usually two or three dimensions) while striving to maintain the relationships among data points in the original space. Formally, a projection algorithm takes in a high-dimensional dataset X = {x 1 , x 2 , ..., x n }, where each x i is an m-dimensional data point, and computes the projection of X in a lower-dimensional space, Y = {y 1 , y 2 , ..., y n }, where each y i is an injective mapping of x i . Projections are now widely used in various fields like data mining, machine learning <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b18">17]</ref>, and bioinformatics <ref type="bibr" target="#b8">[8]</ref> for the superiority in revealing the underlying distribution and topology of high-dimensional data, making it possible for people to make analyses and interpretations. Comparable projections, as extensions of traditional projections, deal with the problem of projecting a sequence of dynamic high-dimensional datasets X 1 , X 2 , ••• , X T , where each X i is captured at a particular time frame into a low-dimensional space, resulting in an equal-sized sequence of projections Y 1 ,Y 2 , ••• ,Y T . A simple method would be to project each X i independently. But due to the stochastic and unpredictable optimization process of many projection techniques, such method often introduces undesirable variations, such as misalignment of identical data points. Thus, the common goal of comparable projections is to achieve visual consistency between sequential projections while maintaining projection reliability. More specifically, we expect the algorithm to automatically find and highlight the inheritances and variations of data across different time frames to provide viewers with accessible observations into the evolving pattern of streaming data.</p><p>Previous attempts for comparable projections can be classified into two categories based upon the type of data evolution: incremental and time-dependent method. First, in the scenario for incremental projection methods, datasets update or expand by an incremental and cumulative pattern in each frame. For example, Alencar et al. <ref type="bibr" target="#b0">[1]</ref> proposed a technique based on least square projection and a backward strategy for creating content-based document maps to visualize temporal changes in document collections. Takanori et al. <ref type="bibr" target="#b14">[13]</ref> augmented an existing incremental PCA algorithm <ref type="bibr" target="#b37">[36]</ref> by applying an affine transformation to find the best overlap of common data points in two adjacent projections and presenting a position estimation algorithm to support adding data points with a non-uniform number of dimensions.</p><p>On the other hand, in contrast to incremental evolution, where previous data usually remain static, time-dependent projection methods deal with dynamic datasets in which the features of all data points can change over different time frames. With the providers of data points fixed in consecutive frames, this scenario can be found almost everywhere in today's information society, such as tracking the physical status of all patients in a hospital or the traffic volume of all crossroads in a city. Jäckle et al. <ref type="bibr" target="#b19">[18]</ref> proposed temporal MDS plots which reduce the multivariate data in each time frame to a 1D slice and align these slices along the time axis by adopting a flipping heuristic to achieve comparability. However, reducing the dimensionality to one discards too much information, making it nearly impossible for the users to analyze in-depth. Ali et al. <ref type="bibr" target="#b2">[2]</ref> treated data points in a frame as a highdimensional vector and projected the whole time-series dataset into a single image. Although such a method can provide users with instant discovery of anomalies and clusters, it is not scalable to datasets which contain hundreds and thousands of data sources, and could not be used in dynamically updated scenarios.</p><p>Dynamic t-SNE developed by Rauber et al. <ref type="bibr" target="#b36">[35]</ref> enhanced the conventional t-SNE by introducing an additional loss term, which penalizes the movement of all data points in between multiple projections, into the objective function of t-SNE and optimizing the position of all data points across projections at the same time. Although the purpose of maintaining visual consistency is achieved, distortions frequently occur due to its rigid constraints on the absolute position of every single point, as seen in Fig. <ref type="figure" target="#fig_0">1</ref>. And since dynamic t-SNE takes in the full sequence of datasets at once and optimizes all projections as a whole, it introduces a heavy computational burden and poses a significant challenge to the memory, thus not suitable for projecting streaming data.</p><p>Our Joint t-SNE is designed for time-dependent data evolution but can also be applied in incremental evolution scenarios with a simple modification as discussed in Sect. 6. By applying vector constraints to adjacent projections based on similarities measured in data space, we provide consistent projections suitable for comparison tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Measuring the Similarity between Graphs</head><p>Measuring the similarity between graphs, a subarea of graph mining, has gained popularity for many years for its significant value in various fields like chemistry <ref type="bibr" target="#b40">[39]</ref>, bioinformatics <ref type="bibr" target="#b13">[12]</ref>, and computer vision <ref type="bibr" target="#b45">[44]</ref>.</p><p>One straightforward approach to measure graph similarity is to compare node sequences by traversing the sequence of vertices or edges such as graph edit distance <ref type="bibr" target="#b7">[7]</ref>. These methods have low computational complexity and are easy to implement but can hardly capture the topological structure within the graph. A more complicated approach is to compare feature maps, which are specific distinguishable attributes extracted from graphs, like fingerprints to humans. Research has shown the remarkable performance of this method as a number of different graphs can be distinguished and identified accurately by referring to feature maps such as degree distribution <ref type="bibr" target="#b34">[33]</ref>, shortest paths <ref type="bibr" target="#b6">[6]</ref>, subtrees <ref type="bibr" target="#b23">[22]</ref>, and most importantly graphlets <ref type="bibr" target="#b35">[34]</ref>.</p><p>Graphlets, first proposed by Pržulj et al. <ref type="bibr" target="#b35">[34]</ref>, are defined as induced non-isomorphic k-node subgraph patterns, where k ∈ {3, 4, 5}. Graphlet Frequency Distribution (GFD), also known as graphlet concentrations or graphlet statistics, is a widely used feature for analyzing graphs in image category recognition <ref type="bibr" target="#b45">[44]</ref>, biological network comparison <ref type="bibr" target="#b17">[16]</ref>, disease gene identification <ref type="bibr" target="#b29">[28]</ref> and a host of other areas.</p><p>Based on statistics of elementary substructures, GFD is able to reflect the differences in the topological structure of graphs. Milenković and Pržulj <ref type="bibr" target="#b30">[29]</ref> generalized the concept of node degree to graphlet degree, which is defined as the number of graphlets connected to one node. By using the vector of graphlet degrees to represent and analyze protein function on protein-protein interaction (PPI) networks, it is proven to be a robust algorithm in comparing the topological structure of a local area. The graphlet spectrum <ref type="bibr" target="#b22">[21]</ref> presented by Kondor et al. applied a grouptheoretic approach to enhance the capability of graph kernels, enabling them to capture the relative positions of subgraphs besides numbers. Recently, Kwon et al. <ref type="bibr" target="#b24">[23]</ref> used a supervised machine learning method to learn the relationship between topological features of existing graphs measured by GFD and their layouts. The trained model could show the most topologically similar graphs in different layouts rapidly to provide users with a quick visual perception of the input graph.</p><p>However, one of the biggest challenges of using GFD in practice is the computational complexity of algorithms since the detection and enumeration of graphlets are costly computations. A multitude of work has been done to accelerate such computation. By adopting a Markov Chain Monte Carlo sampling method, Bhuiyan et al. <ref type="bibr" target="#b4">[4]</ref> proposed GUISE for approximating GFDs of large networks, which provided significant speedup compared with the brute-force counting method. In this work, we employ GFD and GUISE to capture the local topological structures around high-dimensional data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">t-Distributed Stochastic Neighbor Embedding</head><p>t-Distributed Stochastic Neighbor Embedding (t-SNE) <ref type="bibr" target="#b27">[26]</ref>, a commonly used nonlinear projection method, has been widely acknowledged for its outstanding performance in learning the underlying structure of high-dimensional data at different scales. The overall process of t-SNE can be divided into the following three stages:</p><p>Computing Probability Distribution of High-dimensional Data t-SNE starts by calculating the pairwise distance d(x i , x j ) (e.g., the Euclidean distance) of data points in high-dimensional space, resulting in a distance matrix, which is then converted into a probability distribution P using a Gaussian kernel to model the local structure around each data point. The conditional probability p j|i , which measures the pairwise similarity between point x i and x j , can be intuitively interpreted as the probability of data point x i selecting x j as its neighbor. A symmetrized joint probability p i j is computed and used to form distribution P.</p><formula xml:id="formula_0">p j|i = exp −d x i , x j 2 /2σ 2 i ∑ k =i exp −d (x i , x k ) 2 /2σ 2 i , p i|i = 0. (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">p i j = p j|i + p i| j 2N . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Computing Probability Distribution of Projected Points In the low-dimensional space, t-SNE first randomly initializes the projection and then calculate the joint probability distribution Q from pairwise distance matrix, same as it did in high-dimensional space, only this time using a heavy-tailed Student's t-distribution.</p><formula xml:id="formula_4">q i j = 1 + y i − y j 2 −1 ∑ k =l 1 + y k − y l 2 −1 , q ii = 0. (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>Minimizing Mismatch between the Two Distributions Finally, t-SNE minimizes the Kullback-Leibler divergence between distribution P and Q to conserve local structures between high and low-dimensional spaces, using a gradient descent method that updates the position of each projected point iteratively.</p><formula xml:id="formula_6">C = KL(P Q) = ∑ i = j p i j log p i j q i j (4) ∂C ∂ y i = 4 ∑ i = j (p i j − q i j )(y i − y j )(1 + y i − y j 2 ) −1 (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>Dynamic t-SNE To adapt t-SNE for comparable projections, Dynamic t-SNE chooses to penalize the overall movement of each point across projections by adding an extra loss term that restricts projected points to stay in the same position as much as possible. Formally, for a series of high-dimensional datasets</p><formula xml:id="formula_8">X 1 , X 2 , ••• , X T , the position of each point y t i , where t ∈ [1, T ], in the corresponding projections Y 1 ,Y 2 , ••• ,Y T is computed by minimizing the function below: C = T ∑ t=1 C t + λ 2N N ∑ i=1 T −1 ∑ t=1 y t i − y t+1 i 2</formula><p>where C t is the t-SNE cost of projecting X t to Y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graphlet Kernels</head><p>Graphlets are non-isomorphic substructures in graphs with k nodes, typically k ∈ 3, 4, 5 (see Fig. <ref type="figure" target="#fig_1">2</ref>). Any large graph could be disassembled to these basic structures, thus the frequencies of graphlets could be used as a fingerprint to differentiate various graphs. Such fingerprint is defined as Graphlet Frequency Distribution (GFD) and is widely used to characterize and compare the structure of different graphs.</p><p>While GFD is an effective method, counting graphlets is a computationally expensive task, especially on large graphs. The time complexity for enumerating k-node graphlets exhaustively on a graph G(V, E) is O(|V | k ), which forces sampling methods to be introduced into the pipeline.</p><p>One work towards the uniform sampling of graphlets is GUISE <ref type="bibr" target="#b4">[4]</ref>, an efficient algorithm that uses a Markov Chain Monte Carlo (MCMC) method to approximate GFD for large graphs. Formally, given a graph G, assume S G is a set that contains all the graphlets in G. The key to uniform sampling is to ensure the probability of selecting each one of the graphlets g i in S G is 1/|S G |. The MCMC algorithm implemented in GUISE performs a random walk on sample space S G with a transition probability matrix in such a manner that the stationary distribution of the random walk aligns with the independent and identical distribution.</p><p>Graphlet Kernels Graphlet kernels are adaptations of kernel methods for measuring graph similarities. A kernel, also known as generalized dot product, is a function that measures the similarity between two vectors, x, y ∈ R m , by computing the dot product of their counterparts, ϕ(x), ϕ(y) ∈ R n , in a high-dimensional feature space, where ϕ is a mapping that brings x and y into the feature space R n .</p><p>Formally, a graphlet kernel k g which measures similarity between graph G 0 and G 1 is defined as the dot product of their feature vectors.</p><formula xml:id="formula_9">k g (G 0 , G 1 ) =&lt; f G 0 , f G 1 &gt;</formula><p>where feature vector f is usually based on normalized GFD. Frequently used kernels include Gaussian kernel, Laplacian kernel and cosine similarity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">JOINT t-STOCHASTIC NEIGHBOR EMBEDDING</head><p>In this section, we elaborate on Joint t-SNE. We first investigate the design considerations and introduce the basic idea of our solution. Then we give an overview of the algorithm pipeline followed by a detailed description of two technical highlights of Joint t-SNE, graphlet kernel-based edge similarity and edge vector constrained t-SNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design Considerations</head><p>The aim of Joint t-SNE is to generate low-dimensional embeddings that can provide users with consistent visual tracking of data evolution in the high-dimensional space. For most time-series datasets, there are both inheritances and variations of data points in each time frame during the evolution. We expect those inherited structures to be preserved in the projection space to serve as visual landmarks, whereas the varied ones to be distinguished from the others. Therefore, the problem of generating comparable projections can be reduced to realizing the following three goals:</p><p>1. G1: Detecting changes of local structures happening between time frames in data space, 2. G2: Preserving inherited structures in projection space, and 3. G3: Ensuring projection fidelity.</p><p>Note that G1 is the basis of G2, and G2 may be conflicted with G3 since distortions may be introduced.</p><p>Conventional dimensionality reduction methods such as PCA and t-SNE are designed for projecting a single dataset, with G3 as their only concern. When used for projecting datasets from multiple time frames, such methods would generate misaligned layouts. A common idea in dealing with time-series data is the sliding window mechanism. The term "window" refers to an imaginary box holding a batch of data from consecutive time frames. A window of length l takes in and processes data from l frames simultaneously and slides to the next position after finishing the process of current data inside. We adopt such mechanism and set l = 2 to acquire datasets from adjacent time frames, X 0 and X 1 , at a time, as well as to avoid the effect of long-range interference. Using one dataset, say X 0 , whose projection Y 0 has already been computed as a reference, we project X 1 , taking both G2 and G3 into consideration, resulting in Y 1 . Note that we will state this process as "Y 1 is joined by Y 0 ."</p><p>To meet G2, the major challenge is how to incorporate the DR results of the previous frames into the next frames. Dynamic t-SNE uses the absolute positions of the previous frames, which cannot effectively characterize the change of local structures. Recent work <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b43">42]</ref> has shown that edge-vector-based constraints outperform previous methods in preserving local structures. Inspired by that, We develop vector constraints, which try to keep the relative position of point-pairs, and integrate such constraint to the loss function of t-SNE. The goal is to preserve structures that are inherited in data space and make their appearances in the later projection the same as in the former. To this end, we set the weight of each vector as the edge similarity, which measures the variation of the corresponding edge between high-dimensional data points, to guide the optimization process.</p><p>In order to detect local changes and further measure edge similarities, we first construct a kNN graph in high-dimensional space for each dataset to model the neighborhood relationship among data points. Then we measure the similarity of the local subgraph around each node between two graphs with the identity of nodes taken into account. To this end, we use a graphlet kernel to measure such similarity, resulting in one value for each point which represents how much its surrounding topology, as well as neighbors, changed from the former time frame to the latter. Note that high edge similarity indicates minor changes in local structure, while low similarity reflects major changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithm Pipeline</head><p>Given two datasets X 0 and X 1 , the computation pipeline (see Fig. <ref type="figure" target="#fig_2">3</ref>) of Joint t-SNE is as follows:</p><p>1. We construct G 0 and G 1 , the undirected k-Nearest Neighbor (kNN) graphs of X 0 and X 1 , respectively. 2. We compute the Graphlet Frequency Distribution(GFD) vector of local graph structure around each node in G 0 and G 1 ,</p><formula xml:id="formula_10">f v 0 i for v 0 i ∈ V (G 0 ) and f v 1 i for v 1 i ∈ V (G 1 )</formula><p>, respectively (see Fig. <ref type="figure" target="#fig_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">For each node pair</head><formula xml:id="formula_11">(v 0 i , v 1 i ) ∈ {(v 0 i , v 1 i ) | v 0 i ∈ V (G 0 ), v 1 i ∈ V (G 1</formula><p>)}, we compute its point similarity (Eq. 6) using the feature vectors of v 0 i and v 1 i . 4. Then for each edge pair common in both graphs, i.e., (e 0 i j , e 1 i j ) ∈ {(e 0 i j , e 1 i j )|e</p><formula xml:id="formula_12">0 i j = (v 0 i , v 0 j ) ∈ E(G 0 ), e 1 i j = (v 1 i , v 1 j ) ∈ E(G 1</formula><p>)}, we compute its edge similarity (Eq. 7) using the point similarity of its two endpoints. 5. We compute Y 0 , the t-SNE projection of X 0 , using normal t-SNE (Sect. 3.1). Note that this is the case at the very beginning; when the sliding window moves on to subsequent time frames, the latest computed projection would be taken as the new Y 0 directly. 6. We compute Y 1 , the t-SNE projection of X 1 , joined by Y 0 , incorporating vector constraints of common edges between G 0 and G 1 (Sect. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graphlet Kernel based Edge Similarity</head><p>Given a hyperparameter k, which indicates the number of neighbors to consider, Joint t-SNE starts with constructing G 0 and G 1 , the kNN graphs of two datasets X 0 and X 1 , respectively, using a distance metric (e.g., Euclidean). We consider the graphs to be undirected, i.e., an edge e i j is present if and only if v i ∈ kNN(v j ) or v j ∈ kNN(v i ). Note that hereafter we use the two graphs for computing point similarities instead of the two raw datasets, which allows us to correlate datasets with different numbers of dimensions. Then, we introduce the feature vector of a node v, which characterizes the local structure around v in a kNN graph. Graphlets consist of 29 unique connected structures of 3 to 5 nodes, and the frequency distribution of graphlets (GFD) can be used as a measurement of graph topology. We count the number of each structure in the kNN graph that includes v, normalize the counts by dividing each count by the sum, and use the normalized counts as the feature vector f v of v.</p><p>Since counting all graphlets is computationally expensive, especially when k is large, we embed the sampler, the core module of GUISE, into Joint t-SNE and develop an algorithm that computes the feature vectors of nodes using uniformly sampled graphlets from a global scale. The pseudo-code of both the original GUISE algorithm and our modified version is included in our supplementary material.</p><p>To capture the structural similarity between two graphs, we present two types of similarity, point similarity and edge similarity. The point similarity between v 0 i and v 1 i is defined as the cosine similarity between their feature vectors multiplied by the fraction of common neighbors between v 0 i and v 1 i . Since we assumed the two datasets have the same items, we can compute the point similarity of every item i between the two datasets.</p><formula xml:id="formula_13">S p i = |kNN(G 0 , v 0 i ) ∩ kNN(G 1 , v 1 i )| k • &lt; f v 0 i , f v 1 i &gt; (6)</formula><p>Although the cosine similarity of feature vectors captures the structural similarity between the neighbors of two nodes, it does not consider their identity. For example, suppose an item i lies within cluster A in G 0 but moves to cluster B in G 1 . Even if the two clusters have identical structures, the point similarity between v 0 i and v 1 i should be zero since they do not have common neighbors. Therefore, we penalize the cosine similarity by multiplying the fraction of common kNN items between v 0 i and v 1 i . Fig. <ref type="figure" target="#fig_3">4</ref> gives an example of computing point similarity. The edge similarity between e 0 i j and e 1 i j is defined as the multiplication of the point similarities between their two endpoints: S e i j = S p i • S p j <ref type="bibr" target="#b7">(7)</ref> Note that edge similarities can be computed only for the common edges that exist in both G 0 and G 1 while point similarities can be computed for all items.</p><p>During projecting the datasets separately, we will try to keep the edge vectors between projected points with high edge similarities in the data space invariant across projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Edge Vector Constrained t-SNE</head><p>Given the two datasets, X 0 and X 1 , we project X 0 using the conventional t-SNE and get Y 0 , the projection of X 0 in a low-dimensional space, that minimizes the Kullback-Leibler divergence between a joint probability distribution in the high-dimensional space, p 0 i j , and a joint probability distribution in the low-dimensional space, q 0 i j as follows:</p><p>arg min</p><formula xml:id="formula_14">Y 0 C 0 = ∑ i = j p 0 i j log p 0 i j q 0 i j (8)</formula><p>For X 1 , Joint t-SNE generates a coherent projection Y 1 joined by Y 0 , preserving the topological structures of items that possess high similarity in data space between Y 0 and Y 1 . To this end, we introduced a novel constraint to the cost function of t-SNE: vector constraints. The cost function for X 1 is as follows: arg min</p><formula xml:id="formula_15">Y 1 C 1 = ∑ i = j p 1 i j log p 1 i j q 1 i j + γ M ∑ i = j S e i j • (y 0 i − y 0 j ) − (y 1 i − y 1 j ) 2<label>(9)</label></formula><p>where S e i j is the similarities of common edges between graph G 0 and G 1 constructed from X 0 and X 1 respectively, M is the number of those common edges, γ is the weight for vector constraints set by users. The gradient is computed as follows:</p><formula xml:id="formula_16">∂C 1 ∂ y 1 i = 4 ∑ i = j (p 1 i j − q 1 i j )(y 1 i − y 1 j )(1 + y 1 i − y 1 j 2 ) −1 − 2γ M ∑ i = j S e i j ((y 0 i − y 0 j ) − (y 1 i − y 1 j ))<label>(10)</label></formula><p>For each common edge (e 0 i j , e 1 i j ) ∈ (G 0 , G 1 ), edge constraints penalize Y 1 if the two vectors calculated by endpoints of common edges are distant in the Euclidean space, i.e., aim to maintain the relative positions between points. Vector constraints are weighted by a hyperparameter γ. The impacts of the hyperparameter are demonstrated in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>In this section, we quantitatively and qualitatively compare four techniques for generating comparable projections, the original t-SNE <ref type="bibr" target="#b27">[26]</ref> (OT), Equal-initialization t-SNE (ET), Dynamic t-SNE <ref type="bibr" target="#b36">[35]</ref> (DT), and our Joint t-SNE (JT). OT served as a baseline; we projected each dataset D i individually using the original t-SNE algorithm with a random initial layout. For ET, we applied the original t-SNE algorithm to each dataset individually but used the same initial layout, which was generated randomly. For DT, we used the implementation from the original paper <ref type="bibr" target="#b36">[35]</ref>. We describe the hyperparameter settings used for evaluation in the supplementary materials.</p><p>JT requires two hyperparameters, γ and k, while DT requires λ . k was set to 3 in all experiments in our paper and supplementary materials. Since both loss terms of JT and DT are the averaged displacements of edges or points, we set γ = λ for direct comparison between JT and DT. In our paper, they were set to 0.1 by default which was the value used in a previous experiment <ref type="bibr" target="#b36">[35]</ref>.</p><p>To compare the techniques quantitatively, we used the following three metrics:</p><p>1. Local Coherence Error (LCE) of specific clusters between frames: We use LCE to quantify the visual consistency of local structures across consecutive projections. LCE is defined as the sum of Euclidean distances between every two edge vectors in clusters C k in projections Y 0 and Y 1 , whose ground-truth topology is known to be unchanged.</p><formula xml:id="formula_17">LCE(Y 0 ,Y 1 ) = ∑ C 0 k ⊂Y 0 C 1 k ⊂Y 1 ∑ y 0 i ,y 0 j ∈C 0 k y 1 i ,y 1 j ∈C 1 k i&lt; j (y 0 i − y 0 j ) − (y 1 i − y 1 j ) 2 (11)</formula><p>2. kNN Preservation (KNN) for a single frame: We use KNN to measure the preservation of local structures when conducting dimensionality reduction following a previous paper <ref type="bibr" target="#b21">[20]</ref>. KNN is defined as the proportion of k-nearest neighbors of points in the original high-dimensional space X that are preserved in the projection space X pr j .</p><formula xml:id="formula_18">KNN(X) = |kNN(X) ∩ kNN(X pr j )| |kNN(X)| (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>3. KL Divergence (KLD) for a single frame: KLD measures the faithfulness of modeling between high-and low-dimensional spaces. See function 4. Although these metrics are useful in measuring how well projections preserve local structures, they are not enough to understand the fidelity and consistency between projections for different frames. To understand them, we also conduct the qualitative evaluation by investigating projections before and after a certain data transformation. Designing the experiments, we identified that there is a general trade-off between the internal and external validity in our evaluation. For example, if we experiment on the techniques using synthetic high-dimensional data and transformations whose ground-truth meanings are known (e.g., changing the distance between two hypothetical spheres), we can accurately assess the faithfulness and consistency between projections, but the external validity is hurt. Otherwise, if we use two real-world datasets, which are externally valid, we cannot judge whether the projections preserve the structures between the frames faithfully since it is rare to have the ground truth on the topological changes happening between the two real-world datasets.</p><p>To balance the trade-off, we conduct three experiments with different levels of abstraction. In the first experiment (Sect. 5.1), we use two synthetic datasets that samples from random Gaussian clusters: the 10-Gaussian dataset <ref type="bibr" target="#b36">[35]</ref> and the 5-Gaussian dataset. To simulate time-dependent changes, we perturb the datasets by applying known data transformations on clusters, such as translation, split, and overlapping. This design is the most internally valid; we can assess the faithfulness and consistency of projections since the structures of datasets, and the meaning of data transformations are known. To seek the balance between internal and external validity, in the second experiment (Sect. 5.2), we use a real-world dataset, the MNIST dataset, with synthetic data transformations, i.e., replacing digits in images (e.g., "1" to "3"). Finally, in the third experiment (Sect. 5.3), we apply the four techniques to a real-world dataset, the activation tensors from the VGG-16 network <ref type="bibr" target="#b39">[38]</ref>, i.e., the VGG dataset, to understand how high-dimensional representations are transformed on each layer. In this experiment, even the meaning of the transformation that happens between two adjacent datasets is unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Joint t-SNE on Synthetic Datasets</head><p>The goal of the first experiment was to assess the faithfulness and consistency of the four projection techniques when applied to generate comparable projections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Transformation</head><p>We first employed the 10-Gaussian dataset that was used to evaluate Dynamic t-SNE <ref type="bibr" target="#b36">[35]</ref>. The dataset consisted of 2,000 points in a 100-dimensional space that were sampled from ten isotropic Gaussian distributions with a variance of 0.1. The centers of the distributions were chosen randomly between the standard basis vectors for R 100 , i.e., the centers were equidistant from each other.</p><p>The authors of the Dynamic t-SNE paper simulated time-dependent changes by contracting each cluster stepwisely. Specifically, they moved each data point towards the center of the corresponding Gaussian distribution by 10% of the remaining distance to the center. They repeated this contraction operation nine times to generate nine frames (i.e., t = 1 •••9), obtaining the final datasets of ten frames.</p><p>To further experiment on richer types of transformations in addition to the contraction operation, we generated a dataset, the 5-Gaussian dataset. We started by sampling 500 points from five isotropic Gaussian distributions (100 for each) in a 100-dimensional space. The centers of the distributions were chosen randomly between the standard basis vectors with a variance of 0.05 (the first row of Fig. <ref type="figure" target="#fig_4">5</ref>).</p><p>We then generated three more frames (t = 1, 2, 3) by applying one of the following transformations: translation, splitting, and overlapping, and before applying each transformation, we contracted all clusters by 10% as in the 10-Gaussian dataset. At t = 1, we translated the points in the first cluster (the blue cluster in Fig. <ref type="figure" target="#fig_4">5</ref>) by +0.15 in all dimensions (i.e., 3 • variance). At t = 2, we split the second cluster (the green cluster in Fig. <ref type="figure" target="#fig_4">5</ref>) by half. We used the k-means clustering algorithm to make two subclusters (k = 2) and translate the subclusters either by ±0.15 (i.e., ±1.5 • variance) in all dimensions to the opposite direction. At t = 3, we overlapped the third and fourth clusters (the red and purple clusters in Fig. <ref type="figure" target="#fig_4">5</ref>) by translating them to have the same mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Since the 5-Gaussian dataset illustrates a richer set of data transformations, we report the projection results of the 5-Gaussian dataset in Fig. <ref type="figure" target="#fig_4">5</ref>. the result for the 10-Gaussian dataset can be found in the supplementary materials. In Fig. <ref type="figure" target="#fig_4">5</ref>, note that for ease of comparison, we used the Dynamic t-SNE projection at t = 0 as the common projection for all four methods even though it suffered from the long-range interference problem; the blue cluster is already distant from the other clusters even at t = 0 since it will become distant.</p><p>At t = 1, a cluster (i.e., the blue cluster) was pushed away from the other clusters. All the projection results manifested this transformation by placing the cluster distantly from the other clusters, but the original t-SNE produced a misaligned projection; the blue cluster was near the bottom-right corner at t = 0, but it moved to the top at t = 1. Note that this misalignment cannot be fixed by a simple Procrustes transform; one may want to rotate the projection at t = 1 to align the position of the blue cluster with its previous position, but then the red and green clusters will become misaligned. In contrast, DT and JT generated more consistent projections; they preserved the positions of the four untouched clusters. At t = 2, a cluster (i.e., the green cluster) was split into two subclusters. We could see that the separation between the subclusters was relatively small in DT compared with the other three techniques due to the smoothing effect; the two clusters were less separated to minimize the movement of points. The smoothing effect of DT became clearer at t = 3. Here, we overlapped two clusters (i.e., the red and purple clusters) completely, but they overlapped each other only partially in DT. In contrast, JT alleviated the smoothing effect by considering the topological similarity of points between two datasets; the two clusters could be placed faithfully since the neighborhood of points in the clusters was drastically changed, resulting in small edge similarity.</p><p>In terms of Local Coherence Error, JT outperformed the other three techniques (Table <ref type="table" target="#tab_1">1</ref>) for both the 10-Gaussian and 5-Gaussian datasets, suggesting that JT produced more consistent projections. Regarding projection fidelity (Table <ref type="table" target="#tab_2">2</ref>), JT outperformed other techniques in terms of kNN preservation, but it produced higher KL divergence than OT and ET. This is because JT considered an extra loss term (i.e., vector constraints) in addition to KL divergence to maintain consistency.</p><p>Additionally, we investigated the effect of hyperparameters, γ for JT and λ for DT, by testing three values, 0.01, 0.05, and 0.1 (Fig. <ref type="figure">6</ref>). Note that we could not test values larger than 0.1 since the optimization process of DT became unstable and collapsed. This time, we applied the four transformations to the 5-Gaussian dataset at the same time (t = 1) for comparison. Fig. <ref type="figure">6</ref> shows that JT was more robust to distortion, such as the smoothing effect, regardless of λ (see the overlap of the red and purple clusters) while DT was sensitive to its hyperparameter λ ; for example, the smoothing effect between the red and purple clusters became more severe as we increase λ .</p><p>We also investigated the effect of hyperparameter k on the final embeddings with the 5-Gaussian dataset in two aspects, LCE and computational time. The result shows that LCE becomes high both when k is very small (2) and very large (|nodes|/2), see the inset. This is because small k will lead to an insufficient number of edges in kNN graphs for modeling the relationship between nodes effectively, whereas a large k might make GFD becomes ineffective for measuring node similarities. As for computational time, it increases with the increase of k. Therefore, we recommend that users set k to 3, 4, or 5 to achieve a better trade-off between fidelity and efficiency.</p><p>Experiment 1 showed that compared with DT, JT was more con-Fig. <ref type="figure">6</ref>. Comparison of the effect of hyperparameters in Dynamic and Joint t-SNE. We used the result from Dynamic t-SNE as the projection of the first time frame for ease of comparison. For Dynamic t-SNE, as λ increases from 0.01 to 0.1, the effect of long-range interference at t = 0 becomes more obvious as the red and purple clusters gets closer, which is a future change. The same is true for smoothing effect at t = 1. Joint t-SNE is robust to the change of γ; see the red and purple clusters fully overlap regardless of γ.</p><p>sistent and faithful for the tested dataset in terms of both qualitative and quantitative evaluation. Compared with OT and ET, JT was more consistent and had comparable fidelity. These findings suggest that JT achieves all three design goals mentioned in Sect. 4.1, and solved the conflict between G2 and G3 elegantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Joint t-SNE on the MNIST Dataset</head><p>The goal of the second experiment was to assess the consistency of projections generated on a real-world dataset with synthetic transformations.</p><p>Datasets and Transformation We used the MNIST dataset <ref type="bibr" target="#b25">[24]</ref> which consisted of the 60,000 black-and-white images of hand-written digits. As the initial dataset at t = 0, we collected the first 100 images of five digits (i.e., <ref type="bibr">[0,</ref><ref type="bibr" target="#b4">4]</ref>) from the MNIST dataset. In contrast to the Gaussian datasets in the previous experiment, mutating the images by adding or subtracting a constant to each pixel was not meaningful. Instead, we applied two replacement transformations at t = 1 with different levels of resulting similarity. First, we replaced the images of digit 0 with those of digit 9; we used the first 100 images of digit 9. We hypothesized that the existing cluster of digit 4 and the replaced cluster of digit 9 should become closer since the two digits were similar. Second, we replaced the images for digit 1 with those of digit 3; we used 100 images of digit 3 that were different from the images of digit 3 at t = 1. We hypothesized the two clusters should overlap completely at t = 1 since they depicted the same digit, 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>We show the projection results of ET, DT, and JT at t = 0 and t = 1 in Fig. <ref type="figure">7</ref>. First, ET produced the projections that supported our hypotheses; the clusters for digits 0 and 4 (the blue and brown clusters) became closer as digit 0 was replaced with digit 9 at t = 1. Furthermore, the clusters for digits 1 and 3 (the green and purple clusters) were distant at t = 0 but overlapped at t = 1 after digit 1 (the green cluster) was replaced with digit 3. However, ET altered even an untouched cluster, the red cluster for digit 2, possibly giving the user misunderstanding that its local structure also changed.</p><p>The results from DT suffered from long-range interference and smoothing effects that we mentioned. At t = 0, the clusters for digits 0 and 4 (the blue and brown clusters) were close to each other even before the transformation took place. Because they would become similar at t = 1, DT placed them nearby to minimize the length of the locus of points. The projection was distorted early due to the changes happening Fig. <ref type="figure">7</ref>. Comparison of the MNIST dataset projection of three t-SNE techniques. Note that since the effect of long-range interference of Dynamic t-SNE was too serious in this case, we did not use its result for t = 0 as the initial projection for other methods as we did in other cases. a) Projecting each frame separately in Equal-initialization t-SNE could faithfully reveal the underlying structure. But when used for comparison tasks, the results can be misleading; for example, people can think that the red cluster for digit 2 changed and the purple cluster of digit 3 moved to the green cluster of digit 1, but the ground truth is the opposite. b) In Dynamic t-SNE, the changes between two frames are subtle even though there were substantial changes. These projections are too consistent, sacrificing fidelity. c) Joint t-SNE successfully detected the changes in topology while preserving the local structure of the red cluster for digit 2 where no change was made. d) Comparison of local subspace shows that Dynamic t-SNE created an artifact of digit 2 on the right side, whereas other methods did not.</p><p>in the future, i.e., long-range interference. The two clusters for digit 3 at t = 1 exhibited the smoothing effect; they should fully overlap each other but were separated because the points were forced to move minimally, maintaining their previous positions at t = 0. Due to these problems, DT generated very consistent but unfaithful projections.</p><p>In contrast, JT successfully displayed changes with a consistent layout. The two changes we made were clearly visible at t = 1; two clusters for digit 3 (the green and purple clusters) fully overlapped while the clusters for digits 4 and 9 (the brown and blue clusters) partially overlapped. Note that JT also preserved the local structure of the cluster of digit 2 (the red cluster) where no change was made, allowing the user to perform comparison tasks using that cluster as visual landmarks.</p><p>Experiment 2 showed that JT could capture the similarity of local structures between two frames. For similar structures (e.g., the red cluster of digit 2), JT placed the structures consistently in the projection space. Even for the structures that changed abruptly (e.g., the overlapped clusters of digit 3), it could project them faithfully. Note that maintaining the consistency can lower local quality metrics (Table <ref type="table" target="#tab_2">2</ref>) compared with unbound techniques such OT, but we believe the benefit of having consistent projections for comparison tasks can pay the cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Joint t-SNE on the VGG Dataset</head><p>As a real-world use case, we tested the four techniques on the VGG dataset <ref type="bibr" target="#b39">[38]</ref>. We investigated how each layer in the VGG network transforms intermediate representations of images by projecting and comparing the activation. Note that in contrast to previous experiments, it is very difficult to have ground-truth interpretation on the transformation happening on a layer due to the high dimensionality and complexity of the dataset and network.</p><p>Dataset As input images, we collected 700 images of ten classes (70 images for each class) from ImageNet <ref type="bibr" target="#b12">[11]</ref>. The ten classes consisted of eight animal species and two unrelated classes, which were cats (tiger cat and tabby cat), dogs (giant schnauzer and standard schnauzer), sharks (great white shark and tiger shark), fish (goldfinch and brambling), beach wagon, and military uniform.</p><p>The VGG-16 network has 23 layers where the first layer represents a raw input image, and the last layer corresponds to output class probabilities. Given the input images, the activation recorded on each layer forms a single high-dimensional dataset whose rows represent input images and columns represent the activation at each neuron. The datasets had different numbers of columns ranging from 1,000 to 3,497,984. For datasets that had more than 4,096 dimensions, we used random projection <ref type="bibr" target="#b26">[25]</ref> to reduce the dimensionality to 4,096.</p><p>Results and Discussion Fig. <ref type="figure" target="#fig_5">8</ref> shows the evolution of activation on the last four layers, excluding the final softmax prediction layer in the VGG-16 network, projected by four different techniques. Overall, the separation between classes became clear at the first fully connected layer (fc1). In OT and ET, the positions of clusters changed drastically; for example, ET placed the clusters of birds (the red and pink clusters) on the left part of the projection at block5-conv, but the clusters moved to the lower part at fc1 and finally to the lower-right part at fc2. Although it is known that the absolute position of the cluster is not important for t-SNE <ref type="bibr" target="#b44">[43]</ref>, the moving cluster makes the comparison between layers more challenging.</p><p>DT manifested several problems. First, the two classes of birds (the red and pink clusters) were largely separated at the fc2 layer while other techniques placed them nearby. This phenomenon can be seen as an example of the smoothing effect; because the two clusters were distant previously (i.e., at block5-conv), placing the two clusters closer faithfully would cause a substantial loss. A similar problem happened for the two classes of dogs (the green and light green clusters) at fc1. A small subcluster of tiger cats (the light blue cluster) in a lower part remained separated, possibly causing misunderstanding that this subcluster is significantly different from other clusters for cats. Finally, we found that DT did not work on the full dataset due to a scalability issue while other techniques did since it tried to load all datasets in memory to optimize them together.</p><p>In contrast, JT generated projections that are locally similar to OT or ET projections but with consistent global structures. For example, two clusters for sharks (the purple and light purple clusters) became separated at fc1, which was visible in all three techniques. However, OT and ET flipped the two clusters vertically between fc1 and fc2, which did not happen in Joint t-SNE projections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">GENERAL DISCUSSION</head><p>Benefits of Joint t-SNE Through a series of experiments, we could identify the benefits of Joint t-SNE as follows: 1) Joint t-SNE provides visual consistency between projections. The vector constraints we added guide the optimization process to place invariant structures to consistent locations. Note that keeping projections consistent with the vector constraints naturally induces a loss in KL divergence (Table <ref type="table" target="#tab_2">2</ref>), but the loss was small compared to its benefits and can be controlled by a hyperparameter λ . 2) Joint t-SNE is more faithful than Dynamic t-SNE. Even the high-dimensional topology changes abruptly, the GFDbased edge similarity we proposed captures the changes and allows the optimization process to faithfully project the topology. 3) Joint t-SNE is more flexible than Dynamic t-SNE since generating a projection at t only depends on the previous one t − 1 and does not change the previous projection. It is more suitable for dynamic datasets where new time frames are added incrementally and causes a low computational burden since only two adjacent projections reside in memory at a time.</p><p>Analysis of Algorithmic Complexity Our implementation of Joint t-SNE mainly consists of three major parts: graphlet construction, graphlet-based similarity computation, and Joint t-SNE optimization. For a dataset with n nodes and k neighbors of each node are considered, suppose the number of graphlets is m, the time complexities of three parts are O(km), O(n), and O(n 2 ), respectively. In total, the time complexity of Joint t-SNE is O(n 2 + km). When the graph is dense, m might be much larger than n 2 , and the complexity is O(n 2 ) for sparse graphs. We report the actual running time of the three methods in the supplementary material.</p><p>Extending to t-SNE Variants and Other Projection Techniques Since vector constraints do not change the inner workings of a projection technique, they can be integrated into accelerated t-SNE algorithms (e.g., Barnes-Hut t-SNE <ref type="bibr" target="#b41">[40]</ref>), variants that use t-SNE internally (e.g., Hierarchical SNE <ref type="bibr" target="#b32">[31]</ref> or progressive t-SNE algorithms <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b33">32]</ref>), or even other gradient-based projection algorithms.</p><p>One of the most promising extensions is the use of Barnes-Hut t-SNE <ref type="bibr" target="#b41">[40]</ref> that can improve the time complexity of Joint t-SNE from O(n 2 ) to O(nlogn). Barnes-Hut t-SNE internally builds kD trees to identify the k BH neighbors of each point. Note that k, the number of neighbors used for computing feature vectors in our work, and k BH , the number of neighbors for the approximation in Barnes-Hut t-SNE, have different meanings and scales. For k BH , the original paper <ref type="bibr" target="#b41">[40]</ref> suggested a number 3 • perplexity which is about 50 times larger than the minimum value we suggest for k (i.e., k = 3) assuming perplexity is set to 50 as in common settings. One benefit of having smaller k than k BH is that we can reuse the kNN graph built for the Barnes-Hut approximation to reduce the overhead of introducing vector constraints.</p><p>Although Joint t-SNE is based on t-SNE, our concepts, measuring graph similarities and applying vector constraints, can be easily adapted to other projection techniques, such as Multidimensional Scaling (MDS) <ref type="bibr" target="#b9">[9]</ref> or Uniform Manifold Approximation and Projection (UMAP) <ref type="bibr" target="#b28">[27]</ref>. For MDS, we can directly add the vector constraints to the loss function; see the supplementary materials for an example. UMAP constructs a weighted directed kNN graph Ḡ and converts it to symmetric adjacency matrix B. In this case, for GFD computation, we need to sample edges with probabilities proportional to their weights in B in the random walk as in a previous framework <ref type="bibr" target="#b38">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work</head><p>The computation pipeline of Joint t-SNE can be further improved by using approximation <ref type="bibr" target="#b41">[40]</ref> or exploiting loop-based parallelism <ref type="bibr" target="#b11">[10]</ref> although we used a single-threaded version in the experiments. We can also consider an estimation framework <ref type="bibr" target="#b38">[37]</ref> to speed up the graphlet counting process. We leave accelerating Joint t-SNE as future work. Another interesting future work would be to integrate vector constraints in hierarchical projection techniques, such as Hierarchical SNE <ref type="bibr" target="#b32">[31]</ref>, to speed up the computation and ensure the coherency between the projections at different levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We present Joint t-SNE to generate comparable projections of multiple high-dimensional datasets. Based on the GFD feature vectors capturing the local topological structures of points, we introduce a novel loss term to t-SNE, vector constraints, to guide the optimization process to preserve edge vectors between data points depending on their similarity between time frames. Through a series of experiments, we found that Joint t-SNE can generate consistent projections compared with the previous techniques while keeping projection fidelity. Over the years, multidimensional projections have been proven to be of great utility for giving an overview of a single high-dimensional data. Our study further extends their utility to a new dimension, enabling them to be used for multiple high-dimensional datasets such as multivariate time-oriented data even with changing numbers of dimensions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustrative example of t-SNE, Dynamic t-SNE, and Joint t-SNE results. Two intersected 5-node structures, one with changes in topology and one without, are projected by three methods. a) t-SNE projects both structures differently, making it hard for viewers to distinguish between changed and unchanged structures. b) Although Dynamic t-SNE smooths the transition between projections, it also introduces distortions that cannot be ignored. c) Joint t-SNE preserves identical structures based on measuring edge similarities (encoded as thickness in the figure) in data space. The unchanged structure is well-preserved and could serve as a visual landmark for comparison.</figDesc><graphic coords="2,307.43,181.61,107.70,66.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. All graphlets of 3, 4, or 5 nodes.</figDesc><graphic coords="4,47.87,45.12,137.25,96.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Technical Illustration of Joint t-SNE. Note that we only consider 3-node graphlets for simplicity. a) Some changes happened between X 0 and X 1 . Several points broke the neighborhood relationship with the original cluster. Joint t-SNE measures the similarity of local structures to find such changes and computes edge similarities (S e 12 &gt; S e 13 &gt; S e 14 ). b) Using edge similarity as the weight of the corresponding vector constraint, Joint t-SNE generates projection Y 1 , which keeps the relative position of points in Y 0 accordingly.</figDesc><graphic coords="4,65.66,171.22,212.83,194.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An illustrative example of computing point similarity. a) Node v loses a neighbor during the transition from time t = 0 to t = 1. b) The difference between feature vector f v 0 and f v 1 reflects the changes in the local structures of node v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison of the 5-Gaussian dataset projection of four different t-SNE methods. a) t-SNE produced misaligned layouts all across four time frames. b) Equal-initialization t-SNE provides better visual consistency than t-SNE but there are still unnecessary movements of clusters. c) Dynamic t-SNE showed smoothing effect by distorting projections at t = 2 and 3. d) Joint t-SNE generated coherent and reliable projections that reflected the ground-truth transformations of clusters.</figDesc><graphic coords="6,307.43,143.69,143.42,94.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison of the VGG dataset projections of four t-SNE techniques. a) and b) t-SNE and Equal-initialization t-SNE produced faithful but inconsistent projections. See the red and pink clusters moving around the center. c) Dynamic t-SNE produced projections that are too rigid to reflect abrupt changes in topology. For example, the green and light green clusters remain separated at the end, failing to escape from their initial positions. d) Joint t-SNE generated more faithful projections that are robust to such abrupt changes. See the points from a pair of classes (two classes with the same hue) gather as in the t-SNE and Equal-initialization t-SNE projections while providing visual consistency.</figDesc><graphic coords="9,446.03,49.25,68.78,247.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Averaged Local Coherence Error</figDesc><table><row><cell>Datasets</cell><cell>Methods</cell><cell>t-SNE</cell><cell>ET</cell><cell>DT</cell><cell>JT</cell></row><row><cell cols="2">5-Gaussian</cell><cell cols="2">1,972.38 1,468.45</cell><cell cols="2">30.21 13.66</cell></row><row><cell cols="2">10-Gaussian</cell><cell cols="4">7,365.81 6,822.01 249.88 61.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative measurement for projection fidelity</figDesc><table><row><cell>Methods</cell><cell cols="3">kNN preservation</cell><cell></cell><cell></cell><cell cols="2">KL divergence</cell><cell></cell></row><row><cell>Datasets</cell><cell>t-SNE</cell><cell>ET</cell><cell>DT</cell><cell>JT</cell><cell>t-SNE</cell><cell>ET</cell><cell>DT</cell><cell>JT</cell></row><row><cell>5-Gaussian</cell><cell cols="4">0.30 0.30 0.32 0.34</cell><cell cols="4">1.00 1.00 1.05 1.03</cell></row><row><cell>10-Gaussian</cell><cell cols="4">0.19 0.19 0.16 0.23</cell><cell cols="4">1.62 1.62 1.69 1.69</cell></row><row><cell>MNIST</cell><cell cols="4">0.26 0.26 0.21 0.24</cell><cell cols="4">1.00 1.00 1.16 1.05</cell></row><row><cell>VGG</cell><cell cols="4">0.57 0.56 0.48 0.55</cell><cell cols="4">0.60 0.60 1.01 0.65</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the grants of the NSFC (61772315, 61861136012), the Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University (No.VRLAB2020C08), and the CAS grant (GJHZ1862).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Timeaware visualization of document collections</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Alencar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Börner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C F</forename><surname>De Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th</title>
				<meeting>the 27th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<idno type="DOI">10.1145/2245276.2245469</idno>
	</analytic>
	<monogr>
		<title level="j">Annual ACM Symposium on Applied Computing</title>
		<imprint>
			<biblScope unit="page" from="997" to="1004" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Timecluster: dimension reduction applied to temporal data for visual analytics. The Visual Computer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00371-019-01673-y</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1013" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Timeseriespaths: Projection-based explorative analysis of multivariate time series data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guise: Uniform sampling of graphlets for large graph analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2012.87</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 12th International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Embedding comparator: Visualizing differences in global structure and local neighborhoods via small multiples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04853</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2005.132</idno>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE International Conference on Data Mining (ICDM&apos;05)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A graph-theoretic approach to enterprise network dynamics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kraetzl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Wallis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-8176-4519-9</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bnpmda: bipartite network projection for mirna-disease association prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bty333</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3178" to="3186" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A A</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="315" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Heidelberg</forename><surname>Springer Berlin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-33037-014</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Openmp: an industry standard api for sharedmemory programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computational science and engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>doi: 10. 1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Biolayout-an automatic graph layout algorithm for similarity visualization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Enright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ouzounis</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/17.9.853</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="853" to="854" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An incremental dimensionality reduction method for visualizing streaming multidimensional data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shilpika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934433</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="418" to="428" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Clustermap: compare multiple single cell rna-seq datasets across different experimental conditions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gogol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz024</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3038" to="3045" />
		</imprint>
	</monogr>
	<note type="report_type">Bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual comparison for information visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jusufi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1177/1473871611416549</idno>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graphlet-based measures are suitable for biological network comparison</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bts729</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="483" to="491" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Consecutive dimensionality reduction by canonical correlation analysis for visualization of convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hidaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<idno type="DOI">10.5687/SSS.2017.160</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISCIE International Symposium on Stochastic Systems Theory and its Applications</title>
				<meeting>the ISCIE International Symposium on Stochastic Systems Theory and its Applications</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>The ISCIE Symposium on Stochastic Systems Theory and Its Applications</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal mds plots for analysis of multivariate data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jäckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467553</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="150" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Panene: A progressive algorithm for indexing and querying approximate k-nearest neighbors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2869149</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The art of using t-sne for single-cell transcriptomics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kobak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-019-13056-x</idno>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The graphlet spectrum</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553443</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
				<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete structures</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on machine learning</title>
				<meeting>the 19th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What would a graph look like in this layout? a machine learning approach to large graph visualization</title>
		<author>
			<persName><forename type="first">O.-H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Crnovrsanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2743858</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="478" to="488" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very sparse random projections</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
		<idno>doi: 10.1145/ 1150402.1150436</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Systemslevel cancer gene identification from protein interaction network topology applied to melanogenesis-related functional genomics data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Milenković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Memišević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
		<idno type="DOI">10.1098/rsif.2009.0192</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Society Interface</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="423" to="437" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Uncovering biological network function via graphlet degree signatures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Milenković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer informatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>CIN-S680</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The projection explorer: A flexible tool for projection-based multidimensional visualization</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Paulovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C F</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
		<idno type="DOI">10.1109/SIBGRAPI.2007.21</idno>
	</analytic>
	<monogr>
		<title level="m">XX Brazilian Symposium on Computer Graphics and Image Processing (SIBGRAPI 2007)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical stochastic neighbor embedding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12878</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Approximated and user steerable tsne for progressive visual analytics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2570755</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1739" to="1752" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btl301</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="e177" to="e183" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling interactome: scalefree or geometric?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Corneil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jurisica</surname></persName>
		</author>
		<idno>doi: 10. 1093/bioinformatics/bth436</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3508" to="3515" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing time-dependent data using dynamic t-sne</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroVis (Short Papers)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="73" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-007-0075-7</idno>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimation of graphlet counts in massive networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2826529</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph mining: procedure, application to drug discovery and recent advances. Drug discovery today</title>
		<author>
			<persName><forename type="first">I</forename><surname>Takigawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.drudis.2012.07.016</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting stress majorization as a unified framework for interactive constrained graph visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2745919</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="489" to="499" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structure-aware fisheye views for efficient large graph exploration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864911</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="566" to="575" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">How to use t-sne effectively</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00002</idno>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discovering discriminative graphlets for aerial image categories recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno>doi: 10. 1109/TIP.2013.2278465</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5071" to="5084" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
