<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VizSnippets: Compressing Visualization Bundles Into Representative Previews for Browsing Visualization Collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Oppermann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tamara</forename><surname>Munzner</surname></persName>
						</author>
						<title level="a" type="main">VizSnippets: Compressing Visualization Bundles Into Representative Previews for Browsing Visualization Collections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F8643412BD5C64EFB180CACD6A15AAE0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visualization collections</term>
					<term>visualization bundles</term>
					<term>result snippets</term>
					<term>visual inspection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Automatically generated snippets of the same Observable notebook [3]. (1) Vertical snippet layout with image collage of six images, and image carousel to scroll through additional content; (2) Existing visualization snippets in commercial products are limited to a single thumbnail; (3) Horizontal snippet layout; (4) Small minimum image size allows more images to fit in a collage; (5)</p><p>Table layout; (6) Large preview snippet minimizes information loss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>People across organizations and sectors now have access to growing troves of data and collectively amass large collections of visualization content. Business intelligence (BI) tools, such as Tableau, Power BI, Qlik, and Looker, are used daily by millions of people. The primary artifacts created with these tools are visualization bundles, that combine multiple visualizations, dashboards, and data sources. Bundling content to organize, save, and share analysis results is a common practice even beyond BI tools, through computational notebooks such as Observable that encompass substantial amounts of diverse visualization content.</p><p>When users browse or search collections of visualization bundles, they regularly need to choose between multiple results and judge their relevance. To avoid the time cost of loading and examining in detail these results one by one, bundles are summarized as compact previews, which we call snippets. However, existing snippet designs yield poor information density: they fail to help people judge the relevance of bundles because they include only one image and a title, falling far short of communicating the full information content of a bundle that may include multiple views and dashboards. We engaged with Tableau product groups and end users to determine that this problem has substantial real-world impact, and reviewed snippet designs across five major tools to establish that it is pervasive. Although guidance exists for snippet design with data types such as text, images, and video, we are the first to systematically approach visualization snippet design. We propose a computational pipeline to compress visualization bundles into representative snippets, providing visualization system designers with a set of adjustable algorithmic building blocks. It has many controllable features including the ability to adapt to a given pixel budget and to create flexible families of layouts with desired form factors. We also present practical guidance on how visualization snippet designers can use the pipeline to support many use cases through concrete examples.</p><p>We contribute: • A set of eight key challenges pertaining to visualization snippets, and a top-down design framework for visualization snippets with three levels (page, snippet, images+text) and two categories of design choices (size/number of elements, visual layout/encoding).</p><p>• A computational pipeline for the lossy compression of visualization bundles into snippets, consisting of a set of algorithmic building blocks to filter, rank, and display visual and textual content in accordance with an available pixel budget. We validate the pipeline's algorithmic building blocks on two kinds of visualizations bundles, from Tableau Public and Observable, using quantitative measures, visual inspection, and A/B comparisons. For this purpose, we also labeled several thousand images to compare human judgements with model predictions.</p><p>As a secondary contribution, we promote a methodology for extensive visual inspection through random sampling. We reflect on the creation of multiple lightweight visual inspectors, each tuned for one question or algorithm, that sample a substantial corpus of real-world data to gain confidence in algorithm and parameter choices.</p><p>Inspired by the work of Rogers et al. <ref type="bibr" target="#b51">[52]</ref>, we include direct links to research artifacts (SUP1 to SUP7) throughout the paper to transparently provide an abundance of evidence for our claims and our process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VISUALIZATION COLLECTIONS AND SNIPPETS</head><p>We provide a data characterization and describe the collections we use to assess our work, define snippets in the context of visualization collections, survey current practices in snippet design, and discuss our investigation of real-world challenges through engagement with Tableau product groups. We identify key challenges and present a top-down framework for snippet generation to address them. We finally discuss the intended context of use by snippet designers for our computational pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Characterization and Sources</head><p>We define visualization bundles and collections in general, followed by a description of two sample collections that we acquired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Bundles and Collections</head><p>Visualization bundles combine single views (charts), dashboards combining multiple views <ref type="bibr" target="#b52">[53]</ref>, and one or more datasets. We categorize the content into four groups, while noting that some bundle types contain only a subset of this information.</p><p>Bundle meta-data contains the bundle title, author name, date, and usage statistics, such as the number of views and likes.</p><p>Bundle text is bags of words in each of three categories: 1) text within views, including chart and axis titles, captions, and titles; 2) data column names; 3) longer prose text that may be attached.</p><p>Bundle images are stored as PNG files, each corresponding to a view or dashboard. All images conform to the same 4:3 aspect ratio, and may be cropped or padded with white space to fit those dimensions.</p><p>Bundle specification contains additional layout and chart type information, for example which views are embedded in a dashboard.</p><p>We refer to repositories of manually crafted bundles that can be browsed on dedicated platforms as visualization collections. The number of bundles in a collection ranges from hundreds to thousands or more when shared within an organization, or even millions in special cases that are open to larger communities, such as Tableau Public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Data Sources</head><p>We used two collections of real-world visualization bundles to develop and evaluate the proposed computational pipeline.</p><p>Tableau workbooks are a widely used type of visualization bundle, containing all necessary components that are required for loading and displaying the fully interactive visualizations to users.</p><p>We build on our previous work on visualization recommendation <ref type="bibr" target="#b47">[48]</ref>, where we extracted bundle data from the XML files of a set of 2910 hand-crafted workbooks randomly sampled from Tableau Public, a sprawling collection of publicly shared workbooks. Other business intelligence tools, such as Looker, Qlik, and Power BI, use similar approaches to bundle visualizations and data sources into workbooks, reports, or apps, so our findings are generalizable beyond Tableau.</p><p>Observable is a reactive computational notebook for data analysis and visualization, used by a rapidly growing community. A notebook is made up of a series of cells containing JavaScript code, prose, interactive visualizations, and images. This structure follows the paradigm of literate programming <ref type="bibr" target="#b30">[31]</ref> where explanations in natural language are interspersed with code snippets. Although the use cases related to computational notebooks may be very different to those facilitated by BI tools, they exhibit intriguing content similarities; thus we also consider those notebooks as visualization bundles.</p><p>We implemented a Chrome extension to scrape a diverse sample of 178 publicly accessible Observable notebooks. Our tool takes screenshots of Canvas and SVG areas, downloads embedded images, extracts meta-data information, and creates a bag of words from the text found in Markdown cells. All visualizations are treated as single views because inferring the composition of dashboards from source code is non-trivial, and the actual number of dashboards on Observable appears to be very low (0 in a sample of 40 notebooks). [SUP2] contains the source code of the scraper and the raw data of our Observable collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization Snippet Usage</head><p>We define visualization snippets broadly as compact, representative summaries of visualization bundles that help users discriminate between relevant and irrelevant content in visualization collections. Use cases for snippets within visualization platforms include showing search results, supporting faceted browsing, and suggesting recommendations. The form factors vary: many medium-sized snippets can be shown within a large window as with search results, or a few snippets within a small region of the screen as with recommendation results, or a single large snippet may appear upon hover in a large popup window to preview bundle content with as much detail as possible.</p><p>We provide an example scenario to illustrate the search use case for snippets. A marketing analyst searches the company-wide visualization collection to check if any colleagues have previously created a report on product downloads. The collection does have some hierarchical organization into folders, but that structure does not help them scope their search. A query results in multiple pages with over a dozen snippets each. The analyst rules out some bundles as irrelevant from the title and single image visible from the snippet, but often must open the bundle to check whether it contains relevant materials. The analyst spends several minutes opening bundles and clicking through each of their views, only to realize that they are outdated or concern a different product, before ultimately locating the desired analysis report.</p><p>In the information retrieval process, snippets concern the relevance prediction step <ref type="bibr" target="#b19">[20]</ref> where users predict which items summarized in a results display align with the current information need before they open the detailed versions <ref type="bibr" target="#b22">[23]</ref>. Opening many visualization bundles can be a time-consuming and tedious endeavour because of substantial load, build, and explore times, especially when visualizations contain many views or are connected to live data streams. Snippets serve as previews to facilitate the relevance prediction step as surrogates that avoid loading entire bundles via visualization platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Embedded Inquiry Within Tableau</head><p>This work grew out of collaboration with the Recommendation and Search product groups at Tableau, following up on our previous investigation on content-based recommendation models for visualizations <ref type="bibr" target="#b47">[48]</ref>. Over a period of more than a year, we had conversations regularly with product managers, machine learning engineers, and UX designers, who relayed end-user feedback and pain points about many issues to us.</p><p>The key realization that motivated this research project is that the shortcomings of existing snippet designs constitute a significant pain point for Tableau end users. Specifically, we have substantial evidence that it is difficult and often impossible for users to judge a bundle's relevance based on the given snippet, without opening it. We heard this idea from multiple people within Tableau. We verified this common internal perception through direct interviews with external target users. The first author of this paper joined several interviews with target users to evaluate a new visualization recommendation model; despite the intent to focus on the back-end model, participants repeatedly commented on the presentation of the recommendations that were shown as visualization snippets, confirming the problem of insufficient visible information content.</p><p>We also observed that visualization authors commonly start from scratch instead of reusing existing content, even in contexts where re-use would seem to provide benefits; we conjecture that better snippet design might combat this tendency by helping authors find relevant content more easily.</p><p>Finally, we noted that when developing new product features, product groups either simply re-use existing snippet designs or end up making custom modifications based on intuition, without systematic reasoning of how snippets could be explicitly designed to support platform use cases or user analysis goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Review of Existing Snippet Designs</head><p>To ensure generality beyond the particular platform of Tableau, we informally surveyed the content and layout of existing snippet designs used in Tableau Online/Public, Power BI, Qlik, Looker, and Observable. All existing snippet designs show only the bundle title and one image, and sometimes a few meta-data attributes. In many cases these titles are not very descriptive and the single image has low information content, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>, so they fail to support users when assessing the relevance of bundles.</p><p>None of the snippet approaches that we surveyed incorporate multiple images; in all cases a single image thumbnail of a view or dashboard is selected and cropped automatically. For example, Observable chooses the first image, SVG, or Canvas element found in a notebook. Even if a bundle contains useful content, the automatically selected images may show incomplete draft views or appear empty when scaled down to a thumbnail size. Only a few tools allow creators to upload custom thumbnails that may include illustrations that reflect the bundle context but are not directly related to the data <ref type="bibr" target="#b29">[30]</ref>. The thumbnail size varies substantially between platforms.</p><p>In terms of text content, the bundle title is always prioritized, although sometimes significantly truncated. The only approaches that include a textual description are Tableau Public's gallery view that presents human-curated bundles and Qlik's list view. We note that creators are frequently unwilling to invest the additional effort of crafting a description. None of the existing snippet designs include keywords or other textual information extracted from the bundles such as tags, despite tagging functionalities in some of the tools. Meta-data attributes such as author name, date, and number of likes, are included in some snippet designs but the selection of these attributes differs widely across platforms.</p><p>Snippets are arranged in a uniform grid layout by default in all tools, except Tableau Public which uses a list layout. The option to choose between two different layouts is provided in 4 of the 6 platforms. The snippet size is always the same independent of the bundle content, and most of the pixel space is devoted to the thumbnail.</p><p>Further details are included in [SUP1], and Sec. 5.3 presents A/B comparisons between existing product snippet designs and the results of our proposed VizSnippets pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Challenges</head><p>Our two-pronged investigation led us to identify eight challenges for snippet generation: C-OneImage: Limited information content of a single image. A single image of one view or dashboard provides only a very narrow lens on a bundle and may not accurately capture the diverse visual content across multiple views. Blank thumbnail images are surprisingly frequent within our sample collections. C-Titles: Limited information content of titles. We learned through interviews and by reviewing the data samples that short and cryptic bundle titles pose a challenge across all collections. Uninformative titles in similar forms such as "sub-2", "Project 8", "Final Vis", and "Untitled" are seen frequently. Vague titles impede the identification of relevant content for non-authors, and community-generated content is particularly messy. C-Pixels: Limited pixel space. Pixels are a precious commodity that need to be used as effectively as possible, on the desktop and especially for mobile. The full content of a bundle can rarely be shown in its entirety, necessitating compression. C-SparseText: Limited textual content. The text found in bundles that are created with BI tools generally consists of fragments of a few words, such as titles or labels. Although an author might sometimes include captions or annotations of lengthier text, the amount of available text is far less than in standard text documents <ref type="bibr" target="#b47">[48]</ref>. This challenge is not directly applicable to Observable computational notebooks, which commonly include prose with typical grammatical sentences similar to text documents, for instance when created as instructional guides. C-Form: Diversity of form factors. The wide variety of use cases for snippets leads to different form factor requirements, from many medium-sized snippets in a large window to support quick comparisons between bundles, to a few small snippets in a small region using minimal screen real estate, to a full-screen view of a single large snippet to provide a very detailed preview. C-Complex: Diversity of bundle complexity. The content in the reviewed collections ranges from single bar charts to complex bundles with dozens of views and dashboards. Quantitatively, the bundles in our Tableau Public collection contain 2.19 dashboards and 4.11 views on average (the medians are 1 and 3 respectively). Views are often just building blocks for dashboards or contain auxiliary information. C-Quality: Diversity of content quality. We found substantial differences in the quality of information content within bundles: the same bundle might have empty or incomplete drafts in addition to high quality finalized content, or many similar images alongside quite different ones. When summarizing bundles, high quality informative content should be elevated, while incomplete or empty or similar views and less informative text should be filtered or downgraded. C-Goals: Diversity of end-user goals. We identified disparate end-user goals that could be supported by visualization platforms: collaborate with colleagues, create portfolios to share work, find out how other people did something, get visual inspiration, look up information, and save all visualizations in one space. These goals may lead to different snippet affordances. For example, images and chart types are paramount for visual inspiration, but information lookup might require more emphasis on the underlying data sources of a bundle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Design Framework</head><p>To address these challenges, we propose a top-down design framework for snippet generation with three levels and two categories of choices, shown in Fig. <ref type="figure" target="#fig_1">3</ref>. The top level pertains to the layout of a result page, and choices at this level affect options at the middle level of snippet and lowest level of images+text (see [SUP4] for all possible combinations). One category of design choices is the size and number of elements, and the other pertains to the layout and visual encoding. The computational pipeline proposed in Sec. 4 fully supports this framework.</p><p>We consider snippet generation through the lens of lossy compression, where the goal is to provide as much detail as possible within a given pixel budget. Although a very compact snippet that could be used within a grid of many snippets is a highly compressed representation of a bundle, another use case is a full-screen preview of a single bundle that could capture a substantial fraction of the salient content with limited loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Intended Framework and Pipeline Usage</head><p>We instantiate this design framework with a computational pipeline that delivers a powerful and flexible infrastructure to visualization snippet designers who want to automatically create snippets with greater information density than existing designs. We consider snippet designers to be a broad group, encompassing not only UX designers but others involved in creating or customizing snippets related to visualization collections, such as a team working on a recommendation system wanting to extend snippets to add explanations why something gets recommended <ref type="bibr" target="#b61">[62]</ref>.</p><p>We anticipate that snippet designers would use this pipeline after they ascertain the exact requirements for their particular use case. That characterization would probably require targeted user studies, which we leave to those designers rather than attempting to conduct any of them ourselves. Instead, we thoroughly validate the technical underpinnings and performance of the pipeline and its algorithmic building blocks on real-world data, and provide actionable recommendations on how it can be flexibly adapted for different scenarios.</p><p>We hope our work encourages the creation of representative snippets beyond single thumbnails with titles. The diversity of snippet form factors (C-Form) and end-user goals (C-Goals) reveals that different use cases would benefit from more specialized designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>We now discuss result snippets for a range of content types, and existing approaches to visualization compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Result Snippets</head><p>Our work is informed by previous studies that evaluated snippet representations. Result snippets have been investigated for a range of scenarios and content types, such as documents <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60]</ref>, source code <ref type="bibr" target="#b18">[19]</ref>, e-commerce products <ref type="bibr" target="#b43">[44]</ref>, books <ref type="bibr" target="#b39">[40]</ref>, music <ref type="bibr" target="#b71">[72]</ref>, videos <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b67">68]</ref>, images <ref type="bibr" target="#b13">[14]</ref>, and extensively for websites <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b60">61]</ref> in the context of search engine result pages. Although the relative position of results remains one of the most influential factors for assessing their relevance <ref type="bibr" target="#b38">[39]</ref>, the back-end algorithms to find and rank bundle results are not the objective of this project; instead, our focus is specifically on the snippet content and its representation.</p><p>Several studies examined website snippets that contain thumbnails only and in combination with text summaries <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref>, and suggest that thumbnails used along with text summaries help in reducing errors when predicting relevance. Capra et al. <ref type="bibr" target="#b8">[9]</ref> showed that images provide only a small benefit in judgment accuracy compared to text-only snippets. However, they emphasize that image-augmentation helped measurably in several scenarios when textual components are poor, which is the case for visualization bundles. Otherwise, the characteristics of a website are significantly different than visualization bundles.</p><p>Snippets for videos are particularly relevant for our work. Due to their intrinsic graphical and temporal nature, videos exacerbate the need for snippets that offer additional representation facets besides text. Wildemuth et al. <ref type="bibr" target="#b66">[67]</ref> concluded that multi-image compositions or short video sequences are more expressive and favored by users over single images. Displaying multiple key frames of a video <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b66">67]</ref> is similar in spirit, but different in detail, to the image collage technique that we propose in Sec. 4.3.5. Collages are composed of several selected visualizations and dashboards to provide a bird's eye view on the bundle content. However, due to completely different semantics, existing techniques for identifying highlight frames from videos cannot be directly applied to selecting visualization images from a bundle.</p><p>Natural language provides great richness in result snippets and is used beyond textual documents for other types of media, such as videos or photographs. Augmenting a snippet with a description or keywords has been proven effective in previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Recent approaches to automatically generate captions for visualizations are promising but are limited to single views and specific visual features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref>. In contrast, our goal is to summarize visualization bundles, with potentially dozens of views, on a high level. The automated creation of snippets for complex visualization bundles, as we propose in this paper, has not been studied previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approaches to Visualization Compression</head><p>In the visualization literature, many approaches have been discussed to compress visual encodings into more compact representations, for example, to create overviews, to support comparisons across many items, or to allow consumption on mobile devices. This body of work spans across visualization techniques, including trees <ref type="bibr" target="#b36">[37]</ref>, graphs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b69">70]</ref>, time series <ref type="bibr" target="#b12">[13]</ref>, small multiples <ref type="bibr" target="#b6">[7]</ref>, and infographics <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref>. In recent years, there has been a growing interest in responsive visualizations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b68">69]</ref>. In contrast, our goal is different from all these approaches. Rather than adapting visual encodings in response to different screen resolutions, we summarize complex visualization bundles containing existing images of multiple views and dashboards. The methods discussed in Sec. 4 are based on selecting from a given set of images; we do not propose new altered or derived visual encodings.</p><p>Most closely related is the work on visualization thumbnails. Heer et al. <ref type="bibr" target="#b24">[25]</ref> proposed to include thumbnails of single views in graphical histories to support the visualization authoring process. Kim et al. <ref type="bibr" target="#b29">[30]</ref> surveyed current practices in creating visualization thumbnails, specifically for data stories. Our work does not address generating new thumbnails, but rather provides computational methods for selecting from a set of existing thumbnails to generate snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VIZSNIPPETS COMPUTATIONAL PIPELINE</head><p>We propose a pipeline to compress visualization bundles into representative snippets, that is adaptive to given space constraints (C-Pixels and C-Form) and other user-defined preferences. The pipeline, illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>, consists of multiple interchangeable algorithmic building blocks with minimal dependencies. The extraction (green box) and lossy compression of the bundle content (yellow boxes for images, pink for text) is completed in an offline preprocessing step. The creation and arrangement of the snippets (blue box) is determined at run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Inspectors</head><p>We developed and evaluated the building blocks iteratively by using a suite of eight visual inspectors, leveraging an extensive corpus of real-world data. These lightweight inspectors I1 to I8, shown in Fig. <ref type="figure" target="#fig_3">5</ref> (and [SUP3]), are targeted at specific algorithms or analysis questions and present results based on random data samples. This type of extensive visual inspection allowed us to gain further confidence in model choices and parameter settings, beyond standard quantitative analyses. We elucidate how inspectors informed our decisions throughout the following sections, and reflect on this approach as a generalizable method to provide guidance and to foster transferability in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Page and Snippet Layouts</head><p>We support five general page layouts that control how snippets are arranged: the grid layout divides a page into rows and columns, and all snippets have a uniform size (fixed width, fixed height); the strip  layout, also known as masonry layout, divides the page into equal sized columns (fixed width) with variable snippet heights; the list layout displays one snippet per row (full width, variable height); the table layout also shows one snippet per row but meta-data attributes are divided into separate sortable columns and the snippet height is fixed; the preview layout allocates all pixels of the page to a single large snippet. These five layouts support all snippet use cases and form factors (C-Form) that we identified, and, due to the modularity of the pipeline, more options can be flexibly added. To optimize the efficacy of snippets, layout choices should be made according to tasks and typical bundle characteristics in a visualization collection. We discuss design rationales and potential usage scenarios for different layouts in [SUP4]</p><p>The initial page layout selection has cascading effects on lower levels and dictates certain design decisions. For example, the page layout dictates if a snippet has a variable or a fixed size. With the grid layout, we can choose a specific snippet size and decide what ratio of pixel space should be devoted to visual versus textual content (C-Goals). With the strip layout, the snippet height is variable and depends on other parameters related to the text and image content (C-Complex). The actual number of snippets that are visible on screen depends on the snippet size and the chosen page layout.</p><p>We provide three options to arrange the visual and textual content within a snippet: a vertical, horizontal, or table layout. We deemed more complex compositions such as wrapping text around images to be less important for the use cases we identified, but these could be added in the future. The snippet layout defines the canvas that will be filled with text and image content. We describe the algorithms for compressing and displaying the bundle content in subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Snippet Images</head><p>Our investigation confirmed that images are particularly relevant for visualization snippets. Besides conveying the visual style and helping users to recognize previously seen content <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b60">61]</ref>, images capture semantic information. The chart type can indicate underlying data types or the visualization objective. For instance, maps imply geographic data and pie charts show part-to-whole relationships.</p><p>We now describe the algorithmic building blocks for selecting and displaying composite images to increase the information content of a snippet beyond what a single image can convey (C-OneImage). The yellow boxes in Fig. <ref type="figure" target="#fig_2">4</ref> show the image-related blocks: filter empty and embedded images, filter nested images, rank images, filter similar images, re-rank images to increase diversity, and display image compositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Filter Empty and Embedded Images</head><p>In a first step, we filter empty and nearly empty images based on a dominant colour threshold C T . We convert each image to a list of RGB pixels (points in 3D space) and use the k-means clustering algorithm to extract the k dominant colors of an image <ref type="bibr" target="#b56">[57]</ref>, where k corresponds to the number of cluster centroids that are used to iteratively assign the pixels to clusters. We compute the relative size of each cluster, namely how many pixels of the original image are assigned to each dominant colour, to determine if a single colour predominates an image, signalling that it is less informative (C-Quality). A dominant colour threshold C T of 98.2% led to the highest accuracy for the sample visualization collections, with k = 5 colours. Further details about determining these parameters are included in Sec. 5.2.</p><p>For visualization bundles that contain dashboards, we avoid repetition by filtering out images of the embedded views that were the building blocks used to create them. Prioritizing dashboards makes wise use of available pixel space (C-Pixels). This process is straightforward for Tableau data because the views that are embedded in a dashboard are defined in the bundle specification. All images extracted from Observable notebooks are treated as single views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Rank Images</head><p>We create an initial ranking of all images within a bundle. In many cases, the available images do not fit within the allocated pixel budget so some images are hidden or only revealed after user interaction, as we will describe in Sec. 4.3.5. Consequently, the order of images is relevant to increase the explanatory power during the first impression. We divide the images into two groups: dashboard images and single-view images.</p><p>Dashboards are prioritized and sorted by the number of embedded views and colour diversity. The percentage of the most dominant colour that we computed earlier is used as a proxy measure for colour diversity. Through visual inspection with I5 (see Sec. 5.2), we discovered that mostly single-colour images are less informative because they mostly show a background color.</p><p>We group all single-view images by chart type, sort the images within each group by colour diversity, and sort all groups by their maximum colour diversity. Then we iteratively withdraw one image from each group until all groups are empty. All the images get assigned a consecutive number that is used as an initial score. When chart types are not available, such as in Observable collections, the images are only sorted by colour diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Filter Similar Images</head><p>Bundles frequently include very similar or identical views that should be filtered (C-Quality). We compute pairwise distances between all bundle images and filter out those where the distance is below a given threshold. For determining the distance between images, we compared three traditional approaches based on hand-crafted visual features, histogram of colours (HoC), histogram of oriented gradients (HoG) <ref type="bibr" target="#b17">[18]</ref>, and structural similarity index (SSIM) <ref type="bibr" target="#b63">[64]</ref>, and the more recent approach of learning visual features and a similarity function through a Siamese convolutional neural network (CNN) <ref type="bibr" target="#b62">[63]</ref>. The performance of the CNN was comparable with the best hand-crafted feature approach for this particular task of identifying very similar or identical images, so we chose the less complex, unsupervised method (HoG). See further details in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Re-Rank Images to Increase Diversity</head><p>After applying the previous steps, similar images might still be ranked high and close to each other. The chart type specification may not be available or cannot be detected, or views use the same base chart type.</p><p>We propose a re-ranking step using maximal marginal relevance (MMR) <ref type="bibr" target="#b9">[10]</ref> to ensure that the visual diversity of a bundle is conveyed by a very small number of images. MMR is a greedy, iterative algorithm that linearly interpolates between the original ranking score of an image and the diversity to other, already selected images. The algorithm is typically used in diversity-based text retrieval scenarios <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47]</ref> but has been also applied to image re-orderings <ref type="bibr" target="#b55">[56]</ref>.</p><p>The MMR algorithm hinges on the similarity model in Sec. 4.3.3 to accurately compare images. First, the top-ranked image is selected based on the initial score and all remaining images are considered as candidates. Then, the algorithm iteratively selects the image with the highest MMR score ŝ := λ • s − (1 − λ ) • MaxSim, terminating when all candidates have been assigned to the ranking. The relative weight between the original image score, s, and the maximum similarity, MaxSim, is specified with a single tunable parameter λ . For example, λ = 1 ranks entirely by s and λ = 0 selects maximally diverse images irrespective of the initial ranking described in Sec. 4.3.2. We found that λ = 0.75 leads to diverse rankings, as suggested in a systematic MMR evaluation on comment diversification <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Display Image Compositions</head><p>We propose image collages to show multiple images within a snippet. A collage is composed of equal-sized images that are displayed in a uniform grid layout. The number of images depends on the chosen snippet size (see Fig. <ref type="figure" target="#fig_1">3</ref>): (a) for a variable snippet size, designers specify the number of images and the image size, and the overall collage size expands accordingly; (b) for a fixed snippet size, designers specify the minimum image size and the algorithm tries to fit as many images as possible into the collage.</p><p>The construction of the collage layout is based on a grid packing algorithm that determines the optimal number of images iteratively. When there are more image slots than images available, the image size gets increased to make optimal use of the given pixel space (C-Pixels). Although text within the image is not legible, the chart types and visual styles can often be identified.</p><p>In some cases, the selected images exceed the number of slots within a collage. Besides merely hiding overflow images, one option is to create an image carousel that consists of multiple collages and allows users to scroll through additional content. The supplemental video [SUP7] shows image collages and carousels in action, and we discuss limitations and opportunities of multi-image compositions in Sec. 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Snippet Text</head><p>Our pipeline supports three kinds of text displayed within a snippet: the bundle title, keywords, and meta-data. All the BI tools that we reviewed use only the user-specified bundle titles, but our finding that these titles may not accurately reflect the semantic content of bundles (C-Titles) leads us to a more sophisticated approach. We argue that a small set of carefully chosen keywords (or tags) can provide valuable insights without consuming too much space.</p><p>The pink boxes in Fig. <ref type="figure" target="#fig_2">4</ref> show the process for extracting keywords and displaying text content in a snippet: preprocess text, generated ordered list of keywords, filter keywords, add calendar years, select meta-data, and display text content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Process Text and Extract Keywords</head><p>We distinguish between two different bags of words that are extracted from a bundle and used as input data, taking care to glean all information from the limited textual content (C-SparseText). First, the visible text includes chart and axis titles, sheet names, captions, annotations, and descriptions. Second, if available, the column names from data sources are used as additional text data and referred to as hidden text. This hidden text can provide a useful signal about the bundle contents <ref type="bibr" target="#b47">[48]</ref>, to address the problem of limited information content of titles (C-Titles). Both sets can contain human readable text that may, but does not always, capture meaningful semantic information. The goal of this process is to select and rank keywords that best represent the underlying bundle content.</p><p>We remove punctuation, strings with less than three characters, numbers, and English stop words. We also lemmatize the text and apply a custom stop-word filter of 349 words we identified by analyzing frequent but non-informative words or phrases in our sample collections.</p><p>We use the term frequency-inverse document frequency (TF-IDF) model <ref type="bibr" target="#b50">[51]</ref> on both visible and hidden text to extract keywords. Up to 30 keywords are extracted for each bag of words; this threshold is relatively high, and in many cases fewer keywords are available.The numerical TF-IDF-weighted scores of the terms are used for the ranking.</p><p>We remove keywords that are included in the bundle title to avoid repetitions. Although we lemmatize words to capture variant forms of the same word, very similar keywords may still get selected because they are not in the lexical database. To increase the diversity of topranked keywords, we compute Levenshtein distances between pairs of words and filter similar ones <ref type="bibr" target="#b49">[50]</ref>. The Levenshtein distance is a measure for the number of edit-operations that are needed to transform one word into another. A normalized distance of 0 means that strings are identical and 1 if strings are completely different. We identified 0.15 as a useful threshold (see Sec. 5.1).</p><p>We give precedence to visible text irrespective of the TF-IDF scores, and compute the distance between those keywords. If the distance between two words is lower or equal to 0.15, we check the spelling <ref type="bibr" target="#b40">[41]</ref> and choose the alternative that is spelled correctly. We find that this mechanism significantly improves the quality of keywords. We then combine the two sets of keywords by iteratively selecting a hidden text keyword and comparing the distance to already selected keywords until we have reached the desired threshold.</p><p>Finally, we attempt to extract calendar years from the removed set of numbers and add them back to the keywords. We discovered that calendar years are commonly mentioned in visualizations but often not included in the bundle title. We parse the text for four-digit numbers within the range of 1900 to 2050. We conjecture that this range will result only in a small number of false positives, based on our informal manual inspection [SUP5]. We combine sequences of years into single strings, such as 2019-2021 instead of 2019, 2020, 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Display Text Elements</head><p>The core text element is the bundle title specified by the author. The pipeline supports title truncation on demand, but we note it requires careful consideration because longer titles are more descriptive. The titles otherwise remain unchanged because removing stop words or numbers would likely confuse authors.</p><p>The pipeline allows snippet designers to manually choose which meta-data attributes are relevant for their use case, such as the author, the number of likes, or recent date of opening.</p><p>Adding keywords to snippets follows a similar approach to displaying snippet images, where we distinguish between variable-sized and fixed-sized snippets. The priority of the keywords is predetermined by their order, as described in the previous section.</p><p>When the snippet size is variable, designers specify the number of keywords that should be displayed. In case of fixed-size snippets, we fit as many keywords as possible based on the individual title length and the meta-data attributes. More specifically, we add keywords successively and verify the size of the snippet bounding box until the given constraints are reached. This approach is computationally expensive for many results; for production environments, we suggest to predetermine the default number of keywords that should be displayed for specific snippet sizes.</p><p>In the table layout, the title and the keywords are displayed in one column and the meta-data attributes are split into separate columns. In case of all other layouts, the title, the meta-data, and the keywords are shown as three horizontal rows in the text area of a snippet. A high-level parameter controls the font size of the snippet text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We conducted substantial testing on the VizSnippets pipeline during and after its creation. We now present quantitative and qualitative results to demonstrate its utility, validate our claims, and discuss trade-offs related to algorithms and parameter choices. Due to page constraints, we mainly report results based on the larger Tableau Public collection; further details related to the Observable notebooks are in [SUP5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Snippet Text</head><p>We used visual inspectors (I1, I2) and descriptive statistics to analyze the textual content of bundles and snippets.</p><p>We noted that the titles often provide little informative value, confirming our early presumptions. The average length of bundle titles in the Tableau Public collection is 21 characters (median: 18). If we would apply the same text processing steps on Tableau titles as we do for other text content, 16% of titles would be empty strings. The sample Observable titles are significantly longer and contain 31 characters on average (median: 28).</p><p>We examined the raw text content of Tableau bundles using I1, which indicated that the hidden data column names can serve as useful auxiliary data but are often less clear than chart-visible text, confirming our choice to select keywords from visible text first and referring to hidden text only if room remains. The percentage of correctly spelled words is 93% for visible text and 88% for hidden text.</p><p>For the Levenshtein distance to filter very similar keywords, we determined the threshold of 0.15 (on a scale from 0-1) through visually inspecting hundreds of keyword pairs using I2. For larger distances, words frequently have different meanings, in contrast to smaller differences that are typos or word variations not captured during lemmatization. After applying the filtering steps, the average number of keywords decreases from 20 to 16 (the median is 13).</p><p>In assessing the extraction of calendar years that are not included in the bundle title, we found that 19.7% of all bundles contain annual details (within the range of 1900-2050) and visually verified that these years provide valuable information when comparing multiple bundles, such as reports covering different time periods. In 67% of cases, none of the extracted years are mentioned in the bundle title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Snippet Images</head><p>The average number of extracted images per bundle is 5.3 (the median is 4). After applying the filtering steps, the average number is only 2.5 (the median is 2), which demonstrates the potential for compressing visual content with minimal information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Empty Images and Colour Diversity</head><p>To evaluate the algorithm for filtering empty and nearly empty images using a dominant colour threshold, we manually labeled 2600 images using binary labels. We then tested the accuracy for varying colour thresholds C T and different numbers of clusters (k = <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>), where k corresponds to the number of dominant colours. k = 5 and k = 7 led to similar results but the performance of k-means clustering decreases with more clusters. The highest accuracy of 93%, with k = 5, came from setting C T to 98.5%.</p><p>We also used visual inspector I5 to further analyze what type of images are filtered. If C T is too low, line charts with minimal data ink may get falsely discarded, while a very high C T may include images that contain mostly white space and provide no informative value. By using this inspector and adjusting C T interactively, we observed that images with a high colour diversity are more informative when scaled down, compared to mostly single-colour images. Therefore, we incorporate the ratio of the most dominant colour in the image ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Similar Images</head><p>We compared four approaches for computing pairwise image similarities. Three methods are based on hand-crafted visual features: histogram of colours (HoC), histogram of oriented gradients (HoG), and structural similarity index (SSIM). The fourth method is a Siamese convolutional neural network (CNN) trained on image pairs to learn deep representations and the similarity function automatically.</p><p>We manually labeled 6019 image pairs to evaluate the algorithms. We constructed pairs by randomly sampling two images from the same bundle, to increase the chance of similar images and because the algorithms are generally only used to compare images within and not across bundles. Image similarity was determined solely based on the visual style and not the underlying semantics. The deciding factor was if two images provide any additional value or if one representative image is sufficient. 35% of the image pairs were ultimately labeled as similar. We divided the pairs into a training and validation set with an 80/20 split for use with the CNN; the other three methods are unsupervised.</p><p>All methods reach a similar accuracy between 93%-94%. More details about the models and quantitative results are in [SUP5].</p><p>We implemented two visual inspectors (I3 and I4) to qualitatively analyze the results. I3 shows random samples of negatively predicted image pairs (false positives, false negatives). This tool allowed us to better understand when a model diverges from the human annotator. I4 shows a similarity matrix of all images within a bundle to help us analyze the magnitude of values apart from the binary classification.</p><p>The quantitative results already showed that all methods performed similarly, except SSIM has a slightly lower accuracy. The visual inspection revealed that all models detect nearly identical images well but grapple with larger differences that the human annotator considered irrelevant, such as slightly different layouts, or a chart legend that is illegible when the image is rescaled to a thumbnail size.</p><p>The CNN contains only two convolutional layers and was trained on 4815 pairs and learned low-level features. It might learn better high-level concepts by using a deeper architecture and a sufficiently large training set. However, in practice, ample data to train a CNN for diverse collections may not be available. For our objective of filtering only very similar and identical images within a bundle, conventional unsupervised methods, such as HoC and HoG, are adequate and can be directly applied to new visualization collections. When similar images are not detected with less complex models, the MMR algorithm will ensure that these images are far apart in the final ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Image Ranking and Collages</head><p>We created a visual inspector (I6) that reveals which images are getting filtered at which step, and how the original ranking changes after diversifying it based on chart types and image similarity. This type of qualitative assessment was crucial in understanding if combining various building blocks leads to the desired outcome or if important information gets lost during the image compression. Fig. <ref type="figure" target="#fig_4">6</ref> is a screenshot of I6 showing the images of 6 example bundles on a grey background. Coloured crosses indicate why images are filtered. The first 2 bundles are both reduced to a single image because of empty and embedded images. In general, we can see that most images are filtered because they are embedded in a dashboard (blue cross). The visual inspection also revealed that, with more complex dashboards, embedded views may end up very small on screen so keeping them as additional separate images might be helpful. Nevertheless, we made the trade-off to filter all embedded images because the median number of views per dashboard is only 2 and therefore sufficient details are visible in most cases.</p><p>We analyzed the number of images within a collage in relation to overflow images that are discarded or moved to additional collages in the carousel. After applying all filters, 87% of all bundles can be fully represented with collages of size 4. 94% of bundles contain 6 or fewer images. Instead of choosing a standard collage size, snippet designers only must specify the minimum image size and all images are resized or divided into multiple collages accordingly. Hence, the maximal fragmentation of a collage depends on the overall snippet size, which we will discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Page and Snippet Layout</head><p>We created the inspectors I7 to analyze different page and snippet layouts, and I8 to compare our layouts with existing snippet designs from the commercial tools Tableau, Observable, Power BI, and Qlik. Although these tools prioritize a uniform grid or list layout, we argue other layouts can be superior in some scenarios. Fig. <ref type="figure">1</ref> shows multiple automatically generated snippets of the same Observable notebook. Our pipeline supports trade-offs between compactness and information capacity, where the complexity of a bundle (C-Complexity) is directly linked to the amount of pixels used (C-Pixels). For example, snippets in a strip layout expand based on their content, so bundles with rich visual and textual content take up more space than bundles containing only a bar chart. These complexity differences are not apparent in a grid layout. The preview layout, with an example snippet shown in Fig. <ref type="figure" target="#fig_4">1-6</ref>, goes one step further to serve as a minimum-loss representation of a bundle in return for larger pixel budget. The preview layout can, for instance, be used in a modal window to provide additional details instead of a time-consuming opening of the fully interactive version of a bundle. This use case is not supported in any of the reviewed tools, but our results show its promise. Fig. <ref type="figure" target="#fig_5">7</ref> shows a comparison between a snippet of a Tableau workbook generated with our pipeline and three alternative designs used in commercial tools. This example is indicative for many other bundles we have seen in our collection. It consists of dashboards and single views, and the bundle title is vague. The three tools display only one image and a title (see Fig. <ref type="figure" target="#fig_5">7</ref> 2-4). In contrast, the VizSnippets pipeline can create an image collage with four images and augment the snippet with keywords to expose the underlying topic (see Fig. <ref type="figure" target="#fig_5">7-1</ref>). We see that keywords can be highly valuable in these cases where the bundle title is non-informative, and our design requires only a minimal increase of the overall snippet size or a slightly smaller thumbnail. When a user hovers on the snippet, arrows are superimposed and they can scroll through additional image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Designer Feedback</head><p>We obtained feedback from 6 potential snippet designers within Tableau product and research groups on the final version of the VizSnippets pipeline during an informal one-hour session. They confirmed the issues related to existing snippet designs, were highly interested in the overall ability to show more relevant information to users, and saw promise in many specific aspects of the pipeline. In comparison to the baseline designs, the added keywords and the image selection and collage generation significantly enhance the information capacity of snippets. They valued the large preview capability to show more results without loading the fully-interactive version and the flexibility of generating different types of layouts for different tasks.</p><p>The interviewees stated that this work will inform new snippet designs and they could see using the underlying algorithms for future products, although they said they were unlikely to use every supported layout. In particular, they thought some multi-image layouts may be too visually busy when showing search results in a consumer product, although the information density might be much better. To address this specific issue, designers could include only a few images in one collage. This strategy can result in several collages which are then combined into an image carousel that lets users scroll through additional content without leaving the snippet. An alternative approach is to show only one image per snippet and replace it with other images when users point their cursor onto it, similar to interactive video thumbnails <ref type="bibr" target="#b66">[67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUAL INSPECTION THROUGH RANDOM SAMPLING</head><p>We now reflect on our approach to creating multiple lightweight visual inspectors, to cast it as a general method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Inspectors for Iterative Development</head><p>Inspectors can support the common trial and error strategy for model or algorithm development where models and parameters are initially chosen through some combination of experience, reading related work, and ideally by running quantitative analyses with ground truth data. These algorithmic choices are then iteratively refined until results are satisfactory or improvement stagnates.</p><p>A concrete example is the detection of similar images. Using an algorithm based on the histogram of gradients, we manually assigned similarity labels to example image pairs, and then computed statistical measures of the performance of the binary classifier. Precision, recall, and other measures were useful instruments in determining model appropriateness. However, model results invariably led us to followup questions that were amenable to visual inspection. For example, in which cases does the model prediction diverge from the human annotator? Do those images share any special characteristics? These insights can in turn be used to tweak model parameters, such as the size of the cell for which histograms are created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Inspector Characteristics</head><p>Inspectors can facilitate discussions and inquiry about needs with project stakeholders, and most centrally serve to internally probe and validate algorithmic choices and parameter settings. We identify four The process, illustrated in Fig. <ref type="figure" target="#fig_6">8</ref>, is driven by two key ideas. First, randomly load samples from a substantial data corpus for item-level inspection to avoid cherry-picking. Second, spin up many lightweight inspectors targeted at specific analysis tasks or algorithms, instead of building one feature-rich system.</p><p>All inspectors should have a button to load new samples, to help the analyst gradually progress from a partial to a more complete understanding of model or algorithm behavior, to judge its effectiveness and limitations. Random sampling guarantees that analysts scrutinize a diverse range of results. Inspectors may have control widgets, for example to adjust algorithm parameters or to constrain the samples to a specific data range. Visual inspection through random sampling actively mitigates the risk of confirmation bias, but does not provide immunity to it. Analysts may still be enticed to draw premature conclusions after seeing only a small set of examples.</p><p>The scope of each inspector should be narrowly constrained to support ultra rapid prototyping. These tools are a means to an end, and, preferably, the implementation should not take longer than a few hours or days. The design and functionality of the visual inspectors can vary substantially, but we aimed to reuse components in different inspectors and a common database. We implemented web-based inspectors in JavaScript and D3, with a Python back-end, to provide a maximum of flexibility and because we were familiar with the development stack. Ideally, inspectors provide nearly instantaneous feedback during the interaction. Thus, model predictions or algorithm output is either preprocessed on all available data or computed live on the current sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Visual Inspector Related Work</head><p>Numerous visual analytics approaches related to model explanation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b58">59]</ref>, model construction <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref>, and model evaluation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b70">71]</ref> have been suggested. Our concern here is not to illuminate the model internals <ref type="bibr" target="#b65">[66]</ref> or steer the generation of new models <ref type="bibr" target="#b10">[11]</ref>. Instead, we are using visual methods for the evaluation and refinement of computational models that range from simple algorithms to more complex machine learning models. The goal is insights on effectiveness and limitations that can inform new parameter and model choices, or data iteration <ref type="bibr" target="#b26">[27]</ref>. Others have noted that the creation of feature-rich systems tailored to specific models <ref type="bibr" target="#b35">[36]</ref> is not always feasible, particularly when the goal is to apply existing models to new domains.</p><p>We suggest the extensive visual inspection with lightweight tools and real-world data as an alternative approach. Similar to the visual parameter space analysis <ref type="bibr" target="#b53">[54]</ref>, our approach also depends on a sampling mechanism, but we randomly sample data for item-level inspection to see a model in action while avoiding confirmation bias, instead of systematically sampling the parameter space. Closely related is the visual diagnosis of binary classifiers by <ref type="bibr">Krause [32]</ref> that includes presenting model results based on single instances. However, their focus is on instance-level explanations for binary classifications while we discuss a high-level visual-inspection strategy that is model agnostic.</p><p>Kachkaev et al. <ref type="bibr" target="#b28">[29]</ref> provide a single example showing the value of visual inspection with real-world data, in their case survey results; we advocate injecting such inspectors at many places within a computational system to assess performance and set parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND FUTURE WORK</head><p>We created and validated the computational pipeline through quantitative and qualitative analysis of two visualization collections, providing substantial evidence that we achieved our goal of dramatically increased information density within snippets. Follow-up benchmarks on additional collections would strengthen our claims of generality. User studies focused on specific use cases, ideally informed by the nuances of specific visualization platform characteristics and requirements, would shed further light on efficacy and would help designers select suitable page and snippet layouts. Our findings are applicable to other computational notebooks that may contain substantial visual content, such as Jupyter Notebook <ref type="bibr" target="#b48">[49]</ref>, and can complement code summarization techniques <ref type="bibr" target="#b64">[65]</ref>.</p><p>We chose extractive techniques to summarize the content of bundles verbatim. In contrast, abstractive techniques, such as latent Dirichlet allocation (LDA) <ref type="bibr" target="#b5">[6]</ref>, derive topics from a corpus and then use topic keywords to describe a document. These high-level concepts may be more understandable to users but the models need to be trained on large corpora of text, and even then, some bundles will get falsely allocated to a topic and represented by irrelevant keywords. Approaches related to natural language generation <ref type="bibr" target="#b45">[46]</ref> also rely on massive amounts of training data and are not directly applicable to visualization snippets due to the limited text fragments. For example, Chen <ref type="bibr" target="#b11">[12]</ref> generated abstractive summaries of web pages but ignored those with less than 100 words to ensure a sufficient basis for summarization. In comparison, our sample Tableau workbooks contain 37 unique words on average.</p><p>In terms of visual content, all methods are based on a set of screenshots, with the aim to accurately represent the visual style, but views and dashboards are scaled down significantly and may become illegible. One option is to redraw smaller versions of charts, which poses its own research problem, because of exotic chart types and inaccessible data and chart specifications. Another alternative is to analyze the bundle content and summarize it with abstract iconography or graphics <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We present a computational pipeline to compress visualization bundles, such as Tableau workbooks or Observable notebooks, into representative snippets. These snippets allow users to judge the relevance of bundles for a specific task without opening and inspecting them one by one. To gain a better understanding of requirements and challenges, we engaged with Tableau teams of potential snippet designers aware of enduser pain points, and reviewed existing snippets across a broad set of five different tools. We found that existing designs suffered from poor information density, substantially impairing the ability of users to make the expected relevance judgements. Snippets have an immense impact on the consumption of visualization collections, and designing them with higher information content can significantly improve the workflow of use cases including the presentation of search results, navigation through faceted browsing, and recommendation suggestions. The presented VizSnippets computational pipeline is responsive to a specified pixel budget, and can be adapted to many use cases and form factors through a suite of controllable layout families. The added keywords and the image selection and collage generation significantly enhance the information capacity. While the effectiveness of our pipeline was primarily validated through Tableau and Observable bundles, we argue that the suggested compression techniques can be applied to many other visualization collections due to their similar characteristics. The development and evaluation of the pipeline was informed through extensive visual inspection on random samples of real-world data. We reflect on this method and provide guidance for other researchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of existing snippets with low information content in their single images and titles: (1) Tableau Online, (2) Tableau Public.</figDesc><graphic coords="3,53.75,49.25,250.82,98.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Design framework for displaying visualization snippets, with three levels and two categories of choices.</figDesc><graphic coords="4,44.75,100.37,250.82,114.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. VizSnippets computational pipeline to compress visual and textual content of visualization bundles into representative snippets.</figDesc><graphic coords="5,53.63,49.13,513.74,78.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overview of visual inspectors that we used to analyze models and parameter settings. High-resolution screenshots are in [SUP3].</figDesc><graphic coords="5,53.99,158.45,513.02,163.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Inspector I6 to analyze the ranking and which images are filtered ( incomplete/empty, embedded, and similar views).</figDesc><graphic coords="8,44.99,49.37,250.46,140.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Snippet comparison: (1) VizSnippets pipeline, (2) Tableau Online, (3) Tableau Public, and (4) Qlik Sense.</figDesc><graphic coords="8,307.19,49.13,251.18,89.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. A conceptual model for extensive, item-level visual inspection through random sampling. abstract tasks that can be supported by inspectors: 1) see and compare results after applying a model or algorithm on real examples, 2) see how results change with varying thresholds, 3) see intermediate steps instead of the final result, and 4) see negative examples (e.g., false positives, false negatives).The process, illustrated in Fig.8, is driven by two key ideas. First, randomly load samples from a substantial data corpus for item-level inspection to avoid cherry-picking. Second, spin up many lightweight inspectors targeted at specific analysis tasks or algorithms, instead of building one feature-rich system.All inspectors should have a button to load new samples, to help the analyst gradually progress from a partial to a more complete understanding of model or algorithm behavior, to judge its effectiveness and limitations. Random sampling guarantees that analysts scrutinize a diverse range of results. Inspectors may have control widgets, for example to adjust algorithm parameters or to constrain the samples to a specific data range. Visual inspection through random sampling actively mitigates the risk of confirmation bias, but does not provide immunity to it. Analysts may still be enticed to draw premature conclusions after seeing only a small set of examples.The scope of each inspector should be narrowly constrained to support ultra rapid prototyping. These tools are a means to an end, and, preferably, the implementation should not take longer than a few hours or days. The design and functionality of the visual inspectors can vary substantially, but we aimed to reuse components in different inspectors and a common database. We implemented web-based inspectors in JavaScript and D3, with a Python back-end, to provide a maximum of flexibility and because we were familiar with the development stack. Ideally, inspectors provide nearly instantaneous feedback during the interaction. Thus, model predictions or algorithm output is either preprocessed on all available data or computed live on the current sample.</figDesc><graphic coords="9,53.75,49.25,250.94,77.90" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank our collaborators at Tableau and appreciate feedback from Madison Elliot, Steve Kasica, Zipeng Liu, Ben Shneiderman, and Mara Solen.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic summary generation for scientific data charts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Al-Zaidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
				<meeting>AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="658" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ModelTracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</title>
				<meeting>ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Covid-19 viz roundup (Observable notebook)</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Armstrong</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using strahler numbers for real time visual exploration of huge graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Auber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Computer Vision and Graphics</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparison of visual and textual page previews in judging the helpfulness of web pages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fontes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on World Wide Web</title>
				<meeting>Int. Conf. on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparative evaluation of animation and small multiples for trend visualization on mobile phones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="374" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding infographics through textual and visual tag prediction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09215</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Augmenting web search surrogates with images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Capra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Information and Knowledge Management</title>
				<meeting>ACM Int. Conf. Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR Conf. Research and Development in Information Retrieval</title>
				<meeting>ACM SIGIR Conf. Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A user-based visual analytics workflow for exploratory model analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Humayoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="185" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abstractive snippet generation</title>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1309" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing large time-series data on very small screens</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics Conf. Visualization (EuroVis)</title>
				<meeting>Eurographics Conf. Visualization (EuroVis)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diagramflyer: A search engine for data-driven diagrams</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on World Wide Web</title>
				<meeting>Int. Conf. on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="183" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual (dis) confirmation: Validating models and hypotheses with visualizations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Raveendranath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westerfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Information Visualization</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating interfaces for intelligent mobile search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Keane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Web Accessibility (W4A): Building the Mobile Web: Rediscovering Accessibility?</title>
				<meeting>Workshop on Web Accessibility (W4A): Building the Mobile Web: Rediscovering Accessibility?</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ExplainExplore: visual exploration of machine learning explanations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Collaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Pacific Visualization Symp</title>
				<meeting>IEEE Pacific Visualization Symp</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Code thumbnails: Using spatial memory to navigate source code</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. on Visual Languages and Human-Centric Computing</title>
				<meeting>IEEE Symp. on Visual Languages and Human-Centric Computing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Do thumbnail previews help users make better relevance decisions about web search results?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dziadosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chandrasekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR Conf. Research and Development in Information Retrieval</title>
				<meeting>ACM SIGIR Conf. Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="365" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimedia thumbnails for documents</title>
		<author>
			<persName><forename type="first">B</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Berkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Multimedia</title>
				<meeting>Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The impact of summaries: What makes a user click</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Fachry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Dutch-Belgian Information Retrieval Workshop</title>
				<meeting>Dutch-Belgian Information Retrieval Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Previews and overviews in digital libraries: Designing surrogates to support visual information seeking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="380" to="393" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video2Gif: Automatic generation of animated gifs from video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1001" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graphical histories for visualization: Supporting analysis, communication, and evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1189" to="1196" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Techniques for flexible responsive visualization design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</title>
				<meeting>ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding and visualizing data iteration in machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Kery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</title>
				<meeting>ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How people recognise previously seen web pages from titles, urls and thumbnails</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kaasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">People and Computers XVI-Memorable Yet Invisible</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="247" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Glyphs for exploring crowd-sourced subjective survey classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kachkaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dykes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Thumbnails for data stories: A survey of current practices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference (VIS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="116" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Literate programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="111" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A workflow for visual diagnostics of binary classifiers using instance-level explanations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aphinyanaphongs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<meeting>IEEE Conference on Visual Analytics Science and Technology (VAST)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Summary thumbnails: readable overviews for small screen web browsers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</title>
				<meeting>ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="681" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving relevance judgment of web search results with image excerpts</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. World Wide Web</title>
				<meeting>Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autocaption: An approach to generate natural language description from visualization automatically</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Pacific Visualization Symp</title>
				<meeting>IEEE Pacific Visualization Symp</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="191" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards better analysis of machine learning models: A visual analytics perspective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregated dendrograms for visual comparison between many phylogenetic trees</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2732" to="2747" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10441</idno>
		<title level="m">Synthetically trained icon proposals for parsing and summarizing infographics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Effect of snippets on user experience in web search</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arapakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Human Computer Interaction</title>
				<meeting>Int. Conf. on Human Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Judging a book by its cover: interface elements that affect reader selection of ebooks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Buchanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vanderschantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Timpany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Australian Computer-Human Interaction Conference</title>
				<meeting>Australian Computer-Human Interaction Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">PyEnchant spellchecking library</title>
		<author>
			<persName><forename type="first">D</forename><surname>Merejkowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kelly</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/pyenchant/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ProtoSteer: Steering deep sequence model with prototypes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="238" to="248" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Describing complex charts in natural language: A caption generation system</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="467" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An iterative multi-criteria optimization of product snippets enhanced by feature extraction from online reviews</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Data Science and Knowledge Engineering for Sensing Decision Support</title>
				<meeting>Conf. Data Science and Knowledge Engineering for Sensing Decision Support</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Opening the black box: Strategies for increased user involvement in existing algorithm implementations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mühlbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gratzl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1643" to="1652" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGNLL Conf. on Comp. Natural Language Learning</title>
				<meeting>SIGNLL Conf. on Comp. Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Comment ranking diversification in forum discussions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth (2017) ACM Conference on Learning@ Scale</title>
				<meeting>the Fourth (2017) ACM Conference on Learning@ Scale</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="327" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">VizCommender: Computing text-based similarity in visualization repositories for content-based recommendations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oppermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="495" to="505" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<ptr target="https://jupyter.org" />
	</analytic>
	<monogr>
		<title level="j">Project Jupyter. Jupyter notebooks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Keyword extraction for text characterization. Natural language processing and information systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ficzay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hitzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Understanding inverse document frequency: On theoretical arguments for IDF</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Insights from experiments with rigor in an EvoBio design study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Harmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">What Do We Talk About When We Talk About Dashboards?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bartram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2864903</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="682" to="692" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual parameter space analysis: A conceptual framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heinzl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2161" to="2170" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">To click or not to click: Automatic selection of beautiful thumbnails from videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Redi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Information and Knowledge Management</title>
				<meeting>ACM Int. Conf. Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="659" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improving diversity in image search via supervised relevance scoring</title>
		<author>
			<persName><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Ginsca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia Retrieval</title>
				<meeting>ACM Int. Conf. Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="323" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A review: color feature extraction methods for content based image retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wadhvani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gyanchandani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal Computational Engineering &amp; Management</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="9" to="13" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Document thumbnails with variable text scaling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stoffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3pt3</biblScope>
			<biblScope unit="page" from="1165" to="1173" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Document cards: A top trumps visualization for documents</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rohrdantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stoffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1145" to="1152" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visual snippets: summarizing web pages for search and revisitation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</title>
				<meeting>ACM SIGCHI Conf. on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2023" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A survey of explanations in recommender systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Data Engineering</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="801" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Albireo: An interactive tool for visually summarizing computational notebook structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wenskovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Visualization in Data Science (VDS)</title>
				<meeting>IEEE Visualization in Data Science (VDS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The What-If Tool: Interactive probing of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The usefulness of multimedia surrogates for making relevance judgments about digital video objects</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Wildemuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">102091</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Alternative surrogates for video objects in a digital library: users&apos; perspectives on their relative usability</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Wildemuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marchionini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Theory and Practice of Digital Libraries</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="493" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Mobile-VisFixer: Tailoring web visualizations for mobile phones leveraging an explainable reinforcement learning framework</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Graph thumbnails: Identifying and comparing multiple graphs at a glance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yoghourdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wybrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3081" to="3095" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Automatic generation of music thumbnails</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samadani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Multimedia and Expo</title>
				<meeting>Int. Conf. on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
