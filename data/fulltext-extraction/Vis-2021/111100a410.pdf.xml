<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lu</forename><surname>Ying</surname></persName>
							<email>yingluu@zju.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tan</forename><surname>Tang</surname></persName>
							<email>tangtan@zju.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yuzhe</forename><surname>Luo</surname></persName>
							<email>yzluo@zju.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lvkeshen</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Xie</surname></persName>
							<email>xxie@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
							<email>lingyun.yu@xjtlu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yingcai</forename><surname>Wu</surname></persName>
							<email>ycwu@zju.edu</email>
						</author>
						<author>
							<persName><forename type="first">â€¢</forename><forename type="middle">X</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD &amp; CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Sport Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong-Liverpool University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">48E13A9DE20EDF087C51DC16FCFA9FE7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Glyph-based visualization</term>
					<term>machine learning</term>
					<term>automatic visualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Circular glyphs in (a1) EnsembleLens [68], (b1) SmartAdP [35], (c1) VisMatcher [28], (d1) VAICo [51], (e1) SeqDynamics [65], (f1) Visual IVO Editor [38], (g1) DropoutSeer [8], (h1) MutualRanker [34]. (a2)-(h2) shows the circular glyphs generated by GlyphCreator based on (a1)-(h1), respectively. Dashed arrows are used to associate glyph components with corresponding categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Glyphs are widely used for multidimensional data visualization <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b66">67]</ref>. Circular glyphs, one common layout type, are round and encode data using a polar coordinate system. Due to their artistic appearance and effectiveness at encoding in angle and radian channels <ref type="bibr" target="#b18">[19]</ref>, circular glyphs are one of the most commonly used visualizations, and make up 69% of glyphs in one large corpus that combines images from multiple visualization publications <ref type="bibr" target="#b12">[13]</ref>. Visualization practitioners in various domains, including E-learning <ref type="bibr" target="#b7">[8]</ref>, urban applications <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b59">60]</ref> and social media <ref type="bibr" target="#b5">[6]</ref> prefer to use circular glyphs. For instance, TopicPanorama <ref type="bibr" target="#b56">[57]</ref> uses circular glyphs to encode uncertainty, and StreamExplorer <ref type="bibr" target="#b61">[62]</ref> adopts them to represent streaming data.</p><p>However, because there is such a large design space, creating circular glyphs from scratch is not easy <ref type="bibr" target="#b19">[20]</ref>. For users with adequate visualization experience, the creation of a circular glyph involves a two-step approach: 1) design and 2) implementation. To ensure aesthetics and intelligibility, the design step should include a complete process of evaluation, refinement, and standardization, which requires artistic skills and creativity <ref type="bibr" target="#b16">[17]</ref>. This task is quite challenging for junior visualization researchers, and even experts need to iteratively refine the design to achieve a satisfactory result. In the implementation step, users must bind data attributes to visual channels. General-purpose graphic design software (e.g., Adobe Illustrator) provides limited support for data binding. Designers usually need to customize glyph design instances for different data samples, which is tedious and time-consuming. Although computer programming can address the issue of reusability, a large group of designers might encounter difficulty when programming complex layout computations for the glyphs. To ensure design flexibility and ease of implementation, researchers have proposed several tools for creating data visualizations. However, some tools support regular charts but not circular glyphs (e.g., Lyra2 <ref type="bibr" target="#b72">[73]</ref>), while some require users to perform complex operations, such as manually initializing all visual elements (e.g., Charticular <ref type="bibr" target="#b45">[46]</ref>). A systematic learning process for the tools should also be considered.</p><p>With these challenges in mind, we attempt to address both aspects of the problem. For the design step, previous research paradigms (e.g., Text-to-Viz <ref type="bibr" target="#b44">[45]</ref>) have shown that using an existing circular glyph as a reference to create another one <ref type="bibr" target="#b28">[29]</ref> is an effective method. Making comments on a glyph is easier for users than creating one from scratch. Users can take the essence of the initial glyph and try to modify the design and create a similar one, which can shorten the creation time while maintaining quality. We improve the implementation step through automated mapping of data and visual elements. Visual elements in the reference circular glyph bitmap can be analyzed, and data can be bound into these elements automatically to simplify the creation process. To maximize convenience, we imagine whether the analysis and binding processes can be connected without human effort. Similar to data extraction, a model can help parse data from images. Overall, we envision a deep learning model that can deconstruct an online circular glyph example and use it to generate new circular glyphs. However, two critical obstacles exist:</p><p>Lack of Dataset. We currently lack a circular glyph dataset large enough to train a deep neural network. Although the percentage of circular glyphs among all glyphs is large, the overall image number in the VisImages dataset <ref type="bibr" target="#b12">[13]</ref> is only 251, much smaller than the number required to train a model.</p><p>Model Architecture. It is difficult to automatically analyze and deconstruct a circular glyph bitmap. Previous research treated glyphs as entire units, and little work has been conducted to analyze the individual visual entities that make up a circular glyph. Moreover, due to current machine learning models' focus on natural images, no existing model allows a machine to deconstruct a circular glyph into pixels.</p><p>To address these challenges, we need to explicate the compositional elements of circular glyphs and consider the different ways in which data and visual elements are mapped.</p><p>Due to the aforementioned lack of data, it is difficult to extract correlations between visual elements and data. Therefore, we consider the possibility of understanding a circular glyph by analyzing the layout of visual elements. We propose a novel approach for automatically generating a new circular glyph based on imitation by deconstructing an existing circular glyph bitmap. We collect circular glyphs to explore their general patterns and the overall design space. To address the dataset shortage, we generate 11k circular glyph bitmaps based on our design space and train a neural network to interpret the bitmap images automatically. The model aims to detect all possible visual elements in the bitmap image and draw out the embedded layout. Then we implement GlyphCreator, which integrates the deconstruction model into the authoring process for circular glyphs. The major contributions of this study are as follows:</p><p>â€¢ We build a circular glyph dataset that incorporates all circular glyphs collected from VisImages <ref type="bibr" target="#b12">[13]</ref> and other possible combinations covered by our design space. This dataset is available in https://github.com/GlyphCreator/GlyphCreator. â€¢ We propose a framework for circular glyph deconstruction.</p><p>â€¢ We develop GlyphCreator, a system for automatically creating circular glyphs, and demonstrate it through a usage scenario. We also validate its usability through user interviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Here we summarize prior studies that have covered understanding visualization with deep learning, glyph-based visualizations, and currently available authoring tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Understanding visualizations through deep learning</head><p>Understanding visualizations is a common task, and an increasing number of studies have applied machine learning methods to this task.</p><p>Researchers have pursued several directions including data extraction, visualization redesign, and generating visualizations from data. Researchers looking into data extraction have focused on different visualization types. Kembhavi et al. <ref type="bibr" target="#b24">[25]</ref> devised a method based on long short-term memory architecture to parse the structure of diagrams. Siegel et al. <ref type="bibr" target="#b52">[53]</ref> parsed figures from extant research (mostly line charts) by using a convolutional neural network (CNN)-based metric. Cliche et al. <ref type="bibr" target="#b10">[11]</ref> introduced Scatteract, a system that extracts data from scatter plots through deep learning techniques and optical character recognition. Poco et al. <ref type="bibr" target="#b43">[44]</ref> recovered visual encodings from chart images by using an end-to-end pipeline. Other systems, such as ChartSense <ref type="bibr" target="#b23">[24]</ref>, adopt a semi-automated approach, aiming to extract data from various chart images by combining human interaction with CNN.</p><p>With regard to redesign, iVoLVER <ref type="bibr" target="#b38">[39]</ref> requires users to perform accurate chart interpretations through interactive annotation and builds new visualizations with the data. Poco et al. <ref type="bibr" target="#b43">[44]</ref> recovered color mapping by providing an annotation interface and recolored a new image. Sun et al. <ref type="bibr" target="#b53">[54]</ref> trained a dual conditional generative adversarial network (GAN) to colorize the contours of icons. Recently, Chen et al. <ref type="bibr" target="#b8">[9]</ref> used an end-to-end deep neural network (DNN) to extract a timeline template from a bitmap image and generated new timeline infographics. Ma et al. <ref type="bibr" target="#b35">[36]</ref> located and identified charts within an input image by using a learning-based model. Zhou et al. <ref type="bibr" target="#b70">[71]</ref> used a neural network-based method to reverse engineer bar charts. Savva et al. <ref type="bibr" target="#b49">[50]</ref> introduced a pioneer system called ReVision that identifies a chart type by using the SVM model, extracts visual elements and data, and automatically generates and outputs redesigned visualizations. However, the mark extraction in ReVision can only handle regular charts with a single mark type, such as bar charts, pie charts, and scatterplots. ReVision might fail to produce circular glyphs, which are usually composed of different types of visual elements and have complex layouts, sometimes with overlapped elements.</p><p>In addition to redesigning visualizations by generating them from images, redesign through data generation is also common. Wang et al. <ref type="bibr" target="#b57">[58]</ref> proposed DataShot, which automatically creates fact sheets from tabular data. Shi et al. <ref type="bibr" target="#b51">[52]</ref> introduced a visual story-generating system that uses tabular data. They integrated the Monte Carlo tree search algorithm into their system. Cui et al. <ref type="bibr" target="#b11">[12]</ref> selected natural language statements as inputs and automatically generated infographics automatically. Qian et al. <ref type="bibr" target="#b44">[45]</ref> utilized online blueprints to generate infographics by imitating examples using input text.</p><p>All existing approaches focus on how to understand visualization via deep learning methods. Our approach also belongs to this category. However, we focus on glyphs, an underexplored visualization type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Glyph-based visualization and authoring</head><p>Presenting multivariate data <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b68">69]</ref> is not easy. Glyph-based visualization is a common and effective format for presenting multivariate data <ref type="bibr" target="#b2">[3]</ref>. However, to design and create an effective glyph is not trivial.</p><p>Therefore, the question of how to design a good glyph is an important problem for researchers. Ward <ref type="bibr" target="#b58">[59]</ref> proposed a glyph generation pipeline that includes mapping data with visual elements and layout options. Borgo et al. <ref type="bibr" target="#b2">[3]</ref> provided a state-of-the-art approach that focuses on glyph-based visualization and summarized design guidelines and techniques. Fuchs et al. <ref type="bibr" target="#b19">[20]</ref> extended the guidelines from a different perspective by reviewing experimental studies, and obtained a deeper understanding of the glyph design space. Researchers have also explored glyph design in specific fields. Ropinski et al. <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref> built a glyph taxonomy and conducted a survey on medical visualization. In the biological field, Maguire et al. <ref type="bibr" target="#b16">[17]</ref> proposed a systematic approach for glyph design and demonstrated the approach through biological experiments.</p><p>Glyph-based visualization has been widely used in various fields, such as geo-information <ref type="bibr" target="#b15">[16]</ref>, sports <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56]</ref>, and 3D visualization <ref type="bibr" target="#b9">[10]</ref>. Moreover, several systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref> use circular glyphs to represent multidimensional data. Many authoring tools have been proposed to help users create glyphs for their data. Ribarsky et al. <ref type="bibr" target="#b46">[47]</ref> developed Glyphmaker, a system that allows novice users to easily design a glyph. Recently, Xia et al. <ref type="bibr" target="#b63">[64]</ref> introduced DataInk, a system that supports the creation of glyphs by adopting a pen-and-touch interactive input. Ren et al. <ref type="bibr" target="#b45">[46]</ref> focused on layout and presented Charticular, an authoring tool that supports glyph generation. They divided the layout into chart-and glyph-levels and focused on the layouts of glyph-based visualization. However, we are also interested in the layout of glyphs, such as the visual elements that make up glyphs. Users need to create with these tools from scratch, which is a tedious task. We opt to imitate a bitmap glyph image and accomplish the data mapping process automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PIPELINE OF GLYPHCREATOR</head><p>In this section, we first give an overview of GlyphCreator. Then we present the entire glyph creation process, including the construction of a circular glyph dataset and the model for layout deconstruction. We aim to help users design and create circular glyphs through a novel example-based approach. Our method identifies the structure and components of circular glyphs, and then creates a circular glyph according to the input example glyph and its structure and components. To automatically extract the structure and components, we need to train a detection model, which requires a dataset of circular glyph images. However, due to the incomplete investigation of circular glyphs, how to interpret the layout of circular glyphs remains unknown. With these considerations, we propose a working pipeline: generating a circular glyph dataset, training the machine, extracting the layout of the input example, and creating circular glyphs based on the extracted layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>â€¢ Circular Glyph Dataset. Our goal is to construct a dataset that contains a diversity of circular glyphs. We initially select the VisImages dataset <ref type="bibr" target="#b12">[13]</ref> since it is regarded as one of the complete datasets of visualization images. However, the number of circular glyphs in this dataset is still much smaller than the data size required by the training system. To solve this issue, we explore the design space of existing circular glyphs by analyzing the relationships between different visual elements. We list all possible variables that combine to compose a circular glyph, and randomly generate 10k images with annotations based on this design space to form the final dataset. â€¢ Layout Deconstruction. We decide to deconstruct a circular glyph layout automatically via a two-step method (Fig. <ref type="figure" target="#fig_0">2(b)</ref>). First, partial information is detected as visual elements. Second, the whole layout is obtained by mapping the detection results within the circular glyph design space. Based on the aforementioned VisImages dataset, we train a deep learning model to detect all the visual elements in a bitmap and to obtain the bitmap's center point. With the output label, corresponding bounding box, and center point, we calculate the distance between the center point and each element and ultimately acquire a layout by mapping the design space's detection results. â€¢ GlyphCreator. We propose GlyphCreator, an example-based automatic system for circular glyph generation (Fig. <ref type="figure" target="#fig_0">2</ref>(d1)). Users input an example circular glyph image that they like, as well as their data.</p><p>Our system deconstructs the input glyph into components and binds the user's data with these components. As the output, the system generates a list of circular glyph candidates. After selecting a satisfactory glyph, users can export the result in code form (Fig. <ref type="figure" target="#fig_0">2(d2</ref>)) and use it in their system with minimal effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Circular Glyph Dataset</head><p>We obtained a circular glyph dataset by collecting existing images in this category, exploring their design space, and generating various circular glyphs with annotations to form the final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Circular glyphs in VisImages dataset</head><p>To collect circular glyphs, we started with existing image datasets in visualization. Several datasets, such as MassVis <ref type="bibr" target="#b3">[4]</ref>, Beagle <ref type="bibr" target="#b1">[2]</ref>, and VisImages <ref type="bibr" target="#b12">[13]</ref>, collect numerous visualization images and classify them by design type. We selected the VisImages dataset <ref type="bibr" target="#b12">[13]</ref> as our data source because it collects images from top visualization conferences and journals, such as IEEE VAST and InfoVis. Moreover, it specifically contains a "glyph" category, with 251 images. We filtered the circular glyphs according to two criteria: â€¢ C1 images that have at least one visual element with radial contour whether intact or not; â€¢ C2 images that have at least two kinds of visual elements. Based on these criteria, three co-authors of this paper reviewed all 251 images and identified circular glyphs individually. The co-authors reached a consensus on 201 (80%) of the images and ended up with 162 circular glyphs. Within the remaining 50 images, some glyphs were difficult to recognize because they were small or blended into the background. After a thorough discussion of these images, the coauthors identified 16 circular glyphs. Finally, a total of 178 images were selected as circular glyphs that met the aforementioned criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Analysis of circular glyphs</head><p>To explore the design space for state-of-the-art circular glyphs, we started with the template identified by Maguire et al. <ref type="bibr" target="#b16">[17]</ref>, which divides a glyph into three regions; namely, mainbody, exterior, and interior. Although this template focuses on a particular design, this breakdown can be extended to the circular glyph layout in general. Four visualization researchers (co-authors of this work) with rich experience in designing and using glyphs participated in this entire process. Because having too many pictures or iterative rounds might keep them from reaching a conclusive design space, we selected a subset of 20 glyphs and checked whether it could work as well as the full set of 178 glyphs. Based on the selected images, we refined the design space over several iterative rounds, each consisting of three steps: deconstruction and classification, discussion, and refinement.</p><p>â€¢ Deconstruction and classification. We analyzed all selected images based on the template proposed by Maguire <ref type="bibr" target="#b16">[17]</ref> and the current design space, which includes deconstructing the circular glyph into visual elements and classifying all visual elements as belonging to one of these types. â€¢ Discussion. Through the results of the previous stage, we identified difficulties with the current design space. The segmentation degree of visual elements was unified, such as individual sectors in pie charts.</p><p>If visual elements could not be classified within the current design space, we extended the space. â€¢ Refinement. To solve the difficulties of each step, we refined the definition of visual elements and improved the design space. All participants reached a consensus about all visual elements, and identified a design space that can be applied to all 20 circular glyph images. We divided all visual elements into four categories; namely, chart, shape, label, and icon. Chart uses graphic figures for data visualization, such as pie charts or line charts <ref type="bibr" target="#b22">[23]</ref>. When acting as a visual element in a glyph, it is not necessary for a chart to show all the components it normally would, such as axes or a legend. Shape refers to a basic geometric object, such as a line, a circle, or a polygon. Icon refers to a small pictogram or ideogram. Label refers to a text label. After examining all circular glyphs in the dataset, we further discovered diverse patterns in the dominant chart category and categorized them into two sub-categories: circular and variant charts. Circular charts, like donut charts and pie charts, use a polar coordinate system. Others, such as heatmaps and boxplots, need to be transformed in order to achieve a circular shape and be placed in a polar coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. A polar coordinate system</head><p>Without knowing the data underlying the image, we distinguished the visual elements based on their relative positions. If C1 is satisfied, the entire circular glyph can be placed within a polar coordinate system. Therefore, we focused on parameters R and Î¸ to locate each visual element. If several visual elements have the same sub-category and their R values are almost the same, they can be regarded as the same layer. For instance, in Fig. <ref type="figure">1</ref>(d1), the three peripheral circles are regarded as one layer. Then, we divided a circular glyph into several layers. After examining all the collected circular glyph images, we classified all layers into three regions: interior, intermediate, and exterior. As a result, we have obtained a design space that covers the full collected circular glyph dataset, as shown in Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Generating training dataset</head><p>DNN requires a large amount of high-quality training data to ensure the accuracy and effectiveness of the detection model. We need a large and diverse collection of circular glyphs. To build our training dataset, we used D3 <ref type="bibr" target="#b4">[5]</ref> to generate circular glyphs in two ways: by finding all possible layouts of circular glyphs within the design space, and then iterating various styles of a particular layout. â€¢ Layout. We analyzed existing circular glyphs found within our design space. We found that the most common number of layers are two, three, and four, corresponding to zero, one, or two intermediate layers, one interior layer, and one exterior layer. To obtain all possible circular glyph layouts, we enumerated all visual representations for each layer. Therefore, the number of combinations was:</p><formula xml:id="formula_0">C 1 n i Ã— (C 0 n m + C 1 n m + C 2 n m ) Ã— C 1 n e ,</formula><p>where n i , n m , and n e are the numbers of visual representations of the interior, intermediate, and exterior layers. We manually checked the reasonableness of each combination.</p><p>â€¢ Style. We examined the size, position, and color of all visual elements that make up a circular glyph. For each element, we determined the number of parameters, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>(b). We changed red, green, and blue channels based on RGB color space to ensure color randomness. Based on a polar coordinate system (Fig. <ref type="figure">3</ref>  size dimension, like pie charts or circles in the interior layer, then only R is required. For elements that have two size parameters (e.g., donut charts), R1 and R2 are essential. For bar charts, Î”R = (R1 âˆ’ R2) is regarded as the parameter, and for arc, Î”R indicates the thickness. We randomly generated data and assigned them as the parameters of corresponding visual elements (Fig. <ref type="figure" target="#fig_3">4(b)</ref>) for data encoding based on all styles. We used this process to create 10k circular glyphs.</p><p>Annotation. All detection tasks require an annotated dataset. We used the same metrics as in the Microsoft Common Objects in COntext (MS COCO) dataset <ref type="bibr" target="#b30">[31]</ref>, which is cost-efficient and of high quality, to unify the format of annotations. At the labeling stage, we outlined the bounding box of each visual element. When drawn by D3 <ref type="bibr" target="#b4">[5]</ref>, labeling all visual elements through direct calculation was not time-consuming. Finally, each circular glyph was converted from SVG to PNG format and annotated with its representation and layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Layout Deconstruction</head><p>Due to the absence of fixed rules governing the styles and layouts of circular glyphs, parsing one to extract layout information is difficult . We achieve this goal by following two steps: first, obtaining the partial visual element information, second, elucidating the overall layout. The first step involves the detection of visual elements in a circular glyph. The second means obtaining the entire layout by locating the polar coordinate system and mapping the elements with the design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Detecting Visual Elements</head><p>Object detection is a popular and well-developed task within the computer vision domain. It is similar to ours: identify objects in an image with their associated category and outline the profile. However, object detection focus on natural images, whereas we are interested in circular glyph visualization specifically. The detection task is simpler for abstract images compared to natural ones due to clear boundaries. We defined 12 types of target visual elements (Fig. <ref type="figure" target="#fig_3">4</ref>) that may be present in a circular glyph. The 12 categories include: donut charts, rose charts, radar charts, pie charts, sunburst charts, gauge charts in the circle chart category <ref type="bibr" target="#b5">(6)</ref>; icon (1); polygons, circles and arcs in the shape category (3); boxplots and heatmaps in the variant chart category <ref type="bibr" target="#b1">(2)</ref>. Our main goal is to deconstruct the layouts of circular glyphs by detecting all visual elements -in other words, what appears and where.</p><p>A possible solution is to train an existing object detection model using our dataset. State-of-the-art detectors can be broadly divided into one-stage detectors, which have high speed, and two-stage detectors, which have high accuracy. As a one-stage anchor-free detector without post-processing, CenterNet <ref type="bibr" target="#b71">[72]</ref> can identify each object with a category and outline its bounding box in real time. It outperforms a range of state-of-the-art algorithms with a speed-accuracy tradeoff. However, for the second task of locating the polar coordinate system, the network needs modification for center localization. Moreover, CenterNet <ref type="bibr" target="#b71">[72]</ref> cannot handle two objects with the same center because they will be regarded as the same object.</p><p>Inspired by Zhou et al. <ref type="bibr" target="#b71">[72]</ref>, we built a detection model by regarding visual elements as points. With the model, we could detect all visual elements accurately in real time and simultaneously locate the origin of the polar coordinate system.</p><p>Training Dataset. Given the dataset generated in Sec. 3.2.3, we provided extra information about the origin of the polar coordinate system. To guarantee the accuracy and efficiency of the detection model, we preserved the annotation metrics used in the MS COCO dataset <ref type="bibr" target="#b30">[31]</ref>. Specifically, for each circular glyph image in the dataset, instead of giving the x, y value of the center point, we added a bounding box in the annotations whose center is the same as the origin and labeled it as Center. To avoid center point collision -two objects with the same center -two conditions regarding the size of the center bounding box in one circular glyph image must be met.</p><p>â€¢ If the image has one visual element whose center point is the same as the origin (e.g., the center circle in Fig. <ref type="figure">1</ref>(a1)), the corresponding element is annotated with another label Center. â€¢ If all visual elements' centers do not collide with the origin (e.g., the circular glyph in Fig. <ref type="figure">1</ref>(d1)), we define a new, small bounding box containing the center and annotate it with the label Center. Model Architecture. Given an RGB circular glyph bitmap I c âˆˆ R W Ã—HÃ—3 , where R refers to the bitmap, W, H refers to the width and height, 3 refers to three color channels, the model aims to predict the center position of glyph (x c , y c ) and bounding boxes {B vs } of all visual elements. We used a fully-convolutional network to obtain the feature map F from the input image I. The output prediction was downsampled by an output stride r. We selected the stacked hourglass network, up-convolutional residual networks (ResNet <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b65">66]</ref>), and deep layer aggregation (DLA <ref type="bibr" target="#b69">[70]</ref>) as candidates. After the experiment in Sec. 5.3, we employed the hourglass network as the backbone.</p><p>CenterNet <ref type="bibr" target="#b71">[72]</ref>, a network for object detection, approaches this goal through center localization and size regression by modeling an object as a single point. With feature map F extracted by the backbone, CenterNet produces a heatmap for each category using a Gaussian kernel for localization. For regression, CenterNet predicts the height and width of the object and the offset to recover the error due to output stride r. Our task includes object detection and center detection. We used a similar strategy for object detection for all categories in Fig. <ref type="figure" target="#fig_3">4(a)</ref> and considered the label Center for center detection. Focused on the position of the center, we aim to obtain the heatmap of Center. The size prediction and the offset are not necessary. Therefore, for each object K with category c k , the objective loss is</p><formula xml:id="formula_1">L k = L h , c k = Center L h + Î» size L size + Î» o f f L o f f , otherwise<label>(1)</label></formula><p>where L h is the loss of the heatmap, L size is the loss at the center point, and L o f f is the offset loss. These definitions are similar to those in Cen-terNet <ref type="bibr" target="#b71">[72]</ref>, and we refer readers to the description of CenterNet <ref type="bibr" target="#b71">[72]</ref> for additional details. We scaled the loss by two constants Î» size and Î» o f f , and set Î» size = 0.1 and Î» size = 1 in all our experiments. Total loss L is composed of individual loss L k for each object.</p><p>We used a single network to predict heatmap Y , offset O, and size S. Sharing a common fully-convolutional backbone network, all outputs are obtained, and then made to pass through a separate 3Ã—3 convolution ReLU and another 1 Ã— 1 convolution. An overview of the network input and output is shown in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Mapping Elements with Layout</head><p>After understanding the content using the network, the next step was to map the detection elements with the design space. Given the origin of the polar coordinate system, we calculated the distance of different visual elements and compared their values for mapping.</p><p>We define the distance of each visual element using two conditions. For the first condition, as shown in Fig. <ref type="figure" target="#fig_5">6</ref>(a), neither the x range nor y range of the bounding box goes through the corresponding value of the origin. The distance of this visual element is close to the linear distance of origin and the center point of the bounding box. Given the bounding box and element type, predicting the region's actual shape is difficult. Therefore, we opted to use the bounding box for calculation. If one value, whether x or y, is in the bounding box's range, as shown </p><formula xml:id="formula_2">D v = âŽ§ âŽª âŽª âŽ¨ âŽª âŽª âŽ© max i=1,2 (|x B i âˆ’ x O |, |y B i âˆ’ y O |), x O âˆˆ [x B 1 , x B 2 ], y O âˆˆ [y B 1 , y B 2 ] d(B center , O), otherwise<label>(2)</label></formula><p>where O is the origin of the polar coordinate system; B 1 , B 2 is marked in Fig. <ref type="figure" target="#fig_5">6</ref>(a)(b); B center is the center point of the bounding box; x p , y p represent the x, y value of the corresponding point p, which can be O, B 1 , B 2 ; and d(p) is the linear distance of the origin and the point. When the distance between two same-type visual elements was too small, we merged them into the same layer. We discovered that they share the same encoding mode due to falling within the same category and close position for elements of this category. Therefore, they should be bound with the same data attribute. After a thorough investigation of existing circular glyphs, we found that 5% of the radius of a circular glyph (the distance of the most distant visual element) performed well in all situations. Then, we define the distance of each layer as:</p><formula xml:id="formula_3">Dist l = arg v d (v) v âˆˆ l (3)</formula><p>Based on the definition above, we combined visual elements with distances within a small gap into the same layer. We also marked the number of elements for further generation (e.g., the circle 3 in the exterior layer in Fig. <ref type="figure">1(d1)</ref>). Finally, we obtained all layers, together with their category and distance. We sorted all distances by value and assigned the smallest to the interior, the largest to the exterior, and the middle to the intermediate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE GLYPHCREATOR SYSTEM</head><p>This section presents GlyphCreator, an automatic example-based system that facilitates the easy generation of circular glyphs. Users can use GlyphCreator to produce a satisfactory circular glyph nearly effortlessly, simply by uploading relevant data and a reference image designed by experts. The reference image ensures quality, while the tool reduces the high effort usually necessary to produce circular glyphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design Considerations</head><p>We iteratively improved our design considerations through a careful review of relevant literature and discussion with two researchers. The three primary design considerations of GlyphCreator are as follows:</p><p>DC1. Support an easy-to-learn <ref type="bibr" target="#b39">[40]</ref> and easy-to-use generating workflow. The current workflow for generating circular glyphs is timeconsuming, which inspired us to simplify the process. GlyphCreator targets all users of data visualizations, including those who excel in design and those without extensive design experience. The learning cost of a complex tool is high for general users <ref type="bibr" target="#b20">[21]</ref>. Moreover, users are not willing to spend a long time editing after they find a satisfactory reference image. It is necessary to create a tool with a low learning cost <ref type="bibr" target="#b39">[40]</ref> and easy-to-use interactions. Therefore, we designed our glyph-generating tool with a straightforward workflow and a simple editing interface.</p><p>DC2. Generate custom and appropriate glyphs quickly and easily. Different users have different style preferences. Although it is relatively easy for inexperienced users to create a glyph based on an existing design, the final result will lack creativity. Therefore, we aim to support end users in creating their own uniquely styled glyphs easily and intuitively. This requires balancing two considerations: On the one hand, users should be supported in learning layouts from existing circular glyphs. On the other hand, they should have the freedom to design their own circular glyphs in accordance with their preferences.</p><p>DC3. Support a reusable <ref type="bibr" target="#b39">[40]</ref> and editable circular glyph output. In many visualization applications, multiple circular glyphs are combined to make up a larger glyph-based visualization. Thus, a tool that makes it easy to reuse <ref type="bibr" target="#b39">[40]</ref> and edit a circular glyph is attractive. For editability, users of GlyphCreator can modify data and produce a new glyph version. For reusability, users can easily use the output circular glyph in their visualization system to see the real-time effect of any edits, thus saving time that might have been spent on multiple design iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System Workflow</head><p>To support automatic example-based generation of a circular glyph, our system needs two inputs: data and an example circular glyph image. By allowing users to upload an example image, we have transformed a labor-intensive and time-consuming workflow into an automatic and time-saving one, in which our system easily understands the reference image and binds data with the visual elements.</p><p>Following DC1, users first upload the multivariate data and circular glyph bitmap to the system. Next, our deconstruction model extracts the layout and creates an initial circular glyph for the input data. The layout of a circular glyph rather than the specific style (such as the color of a visual element) is extracted following DC2.</p><p>We used a heuristic approach to pre-process the input data. For each data attribute, we first determine the number of parameters required for visual elements. Specifically, singular value (e.g., average value), paired values (e.g., min-max range), and other values require 1, 2, and n parameters, respectively. Second, we match the data attributes to visual elements according to the number of parameters (Fig. <ref type="figure" target="#fig_3">4(b)</ref>). A successful match is shown in Fig. <ref type="figure" target="#fig_7">8</ref>. For example, "the risk of developing cancer" with one parameter was matched to the color of the circle, which also has one parameter. By default, we recommend the circular glyph whose encoding is commonly used by experts. Users can then skim through all the images generated by our system and choose one for further editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">System Interface</head><p>Following DC1, we placed one function into one view. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, our system has four views, namely, Upload, Preview, Edit, and Options. In the Upload view, a user can upload data (in JSON format) and an image. Next, multivariate data are shown in detail as name and value attributes in the UploadData panel, and the bitmap image is presented in a small preview window. Then, the user can click the generate button to go to the next step. Many generated circular glyphs are shown in the Preview view and Options view. The image in the preview windows is the one with the most common encoding from the Options view. In the Options view (Fig. <ref type="figure" target="#fig_6">7(d)</ref>), only the center column (the tick column) corresponds to the valid input data. The left and right columns (with up and down arrows) highlight the encoding attribute for each visual element by making the value smaller or larger. We also present the encoding information in a text format to help users ensure the correct encoding. When hovering over the glyphs in the Options view, the encoding information can be seen as shown in Fig. <ref type="figure" target="#fig_6">7(d)</ref>. By dragging data (using the corresponding arrows in Fig. <ref type="figure" target="#fig_6">7(a)</ref>) to the visual elements in the Preview view (Fig. <ref type="figure" target="#fig_6">7(e)</ref>)), users can bind their data with the glyph. After dragging, some unmatched encoding in the Options view disappears. The user can choose one circular glyph in the Options view by selecting the suitable attribute that encodes the data. After selecting one circular glyph from the alternatives, the Preview view is replaced by the new circular glyph and corresponding details, including the layout and encoding information near the upper right-hand corner (Fig. <ref type="figure" target="#fig_6">7(e)</ref>). We developed one panel for each visual element in the Edit view (Fig. <ref type="figure" target="#fig_6">7(c</ref>)) to support small changes, such as those in color and size. Notably, we encoded the attribute with unavailable data to ensure the correctness of data expression. For charts that use multiple colors, such as pie charts, we provided users with several popular color schemes for an improved result. For the other numerical attributes, users can drag the button on the slider to change the value. Then, the well-designed circular glyph can be exported in JavaScript, following DC3. With the exported file, the user can easily create new circular glyphs with new data by calling on the DrawGlyph function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>We employ a client-server architecture to develop GlyphCreator, which comprises a backend that runs the deconstructing model to understand a circular glyph bitmap and a web interface that allows users to choose and edit glyphs. The web interface is implemented using JavaSrcipt and Vue framework, which supports uploading data and images and the editing of circular glyphs. The server side is built in python and PyTorch <ref type="bibr" target="#b42">[43]</ref>, the popular machine learning library. We also use a well-established graphic library, namely D3 <ref type="bibr" target="#b49">[50]</ref>, to render all visual elements in circular glyphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>This section presents two usage scenarios, user interviews, and a quantitative experiment to demonstrate the effectiveness of GlyphCreator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Usage Scenarios</head><p>This section presents two usage scenarios using different datasets. The first scenario demonstrates the entire process of creating tailored glyphs with GlyphCreator as performed by a junior visualization researcher, Lucy. Lucy's job is to implement a system for analyzing the worldwide distribution of cancer. The dataset <ref type="bibr" target="#b41">[42]</ref> that she is using comprises the incidence (new cases), mortality (deaths), prevalence, and region of each type of cancer in 2020. To see how GlyphCreator helps Lucy design and use circular glyphs, we lay out her workflow step by step.</p><p>Because Lucy was unfamiliar with glyph visualization, she looked for inspiration on the Internet. Cao et al. <ref type="bibr" target="#b5">[6]</ref> proposed an interesting glyph to visualize the overall behaviors of an online learner (Fig. <ref type="figure" target="#fig_0">2(a1)</ref>). The size of the inner circle of this glyph encodes an important piece of data about the learner, while the bar chart wrapped in the glyph encodes more detailed information. Lucy's data can easily be encoded by such a glyph design. Each of the world's regions can be represented by a glyph. The bar chart can be used to show the percentages of new cases represented by different cancers (e.g., breast cancer accounts for 16.8% of all cancers in Africa). Lucy chose to include the top five most prevalent cancers in the world. The ratio of new cancer cases to overall population is the most important attribute, and is thus encoded by the circle size. Moreover, Lucy decided to use colors to encode the risk of developing cancer before the age of 75. In this manner, regions with a large ratio and a high risk are highlighted.</p><p>Lucy prepared the data and wrote a JSON file containing all data dimensions for a particular region, Africa. After uploading the data, our system showed the information in bind (initially an empty box), name, and value (Fig. <ref type="figure" target="#fig_6">7</ref>). Next, Lucy saved one individual glyph image from the Cao et al. paper, then uploaded it to the GlyphCreator system. After Lucy clicked the "generate" button, she immediately noticed the circular glyph in the Preview view in the center of the webpage, the two bottom editing panels (CirclePanel and BarchartPanel), and the various images in the corresponding Options view. To create her desired encoding, she dragged the arrow of the "new case ratio" row to the bar chart in the center. Then, she noticed the options shown in three columns (Fig. <ref type="figure" target="#fig_6">7(d)</ref>). She compared the left and right images of each row to see what the glyph would look like if the value of the data increased or decreased. Moreover, when she hovered over the circular glyph image in the Options view, text appeared describing how the data were encoded. For example, total risk -circle -color indicates that the total risk data will be bound with the color of the inner circle.</p><p>Given the previous considerations, Lucy decided to check the Options view to find a glyph version that she likes. She chose the encoding total risk -circle -color, new case ratio -bar chart -size, new case/population -circle -size. The mappings are shown in Fig. <ref type="figure" target="#fig_7">8</ref>.</p><p>The size and the color of the circle encode the data labeled "new case/population" and "total risk," respectively. The size of the bar chart represents the proportion of new cases of different types of cancer to the overall number of new cancer cases in Africa.</p><p>After seeing the default glyph in the Preview view, Lucy decided to make several changes. Satisfied with the inner circle, she chose to adjust the bar chart. Excluding the size encoded with data, she changed the bar chart's starting angle by dragging the small button in the corresponding slider (Fig. <ref type="figure" target="#fig_6">7(e)</ref>).</p><p>Ultimately, she exported the circular glyph in code form, which can be easily read by her system. With the export file in JavaScript format, Lucy called on the function DrawGlyph in her system by providing the specified variables: the center point of the expected circular glyph location, the glyph size, and, most crucially, the data in a JSON format. Moreover, by iterating the functions using the data of different regions, such as Asia and Europe, Lucy drew six circular glyphs corresponding to six continents in one map with ease. The result is shown in Fig. <ref type="figure" target="#fig_8">9</ref>. Now familiar with the system workflow, Lucy went on to design other versions by making different choices about the inner circle.</p><p>To demonstrate how a more comprehensive glyph can be designed by GlyphCreator, we present another usage scenario. Andrew is a business intelligence analyst with basic data visualization knowledge. He wanted to analyze car sales over the past 12 years using Skoda's UK Used Car Dataset <ref type="bibr" target="#b0">[1]</ref>. He followed Lucy's approach to create a more complex tailored glyph using GlyphCreator, which he subsequently adapted into a glyph-based visualization (Fig. <ref type="figure" target="#fig_8">9(b)</ref>). In the visualization,  Andrew discovered some interesting patterns with this visualization. In general, glyphs at lower positions (oval shadow in Fig. <ref type="figure" target="#fig_8">9</ref>(b)) had a larger donut width, indicating that cars with lower mileage (between 5,000 and 40,000) sold slightly better than those with higher mileage. In the two most recent years (2018 and 2019), no car was sold with a mileage higher than 60,000 (Fig. <ref type="figure" target="#fig_8">9</ref>(b1)). However, the lowest-mileage cars in a given year do not generally have the highest sales numbers (Fig. <ref type="figure" target="#fig_8">9</ref>(b2)), likely because many of them are new cars and thus unlikely to be resold. An interesting finding is that sales of low-mileage cars increased drastically in 2019. This may be related to the fact that Skoda recalled thousands of cars in the UK that year <ref type="bibr" target="#b40">[41]</ref>. As a result, people might have lost confidence in the brand and sold their new cars, even though the models they owned were not recalled. Andrew also noticed that the glyph at the bottom right (Fig. <ref type="figure" target="#fig_8">9</ref>(b3)) has an outer arc that is significantly longer than those of the other glyphs, but the bullet position is similar to the neighboring glyphs. By exploring the data, he found that one car of a common model was sold at an extremely high price for an unknown reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">User Interview</head><p>To evaluate the effectiveness and usability of GlyphCreator, we conducted semi-structured interviews with four end users who were familiar with data visualization. The first user (U1) was a senior researcher who graduated from a professional design school. He also studied visualization and visual analysis for four years. The second user (U2) worked as a senior visualization researcher for three years. However, he did not have systematic design training. The third user (U3) was a student majoring in computer science and had little prior experience in visualization, all of which was obtained from an Information Visualization course (eight weeks, four lectures per week). These three users (U1, U2, U3) evaluated the workflow of GlyphCreator and discussed its potential applications. The fourth user (U4) was a professional UI/UX designer who worked for a visualization group. Based on her visualization knowledge and design experience, she evaluated the output circular glyphs generated by GlyphCreator. She also compared the system with various commercial design tools (e.g., Adobe AI/PS).</p><p>Each 60-minute interview began with a five-minute introduction to the system workflow. Afterward, we demonstrated GlyphCreator by going through a three-minute example case. We let users become thoroughly familiar with our system through free exploration. They were then asked to generate two circular glyphs using a given reference image and data. Afterwards, a semi-structured interview was conducted to obtain opinions on the usability and quality of our system. In the interview, the users were asked to demonstrate how to use the system to create circular glyphs. We asked about their previous experience with glyph creation. We also asked them to freely share their thoughts and suggestions about the glyphs and the system as a whole, and to point out any causes of confusion.</p><p>Overall, the users were impressed with our system's convenience and intelligence. For example, U3 said, "The process of editing a circular glyph by GlyphCreator is easy and straightforward." U2 said, " It does a great job of shortening the time for creating a glyph." U4 focused on system design, including the workflow, design of interactions, and user interface. She commented that "the interface is easy to follow and the interactions are intuitive."</p><p>The users liked the entire workflow design. Regarding the glyph generation process, U1 and U2 began from the data, whereas U3 started with the reference images. All three users (U1, U2, and U3) said they read papers to find glyph designs that could meet their needs. U1, who has solid design skills, regularly collects good glyph designs in case they come in handy. The three users are used to designing and implementing glyphs through programming. Based on their previous design experience, they expressed the need for a system for generating circular glyphs because it could reduce the iteration time. Due to an indispensable reference image, they agreed with our input, including data and image. U4 focused on the workflow of GlyphCreator. Comparing GlyphCreator with professional design tools, she commented, "GlyphCreator is more intelligent and efficient. I only needed several minutes to learn how to use the system." She also pointed out several possible improvements to the interface, such as adding hints on three columns in the Options view. We further improved our interface by providing it with a user-friendly, built-in user guide.</p><p>All users thought the system was easy to understand. We asked users to score the system on the generated circular glyphs' quality based on a 5-point Likert Scale, where 5 is the best and 1 is the worst. U1 gave a 5, and U2, U3, U4 gave a 4, which shows their high appraisal of the glyph aesthetic and its similarity to the original reference image. All users thought that the generated glyphs properly adopted the layouts of the original images. U1 commented, "The glyphs look similar to the original ones, and I can freely edit other attributes." U4, a designer, said, "From the perspective of data encoding, I think the generated picture is reasonable. Calculating the size of each component when using a professional design tool is tedious."</p><p>We also received suggestions for further improvements. First, the users suggested we consider the data range to avoid blocking each visual element. For example, when using a circle as an interior layer and a boxplot as an exterior layer (Fig. <ref type="figure">1</ref>(h1)), the circle's radius should not be larger than the boxplot's, even if users encode data on this attribute. To ensure correctness, we used a relative size for the inner element to replace the previous size. Moreover, the users suggested uploading two referred images or more to compare other possibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Experiments</head><p>Our model was implemented using PyTorch <ref type="bibr" target="#b42">[43]</ref> with four types of CNN backbone, namely, ResNet-18 <ref type="bibr" target="#b21">[22]</ref>, ResNet-101 <ref type="bibr" target="#b65">[66]</ref>, DLA-34 <ref type="bibr" target="#b69">[70]</ref> and Hourglass-104 <ref type="bibr" target="#b26">[27]</ref>. We used the Hourglass-104 network as the standard and modified both the ResNets and DLA-34 by using deformable convolution layers. To evaluate the performance of parsing a circular glyph, we adopted the average precision (AP) <ref type="bibr" target="#b17">[18]</ref> value from the precision/recall curve to access two tasks, namely, what and where. Each detection was considered true or false based on the area that overlapped with the ground truth bounding boxes. Overlap area was calculated by the formula: IoU = area(B p âˆ©B gt ) area(B p âˆªB gt ) , where B p is the predicted bounding box and B gt is the ground truth bounding box. If IoU exceeded a threshold, then we considered the detection as a true one. Without test augmentation, we evaluated our object detection performance on our circular glyph dataset, which contained 8k training images and 2k validation images. We reported the average precision over all IoU thresholds (AP) and AP at thresholds of 0.5 (AP 50 ) and 0.75 (AP 75 ). Table <ref type="table" target="#tab_0">1</ref> shows results with different backbones. According to the definition, a high AP value denotes a good detection model.  <ref type="bibr">4 20</ref> Table <ref type="table" target="#tab_0">1</ref> shows the results with different backbones. The running time was tested on a local machine, with Intel(R) Xeon(R) Platinum 8260 CPU, Tesla V100 GPU, PyTorch 1.5.0, and CUDA 10.2. Considering that all backbones take less than 0.1 seconds to deal with one image, we selected the best one. Among all backbones, Hourglass-104 had the highest accuracy with a relatively good speed. Therefore, we chose the Hourglass-104 backbone for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>This section discusses the implications and limitations of GlyphCreator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implications</head><p>Loop of Visualization and Artificial Intelligence (AI). Recently, using AI to aid in data visualization has become more common <ref type="bibr" target="#b60">[61]</ref>. We use AI to simplify the visualization task, and use visualization to help in the machine learning task, creating a loop of visualization and AI. To deconstruct a circular glyph for the former arrow automatically, we trained a detection model that satisfied our scenario. The lack of a training dataset led us to draw the second arrow. We used D3.js <ref type="bibr" target="#b4">[5]</ref> to draw circular glyphs, and saved the PNGs and annotations as the training dataset. Moreover, we utilized simple calculation as a substitute for the labor-intensive annotation process. For an improved authoring tool benefiting the community, we need a better model with high accuracy that is fast enough for quick editing. For a perfect model, a comprehensive understanding of the design space of circular glyphs for data visualization must be obtained. Through multiple iterative rounds, we achieved a well-behaved model and a tool, GlyphCreator, with high efficiency. GlyphCreator is also inspiring, providing a successful work example rooted in two fields of knowledge and thus inspiring the development of future visualization tools.</p><p>Example-based Circular Glyph Generation. As combinations of data-driven visual entities, glyphs use different visual channels to encode multiple informational dimensions. Although glyphs are effective, their complex layouts and multiple encodings can make the design and realization process difficult. Therefore, automating the generation of glyphs is a meaningful task. Compared with creating from scratch, example-based generation is more efficient. In real scenarios, users with different design skills in the visualization field can use GlyphCreator. Users with design expertise usually have a clear goal for their glyph, and want to see the final result as fast as they can for iteration. Unlike when this process required tedious drawing and binding, they can see the image only after a few minutes of operation. Moreover, they can use the glyph in their system, previewing it quickly with our tool. Users with little design experience have a clear understanding of glyphs but find the design process difficult. They want to use an existing glyph as a reference for their own data. With our tool, they can obtain their desired glyph even when they have little understanding of the original image. These scenarios lead us to believe that our example-based glyph generation method provides improved efficiency to users who need to design glyphs, even without a complete understanding of the form. Such a useful and effective means of representing multivariate data visualization could have a wide range of applications.</p><p>Application of the Dataset. Our dataset of circular glyphs can be used for other tasks, such as predicting visualization types and searching for a glyph image using keywords. For each glyph, we labeled each visual element with its category. A new model for chart prediction can be feasibly trained for all visual elements in circular glyphs. Moreover, people can use the labels in annotations to search for a desired circular glyph by inputting several keywords about the corresponding visual elements. During the design process, people aim to find a glyph with a particular visual element that is suitable for one data type, like a pie chart for proportional data. However, obtaining content results by inputting related keywords is difficult with popular search engines, such as Google. With our dataset that contains numerous images, users can acquire related images for inspiration or other possible usages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Limitations</head><p>Our approach has several limitations. First, GlyphCreator has limited support for glyph style customization. There is a default style for all users, and users must edit it manually. A possible solution is improving the model to learn the style parameters of input images, such as color scheme, and automatically configure the generated glyphs. Second, we applied our approach only to circular glyphs, but it can be potentially generalized to other glyph types. By analyzing and exploring the design space of other glyphs, we can extend our dataset and build a model for parsing all kinds of glyphs. Third, the deconstruction model is not diverse enough to support circular glyphs in other fields. We collected circular glyphs designed by experts in the visualization field. Other fields, such as journalism, could use glyphs for data representation, and these glyphs could also be collected for diversity. The deconstruction model could be utilized for many other images with high accuracy by adopting a dataset with a broadened range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we introduce GlyphCreator, an automatic system for generating circular glyphs based on an example-based method. We collected existing circular glyphs and created a design space of circular glyphs to analyze the relationship among different visual elements. With this design space, we built a circular glyph dataset and proposed a framework that includes a deconstruction model to obtain the layout of circular glyphs. By utilizing the uploaded circular glyph reference image and multi-dimensional data in GlyphCreator, users can bind the data with the layout in a straightforward manner and generate a new circular glyph after revising the style. We evaluated the deconstruction model through a quantitative experiment. We also demonstrated the expressiveness and usability of our approach through a usage scenario and user interviews. We believe that our work provides a new idea for automatically generating glyphs in all visualizations via a two-way method, a loop of visualization and AI. In the future, we plan to extend this pipeline by analyzing other glyphs and expand the authoring tools of GlyphCreator to support other creative designs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The pipeline of GlyphCreator. (a1) Image and (a2) data are input. (b) The visual encodings are decoded and the layout is extracted by a CNN model. (c) The circular glyph dataset is generated for training the model. (d1) Users use GlyphCreator to combine the data and the layout and obtain the glyph renderer (d2).</figDesc><graphic coords="3,56.15,410.45,246.36,273.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4. â€¢ Interior. The interior represents the closest layer of the circular glyph, which is indispensable. The visual element of the interior layer has three parts: chart, icon, and shape. Designers prefer pie charts (11/17) and donut charts (3/17) in the circle chart category. For the shape category, they use the circles (76/95) most. In Fig. 1(d1), the interior layer is denoted by the red polygon. In Fig. 1(a1)(b1)(c1)(h1), the center circle represents the interior, and in Fig. 1(g1), the layer denoted by the donut chart is the interior layer. â€¢ Exterior. The exterior represents the outside layer of one circular glyph. The exterior layer can be classified into three categories: shape, chart, and icon. In the shape type, circles (90/120) are mostly used, whereas variant charts (12/21), such as boxplots and bar charts, are often used in the chart type. For example, in Fig. 1(d1), the three circles are included in the exterior as the same layer. The modified dashboard in Fig. 1(b1), the circle in Fig. 1(g1) and the variant bar chart in Fig. 1(a1) also denote exterior layers. â€¢ Intermediate. The intermediate represents layers between the inte-rior and exterior. The number of layers is greater than or equal to zero. An intermediate layer includes two categories of visual elements: chart and shape. No intermediate layer exists in Fig. 1(a1)(c1)(e1)(f1)(h1). The circular glyph in Fig. 1(b1) has three intermediate layers: an arc, a variant heatmap, and a circle set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>), we used R to indicate the radial coordinate and Î¸ to indicate the angular coordinate. We combined two parameters and located the visual element's position. Notably, the elements in the interior layer have fixed positions. The same applies to all circle charts in other layers. For several variant charts (e.g., bar chart) and shapes (e.g., arc), two parameters are needed to indicate the angle: the starting angle and the angle range. When the visual element has only one parameter in the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) The design space of circular glyphs. A circular glyph can be divided into three regions: interior, intermediate and exterior. The second column indicates the contained categories of visual elements for each region. We used four different colors to represent four different categories. (b) The number of parameters for each visual element is shown in three dimensions: color, position and size. One number indicates a fixed number of parameters. "a | b" means "a" or "b".</figDesc><graphic coords="4,311.63,414.53,242.33,249.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Model architecture to detect visual elements and the center point. The backbone network extracts the feature map from the input image. Four neural networks then predict the bounding box and center location separately.</figDesc><graphic coords="5,321.59,49.49,240.56,116.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Process of mapping each visual element with the layout. The first step is to calculate the distance for each visual element based on all bounding boxes and a center point. (a) The bounding box without the center point and (b) the bounding box containing the center point correspond to the two conditions in Equation. 2. (c) The elements are combined. (d) The final layout is obtained.</figDesc><graphic coords="6,47.51,49.49,245.47,136.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. GlyphCreator: (a) Data and (b) image can be upload. (c) Users can manipulate each visual element in individual editing panels. (d) Options for different encodings are shown to users. (e) Users can preview the generated circular glyph design and (f) export it.</figDesc><graphic coords="6,307.67,49.49,250.45,157.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. A mapping example between visual elements and input data. The numbers in dark green boxes show the parameter number for each attribute or each data dimension. The arrow indicates the mapping.</figDesc><graphic coords="7,320.87,210.89,242.04,68.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Two visualizations based on the tailored glyphs created by GlyphCreator using the worldwide cancer dataset (a) and the UK Used Car Dataset of Skoda (b).</figDesc><graphic coords="8,49.31,49.37,241.82,292.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>each glyph represents information about used cars sold over the past 12 years. The visual elements of the circular glyphs encode multivariate information about the cars. For instance, the inner donut chart shows three different types of cars (pink: semi-automatic cars, purple: manual cars and yellow: automatic cars). The width of the inner donut chart (R outer âˆ’ R inner ) shows the number of sold cars (a thicker band indicates that more cars were sold). The arc in the intermediate layer illustrates the price range, and the circle within the arc presents the average price. Glyphs are placed in a Cartesian coordinate system, where the x-axis represents the year and the y-axis represents the mileage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,77.63,130.97,465.94,331.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average precision of detecting visual elements using different backbones in the circular glyph dataset.</figDesc><table><row><cell>Backbone</cell><cell>AP</cell><cell cols="3">AP 50 AP 75 Time(ms)</cell></row><row><cell cols="3">Hourglass-104 82.6 98.4</cell><cell>94.1</cell><cell>92</cell></row><row><cell>DLA-34</cell><cell cols="2">82.2 98.2</cell><cell>93.2</cell><cell>45</cell></row><row><cell>ResNet-101</cell><cell cols="2">80.8 98.1</cell><cell>92.7</cell><cell>36</cell></row><row><cell>ResNet-18</cell><cell cols="2">79.0 97.6</cell><cell>91.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by NSFC (62072400), Zhejiang Provincial Natural Science Foundation (LR18F020001), and the Collaborative Innovation Center of Artificial Intelligence by MOE and Zhejiang Provincial Government (ZJU).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UK Used Car Data set</title>
		<ptr target="https://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes" />
		<imprint>
			<biblScope unit="page" from="2021" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beagle: Automated Extraction and Interpretation of Visualizations from the Web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mukusheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">594</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Glyph-based Visualization: Foundations, Design Guidelines, Techniques and Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laramee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics</title>
				<meeting>Eurographics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="39" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Borkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<title level="m">What Makes a Visualization Memorable? IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2306" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">D 3 Data-Driven Documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2301" to="2309" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TargetVue: Visual Analysis of Anomalous User Behaviors in Online Communication Systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="280" to="289" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ConfVisExplorer: A Literature-based Visual Analysis System for Conference Comparison</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DropoutSeer: Visualizing Learning Patterns in Massive Open Online Courses for Dropout Reasoning and Prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Visual Analytics Science and Technology</title>
				<meeting>IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="917" to="926" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multivariate Glyphs for Multi-Object Clusters</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Chlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Symposium on Information Visualization</title>
				<meeting>IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="141" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scatteract: Automated Extraction of Data from Scatter Plots</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cliche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madeka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<meeting>European Conference on Machine Learning and Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10534</biblScope>
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="906" to="916" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">VisImages: A Large-scale, High-quality Image Corpus in Visualization Publications. CoRR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AirVis: Visual Analytics of Air Pollution Propagation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="800" to="810" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual Cascade Analytics of Large-scale Spatiotemporal Data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal Visualization of Boundary-based Geo-information Using Radial Projection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Drocourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scharrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="981" to="990" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Taxonomy-Based Glyph Design -with a Case Study on Visualizing Workflows of Biological Experiments</title>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Rocca-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanna-Assunta</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2603" to="2612" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes Challenge: A Retrospective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gone full circle: A Radial Approach to Visualize Event-based Networks in Digital Humanities</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Filipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Schetinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Soursos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zapke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="60" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Systematic Review of Experimental Studies on Data Glyphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bezerianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1863" to="1879" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning, Performance, and Analysis Support for Complex Software Applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kannampallil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHCI Proceedings</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Harvard Graphics 3: the Complete Reference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>McGraw-Hill Osborne Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ChartSense: Interactive Data Extraction from Chart Images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6706" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Diagram is Worth a Dozen Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salvato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="235" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Kui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<title level="m">TVseer: A Visual Analytics System for Television Ratings. Visual Informatics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting Objects as Paired Keypoints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="765" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Designing with Interactive Example Galleries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Klemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Human Factors in Computing Systems</title>
				<meeting>ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2257" to="2266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MatchPad: Interactive Glyph-Based Visualization for Real-Time Sports Performance Analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Parry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1255" to="1264" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SmartAdP: Visual Analytics of Large-scale Taxi Trajectories for Selecting Billboard Locations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual Exploration of Urban Functional Zones Based on Augmented Nonnegative Tensor Factorization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Man</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="331" to="347" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An Uncertainty-Aware Approach for Exploratory Microblog Retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="250" to="259" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delorey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Human Factors in Computing Systems</title>
				<meeting>ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">123</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">LADV: Deep Learning Assisted Authoring of Dashboard Visualizations from Images and Sketches. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gerych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alajaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buquicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rundensteiner</surname></persName>
		</author>
		<title level="m">ARGUS: Interactive Visual Analysis of Disruptions in Smartphone-detected Bio-Behavioral Rhythms. Visual Informatics</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards Utilizing GPUs in Information Visualization: A Model and Implementation of Image-Space Operations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcdonnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1105" to="1112" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">iVoLVER: Interactive Visual Language for Visualization Extraction and Reconstruction</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>MÃ©ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nacenta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenheste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4073" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Murray</surname></persName>
		</author>
		<title level="m">Theory-based Authoring Tool Design: Considering the Complexity of Tasks and Mental Models. Design Recommendations for Intelligent Tutoring Systems. Authoring Tools and Expert Modeling Techniques</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Volkswagen, audi &amp; skoda recall: Thousands of cars suddenly lose power in dangerous fault</title>
		<author>
			<persName><forename type="first">L</forename><surname>O'callaghan</surname></persName>
		</author>
		<ptr target="https://www.express.co.uk/life-style/cars/1190969/Volkswagen-Audi-Skoda-recall-news-affected-model-list-news" />
		<imprint>
			<biblScope unit="page" from="2021" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Population Fact Sheets about Cancer</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Organization</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Py-Torch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayhua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="452" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Charticulator: Interactive Construction of Bespoke Chart Layouts</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="789" to="799" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Glyphmaker: Creating Customized Visualizations fo Complex Data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="57" to="64" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Survey of Glyph-based Visualization Techniques for Spatial Multivariate Medical Data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oeltze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="392" to="401" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Taxonomy and Usage Guidelines for Glyphbased Medical Visualization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SimVis</title>
				<meeting>SimVis</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">522</biblScope>
			<biblScope unit="page" from="121" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ReVision: Automated Classification, Analysis and Redesign of Chart Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chhajta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on User Interface Software and Technology</title>
				<meeting>the ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">VAICo: Visual Analysis for Image Comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>GrÃ¶ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2090" to="2099" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Calliope: Automatic Visual Data Story Generation from a Spreadsheet</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="453" to="463" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">FigureSeer: Parsing Result-Figures in Research Papers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="664" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adversarial Colorization of Icons Based on Contour and Color Conditions</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
				<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improving Symbolic Data Visualization for Pattern Recognition and Knowledge Discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Umbleja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ichino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Tac-Miner</surname></persName>
		</author>
		<title level="m">Visual Tactic Mining for Multiple Table Tennis Matches. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2770" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">TopicPanorama: A Full Picture of Relevant Topics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2508" to="2521" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">DataShot: Automatic Generation of Fact Sheets from Tabular Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="895" to="905" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multivariate Data Glyphs: Principles and Practice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Data Visualization</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="179" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Towards Better Bus Networks: A Visual Analytics Approach. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="817" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stream-Explorer: A Multi-Stage System for Visually Exploring Events in Social Streams</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2758" to="2772" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Towards Better Detection and Analysis of Massive Spatiotemporal Co-Occurrence Patterns</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3387" to="3402" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">DataInk: Direct and Creative Data-Oriented Drawing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R D</forename><surname>AraÃºjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wigdor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">223</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">SeqDynamics: Visual Analytics for Evaluating Online Problem-solving Dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="511" to="522" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Simple Baselines for Human Pose Estimation and Tracking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11210</biblScope>
			<biblScope unit="page" from="472" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">PassVizor: Toward Better Understanding of the Dynamics of Soccer Passes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1322" to="1331" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">EnsembleLens: Ensemblebased Visual Exploration of Anomaly Detection Algorithms with Multidimensional Data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="119" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="860" to="869" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep Layer Aggregation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Reverse-engineering Bar Charts Using Neural Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="419" to="435" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Objects as Points. CoRR, abs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904">1904.07850, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Lyra 2: Designing Interactive Visualizations by Demonstration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barnwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neogy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="314" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
