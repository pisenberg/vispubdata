<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces for Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brittany</forename><forename type="middle">Terese</forename><surname>Fasy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carola</forename><surname>Wenk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Summa</surname></persName>
						</author>
						<title level="a" type="main">A Domain-Oblivious Approach for Learning Concise Representations of Filtered Topological Spaces for Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9B07209C74D1D734D9F7DFD3E683C23D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Persistence diagrams have been widely used to quantify the underlying features of filtered topological spaces in data visualization. In many applications, computing distances between diagrams is essential; however, computing these distances has been challenging due to the computational cost. In this paper, we propose a persistence diagram hashing framework that learns a binary code representation of persistence diagrams, which allows for fast computation of distances. This framework is built upon a generative adversarial network (GAN) with a diagram distance loss function to steer the learning process. Instead of using standard representations, we hash diagrams into binary codes, which have natural advantages in large-scale tasks. The training of this model is domain-oblivious in that it can be computed purely from synthetic, randomly created diagrams. As a consequence, our proposed method is directly applicable to various datasets without the need for retraining the model. These binary codes, when compared using fast Hamming distance, better maintain topological similarity properties between datasets than other vectorized representations. To evaluate this method, we apply our framework to the problem of diagram clustering and we compare the quality and performance of our approach to the state-of-the-art. In addition, we show the scalability of our approach on a dataset with 10k persistence diagrams, which is not possible with current techniques. Moreover, our experimental results demonstrate that our method is significantly faster with the potential of less memory usage, while retaining comparable or better quality comparisons.</p><p>Index Terms-Topological data analysis, Persistence diagrams, Persistence diagram distances, Learned hashing, Clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The features quantified by topological data analysis (TDA) <ref type="bibr" target="#b22">[23]</ref> have been shown to express the fundamental structure of scalar fields in a way that is generally applicable to many domains. TDA approaches-such as persistent homology <ref type="bibr" target="#b23">[24]</ref>, contour trees <ref type="bibr" target="#b11">[12]</ref>, Reeb graphs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b54">55]</ref>, and Morse(-Smale) complexes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>-have demonstrated ability to extract meaningful structure in a variety of research applications, including 3D shape matching <ref type="bibr" target="#b13">[14]</ref>, combustion physics <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, nuclear physics <ref type="bibr" target="#b48">[49]</ref>, fluid dynamics <ref type="bibr" target="#b37">[38]</ref>, chemistry <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>, Alzheimer's disease <ref type="bibr" target="#b46">[47]</ref>, autism spectrum disorders <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b62">63]</ref>, cancer histology <ref type="bibr" target="#b43">[44]</ref>, protein folding <ref type="bibr" target="#b80">[81]</ref>, and bio-molecular analysis <ref type="bibr" target="#b51">[52]</ref>. More generally, recent work includes using TDA quantification as input to machine learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>As described in detail in Section 2.1, a persistence diagram is a common way to present the topological structure in a dataset. The distance between these diagrams is often used to measure the topological (dis)similarity between data, which has important applications in scientific visualization <ref type="bibr" target="#b82">[83]</ref>. Moreover, for approaches that cluster based on topological similarly <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b75">76]</ref>, computing the distance between diagrams is a fundamental operation. However, computing these distances is costly in practice.</p><p>The most widely accepted persistence diagram distance measures, the Wasserstein distances, require expensive matching of all persistence points between two diagrams. As discussed in Section 2, to combat this complexity, many approaches have attempted to reduce this cost. The goal of our work is the same, but provides significant advances over the state-of-the-art. As we detail in this paper, we provide a new representation for expressing topological structure that is more concise than previous works, but also leads directly to faster computations of distances. In particular, we show how to reduce diagrams to simple 64-bit binary codes. The key to this representation is a learned hash function. As we show, this hash can be learned purely from random, synthetically generated diagrams. We have found that the only constraint on generating training data is that the generated diagrams should have approximately the same average number of persistence points as are in the test data. In other words, the training is domain-oblivious with a model being potentially used on a wide variety of datasets without the need for retraining. In this new representation, distances are calculated by a simple bit-wise count comparison between binary codes (the Hamming distance). This makes distance computation extremely fast and scalable. We illustrate this scaling through the clustering of a dataset with 10k diagrams, a size which is not achievable for several existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>The specific contributions of this work are:</p><p>• A concise binary code representation of persistence diagrams that maintains topological (dis)similarity in Hamming space;</p><p>• A procedure to train the binary code hash function that can run purely on synthetic data and therefore is domain-oblivious; and</p><p>• Applications to topological clustering of real-world datasets that provide: Significant comparison speedups, potentially lower memory footprints, and comparable or better quality clustering results than other vectorized representations of persistence diagrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>This section outlines the technical background for persistent homology and hashing, as relevant to the methods developed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Persistent Homology and Persistence Diagrams</head><p>Given a dataset, we view it as a topological space or a sequence of (nested) topological spaces, called a filtration. Then, we employ homology and persistent homology, respectively, to qualitatively and quantitatively describe it. Homology is a concept from algebraic topology that captures the fundamental structure of a topological space <ref type="bibr" target="#b53">[54]</ref>. The structure is qualitatively described through the homology groups, whose generators we call features. Each feature has a dimension associated to it; dimension zero features are connected components, dimension one features are loops or tunnels, dimension two features in R 3 correspond to voids, etc. Persistent homology quantifies the homology of an entire filtration <ref type="bibr" target="#b22">[23]</ref>. In particular, each feature f i also has a birth time b i and a death time d i , indicating the parameters of the filtration for which that feature "lives"; we call the difference d i − b i the lifetime or persistence of the feature. We represent this feature as the persistence point</p><formula xml:id="formula_0">(b i , d i ) ∈ R 2 .</formula><p>Since a feature must be born before it dies with respect to the filtration parameter, persistence points are restricted to lying above the diagonal defined by the line x = y. 1 The persistence diagram is the multi-set of birth-death pairs. This abstract concept is best illustrated by describing example filtrations. For scalar fields, a common filtration is the evolution of sublevel sets. The sublevel set filtration is the progression of a watershed transformation <ref type="bibr" target="#b5">[6]</ref>, where water sources grow from local minima (i.e., basins) of the field. The zero-dimensional features (i.e., connected components) are the watersheds that begin at the local minima. Onedimensional features form when a watershed completely surrounds a local maximum (i.e., peak). The lifetimes of these features are recorded as the scalar value (i.e., water height) from where a feature first appears (birth) to the value at which it merges with an "older" feature (death). All birth and death events occur at critical points.</p><p>For unstructured points (i.e., point cloud data that is given as a discrete set of points with a pairwise distance defined), a filtration can be built from the evolution of a Vietoris-Rips complex <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b76">77]</ref>. Here, simplices are formed in the complex from an ever-increasing neighborhood around each point. In this filtration, keeping track of the connected components can detect the size and the number of distinct clusters and recording the evolution of one-dimensional features can detect the presence and size of circular features in the data. These features are agnostic to the domain of the data or even the dimension of the space. For this reason, one can say that these features represent the fundamental structure of data.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> illustrates a simple example of the zero-dimensional features of a sublevel set filtration on a scalar field with two basins. As the sublevel set increases, first the purple feature is born in the deepest basin, followed by the green feature. As the filtration continues, the Wasserstein and Bottleneck Distances Letting D denote the collection of all diagrams, the q-Wasserstein distance d q : D × D → R is defined by</p><formula xml:id="formula_1">d q (p 1 , p 2 ) := min M ∑ (a,b)∈M ||a − b|| q ∞ + 1 2 q−1 ∑ a∈M c |a x − a y | q 1/q</formula><p>, where M ranges over all matchings between persistence diagrams p 1 and p 2 , and M c is the set of persistence points in p 1 p 2 that do not appear in the matching M; see <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53]</ref>. The Wasserstein distance optimizes a matching between two diagrams and sums the distances between matched points (M) as well as the point-to-diagonal distances for unmatched points (M c ). Letting q → ∞, gives the bottleneck distance (or, interleaving distance) <ref type="bibr" target="#b17">[18]</ref>. Setting q = 1 gives the one-Wasserstein distance (W1), a popular choice in applications and therefore the target of our work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. Note that when we refer to the Wasserstein distance without specifying q, we are referring to q = 1. These diagrams are (Lipschitz) stable in the presence of slight perturbations or noise in data <ref type="bibr" target="#b18">[19]</ref>. Given this stability, diagrams that are close in distance are often considered to be topologically similar. Computationally, however, both the Wasserstein and the bottleneck distances are expensive, as they require computing the optimal matching between persistence points in the two diagrams. In particular, computing the Wasserstein distance between two diagrams takes O(n 2 log 2 n) time, where n is the total number of simplices (which, in turn, can be exponential in the size of the input data) <ref type="bibr" target="#b74">[75]</ref>.</p><p>Approximating Distances When many distances between diagrams need to be computed, the roughly quadratic computation can be daunting. Thus, several approaches have been introduced to approximate computing the Wasserstein distances <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref>. One approach that is quite successful is to simplify the input representation, before a persistence diagram is even computed <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b77">78]</ref>. However, this makes assumptions on the underlying domain (e.g., a 2D or 3D image).</p><p>Kerber et al. <ref type="bibr" target="#b39">[40]</ref> introduced an approximate Wasserstein distance algorithm to accelerate the computation of the matching using a k-d tree. This iterative computation bounds either quality or time to approximate the Wasserstein distances; we call this distance the progressive Wasserstein (PW) distance. This algorithm was extended by Vidal et al. <ref type="bibr" target="#b75">[76]</ref> for the problem of computing barycenters of persistence diagrams. Although very fast, these approaches still require the pairwise matching and, as we illustrate in Section 4.4, the memory requirements can be significant as data sizes grow.</p><p>In many circumstances, the diagrams we have are bounded, that is, there exists a square D ⊂ R 2 such that all persistence points lie inside D. Then, for a given input parameter d ∈ N, we create a d × d grid over D. Using this grid, we define a histogram, where we count the number of persistence points in each grid cell. Fasy et al. <ref type="bibr" target="#b24">[25]</ref> use these histograms to design a data structure that supports searching for near neighbors based on the bottleneck distance and Lacombe et al. <ref type="bibr" target="#b42">[43]</ref> computes Wasserstein distances (optimal transport in this space) between the histograms. While this work can benefit from several fast optimal transport computation approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b64">65]</ref>, it still poses significant costs for distance computations. We call the Wasserstein distance between histograms the Histogram Wasserstein (HW) distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other Topological Descriptors</head><p>The previous approaches for histograms can be thought of as representations of the distribution of the persistence points. In this line of research, density estimators built over the persistence points have been studied by several groups of researchers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. One benefit of considering density estimators is that they can be vectorized. With vectorized representations, the space cost of the discretization can be weighed against the speedup gained by computing L p distances between vectors instead of distances between persistence diagrams.</p><p>Persistence Images (PI) The persistence image (PI) is a discretization of a weighted kernel density estimator (a non-parametric density estimator) built on the rotated points of a persistence diagram <ref type="bibr" target="#b0">[1]</ref>). Roughly, these images estimate the density of points by summing a Gaussian kernel centered at each point. In practice, using the PI requires choosing: a bounding box, a discretization resolution, and a weight function on the set of points in the persistence diagram. Choosing these parameters can be non-trivial in practice, but various heuristics have been successfully employed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b83">84]</ref>.</p><p>Betti Curves (BC) Other topological invariants include the Euler characteristic and the Betti numbers. For a given integer k ∈ N, the k th Betti curve (BC) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b81">82]</ref> is the rank of the dimension k homology group with respect to the filtration parameter. Roughly, this is the count of the topological features present as the filtration parameter increases. Each feature associated to a persistence point (b i , d i ) contributes +1 to the Betti curve in the interval from b i to d i . Hence, the more persistent a feature, the more it contributes to the Betti curve. The L p distance between Betti curves can be computed explicitly, or can be approximated by sampling or discretizing the domain. Often, the latter approach is preferred in practice. Hence, using the Betti curves requires selecting the dimension(s) of interest, a bounding box, and a discretization resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning to Hash</head><p>Given the ability of binary codes to significantly boost distance computation for searching, hashing methods have attracted increasing attention for large-scale approximate nearest-neighbor search <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b56">57]</ref>. In this paper, we focus on using unsupervised machine learning to build a good hash function that maps high-dimensional data into lowdimensional Hamming space.</p><p>Unsupervised building of hash functions can be roughly divided into two groups: non-deep hashing and deep hashing. Typical non-deep hashing includes PCA hashing <ref type="bibr" target="#b78">[79]</ref>, spectral hashing <ref type="bibr" target="#b79">[80]</ref>, and iterative quantization <ref type="bibr" target="#b28">[29]</ref>, which all attempt to preserve a pairwise similarity of the original data in their resulting binary codes. For example, spectral hashing uses eigenfunctions of the data similarity graph to build their hash. More recently, deep hashing <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b66">67]</ref> has been introduced due to the great advances made in deep learning. The non-linear structure of a convolutional neural network (CNN) can extract multiple hierarchical feature representations of input data and learn their nonlinear relationships to build a binary representation. However, the need for data to be labeled for CNNs means that unsupervised approaches to learn a hash function cannot take full advantage of a deep learning model. Inspired by the introduction of the generative adversarial network (GAN) <ref type="bibr" target="#b29">[30]</ref>, other work focuses on the unsupervised learning of hash functions using a GAN without the need for labeled data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b65">66]</ref>. Overall, previous hashing approaches mainly focused on the image retrieval tasks, which live in Hilbert space and have nice statistical properties. In our work, we show that a natural image hashing approach <ref type="bibr" target="#b65">[66]</ref> can be used to hash persistence diagrams in non-Hilbert space. In doing so, we present the first approach to transform topological features into a binary representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING TOPOLOGICAL BINARY CODES</head><p>In this work, we explore the use of concise binary codes to represent persistence diagrams. Below, our process takes as input N persistence diagrams P = {p i } N i=1 and trains neural networks to hash persistence diagrams to binary codes. Then, the set of binary codes and their Hamming distances can be used in lieu of persistence diagrams and their Wasserstein distances. See Fig. <ref type="figure" target="#fig_2">3</ref> for an illustration of the architecture for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vectorizing Input</head><p>The first step in our hash function is to take as input a set of diagrams P = {p i } N i=1 and to convert it into a vectorized form appropriate to use as input to train networks. As mentioned in Section 2, we have several choices for vectorized representations of the input persistence diagrams, including 2D histograms using Wasserstein distance (HW) <ref type="bibr" target="#b42">[43]</ref>, persistence images using L 2 distance (PI) <ref type="bibr" target="#b0">[1]</ref>, and Betti curves using L 2 distance (BC) <ref type="bibr" target="#b26">[27]</ref>. Using the parameters outlined in the respective papers, we compare the use of these three vectorizations for clustering relative to clustering persistence diagrams using Wasserstein distance. Table <ref type="table" target="#tab_1">1</ref> provides the results using the Fowlkes-Mallows score (FMS), the evaluation measure used in this work (see Section 4.2). The scores can be in the interval [0, 1], with a score of 1 indicating a perfect match. As Table <ref type="table" target="#tab_1">1</ref> illustrates, HW provides the most accurate clustering. Therefore, we use 2D histogram vectorization for our training and hashing.</p><p>We transform our diagrams to histograms on a 2D uniform grid of size 50 × 50 on [0, 1] 2 with the entropic term: 0.1/avg N i=1 |p i | (the recommended parameter values <ref type="bibr" target="#b42">[43]</ref>). Each cell of the grid counts the number of persistence points in the diagram that lie inside each, with an additional cell that contains a count of the total number of persistence points. In addition, similar to Reininghaus et al. <ref type="bibr" target="#b57">[58]</ref>, we augment this representation by reflecting counts across the diagonal to the empty space below, see Fig. <ref type="figure" target="#fig_3">4</ref>. We found that this augmentation improves the quality of our clustering results over the standard histogram (improvement of 3% for the 3D Shape-1 dataset of Section 4.1). In summary, the first step of training our hash function is to convert each diagram p i ∈ P = {p i } N i=1 into a 2D histogram v i . In Fig. <ref type="figure" target="#fig_2">3</ref>, V denotes the set of all such histograms.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similarity Matrix</head><p>Before training our model, we need one more object: a N × N similarity matrix, S P = {s P i j } N i, j=1 , that stores all pairwise similarities between histograms in V. We explore two different methods to define similarity. Our first approach is to invert a topological distance measure. The most natural choice is to invert Wasserstein distance, since this is the distance that we would like to mimic using Hamming distances of binary codes. As our experiments in Section 4 illustrate, computing these distances for potentially thousands of training datasets is prohibitively expensive. Therefore we adopt the HW distance on V as used in Lacombe et al. <ref type="bibr" target="#b42">[43]</ref>. This gives us a computationally feasible way to build the matrix, and also aligns well with our choice of intermediate vectorization. Specifically, we compute a real-valued similarity matrix with s P i j := 1 − d 1 (v i , v j ). To give more flexibility to the learning algorithm, we also explored the use of a less rigid binary similarity matrix. Rather than using the real/original values above, the matrix is formed by setting s P i j = 1 if v i and v j are similar and s P i j = −1 if they are dissimilar. Defining this similarity can take many forms. We found that thresholding based on the closest k-nearest neighbors for each diagram was not sufficient. This had the tendency to count distant persistence diagrams as similar if a training diagram was not close to many others (also, close diagrams were treated as dissimilar if a diagram had many neighbors). We opted to use a rejection approach where all diagrams that have distance greater than the mean distance value are rejected as dissimilar. This is done in two passes on a per row (diagram) basis. This approach allows the binary labeling to have a soft threshold that maintains small distances, discounts large distances, and is less rigid than a nearest neighbor approach. As we show in Section 4, this flexible binary matrix approach outperforms real-valued distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PD-GAN Model</head><p>We now describe how we generate a 64-bit binary code by learning a hash function h : D → {0, 1} 64 , where D is the original space of diagrams. The hash is designed to maintain topological similarity when comparing binary codes with Hamming distance. Our learning framework uses the image hashing approach of Song et al. <ref type="bibr" target="#b65">[66]</ref>. Below we describe their approach and how it is used in our context. To build our hash function, two key parts are trained for the PD-GAN model, the encoder and the GAN; see Fig. <ref type="figure" target="#fig_2">3</ref>:</p><p>Encoder The encoder extracts the features of input diagrams based on a pretrained VGG19 network <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b68">69]</ref> with five groups of convolution layers with max pooling. The number of filters in each of these groups are 64, 128, 256, and 512. The output size of the last fully connected layer is the bit length L = 64. The training of the encoder is driven by minimizing a similarity loss function; see Section 3.3.1. After training, each binary code, h(v i ) = b i ∈ B, can be formed from the signs of the values of the last fully connected layer when v i is run through the encoder (i.e., h(v i ) k = sgn(x k ), where x k is the k-th value in the last layer). We call this last layer the binary-code layer. However, a non-smooth, binary representation can be problematic for the gradient computation needed in training. To avoid this problem, an intermediary, real representation of the binary code, B = {h</p><formula xml:id="formula_2">(v i )} N i=1 , is used during training: h (v i ) k = ⎧ ⎨ ⎩ +1 for x k ≥ 1 x k for 1 ≥ x k ≥ −1 −1 for x k ≤ −1,</formula><p>where k is the k-th element of the binary code. After training, the binary codes, B, can be extracted from the binary-code layer as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN: Generator and Discriminator</head><p>To improve the accuracy of the learned hash function, a GAN <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b65">66]</ref> is used. See Fig. <ref type="figure" target="#fig_2">3</ref> (blue). Specifically, the generator G can be considered as an inverse encoder, where the output of the encoder is used as the input to the network with four deconvolutional layers. G creates a set of synthetic histograms, V * , from their training codes, B . The generator's goal is to create a V * which cannot be distinguished from V by the discriminator D. G is trained by minimizing diagram loss and adversarial loss functions defined below. The discriminator informs the generator to improve V * , while the generator then informs the encoder to improve the hash function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Loss Functions</head><p>Loss functions need to be defined for the above components to minimize: similarity loss, diagram loss, and adversarial loss.</p><p>Similarity Loss Given the similarity matrix S P = {s P i j } N i, j=1 from Section 3.2 and the training codes B , the similarity loss captures a direct connection between our binary representations and topological distances. Let S B = {s B i j } N i, j=1 , where</p><formula xml:id="formula_3">s B i j = 1 L h (v i ) T h (v j )</formula><p>and L is the bit length. Then the similarity loss is defined as:</p><formula xml:id="formula_4">l sim = 1 2 ||S B − S P || 2 + ||B − B|| 2 ,</formula><p>where ||.|| are Euclidean norms.</p><p>Diagram Loss Intuitively, the diagram loss compares the generated histograms, V * , to the corresponding input histograms, V. The diagram loss function is the combination of a pixel-wise Mean Squared Error (MSE), and the perceptual loss <ref type="bibr" target="#b65">[66]</ref>. Perceptual loss is given by the last layer of the discriminator, D. Perceptual loss accounts for the observation <ref type="bibr" target="#b44">[45]</ref> that pixel-wise MSE optimization often lacks highfrequency content. We verified experimentally in our test dataset that including perceptual loss provided more accurate results. These two losses are summed to form the diagram loss: l dia = l mse + l perceptual .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Loss</head><p>The adversarial loss is designed to improve the reconstruction quality of generator G and is defined as</p><formula xml:id="formula_5">l adv = log(D(V)) + log(1 − D(V * )).</formula><p>Combined Loss Finally, the combined loss used in training is the weighted sum of the three losses: l = l sim + ω 1 l dia + ω 2 l adv , where ω 1 = ω 2 = 0.1 were used in our experiments as in Song et al. <ref type="bibr" target="#b65">[66]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Learning</head><p>To train the PD-GAN, the loss functions are minimized in the following steps. First, the training codes B are created by the encoder, using parameters φ for the VGG19 part of the encoder and W for the binary-code layer. Then the generator G reconstructs the diagrams V * with a parameter vector θ . The discriminator D uses the parameter vector σ . Back-propagation for learning and stochastic gradient descent are used to find the (locally) optimal parameters based on the loss functions. Specifically, the parameters {φ , θ , σ , W} are updated during each iteration, where τ = 0.001 is the default learning rate in our experiments:</p><formula xml:id="formula_6">W ← W − τ W (l sim + l dia ) φ ← φ − τ φ (l sim + l dia ) θ ← θ − τ θ (l dia + l adv ) σ ← σ + τ σ l adv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hash Function and Distance Computation</head><p>After training, diagrams can be hashed by first converting them into their 2D histogram representation and then running each through the PD-GAN encoder to map each to a 64-bit integer. See Fig. <ref type="figure" target="#fig_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(green).</head><p>A direct consequence of this binary encoding is that the representation is concise and distances between codes are computed in Hamming space. Distance computations are now a simple bit-wise operation: a population count of the XOR of the bits (popcount(X ⊕Y ) for binary codes X and Y ). Not only is this distance computation simple, it is also supported in hardware on modern CPUs. In Section 4 we show the speed of this computation in our standard Python implementation, but also with a C++ implementation that leverages this hardware support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>To illustrate the effectiveness of our approach, we use our generated binary representation in clustering applications. We experimented on five datasets and refer to the dataset information in Section 4.1. In order to evaluate the quality of the distance approximations of our approach, we apply a distance-based single-linkage hierarchical clustering algorithm by using the scikit-learn <ref type="bibr" target="#b55">[56]</ref> Python library. It takes a distance matrix as input, which contains all pairwise distances between histograms. The objective of single-linkage hierarchical clustering is to produce a nested sequence of partitions by successively merging clusters in a bottom-up fashion until k clusters in total are reached.</p><p>For the datasets that have undefined k topological clusters, we use the elbow method <ref type="bibr" target="#b71">[72]</ref> to determine the number of clusters. For this we deploy the hierarchical clustering for a sequential set of potential values for k and then plot the total within-cluster sum of square distances versus k (which measures the compactness of the clustering). The final number k of clusters is then chosen by the elbow of the curve.</p><p>Results are evaluated against clustering results using the Wasserstein distance. Test datasets are described in Section 4.1. Evaluation methods are detailed in Section 4.2. Next, we describe how training can be domain-oblivious in Section 4.3. Finally, performance and quality results are reported in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated the clustering of five datasets that include 3D shapes, ensemble simulation data, and 2D medical images.</p><p>3D Shape-1 <ref type="bibr" target="#b67">[68]</ref>: This dataset contains 6 different 3D shape classes including camels, horses, elephants, cats, human heads, and faces. We created 200 persistence diagrams for each class using the implementation of Carrière et al. <ref type="bibr" target="#b13">[14]</ref> to produce a Vietoris-Rips filtration. This previous work showed that there were k = 2 distinct topological clusters in this dataset. The average number of persistence points per diagram is 63, ranging from 49 to 100. Fig. <ref type="figure" target="#fig_4">5</ref> shows example shapes and a persistence diagram.</p><p>3D Shape-2 <ref type="bibr" target="#b14">[15]</ref>: This dataset contains 3D shapes across 19 object categories. 1,900 diagrams are produced in the same way as the previous dataset. The average number of persistence points per diagram on this set is 22, ranging from 10 to 78. We use the elbow method <ref type="bibr" target="#b71">[72]</ref> to find that k = 7 clusters exist. See Fig. <ref type="figure" target="#fig_5">6</ref>.</p><p>Vortex Street <ref type="bibr" target="#b75">[76]</ref>: This ensemble dataset includes 45 examples of a 2D simulation of flow turbulence behind an obstacle for k = 5 clusters of different viscosity. The average number of persistence points in this set is 22, ranging from 20 to 50. Diagrams are produced via sublevel set filtration <ref type="bibr" target="#b75">[76]</ref> using TTK <ref type="bibr" target="#b72">[73]</ref>. Fig. <ref type="figure" target="#fig_6">7</ref> shows examples and a representative persistence diagram.</p><p>Starting Vortex <ref type="bibr" target="#b75">[76]</ref>: This ensemble dataset includes 12 examples of a 2D simulation with the formation of a vortex behind a wing giving k = 2 topological clusters. The average number of persistence points of this set of persistence diagrams is 36 with 30 to 60 each. Diagrams are produced similarly to the previous ensemble. See Fig. <ref type="figure" target="#fig_7">8</ref>.</p><p>Colorectal Cancer <ref type="bibr" target="#b38">[39]</ref>: This is a set of 10,000 regions of interest images from hematoxylin &amp; eosin (H&amp;E) stained histological images with 9 classes. The average number of persistence points in this set is 498, ranging from 78 to 802. We conduct experiments on the full dataset and on a subset, since other approaches cannot run on the full data. The subset contains 200 images per class and produces 1,800 persistence diagrams in total with an average of 503 persistence points ranging from 95 to 789. Diagrams are obtained via sublevel set filtration <ref type="bibr" target="#b43">[44]</ref> using the Giotto-tda library <ref type="bibr" target="#b69">[70]</ref>. We use the elbow method <ref type="bibr" target="#b71">[72]</ref> to determine that there are k = 8 distinct topological clusters. See Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of Clustering</head><p>Given a set of input persistence diagrams, P, our approach computes a set B, of binary representations. To evaluate these binary representations, as well as other representations such as persistence images and Betti curves, we compare their clustering performance against clustering using Wasserstein distance. As it is considered the standard distance for diagrams, we treat this distance as our ground truth. Let C 1 be the set of clusters obtained by performing hierarchical clustering on P using Wasserstein. And let C 2 be a set of clusters obtained by performing hierarchical clustering on the set of vectorized representations of the persistence diagrams with their associated distances (for example B uses Hamming distance; Betti curves use distance).</p><p>Table <ref type="table">2</ref>: Running times (in seconds) for the approaches outlined in this work to compute the distance matrix of the datasets with N diagrams with average persistence points (Avg Pts). Wasserstein (W1), Hera, 2D histograms (HW), progressive Wasserstein (PW), persistence images (PI), Betti Curves (BC), and our approach are provided. The speedup of our approach compared to the next fastest is also provided. We compare these two clusterings C 1 and C 2 using the Fowlkes-Mallows score (FMS) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref> to quantify the similarity of the clustering. This is defined as the geometric mean of precision and recall:</p><formula xml:id="formula_7">FMS(C 1 , C 2 ) = T P (T P + FP)(T P + FN) ,</formula><p>where T P (true positive) is the number of pairs of persistence diagrams that belong to the same clusters in C 1 and C 2 . FP (false positive) is the number of such pairs that are in different clusters in C 1 , but in the same cluster in C 2 . FN (false negative) is the number of such pairs that are in the same cluster in C 1 , but in different clusters in C 2 . A FMS value of 1 means a perfect match with the minimum value being 0. As our experiments show in Section 4.4, our results are all close to 1. In addition, multidimensional scaling (MDS) <ref type="bibr" target="#b8">[9]</ref> plots are provided in Fig. <ref type="figure" target="#fig_4">5</ref>-8 to visualize clustering results using Wasserstein and our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Domain-Specific vs. Domain-Oblivious Training</head><p>Our first approach to training is the obvious one: we train on domainspecific data to evaluate if our hash function can sufficiently preserve this space. We evaluated our approach on the 3D shape-1 dataset, splitting the 1,200 diagrams into training and test sets of size 900 and 300, respectively. Clustering with this trained model gave a FMS of 0.83 when compared to the ground-truth clustering using Wasserstein. While this test shows that using domain-data is a viable strategy for our approach, limiting training to domain-specific data would hinder its applicability. As with all machine learning approaches the availability of data is critical to build well-trained models. While datasets like ensemble simulations may have enough data to train the model, this is not guaranteed. Therefore, we evaluated a more general approach. Ideally, training should be domain-oblivious, thereby removing the need for plentiful domain data. To evaluate this possibility, we have trained our model purely on synthetic data. We note that persistence diagrams can be thought of as a specialized 2D scatter plot. Therefore we can produce synthetic diagrams by creating random scatter plots with a uniform persistence point distribution (rejecting points under the diagonal). Training using this naive, synthetic data provided surprising results. We trained our model with diagrams with 50 randomly distributed persistence points and clustered 3D shape-1. The synthetic data produced the same FMS within 0.001 of clustering using the domainspecific model. In our experiments, we found that the only requirement for this approach is that the synthetic diagrams used in training should each have a number of points close to the average number of persistence points in the data to be clustered. This result is not only not obvious, but one would automatically think the opposite: that a set of naive, random diagrams would not sufficiently sample the space of potential diagrams. Our experimental findings raise interesting questions on this space that we discuss in Section 5.</p><p>We adopt this domain-oblivious approach as the primary method for training in this work. We train three models for evaluation: Model-20, Model-50, and Model-100 with 4000 diagrams each with 20, 50, and 100 persistence points per diagram respectively. Table <ref type="table" target="#tab_2">3</ref> illustrates the requirement described above where matching the number of persistence points in training diagrams to the average number of points in the clustered data leads to higher quality results. In our experiments, 3D shape-2 and Vortex Street were tested with Model-20. Next, 3D shape-1 and Starting Vortex were tested with Model-50. Finally, both the full Colorectal Cancer dataset and its subset were tested using Model-100. Note the cross-domain applicability of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>All of our experiments were made on Intel Core 3.60GHz × 8 cores (CPU) and Nvidia GeForce GTX 1660 (the GPU was only used for training models) with 32GB of RAM. Our method is implemented in Python with the Tensorflow platform using the implementation 2 of Song et al. <ref type="bibr" target="#b65">[66]</ref> for our training architecture. We also provide a lightweight C++ program for hardware-accelerated Hamming distance computation. Our code and data are available in an OSF repository. 3  We compare the running time and memory usage of our approach with two popular vectorized persistence representations: Persistence Images (PI) <ref type="bibr" target="#b0">[1]</ref> and Betti Curves (BC) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b59">60]</ref> using GUDHI <ref type="bibr" target="#b70">[71]</ref>. In addition, we evaluate our approach against two state-of-the-art approximations of Wasserstein distance: 2D histograms with Optimal Transport (HW) <ref type="bibr" target="#b42">[43]</ref> and Progressive Wasserstein (PW) <ref type="bibr" target="#b75">[76]</ref> with their implementations. As a ground truth we compare against Wasserstein distance (W1) <ref type="bibr" target="#b17">[18]</ref> using scikit-tda <ref type="bibr" target="#b60">[61]</ref> and a fast implementation of W1, the Hera method <ref type="bibr" target="#b39">[40]</ref> using GUDHI <ref type="bibr" target="#b70">[71]</ref>, following the common parameter values for the above. We use a PI bandwidth of h = 0.02 with the standard weight function (1/persistence) and the entropic term for HW is 0.1/avg N i=1 |p i |. The grid resolution for all is 50 × 50 and bounded by the min/max coordinate of the diagram. This resolution was determined through experimental evaluation of the range [10 2 , 100 2 ] with a step-size of 1. We found in our testing of 3D shape-1, that sizes over 50 only provided minimal improvement of the FMS (approximately 0.001). Therefore 50 was chosen as the minimum sized representation that still provides good quality results. The PI bandwidth was also determined experimentally by testing the range [0.001, 1.0] with step size 0.001. (d) and (e) illustrate the MDS plots for Wasserstein distance and for Hamming distance for our generated binary codes, respectively. The FMS of 0.97 indicates that our method almost perfectly matches the original clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Speed</head><p>Table 2 details our full comparison of runtimes to compute the distance matrix for all pairs in each dataset. This is the input to single-linkage clustering and is therefore the only point at which each technique differs. We separately list the time to generate all representations (e.g., compute persistence images, compute our binary code representation, etc.) and to compute the pairwise distances. W1, Hera, and PW have no generation time, and the generation time for HW is nominal compared to the costly distance computation; therefore these times are presented as total runtimes only. Runtimes for PW are provided but marked with an asterisk since direct comparison is not possible. They use a fast C++ implementation (compared to our Python) and compute distances while clustering. Therefore the distance calculation cannot be separated. Parallelism was allowed for generation of the vectorizations, but distances were computed serially. This was chosen to highlight and simplify the runtime comparisons. As the distance computation is embarrassingly parallel, all approaches should be parallelizable with similar comparisons.</p><p>Let us first consider the runtimes for W1, Hera, and HW. As this table illustrates, these approaches are prohibitively slow. In fact, it was not feasible to run them on our largest dataset with 10k diagrams. For a fair comparison to our Python distance calculation, we do not run HW using GPU acceleration. We note that Latombe et al. <ref type="bibr" target="#b42">[43]</ref> showed that HW still takes roughly 40 − 80 minutes for k-means clustering even with GPU acceleration on 5k persistence diagrams with 50 − 100 persistence points. Next we evaluated the PW approach. This is a progressive approach, and for a fair comparison we ran it to completion. As the table illustrates, this approach is extremely fast for the small test datasets. Although in our testing the author's implementation of this approach quickly reached the memory limits of our system (32 GB) and therefore did not scale to our larger datasets (Colorectal Cancer, 3D shape-1 and 3D shape-2). We ran PW on a 50 image subset of the Colon Cancer dataset and it took over an hour.</p><p>Comparing to other vectorized approaches, our method offers significant performance improvements for our larger datasets giving speedups of 3.9X ∼ 6.1X when compared to the fastest alternative approach. As the table shows, our distance computation is incredibly fast leaving the bottleneck of our approach to be the generation of the binary codes. As such, for datasets that are small, where distance computation does not dominate, our runtimes are comparable to other vectorized approaches.</p><p>As a final illustration of the speed, we have implemented the distance calculation of our approach in C++ and leverage the population count and XOR support that exists on modern CPUs. These results are provided for our largest datasets in Table <ref type="table" target="#tab_3">4</ref>. The runtimes are 2-3 orders of magnitude faster than our Python implementation and take all-pairs distance computation down to just milliseconds. Overall, these results show the speed and scalability of our proposed method and how well positioned our binary codes would be for large-scale tasks or databases.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Memory and Storage</head><p>Table <ref type="table" target="#tab_4">5</ref> describes the potential memory and storage gains from our representation. The 2D persistence points of the diagrams (PD) were saved using 64-bit precision. The grids for PI/BC were also saved with 64-bit precision. We compare this requirement to our approach that saves a single 64-bit number. Reduction rate is compared to the next smallest representation. As this table illustrates, as one would expect, saving a single integer can have significant benefits for storage and memory. The encoder size for our approach, which would also have to be saved, is not included in these results. This cost can vary. Our encoder for this work was the same as Song et al. <ref type="bibr" target="#b65">[66]</ref>: a standard, pretrained VGG19 <ref type="bibr" target="#b68">[69]</ref> model, which is approximately 500MB. However, it may be possible to compress this size <ref type="bibr" target="#b16">[17]</ref> . For instance, work has shown VGG19 can reduce its parameters 5 − 20X <ref type="bibr" target="#b49">[50]</ref>. Other models like SqueezeNet <ref type="bibr" target="#b35">[36]</ref> reduce to less than 0.5MB. Overall, since our approach is domain-oblivious, a single model can be potentially used on a multitude of datasets from a large number of domains. Thus the cost of storing the encoder would be amortized in practice and will not grow as data sizes increase. Therefore this approach has the potential to greatly reduce storage overheads as the use of TDA grows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Quality</head><p>Table <ref type="table" target="#tab_5">6</ref> shows the evaluation of clustering quality, comparing our clustering results with other methods using the evaluation method described in Section 4.2. The quality is determined by the FMS between the clusterings obtained for the different persistence diagram representations compared to the clustering produced when using Wasserstein distance directly on the persistence diagrams. As such, we could only run comparisons on a 1,800 diagram subset of the colorectal cancer dataset since running the full dataset was not possible (W1, Hera) or failed (PW) for some approaches. For our approach, we provide the FMS for our two methods for forming a similarity matrix: real-valued or binary similarity. As this table illustrates, our approach provides comparable or better quality results when compared to progressive Wasserstein (PW), persistence images (PI), or Betti Curves (BC). We time-limit PW to the total runtime of our approach as reported in Table <ref type="table">2</ref>. PW would, in time, converge to the exact Wasserstein distance. Therefore for a fair comparison, we only look at their quality for the same amount of running time. Not only are the results from the binary codes on par with other approaches, our approach almost achieves perfect reproduction of the clustering for the Colorectal Cancer and 3D Shape-2 datasets. Moreover, it perfectly matches the clustering of Starting Vortex. Note these results use our domain-oblivious training approach and therefore the same models were applied to more than one type of data. To further evaluate the quality of distance preservation using our binary codes, we provide scatterplots in Fig. <ref type="figure" target="#fig_8">9</ref> by setting Wasserstein and Hamming distance as x-y coordinates for each point pair. To avoid overdrawing, each point is drawn as a Gaussian kernel density estimator. Each point in the plot is a diagram with the horizontal position given by the W1 distance and vertical being the Hamming distance. If distances are being maintained, this plot should be linear about a diagonal. As this figure illustrates, this is the case for our binary codes. In Fig. <ref type="figure" target="#fig_8">9</ref> (a) the points are provided as well (yellow). This shows there is clear separation in both dimensions (dotted line) meaning that these clusters are well-maintained in both distances. Finally, Fig. <ref type="figure" target="#fig_8">9</ref> (b) illustrates the plot for Vortex Street, an example with a lower FMS. As highlighted with red arrows, there are clear clusters of points where distance is not being maintained and likely give the lower score (although still higher quality than PI and BC).</p><p>Next, we evaluate quality in FMS in relation to the number of bits used in the binary code. We experimented with the clustering result of 3D shape-1, 3D shape-2, Vortex Street, and Starting Vortex by varying Each is drawn with a kernel density estimator. All exhibit a linear, diagonal shape meaning that distances are being preserved. (a) also plots the points directly (yellow). There is a clear separation of the two clusters both horizontally and vertically. (b) highlights the two areas that break this linear property that likely give its lower FMS.</p><p>bit lengths from 24-256. The results of this experiment are provided in Table <ref type="table" target="#tab_6">7</ref>. As this table shows, and as one would expect, increases in bit length result in increases in quality of the final result, although with a noticeable falloff in gains after 64 bits. Therefore we opted for 64-bit codes since they provide high-quality results with simpler implementations than the higher bit counts. Table <ref type="table" target="#tab_7">8</ref> illustrates a FMS comparison of clustering the 3D Shape-1 dataset with different similarity matrix strategies. In this table, Real denotes the real-valued similarity matrix. S-X denotes the use of a binary similarity matrix built from the real-valued distance matrix. S-1 uses fixed number of k-nearest neighbors, where k = 1000 out of the 4000 training set. Similarly, S-2 uses k = 600 and S-3 uses k = 1400. For a soft threshold similarity S-4 uses a strategy of limiting similarity to use a global threshold of 25% percent. Finally, S-5 is our two pass mean rejection. As this figure illustrates, the two pass approach leads to more accurate clustering and is therefore used by our work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In the paper, we present an approach to produce concise binary codes of persistence diagrams that maintain topological similarity. The key to this approach is the training of a machine learning model that learns a hash, not on domain-specific data, but on randomly generated 2D scatter plots. This leads to a technique that is domain-oblivious, where a model can be applied across multiple domains or types of data without the need for retraining. As this is a hashing approach, our technique is not likely to maintain small distances. For applications where close distances are discounted, our approach is well-suited. It is still an open question if a hashing approach could be designed such that small distances are maintained. The data used in our synthetically trained model only needs to roughly match the average number of persistence points of the testing dataset. In practice, we have found this is not an overly strict requirement. For instance, the Colorectral Cancer dataset has 500 persistence points on average, but Model-100 worked well in our tests. In regards to storage, while our binary code is small, one would still need to save the encoder, which for deep networks can be many MB. As we mentioned, given that a single model can be applied to many datasets across many domains, we argue that the amortized cost could be nominal in practice. Although, we plan to explore how to reduce this overhead in future work. Finally, this work illustrated the benefits of this representation through examples from topological clustering, where our new binary codes provide fast, high-quality results. Moreover, the scalability of such an approach was highlighted through the potential low storage requirements of the binary codes along with extremely fast distance computations using on-chip acceleration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: (a) Examples from a dataset of 10k histology images of colorectal cancer. (b) An example persistence diagram that encodes the topological structure in an image. The inset illustrates an elbow method plot run from clustering a subset of 1800 images using one-Wasserstein distance. This shows there are approximately 8 topologically distinct clusters. (c) Clustering result using one-Wasserstein distance on the subset. (d) Our high-quality concise representation uses only a fraction of the memory and computation time. (e) Our approach scales to the full dataset, which is not feasible with 1-Wasserstein.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Progression of a sub-level set of a scalar field. The feature (green) is born at a minimum and dies when it merges with an older feature (purple). The birth and death are represented as a point in the zero-dimensional persistence diagram. The L 1 distance from this point to the diagonal is the lifetime, or persistence, of the feature (red). green feature eventually dies when it merges with the purple connected component. The green feature is represented in the diagram by plotting these birth and death values. Note, that since the purple feature has not yet died, it is not yet in the diagram.Wasserstein and Bottleneck Distances Letting D denote the collection of all diagrams, the q-Wasserstein distance d q : D × D → R is defined by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Training architecture of the PD-GAN model that takes a set of persistence diagrams P as input and learns corresponding binary codes B. Yellow arrows depict flow of diagrams through training, while grey arrows represent information flow (loss functions, etc.). A Generative adversarial network (GAN) is used to train the encoder of binary codes such that the similarity matrix of codes, S B , closely matches the similarity of the input diagrams, S P . Our final hash function that takes in a diagram and produces a binary code is highlighted in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The initial 2D histogram representation used for training and an intermediate step before hashing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: 3D shape-1: The representative topological clusters are shown (a) by color with an example persistence diagram (b). Two MDS plots show results (c) based on Wasserstein distance and (d) Hamming distance for our generated binary codes. The FMS score is the clustering performance measure between two sets of clusters (c) and (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: 3D shape-2 dataset: (a) Example shape meshes from the dataset and (b) one diagram example. As this dataset does not have a known amount of topologically distinct clusters, we use (c) the elbow method to find that k = 7 clusters exist with respect to the Wasserstein distance. (d) and (e) illustrate the MDS plots for Wasserstein distance and for Hamming distance for our generated binary codes, respectively. The FMS of 0.97 indicates that our method almost perfectly matches the original clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Vortex Street: This dataset contains k = 5 clusters with (a) showing the representative data for each cluster. (b) An example diagram is provided. The MDS plot for this dataset is provided for (c) the Wasserstein distance and (d) the Hamming distance of our binary codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Starting Vortex: (a) This ensemble set contains 2 clusters. (b) An example persistence diagram is provided. (c) and (d) The FMS of 1.0 shows that our method achieves a perfect matching among clusters compared to clustering using Wasserstein.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig.9: (a-d) Plots to illustrate distance preservation using binary codes for various datasets. Each is a scatterplot of points for each diagram whose horizontal position is W1 distance and vertical is Hamming. Each is drawn with a kernel density estimator. All exhibit a linear, diagonal shape meaning that distances are being preserved. (a) also plots the points directly (yellow). There is a clear separation of the two clusters both horizontally and vertically. (b) highlights the two areas that break this linear property that likely give its lower FMS.</figDesc><graphic coords="9,60.59,157.89,111.60,90.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of vectorized representations using Fowlkes-Mallows score. Values closer to one are better.</figDesc><table><row><cell>Dataset</cell><cell>HW</cell><cell cols="2">PI BC</cell></row><row><cell cols="3">3D Shape-1 0.98 0.81</cell><cell>0.8</cell></row><row><cell></cell><cell cols="2">1 1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell>1</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>1 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4</cell></row><row><cell>Persistence diagram</cell><cell cols="3">Input 2D histogram</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of clustering results with different training sets using Fowlkes-Mallows score, the Model-20 indicates we use a synthetically random persistence diagram with 20 persistence points each for training, 50 is with 50 persistence points and 100 is with 100 persistence points.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>W1</cell><cell>Hera</cell><cell>HW</cell><cell>PW</cell><cell></cell><cell>PI</cell><cell></cell><cell></cell><cell>BC</cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell>Speedup</cell></row><row><cell>Dataset</cell><cell cols="2">N Avg Pts</cell><cell>Total</cell><cell>Total</cell><cell>Total</cell><cell cols="3">Total Generate Distance</cell><cell cols="3">Total Generate Distance</cell><cell cols="3">Total Generate Distance</cell><cell>Total</cell><cell></cell></row><row><cell>Colon Cancer</cell><cell>10,000</cell><cell>498</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>162.21</cell><cell cols="2">1208.8 1371.01</cell><cell>76.36</cell><cell cols="2">1257.31 1333.67</cell><cell>135.88</cell><cell>119.08</cell><cell>254.96</cell><cell>5.2X</cell></row><row><cell>Colon Cancer-sub</cell><cell>1,800</cell><cell>503</cell><cell>&gt;10D</cell><cell>&gt;7D</cell><cell>&gt;4D</cell><cell>-</cell><cell>31.58</cell><cell>73.04</cell><cell>104.82</cell><cell>4.49</cell><cell>76.59</cell><cell>81.08</cell><cell>9.36</cell><cell>3.98</cell><cell>13.34</cell><cell>6.1X</cell></row><row><cell>3D Shape-1</cell><cell>1,200</cell><cell cols="4">63 78037.24 7523.12 1588.35</cell><cell>-</cell><cell>4.78</cell><cell>38.28</cell><cell>43.06</cell><cell>2.17</cell><cell>35.14</cell><cell>37.31</cell><cell>6.92</cell><cell>2.62</cell><cell>9.54</cell><cell>3.9X</cell></row><row><cell>3D Shape-2</cell><cell>1,900</cell><cell cols="3">22 15321.68 1832.94</cell><cell>633.52</cell><cell>-</cell><cell>6.48</cell><cell>67.4</cell><cell>73.88</cell><cell>3.13</cell><cell>67.69</cell><cell>70.92</cell><cell>8.1</cell><cell>3.9</cell><cell>12</cell><cell>5.9X</cell></row><row><cell>Vortex Street</cell><cell>45</cell><cell>14</cell><cell>15.26</cell><cell>9.76</cell><cell cols="2">0.63 0.09*</cell><cell>0.19</cell><cell>0.041</cell><cell>0.231</cell><cell>0.14</cell><cell>0.04</cell><cell>0.18</cell><cell>0.72</cell><cell>0.0033</cell><cell>0.72</cell><cell>-</cell></row><row><cell>Starting Vortex</cell><cell>12</cell><cell>36</cell><cell>9.27</cell><cell>1.23</cell><cell cols="2">0.14 0.18*</cell><cell>0.06</cell><cell>0.03</cell><cell>0.09</cell><cell>0.02</cell><cell>0.044</cell><cell>0.064</cell><cell>0.23</cell><cell>0.026</cell><cell>0.26</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FMS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="7">N Avg Pts Model-20 Model-50 Model-100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D Shape-2</cell><cell>1,900</cell><cell></cell><cell>22</cell><cell>0.97</cell><cell>0.96</cell><cell></cell><cell>0.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D Shape-1</cell><cell>1,200</cell><cell></cell><cell>63</cell><cell>0.77</cell><cell>0.83</cell><cell></cell><cell>0.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Colon Cancer-sub 1,800</cell><cell></cell><cell>503</cell><cell>0.71</cell><cell>0.76</cell><cell></cell><cell>0.99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Timings for comparisons using a C++ implementation that leverages hardware acceleration to compute Hamming distances.</figDesc><table><row><cell>Dataset</cell><cell>N</cell><cell>Python</cell><cell cols="2">C++ Speedup</cell></row><row><cell cols="4">Colon Cancer 10,000 119.08s 161.61ms</cell><cell>737X</cell></row><row><cell>3D Shape-1</cell><cell>1,200</cell><cell>2.62s</cell><cell>2.27ms</cell><cell>1154X</cell></row><row><cell>3D Shape-2</cell><cell>1,900</cell><cell>3.9s</cell><cell>5.32ms</cell><cell>733X</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of size (in MB) with persistence summaries. As vectorized summaries of persistence diagrams, here PI and BC have the same vector size (50x50).</figDesc><table><row><cell>Dataset</cell><cell cols="2">N Avg Pts</cell><cell>PD</cell><cell cols="3">PI/BC Ours (64bits) Reduction</cell></row><row><cell>Colon Cancer</cell><cell>10,000</cell><cell cols="3">498 800.96 201.21</cell><cell>0.64</cell><cell>192X</cell></row><row><cell>Colon Cancer-sub</cell><cell>1,800</cell><cell cols="2">503 144.96</cell><cell>36.2</cell><cell>0.12</cell><cell>254X</cell></row><row><cell>3D Shape-1</cell><cell>1,200</cell><cell cols="2">63 130.12</cell><cell>24.13</cell><cell>0.08</cell><cell>301X</cell></row><row><cell>3D Shape-2</cell><cell>1,900</cell><cell cols="2">22 152.96</cell><cell>36.2</cell><cell>0.12</cell><cell>301X</cell></row><row><cell>Vortex Street</cell><cell>45</cell><cell>14</cell><cell>0.046</cell><cell>0.9</cell><cell>0.00077</cell><cell>1168X</cell></row><row><cell>Starting Vortex</cell><cell>12</cell><cell>36</cell><cell>0.019</cell><cell>0.24</cell><cell>0.00002</cell><cell>1200X</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of clustering results using the Fowlkes-Mallows score, as described in Section 4.2. Scores range from 0 to 1; a score of 1 indicates identical clusters. The clusterings using different persistence diagram representations (and their distances) are compared to the Wasserstein-based clustering of the input persistence diagrams.</figDesc><table><row><cell>Ours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of clustering results with different of bits, using the Fowlkes-Mallows score.</figDesc><table><row><cell>Dataset</cell><cell>N</cell><cell cols="6">Avg Pts 24 bits 48 bits 64 bits 128 bits 256 bits</cell></row><row><cell>3D Shape-1</cell><cell>1,200</cell><cell>63</cell><cell>0.64</cell><cell>0.75</cell><cell>0.83</cell><cell>0.84</cell><cell>0.86</cell></row><row><cell>3D Shape-2</cell><cell>1,900</cell><cell>22</cell><cell>0.82</cell><cell>0.89</cell><cell>0.97</cell><cell>0.97</cell><cell>0.98</cell></row><row><cell>Vortex Street</cell><cell>45</cell><cell>14</cell><cell>0.67</cell><cell>0.77</cell><cell>0.81</cell><cell>0.83</cell><cell>0.86</cell></row><row><cell>Starting Vortex</cell><cell>12</cell><cell>36</cell><cell>0.94</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison of clustering results with different similarity matrix computation methods, using the Fowlkes-Mallows score.</figDesc><table><row><cell>Trained Model</cell><cell cols="3">N Avg Pts Real</cell><cell>S-1</cell><cell cols="2">S-2 S-3</cell><cell>S-4</cell><cell>S-5</cell></row><row><cell>Model-50</cell><cell>4,000</cell><cell>50</cell><cell cols="3">0.82 0.77 0.78</cell><cell>0.8 0.81</cell><cell>0.83</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In general, it is possible for some features to have a birth but no associated death. In our experiments, these features are not as informative as the features with defined birth and death times. For this reason, we do not include them in our definition above.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ht014/BGAN 3 https://osf.io/q58c3/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Persistence images: A stable vector representation of persistent homology</title>
		<author>
			<persName><forename type="first">H</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Emerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chepushtanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Motta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ziegelmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vertical decomposition of shallow levels in 3-dimensional arrangements and its applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="912" to="953" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th Annual IEEE Symposium on Foundations of Computer science (FOCS&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Riemannian framework for statistical analysis of topological persistence diagrams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Natesan Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="68" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new algorithm for the assignment problem</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="171" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">International workshop on image processing: Real-time edge and motion detection/estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lantuejoul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979-09">September 1979</date>
		</imprint>
	</monogr>
	<note>lecture notes</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Topoms: Comprehensive topological exploration for molecular and condensed-matter systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Gyulassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Pask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Chemistry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="936" to="952" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reeb graphs for shape analysis and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biasotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spagnuolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falcidieno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">392</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="5" to="22" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Modern Multidimensional Scaling: Theory and Applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Groenen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive exploration and analysis of large-scale simulations using topologybased data segmentation</title>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tierny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1307" to="1324" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HashGAN: Deep learning to hash with pair conditional Wasserstein GAN</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1287" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computing contour trees in all dimensions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoeyink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Axen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="94" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sliced Wasserstein kernel for persistence diagrams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carriere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oudot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="664" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stable topological signatures for points on 3d shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carrière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Oudot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A benchmark for 3D mesh segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Statistical analysis of persistence intensity functions</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02502</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09282</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stability of persistenceddiagrams</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lipschitz functions have L p -stable persistence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mileyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="139" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Morse complexes for shape segmentation and homological analysis: Discrete models and algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Floriani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fugacci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iuricich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Magillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="761" to="785" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to hash with binary deep neural network</title>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-D</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Computational Topology: An Introduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Topological persistence and simplification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Letscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomorodian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 41st Annual Symposium on</title>
				<meeting>41st Annual Symposium on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
	<note>Foundations of Computer Science</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Approximate nearest neighbors in the space of persistence diagrams</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Fasy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Micka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11257</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A method for comparing two hierarchical clusterings</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Mallows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">383</biblScope>
			<biblScope unit="page" from="553" to="569" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Topological characterization of spatial-temporal chaos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gameiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mischaikow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kalies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">35203</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video captioning with attention-based lstm and semantic consistency</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2045" to="2055" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2916" to="2929" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1050:10</idno>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets. Stat</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Characterizing molecular interactions in chemical systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Boto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Contreras-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Piquemal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tierny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2476" to="2485" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stability of dissipation elements: A case study in combustion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gyulassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A practical approach to morse-smale complex computation: Scalability and generality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gyulassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1619" to="1626" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the Vietoris-Rips complexes and a cohomology theory for metric spaces</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Hausmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prospects in Topology: Proceedings of a Conference in Honor of William Browder</title>
				<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="175" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Matroid filtrations and computational persistent homology</title>
		<author>
			<persName><forename type="first">G</forename><surname>Henselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghrist</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00199</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt; 0.5 mb model size</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the translocation of masses</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Kantorovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1381" to="1382" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-dimensional time-dependent vortex regions based on the acceleration magnitude</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reininghaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2080" to="2087" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">000 histological images of human colorectal cancer and healthy tissue</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Halama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marx</surname></persName>
		</author>
		<idno>doi: 10.5281/ zenodo.1214456</idno>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Geometry helps to compare persistence diagrams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nigmetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Algorithmics (JEA)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1042" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kernelized locality-sensitive hashing for scalable image search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2130" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large scale computation of means and clusters for persistence diagrams using optimal transport</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lacombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oudot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9770" to="9780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Persistent homology for the quantitative evaluation of architectural features in prostate cancer histology</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sholl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Fasy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discriminative persistent homology of brain networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international symposium on biomedical imaging: from nano to macro</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="841" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hole detection in metabolic connectivity of Alzheimer&apos;s disease using k-Laplacian</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning compact binary descriptors with unsupervised deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Topology-inspired partition-based sensitivity analysis and visualization of nuclear simulations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maljovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alfonsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pastore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rabiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE PacificVis</title>
				<meeting>of IEEE PacificVis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Domain-adaptive deep network compression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4289" to="4297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Comparing clusterings -an information based distance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="873" to="895" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Weighted persistent homology for biomolecular data analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Mémoire sur la théorie des déblais et des remblais. Histoire de l&apos;Académie Royale des Sciences de Paris</title>
		<author>
			<persName><forename type="first">G</forename><surname>Monge</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1781</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Algebraic Topology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Munkres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust online computation of reeb graphs: simplicity and speed</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scorzelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mascarenhas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Locality-sensitive binary codes from shiftinvariant kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1509" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A stable multi-scale kernel for topological machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reininghaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4741" to="4748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Topological machine learning with persistence indicator functions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sadlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leitte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topological Methods in Data Analysis and Visualization</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="87" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Computational topology for point data: Betti numbers of α-shapes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Morphology of Condensed Matter</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="261" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Scikit-tda: Topological data analysis for python</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tralie</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.2533369</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sketching persistence diagrams</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Sheehy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th International Symposium on Computational Geometry</title>
				<imprint>
			<date type="published" when="2021">SoCG 2021. 2021</date>
		</imprint>
	</monogr>
	<note>Schloss Dagstuhl-Leibniz-Zentrum für Informatik</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Persistent homology analysis of brain transcriptome data in autism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Voineagu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Voineagu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Society Interface</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">158</biblScope>
			<biblScope unit="page">20190531</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Lifted Wasserstein matcher for fast and robust topology tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plainchault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Conche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tierny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 8th Symposium on Large Data Analysis and Visualization (LDAV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Convolutional Wasserstein distances: Efficient optimal transportation on geometric domains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Goes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Binary generative adversarial networks for image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep region hashing for generic instance search from images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deformation transfer for triangle meshes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="405" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">giotto-tda:: A topological data analysis toolkit for machine learning and data exploration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tauzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lupo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Medina-Mardones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dassatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="39" to="40" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">The GUDHI Project. GUDHI User and Reference Manual. GUDHI Editorial Board</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>3.4.1 ed.</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Who belongs in the family?</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Thorndike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The topology toolkit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tierny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Favelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gueunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="832" to="842" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fréchet means for distributions of persistence diagrams</title>
		<author>
			<persName><forename type="first">K</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mileyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="70" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Geometry helps in matching</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Vaidya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1201" to="1225" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Progressive Wasserstein barycenters of persistence diagrams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tierny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Über den höheren Zusammenhang kompakter Räume und eine Klasse von zusammenhangstreuen Abbildungen</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vietoris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="454" to="472" />
			<date type="published" when="1927">1927</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Efficient computation of persistent homology for cubical data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topological Methods in Data Analysis and Visualization II</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="91" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for largescale search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Persistent homology analysis of protein structure, flexibility, and folding. International journal for numerical methods in biomedical engineering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-W</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="814" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Finding cosmic voids and filament loops using topological data analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cisewski-Kehe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nagai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy and Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="34" to="52" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Scalar field comparison with topological descriptors: Properties and applications for scientific visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sridharamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14331</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="599" to="633" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning metrics for persistence-based summaries and applications for graph classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9859" to="9870" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
