<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tan</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanhong</forename><surname>Wu</surname></persName>
							<email>ycwu@zju.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
							<email>lingyun.yu@xjtlu.edu.cn.•</email>
						</author>
						<author>
							<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yingcai</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tan Tang</orgName>
								<orgName type="laboratory" key="lab1">Yanhong Wu and Yingcai Wu are with State Key Lab of CAD&amp;CG</orgName>
								<orgName type="laboratory" key="lab2">Zhejiang Lab</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong-Liverpool University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46E496B5DD610E676923098EE015BA77</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video moderation</term>
					<term>video visualization</term>
					<term>e-commerce livestreaming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video moderation, which refers to remove deviant or explicit content from e-commerce livestreams, has become prevalent owing to social and engaging features. However, this task is tedious and time consuming due to the difficulties associated with watching and reviewing multimodal video content, including video frames and audio clips. To ensure effective video moderation, we propose VideoModerator, a risk-aware framework that seamlessly integrates human knowledge with machine insights. This framework incorporates a set of advanced machine learning models to extract the risk-aware features from multimodal video content and discover potentially deviant videos. Moreover, this framework introduces an interactive visualization interface with three views, namely, a video view, a frame view, and an audio view. In the video view, we adopt a segmented timeline and highlight high-risk periods that may contain deviant information. In the frame view, we present a novel visual summarization method that combines risk-aware features and video context to enable quick video navigation. In the audio view, we employ a storyline-based design to provide a multi-faceted overview which can be used to explore audio content. Furthermore, we report the usage of VideoModerator through a case scenario and conduct experiments and a controlled user study to validate its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>E-commerce livestreaming, in which a vendor is a livestreamer who advertises and promotes his/her products on e-commercial platforms <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b55">58]</ref>, has taken off. Such online commercial activities generate a large amount of user-generated videos that may include misinformation or offensive content, which can exert a negative or damaging impact on e-commerce platforms <ref type="bibr" target="#b51">[54]</ref>. To protect platforms' business and attract consumers by using innovative content, video moderation, which involves reviewing videos to remove unnecessary or explicit content <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b32">35]</ref>, has become a crucial task. Formally, we refer to videos that violate platform policies or guidelines as deviant videos, and crowd workers are recruited to review videos as moderators.</p><p>Manually reviewing the sheer volume of videos is a labor-intensive task for human moderators; thus, several mixed-initiative methods are employed to guard against deviant content <ref type="bibr" target="#b25">[28]</ref>. Such methods integrate human intelligence with computational power through a "filter first and review later" strategy, in which moderators review only suspicious videos recalled by machine learning models <ref type="bibr" target="#b3">[6]</ref> (Fig. <ref type="figure">1</ref>). Although many advanced machine learning models <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b44">47,</ref><ref type="bibr" target="#b64">67]</ref> can achieve comparable or better performance than people in mixed-initiative systems, they may disrupt human moderators owing to their low accuracy for realworld videos and "black box" feature <ref type="bibr" target="#b11">[14]</ref>. For example, such models may generate uncertain or even erroneous results contradicting human judgement, thereby increasing the burden of moderators, who need to not only review the videos but also consider "who" is right.</p><p>To better leverage human moderators with computational power, we argue that reviewing multimodal video content while being aware of machine-generated risk information through interactive visualizations would be beneficial. Formally, we refer to risk as the possibility of deviant video content that disobey moderation policies. To validate our assumption, we collaborated with a leading domestic moderation company that employs thousands of moderators for daily video-reviewing tasks. We first developed an alpha prototype that integrates a timeline visualization to demonstrate temporal risks and deployed it as an internal tool for the employees. We then conducted a preliminary study to observe the moderators who adopted our tool to complete their works. By comparing their daily and historical performances, we found that their average time efficiency increased by 22.1%, while the missing rate decreased by 81%. The promising results motivated us to propose a risk-aware framework that deeply integrates machine-generated insights into the reviewing process through visualizations. However, the development of such a framework involves two major obstacles:</p><p>Extraction of multimodal risk information. The multimodal semantic content of videos can be successfully obtained using deep learning-based models, which can lead to many successful applications <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b67">70]</ref>. Nevertheless, such models cannot be directly employed in e-commerce videos owing to the lack of domain-specific datasets and the diversity of moderation policies. The extraction of semantic risk content from e-commerce livestreaming videos requires a novel multimodal model, which remains an unsolved challenge.</p><p>Visualization of risk-aware multimodal video content. The massive multimodal video content requires considerable human efforts to review entire videos, which is time consuming. Moreover, extracted risk information is typically dynamic and high-dimensional, which may produce heavy visual clutter when being visualized with informative video context. Thus, the integrated visualization of multimodal video content and associated risks poses the second challenge, which demands novel visual summarization techniques.</p><p>In this work, we propose VideoModerator, a risk-aware framework, to facilitate multimodal video moderation by using a mixed-initiative method. To address the first challenge, we employ state-of-the-art deep learning models <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b49">52,</ref><ref type="bibr" target="#b56">59]</ref> to extract risk tags from potentially deviant frames and detect risk words from audio scripts. To overcome the lack of training data, we use a "learning with reviewing" strategy, in which reviewed videos with ground-truth labels are used to train deep learning models iteratively. For the second challenge, we propose a multimodal visualization interface consisting of three components: a) A video view involves a video player integrated with a segmented timeline presenting the high-risk periods that moderators should focus on. b) A frame view involves visually summarized video frames to provide a compact overview at the frame-, shot-, and scene levels. We project the frames on the vertical axis and cluster them to construct different shots and scenes. The horizontal axis is the timeline and circular glyphs are introduced to visualize the associated risk tags. c) An audio view involves the automatic translation of audio clips into a list of sentences that may contain risk words. We employ a histogram to indicate the frequencies of the risk words and adopt a storyline-based layout to demonstrate the audio context. With the proposed interface (Fig. <ref type="figure" target="#fig_2">4</ref>), we provide moderators a simple and efficient workflow. A moderator starts his/her task by first clicking the load button. The video view enables the moderator to notice high-risk periods promptly, and the frame and audio view provide detailed risk information to help him/her make a quick decision. The moderator completes the task by selecting a color label and clicking the submit button.</p><p>The main contributions are as follows:</p><p>• We introduce a risk-aware framework, namely, VideoModerator, to facilitate the efficient moderation of e-commerce videos. • We propose novel visual summarization techniques to visualize multimodal video content and the risk-aware information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• We conduct experiments and a controlled user study to evaluate</head><p>VideoModerator and report its usage through a case scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we summarize the key techniques proposed for video moderation and relevant studies on video visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Moderation</head><p>Video moderation means reviewing videos to protect viewers from deviant content violating explicit rules and community guidelines <ref type="bibr" target="#b51">[54]</ref>.</p><p>One intuitive moderation strategy is to allow viewers to report video feeds with deviant content and protect themselves by using blocklists <ref type="bibr" target="#b32">[35,</ref><ref type="bibr" target="#b38">41]</ref>. However, the effectiveness of this method relies on user engagement with systems that may disrupt the normal usage of video platforms <ref type="bibr" target="#b36">[39]</ref>. An extreme situation would involve all users becoming moderators, trapped in the fight against deviant content, rather than enjoying Internet surfing. Thus, online platforms should adopt moderation services to protect their users and brand value <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b20">23]</ref>. Despite previous investigations on online deviant content, video moderation has garnered renewed attention owing to the emergence of state-of-the-art machine learning methods <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b65">68]</ref>.</p><p>To identify violent content in videos, Hanson et al. <ref type="bibr" target="#b23">[26]</ref> introduced a spatiotemporal encoder built on the bidirectional convolutional LSTM architecture that generates improved video representations by leveraging spatial features with long-range temporal information. Nevertheless, this solution takes videos as a whole without considering underlying violent concepts. Peixoto et al. <ref type="bibr" target="#b47">[50]</ref> proposed a novel methodology that employs two separate deep neural networks to learn subjective-and conceptual-level spatiotemporal information. Moreover, the authors aggregated deep representations <ref type="bibr" target="#b71">[74]</ref> under each specific concept by training a binary classifier to moderate videos. Considering the multimodal features of videos, Wu et al. <ref type="bibr" target="#b65">[68]</ref> proposed a neural network that contains holistic, localized, and score branches to determine deviant videos by considering video and audio signals at different levels. Meanwhile, Giannakopoulos et al. <ref type="bibr" target="#b17">[20]</ref> employed a "fusion" strategy combining audio and visual information to moderate video segments.</p><p>Despite the advancements in deep learning-based video moderation techniques <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b53">56]</ref>, using solely machine intelligence is insufficient owing to the limitations of model generalization <ref type="bibr" target="#b11">[14]</ref> and the changes in moderation guidelines <ref type="bibr" target="#b18">[21]</ref>. For many online moderation applications [2, 3], first, machine learning models are employed to recall underlying deviant videos; then, human moderators are recruited to review the videos closely. This two-step strategy intends to achieve an improved trade-off between preventing irregular content and protecting wrongfully identified videos. However, this strategy ignores the risk-aware information extracted by machine learning models, which is important for discovering deviant content. To promote an improved moderation paradigm, we propose a risk-aware framework that leverages human moderators with the deep analysis of machine-generated insights. To the best of our knowledge, this study is the first to employ visual analytic approaches to moderate multimodal video content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Visualization</head><p>Video visualization has been investigated in the fields of multimedia analysis <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b72">75]</ref> and information visualization <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b54">57,</ref><ref type="bibr" target="#b61">64,</ref><ref type="bibr" target="#b62">65,</ref><ref type="bibr" target="#b66">69,</ref><ref type="bibr" target="#b68">71,</ref><ref type="bibr" target="#b69">72]</ref> for decades. We consider the relevant studies on visual video summarization and visual video analytics that are associated with video reviewing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Visual Video Summarization</head><p>Visual video summarization refers to the process of transforming a raw video into a compact visual form without losing substantial information. The goal of this process is to facilitate rapid video navigation by transforming video content and associated features into volume- <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b46">49]</ref>, feature- <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b58">61]</ref>, and context-based visualizations <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b43">46,</ref><ref type="bibr" target="#b52">55]</ref>. Fig. <ref type="figure">1</ref>. A typical mixed-initiative framework for video moderation. Videos are firstly filtered by machine learning models and the suspicious videos are further reviewed by human moderators. Normal videos would be presented to public directly while deviant videos would be handled by different community policies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">4]</ref>.</p><p>Volume-based video visualizations, which are also known as video graphics <ref type="bibr" target="#b4">[7]</ref>, employ volume rendering to transform video frames into a single graphic. Daniel et al. <ref type="bibr" target="#b13">[16]</ref> formally proposed the idea of video visualization and employed different volume visualization techniques to visualize videos as volume cubes, which inspired several studies <ref type="bibr" target="#b35">[38,</ref><ref type="bibr" target="#b46">49]</ref>. For example, Kang et al. <ref type="bibr" target="#b35">[38]</ref> proposed a space-time video montage that represents a video by using volumetric layers and combines highly informative portions into an integrated visualization. Nguyen et al. <ref type="bibr" target="#b46">[49]</ref> developed VideoSummagator, which transforms a video into a space-time cube by combining the most informative frames to facilitate a quick video overview. Instead of visualizing raw videos, feature-based video visualizations focus on presenting extracted video features. Chen et al. <ref type="bibr" target="#b8">[11]</ref> introduced visual signatures into video visualization and investigated different types of visual signatures through a user study. Duffy et al. <ref type="bibr" target="#b15">[18]</ref> proposed glyph-based video visualizations that employ tadpole-like glyphs to exhibit the spatiotemporal motion features of semen for semen analysis. Storyline visualizations were also developed to understand the scenic interactions among characters for movie analysis <ref type="bibr" target="#b57">[60,</ref><ref type="bibr" target="#b58">61]</ref>. Context-based video visualizations have been established in feature-based visualizations with scene context. Romero et al. <ref type="bibr" target="#b52">[55]</ref> introduced Viz-A-Vis, which visualizes human activities on a room map, and Botchen et al. <ref type="bibr" target="#b5">[8]</ref> aggregated human motion trajectories on top of representative video frames. For surveillance videos, Höferlin et al. <ref type="bibr" target="#b29">[32]</ref> developed schematic summaries aggregating trajectories on video scenes, and Meghdadi et al. <ref type="bibr" target="#b43">[46]</ref> employed a space-time cube to layer the motion paths of moving objects on top of a video.</p><p>Despite these methods effectively summarize entire videos, they are established in various domains and designed for different purposes and cannot be directly applied in video moderation. Moreover, they mainly focus on the visual aspect of videos while ignoring audio context. This study focuses on visualizing multimodal video content and integrating risk-aware information to facilitate quick video navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Visual Video Analytics</head><p>Visual video analytics refers to the reasoning process facilitated by a visualization interface to complete video analytic tasks <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b30">33]</ref>. Various visual analytic approaches have been developed for movies <ref type="bibr" target="#b37">[40,</ref><ref type="bibr" target="#b41">44]</ref>, news <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b50">53]</ref>, presentations <ref type="bibr" target="#b63">[66,</ref><ref type="bibr" target="#b74">77]</ref>, and education <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b73">76]</ref>.</p><p>Movies provide multimodal information that can assist analysts in understanding narratives. To analyze such information, Kurzhals et al. <ref type="bibr" target="#b37">[40]</ref> proposed multilayer segmented timelines to characterize the visual content of movies and associated video frames with semantic information derived from subtitles or scripts. Meanwhile, Ma et al. <ref type="bibr" target="#b41">[44]</ref> focused on the emotional content of movies and employed a map metaphor for tracking and understanding the evolution of characters' emotions. News videos are important for journalists to discover significant topics and emergent events. John et al. <ref type="bibr" target="#b33">[36]</ref> utilized multiple coordinated views to associate news videos with textual reports, and Renoust et al. <ref type="bibr" target="#b50">[53]</ref> proposed FaceCloud, which follows word clouds to present images and words in a compact view. Presentation videos have become prevalent owing to their easy-to-access feature <ref type="bibr" target="#b63">[66,</ref><ref type="bibr" target="#b74">77]</ref>. Wu et al. <ref type="bibr" target="#b63">[66]</ref> employed Sankey diagrams integrated with segmented timelines to demonstrate presentation skills adopted by speakers, and Zeng et al. <ref type="bibr" target="#b74">[77]</ref> established a hybrid visualization combining speakers' faces and video captions to understand speakers' emotion coherence. To assist educators in improving teaching quality, Zeng et al. <ref type="bibr" target="#b73">[76]</ref> de-veloped EmotionCues, which employs a novel flow visualization to track students' emotions and understand their mental states.</p><p>Although previous studies have utilized coordinated views to analyze multimodal video content, they are established in a typical analytic framework <ref type="bibr" target="#b30">[33]</ref> that primarily focuses on the visualization side. Höferlin et al. <ref type="bibr" target="#b28">[31]</ref> proposed an interactive learning framework, in which users can annotate videos in a mixed-initiative way. Inspired by this framework, we propose a risk-aware framework that deeply integrates human knowledge with computational power by using novel interactive video visualizations to facilitate efficient video moderation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>This section presents the background and data for video moderation in e-commerce platforms where vendors intend to advertise their products through online livestreaming <ref type="bibr" target="#b6">[9]</ref>. On the one hand, such activities produce a sheer volume of videos that enable vendors to attract potential consumers and increase conversion rates. On the other hand, usergenerated videos may inevitably introduce potential risks due to the possibility of deviant content. Thus, video moderation becomes an essential task for e-commerce platforms that employ numerous crowd workers, also known as moderators, to review and moderate videos. We first describe the basic characteristics of e-commerce livestreamings and introduce e-commerce moderation policies. We then provide a preliminary study which motivates us to complete this study.</p><p>E-commerce livestreaming refers to the broadcasting of live videos in real time on the Internet to promote and sell goods to the public, which is regarded as the future of business and marketing. Despite the online feature of livestreams, we focus on an off-line scenario in which moderators review ended livestreams to determine whether the livestreamers followed or violated platform policies. Given that we focus only on ended livestreams, we do not distinguish between online and off-line videos in the rest of the paper. E-commerce livestreaming videos can be characterized by four narrative elements, namely, the streamer, goods, scene, and speech (Fig. <ref type="figure" target="#fig_0">2</ref>). Streamer refers to the speaker who is the main "actor" in a livestream. Occasionally, two or more streamers, who are often small business owners on e-commerce platforms, may appear in a single livestream. Goods represent the products promoted by a streamer, and scene is the location of a livestream, which is typically recorded against a fixed background. Lastly, speech refers to what a streamer is saying to advertise his/her goods. E-commerce livestreaming videos are typical multimodal data.</p><p>Generally, e-commerce videos can be divided into two types, namely, normal and deviant. Normal videos are presented directly to viewers, whereas deviant videos are handled by different community guidelines or policies. For example, YouTube may take down copyright-violated videos <ref type="bibr" target="#b0">[1]</ref>, and Twitter may suspend accounts that upload videos containing violence or threats [4]. To understand moderation policies for e-commerce livestreaming videos fully, we collaborate with two domain experts who have been working in a leading e-commerce company for decades. Four common moderation policies, namely, false advertising, protected products, inappropriate business, and sensitive content, exist; they are designed to protect platforms' business and brand value. The relationships between the moderation policies and narrative elements are explained below:</p><p>• False advertising refers to the usage of exaggerated statements or false claims on properties, goods, or services for the public. Moderators typically pay attention to the goods and speech elements to examine whether streamers use misleading words. • Protected products are goods protected by laws, such as smuggled goods, protected animals or plants. Moderators mainly focus on the goods element to determine whether an advertised product belongs to the white or black list. • Inappropriate business refers to the deviant behaviors of streamers who deliberately ruin platform publicity. Moderators usually examine the scene element to detect whether pictures or slogans aim to ruin brand image deliberately. • Sensitive content refers to violent, hateful, or explicit content inappropriate for all-ages e-commerce platforms. Moderators should pay attention to the streamer, scene, and speech elements to determine whether deviant content exists. Owing to the diversity of videos, reviewing videos by relying solely on high-level policies is difficult for moderators. According to experts, we define dozens of specific risk tags and words for each policy to moderate videos and audio clips, respectively. We refer to each policy as a risk category and obtain more than 100 risk tags and words for 4 risk categories. To moderate a video, moderators must first review the entire video to check for deviant content and then label the risk category to indicate which policy it violates. Moreover, moderators must submit evidence supporting their conclusion, including at least one screenshot of the deviant content and associated risk tags or words, to help streamers improve their livestreams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RISK-AWARE FRAMEWORK</head><p>This section first presents a preliminary study that motivates our riskaware framework for multimodal video moderation. A detailed description of the framework and its two back-end components, namely, the data-processing procedure and data-filtering method, is followed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminary Study</head><p>The existing video moderation framework (Fig. <ref type="figure">1</ref>) integrates a generic reviewing tool <ref type="bibr" target="#b1">[3]</ref> consisting of a video player with small multiples of video frames (Fig. <ref type="figure" target="#fig_5">7</ref>). The basic idea behind this framework is that reviewing less information means better time performance. Thus, moderators are required to review original video content with minimum additional information, such as machine-generated scores. However, we present a different assumption that enabling moderators to explore multimodal video content and extracted risk information concurrently may also improve their performance. We conducted a preliminary study in collaboration with a moderation group in an e-commerce company to validate this assumption.</p><p>Apparatus. We adopted the moderation tool that was used in the e-commerce company as the baseline interface, which was similar to Fig. <ref type="figure" target="#fig_5">7</ref>. To illustrate the benefits of integrating machine-generated insights, we enhanced the baseline interface by providing a segmented risk timeline (Fig. <ref type="figure" target="#fig_2">4a</ref>). Specifically, we divided the timeline into different segments and visualized their risk categories using the scores extracted by machine learning models. Detailed discussions can be found in Sec. 5.2.1. Except for the risk timeline, both interfaces shared the same components, namely, a video player and a photo wall presenting key frames. We deployed the enhanced interface on the internal moderation platform of the e-commerce company.</p><p>Participants and Procedure. We invited 80 professional moderators to complete their daily tasks on the enhanced interface and collected their working logs for 1 week (5 days). Generally, an e-commerce livestreaming lasts several hours that can be divided into hundreds of video clips and each moderator would review more than 10, 000 video clips every day. Owing to the large number of reviewed videos, we sampled 10% videos to evaluate the performance of moderators. The sampled videos would be reviewed again and randomly assigned to the moderators. The final reviews were regarded as ground truth.</p><p>Metrics and Results. We employed two crucial metrics, namely, time efficiency (TE) and missing rate (MR). Time efficiency refers to the number of videos that a moderator can review during a certain time. Missing rate indicates the ratio of deviant videos that a moderator cannot discover in a predefined group of videos. They are two primary metrics because a higher TE indicates the expansion of their business, while a lower MR ensures the strong control of risk level. We obtained the two metrics by analyzing the collected working logs and checking the duplicate reviews of the sampled videos. Compared with the historical statistics provided by our collaborators, we found that our risk-aware visualization could improve the average time efficiency of the moderators by 22.1% and decrease their average missing rate by 81%. Despite this pilot study is established on the video view, it motivates us to develop more visualizations that integrate human knowledge and machine intelligence deeply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Framework</head><p>To handle the sheer volume of videos, existing systems [2, 3] follow a typical framework that empowers human moderators with computational models (Fig. <ref type="figure">1</ref>). Machine learning models were initially employed to detect potentially deviant video content violating community guidelines or policies. Once deviant clips were detected, an entire video would be recalled for further review, whereas normal videos would be presented to the public. Although machine learning models can drastically reduce the number of videos to be moderated, two bottlenecks exist in the typical mixed-initiative framework. First, the human reviewing process is hindered by the linear reading habit. Moderators browse videos linearly to avoid skipping deviant content, which is labor intensive and time consuming. Second, the complexity and diversity of real-world video data can decrease model accuracy. Moderators need to review a large amount of normal videos wrongly selected by models, which increases their reviewing efforts.</p><p>To overcome these obstacles, we propose a risk-aware framework that deeply integrates human knowledge with machine-generated insights through interactive visualizations <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b22">25]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, our framework involves three steps, namely, video processing, video filtering, and video reviewing. The novelty of our framework lies in two aspects. First, we consider the multimodal video content and risk information extracted using machine learning models. Owing to the emergence of deep learning techniques <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b49">52]</ref>, we employ state-of-theart models <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b49">52,</ref><ref type="bibr" target="#b56">59]</ref> to detect objects in video frames and translate speech from audio content. Such information contains uncertainties, such as risk tags and the associated probabilities, which are usually ignored by the typical framework. Our framework not only visualizes the multimodal video content but also presents such risk information to ensure effective video moderation. Second, we establish a tight connection between machine learning models and human moderators by providing easy-to-use interactions. To reduce the number of videos to be moderated, we train a binary classifier to select high-risk videos that may include deviant content automatically. The typical framework employs a "filtering first and reviewing later" method, in which pretrained models are adopted as classifier, whose accuracy may decrease considerably for new videos. Our framework employs a "learning with reviewing" strategy, in which newly reviewed videos are used as ground-truth to update the training process of the classifier periodically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Video Processing</head><p>The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b49">52,</ref><ref type="bibr" target="#b56">59]</ref> to extract multimodal features.</p><p>Visual Feature Extraction. We extract visual features at frame and video levels using deep learning-based techniques <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b49">52,</ref><ref type="bibr" target="#b56">59</ref>]. An input video would be first decomposed into a sequence of video frames that contain redundant information. We employ an adaptive sampling strategy <ref type="bibr" target="#b75">[78]</ref> to reduce the time cost of the processing procedure. Specifically, we sample one frame every 1 s for short videos (e.g., less than 30 min) and one frame every 2 s for long videos. We employ object detection <ref type="bibr" target="#b49">[52]</ref> and classification <ref type="bibr" target="#b27">[30]</ref> methods to generate the risk tags and scores for the sampled frames. The first model <ref type="bibr" target="#b49">[52]</ref> employs a region proposal network to predict object bounds and scores and the second model <ref type="bibr" target="#b27">[30]</ref> proposes a deep residual neural network to facilitate image recognition task. However, there are some risks that could not be detected using the frame-level information, such as the deviant behaviors of streamers. Thus, we also divide a video into a set of video clips and adopt the inception model <ref type="bibr" target="#b56">[59]</ref> to obtain the video-level risk tags. Given that there are numerous inherently different risk tags, we divide them into different groups and train every model to separately predict the risk tags within a group. For example, the object detection model <ref type="bibr" target="#b49">[52]</ref> is trained to only generate the risk tags about protected products. We employ an aggregation method to combine the risk tags generated by different models. Each video V is represented by a sequence of frames {F 1 , F 2 , ..., F n }, and each frame is depicted by a set of risk tags {a 1 , a 2 , ..., a s }. We denote a frame as F n = (p 1 , p 2 , ..., p s ), where p s = ∑ i p i s and p i s refer to the predicted score of risk tag a s by the i-th model. Especially, p i s = 0 if the risk tag a s cannot be detected by the model i. The training of these advanced models requires a large amount of e-commerce livestreaming videos with ground-truth labels, which cannot be obtained from existing public datasets. We first adopt pretrained models to initiate the framework and then employ a "learning with reviewing" method, in which the reviewed videos are collected to re-train the models.</p><p>Audio Feature Extraction. We extract audio features at different regular time intervals to preserve the time context of streamers' speech. Considering moderators' linear reading habit in receiving audio information, we employ the speech-to-text technique to support the visual exploration of streamers' words. Specifically, we first adopt the automatic speech recognition (ASR) method <ref type="bibr" target="#b45">[48]</ref> to translate spoken language into text and then detect potentially deviant content by using a list of risk words. This method <ref type="bibr" target="#b45">[48]</ref> proposes a transformer based streaming ASR system to generate output shortly after each spoken word, which can be applied to video applications. To obtain sufficient training data, we automatically extract audio clips and detect textual captions from movies. Such audio and text pairs can be used as ground-truth datasets to train the advanced ASR model <ref type="bibr" target="#b45">[48]</ref>. We extract audio features by calculating the frequencies of risk words detected from the input video. Each video V is divided into a set of audio clips {A 1 , A 2 ,...,A m } at regular intervals. With the ASR model, each audio clip A m is translated into a bag of words, in which risk words {w 1 , w 2 , ..., w t } are identified. We denote an audio clip as vector A m = (v 1 , v 2 , ..., v t ) where v t refers to the frequency of word w t . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Video Filtering</head><p>To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of "learning with reviewing," we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.</p><p>Given that extracted features demonstrate how videos may potentially contain deviant clips, we define the risk value R for each video V by accumulating the probabilistic information of risk tags and words. We notice that the visual and audio features are extracted by different models. To aggregate them, we first normalize the predicted scores that p s , v t ∈ [0, 1] then employ a linear combination method as following:</p><formula xml:id="formula_0">R(V ) = 1 m + n ( n ∑ i=1 R F (F i ) + m ∑ j=1 R A (A j ))<label>(1)</label></formula><p>where R F and R A refer to the risk values of video frames and audio clips, respectively. An intuitive strategy is to adopt the average values of F i and A j to define R F and R A . Nonetheless, the vectors of risk tags or words are often sparse, which may underestimate the risk values of possible deviant videos. Thus, we employ a max aggregation strategy that defines R F (F i ) = max s F s i and R A (A j ) = max t A t j where F s i and A t j refer to the s-th value of the i-th frame and t-th vector value of the j-th audio vector, respectively. In accordance with Eq. 1, we refer to the videos with risk values above the threshold R as high-risk videos.</p><p>The linear classifier may be inadequate for classifying diverse and complex video data. After obtaining enough reviewed videos, we adopt a state-of-the-art neural network <ref type="bibr" target="#b27">[30]</ref> as the filter, which can fit a nonlinear hyperplane. Instead of obtaining the original video content, our classifier regards the extracted features as the input and is then trained using the moderator-reviewed labels. The input data can be formulated as two matrices (F 1 , F 2 ,...,F n ) and (A 1 , A 2 ,...,A n ) that represent visual and audio features, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUALIZATION TECHNIQUES</head><p>This section describes the design process and considerations for Video-Moderator followed by the visual designs and interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design Process and Considerations</head><p>To conduct the pilot study, we first developed the video view that illustrated the benefits of integrating human knowledge and machine intelligence. After that, we further took a step to work closely with two domain experts. They were also involved in the discussion of moderation policies and designed the baseline moderation tool that was used in the company. Through regular meetings and discussions, we derived two high-level design considerations (DCs) followed by a set of specific design requirements (R). DC1 Visual Summarization of Multimodal Video Content. R1 Highlighting high-risk periods in a video. Browsing a video stream without visual hints may be tedious. Thus, the system should automatically detect and highlight critical periods that moderators should consider in priority order. R2 Presenting a multilevel overview of frames. Long videos contain numerous frames that can hinder the exploration of video content. Hence, the system should provide a scalable and compact summarization of frames to support an effective overview. R3 Displaying a multifaceted overview of audio content. The linear reading habit makes searching audio clips time consuming. Therefore, the system should provide a multifaceted overview to enable the efficient exploration of audio content. DC2 In-depth Exploration of Risk Information. R4 Contextualizing risk information with multimodal content. Risk information is extracted from video or audio content. The system should combine multimodal content with associated risks to provide concrete visual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R5 Linking associated risk-aware visualizations in coordinated views.</head><p>A gap exists in the risk information extracted from multimodal content. The system should visually connect risk-aware visualizations in different views to facilitate quick labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Design</head><p>Figure <ref type="figure" target="#fig_2">4</ref> shows our interface with three major views, that is, a video view for navigating videos and identifying high-risk periods (Fig. <ref type="figure" target="#fig_2">4a</ref> for R1), an audio view for displaying a multifaceted risk-aware audio overview (Fig. <ref type="figure" target="#fig_2">4b</ref> for R2 and R4), and a frame view for providing a compact visual representation of videos (Fig. <ref type="figure" target="#fig_2">4c</ref> for R3 and R4).</p><p>A set of easy-to-use interactions is developed to link the associated risk-aware visualizations in coordinated views (R5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Video View</head><p>The video view consists of a video player and a segmented timeline that displays the risk distribution of different time periods. The video player integrates a set of common video tools (e.g., play/stop and fast-forward) to assist the moderators in flexibly browsing the video content. To highlight high-risk periods in a video (R1), we propose three alternative designs (Fig. <ref type="figure" target="#fig_2">4a</ref> from the bottom to the top), namely, a segmented timeline with risk indicators, a timeline with in-line flow visualization, and a timeline with triangle glyphs. The design with triangular glyphs merely highlights the high-risk moments but lacks the necessary risk context, which may lead to the skipping of potentially deviant frames.</p><p>To overcome this limitation, we insert a flow chart depicting the risk distribution of an entire video into the timeline. Nevertheless, the flow chart is excessively informative to highlight high-risk moments effectively. To balance the two extremes, we adopt a segmented timeline that is divided into different periods, in which the associated risk categories are indicated by color blocks. We encode the risk categories corresponding to the four moderation policies by using a color scheme extracted from ColorBrewer <ref type="bibr" target="#b24">[27]</ref> (see Fig. <ref type="figure" target="#fig_2">4d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Frame View</head><p>The frame comprises a multilevel overview of the video frames (R2) and a risk-aware circular glyph (R4) (see Fig. <ref type="figure" target="#fig_2">4c</ref>). To provide a concrete video summarization, we employ a well-established narrative model <ref type="bibr" target="#b37">[40,</ref><ref type="bibr" target="#b41">44]</ref> that characterizes video content at scene, shot, and frame levels. A shot refers to a set of similar consecutive frames (the orange box in Fig. <ref type="figure" target="#fig_3">5a</ref>), and a scene refers to a list of similar shots (the two blue boxes in Fig. <ref type="figure" target="#fig_3">5a</ref>). Moreover, we adopt circular glyphs that surrounds a video thumbnail with risk-aware visualizations to keep moderators aware of the risks (Fig. <ref type="figure" target="#fig_2">4h</ref>).</p><p>A video can be regarded as a linear visual signal consisting of consecutive frames. To accelerate video browsing, representative video frames have been extracted to present most informative content by using a list <ref type="bibr" target="#b26">[29]</ref> or small multiples <ref type="bibr" target="#b42">[45]</ref>, which is known as video summarization <ref type="bibr" target="#b70">[73]</ref>. However, these approaches may not be applicable to video moderation due to the gap between informative content and associated risks. For example, the frame with the highest risk value may not be regarded as informative by state-of-the-art methods <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b42">45]</ref>. To avoid skipping potentially deviant frames, we propose a novel visual summarization method that organizes video frames in an intuitive and compact layout. Our goal is to convert the reviewing of redundant video content into the browsing of several groups of video frames, which can be completed in a short period of time. Specifically, we employ a projection method to align video frames along the temporal dimension and adopt a clustering technique to group the projected frames visually.</p><p>Our approach regards the sampled frames and extracted risk information as the input and processes these data by using three steps: a) frame projection: video frames are typical high-dimensional data that can be projected into a low-dimensional space in accordance with their spatial similarity <ref type="bibr" target="#b59">[62]</ref> or semantic proximity <ref type="bibr" target="#b67">[70]</ref>. An intuitive way to visualize video frames is to project them into a 2D space that can facilitate the visual grouping of the frames. Nonetheless, 2D-based projections ignore the temporal context of video frames, which is important for discovering shot-level insights. Thus, we adopt the idea of temporal MDS plot <ref type="bibr" target="#b34">[37]</ref>, which can project the frames on the vertical axis. The horizontal axis represents the temporal dimension. Instead of using the MDS method <ref type="bibr" target="#b12">[15]</ref>, we employ the tSNE technique <ref type="bibr" target="#b59">[62]</ref> to project the video frames (Fig. <ref type="figure" target="#fig_3">5b</ref>) due to its strength in creating tight frame groups for visualization. b) shot clustering: once obtaining the projected frames, we detect the shots consisting of a set of similar frames. Considering that the relative vertical differences among the projected frames indicate their similarity, we further cluster them in accordance with their vertical positions (Fig. <ref type="figure" target="#fig_3">5c</ref>). Specifically, we employ the density-based clustering technique to group the video frames heuristically, in which each shot refers to a cluster of consecutive frames. c) scene alignment: we further group the shots to construct scenes according to their vertical positions. As shown in Fig. <ref type="figure" target="#fig_3">5d</ref>, we align the shots whose relative vertical distances are less than a threshold and a scene is constructed by a set of shots on the same horizontal level.</p><p>Given a scene, we further visualize and aggregate the frame risks to assist moderators in discovering deviant content promptly. Each scene S contains multiple frames {F i } i∈S . The risks of the i-th frame can be defined using vector F i = (p 1 , p 2 ,..., p s ), where p s indicates the score of risk tag a s . We aggregate the frame-based risks within a scene by R(S) = ∑ i∈S F i . As shown in Fig. <ref type="figure" target="#fig_2">4h</ref>, we consider three alternative designs, namely, rose diagram, radial bar chart, and donut chart, to visualize R(S). Moreover, we integrate the circular designs <ref type="bibr" target="#b16">[19]</ref> with a representative frame whose risk score is the highest in the scene (R4). The radial bar chart visualizes risk tags by using bar sectors and the bar height to encode the risk score, which is easy to interpret. The donut chart encodes the risk score by using the sector angle, which is scalable to sparse risk information. The rose diagram encodes the risk score with the sector area, which highlights extreme risk scores effectively. Considering that the three designs demonstrate their strengths in different tasks, we provide a radio button to enable users to select different designs in accordance with their requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Audio View</head><p>The audio view provides a multifaceted overview of streamers' speech (R3), which consists of a horizontal histogram and a storyline-based visualization (Fig. <ref type="figure" target="#fig_2">4b</ref>). The horizontal histogram is designed to demonstrate the frequency distribution of the risk words extracted from the audio content, in which each bar refers to a word. The height of a bar represents the count of the word mentioned in the speech, and its color represents the risk category, which is the same as the circular glyph. We rank the risk words in descending order to enable moderators to review the risky content from highest to lowest. However, moderating audio content by only reviewing the risk word summaries is insufficient, because moderators cannot make a reliable decision without the temporal context. Thus, we adopt storyline-based visualization to reveal the temporal concurrence of risk words.</p><p>Audio content can be regarded as a sequence of sentences translated using an ASR technique <ref type="bibr" target="#b45">[48]</ref>. Each sentence can be denoted as a tuple (start time, end time, words) (Fig. <ref type="figure" target="#fig_4">6a</ref>), in which some words are considered risky if they violate moderation policies. To review audio content, moderators must know not only which risk word but also when it is mentioned. They must also know what other risk words are spoken concurrently. We consider several alternative designs to provide such a complex context. We consider a segmented timeline, in which each line represents a risk word and each filled segment indicates a word spoken during a certain period (Fig. <ref type="figure" target="#fig_4">6b</ref>). The strength of the segmented timeline lies in its ability to demonstrate which risk word and when it is spoken effectively. Understanding concurrent words in the same period is easy, but tracking the historical context may be difficult owing to the line segment disconnectivity. Moreover, the design may cost redundant vertical space when risk words are sparse. To overcome these limitations, we first compress the vertical space of the line segments then connect the segments of the same words to make them trackable (Fig. <ref type="figure" target="#fig_4">6c</ref>). The connection of line segments may produce heavy visual clutter, as it increases line crossings and wiggles. To obtain a legible and aesthetic layout, we employ the storyline algorithm to minimize the line crossings and wiggles <ref type="bibr" target="#b57">[60,</ref><ref type="bibr" target="#b58">61]</ref>. Moreover, we visualize the risk words using a word cloud to replace line groups to enable moderators to obtain a highly intuitive and comprehensive context (Fig. <ref type="figure" target="#fig_4">6d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interactions</head><p>In addition to basic interactions, such as playing or fast-forwarding a video, VideoModerator integrates several other interactions to facilitate the fast reviewing of multimodal video content (R5).</p><p>• Locating high-risk periods. Moderators can locate high-risk periods by using the segmented timeline in Fig. <ref type="figure" target="#fig_2">4a</ref>. The system will update the video content during the located period when moderators click on the blocks. • Linking frames with risk tags. Moderators can link video frames with associated risk tags to review deviant content quickly. In Fig. <ref type="figure" target="#fig_2">4b</ref>, the system will automatically enlarge the associated frames when moderators hover over the sectors of the risk glyph. Moreover, moderators can update video content by clicking on a frame to locate deviant content promptly. • Exploring audio context. Moderators can effectively explore audio context by clicking on a risk word or a histogram bar in Fig. <ref type="figure" target="#fig_2">4c</ref>. The system will enlarge the links to highlight the associated temporal context. Moreover, moderators can click on a word cloud to specify the period they want to review further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USAGE SCENARIO</head><p>This section presents a usage scenario to demonstrate the usability <ref type="bibr" target="#b60">[63]</ref> and usefulness of VideoModerator. We describe how Alan, an experienced moderator, reviews multimodal video content to discover deviant clips. He starts his task by loading an e-commerce video, in which two streamers are advertising their orange juice (Fig. <ref type="figure" target="#fig_2">4</ref>). After loading the video, he notices that the segmented timeline immediately demonstrates the risk distribution across the entire video, whose deviant clips may be in the middle part and second half (Fig. <ref type="figure" target="#fig_2">4a</ref>). He then turns to the frame view and observes that the video is divided into four scenes, namely, the first scene with the two streamers and a QR code, the second scene with a turtle picture, the third scene with a tree background, and the fourth scene with a drink (from top to bottom). He first examines the top scene because it shows the highest risk (Fig. <ref type="figure" target="#fig_2">4f</ref>). He immediately finds that the QR code disobeys the policy of inappropriate business because it links to a competitive e-commerce platform. Once confirming one deviant clip and associated policy, he could choose to complete the task by clicking the label button (Fig. <ref type="figure" target="#fig_2">4d</ref>). However, he is interested in whether this video violates other polices and decides to examine the second scene wherein a turtle picture is shown. He has no clues about the potential risk of the shown picture, so he hovers the rose diagram and finds that this is a protected turtle in accordance with the shown tooltip (Fig. <ref type="figure" target="#fig_2">4e</ref>). Thus, he confirms that the video also disobeys the policy of protected animals that are not allowed to sell. He also discovers a possible deviant clip that shows no risk in the frame view (Fig. <ref type="figure" target="#fig_2">4g</ref>). Therefore, he turns to the audio view and examines the most left word cloud. He notices that the streamers claim that their product can cure any disease, which is beyond the scope of a drink and violates the policy of false advertising. Owing to the benefits of multimodal visualizations, he has discovered all deviant clips and determined to finish the task by assigning the most apparent risk label. The online demo can be found at https: //videomoderator.github.io/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>Implementation We employ a client-server architecture to implement our mixed-initiative framework. A back-end server is developed to integrate various machine learning models, and a visualization interface is developed to facilitate multimodal video moderation. We implement the interface by using React.js, and develop visualizations by using D3.js. We implement the back-end server in Python and develop the machine learning models with a state-of-the-art framework, namely, PyTorch. We deploy the back end on cloud computing devices and connect the client and server by using the web protocol HTTP.</p><p>We conducted online experiments to understand the performance of the machine learning models incorporated in VideoModerator. The main goal of the experiments is to validate the effectiveness of the general framework instead of evaluating specific machine learning models. By collaborating with the e-commerce company, we deployed our back-end on high-performance computing clusters powered by elastic algorithm services 1 . We trained our models with the livestreaming videos reviewed by moderators to detect sensitive content, including abusing (audio) and explicit visual content (video). There were 21 risk tags associated to explicit visual content and 11 risk words related to abusing sentences. We mainly focused on these specific risks because covering full risks required a large amount of computational resources which were not accessible in the experiments.</p><p>The trained models were tested using the real-world videos on our collaborators' moderation platform. In total, our models processed around 100 thousands live streaming videos, including 0.4 billions video frames and 36 millions audio clips. The average accuracy of the models are 33.2% and 35% for detecting audio and video risks, respectively. The recalling rate of the binary classifier is 99.5% for filtering possible deviant videos with abusing or explicit content. The model accuracy is far from the expectations due to the diversity and complexity of real-world video data. However, they are still acceptable according to our collaborators because platforms intend to have higher recalling rates, even by sacrificing model accuracy in real practices. This strategy is understandable because the main goal of e-commerce platforms is to protect not prevent user-generated content. Thus, moderators would like to have things under control and review "possibly" deviant videos to avoid wrongly-detect videos as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">USER EVALUATION</head><p>This section presents an in-lab user study to evaluate the user interface of VideoModerator by comparing it with a baseline method.</p><p>Baseline. VideoModerator is established on the basis of idea of showing moderators' additional but essential information, which can possibly increase their performance in reviewing videos. Our hypothesis is that our visualization designs are effective, such that the benefits of exhibiting machine insights are higher than the cost of reading additional information. To validate this hypothesis, we conducted a task-based user study that compared VideoModerator with a baseline tool that merely presented original video content. Although many reviewing tools [2, 3] satisfied our requirements, we selected Content-Moderator <ref type="bibr" target="#b1">[3]</ref> as the baseline method due to its detailed documents and intuitive interface (Fig. <ref type="figure" target="#fig_5">7</ref>). We re-implemented ContentModerator, which comprised video and frame components, to make it suitable for our studies. Specifically, we changed the video component to make it similar to our video view and presented the same frames in Fig. <ref type="figure" target="#fig_5">7a</ref>.</p><p>Participants. Our experiment used a within-subject design. We recruited 12 volunteers (6 males and 6 females) to complete two groups of video moderation tasks by using VideoModerator and the baseline tool. Their average age was 24.3 years old (range=21-28 years). All participants could read basic data visualizations, such as bar charts, and were knowledgeable about machine learning models. They were from various backgrounds, including computer science, social science, and industry design. Two of them had been involved in jobs related to video moderation, while the others knew it from news or social media. Each participant was paid 10$ for the 1-h study.</p><p>Data and Tasks. To simulate the real environment of video moderation, we first collected a bunch of videos that were detected as possibly deviant by machine learning models. We divided the videos into three groups in accordance with the video length, namely, the short group (&lt; 5 min), the long group (&gt; 30 min), and the medium group (between two bounds). The video length indicated the difficulty of reviewing videos, in which longer videos usually required more reviewing efforts. To construct a diverse dataset, we randomly selected videos from the three groups and ensured that each group contained at least two selected videos. In total, we obtained 20 videos, whose average duration was 8.6 min (ranging from 10 s to 41 min). Given that these videos may be wrongly recalled, we determined their ground-truth labels by collaborating with the two experts who were also involved in our discussion about e-commerce moderation policies (Sec. 3). Five out of the 20 videos were normal videos, whereas the others were deviant videos.</p><p>Video moderation refers to the process in which moderators first review a video and then assign a label to the video. We defined one normal label and four deviant labels that corresponded to the four moderation policies discussed in Sec. 3. Our task required the participants to moderate videos by assigning them a normal or risk label. We used two videos to demonstrate the two reviewing tools and enable the participants to practice. We used the rest of the videos for formal testing and obtained 18 tasks (T1-T18) in total.</p><p>Procedure. The study consisted of three stages, namely, training (20 min), testing (30 min), and interviewing (10 min). At the first stage, we demonstrated the basic concepts of video moderation and introduced the four policies that would be used to review videos. Considering that the participants had various backgrounds, we trained them with concrete examples to remove the ambiguity and ensure that they had the same level of understandings about the policies. The testing stage was further divided into two similar phases (15 min). At each phase, the participants were first required to complete a practice task and then finish the formal tasks. During the practice task, the participants could freely explore reviewing tools and ask for our assistance at any time, which were not allowed for the formal tasks. We counterbalanced the order of two reviewing tools and tasks for a fair comparison. After the participants completed the formal tasks with one tool, they were asked  to give a rating on it by completing a subjective questionnaire.</p><p>Questionnaire and Interview. To compare VideoModerator and the baseline tool further, we collected subjective ratings and qualitative feedback from the participants. Specifically, the participants were required to give scores on each moderation tool by using five-point Likert scales from three perspectives, namely, easy of use, easy of learning, and effectiveness. We also interviewed the participants to understand their user experiences and obtain their opinions about model interpretability <ref type="bibr" target="#b40">[43]</ref>. The questions were as follows: Q1 How often each component was used when reviewing videos? Q2 What order was followed when using these components? Q3 What are your suggestions for further improvements?</p><p>Results. We defined two crucial metrics, namely, time cost and accuracy, to measure the performance of each reviewing tool. For each task, we calculated the average time cost of moderating the video and obtained the accuracy by comparing the labels assigned by the participants with the ground-truth labels. In accordance with Fig. <ref type="figure" target="#fig_6">8</ref>, the average time cost of our system is less than that of the baseline tool (32.5s &lt; 39.6s), but our accuracy is higher (72% &gt; 62%). The results indicated that VideoModerator could achieve better performance than the baseline tool in terms of the two metrics. We also observed three tasks, namely, T2, T11, and T14, in which the participants spent more time with VideoModerator. Task T2 showed a game seller playing a 40-min video game that contained diverse and numerous virtual scenes, which produced a large number of shot clusters. The participants spent more time on browsing video frames with our tool but finally discovered deviant clips due to the machine-generated insights, which produced higher accuracy than the baseline method. Task T4 contained a deviant clip at the end of the video, which was difficult to identify. The participants who used the baseline tool promptly browsed the video but assigned a wrong label due to the neglect of the deviant clip. Task T14 was a normal video but was detected as a deviant one by using machine learning models. In general, the participants spent more time on thinking whether the machine-generated insights were correct by using VideoModerator. The baseline tool did not present such contradicted information so that the participants could make quick decisions and achieve comparable performance in T14. By comparing the statistics reported in the experiments, we found that there was a significant improvement over the machine learning models due to the benefits of human involvement. Despite this comparison is not rigorous, the huge increase indicates that it is necessary to involve users into the moderation process by combining human and machine intelligence.</p><p>The participants also filled questionnaires to compare the two reviewing tools with regard to easy of use, easy of learning, and effectiveness. Figure <ref type="figure">9</ref> presents the subjective questions and average user scores. The two systems have comparable performance in terms of easy of use and easy of learning. Considering that all scores are above 4, we concluded that novice users could complete complex moderation tasks with minimum guidance by using VideoModerator. Moreover, VideoModerator and its two components, namely, the video and frame views, are considered more effective than the baseline conditions. This encouraging outcome indicates that the benefits of visualizing machine-generated insights are larger than the costs of integrating additional information. Our audio view could also be considered useful due to its high scores.</p><p>Fig. <ref type="figure">9</ref>. Analysis of the ratings: all ratings of VideoModerator were greater than four (95%CI), which was very encouraging.</p><p>User Feedback. We collected the participants' qualitative feedback about VideoModerator to understand model interpretability and the usefulness of each component (Q1), discuss the general workflow of moderators (Q2), and obtain general suggestions (Q3).</p><p>Usability and Interpretability . Ten participants were impressed by the frame view that visually summarized video content and the rose diagrams that informed users of machine-generated risk labels. P1 mentioned that"The scene projection and shot-clustering methods remarkably decreased the search space and saved the time in watching duplicate frames." The participants confirmed the benefits of integrating risk information and the intuitiveness of the visualization designs. However, the segmented timeline was helpful but was only used when users could not locate risk frames or deviant audio clips. P11 mentioned that "I only use it to confirm whether the video is normal because the timeline already demonstrated the overall risk distribution." We observed that the timeline seemed to be more isolated from the video content than the other visualization components. We infer that users intend to receive the machine insights which are deeply embedded with the video context. The audio view was not as frequently used as the two other views because most deviant videos could be determined solely by using visual information. However, the storyline-based design was appreciated by the participants due to its intuitiveness and expressiveness for visually summarizing audio content. As P2 indicated, "I could not listen to multiple audio clips at once, but, now, I could 'watch' them because they were transformed into an image." This comment indicates the big advantages of providing risk information once the machine-generated findings can be easily interpreted.</p><p>Workflow. The participants showed diverse workflows because of their preferences and personal working experiences. Eight participants started moderation tasks with the frame view, whereas two participants started with the audio view. The rest of the participants did not follow any specific workflow and decided where to start in accordance with the shown risks. P6 mentioned that"I intended to start with the frame view or audio view because they directly presented the detected deviant content." Despite that all participants confirmed that the video view was necessary, they thought it mainly served as a detailed view. Most of them used the video view as the second step. The audio view was usually used as the third step to inspect audio content. The video view with the segmented timeline would be again to confirm whether the video is normal if no evident deviant clips were discovered.</p><p>Suggestions. The participants provided valuable suggestions on how to improve VideoModerator. P10 suggested that "It would be better to enable users to combine similar scenes so that users could focus on most risky scenes in the frame view." P8 provided a practical advice for the audio view that enabling users to highlight frames by clicking the horizontal bars could foster a better review of multimodal content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">DISCUSSION</head><p>This section summarizes the generalization, lessons learned, limitations and future work for the proposed system.</p><p>Generalization. Although our study is established on e-commerce livestreaming videos, the proposed system can be extended to other advertising videos because these videos share the same narrative elements. In addition to e-commerce, VideoModerator can also be applied to other applications (e.g., surveillance) because the framework employs a "learning with reviewing" strategy that improves machine learning models when dealing with a new type of videos. Video moderation is an inter-discipline among video summarization, video visualization <ref type="bibr" target="#b9">[12]</ref>, and video annotation <ref type="bibr" target="#b28">[31]</ref>. Generally, video moderation is regarded as a more complicated task than the video annotation task <ref type="bibr" target="#b14">[17]</ref> due to a larger search space that requires well-trained human moderators. However, the shared workflow in which crowd workers first review videos and then assign labels makes it possible to extend our framework to video annotation. Our storyline-based design sheds light on text visualizations. Despite that we mainly focus on risk words, the storyline-based design can be easily extended to temporal text data, in which named entities can replace risk words to construct storylines.</p><p>Lessons Learned. We present the design lessons learned when developing VideoModerator. The visual exploration of machine-generated insights can foster an enhanced collaboration between models and human moderators. Although the typical framework (Fig. <ref type="figure">1</ref>) has already integrated automatic models to moderate videos, it mainly targets at the machine side while ignoring the possibility of improving moderators' performance. Existing moderation tools [2, 3] merely present original videos; thus, the reviewing process is tedious and time consuming. Our study successfully demonstrates that showing moderators additional necessary information will not hinder the reviewing processes but improve their time efficiency and accuracy. Hence, our framework provides a new paradigm for video moderation systems. We also summarize three design guidelines to inspire the usage of explainable artificial intelligence (AI) in visual analytic systems. a) Interpretability: as "a picture is worth a thousand words," it becomes a common practice to visualize the machine-generated scores rather than telling users the exact numbers. b) Proximity: the visualizations of AI-generated results should be closely accompanied with the original information so that users can understand data insights promptly. c) Simplicity: it is necessary to avoid over-exaggerated design or enable excessive control because the cognitive capability of users may be overwhelmed.</p><p>Limitations and future work. Our work has several limitations. First, a gap exists between high-level risk labels and low-level risk words, which hinders the usage of the "learning with reviewing" strategy in ASR techniques. To bridge the gap, we intend to detect risk words from audio clips directly so that the labeled videos could be used as the training data. Second, the projection and clustering methods may produce numerous frame groups when dealing with videos that contain diverse scenes or shots. We aim to adopt a hierarchical clustering method to combine redundant video shots. Third, the scope of this study mainly focuses on moderating off-line videos. Some online features of livestreamings, such as the interactions between livestreamers and audience, are also important to demonstrate underlying risks, which inspires promising future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>We have presented VideoModerator, a risk-aware framework that enables quick navigation and supports fast moderation of deviant videos in e-commerce. The framework bridges the gap between human moderators and machine learning models through interactive multimodal video visualizations. First, our framework adopts a "learning with reviewing" strategy that iteratively improves model accuracy by using the ground-truth labels provided by users. Second, we propose an interactive interface that visualizes multimodal machine-generated insights to foster an improved mixed-initiative environment. Our evaluation indicates that VideoModerator can assist users in promptly understanding video content and effectively discovering deviant clips. In the future, we plan to deploy our system in real-world applications and evaluate it through crowd-sourcing studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. E-commerce livestreaming examples. (a) The four essential narrative elements of e-commerce livestreams. (b), (c), and (d) are frames violating the policies of false advertising, protected products, and inappropriate business. The dashed white box is manually annotated.</figDesc><graphic coords="3,370.67,52.01,58.58,87.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The risk-aware framework for video moderation: (a) video processing employs multimodal techniques to process the audio and visual information of livestreaming videos separately. (b) video filtering intends to select possibly deviant videos which would be further reviewed by moderators. (c) video reviewing employs interactive visualizations to facilitate efficient video moderation by keeping moderators aware of risks. The solid lines refer to video data and associated risk information, whereas the dashed lines refer to ground-truth labels provided by moderators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Multimodal user interface of VideoModerator: (a) a video view with a segmented timeline that demonstrates risk distributions; (b) an audio view with a combination of histograms and a storyline-based words visualization; (c) a frame view that summarizes the video frames; (d) a control panel that integrates a color legend encoding four risk categories; (e) and (f) are discovered insights for video moderation; (g) a moving window that visually associates audio and frame content; (h) circular glyphs that visualize risk-aware information. The dashed lines are manually added.</figDesc><graphic coords="5,75.71,52.01,483.62,284.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The visual video summarization method: (a) input: video frames and the extracted risk information; (b) frame projection: video frames are projected on the vertical axis using tSNE; (c) shot clustering: the projected frames are clustered using the density-based clustering technique; (d) scene alignment: the similar shots are aligned to construct different scenes. The vertical axis represents the 1D projection space in (b), (c) or scenes in (d). The horizontal axis refers to the timeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The visual summarization of audio content: (a) the input data contains four risk words; (b) a histogram and a segmented timeline to demonstrate when and which risk word is spoken; (c) a storyline layout is employed to connect line segments and compress redundant vertical space; (d) word clouds are adopted to provide specific context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. ContentModerator<ref type="bibr" target="#b1">[3]</ref> consists of (a) small multiples to visualize video frames, (b) a player to browse both video and audio content, and (c) a form view presenting meta-data and risk information.</figDesc><graphic coords="8,45.59,601.01,245.21,100.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The completion time and task accuracy with 95% CI.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.alibabacloud.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work was supported by NSFC (62072400) and Zhejiang Provincial Natural Science Foundation (LR18F020001). This work was also supported by Alibaba-Zhejiang University Joint Institute of Frontier Technologies and the Collaborative Innovation Center of Artificial Intelligence by MOE and Zhejiang Provincial Government (ZJU).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An introduction to YouTube policies and guidelines. Website</title>
		<ptr target="https://creatoracademy.youtube.com/page/lesson/copyright-guidelines#strategies-zippy-link-3" />
		<imprint>
			<date type="published" when="2020">Retrieved Dec. 1st. 2020</date>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Video moderation with the Review tool. Website, 2020. Retrieved</title>
		<ptr target="https://docs.microsoft.com/en-us/azure/cognitive-services/content-moderator/video-moderation-human-review" />
		<imprint>
			<date>Dec. 1st</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theoretical model for pattern discovery in visual analytics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Violence Detection in Video Using Computer Vision Techniques</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bermejo Nievas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State of the Art Report on Video-Based Graphics and Video Visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Daubney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leitte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2450" to="2477" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action-Based Multifield Video Visualization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Botchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bachthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="885" to="899" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Live Streaming Commerce: Uses and Gratifications Approach to Understanding Consumers&apos; Motivations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Wohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Hawaii International Conference on System Sciences</title>
				<meeting>the Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal Classification of Moderated Online Pro-Eating Disorder Content</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chancellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
			<biblScope unit="page" from="3213" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Thornton. Visual Signatures in Video Visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Botchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hashim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1093" to="1100" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Augmenting Sports Videos with VisCommentator</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>p. to appear</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Chinchor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Christel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Analysis + Visual Analytics = Multimedia Analytics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Limitations of Deep Learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning with Python. Manning</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multidimensional Scaling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Data Visualization</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="315" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video Visualization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization</title>
				<meeting>the IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EventAnchor: Reducing Human Interactions in Event Annotation of Racket Sports Videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glyph-Based Video Visualization for Semen Analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thiyagalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trefethen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Kirkman-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Gaffney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="980" to="993" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gone full circle: A radial approach to visualize event-based networks in digital humanities</title>
		<author>
			<persName><forename type="first">V</forename><surname>Filipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Schetinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Soursos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zapke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="60" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Audio-Visual Fusion for Detecting Violent Scenes in Videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Perantonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence: Theories, Models and Applications</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gillespie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Yale University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DanceVis: Toward Better Understanding of the Cheer and Dance Training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>p. to appear</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Empowering First Responders through Automated Multimodal Content Moderation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumaraguru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Buduru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Cognitive Computing</title>
				<meeting>the IEEE International Conference on Cognitive Computing</meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Creating Summaries from User Videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="505" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Netv.js: A web-based library for high-efficiency visualization of large-scale graphs and networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Informatics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="61" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bidirectional Convolutional LSTM for the Detection of Violence in Videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pnvr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnagopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops</title>
				<meeting>the European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="280" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ColorBrewer.org: An Online Tool for Selecting Colour Schemes for Maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harrower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="27" to="37" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Violent flows: Real-time detection of violent crowd behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Itcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Augmented segmentation and visualization for presentation videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Multimedia</title>
				<meeting>the ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inter-active Learning of Ad-hoc Classifiers for Video Visual Analytics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Netzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visual Analytics Science and Technology</title>
				<meeting>the IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interactive Schematic Summaries for Faceted Exploration of Surveillance Video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="908" to="920" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable Video Visual Analytics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="26" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geo-Context Aware Study of Vision-Based Autonomous Driving Models and Spatial Video Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jamonnak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amiruzzaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>p. to appear</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online Harassment and Content Moderation: The Case of Blocklists</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jhaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual Exploration of Topics in Multimedia News Corpora</title>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Information Visualisation</title>
				<meeting>the International Conference Information Visualisation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal mds plots for analysis of multivariate data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jäckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="150" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Space-Time Video Montage</title>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regulating Behavior in Online Communities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kiesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kraut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Building successful online communities: Evidencebased social design</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="125" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznecov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<title level="m">Visual Movie Analytics. IEEE Transactions on Multimedia</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2149" to="2160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Slash (dot) and Burn: Distributed Moderation in a Large Online Conversation Space</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RallyComparator: Visual Comparison of the Multivariate and Spatial Stroke Sequence in A Table Tennis Rally</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>p. to appear</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">EmotionMap: Visual Analysis of Video Emotional Content on a Map</title>
		<author>
			<persName><forename type="first">C.-X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-A</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="576" to="591" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">VSCAN: An Enhanced Video Summarization Using Density-Based Spatial Clustering</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Analysis and Processing</title>
				<meeting>the International Conference on Image Analysis and Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interactive Exploration of Surveillance Video through Action Shot Summarization and Trajectory Visualization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Meghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2119" to="2128" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Streaming automatic speech recognition with the transformer model</title>
		<author>
			<persName><forename type="first">N</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<biblScope unit="page" from="6074" to="6078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video summagator: an interface for video summarization and navigation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="647" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Toward Subjective Violence Detection in Videos</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pereira Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="8276" to="8280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reverse-Engineering Visualizations: Recovering Visual Encodings from Chart Images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">FaceCloud: Heterogeneous Cloud Visualization of Multiplex Networks for Multimedia Archive Exploration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Renoust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Melanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Viaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
				<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1235" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Commercial Content Moderation: Digital Laborers&apos; Dirty Work</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Media Studies Publications</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Viz-A-Vis: Toward Visualizing Video through Computer Vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Summet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Abowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1261" to="1268" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Eye in the Sky: Real-Time Drone Surveillance System (DSS) for Violent Individuals Identification Using ScatterNet Hybrid Deep Learning Network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Omkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Soure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>p. to appear</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">How Live Streaming Purchase Intentions in Social Commerce: An IT Affordance Perspective</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Commerce Research and Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">100886</biblScope>
			<date type="published" when="2019-08">08 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Plotthread: Creating Expressive Storyline Visualizations using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knittel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="294" to="303" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">iStoryline: Effective Convergence to Hand-drawn Storylines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rubab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="769" to="778" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">What Makes a Scatterplot Hard to Comprehend: Data Size and Pattern Salience Matter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>p. to appear</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Tac-Miner</surname></persName>
		</author>
		<title level="m">Visual Tactic Mining for Multiple Table Tennis Matches. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2770" to="2782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Towards Better Bus Networks: A Visual Analytics Approach. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="817" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multimodal Analysis of Video Collections: Visual Exploration of Presentation Techniques in TED Talks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2429" to="2442" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01330</idno>
		<title level="m">AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Not only Look, But Also Listen: Learning Multimodal Violence Detection Under Weak Supervision</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="322" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Towards Better Detection and Analysis of Massive Spatiotemporal Co-Occurrence Patterns</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3387" to="3402" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A semantic-based method for visualizing large image collections</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2362" to="2377" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">PassVizor: Toward Better Understanding of the Dynamics of Soccer Passes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1322" to="1331" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="860" to="869" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Techniques for movie content analysis and skimming: tutorial and overview on video abstraction techniques</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Hung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Hung</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="89" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">A Survey of Visual Analytics Techniques for Machine Learning. Computational Visual Media</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Towards interactive, intelligent, and integrated multimedia analytics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zahalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visual Analytics Science and Technology</title>
				<meeting>the IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>p. to appear</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="927" to="937" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Preserving Minority Structures in Graph Sampling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1698" to="1708" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
