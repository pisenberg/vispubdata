<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context Matters: A Theory of Semantic Discriminability for Perceptual Encoding Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kushin</forename><surname>Mukherjee</surname></persName>
							<email>kmukherjee2@wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><surname>Yin</surname></persName>
							<email>brianyin@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Kushin Mukherjee, Psychology and Wisconsin Institute for Discovery</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Brianne E. Sherman, Neurobiology and Wisconsin Institute for Discovery</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Karen B. Schloss, Psychology and Wisconsin Institute for Discovery</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brianne</forename><forename type="middle">E</forename><surname>Sherman</surname></persName>
							<email>besherman2@wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Lessard</surname></persName>
							<email>l.lessard@northeastern.edu</email>
						</author>
						<author>
							<persName><forename type="first">Karen</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
							<email>kschloss@wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Kushin Mukherjee, Psychology and Wisconsin Institute for Discovery</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Brianne E. Sherman, Neurobiology and Wisconsin Institute for Discovery</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Karen B. Schloss, Psychology and Wisconsin Institute for Discovery</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Cognitive Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Mechanical and Industrial Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context Matters: A Theory of Semantic Discriminability for Perceptual Encoding Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D5F02912171158BA6B1B4CDCC0658F5D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large distribution difference Medium distribution difference Small distribution difference High capacity for semantic discriminability Medium capacity for semantic discriminability Low capacity for semantic discriminability Visual Reasoning</term>
					<term>Information Visualization</term>
					<term>Visual Communication</term>
					<term>Visual Encoding</term>
					<term>Color Cognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Color-concept association distributions for concept pairs with large, medium, and small distribution differences, resulting in high, medium, and low capacities for semantic discriminability, respectively (terms defined in Section 3). Color-concept association ratings were collected in Experiment 1 for the UW-71 colors (colored stripes in the plots, sorted by CIE LCh hue angle).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Bananas are shades of yellow, blueberries are shades of blue, and cantaloupes are shades of orange. It is well-established that color semantics influences people's ability to interpret information visualizations when those visualizations represent concepts that have specific, strongly associated colors (e.g., fruits). Such visualizations are easier to interpret if concepts are encoded with strongly associated colors (e.g., bananas encoded with yellow, not blue) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref>. But, how often do real-world visualizations really depict information about fruit, or other concepts with specific, strongly associated colors? If color semantics mainly influences interpretability for visualizations of concepts with specific, strongly associated colors (as previously suggested <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>), then scenarios in which color semantics matters would be severely limited.</p><p>The present study suggests people's ability to infer meaning from colors is more robust than previously thought. Conditions arise in which people can interpret meanings of colors for concepts previously considered "non-colorable". Specifically this when the colors are semantically discriminable. Semantic discriminability for colors is the ability to infer unique mappings between colors and concepts based on colors and concepts alone (i.e., without using a legend) <ref type="bibr" target="#b30">[31]</ref>. This is distinct from semantic interpretability, which is the ability to interpret the correct mapping between colors and concepts, as specified in an encoding system (for further discussion of this distinction, see <ref type="bibr" target="#b30">[31]</ref> and Supplementary Material Section S.7 in the present paper). The key question is, what determines whether it is possible to select semantically discriminable colors for a set of concepts?</p><p>We address this question in semantic discriminability theory, a new theory on constraints for generating semantically discriminable perceptual features for encoding systems that map perceptual features to concepts. We tested two hypotheses that arise from the theory. First, the capacity to create semantically discriminable color palettes for a set of concepts depends on the difference in color-concept association distributions between those concepts, independent of properties of the concepts in isolation (Experiment 1). Second, people can accurately interpret mappings between colors and concepts for concepts previously considered "non-colorable," to the extent that the colors are semantically discriminable (Experiment 2). We focus on color in this study, but present the theory in terms of perceptual features more generally because of its potential to extend to other types of visual features (e.g., shape, orientation, visual texture) and features in other perceptual modalities (e.g., sound, odor, touch).</p><p>Contributions. This paper makes the following contributions: <ref type="bibr" target="#b0">(1)</ref> We define semantic discriminability theory (Section 3) and test hypotheses motivated by the theory in Experiments 1 and 2 (Sections 4-5), and <ref type="bibr" target="#b1">(2)</ref> We define a new metric for operationalizing distribution difference between sets of more than two concepts (Section 3.2) and show that it predicts capacity for semantic discriminability (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Color is a strong cue for signaling meaning in nature and some argue that color vision evolved for the purpose of visual communication <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref>. Historically, discussions on the role of color semantics in information visualization have tended to focus on few cases of typical associations (e.g., red for hot, green for grass) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. More recent work has sought to understand the potential and limitations of using color to communicate meaning in visualizations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. The semantics of color in visualizations operates on two main levels: meaning of a color palette as a whole <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15]</ref> and meaning of the individual colors in a palette <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. We focus on meanings of individual colors because that is central to the present work. People have expectations about how colors will map onto concepts, and visualizations that violate those expectations are harder to interpret, even if there is a legend <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. Thus, understanding these expectations is important for optimizing palette design for visual communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Color-concept associations</head><p>Color-concept associations represent the degree to which individual colors are associated with individual concepts. Color-concept associations can be quantified using various methods, including human judgments <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>, image statistics <ref type="bibr">[20-22, 27, 33]</ref>, and natural language corpora <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. Some approaches focused on identifying the strongest, or strongest few colors associated with a concept <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref>, but color-concept associations can be treated as a continuous property over all possible colors in a perceptual color space <ref type="bibr">[20-22, 27, 29]</ref>. When quantifying color-concept associations over all of color space, researchers typically bin or sub-sample parts of the space to make measurements computationally tractable. An assumption is that the space is continuous, so nearby colors will have similar associations. Figure <ref type="figure">1</ref> shows examples of color-concept associations for colors systematically sampled over CIELAB space (see Experiment 1), plotted over one dimension (sorted by hue angle and chroma with achromatics at the beginning of the list). Perceptual color spaces are three-dimensional so this representation does not necessarily position perceptually similar colors in close proximity <ref type="bibr" target="#b40">[41]</ref>, but it does highlight how some concepts, like peach and celery, have specific, strongly associated colors, whereas other concepts, like driving and comfort, are more uniform (Figure <ref type="figure">1</ref>). We refer to this 'peakiness' property as specificity of the color-concept association distributions. 1  Questions remain concerning how color concept-associations are formed, but many have suggested that they are learned through experiences <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> and may be continually updated through each new experience in the world <ref type="bibr" target="#b28">[29]</ref>. Some color-concept associations are shared cross-culturally, and others are subject to cultural differences <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref>. We will consider the role of cultural differences with respect to the present work in the General Discussion.</p><p>Color-concept associations contribute to people's expectations about the meanings of colors in information visualizations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, called inferred mappings. However, associations and inferred mappings are not the same, and sometimes they conflict <ref type="bibr" target="#b31">[32]</ref>. We explain this point in Section 2.3 on assignment inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Colorabilty scores</head><p>Some have suggested that the effectiveness of colors for encoding meaning is limited to concepts that have strong associations with particular colors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. This idea is explained by invoking colorability scores, which broadly measure how strongly individual concepts can be mapped to specific colors. Generally, concepts with specific, strongly associated colors ('banana') are thought to be colorable, whereas more abstract concepts, such as 'comfort' or 'leisure', that lack such strongly associated colors, have been called non-colorable.</p><p>Different methods have been used to define colorability. Lin et al. <ref type="bibr" target="#b19">[20]</ref> quantified colorability by having participants assign colors to concepts and rate the strength of the assignment. The mean of these ratings over all colors for a concept was used to generate a colorability score for that concept. They found that participants were better at interpreting bar charts when palettes were optimized for color semantics compared to when palettes had the default Tableau color ordering, but this benefit was mostly limited to highly colorable concepts. Setlur and Stone <ref type="bibr" target="#b32">[33]</ref> quantified colorability with an automated method, using Google N-grams to determine how frequently a concept word co-occurred with basic color terms <ref type="bibr" target="#b2">[3]</ref> in linguistic corpora. They then excluded concepts they found to be non-colorable when developing methods to optimize palette design.</p><p>These prior studies highlighted the importance of considering color semantics in palette design. However, our work suggests that restricting notions of colorability to concepts in isolation may have led to underestimating people's ability to infer meaning from colors in visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Assignment inference</head><p>Evidence suggests that people's inferences about the meanings of colors in encoding systems of visualizations do not merely depend on color-concept associations in isolation. We illustrate this point with an example from Schloss et al. <ref type="bibr" target="#b31">[32]</ref>. Participants saw pairs of unlabeled bins and were asked to choose which bin was for the target concept written at the top of the screen. Figure <ref type="figure" target="#fig_0">2</ref> shows two examples when trash was the target concept. The other concept, not pictured here but judged on other trials, was paper. To the left of the example trials are bipartite graphs, which use line thickness to represent the association strength between each concept (trash, T, and paper, P) and each color in the corresponding trial. An easy way to approach this task would be to choose the color that is most strongly associated with trash within each trial (local assignment). Alternatively, participants could choose the color that results in maximizing association strengths of all color-concept pairings across trials (global assignment).</p><p>In the top row of Figure <ref type="figure" target="#fig_0">2</ref>, these two approaches lead to the same outcome. Locally, trash is more strongly associated with dark yellow (Y) than white (W). Globally, the assignment trash-yellow/paper-white has a larger overall association strength than trash-white/paper-yellow. Not surprisingly, participants inferred trash is mapped to dark yellow. However, in the bottom row, the two approaches lead to opposite outcomes. Locally, trash is more associated with white than purple (Pu), but globally the assignment trash-purple/paper-white has a larger overall association strength (greater total thickness of edges) than trashwhite/paper-purple. Participants inferred that trash maps to purple, even though white was a more strongly associated alternative. Each trial was independent, so participants need not account for paper on trials for trash, but they did so nonetheless. This example highlights the  <ref type="bibr" target="#b31">[32]</ref>). Left: Bipartite graphs show colorconcept association strengths for concepts trash (T) and paper (P) with colors dark yellow (Y), white (W), and purple (Pu) (thicker edges connecting concepts and colors indicate stronger associations). Right: example trials where participants infer which color maps to trash.</p><p>important distinction between color-concept associations for a single color and concept, and inferred mappings between a color and concept in the context of an encoding system. Schloss et al. <ref type="bibr" target="#b31">[32]</ref> called this process of inferring mappings between colors and concepts assignment inference because it is analogous to an assignment problem in optimization. In assignment problems, every possible pairing of items in one category (e.g., colors) i and another category (e.g., concepts) j is given a numerical merit score m i j . Here, let's assume that larger scores indicate a more desirable pairing, but that is not always true (e.g., to optimize delivery route efficiency, merit might be delivery time and smaller scores would be better). Solving an assignment problem means finding the pairing of items that maximizes (or minimizes) the sum of the merit scores of all chosen pairs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Although assignment inference is analogous to assignment problems, they are not the same. Assignment problems have deterministic results, whereas assignment inference is stochastic-inferred mappings can vary among individuals and even within individuals over time. This stochasticity can be explained in terms of noise in people's color-concept associations affecting the outcome of assignments in assignment inference, depending on whether assignments are robust or fragile <ref type="bibr" target="#b30">[31]</ref>. In robust assignments, adding noise to the system (e.g., perturbing the color-concept association strengths) has no effect on the outcome, but in fragile assignments adding noise can change the outcome of the assignment.</p><p>The robustness of an assignment in assignment inference can be understood as semantic discriminability-the ability for people to infer a unique mapping between colors and concepts <ref type="bibr" target="#b30">[31]</ref>. Evidence suggests that semantic discriminability predicts people's ability to interpret colors in encoding systems, independent of that predicted by perceptual discriminability and color-concept associations in isolation <ref type="bibr" target="#b30">[31]</ref>. We describe ways of operationalizing semantic discriminability in Section 3.2 as they pertain to the present study.</p><p>So far, we focused on encoding systems with two concepts and colors, and implied that merit m i j in assignment inference is colorconcept association strength (Figure <ref type="figure" target="#fig_0">2</ref>). However, there are other possible ways to define merit, especially when there are more than two colors and concepts, as in the present study. Schloss et al. <ref type="bibr" target="#b31">[32]</ref> sought to understand which merit people use in assignment inference to study (1) how humans infer meaning from colors, and (2) how to design palettes that match people's expectations, making palettes more interpretable. To approach this goal, they created two definitions of merit. The isolated merit function simply uses association strengths between items i and j, m i j := a i j . The balanced merit function is defined as</p><formula xml:id="formula_0">m i j := a i j max k6 = j a ik .<label>(1)</label></formula><p>The balanced merit score for a given color-concept pair is the association strength for that pair, minus the association strength between that color and the next most strongly associated concept. In order for m i j to be large, color i should be strongly associated with concept j and weakly associated with all other concepts. (Note: in the case of two concepts and colors these two definitions reduce to the same outcome.)</p><p>Next, they generated color palettes using an assignment problem under each definition, with human color-concept association ratings as the input. Finally, they presented different participants with those palettes in the form of six unlabeled colored bins. Participants inferred which bin was for each of six objects: paper, plastic, trash, metal, compost, and glass. Responses were scored as "correct" interpretations if they matched the encoded mapping. Encoded mappings can be produced in different ways, including by designers, software defaults, or optimization algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Here, they were determined by the optimal assignments in assignment problems used to generate the palettes. The logic was that participants would be better at interpreting palettes generated using a merit function that more closely matched merit in assignment inference. Performance was better for the palette generated using the balanced merit function, which suggests that this was the function that better captured merit in assignment inference. Thus, we use balanced merit in the present study.</p><p>Balanced merit can lead to unexpected assignments. For example, the bin for plastic was assigned a red color, even though red was weakly associated with plastic, because that color was more associated with plastic than with any of the other concepts. Thus, the assignment of plastic-red was interpretable. Given that weakly associated colors can prove useful when designing encoding systems, approaches that focus only on the top associates may be limited <ref type="bibr" target="#b10">[11]</ref>. It is important to quantify associations between concepts and a large range of colors, not just the top few associates, when optimizing palette design <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SEMANTIC DISCRIMINABILITY THEORY</head><p>Semantic discriminability theory characterizes the ability to generate semantically discriminable perceptual features for encoding a set of concepts. We begin with some key definitions.</p><p>Concept set: This is the set of all concepts that are represented in an encoding system. These concepts could refer to any information that is categorical (e.g., food, weather, activities, places, and animals). We label concepts in the concept set using the index j 2 {1, 2,...,n}.</p><p>Feature source: This is the set of all possible instances of a feature type. Perceptual color spaces (e.g., CIELAB) are well-defined feature sources for color, as they represent all colors humans can perceive <ref type="bibr" target="#b40">[41]</ref>.</p><p>Feature library: This is a subset of candidate features from the feature source used in an encoding system. For example, the Tableau 20 colors or UW-58 colors <ref type="bibr" target="#b26">[27]</ref> are feature libraries if design is constrained to those groups of colors. We focus on a feature library defined over color, but they can be defined over any type of perceptual feature (e.g., shapes, sizes, textures). We label features in the feature library using the index i 2 {1, 2,...,N}.</p><p>Feature set: This is a subset of features from the feature library, selected to encode a concept set. Feature sets can be constructed from any type of perceptual features (e.g., colors, shapes, sizes) <ref type="bibr" target="#b3">[4]</ref>. For colors, they are called "palettes." If there are n concepts, then the feature set should contain n features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature-concept association distributions</head><p>Feature-concept association distributions represent the degree to which a given concept is associated with each feature in a feature library (see Figure S.5A. in the Supplementary Material). For color, these are color-concept association distributions. Feature-concept association distributions can be described as raw association values over the feature library (e.g., mean ratings, pixel counts, word counts). In this case, we write a i j to denote the association between feature i 2 {1,...,N} and concept j 2 {1,...,n}. For each concept j, we also define normalized associations p j (•) as</p><formula xml:id="formula_1">p j (i) := a i j Â N k=1 a k j for: i 2 {1,...,N}.<label>(2)</label></formula><p>The list</p><formula xml:id="formula_2">⇥ p j (1) p j (2) ••• p j (N)</formula><p>⇤ can be interpreted as a discrete probability distribution over features in the feature library.</p><p>We now define useful properties and operations related to featureconcept association distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Specificity</head><p>Specificity is the degree to which a concept has strong, specific associations with features over the feature library. For color, specificity refers to the 'peakiness' of a color-concept association distribution. Concepts can have strong color associations that are concentrated in one part of color space (e.g., reds for concepts like raspberry) or divided over different parts of color space (e.g., reds and greens for watermelon) <ref type="bibr" target="#b26">[27]</ref>. Thus, we quantify specificity using entropy of the distribution, which captures how 'flat' vs. 'peaky' a distribution is, regardless of how many peaks there are.</p><p>Entropy for a feature-concept association distribution is defined as:</p><formula xml:id="formula_3">H j := N Â i=1 p j (i) log p j (i).<label>(3)</label></formula><p>If all features in the feature library are equally associated with concept j, the distribution p j will be uniform, entropy will be high, and specificity will be low. If a concept j is strongly associated with some features and not others, then entropy will be lower and specificity will be higher. This property of color-concept association distributions aligns with previous measures of colorability <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref> (see Figure S.2 in the Supplementary Material).</p><p>Mean entropy of a concept set is the mean of the entropy of all concepts in the set:</p><formula xml:id="formula_4">H µ := 1 n (H 1 + ••• + H n ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Distribution difference</head><p>We quantify distribution difference between concepts by comparing their normalized feature-concept associations. Total variation (TV) is what we use when comparing two concepts, say j 1 and j 2 . TV is defined as follows.</p><formula xml:id="formula_5">TV( j 1 , j 2 ) := 1 2 N Â i=1 p j 1 (i) p j 2 (i) .<label>(4)</label></formula><p>TV ranges between 0 and 1, where TV = 0 means the two distributions are identical, and TV = 1 means they are disjoint (for each feature i, either p j 1 (i) or p j 2 (i) must be zero). Generalized total variation (GTV) is a generalization of TV that we defined for cases when more than two concepts must be compared, say j 1 ,..., j k . We define GTV as follows.</p><p>GTV( j 1 ,..., j k ) := 1</p><formula xml:id="formula_6">+ N Â i=1 max(p j 1 (i), p j 2 (i),..., p j k (i)). (5)</formula><p>In the case where k = 2, GTV reduces to TV. In other words, GTV( j 1 , j 2 ) = TV( j 1 , j 2 ). For details on the motivation behind our definition of GTV, see the Supplementary Material, Section S.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Structure-agnostic property</head><p>The notions of entropy, TV, and GTV are agnostic to intrinsic structure of the feature source. For example, perceptual color spaces are structured according to perceptual similarity, but entropy of a color-concept distribution depends on the fraction of the colors that are highly associated with the concept, regardless of perceptual similarity. We chose structure-agnostic metrics for specificity and distribution difference so that semantic discriminability theory could readily generalize to feature sources with less well-defined metric spaces (e.g., shape, texture, odor).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic discriminability</head><p>As described in Section 2.3, semantic discriminability of perceptual features is the ability to infer a unique mapping between features and concepts. It is reflected in the degree to which inferred mappings vary among individuals or within individuals between trials. We model this variability by treating feature-concept associations as random variables. Rather than solving an assignment problem using the mean a i j values, we look at the probability of the likeliest assignment, where probability is computed with respect to uncertainty in the a i j . We now make this notion more precise.</p><p>Semantic distance is a way to operationalize semantic discriminability in the case where there are n = 2 features and concepts <ref type="bibr" target="#b30">[31]</ref>. Figure <ref type="figure">3</ref> illustrates an example in which we have concepts {M,W} and colors {1,2}. The color-concept associations between all possible pairs are x 1 ,...,x 4 , as shown in Figure <ref type="figure">3</ref>. We assume each x k is normally distributed with mean xk equal to the corresponding a i j and standard deviation s k = 1.4 • xk (1 xk ), which was found to be a good fit to experimental data <ref type="bibr" target="#b30">[31]</ref>. The outcome of the assignment problem is determined by the quantity Dx := x 1 x 2 + x 3 x 4 . The optimal assignment is: (M-1 and W-2 if Dx &gt; 0) and (M-2 and W-1 if Dx &lt; 0). Semantic distance is defined by the equation</p><formula xml:id="formula_7">DS = |Prob(Dx &gt; 0) Prob(Dx &lt; 0)|. (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>Since the x k are assumed to be normally distributed, so is Dx, and the probabilities in ( <ref type="formula" target="#formula_7">6</ref>) can be computed analytically:</p><formula xml:id="formula_9">Prob(Dx &gt; 0) = F 0 @ (x 1 + x 4 ) (x 2 + x 3 ) q s 2 1 + s 2 2 + s 2 3 + s 2 4 1 A ,<label>(7)</label></formula><p>and Prob(Dx &lt; 0) = 1 Prob(Dx &gt; 0), where F(•) is the cumulative distribution function (cdf) of the standard normal distribution. When DS is close to 0, Dx has a similar probability of being positive or negative, so the assignment is fragile. When DS is close to 1, Dx is almost always positive or almost always negative, so the assignment is robust. This notion of semantic distance can be used even when the features are not colors, by replacing the color-concept associations with feature-concept associations, and adjusting the formula for s k as appropriate.</p><p>-2 -1 0 1 2 <ref type="bibr" target="#b30">[31]</ref> that shows how association ratings between concepts {M,W} and colors {1,2} produce a distribution for Dx. Semantic distance is the absolute difference of the area under the curve to the left and right of zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of</head><formula xml:id="formula_10">Δx = (x 1 + x 4 ) -(x 2 + x 3 ) Association Rating Prob. Density Probability Density Concept W Concept M Concept M Concept W x 1 x 3 x 2 x 4 + + - ( ) ( ) Color 1 Color 2 Color 2 Color 1 M W 1 x 1 x 2 x 4 x 3 2 M W 1 x 1 x 2 x 4 x 3 2 Prob(Δ x &lt; 0) Prob(Δx &gt; 0) Figure 3: Diagram from</formula><p>Generalized semantic distance is an extension of semantic distance to the case where there are n &gt; 2 features and concepts. In this case, there will be n! (n factorial) possible assignments. We define generalized semantic distance in a manner analogous to semantic distance; we label the feature-concept associations between all possible pairs as x 1 , x 2 ,...,x n 2 and assume they are normally distributed random variables. 2 In this more complicated scenario, the assignment is not determined by a simple quantity such as Dx and no formula analogous to <ref type="bibr" target="#b6">(7)</ref> exists to determine the assignment. Instead, we use the following Monte Carlo approach.</p><p>1. Sample x 1 ,...,x n 2 from the distribution of merit scores 2 and solve an assignment problem using the sampled merit scores. 2. Repeat step 1 a large number of times and count the number of times each distinct assignment occurs. Let p be the proportion of times that the most frequent assignment occurred. Since there are n! possible assignments, we must have Freq. In each column, the distribution for concept A is the same and concept B varies. The histograms to the right show how distribution difference affects capacity with arrows pointing to maximum semantic distance (DS) for the concept set. Corresponding bipartite graphs show the color set with maximum semantic distance (this is arbitrary when the distributions are parallel because semantic distances for all color pairs are equally poor).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Higher Capacity</head><note type="other">Lower</note><p>3. The generalized semantic distance DS is defined as a linear rescaling of p to ensure that 0  DS  1. The formula is:</p><formula xml:id="formula_11">DS = n!p 1 n! 1 .<label>(8)</label></formula><p>A similar Monte Carlo approach was used in <ref type="bibr" target="#b31">[32]</ref> to predict the results of assignment inference in a recycling task (6 concepts and 6 colors). Just like semantic distance, generalized semantic distance is a number between 0 and 1, where a larger number indicates more robust assignments, and consequently, higher semantic discriminability. We use the same symbol DS for both notions of distance because in the case where n = 2, generalized semantic distance is (on average) equal to semantic distance, and the approximation becomes exact as the number of samples in step 2 tends to infinity. Conversely, in the limit n ! •, we have DS ! p and the rescaling in (8) has no effect.</p><p>Semantic contrast is similar to generalized semantic distance, except it estimates the proportion of times a given color is assigned to the "optimal" concept (compared to all other assignments). This estimation is computed using the Monte Carlo method described earlier, with optimal defined by the solution to an assignment problem using the balanced merit function computed on feature-concept associations.</p><p>For a given concept, the optimal color for that concept may have higher semantic contrast in one context and lower semantic contrast in another context, depending on the other colors and concepts in the encoding system. A concept set that has higher capacity for semantic discriminability (Section 3.3) should enable higher semantic contrasts among colors in its optimal palette.</p><p>The steps to computing semantic contrast are: (1) Solve an assignment problem (see Section 2.3) using the mean association ratings x1 ,..., xn 2 . We call this the optimal assignment. (2) Sample x 1 ,...,x n 2 from the distribution of merit scores and solve an assignment problem using the sampled merit scores. (3) Repeat step 2 a large number of times and count the proportion of times each feature was assigned to the same concept as in the optimal assignment This proportion is each feature's semantic contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Capacity for semantic discriminability</head><p>Capacity for semantic discriminability is the extent to which it is possible to produce semantically discriminable features for a given set of concepts. We operationalized capacity for semantic discriminability (capacity for short), using max capacity. This is a scalable measure that returns the semantic distance of the most semantically discriminable feature set for a concept set, given a feature library.</p><p>To compute max capacity for a given concept set, we solve an assignment problem using the balanced merit function (Section 2.3) over the entire feature library. This yields a feature set. We define max capacity as the (generalized) semantic distance of this feature set for the given concept set. High max capacity indicates that the feature library contains at least one feature set with high semantic discriminability for the concept set. Low max capacity indicates no such feature set exists for that concept set, at least given the feature library.</p><p>In the case of two concepts, the balanced merit approach for computing max capacity gives the same result as exhaustively computing the semantic distance for each pair of colors, then finding the maximum of those semantic distances. Using balanced merit, though, allows max capacity to scale easily; it can be efficiently computed for large concept sets and feature sets. We also explored alternative ways to operationalize capacity (see Supplementary Material Section S.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The theory</head><p>Semantic discriminability theory posits that the capacity to produce semantically discriminable perceptual features for a set of concepts depends on the difference in feature-concept association distributions over a feature library. Briefly, distribution difference predicts capacity, distinct from the contribution of specificity. This idea differs from previous approaches, which primarily focused on color-concept associations for concepts in isolation when evaluating the potential to meaningfully encode particular concepts using color <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Figure <ref type="figure">1</ref> shows the distinction between distribution difference and specificity of color-concept associations, with respect to capacity. It includes concept sets with large, medium, and small distribution differences. Capacity is illustrated with histograms below each concept set. They show the frequency of color sets across values of semantic distance (2485 possible 2-color sets from the UW-71 color library), with an arrow pointing at maximum semantic distance. Concept sets with large, medium, and small distribution differences result in high, medium, and low capacity, respectively. Yet, the concepts with medium capacity (driving and comfort) have far lower specificity than concepts with low capacity (eggplant and grape). The reason that concepts with low specificity can result in higher capacity than concepts with high specificity is that semantic discriminability depends on the difference in merit of each possible set of feature-concept assignments, not just isolated feature-concept associations (Section 2.3).</p><p>Figure <ref type="figure" target="#fig_1">4</ref> further illustrates this point with hypothetical color-concept association distributions for 2-concept sets that have higher capacity (top row) and lower capacity (bottom row). The colored dots on the distributions indicate the optimal assignment according to balanced merit (though this is arbitrary when the distributions are parallel because all assignments are equally poor). Next to each distribution pair is a histogram of semantic distances (as in Figure <ref type="figure">1</ref>) and a bipartite graph for the colors with maximum semantic distance (thicker edges connecting colors and concepts indicate greater merit). Semantic distance (DS) is indicated below the bipartite graphs, and can be visually inspected by comparing the total merit of the outer edges vs. inner edges and assessing the degree to which one sum is larger. When distribution difference is high (top row), capacity is high, even if one concept has a uniform distribution (i.e., no specificity). However, when distribution difference is lower (bottom row), capacity is lower, even if both concepts have high specificity.</p><p>We chose the particular examples in Figure <ref type="figure">1</ref> and Figure <ref type="figure" target="#fig_1">4</ref> to highlight the dissociation between distribution difference and specificity, but we systematically tested for effects of these factors on capacity in Experiment 1.</p><p>Experiment 1 tested the hypothesis that capacity for semantic discriminability is predicted by distribution difference, independent of specificity. We first collected color-concept association data from human participants, and used those data to calculate capacity, distribution difference, and specificity. We then tested our hypothesis on 2-concept sets (Section 4.2.1) and 4-concepts sets (Section 4.2.2). Semantic discriminability predicts people's ability to interpret palettes in visualizations <ref type="bibr" target="#b30">[31]</ref>, so our modeling approach for understanding capacity for semantic discriminability should have implications for interpretability. The code and data for all experiments is at: https://github.com/SchlossVRL/sem_disc_theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Participants</head><p>185 undergraduates participated for credit in Introductory Psychology (mean age =18.66, 99 females, 86 males, gender provided through free-response). All gave informed consent and the UW-Madison IRB approved the protocol. Color vision was assessed by asking participants if they had difficulty distinguishing between colors relative to the average person and if they considered themselves colorblind. Participants were excluded if they answered yes to either (5 excluded).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Design, Displays, and Procedure</head><p>Participants judged the association between each of 71 colors and each of 20 concepts. The colors were the UW-71 color library, an extension of the UW-58 colors <ref type="bibr" target="#b30">[31]</ref>, see Supplementary Material for details and Table <ref type="table">S</ref>.1 for CIELAB coordinates. 3 The concepts were from Lin et al. <ref type="bibr" target="#b19">[20]</ref>, including 5 concepts in each of four concept categories (fruits, vegetables, activities, and properties) (Table <ref type="table" target="#tab_1">1</ref>). Participants were randomly assigned to one of four categories (fruits n = 46, vegetables n = 45, activities n = 45, properties n = 44). They judged all colors for all five concepts within their assigned category (71 colors ⇥ 5 concepts = 355 trials). Trials were presented in a blocked randomized designall colors were presented in a random order for a given concept before starting the next concept, and concept order was also randomized.</p><p>The displays included the concept word centered at the top of the screen (font-size: 24 pt, font-family: Lato) and colored square centered below (80 px ⇥ 80 px). Below the colored square, was a line-mark slider scale (400 px long), with the left end labeled "not at all" and the right end labeled "very much" and the center marked with a vertical line (3 px wide and 32 px tall). The background was gray (CIE Illuminant D65, x = .3127, y = .3290, Y = 10 cd/m 2 ), so that very dark colors (e.g., black) and very light colors (e.g., white) could be seen against the background. Data were recorded in pixel units, and scaled to range from 0-1. Displays were generated using the jsPsych JavaScript library <ref type="bibr" target="#b8">[9]</ref>, presented on participants' personal devices.</p><p>Participants were told they would see a set of concepts and series of colors, one concept and color at a time. Their task was to rate how much they associated the color with the concept by moving the slider on the scale from "not at all" to "very much", and clicking "next" to continue. Before beginning, they were shown a list of all concepts and the UW-71 colors. They were asked to anchor the endpoints of the rating scale for each concept <ref type="bibr" target="#b25">[26]</ref> by thinking about which color they associated the most/least with that concept, and considering these colors as representing the ends of the slider scale for that concept. During the experiment, ratings were blocked by concept, and after each block participants were told how many blocks remained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">2-Concept sets</head><p>We began by calculating the mean color-concept association ratings over participants. Next, for all k = 2 concepts out of the n = 20 concepts  in Table <ref type="table" target="#tab_1">1</ref> (190 2-concept sets in total), we used the mean color-concept associations to calculate capacity for semantic discriminability, distribution difference, and mean specificity. To calculate capacity, we followed the method in Section 3.3. To calculate distribution difference, we used total variation (TV) in Equation ( <ref type="formula" target="#formula_5">4</ref>) and normalized the TV values to range from 0 to 1. To calculate mean specificity, we first computed entropy (H) for each concept (Equation ( <ref type="formula" target="#formula_3">3</ref>)) over N = 71 colors, and then computed the mean entropy over concepts within each set. Given that higher specificity corresponds to lower entropy, we normalized mean entropy to range from 0 to 1 and subtracted the scores from 1, such that larger numbers indicated higher specificity. Figure S.2 in the Supplementary Material shows the raw entropy for each concept. Concepts with lower entropy/higher specificity corresponded to colorable concepts in <ref type="bibr" target="#b19">[20]</ref>, and concepts with higher entropy/lower specificity corresponded to non-colorable concepts in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Figure <ref type="figure" target="#fig_2">5A</ref> shows the relation between capacity for semantic discriminability and distribution difference (left), and mean specificity (right). For both distribution difference and mean specificity, we plotted the log of the normalized scores to preserve linearity. The correlation between capacity and distribution difference over all 190 2-concept sets was strongly positive (r(188) = .93, p &lt; .001), with a strong trend for capacity to increase with increased distribution difference. The correlation between capacity and mean specificity was also significantly positive (r(188) = .82, p &lt; .001), but was significantly weaker than the correlation with distribution difference (Fisher's r-to-z transformation z(188) = 4.85, p &lt; .001). This weaker correlation can be attributed, in  <ref type="figure" target="#fig_2">5</ref>, right).</p><p>To examine whether distribution difference and mean specificity contributed independently to capacity, we used a multiple linear regression model to predict capacity from these two factors (z-scored to center them and put them on the same scale). As shown in Table <ref type="table" target="#tab_2">2</ref>, distribution difference was a strong significant predictor, and mean specificity was not significant. Thus, the variance explained in capacity by distribution difference was independent from mean specificity, and mean specificity did not contribute after accounting for distribution difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">4-Concept sets</head><p>For all k = 4 concepts out of the n = 20 concepts in Table <ref type="table" target="#tab_1">1</ref> (4845 4-concept sets in total), we used the mean color-concept associations to calculate capacity, distribution difference, and mean specificity, as described in Section 4.2.1 for 2-concept sets. However, instead of semantic distance to compute capacity we used generalized semantic distance (Section 3.2), and instead of using TV to compute distribution difference, we used GTV (Equation 5, Section 3.1.2).</p><p>Figure <ref type="figure" target="#fig_2">5B</ref> shows the relation between capacity for semantic discriminability and distribution difference (left), and mean specificity (right) for 4-concept sets. As for 2-concept sets, we used the log of the normalized distribution difference and mean specificity scores to preserve linearity. Capacity was positively correlated with both distribution difference (r(4843) = .74, p &lt; .001) and mean specificity (r(4843) = .61, p &lt; .001), but the correlation with distribution difference was greater (Fisher's r-to-z transformation (z(4843) = 11.88, p &lt; .001).</p><p>Using the same regression analysis as for 2-concept sets, distribution difference was a strong significant predictor (Table <ref type="table" target="#tab_2">2</ref>). Mean specificity a weak significant predictor, but surprisingly it was negative, such that less specificity resulted in greater capacity in the context of this model.</p><p>In summary, Experiment 1 supports the hypothesis that the capacity to produce semantically discriminable color palettes for a set of concepts depends on the difference in color-concept association distributions, independent of specificity. Considering specificity of colorconcept associations in isolation is insufficient. These results emphasize the importance of considering relative color-concept associations for a given set of concepts, rather than the concepts in isolation, when evaluating the potential for semantically discriminable color palettes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT 2</head><p>Semantic discriminability theory implies that if concept sets have high capacity for semantic discriminability, it should be possible to create encoding systems assigning those concepts to colors that people can interpret. People should be able to interpret the correct mappings between colors and concepts, even for concepts previously considered "non-colorable," insofar as the colors are semantically discriminable. We tested this hypothesis in Experiment 2. We defined accuracy as the proportion of responses that matched the optimal mapping specified by an assignment problem using the balanced merit function (see Section S.7 in the Supplementary Material for a further discussion on accuracy, and its relation to measures of semantic discriminability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Participants</head><p>98 participants (74 males, 24 females) were recruited on Amazon Mechanical Turk. All gave informed consent, and the UW-Madison IRB approved the protocol. Eight were excluded for not reaching 100% accuracy on catch trials (Section 5.1.2), three of which reported atypical color vision. All other participants reported typical color vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Design, Displays, and Procedure</head><p>For each trial, participants were presented with a bar graph centered on the screen, consisting of four colored bars (Figure <ref type="figure" target="#fig_3">6A</ref>). Each bar was 130 px wide and varied in height randomly (from 260-300 px high). The bars were spaced 45 px apart. At the start of the trial, a set of four concepts (22 pt font) was centered above the graph in a random order. The y-axis was unlabeled. Below the x-axis, there were empty boxes 120-px wide and 50 px high. During the trial, participants labeled each bar by clicking on a label and dragging/dropping it in the empty box below the bar. The displays were generated using the Charts.js and jsPsych JavaScript libraries.</p><p>Each participant completed 64 trials, which included 8 color-concept sets ⇥ 8 color positionings within each set. Figure <ref type="figure" target="#fig_3">6B</ref> shows the palettes for each set. The stimuli were constructed using displays like in Figure <ref type="figure" target="#fig_3">6A</ref>, but swapping out the concept sets and corresponding color palettes, and balancing the bar color positioning as follows.  Concept sets. To generate the concept sets, we randomly selected four concepts from each of the concept categories from Experiment 1 (fruits (F), vegetables (V), activities (A), properties (P)) and labeled them 1-4 (Table <ref type="table" target="#tab_1">1</ref>, Figure <ref type="figure" target="#fig_3">6B</ref>). We then tied pairs of concepts within each category (e.g., V1-V2, V3-V4). We combined pairs of concepts such that all participants saw (1) vegetables with fruits, (2) vegetables with properties, (3) activities with fruits, and (4) activities with properties. Using this design, we created two groups of stimuli, divided over two groups of participants to reduce the number of trials for any one participant. Group 1 saw sets of four concepts, with concepts 1 and 2 in one category paired with concepts 1 and 2 in the other category (e.g., V1-V2 with F1-F2), and sets of four concepts with concept 3 and 4 in one category paired with concepts 3 and 4 in the other category (e.g., V3-V4 with F3-F4). Group 2 saw the opposite pairings (e.g., V1-V2 with F3-F4, V3-V4 with F1-F2). Within this design, all participants saw each concept an equal number of times. Participants were randomly assigned to Group 1 (n = 47) or Group 2 (n = 43).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A</head><p>Color palettes. For each concept set, we generated its color palette using the balanced merit function (Equation ( <ref type="formula" target="#formula_0">1</ref>)) in an assignment problem. The resulting assignments determined the encoded mapping we defined as "correct." We used the balance merit function because previous evidence suggested it aligns with the merit people use in assignment inference (see Section 2.3). Merit was computed over the color-concept association data reported in Experiment 1 for all 71 colors in the UW-71 library 4 The color palettes are shown in Figure <ref type="figure" target="#fig_3">6B</ref>. The CIELAB coordinates for the palette colors can be found at https: //github.com/SchlossVRL/sem_disc_theory. The graphs were presented on a gray background approximating CIE Illuminant D65 (x = .3127, y = .3290, Y = 10, cd/m 2 ).</p><p>Bar color positioning. Each of the eight color-concept sets for a given group (Figure <ref type="figure" target="#fig_3">6B</ref>) was presented eight times in eight bar color positionings along the x-axis. This was done using a blocked randomized design, so all eight color-concept sets appeared once in a random order, randomly assigned to a color positioning within a block, before starting the next block. The eight possible color positionings were defined using a Latin square design (four positionings, left/right reversed). Thus, within a color set, each color appeared in each of four positions twice, with the colors to its left/right in opposite positions.</p><p>Catch trials. We included eight catch trials, one per block, in which bars were colored a shade of red, yellow, green, and blue, and the labels were "red", "yellow", "green", and "blue." We set an a priori exclusion criterion that participants must be 100% accurate on these catch trials, otherwise their data would be excluded from analysis.</p><p>Participants were told they would see a series of colored bar graphs, with four bars and four words at the top of the screen. Their task was to match each word to its corresponding bar color by clicking and dragging the label to the empty box below the bar. They were told to use their best guess if they were unsure how to match the labels to the bar colors. They then completed a practice trial with four concepts that were not in the main experiment (blueberry, mango, strawberry, lemon) and colors chosen by the balanced merit function. Associations for these concepts had been collected for a different project. During the trials, all bars had to be labeled before a "continue" button could be pressed to go to the next trial. Once placed in a box, a label could be dragged to another box and all labels could be reset to the starting position by pressing a "reset label" button. Trials were separated by a 100 ms. inter-trial interval. Participants received breaks after each block, and were told the proportion of completed trials at each break.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>For each participant, we calculated the proportion of times they chose each concept for each color in each color-concept set, averaged over bar color positioning. These results are shown for a subset of the concept sets in Figure <ref type="figure" target="#fig_5">7A</ref> (top row), and for all concepts sets in Figure S.6. For each color-concept pairing, we calculated accuracy as the proportion of trials in which participants selected the optimal pairing (defined with respect to balanced merit) (Section S.7). The arrows below the x-axis in Figures <ref type="figure" target="#fig_5">7A and S</ref>  We first tested whether concept sets with higher capacity enabled creating encoding systems that were easier to interpret. To do so, we correlated max capacity for each of the 16 concept sets with mean accuracy over all colors within each set. There was a significant relation (r = .58, p &lt; .02), indicating greater capacity for semantic discriminability corresponded to greater interpretability.</p><p>Next, we tested whether participants' patterns of color choices for each concept were correlated with model predictions computed by solving an assignment problem with perturbed association ratings (the Monte Carlo process described in Section 3.2 over 1000 iterations). These predictions are shown in the bottom row of Figure <ref type="figure" target="#fig_5">7A</ref> and in Figure S.6. In the model predictions, the height of the bars correspond to the proportion of times each color was assigned to each concept. The predictions strongly correlated with participant responses over the full dataset of 4 colors ⇥ 16 4-concept sets (r(126) = .95, p &lt; .001), with high correlations for each group (Group 1: r(126) = .96, Group 2: r(126) = .94, ps &lt; .001).</p><p>Finally, we tested our hypothesis that participants would be able to interpret the correct mappings between individual colors and concepts, insofar as the colors were semantically discriminable. Figure <ref type="figure" target="#fig_5">7A</ref> shows that participants chose the correct colors well above chance, even for concept sets in which all concepts have been called non-colorable (e.g., {sleeping, driving, safety, speed}. To examine whether accuracy for given a concept varied depending on semantic discriminability of its correct color, in 7B, we plotted accuracy for each concept as a function of the semantic contrast of its correct color (see Section 3.2 and Section S.7 for details on semantic contrast). Plots are separated by concept category, with four points per concept, corresponding to the four colorconcept sets in which it appeared. Generally, the slopes of the best fit lines for each concept were positive, indicating that accuracy increased with semantic contrast. Responses for some concepts (e.g., fruits) were highly accurate for all color-concept sets because their optimal colors have high semantic contrast in all concept sets we tested.</p><p>We analyzed this pattern of accuracies using a mixed-effect logistic regression model predicting accuracy for each concept in each set using three factors: semantic contrast of the correct color for that concept (relative to the other colors in the palette), specificity of the concept as defined in Experiment 1, and association strength between the concept and its correct color (previously shown to influence accuracy in similar tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>). These predictors were calculated using data from Experiment 1 (different participants from Experiment 2). We also included by-subject random intercepts and by-subject random slopes for each factor. We z-scored the individual predictors to put them on the same scale and set the correlations between the random slopes to be 0 to help the model converge. As shown in Table <ref type="table" target="#tab_4">3</ref>, accuracy significantly increased with greater semantic contrast and with greater specificity. Association strength was not significant.</p><p>Overall, accuracy was greater for concepts previously considered colorable (fruits and vegetables) (M = 0.76, SD = 0.23) than those considered non-colorable (activities and properties) (M = 0.56, SD = 0.24) (Figure <ref type="figure" target="#fig_5">7B</ref>). But, all activities and properties had at least one instance that was as accurate as fruits and vegetables, and all instances were above chance. Moreover, accuracy for a given concept varied based on semantic contrast with its correct color, which cannot be explained by specificity of the concept in isolation. These results suggest that any concept has potential to be meaningfully encoded using color if the color has sufficient semantic contrast with other colors in the palette.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">GENERAL DISCUSSION AND CONCLUSION</head><p>In this paper we presented semantic discriminability theory to specify constraints on producing semantically discriminable perceptual features for visual communication. The theory states that capacity for creating semantically discriminable features for a concept set is constrained by the difference in feature-concept association distributions for those concepts. Supporting the theory, Experiment 1 showed that distribution difference between color-concept association distributions predicted capacity for semantic discriminability in 2-and 4-concept sets, independent of specificity. And, Experiment 2 indicated people can correctly interpret mappings for concepts previously considered non-colorable, but their ability to do so depended on semantic contrast with respect to the other colors in the encoding system.</p><p>Semantic discriminability theory is rooted in feature-concept associations, which can vary cross-culturally <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref>. The theory implies that distribution difference will predict capacity for semantic discriminability in different cultures, as long as the association distribution data reflect the associations held by a given culture.</p><p>The theory further implies that any factor that influences distribution difference for a set of concepts can affect capacity. Below, we propose criteria for producing distribution differences that support adequate capacity for semantic discriminability. Evaluating these criteria will help guide future work on the potential and limitations of semantic discriminability for colors and for other perceptual features.</p><p>Criterion 1: Need for some specificity. At least some concepts in the concept set must have association distributions with some specificity. If all concepts in a set have uniform distributions, there will be no capacity for semantic discriminability (Figure <ref type="figure" target="#fig_1">4</ref>). Some perceptual features may not support specificity as well as color does, such as line orientation. If so, such features might be less useful for communicating meaning in information visualizations.</p><p>Criterion 2: Need for feature library variability. To be sensitive to differences in feature-concept associations, if they exist, the feature library must be sufficiently variable. In color, variability is achieved by sampling widely over color space, as opposed to sampling say, only the bluish part of the space. One can systematically sample over color spaces because color spaces are well-defined feature sources. But, such sampling may pose a challenge for less well-specified feature sources (e.g., all possible shapes or all possible textures). Criterion 3: Need for large enough feature library. The feature library must be large enough to detect small, but important differences between feature-concept association distributions. E.g., a library with only two colors, a blue and red, might be large enough to produce distinct association distributions for the concepts sky and rose, but a library with more colors (e.g., more shades of blue) would be needed to produce distinct distributions for concepts like noon sky and night sky.</p><p>Conclusion. We presented and evaluated semantic discriminability theory to define constraints on creating semantically discriminable features for perceptual encoding systems. The theory implies that any concept has potential to be meaningfully encoded using color, if the criteria above are met. Thus a concept that has low specificity (i.e., uniform distribution), can meaningfully be encoded by a color, if other concepts in the set have sufficiently different distributions. This is possible because people infer globally optimal mappings between colors and concepts, even if that means inferring concepts map to weakly-associated colors. The theory implies, and our results suggest, color is more robust for visual communication than previously thought.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distinction between color-concept associations and inferred mappings (figure based on<ref type="bibr" target="#b31">[32]</ref>). Left: Bipartite graphs show colorconcept association strengths for concepts trash (T) and paper (P) with colors dark yellow (Y), white (W), and purple (Pu) (thicker edges connecting concepts and colors indicate stronger associations). Right: example trials where participants infer which color maps to trash.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Hypothetical color-concept association distributions for concepts A and B, showing how capacity varies with distribution difference (top row: higher capacity; bottom row: lower capacity). In each column, the distribution for concept A is the same and concept B varies. The histograms to the right show how distribution difference affects capacity with arrows pointing to maximum semantic distance (DS) for the concept set. Corresponding bipartite graphs show the color set with maximum semantic distance (this is arbitrary when the distributions are parallel because semantic distances for all color pairs are equally poor).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Relations between capacity for semantic discriminability and distribution difference (log(normalized (generalized) total variation distance); left) and specificity (log(1 normalized mean entropy); right) for 2-concept sets (top) and 4-concept sets (bottom). For 4-concept sets we downsampled from 4845 points to 500 points to avoid overplotting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (A) Example trial in Experiment 2. Participants labeled each bar by clicking the label and dragging/dropping it in the box below the bar. (B) Palettes and corresponding concepts used to construct the stimuli (see text for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.6 point up to the correct color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (A) Proportion of times participants chose each color (top) and predicted proportions from generalized semantic distance (bottom) for a subset of palettes from group 1 (see Figure S.6 in the Supplementary Material for the full dataset). The correct response for each concept is marked by an arrow along the x-axis. The colors of the bars correspond to the colors of the stimuli. (B) Mean proportion correct for each concept in each palette as a function of semantic contrast of its correct color (best fit lines drawn for each concept). All the points for a given concept and corresponding best fit line are shown in the same color to help group the points in this figure (these colors were not necessarily the colors shown in the experiment. In (A) and (B) gray horizontal lines correspond to chance (.25) and error bars represent ± standard errors of the means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Full set of concepts in Experiment 1 (first four columns of concepts were used in Experiment 2).</figDesc><table><row><cell>Category</cell><cell>Concepts</cell></row><row><cell cols="2">Fruits Vegetables corn peach Activities working leisure sleeping driving eating cherry grape banana apple carrot eggplant celery mushroom Properties efficiency speed safety comfort reliability</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Multiple linear regression predicting capacity for semantic discriminability from distribution difference and mean specificity for all 2-concept sets and 4-concept sets.</figDesc><table><row><cell>Model</cell><cell>Factor</cell><cell>b</cell><cell>SE</cell><cell>t</cell><cell>p</cell></row><row><cell cols="2">2-concept Intercept</cell><cell cols="4">.867 .005 181.9 &lt; .001</cell></row><row><cell></cell><cell>Distribution diff.</cell><cell cols="2">.160 .010</cell><cell cols="2">15.6 &lt; .001</cell></row><row><cell></cell><cell>Specificity</cell><cell cols="2">.002 .010</cell><cell>.201</cell><cell>.841</cell></row><row><cell cols="2">4-concept Intercept</cell><cell cols="4">.772 .002 483.8 &lt; .001</cell></row><row><cell></cell><cell>Distribution diff.</cell><cell cols="2">.235 .004</cell><cell cols="2">53.6 &lt; .001</cell></row><row><cell></cell><cell>Specificity</cell><cell cols="2">.112 .004</cell><cell cols="2">25.5 &lt; .001</cell></row><row><cell cols="6">part, to there being concept sets with high capacity, despite moderate to</cell></row><row><cell cols="6">low mean specificity, and concept sets with low capacity despite high</cell></row><row><cell cols="2">mean specificity (Figure</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Logistic mixed-effect model predicting accuracy from specificity of the concept, semantic contrast of the concept's correct color, and association between the concept and its correct color.</figDesc><table><row><cell>Fixed Effects</cell><cell>b b b</cell><cell>SE</cell><cell>z z z</cell><cell>p p p</cell></row><row><cell>Intercept</cell><cell cols="2">1.272 .176</cell><cell cols="2">7.249 &lt; .001</cell></row><row><cell>Specificity</cell><cell cols="2">.226 .088</cell><cell>2.577</cell><cell>.001</cell></row><row><cell>Semantic Contrast</cell><cell cols="2">.645 .081</cell><cell cols="2">7.926 &lt; .001</cell></row><row><cell>Association Strength</cell><cell cols="2">.064 .057</cell><cell>1.12</cell><cell>.262</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Specificity is similar to color diagnosticity<ref type="bibr" target="#b36">[37]</ref>, but color diagnosticity concerns whether a concept has a single strongly associated color<ref type="bibr" target="#b36">[37]</ref>, and specificity concerns the degree to which a concept is associated with some colors more than others in a color-concept association distribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here, we use color-concept association ratings, so we assume the x k are distributed with the same s k used to define semantic distance<ref type="bibr" target="#b30">[31]</ref>. In principle, the distributions of the x k can be changed to suit other cases beyond color.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We converted CIELAB to RGB using MATLAB's lab2rgb function, which makes assumptions about monitor characteristics, so the colors were not exact renderings of CIELAB coordinates. Without calibration, the colors rendered by RGB coordintes may vary across monitors, but using a fixed correspondence between D65 CIELAB and RGB can approximate intended colors online<ref type="bibr" target="#b35">[36]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Due to a scaling issue during palette creation, 15 of the 64 color-concept pairings were not optimal. This did not affect the analyses, but accuracy may have been greater if participants had seen fully optimized palettes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Rob Nowak, Melissa Schoenlein, Kevin Lande, Tim Rogers, Chris Thorstenson, Anna Bartel, and Maureen Stone for helpful discussions. This project was supported by the UW-Madison Office of the Vice Chancellor for Research and Graduate Education, Wisconsin Alumni Research Foundation, and NSF (BCS-1945303 to KBS).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Affective congruence in visualization design: Influences on reading categorical maps</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Affective color in visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bartram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1364" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Basic Color Terms: Their Universality and Evolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Berlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>University of California Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semiology of graphics: Diagrams, Networks, Maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bertin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>University of Wisconsin Press</publisher>
			<pubPlace>Madison</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The unbearable distraction of color</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Brockmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Professional Communication</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="159" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Assignment Problems: revised reprint</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dell'amico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bare skin, blood and the evolution of primate colour vision</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Changizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimojo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biology Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="221" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Color vision, cones, and color-coding in the cortex</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Conway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Neuroscientist</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="274" to="290" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">jspsych: A javascript library for creating behavioral experiments in a web browser</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>De Leeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Color and psychological functioning: the effect of red on performance attainment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Moller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Categorical colormap optimization with visualization case studies</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Delahaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Storchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="871" to="880" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Paradoxical impact of memory on color appearance of faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasantash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lafer-Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Afraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Conway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated color selection using semantic knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holmgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 AAAI Fall Symposium Series</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The colour currency of nature</title>
		<author>
			<persName><forename type="first">N</forename><surname>Humphrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Colour for Architecture Today</title>
				<editor>
			<persName><forename type="first">T</forename><surname>Porter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Mikellides</surname></persName>
		</editor>
		<imprint>
			<publisher>Taylor &amp; Francis</publisher>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="95" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Colorsmessengers of concepts: Visual design mining for learning color semantics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keshvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The sun is no fun without rain: Physical environments affect how we feel about yellow across 55 countries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jonauskaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Abdel-Khalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abu-Akel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Al-Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Antonietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><forename type="middle">G</forename><surname>Ásgeirsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Atitsogbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bogushevskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Environmental Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101350</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A machine learning approach to quantify the specificity of colour-emotion associations and their cultural differences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jonauskaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Havelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papadatou-Pastou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oberfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Society Open Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">190741</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Linguistics of Color Terms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Smelser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Baltes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Comrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selecting semantically-resonant colors for data visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Conference on Visualization</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
	<note>Computer Graphics Forum</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What is the color of chocolate?-extracting color values of semantic expressions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Colour in Graphics, Imaging, and Vision</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
	<note>Society for Imaging Science and Technology</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A large-scale multilingual color thesaurus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Society for Imaging Science and Technology</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
	<note>Color and Imaging Conference</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Matrix Information Geometry</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A study of colour emotion and colour preference. Part I: Colour emotions for single colours</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woodcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Color Research &amp; Application</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="240" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual aesthetics and human preference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sammartino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="77" to="107" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating colorconcept associations from image statistics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Leggon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1226" to="1235" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Look of Maps</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Robinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952">1952</date>
			<publisher>University of Wisconsin Press</publisher>
			<pubPlace>Madison</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A color inference framework</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Colour Studies: Cognition, Language, and Beyond. John Benjamins</title>
				<editor>
			<persName><forename type="first">G</forename><forename type="middle">V P L</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Biggam</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mapping color to meaning in colormap data visualizations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Gramazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="810" to="819" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic discriminability for visual communication</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Leggon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1022" to="1031" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Color inference in visual communication: the meaning of colors in recycling</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Walmsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Foley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Research: Principles and Implications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A linguistic approach to categorical color assignment for data visualization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="698" to="707" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Review of graph comprehension research: Implications for instruction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoeffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="69" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The relation between color and spatial structure for interpreting colormap data visualizations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Sibrel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Schloss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7" to="7" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An engineering model for color difference as a function of size</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Society for Imaging Science and Technology</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
	<note>Color and Imaging Conference</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Color diagnosticity in object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Presnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1140" to="1153" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A systematic investigation of conceptual color associations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S Y</forename><surname>Tham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Sowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grandison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1311" to="1332" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Emotion-color associations in the context of the face</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Thorstenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Pazda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1032" to="1042" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Object knowledge modulates colour appearance. i-Perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>Witzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="13" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Color Science</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wyszecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Stiles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
