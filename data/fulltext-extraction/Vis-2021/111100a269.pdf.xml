<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scope2Screen: Focus+Context Techniques for Pathology Tumor Assessment in Multivariate Image Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jared</forename><surname>Jessup</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Krueger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Warchol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Hoffer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jeremy</forename><surname>Muhlich</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cecily</forename><forename type="middle">C</forename><surname>Ritch</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Giorgio</forename><surname>Gaglia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shannon</forename><surname>Coy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu-An</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jia-Ren</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sandro</forename><surname>Santagata</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
						</author>
						<title level="a" type="main">Scope2Screen: Focus+Context Techniques for Pathology Tumor Assessment in Multivariate Image Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">09FD65B1CC57DAF575F2D213C153A4DF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Histopathology</term>
					<term>Focus+Context</term>
					<term>Image Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1: Scope2Screen offers (A) Channel &amp; color selection for multi-channel rendering. (B) A WebGL-based viewer capable of rendering 100+GB sized high-plexed and high-resolution (≥ 30k × 30k) image data in real-time. (C) Interactive lensing for close-up analysis -the lens shows a multi-channel immune setting that is different from the global context highlighting basic tissue composition. (D) Dotter panel -stores and organizes snapshots of annotated ROIs to filter, restore, navigate to the image location.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>• 1 : Robert <ref type="bibr">Krueger and</ref>  Since the end of the 19th century, the diagnosis of many diseasescancer in particular -has involved human inspection of stained tissue sections using a simple light microscope <ref type="bibr" target="#b16">[19]</ref>. Histopathology in both research and clinical settings still involves microscopy-based inspection of physical slides but a rapid shift to digital instruments and computational analysis (scope to screen) is now underway <ref type="bibr" target="#b16">[19]</ref>.</p><p>Digital pathology <ref type="bibr" target="#b70">[74]</ref> in a clinical setting focuses on the analysis of tissues stained with colorimetric dyes (primarily hematoxylin and eosin, H&amp;E <ref type="bibr" target="#b63">[67]</ref>) supplemented by single-color immunohistochemistry methods that use antibodies to detect molecular features of interest <ref type="bibr" target="#b48">[52]</ref>.</p><p>In research settings, recently developed high-plex imaging methods such as CyCIF <ref type="bibr" target="#b32">[35,</ref><ref type="bibr" target="#b33">36]</ref>, CODEX <ref type="bibr" target="#b23">[26]</ref>, and mxIF <ref type="bibr" target="#b22">[25]</ref> to measure the levels and sub-cellular localization of 20-60 proteins, providing single-cell information on cell identities and states in a preserved tissue environment. The resulting data are complex, involving multi-channel gigapixel images having 10 6 or more cells. Underdevelopment of analytical and visualization methods is a barrier to progress in digital pathology, explaining the continuing dominance of physical slides. Machine learning on high-plex tissue images has shown promise, particularly with respect to automated classification of cell types <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b34">37]</ref>, tissue morphologies <ref type="bibr" target="#b61">[65]</ref>, and cellular neighborhoods <ref type="bibr" target="#b36">[40]</ref>. However, such data-driven approaches do not leverage hard-won information known primarily to anatomic pathologists on which cell and tissue morphologies are significantly associated with disease outcome or response to therapy. A hundred years of clinical pathology has investigated many striking and recurrent image features whose significance still remains unknown. A critical need, therefore, exists for new software tools that optimally leverage human-machine collaboration in ways that are not supported by existing interfaces <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b58">62]</ref>.</p><p>Pathologists are very efficient at extracting actionable information from physical slides, frequently panning across a specimen while switching between low and high magnifications. They record key observations in notes and by placing dots on slides next to the key features. Digital software needs to reproduce this efficiency and functionality (including a 'dotting' function) while using visual metaphors to present associated data and using machine learning to find similar and dissimilar visual fields. Designing scalable visual interfaces that will work in the context of high-volume clinical workflows <ref type="bibr" target="#b43">[47]</ref> and to high-dimensional research data represents a substantial challenge.</p><p>We addressed these challenges as a team of visualization researchers, pathologists, and cell biologists via a process of goal specification, iterative testing and design, and real-world implementation in a biomedical research laboratory. We make three primary contributions. <ref type="bibr" target="#b0">(1)</ref> We demonstrate task-tailored, lens-centric focus+context technique, which enables intuitive interaction with large (ca. 100 GB) multi-channel images and linked multivariate data (Fig. <ref type="figure">1</ref>). The lensing technique allows users to focus on different aspects of a region for close-up analysis while maintaining the surrounding context. We design novel domainspecific encodings in which features computed from the image (spatial cross-correlation or cell identity) can be accessed in conjunction with the image. (2) We integrate interactive real-time spatial histogram similarity search algorithms able to identify recurrent patterns across gigapixel multi-channel images at different resolutions. Integrated into the lens, this search guides analysts to regions similar to the one in focus, enabling exploratory analysis at scale. <ref type="bibr" target="#b2">(3)</ref> We present a scalable system that combines lens and search features with interactive annotation tools, enabling a smooth transition from exploration to knowledge externalization. Analysts can save, filter, and restore regions of interest (ROIs) within the image space (along with underlying statistics of the filtered single-cell data, channel identities, and color settings) and export them for continued study. Two use-cases demonstrate the applicability of our approach to patient-facing (translational) cancer research and point to future applications in diagnosis and patient care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The related work is three-fold. We first discuss large-scale image viewers as an enabler for our approach. We then summarize focus+context techniques in comparison to overview+detail and pan&amp;zoom, with a focus on image data. Lastly, we compare ROI annotation approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scalable Image Viewers For Digital Pathology</head><p>Many biomedical visualization systems focus on the display of large 2D imaging data and apply multi-resolution techniques such as image pyramids <ref type="bibr" target="#b7">[10]</ref> to handle large data sizes at interactive rates. Deep-Zoom <ref type="bibr" target="#b40">[44]</ref> hierarchically divides images into tile pyramids and delivers pieces as required by the viewer. Zarr <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b41">45]</ref>, a file format and library, abstracts this concept by providing storage of chunked, compressed, Ndimensional arrays. Viewers such as OpenSeadragon <ref type="bibr" target="#b2">[3]</ref> and Viv <ref type="bibr" target="#b37">[41]</ref> leverage these libraries and add GPU-accelerated rendering capabilities. On top of that, many solutions offer data-management, atlas, and analysis capabilities. OMERO PathViewer <ref type="bibr" target="#b8">[11]</ref> is a widely used web-based viewer for multiplexed image data. As an extension to the data management platform OMERO, it supports a variety of microscope file formats. Online cancer atlases such as Pancreatlas <ref type="bibr" target="#b53">[57]</ref> and Pan-Cancer <ref type="bibr" target="#b69">[73]</ref> support data exploration with storytelling capabilities. Minerva Story <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b51">55]</ref> is a new tool used to create atlases for the Human Tumor Atlas Network <ref type="bibr" target="#b52">[56]</ref>. Other solutions focus on combining image visualization with analytics. Napari <ref type="bibr" target="#b58">[62]</ref> is a fast and light-weight multi-dimension viewer designed for browsing, annotating, and analyzing large multi-dimensional images. Written in Python, it can be extended with analytic functionality, e.g., in combination with SciMap <ref type="bibr" target="#b47">[51]</ref>. Other analytical tools focus on end-users, such as the open-source solutions histoCAT <ref type="bibr" target="#b54">[58]</ref> and Facetto <ref type="bibr" target="#b30">[33]</ref>, and commercial tools such as Halo <ref type="bibr" target="#b27">[30]</ref> and Visiopharm's TissueAlign <ref type="bibr" target="#b4">[6]</ref> supporting split-screen comparison for serial sections. Screenit <ref type="bibr" target="#b18">[21]</ref> presents a design to analyze smaller histology images at multiple hierarchy levels. Similarly, ParaGlyer <ref type="bibr" target="#b46">[50]</ref> is an analysis approach for multiparametric medical images that permits analysis of associated feature values and comparisons of volumetric ROIs by voxel subtraction. Somarakis et al. <ref type="bibr" target="#b59">[63]</ref> offer comparison views with a focus on spatially-resolved omics data in a standard viewer. These tools feature multiple linked views for overview+detail exploration. In comparison, our solution focuses on interactive focus+context and rich annotation with contextual details displayed near the ROI and supports a neighborhood-aware similarity search on top of local image pixel and feature value comparison. Most viewers operate on much smaller datasets. Our viewer builds on Facetto <ref type="bibr" target="#b30">[33]</ref> and Minerva <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b51">55]</ref> and supports multi-channel and cell-based rendering with linked data at a scale few other solutions support. The main contribution of this paper, however, is the embedded interactive lensing technique for multivariate image data and its task-tailored features supporting the digital pathology workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Focus+Context-based Image Exploration</head><p>Cockburn et al. <ref type="bibr" target="#b17">[20]</ref> categorize interaction techniques to work at multiple levels of detail into focus+context (F+C), overview+detail (O+D), zooming, and cue-based views. F+C minimizes the seam between views by displaying the focus within the context, O+D uses spatial separation, zooming temporal separation <ref type="bibr" target="#b67">[71]</ref>, and cue-based methods selectively highlight or suppress items. Comparative studies show that F+C techniques are often preferred and allow for efficient and effective target acquisition <ref type="bibr" target="#b57">[61]</ref> and steering tasks <ref type="bibr" target="#b24">[27]</ref> in multi-scale scenarios. A common F+C technique is the lens <ref type="bibr" target="#b11">[14]</ref>, a generic see-through interface that lies between the application and the cursor. Tominski et al. <ref type="bibr" target="#b64">[68,</ref><ref type="bibr" target="#b65">69]</ref> present a conceptual pipeline for lensing consisting of selection (what data), the lens-function (filters, analysis), and a join operation with the underlying visualization (mapping, rendering). They further categorize into lens properties (shape, position, size, orientation), and into data tasks, e.g., (geo)spatial analysis. Different lenses to magnify, select, filter, color, and analyze image data were proposed: Carpendale et al. <ref type="bibr" target="#b14">[17]</ref> present a categorization of 1-3D distortion techniques to magnify in 2D uniform grids. Focusing on lens-based selection, MoleView <ref type="bibr" target="#b26">[29]</ref> selects spatial and attribute-related data ranges in spatial embeddings and Trapp et al. present a technique for filtering multi-layer GIS data for city planning <ref type="bibr" target="#b66">[70]</ref>. Similarly, Vollmer et al. propose a lens to aggregate ROIs in a geospatial scene to reduce information overload <ref type="bibr" target="#b68">[72]</ref>. Flowlens <ref type="bibr" target="#b20">[23]</ref> features a lens for biomedical application: to minimize visual clutter and occlusions in cerebral aneurysms. There are a few tools in digital histopathology with lensing capabilities. Vitessce <ref type="bibr" target="#b21">[24]</ref>, positions linked views around an image viewer <ref type="bibr" target="#b37">[41]</ref> and includes a lens to show a predefined set of channels. However, by design, they do not focus on supporting a specific pathology process, nor does the lens support magnification, feature augmentation, comparison, or search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Handling and Visualization of ROI Annotations</head><p>Different techniques exist to mark, visualize, and extract ROIs in images, but only a small subset is used in the digital pathology domain. QuPath <ref type="bibr" target="#b10">[13]</ref>, an extensible software platform, allows annotating histology images with free form selection tools and more advanced selection options like pixel-based nearest neighbors and magic wand, extending from the clicked pixel to neighboring areas with a threshold.</p><p>Visopharm's viewer <ref type="bibr" target="#b5">[7]</ref> similarly provides different geometric shapes to annotate images. In Orbit <ref type="bibr" target="#b62">[66]</ref> and Halo <ref type="bibr" target="#b27">[30]</ref> users can define inclusion and exclusion annotations, organize them in groups, and train a classifier. Going beyond manual annotation, Quick-Annotator <ref type="bibr" target="#b39">[43]</ref> leverages a deep-learning approach to search and suggests regions similar to a given example. Similarly, Ip and Varnsesh <ref type="bibr" target="#b28">[31]</ref> narrow down and cull out ROIs of high conformity and allow users to interactively identify the exceptional ROIs that merit further attention on multiple scales. We incorporate these ideas but instead apply a fast neighborhood-based histogram search running on multiple image channels in real-time to guide the user to similar areas in the viewport.</p><p>When large images are annotated on different scales it becomes challenging to navigate in an increasingly cluttered space. Some features might even be too small to be identifiable at certain zoom levels. Scalable Insets <ref type="bibr" target="#b31">[34]</ref> is a cue-based technique that lays out regions of interest as magnified thumbnail views and clusters them by location and type. TrailMaps <ref type="bibr" target="#b71">[75]</ref> proposes an algorithm to automatically create such insets (here bookmarks) based on user interaction and previously viewed locations. They also offer timeline-and category-based groupings for a better overview and faster navigation. We choose a more familiar design to cater to the application domain needs and conventions but enhance our approach by supporting rich annotations that store not only geometry but also linked single-cell data and descriptive statistics. Closely related to our approach is the work by Mindek et al. <ref type="bibr" target="#b42">[46]</ref> that proposes annotations linked to contextual information so that they remain meaningful during the analysis and possible state changes. We extend this idea with overview, search, and restoring capabilities integrated into focus+context navigation in large-scale multivariate images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND: MULTIPLEX TISSUE IMAGING</head><p>We analyze multiplexed tissue imaging data generated with CyCIF <ref type="bibr" target="#b32">[35]</ref> but our visualization approach can be applied to images acquired using other technologies such as CODEX <ref type="bibr" target="#b23">[26]</ref>. Images are segmented and signal intensity is measured at a single-cell level. Here we provide a brief overview of the process and data (Fig. <ref type="figure" target="#fig_0">2</ref>). Acquisition. Multiplexed tissue imaging allows to analyze human tissue specimens obtained from patients for pathologic diagnosis. The approach used by the investigators, as described in our previous work <ref type="bibr" target="#b30">[33]</ref>), involves iterative immunofluorescence labeling with 3-4 antibodies to specific proteins followed by imaging with a high-resolution optical microscope in successive cycles. This results in 16-bit fourchannel image datasets for up to 60 proteins of interest (60 images), 30k x 30k in resolution, and often greater than 100GB in size, allowing for extensive characterization and correlation of markers of interest in large tissue areas at sub-cellular resolution. Processing. High-resolution optical microscopes have limited fields of view, so large samples are imaged using a series of individual fields which are then stitched together computationally to form a complete mosaic image using software such as ASHLAR <ref type="bibr" target="#b44">[48,</ref><ref type="bibr" target="#b45">49]</ref>. A nonrigid (B-spline) method <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b38">42]</ref> is applied to register microscopy histology mosaics from different imaging processes <ref type="bibr" target="#b12">[15]</ref>, e.g., CyCIF and H&amp;E. CyCIF mosaics can be up to 50,000 pixels in each dimension and contain as many as 60 channels, each depicting a different marker. Mosaic images are then classified pixel-by-pixel to discriminate cells using, e.g., a random forest <ref type="bibr" target="#b60">[64]</ref>, then individual cells are segmented <ref type="bibr">[38]</ref>. Segmentation information is stored in 32-bit masks that define the cell ID for each pixel in a multi-channel image stack. Next, per-cell mean intensities are extracted for the 10 6 or more individual cells in a specimen. The processing steps are combined in an end-to-end processing pipeline named MCMICRO <ref type="bibr" target="#b55">[59]</ref>. The resulting 16bit multi-channel images (≈100GB), 32bit segmentation (≈5GB), and high-dimensional feature data (≈2GB) are then ready for interactive analysis. Terminology and Data Characteristics. Our datasets contain (1) a multi-channel tissue image stack with 1-60 channels in OME-TIFF format <ref type="bibr" target="#b1">[2]</ref>, (2) a segmentation mask also in TIFF format, and (3) a table of extracted image features in CSV format (Fig. <ref type="figure" target="#fig_0">2</ref>). Each image channel in the multi-channel image stack represents data from a distinct antibody stain and is stored as an image pyramid (in the OME-TIFF) for efficient multi-resolution access. These channels can result from different imaging processes (e.g., CyCIF and H&amp;E). A segmentation mask labels individual cells in each tissue specimen with a unique cell ID. Similar to each image channel, the mask is stored in pyramid form. A CSV file stores single-cell features (columns) for each cell (row). These features consist of extracted mean intensity values per image channel for that cell, x and y position of the cell in image space, and its cell ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DOMAIN GOALS AND TASKS</head><p>This project is rooted in a collaboration with physicians and researchers in the Department of Pathology and the Laboratory of Systems Pharmacology (LSP) at Harvard Medical School. Four experts in the domain of digital histopathology participated in the project. The team consists of two pathologists, two computational biologists, and four computer scientists. The overall goal of our collaborators is to characterize the features of tumors including cell types &amp; states, their interactions, and their morphological organization in the tumor microenvironment. Pathologists are physicians who diagnose diseases by analyzing samples acquired from patients. Anatomic pathologists specialize in the gross and microscopic examination of tissue specimens. They characterize cell and tissue morphology using light microscopy, and molecular features using immunohistochemistry and immunofluorescence. The pathologists involved in this project engage in research and have expertise in imaging, computational biology, and defining the role of diverse cell states in shaping and regulating the tumor microenvironment.</p><p>Computational and Cell Biologists complement expertise in biomedical science with skills in technical fields including mathematics, computer science, and physics. Multiplexed immunofluorescence experiments involve collection of primary imaging data which is used for a wide variety of complex computational tasks including image registration and segmentation, and extraction of numerical feature data, as well as downstream analyses of cell states, spatial statistics, and other phenotypes. Biologists interpret aspects of cell morphology and marker expression, but pathologists complement these analyses with greater depth of experience with human tissue morphology and disease states. Visualization Experts. By contrast, the computer scientists provide expertise in visualization and visual data analysis. They work in close collaboration with the aforementioned investigators to provide novel analytics prototypes that perform a variety of analysis tasks and can be integrated into research studies and laboratory IT infrastructure. To understand domain goals, the visualization experts in this study participated in weekly meetings focused on image processing, biomedical topics, and on iterative goal-and-task analyses for the proposed approach. The collaboration with the LSP started in 2018 with Facetto <ref type="bibr" target="#b30">[33]</ref>. In Fall 2020, this team began working together to develop advanced tools that cater to visual exploration, inspection, and annotation. We followed the design study methodology by Sedlmair et al. <ref type="bibr" target="#b56">[60]</ref>. This methodology describes a setting in which visualization experts analyze a domain-specific problem, design a visualization approach to solving it, validate their design, and reflect on lessons learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks and Challenges</head><p>In weekly sessions, we identified a workflow (Fig. <ref type="figure">3</ref> Fig. <ref type="figure">3</ref>: The pathological workflow starts with exploratory navigation in the image (T1). ROIs are magnified, measured, and analyzed (T2) by switching and combining image channels (T3) and investigating single-cell marker statistics (T4). Identified regions often appear in patterns across the image. Finding such similar regions (T5) can ease manual search. ROIs are then annotated (T6). These steps build an iterative process where annotations are refined, and further areas are explored. The ROIs are stored or exported to discuss with colleagues or for examination. slides physically on a microscope stage and switching between view magnifications levels. They depend on a seamless visual experience to diagnose diseases or conditions. Challenge: Image analysis must not only provide seamless pan &amp; zoom, but also switching between channels of different image modalities. Existing solutions do not scale beyond 4 to 5 channels. They also lack on-demand rendering, blending of multiple channels, and ways to highlight and recall ROIs. T2. Close-up ROI Analysis: Once a region of interest is found in the tissue specimen, experts focus and zoom in on the area for close-up inspection and measurement, without losing the spatial context of e.g., a tumor region's surrounding immune cells. Challenge: In addition to interactive rendering of different resolution levels in a combined space, experts need to focus and measure without losing proportions and larger context. Panning and zooming between overview+detail and individual marker channels requires a large amount of mental effort as either context or details are lost <ref type="bibr" target="#b31">[34]</ref>. T3. Regional Comparison of Image Markers: For a region in focus, cell biologists need to relate and compare between different marker expressions (e.g., DNA, CD45, Keratin) of different image modalities (CyCIF <ref type="bibr" target="#b32">[35,</ref><ref type="bibr" target="#b33">36]</ref>, H&amp;E <ref type="bibr" target="#b63">[67]</ref>, CODEX <ref type="bibr" target="#b23">[26]</ref>, mxIF <ref type="bibr" target="#b22">[25]</ref>, etc.), encoded in individual image channels. Challenge: Whole-slide switching between channels can lead to losing focus and change blindness due to different morphological structures. T4. Relate to Spatially Referenced Single-Cell Expressions: Besides looking at the raw image, experts analyze extracted singe-cell marker values and their spatial statistics in (A) image and (B) highdimensional feature space. Of special interest is cell density in the tissue, counts and spatial arrangements of cell-types, and distributions of marker intensities. For each of these descriptive statistics, it is important to relate regional phenomena to statistics of the whole image. Challenge: Providing complex spatially referenced information in proximity while dealing with a dense cellular image space and catering to highly specific domain conventions. T5. Find Similar Regions: Analyzing a whole slide image is timeconsuming. Often the cancer micro-environment consists of repetitive patterns of cell-cell interactions and morphological structures across channels that pathologists annotate and compare to each other. Challenge: Finding and guiding users to such structures in an interactive fashion on different spatial scales and across image dimensions. T6. ROI Annotation and Summaries: A common pathology task concerns the manual delineation of tumor mass and other structures on the digitized tissue slide, known as region annotation. These annotated regions need to be extracted, semantically grouped, and summarized in a structured way for collaboration and examination. Challenge: The annotation process must be integrated seamlessly with the analysis so that experts can extract, group, and refine patterns along the way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPROACH</head><p>We used the tasks of Section 4 to guide the design and implementation of Scope2Screen, playing the translator role put forth in the design study methodology by Sedlmair et al. <ref type="bibr" target="#b56">[60]</ref>. Fig. <ref type="figure">1</ref> offers an overview of the user interface. After importing the data, analysts can explore (Fig. <ref type="figure">1</ref>) the image by activating a set of different channels (A) with distinct color configurations and adjustable intensity ranges. The selections are then rendered in a combined view (B) for interactive panning and zooming (T1). Using a virtual lens (C), users can focus, magnify, and measure regions of interest for close-up analysis (T2). By toggling image channel combinations inside and outside of the lenses, one can regionally compare different combinations of marker expressions (T3). Other lens filters link to underlying single-cell data, offering descriptive statistics about marker distributions and cell counts and types (T4). Using a search-by-example approach, the tool guides users to regions with similar patterns as those in scope (T5). To save and extract a region, analysts can take snapshots that save the ROI together with relevant notes (Fig. <ref type="figure">1 D</ref>), current settings, and interior single-cell data for the region (T6). Annotated areas can be filtered and exported to share with collaborators and to recall for further examination. In the following sections, we introduce the corresponding techniques and features (F1-F6) and discuss design decisions to enable these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scalable Image Exploration (F1)</head><p>We designed Scope2Screen for interactive exploration and multi-scale visualization (T1) of resection specimens (Sec. 3). To make the viewer scalable for high-resolution data, we decided to leverage image pyramids <ref type="bibr" target="#b7">[10]</ref> to load only sections of the image for a given viewport and zoom level. To enable flexible exploration of image channels and to visualize data in the viewport, the viewer operates in two multi-resolution rendering modes: channel-based and cell-based. Channel-based rendering maps intensity values of selected image channels to color. It then computes a mixed color value for each pixel in the viewport. Cellbased rendering leverages a layered segmentation mask that indexes each pixel to a cell. This way, each cell can be colored individually to visually express selections, cell types, etc. Both rendering modes operate at interactive rates, allowing users to pan &amp; zoom, select regions, and to color and mix channels in real-time. These rendering modes cater to our experts' needs to analyze on tissue and single-cell level. Sec. 6 gives details on the implementation of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Lensing Features for Multivariate Image Data</head><p>Conveniently, the principal technology of the microscope, the lens, is highly adaptable. To enable close-up analysis (T2) of ROIs and to connect the optical and digital experiences for users, we introduced a digital lens designed to imitate the familiar experience of inspecting through an eyepiece. To support the requirements of our collaborators, we equip the lens with features (Fig. <ref type="figure">4,5</ref>), ranging from magnification and channel filters (F3) to descriptive single-cell statistics (F4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Magnification and Measuring (F2)</head><p>Our users rely on being able to toggle quickly between high and low magnification powers as part of an established workflow for considering a region of interest up-close, as a localized arrangement of cells, and incontext, as part of a larger tissue sample (T2). Within the virtual viewer, this zoom-level interchange can be challenging to control. Constraining magnification to the lens's boundary (Fig. <ref type="figure">5</ref>) while maintaining the contextual overview then becomes a convenient strategy for handling simultaneous focal analysis. Because the magnifying lens (Fig. <ref type="figure">5 A</ref>), when active, occludes part of the image space, we experimented with a common spatial manipulation to create a faux-spherical representation: the fisheye (B). However, distortion is a troubling approach for experts who make evaluations based on morphology, leading us to introduce a hybrid plateau model (C) that maintains the original composition within the central area of the lens using a standard zoom and only compresses the periphery to seamlessly transfer into the context without occlusion. The scale of an ROI (in microns) is closely tied to magnification interpretability. Users consider area-based standards for clinical validation as part of their inspection methodology. Visible axes around the lens allow for a quick understanding of scale (Fig. <ref type="figure">1 C</ref>). Additionally, this embedded conversion capability between digital and physical units is a useful tool for extended functionalities that emulate related user tasks (e.g., cell prevalence counting and density analysis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Channel Analysis (F3)</head><p>To address regional comparison of channels (T3) that represent data from the same modality (e.g., CyCIF), other modalities (e.g., H&amp;E <ref type="bibr" target="#b63">[67]</ref>), and across different planes (e.g., slide sectioning), we iterated over different designs and finally settled on the following features.</p><p>The first lens feature allows users to quickly isolate each selected channel individually for improved views of a distinct channel (Fig. <ref type="figure">4</ref> A) in focus while keeping the multi-channel setting in the context. The second feature combines multiple channels in the scope (Fig. <ref type="figure">4</ref> B, Fig. <ref type="figure" target="#fig_7">9</ref>) while the context can keep a distinct setting (Fig. <ref type="figure">1</ref>). We specifically designed this setting for semantically dependent channels such as RGB images for H&amp;E staining. It also addresses our experts' needs to analyze spatial relations of a set of independent channels, for example, from specific immune markers in a region of interest, while keeping globally a different set of more general cancer and stromal markers. Addressing early feedback from our experts, we added the capability to equip the lens with multiple sets of such channel combinations (Fig. <ref type="figure" target="#fig_7">9</ref>) in advance or add them during analysis. These sets can be quickly toggled during exploration to investigate different biological questions, e.g., focusing on immune reactions, or tissue architecture. To overcome occlusion, we chose to offer two solutions leveraging temporal and spatial separation. Firstly, we introduce adjustable interpolation controls allowing to blend seamlessly from the overlaying lens-image to the underlying global channel combination. This transition helps to visually align and keep track of often very different structures in different channel combinations. To further reduce change blindness, a split-screen lens juxtaposes a second lens instance in proximity, which displays (copies) the occluded part in the original global channel setting (Fig. <ref type="figure">4 C</ref>). This allows for side-by-side comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Feature Augmentation (F4)</head><p>Our multiplex image data (Sec. 3), often comprises up to 60 channels. While three channels can guarantee non-overlapping (RGB) colors, adding even more channels makes visual decoding for analysts mentally challenging and increasingly inaccurate. Additionally, color encodings of quantitative data are often hard to gauge <ref type="bibr" target="#b35">[39]</ref>. Instead, to enable quantitative analysis of selected regions, we chose to augment the image space with descriptive statistics of the extracted single-cell data (T4) using more abstract visual encodings (Fig. <ref type="figure">4 D-F</ref>). We developed three task-tailored lens settings showing marker distributions, density reports, and cell types and counts. With every update of the lens position on screen, the back-end queries the in-memory CSV table (Sec. 3) for cells in the lens's area and returns cell Id values along with the requested statistics, which are then processed and rendered into different charts. To speed this up, we execute spatial range queries with a ball-tree index structure <ref type="bibr" target="#b19">[22]</ref>. This yields a run time complexity of O(n log n). Single-Cell Histograms. To analyze marker distribution in a selected region, we compute binned histograms of the single-cell aggregates (see CSV table, Sec. 3) for selected channels. We present these histograms (Fig. <ref type="figure">4 D</ref>) in proximity to the focus area for quick look-ups. We decided to arrange the histograms in a vertical layout to ease comparison. According to our domain collaborators, absolute comparison of individual markers is statistically not meaningful as the signal-to-noise ratio changes per cycle and stain in the imaging process (Sec. 3). Instead, they are interested in a relative comparison of the distributions. We use a log 2 transformation to make these skewed marker distributions comparable, followed by a cut-off of the 1st and 99th percentile to remove outliers.  proximity to the lens (Fig. <ref type="figure">4 E</ref>). This decision allows for a more compact representation of the multivariate single-cell data. When we first showed this to our collaborators, they missed a reference to compare the region of interest to global image statistics. In a second design iteration, we thus encoded arithmetic means for the whole tissue. The histograms show these whole-slide references as orange bins (background) and the radial plot encodes the information as a polyline.</p><formula xml:id="formula_0">1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 2 1 0 3 3 1 1 2 1 3 2 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 3 3 3 1 1 3 1 3 3 1 1 2 0 1 1 0 0 0 0 0 0 1 2 0 3 2 2 0 0 0 1 0 0 1 2 0 3 2 1 3 3 2 3 3 3 3 3 3 2 2 2 1 2 2 2 2 1 3 2 1 0 0 0 1 0 0 2 1 1 0 1 2 2 2 3 1 1 3 1 2 2 1 0 3 3 1 1 1 2 3 3 3 3 2 2 2 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 2 0 3 3 0 0 1 0 3 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1 0 2 3 3 3 3 2 2 3 3 3 3 2 2 2 1 2 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 2 1 0 3 3 1 1 2 1 3 2 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 3 3 3 1 1 3 1 3 3 1 1 2 0 1 1 0 0 0 0 0 0 1 2 0 3 2 2 0 0 0 1 0 0 1 2 0 3 2 1 3 3 2 3 3 3 3 3 3 2 2 2 1 2 2 2 2 1 3 2 1 0 0 0 1 0 0 2 1 1 0 1 2 2 2 3 1 1 3 1 2 2 1 0 3 3 1 1 1 2 3 3 3 3 2 2 2 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 2 0 3 3 0 0 1 0 3 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1 0 2 3</formula><p>Cell Types and Counts. Our collaborators want to validate results of cell-type classification and clustering <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b34">37]</ref> and set them into spatial context to expressions in the image channels. We color-code cell boundaries by detected cell types, using the cell-based rendering mode. This mode utilizes the segmentation mask to retrieve which pixel is linked to which cellId (Sec. 5.1). The colored boundaries (Fig. <ref type="figure">4</ref>, right) allow to display the data at its spatial image position and still see the channel colors. As classification often strongly depends on the expressions in a few channels, users can pick matching channel colors and check if the pattern spatially aligns with the cell types. To ease quantification in the localized region, we also provide an ordered list of cell types and counts near the lens. Counts are encoded as bars that dynamically adapt while hovering over the data. Users can switch between two orderings: Locked cell type positions allow to observe increasing or decreasing presence of specific types; ranking by count is preferred to spot the most prominent cell types in the focus region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Finding Similar Regions (F5)</head><p>Once identified, a region of interest often serves as a reference point to find similar areas within the image (T5). Examples for repetitive patterns are, e.g., tumor-immune boundaries or germinal centers where mature B cells proliferate, differentiate, and mutate their antibody genes.</p><p>To find similar regions, we chose to provide a method operating directly on the image to align as close as possible with the visual perception (see Fig. <ref type="figure" target="#fig_4">6</ref>). We consider regions similar if they have a similar intensity distribution. To compare a region to all possible areas in an image, we employ a sliding-window strategy that compares histograms of regional marker intensity distributions across the image. To trigger the search, the HistoSearch lens can be placed over the pattern of interest in the image. HistoSearch is equipped with a slider to set a contour threshold, allowing for fine-tuning of what's considered similar. The applied integral histogram method <ref type="bibr" target="#b49">[53,</ref><ref type="bibr" target="#b50">54]</ref> makes it possible to employ even an exhaustive search process in real-time. We adapt and extend a Python implementation <ref type="bibr" target="#b3">[5]</ref>. Our method works in four steps (Fig. <ref type="figure" target="#fig_4">6</ref>, 1-4):</p><p>Step 1: A box-or circle-shaped region (the lens area) is extracted, and a histogram of its greyscale values is computed (Fig. <ref type="figure" target="#fig_4">6</ref>, Step 1).</p><p>Step 2: For each pixel in the whole channel image, a histogram of the greyscale values surrounding (lens radius) the pixel is computed.</p><p>Semantically, this means that we take into account spatial neighborhood information and not simply compare pixel by pixel (Fig. <ref type="figure" target="#fig_4">6</ref>, Step 2).</p><p>Step 3: The local histogram for the region surrounding each pixel in the image is then compared to that of the lens using Chi-square distance. We apply Porikli's integral histogram <ref type="bibr" target="#b50">[54]</ref>, which recursively propagates histogram bins of previously visited image areas using values from neighboring data points instead of repetitively executing the full histogram computation. This is then compared to the lens histogram using Chi-square distance (see Eq. 1) for two arrays X and Y with N dimensions across all channels C. This leads to a similarity map with a similarity value for each pixel in the image (Fig. <ref type="figure" target="#fig_4">6</ref>, Step 3).</p><formula xml:id="formula_1">1 c C ∑ c=1 N ∑ i=1 ((x i − y i ) 2 /(x i + y i ))<label>(1)</label></formula><p>Step 4: We use marching squares <ref type="bibr" target="#b49">[53]</ref> to detect contours in the similarity map and extract these contours as polygons that we render on screen (Fig. <ref type="figure" target="#fig_4">6</ref>, Step 4).</p><p>Step 2 and 3 can be computed for multiple channels. To not lose information, we compute each channel's similarities separately and then aggregate them to a combined similarity map. Similar to our multi-resolution rendering strategy (Sec. 5.1), we execute the histogram search algorithm on the fly on the respective TIFF-file layer (Sec. 3) that aligns with the current zoom level. Fully zoomed out, the aggregation level is higher; while zoomed in, we process the highest resolution but for a smaller image area. The approach can also be employed in full resolution to the whole image (Sec. 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Descriptive ROI Summaries (F6)</head><p>When using a light microscope, a pathologist often 'dots' whole slides with a pen to indicate ROIs for later examination. We introduced a digital "dotter" to represent this approach but with extended support for annotation (T6) during exploration (Fig. <ref type="figure" target="#fig_5">7</ref>). The lens functions as a camera lens that can snapshots whenever an ROI is in focus. Our collaborators currently narrate image annotations to data stories for examination, teaching and outreach of their research <ref type="bibr" target="#b51">[55]</ref>. The link to analysis results is often lost as annotations must be manually recreated in the used tools. To maintain analytic provenance, we developed a novel rich snapshot method that not only saves the image area under the lens but also stores all associated data: a list of active image channels in focus and in the peripheral context, channel colors, and range settings, the linked single-cell data in scope, and textual annotation such as title and a more detailed description pathologists and cell biologists can add.</p><p>These rich snapshots are available as thumbnails in the Dotter panel (Fig. <ref type="figure" target="#fig_5">7 C</ref>) and are interactively linked to overlays within the viewport. Inside the panel, the user can save, delete and load ROIs from a database. Names and descriptions can be edited and referenced as tags for filtering results. It is important to our users to be able to quickly recall and restore the snapshots for further analysis and fine tuning, but also to return to ROIs in their original context. Thus, by clicking on thumbnails, the viewer navigates back to the coordinates and zoom level of the overlay. By clicking on the overlay's marker icon, we fully restore the lens, along with its global view setting, i.e. active channels, range and color mappings for the context (Fig. <ref type="figure" target="#fig_5">7 D</ref>). This workflow facilitates hand-offs between colleagues who benefit from shared evaluation.</p><p>To extend the search for an ROI in the Dotter panel, users can use HistoSearch (Sec. 5.3) to find regions alike in the whole image space (other than the viewport during interactive analysis). We render these annotations as image-overlays but as soon as the user picks up a region, we restore the lens, and the user can alter the region as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ARCHITECTURE AND IMPLEMENTATION</head><p>Scope2Screen is an open-source web-application (available here: [4]) with a back-end Python server built on Flask and a Javascript frontend. The server's restful API allows to retrieve image and feature data and to steer analytics and is easily extendable with new methods and corresponding API endpoints. The frontend is built using Bootstrap, D3.js, and OpenSeadragon (OSD) <ref type="bibr" target="#b2">[3]</ref>, a web-based zoomable image viewer, which we extend significantly.</p><p>We developed a lensing library that supports a subset of the interactive features of Scope2Screen as a generic plugin <ref type="bibr" target="#b0">[1]</ref> for OSD. The library relies on a hidden viewer that provides access to both in-view and out-of-view image data to support controlled rendering within the lens, along with other complementary features. Our application utilizes and extended lensing's logic with additional features (Sec. 5.2).</p><p>Scope2Screen also builds on Facetto <ref type="bibr" target="#b30">[33]</ref> but makes improvements to the architecture. Instead of preprocessing OME-TIFF <ref type="bibr" target="#b1">[2]</ref> image stacks to DeepZoom <ref type="bibr" target="#b40">[44]</ref> we now read chunks (cropped 2D arrays of the respective layer in the image channel/mask) on-the-fly from layered OME-TIFFs to render it in the viewport at multiple resolution levels, depending on the current zoom setting. We rely on Zarr <ref type="bibr" target="#b41">[45]</ref>, a format for the storage and handling of chunked, compressed, N-dimensional arrays. The client-side rendering is hardware accelerated. It relies on WebGL [8] shaders from Minerva <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b51">55]</ref> and supports Facetto's channel and cell-based rendering modes (Sec. 5.1) for both the lens (focus) and the whole viewport (context). To access and filter the linked single cell feature (CSV) data more dynamically and at scale, we moved data processing and ball-tree <ref type="bibr" target="#b19">[22]</ref> indexed querying (Section 5.2.3) to the back-end so that the client only loads requested pieces (in lens or viewport), aligning with our multi-resolution rendering strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">USE CASES</head><p>We applied Scope2Screen to study two cancer datasets that we acquired from sections of lung and colon cancer using CyCIF <ref type="bibr" target="#b32">[35]</ref>. Immediately adjacent sections were H&amp;E stained <ref type="bibr" target="#b63">[67]</ref> and used to evaluate tissue morphology. We carried out the analysis together with our collaborators over zoom using a Pair-Analytics approach <ref type="bibr" target="#b9">[12]</ref>; we steered the tool guided by the domain collaborators. This method is advantageous because it pairs Subject Matter Experts (SME) with expertise in a domain with Visual Analytics Experts (VAE) who have the technical expertise in the operation of VA tools but lack contextual knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Use Case 1: Colorectal Cancer</head><p>Tumors are complex ecosystems of numerous cell types arranged into recurrent 3D structures. However, the patterning of specific tumor and immune cell-states is poorly understood due to the difficulty of mapping high-dimensional data onto large tissue sections. In use case 1, two pathologists and one cell biologist analyzed a human colorectal carcinoma (CRC1) from the Human Tumor Atlas Network (HTAN) (PMID 32302568) to explore tumor and immune cell interactions. Data: CRC1 is a human colorectal adenocarcinoma that was serially sectioned at 5um intervals into 106 sections. 24-marker CyCIF was performed on every 5th section. Every adjacent section was stained with H&amp;E. CyCIF and H&amp;E images were registered and stitched, and single-cell segmentation and fluorescence intensity quantification were performed using MCMICRO <ref type="bibr" target="#b55">[59]</ref>. Cell types were defined by expression of lineage-&amp; state-specific markers. Here, we analyze one CyCIF and an adjacent H&amp;E image. The data included 40 CyCIF channels and 3 (RGB) H&amp;E channels encompassing 1.28 mio cells in a field 26,139 x 27,120 pixels (8,495 x 8,814 microns) in size (87.66 GB). Analysis: We began the analysis with a low magnification overview of the CyCIF images using DAPI (blue), keratin (white), and aSMA (red) channels in the whole viewport. In combination, these channels illuminated pathologically-relevant structures of the tissue, including the morphology of nuclei (DAPI), abnormal epithelial cells (keratin), and muscular layers (aSMA) (Fig. <ref type="figure" target="#fig_6">8 A, B</ref>). A second channel defined immune populations including CD4+ helper T cells (red), CD8+ cytotoxic T cells (green), and CD20+ B cells (white) to analyze immune interactions with tumor and adjacent normal regions (Fig. <ref type="figure" target="#fig_6">8 C</ref>).</p><p>For each marker, we defined an upper and lower color mapping range using the channel range sliders (Fig. <ref type="figure">1 A</ref>). The low-magnification view revealed a small region of tumor budding cells (≤ 1mm 2 ), a phenomenon in which infiltrating single tumor cells or small clusters of cells (≤ 4) appear to "bud" from the tumor-stromal interface at the invasive margin, correlating with poor clinical outcomes (Fig. <ref type="figure" target="#fig_6">8 D</ref>). We used the standard lens magnifier to focus analysis on the budding region while maintaining spatial context.</p><p>To further explore spatial patterns of marker expression, we activated the "single-cell radial chart". It provides a dynamic display of the mean single-cell expression levels of all CyCIF markers within the lens and the global mean of the markers across the entire image for comparison. This enabled the experts to see that tumor and immune cells in the several regions, including the budding region, were positive for PD-L1, a protein that suppresses the activity of cytotoxic CD8 T cells, which is often clinically targeted by antibody therapies. To capture images and associated single-cell data of these ROIs for subsequent review of immune interactions and tumor features, we took snapshots and annotated the areas 'PD-L1 rich region' (Fig. <ref type="figure" target="#fig_6">8 E</ref>) for later analysis.</p><p>We next used the split-screen lens (Fig. <ref type="figure">4 C</ref>) to view H&amp;E and CyCIF images side-by-side. Using this tool, pathologists validated the alignment of H&amp;E and CyCIF channels acquired from adjacent sections to compare histologic and molecular features. They identified areas in the H&amp;E images with marked chronic inflammation (lymphocytes and macrophages) in the peri-tumoral stroma for further evaluation. Direct comparison of the H&amp;E and CyCIF data in these tissue regions using the split-screen lens allowed the pathologists to characterize lymphocyte aggregates with predominantly cytotoxic CD8+ T cells in peri-tumoral stroma (Fig. <ref type="figure" target="#fig_6">8 E</ref>), as well as clusters of CD20+ B cells and CD4+ helper T cells forming 'tertiary lymphoid structures' (TLS). Direct comparison of H&amp;E and CyCIF data in this manner allowed for targeted molecular characterization of immune populations such as lymphocytes which is not possible with H&amp;E alone (Fig. <ref type="figure" target="#fig_6">8 F</ref>). To compare the marker intensity value distributions more precisely, we switched to the 'lens histogram' (Fig. <ref type="figure">4 A</ref>) and compared the results with the 'cell-type' lens within each region to assess marker expression with the results of the automated cell type classifier.</p><p>Based on review of the images, the pathologists used the Dotter's 'snapshot' function to annotate three immunologically distinct regions (described above), three tumor regions with distinctive histomorphology (glandular, solid, and mucinous regions), and adjacent normal colonic mucosa. Using the 'similarity search' on the normal mucosal region (Fig. <ref type="figure" target="#fig_4">6</ref>, top), we identified areas with similar histologic features, confirmed by pathologist inspection, embedded within the tumor mass which were not readily apparent on initial low-magnification review of the H&amp;E images, validating the utility of the search method. We saved these findings to our database for subsequent retrieval. Feedback: Although we worked with our collaborators in weekly sessions over several months, we received additional comments on design improvements and future features within the 2-hour analysis session. They found the normal lens magnification the most useful. The fisheye lens was problematic for review of tissue images due to distortion of cell morphology and tissue architecture, which may complicate the interpretation of important pathologic features such as nuclear shape. This aligns with earlier feedback motivating the design of the plateau lens, which they confirmed as a helpful improvement. One pathologist also suggested to equip the lens with different predefined filter settings to be able to quickly toggle for different analysis tasks. While both the radial chart and histograms were well received, they asked to highlight under-or over-represented marker values more prominently. The histograms were described as easier to read and should be extended to represent non-active image channels as well. They also asked for a) a heatmap that color codes marker correlation, and b) the ability to define channel combinations inside the lens on-the-fly, not only in pre-configuration. A recurring piece of feedback during the session was to add more descriptive statistics such as bin sizes in the histogram, ratios of cell populations, and ways to precisely define intensity ranges by value. The proximity of single-cell data to the inspected area and the H&amp;E channel comparison, which is limited or non-existent in other tools, were described as particularly useful. The ability to simultaneously inspect and mark different regions of the tumor was perceived as a promising area for further development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Use Case 2: Lung Adenocarcinoma</head><p>Lung adenocarcinoma is a common subset of lung cancer that, in later stages, often does not respond to chemotherapy. Immunotherapy has shown great promise, but patient response varies according to each tumor's specific microenvironment. To assess why only certain patients respond, one needs an in-depth understanding of the tumor and immune landscape. Together with two biologists, we apply Scope2Screen to explore the immune reactions in lymphocyte structures. Data: Using t-CyCIF, we have prepared a dataset of human lung adenocarcinoma wedge resection. The image data consists of 38 channels, each with a resolution of 39,843 x 29,227 pixels (12,949 x 9,499 microns) and 118.43 GB in size containing 534,677 segmented cells. The data is enriched with cell-type classification from a deep immune profiling of the tumor microenvironment. Analysis: We began the analysis with the whole tissue in the viewport and activated the DNA channel for a general overview of the tissue architecture. Addressing the feedback from Use Case 1, we equipped the lens with a set of four predefined channel combinations, designed to investigate the tumor's immune response from a high level to detail.</p><p>The first channel combination (Fig. <ref type="figure" target="#fig_7">9</ref> A) consisted of basic cell markers and was designed to assess overall tissue composition. Lung cells are marked by TTF-1, which enabled the detection of the aberrant tumor morphology in the upper region of the sample. The lens magnifier allowed the biologists to see that the tumor is heavily infiltrated with immune cells (marked by CD45 and Vimentin). We then switched to a second channel combination for immune cell markers (Fig. <ref type="figure" target="#fig_7">9 B</ref>) to further investigate the types of immune cells infiltrating the tumor. This revealed dense aggregates of immune cells, mainly composed of a core of B cells, surrounded by T cells, which are known tumor-associated tertiary lymphoid structures (TLS). Lens magnification allowed our experts to quickly detect, mark and further characterize the large number (10-12) of TLS's in this lung specimen. By switching to a third channel composition customized for TLS markers (Fig. <ref type="figure" target="#fig_7">9 C</ref>), we inspected each TLS more closely (see Sec. 7.1 for a closer description).</p><p>We used the Dotter's snapshot capability and placed landmarks as we reviewed the TLS's. We measured the TLS size using the ruler functionality (1,200 microns on average). We activated the cell type lens equipped with immune and cancer type classifications which showed B cells in the center (Fig. <ref type="figure">4</ref> F, Fig. <ref type="figure" target="#fig_7">9 C</ref>) surrounded by epithelial lung cells. The cell biologists also used this setting to check the dataset segmentation, which they rated as very accurate. While focusing on one of these TLS, we activated HistoSearch to find areas with similar marker patterns, successfully identifying the other TLS areas. Surprisingly, in some TLS the HistoSearch detected the structure's outer rim but not its center. Using the magnifier to zoom into these regions, we noted a distinctly higher level of B cell marker CD20. We focused the lens on this area and magnified for close-up inspection. To better understand the marker distribution in this region, we switched the lens function to the radial distribution chart (Fig. <ref type="figure">4 B</ref>), revealing a high Ki67 and PCNA concentration, markers for cell proliferation and growth.</p><p>Subsequently, we switched to the fourth channel group to compare lymphocyte markers across a TLS (Fig. <ref type="figure" target="#fig_7">9 D</ref>). We activated the histograms and moved the lens outside of a TLS towards the lung epithelial cells, recognizing TTF-1 positive tumor cells. Some of these were MHC-II positive. By shrinking the lens scope to single-cell size, we compared the TTF-1 cells, finding that cells with MHC-II expression are non-proliferative. This suggests that transient MHC-II expression is coupled with entry into a non-proliferative state. Feedback: While users were able to make meaningful observations throughout the evaluation, their commentary indicated two categories of improvements. Channel views and statistical views could be merged so users do not have to rely on rapid memory recall required for toggling filters. For example, simultaneous access to the single-cell marker intensity distribution would have been helpful to monitor non-selected markers for early exploration of broad immune cell lineages. Acknowledging that our tools do not support a wide range of unanticipated biological questions, the absence of tools for spatial analysis stalled certain leading lines of inquiry. Measuring the degree of cell proliferation around immune cell clusters would have been an important next step for our users, who recommended that we prioritize the introduction of supporting algorithms and visualizations for spatial correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">HANDS-ON USER STUDY AND QUESTIONNAIRE</head><p>To further evaluate the intuitiveness and usefulness of Scope2Screen's interactive interfaces and lens features, we conducted hands-on user studies in which three of our four domain collaborators (that were involved in goal specification, iterative testing and design, and usecases), two biologists and one pathologist, steered the tool, gave thinkaloud feedback, and additionally filled out a questionnaire. Study Setup. The sessions were conducted via Zoom with one subject matter expert (SME) at a time and two members from our visualization team. Scope2Screen was installed on a remote machine to which the experts had access. We used a "think aloud" approach <ref type="bibr" target="#b15">[18]</ref> as an opportunity for users to share feedback from their own interaction with the application. We recorded video and audio. The users worked sequentially through a list of Scope2Screen's features before freely exploring with the features practiced, following regions of biological interest. Sessions took between 70 and 90 minutes. They then filled out a questionnaire rating the usefulness of individual features with a 5 bin Likert scale (strongly agree to strongly disagree).</p><p>Study and Feedback. At first, the experts were asked to make use of global viewer features such as toggling image channels, panning &amp; zooming, and setting color ranges. Overall, they rated the application interface as intuitive and accessible (strongly agree, agree, and neutral). All agreed that focus+context improved exploration over a pure O+D approach and strongly agreed that the lens magnifier is helpful for observing local regions. Aligning with use-case feedback, the experts preferred the normal magnifier and found the plateau lens helpful to overcome distortion of the fish-eye but less intuitive. The pathologist preferred the circular shape for exploration and the rectangular lens for snapshots. All experts agreed that the snapshot capabilities were extremely helpful but in some situations, the overlay can occlude areas underneath, hence functionality to show or hide them is needed. The dotting panel and annotation capabilities were used intermittently during evaluation and were rated helpful (2x strongly agree, 1x agree). The selection and combination of distinct channels (Section 5.2.2) within the lens achieved the same rating. One expert mentioned that these were especially beneficial for checking biases in assigned cell types, and two experts suggested to provide means to store channel combinations and color settings of the lens for repetitive analysis tasks. Most liked of these features was the split-screen lens to relate, e.g, CyCIF to H&amp;E data, and to validate alignment (3x strongly agree). The featureaugmentation lenses (Section 5.2.3) were also rated to be helpful (1x agree, 2x strongly agree). The linear histograms were preferred over the radial chart as they were easier to comprehend, but one expert mentioned that the radial chart provided a good relative overview of the distribution and might lead to unexpected discoveries. Another comment proposed that showing numbers, in addition to the visual encoding, would be helpful. Using the cell type lens, one expert said that the tooltip, similar as provided for the histogram and radial chart, should supplement the local cell type counts with cell type counts from the whole tissue in order to aid comparison and give context. It should further be possible to hide cell types. Lastly, users agreed that similarity search results (Section 5.3) align with a visual similarity impression, with slightly more conservative feedback (2x agree, 1x neutral), mostly due to the nature of the sensitivity threshold, which requires repetitive fine-tuning depending on the underlying image area. This could be improved with parameter optimization. One expert commented that "what is considered similar" depends on the morphological or molecular questions in focus, and hence the lens could be equipped with additional similarity methods. Overall, the experts found the tool "easy to learn and use" (2x agree, 1x strongly agree).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION AND FUTURE WORK</head><p>We present a design study aimed at supporting single-cell research into the composition, molecular states, and phenotypes of normal and diseased tissues, a rapidly growing area of basic and translational biomedical research, as well as pathologists studying human tissues for the purpose of diagnosis and disease management. Our Scope2Screen system supports fluid interactivity based on familiar microscopy metaphors while enabling multivariate exploration of quantitative and image data using a lens. Throughout the design process and expert feedback, we identified three key areas in which current work could be most usefully extended. Combining Vision and Statistics: According to our experts, visual needs tools for presenting numerical data alongside image data. In many cases, generation of the numerical data is not the problem: computational biologists are familiar with scripting and statistical tools for deriving single-cell data from images (via segmentation) and linking it to external sources (e.g., genomic data) but the information is most useful alongside the original images. Pathologists in particular need to combine deep knowledge of tissue architecture with quantitative data. However, most visual tools do not offer sufficient flexibility, and scripts or notebooks (e.g., Jupyter) lack the interactive visual exploration. We intend to extend Scope2Screen to support scripted statistical queries integrated with lensing. High-dimensional Features on the Horizon: Recent development in digital imaging such as the ability to measure spatial distribution of RNA expression will result in data with thousands, not dozens, of dimensions. Mapping such data into the image space while extracting relevant information will require dimensional reduction techniques and suitable visual representations of found features so that only the most relevant or explanatory data are presented. Very high-resolution 3D microscopy of tissues is also being integrated with the high-plex 2D data described here and this will require appropriate visual metaphors for moving between resolutions and data modalities. Scalability across Datasets: Our use cases demonstrate uses for Scope2Screen in the analysis of a single dataset stored locally. However, digital histology is expected to transition to the analysis of multiple datasets accessed via the cloud. While Scope2Screen scales to a large set of images, it does not yet support analysis and annotation of image collections or work interactively with Docker-based image analysis pipelines. Adding this functionality will close the gap from data exploration and analysis to generation of machine-assisted interpretative data reports for research and clinical applications including interactive publication via tools such as Minerva <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b51">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">ACKNOWLEDGEMENTS</head><p>This work is supported by the Ludwig Center at Harvard Medical School, and by NIH/NCI grants NCI U2C-CA233262, NCI U2C-CA233280 and NCI U54-CA225088.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">OUTSIDE (COMPETING) INTERESTS</head><p>PKS is a member of the SAB or Board of Directors of Glencoe Software, Applied Biomath, and RareCyte Inc. and has equity in these companies; PKS is also a member of the SAB of NanoString. Sorger declares that none of these relationships have influenced the content of this manuscript. Other authors declare no competing interests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Our histological tissue image data consists of a multi-channel image stack, a segmentation mask, and extracted tabular marker intensity values (arithmetic mean) for each cell. The tabular data is linked via cell ID and X,Y position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) of consecutive tasks (T1-T6) leading from image exploration and close-up inspection of regions to annotation and extraction of patterns. T1. Explore Multimodal, Highplexed Image and Feature Data in Combined Setting: A pivotal task is rapid navigation and visualization of multi-channel images. Pathologists normally operate by moving</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig. 4: Top: Settings for channel analysis: (A) Single channel option, out of three in the context. (B) Multi-channel lens. (C) Split-screen lens enabling juxtaposed comparison of the same area with different multi-channel settings (here CyCIF-DNA and H&amp;E-RGB). Bottom: Feature augmentation: (D) Single-cell histograms for detailed vertical comparison of selected cell marker distribution (channel-based rendering); (E) Radial single-cell plot a for compact summary of cell marker distribution; (F) Segmentation, cell types and counts showing classification results.</figDesc><graphic coords="5,77.99,132.77,462.26,75.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 1 (C) shows the lens operating in this setting. To further analyze where cells lie in that spectrum, we chose to offer a brush functionality. The user can filter a range (min-max) in the histogram, which highlights cells in the lens matching the updated single-cell marker values. Radial Chart. Additionally to histograms, we provide an overview of all markers by arranging their mean values in a radial layout in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: HistoSearch allows to find regions similar to those covered by the lens, taking into account activated channels. Top: HistoSearch is applied at different scales to find mucosal regions. The search works in two settings, for the current viewport (computation time ≈ 1 second for Full HD) and for the whole image in the highest resolution. Bottom: The spatial histogram similarity search consists of four steps (Sec. 5.3 for details).</figDesc><graphic coords="6,46.31,49.37,511.46,90.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The rich snapshot and annotation process. (A) During close-up analysis, the user focuses on an ROI and takes a snapshot. (B) The snapshot is annotated with title and description. (C) The Dotter panel links snapshots to the image space (left). Lens-settings such as channel combination and colors are preserved. (D) Annotated regions can be reactivated as lenses to explore further or fine-tune.</figDesc><graphic coords="7,91.31,49.37,436.22,145.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Use Case 1. Rich snapshots capture ROIs and important insights: (A) Broad population; (B) Healthy tissue; (C) Immune cell rich; (D) Tumor budding ; (E) Tumor suppression; (F) H&amp;E -lymphocyte.</figDesc><graphic coords="8,56.27,49.49,225.50,119.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Use Case 2, Multi-channel lenses in 4 settings: (A) 'Basic Cell Typing' shows tissue composition -stromal, immune, and cancer cells. The dense structure is a result of tumor growth in the lung; (B) 'Immune Cell Typing' distinguishes between immune and non-immune cells for a broad overview of immune regions (orange); (C) 'Lymphocytes and TLS' combines CD-channels reveal distinct immune types, e.g., cytotoxic T cells attacking the cancer; (D) 'Lymphocyte Phenotyping' for finer distinction, showing proliferating B-cells for antibody production (in blue).</figDesc><graphic coords="9,116.99,49.37,384.86,108.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,77.63,144.53,465.62,200.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Jared Jessup contributed equally to this work. • Robert Krueger, Jared Jessup, Simon Warchol and Hanspeter Pfister are with the School of Engineering and Applied Sciences, Harvard University.</figDesc><table><row><cell>Contact: krueger@g.harvard.edu</cell></row><row><cell>• Giorgio Gaglia, Cecily C. Ritch, Shannon Coy, and Sandro Santagata are</cell></row></table><note>with Brigham and Women's Hospital, Harvard Medical School. • Robert Krueger, John Hoffer, Jeremy Muhlich, Jia-Ren Lin, Yu-An Chen, Sandro Santagata, and Peter K Sorger are with the Laboratory of Systems Pharmacology, Harvard Medical School. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lensing</surname></persName>
		</author>
		<ptr target="https://www.npmjs.com/package/lensing" />
		<imprint>
			<date type="published" when="2021-06-08">8/06/2021</date>
		</imprint>
	</monogr>
	<note>an npm package</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The OME-TIFF format -OME Data Model and File Formats 5</title>
		<ptr target="https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/,lastaccessed:3/31/2021" />
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">OpenSeadragon -An open-source, web-based viewer for high-resolution zoomable images</title>
		<ptr target="https://openseadragon.github.io" />
		<imprint>
			<biblScope unit="page" from="3" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sliding window histogram, skimage: image processing in python, v0.18.0 docs -scikit-image.org/docs/stable/auto examples/features detection/plot windowed histogram</title>
		<imprint>
			<date type="published" when="2021">31/2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://visiopharm.com/visiopharm-digital-image-analysis-software-features/tissuealign" />
		<title level="m">Visiopharm TissueAlign -high-quality alignment of serial sections</title>
				<imprint>
			<date type="published" when="2021">6/30/2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="https://visiopharm.com/visiopharm-digital-image-analysis-software-features/viewer" />
		<title level="m">Visiopharm Viewer -AI-driven Pathology software</title>
				<imprint>
			<date type="published" when="2021">6/30/2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Zarr -zarr 2.6.1 documentation</title>
		<ptr target="https://zarr.readthedocs.io/en/stable/,lastaccessed:3/31/2021" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramid methods in image processing</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RCA engineer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">OMERO: flexible, model-driven data management for experimental biology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Burel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Loynton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.1896</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pair Analytics: Capturing Reasoning Processes in Collaborative Visual Analytics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arias-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Kaastra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="DOI">10.1109/HICSS.2011.339</idno>
	</analytic>
	<monogr>
		<title level="m">44th Hawaii International Conference on System Sciences</title>
				<imprint>
			<date type="published" when="2011-01">2011. Jan. 2011</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bankhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Loughrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dombrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Mcart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcquaid</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-017-17204-5</idno>
		<title level="m">Open source software for digital pathology image analysis. Scientific Reports</title>
				<imprint>
			<date type="published" when="2017-12">Dec. 2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16878</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Toolglass and Magic Lenses: The See-Through Interface</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Bier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Derose</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic Non-Rigid Histological Image Registration Challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kybic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Khvostikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2020.2986331</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3042" to="3052" />
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Geneslaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miraflor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Werneck Krauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><surname>Busam</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-019-0508-1</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2019-08">Aug. 2019</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extending distortion viewing from 2D to 3D</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cowperthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fracchia</surname></persName>
		</author>
		<idno type="DOI">10.1109/38.595268</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating information visualizations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information visualization</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="19" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From Scope to Screen: The Evolution of Histology Education</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Swailes</surname></persName>
		</author>
		<idno>doi: 10.1007/ 978-3-030-47483-6 5</idno>
	</analytic>
	<monogr>
		<title level="m">Biomedical Visualisation</title>
				<editor>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Rea</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of overview+detail, zooming, and focus+context interfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
		<idno type="DOI">10.1145/1456650.1456652</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Screenit: Visual Analysis of Cellular Screens</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dinkla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Genest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reiling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Borowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2016.2598587</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="591" to="600" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ball*-tree: Efficient spatial indexing for constrained nearest-neighbor search in metric spaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dolatshah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Minaei-Bidgoli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00628</idno>
		<idno>arXiv: 1511.00628</idno>
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The FLOWLENS: A Focus-and-Context Visualization Approach for Exploration of Blood Flow in Cerebral Aneurysms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beuing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preim</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.243</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2183" to="2192" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vitessce -A Framework and Visual Integration Tool for Exploration of (spatial) single-cell experiment data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gehlenborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gold</surname></persName>
		</author>
		<idno type="DOI">10.31219/osf.io/wd2gu</idno>
	</analytic>
	<monogr>
		<title level="s">2021. OSF Preprints</title>
		<imprint>
			<date>August 12</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Highly multiplexed single-cell analysis of formalin-fixed, paraffin-embedded cancer tissue</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Sevinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordwell</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1300136110</idno>
	</analytic>
	<monogr>
		<title level="j">Publisher: National Academy of Sciences Section: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="11982" to="11987" />
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
	<note>Proceedings of the National Academy of Sciences</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Profiling of Mouse Splenic Architecture with CODEX Multiplexed Imaging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goltsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Samusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy-Darling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2018.07.010</idno>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="968" to="981" />
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fisheyes are good for large steering tasks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skopik</surname></persName>
		</author>
		<idno type="DOI">10.1145/642611.642648</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;03</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2003-04">Apr. 2003</date>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minerva: a light-weight, narrative image browser for multiplexed tissue images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Muhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruokonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.02579</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">54</biblScope>
			<biblScope unit="page">2579</biblScope>
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MoleView: An Attribute and Structure-Based Semantic Lens for Large Element-Based Plots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ersoy</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.223</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2600" to="2609" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HALO -image analysis platform for quantitative tissue analysis in digital pathology</title>
		<ptr target="https://indicalab.com/halo/,last-accessed:7/1/2021" />
	</analytic>
	<monogr>
		<title level="m">Indica Labs</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency-Assisted Navigation of Very Large Landscape Images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.231</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1737" to="1746" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">elastix: A Toolbox for Intensity-Based Medical Image Registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
		<idno>doi: 10. 1109/TMI.2009.2035616</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="205" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934547</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="227" to="237" />
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lekschas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kerpedjiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gehlenborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934555</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="611" to="621" />
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cyclic Immunofluorescence (CycIF), A Highly Multiplexed Method for Single-cell Imaging</title>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fallahi-Sichani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y.</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpch.14</idno>
		<idno>doi: 10.1002/cpch.14</idno>
		<ptr target="https://currentprotocols.onlinelibrary.wiley.com/doi/pdf/10.1002/cpch.14" />
	</analytic>
	<monogr>
		<title level="j">Current Protocols in Chemical Biology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="251" to="264" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Highly multiplexed immunofluorescence imaging of human tissues and tumors using t-CyCIF and conventional optical microscopes</title>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Izar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.31657</idno>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comparison framework and guideline of clustering methods for mass cytometry data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<idno>doi: 10.1186/ s13059-019-1917-7</idno>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">297</biblScope>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automating the design of graphical presentations of relational information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<idno type="DOI">10.1145/22949.22950</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="141" />
			<date type="published" when="1986-04">Apr. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image analysis and machine learning in digital pathology: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2016.06.037</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="170" to="175" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Manz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W H</forename><surname>Ii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Börner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Spraggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gehlenborg</surname></persName>
		</author>
		<author>
			<persName><surname>Viv</surname></persName>
		</author>
		<idno>type: article. doi: 10</idno>
		<title level="m">Multiscale Visualization of High-Resolution Multiplexed Bioimaging Data on the Web</title>
				<imprint>
			<date type="published" when="2020-08">Aug. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">OSF Preprints</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SimpleElastix: A User-Friendly, Multi-lingual Library for Medical Image Registration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marstal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2016.78</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="574" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Quick Annotator: an open-source digital pathology based rapid image annotation tool</title>
		<author>
			<persName><forename type="first">R</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Janowczyk</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Silverlight) Deep Zoom File Format Overview</title>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://docs.microsoft.com/en-us/previous-versions/windows/silverlight/dotnet-windows-silverlight/cc645077(v=vs.95" />
		<imprint>
			<date type="published" when="2021-10-31">3/31/2021, Oct. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Durant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bourbeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3773450</idno>
		<imprint>
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Managing Spatial Selections With Contextual Snapshots</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mindek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12406</idno>
		<idno>doi: 10.1111/cgf.12406</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12406" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Diagnostic Review with Digital Pathology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Molin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Chalmers Tekniska Hogskola</publisher>
			<pubPlace>Sweden</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stitching and registering highly multiplexed whole slide images of tissues and tumors using ashlar software</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Muhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Ashlar -alignment by simultaneous harmonization of layer/adjacency registration</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Muhlich</surname></persName>
		</author>
		<ptr target="https://github.com/labsyspharm/ashlar" />
		<imprint>
			<date type="published" when="2021-03-30">6/30/2021, Mar. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ParaGlyder: Probedriven Interactive Visual Analysis for Multiparametric Medical Imaging Data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mörth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Haldorsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Smit</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-61864-329</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Graphics</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><surname>Nirmal</surname></persName>
		</author>
		<ptr target="https://scimap-doc.readthedocs.io/en/latest/,lastaccessed" />
		<title level="m">Scimap-Single-Cell Image Analysis Package</title>
				<imprint>
			<biblScope unit="page" from="2021" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The future of pathology is digital</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Pallua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zelger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schirmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haybaeck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.prp.2020.153040</idno>
	</analytic>
	<monogr>
		<title level="j">Pathology -Research and Practice</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">153040</biblScope>
			<date type="published" when="2020-09">Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Median Filtering in Constant Time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Perreault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hebert</surname></persName>
		</author>
		<idno>doi: 10. 1109/TIP.2007.902329</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2389" to="2394" />
			<date type="published" when="2007-09">Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Integral histogram: a fast way to extract histograms in Cartesian spaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2005.188</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
				<imprint>
			<date type="published" when="2005-06">2005. June 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="829" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interpretative guides for interacting with tissue atlas and digital pathology data using the Minerva browser</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Muhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.03.27.001834</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020-03">Mar. 2020</date>
		</imprint>
	</monogr>
	<note>Publisher: Cold Spring Harbor Laboratory Section: New Results</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The Human Tumor Atlas Network: Charting Tumor Transitions across Space and Time at Single-Cell Resolution</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rozenblatt-Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Regev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oberdoerffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hupalowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Rood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ashenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cerami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Demir</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cell.2020.03.053</idno>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="249" />
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pancreatlas: Applying an Adaptable Framework to Map the Human Pancreas in Health and Disease</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kusmartseva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Cartailler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brissova</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patter.2020.100120</idno>
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">100120</biblScope>
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">histoCAT: analysis of cell phenotypes and interactions in multiplex image cytometry data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raghuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="873" to="876" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">MCMICRO: A scalable, modular image-processing pipeline for multiplexed tissue imaging</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Muhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Nariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruokonen</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.03.15.435473</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
			<date type="published" when="2021-03">Mar. 2021</date>
		</imprint>
	</monogr>
	<note>Publisher: Cold Spring Harbor Laboratory Section: New Results</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Design Study Methodology: Reflections from the Trenches and the Stacks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2012.213</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2431" to="2440" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Supporting multi-point interaction in visual workspaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shoemaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<idno>doi: 10.1145/ 1240624.1240777</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;07</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007-04">Apr. 2007</date>
			<biblScope unit="page" from="999" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Sofroniew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bokota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Solak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Buckley</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4470554</idno>
		<ptr target="na-pari/napari:0.4.4rc0" />
		<imprint>
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visual cohort comparison for spatial single-cell omics-data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Somarakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Ijsselsteijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kenkhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F C C</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030336</idno>
		<idno type="arXiv">arXiv:2006.05175</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="733" to="743" />
			<date type="published" when="2021-02">Feb. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ilastik: Interactive learning and segmentation toolkit</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Straehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI.2011.5872394</idno>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
				<imprint>
			<date type="published" when="2011-03">Mar. 2011</date>
			<biblScope unit="page" from="230" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">CytoMAP: A Spatial Analysis Toolbox Reveals Features of Myeloid Cell Organization in Lymphoid Tissues</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Stoltzfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Filipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Gern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Olin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Leal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.celrep.2020.107523</idno>
	</analytic>
	<monogr>
		<title level="j">Cell Reports</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">107523</biblScope>
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Orbit Image Analysis: An opensource whole slide image analysis tool</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stritt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Stalder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vezzali</surname></persName>
		</author>
		<idno>doi: 10. 1371/journal.pcbi.1007313</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">e1007313</biblScope>
			<date type="published" when="2020-02">Feb. 2020</date>
			<publisher>Publisher: Public Library of Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Publisher: Taylor &amp; Francis eprint</title>
		<author>
			<persName><forename type="first">M</forename><surname>Titford</surname></persName>
		</author>
		<idno type="DOI">10.1080/10520290500138372</idno>
		<idno>doi: 10.1080/ 10520290500138372</idno>
		<ptr target="https://doi.org/10.1080/10520290500138372" />
	</analytic>
	<monogr>
		<title level="m">Biotechnic &amp; Histochemistry</title>
				<imprint>
			<date type="published" when="2005-01">Jan. 2005</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
	<note>The long history of hematoxylin</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A Survey on Interactive Lenses in Visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gladisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<idno>doi: 10.2312/ EUROVISSTAR.20141172</idno>
	</analytic>
	<monogr>
		<title level="m">The Eurographics Association</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Interactive Lenses for Visualization: An Extended Survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gladisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dachselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12871</idno>
		<idno>doi: 10.1111/ cgf.12871</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12871" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="173" to="200" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">3D Generalization Lenses for Interactive Focus + Context Visualization of Virtual City Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Glander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Döllner</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV.2008.18</idno>
	</analytic>
	<monogr>
		<title level="m">12th International Conference Information Visualisation</title>
				<imprint>
			<date type="published" when="2008-07">2008. July 2008</date>
			<biblScope unit="page" from="356" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Smooth and efficient zooming and panning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Wijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nuij</surname></persName>
		</author>
		<idno>doi: 10. 1109/INFVIS.2003.1249004</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hierarchical Spatial Aggregation for Level-of-Detail Visualization of 3D Thematic Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Vollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Döllner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3234506</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Spatial Algorithms and Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2018-09">Sept. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The Cancer Genome Atlas Pan-Cancer analysis project</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R M</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Ozenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ellrott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shmulevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<idno type="DOI">10.1038/ng.2764</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1113" to="1120" />
			<date type="published" when="2013-10">Oct. 2013</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Prospects for telepathology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Weinstein</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0046-8177(86)80028-4</idno>
	</analytic>
	<monogr>
		<title level="j">Human Pathology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="434" />
			<date type="published" when="1986-05">May 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">TrailMap: facilitating information seeking in a multi-scale digital map via implicit bookmarking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wigdor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<idno>doi: 10. 1145/2470654.2481417</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>Paris France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-04">Apr. 2013</date>
			<biblScope unit="page" from="3009" to="3018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
