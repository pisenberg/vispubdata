<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Visual Explainable Active Learning for Zero-Shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shichao</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zeyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Nuo</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Towards Visual Explainable Active Learning for Zero-Shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">65C8398784A7908AA5AD5E8783E56C75</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active Learning</term>
					<term>Explainable Artificial Intelligence</term>
					<term>Human-AI Teaming</term>
					<term>Mixed-Initiative Visual Analytics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Semantic navigator is a mixed-initiative visual analytics system for zero-shot classification. (a) The machine asks contrastive questions to guide analysts to come up with new attributes. (b) The semantic map explains the machine's status and presents the label recommendations (striped contours). Analysts select partial classes as positive (solid red contours) or negative (solid blue contours) to adjust the label recommendations ((c) and (d)). (e) The line chart monitors the training accuracy for seen classes and testing accuracy for unseen classes. (f) The class-attribute matrix is built interactively via collaboration between analysts and the machine.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>• S. Jia, Z. Li, N. Chen and J. <ref type="bibr">Zhang</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>are with College of Intelligence and</head><p>Computing, Tianjin University. E-mail: {jsc se, lzytianda, nicole 0420, jwzhang}@tju.edu.cn. J. Zhang is the corresponding author. • J. <ref type="bibr">Zhang</ref>  Zero-shot classification <ref type="bibr" target="#b71">[72]</ref> is a dominant and promising learning paradigm, aiming to recognize instances that are unseen during training.</p><p>Compared with traditional supervised classification, which needs enormous labeled training data and can only classify instances covered by seen classes, zero-shot classification is more applicable and more like the human reasoning process. Humans can recognize unseen objects once we provide the description of objects. For example, we can teach children to recognize zebras by describing that zebras look like horses with black and white stripes. Zero-shot classification is based on this idea, and due to its practical value, it gains lots of attention in recent machine learning and computer vision studies <ref type="bibr" target="#b69">[70]</ref>.</p><p>One critical process to achieve zero-shot classification is building a class-attribute matrix (matrix with rows as classes and columns as attributes, see Fig. <ref type="figure" target="#fig_0">2</ref>). The attributes are human interpretable semantic words designed by experts, such as big eyes, black hair, etc. We can distinguish classes (including seen and unseen classes) by describing whether or how likely they have the attributes. Once the machine learns the concepts of attributes from seen classes, it can recognize objects from unseen classes leveraging the description using attributes.</p><p>However, constructing the class-attribute matrix is very challenging. Firstly, this process is hard to be automated with interpretability preserved. Designing attributes usually involves manually picking descriptive words for the images under consideration by domain specialists <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>. Yu et al. <ref type="bibr" target="#b73">[74]</ref> try to avoid human labor to build the class-attribute matrix automatically. However, the attributes designed by this approach have no concise names and may not be semantically interpreted. Secondly, the attributes used in the zero-shot classification are selective. Not all attributes are necessary and helpful for zero-shot classification, and some are even harmful and lead to wrong predictions <ref type="bibr" target="#b27">[28]</ref>. Thirdly, once the analyst 1 comes up with new attributes, he/she needs to go through the tedious and painful process to label every class. In summary, up to now, it can be tricky to design the classattribute matrix for zero-shot classification using solely machine-centric processes or solely human-centric processes.</p><p>One promising approach to the problem is teaming the machine and analysts with the machine guiding the attribute design process. Approaches to alleviating the labeling burden with the machine querying labels from humans, aka active learning <ref type="bibr" target="#b60">[61]</ref>, have been widely used in traditional supervised learning. While in this paper, instead of asking for labels of instances, the machine seeks attributes to differentiate classes. However, active learning merely recognizes humans as labeling machines so that analysts can not steer the process. It has shown that combining visual interactive labeling with active learning in a mixed-initiative way can accelerate the labeling process <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Meanwhile, explainability is a crucial factor in promoting human-AI teaming <ref type="bibr" target="#b44">[45]</ref> as humans need to understand the current state of the machine. Considering these, we propose a visual explainable active learning approach to zero-shot classification with four key actions (ask, explain, recommend and respond): machine asks questions to guide analysts to come up with new attributes; visualization explains the current state of the machine; machine recommends labels of attributes for each class; analysts provide feedback to the recommendations and steer the process.</p><p>Achieving the above human-AI teaming tasks needs to facilitate a shared mental model between the analyst and the machine <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b75">76]</ref>. Without understanding the state of the machine, the analyst will not trust the machine <ref type="bibr" target="#b11">[12]</ref> and can not know how to steer the process. Similarly, without inferring how the analyst reasons the task, the machine can not aid the design process. Specifically, current deep neural networks recognize objects utilizing enigmatic features that humans hardly understand. Contrastively, humans, describe classes leveraging high-level interpretable attributes, which is not the logic to distinguish objects for the machine. This semantic gap between the machine and the analyst seriously hinders the effectiveness of human-AI teaming.</p><p>To solve this technical challenge, we bridge the two spaces of features and attributes into one shared space facilitating mutual coordination. We then design a semantic map to visualize this space as the main interface for cooperative attribute design. This visualization enables analysts to reason the misclassification of the zero-shot model and inspire them to come up with new attributes. Based on this, we further design and implement a mixed-initiative visual analytics system called semantic navigator to facilitate other teaming tasks. The semantic navigator guides analysts with contrastive questions to elicit attributes by comparing two class exemplars. Contrastive questions require analysts to focus on the differences between classes, which is easier than the method to let humans come up with new attributes directly. Meanwhile, we leverage semisupervised support vector machines to provide label recommendations and enable analysts to interactively provide feedback to the recommendations.</p><p>In summary, our technical contributes are as follows:</p><p>• We propose a human-AI teaming approach called visual explainable active learning to elicit human knowledge and enable analysts to steer models at the same time.</p><p>• We design and implement a visual analytics system called semantic navigator that guides analysts to interactively build zero-shot classification models.</p><p>• We evaluate our technique with case studies and user studies. We find the semantic navigator can improve the efficiency of analysts and accuracy of zero-shot models compared with the method without guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we describe prior work relevant to our study, including methods of designing visual attributes (Sec. 2.1), interactive classification (Sec. 2.2) and guidance in visual analytics (Sec. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Designing Visual Attributes</head><p>Traditionally, visual attributes are designed by manually picking a set of words that are descriptive and distinguishable for the images <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>. Therefore, this process needs special expertise and great effort for domain experts, which is expensive for laypersons. To alleviate the burdens, Berg et al. <ref type="bibr" target="#b1">[2]</ref> propose an automatic approach to tag images with visual attributes by mining text and image data sampled from the Internet. However, the discovered attributes from text-mining can contain irrelevant semantics for the classification task at hand or may not be separable in the visual feature space. Considering this, Parikh et al. <ref type="bibr" target="#b51">[52]</ref> propose to build nameable and discriminative attributes with human-in-the-loop. Based on that, Duan et al. <ref type="bibr" target="#b15">[16]</ref> propose a method to discover localized attributes for fine-grained recognition. Yu et al. <ref type="bibr" target="#b73">[74]</ref> propose an automatic method to design discriminative category-level attributes. However, the learned attributes will not have concise semantics as the manually specified attributes. Moreover, for unseen classes, this approach needs humans to model the similarity between seen classes and unseen classes, which is more difficult and unintuitive than directly specifying visual attributes of unseen classes. Considering interpretability is inevitable in making human and machine communication and trust, we step back to think about improving human efficiency when designing visual attributes with human-in-the-loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interactive Classification</head><p>Interactive machine learning <ref type="bibr" target="#b20">[21]</ref>, or machine teaching <ref type="bibr" target="#b61">[62]</ref>, places a human-centered perspective on the building process of machine learning models. This paradigm leverages end-user involvement to enable rapid, focused, and incremental interaction cycles with various applications <ref type="bibr" target="#b34">[35]</ref>. Visualization as the interface between humans and machines plays a crucial role in seamlessly fitting analytics into existing interactive process <ref type="bibr" target="#b18">[19]</ref>. We mainly discuss methods of interactive building classification models. Labeling data instances is an important task for interactive classification. Active learning <ref type="bibr" target="#b60">[61]</ref> takes the machine-centric approach to ease the labeling burden by querying labels from users. In contrast, visual-interactive labeling <ref type="bibr" target="#b59">[60]</ref> takes the human-centric approach leveraging humans' ability to identify patterns with interactive visualization. Bernard et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> systematically compare the performance of the two paradigms and summarize different user strategies for visual interactive labeling. Heimerl et al. <ref type="bibr" target="#b29">[30]</ref> compare different combinations of active learning and visual interactive learning for document retrieval. Similarly, Höferlin et al. <ref type="bibr" target="#b30">[31]</ref> combine the two approaches for video visual analytics, and Sun et al. <ref type="bibr" target="#b63">[64]</ref> present Label-and-Learn system to help analysts understand the classifier's behavior. Jose Gustavo et al. <ref type="bibr" target="#b50">[51]</ref> propose an incremental visual classification method that enables users to steer the classification process by annotating items inside the similarity tree of data. Liu et al. <ref type="bibr" target="#b42">[43]</ref> propose a visual analytics method to improve the crowdsourced annotations while improving the classification model. Xiang et al. <ref type="bibr" target="#b72">[73]</ref> solve a similar problem with a different approach called DataDebugger. Felix et al. <ref type="bibr" target="#b22">[23]</ref> propose exploratory labeling to facilitate label ideation, specification, and refinement with machine-driven recommendations. Our research expands the research space considering attribute ideation guided by the machine and label recommendations with human feedback for zero-shot classification.</p><p>Sahoo et al. <ref type="bibr" target="#b57">[58]</ref> step first to study interactive zero-shot classification for diagnosing and steering its procedure. However, this process is under the assumption that the class-attribute matrix is provided. In contrast, our research focuses on a more fundamental problem that how to facilitate the constructing process of the class-attribute matrix. Analysts build classification models by describing classes using visual attributes via collaboration with the machine. Another advantage of our method is that the whole process is naturally interpretable when visual features extracted from the deep neural networks are involved. Our technique contributes to another way of interpretable machine learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>. Previous works either open the black boxes <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b68">69]</ref> which need analysts to have machine learning knowledge or explain the black boxes using surrogate models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref>, which adds another layer of uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Guidance</head><p>Guidance is a mixed-initiative process to assist analysts with a set of visual means to close the knowledge gap via the collaboration of the machine. Ceneda et al. <ref type="bibr" target="#b9">[10]</ref> present a systematic review of guidance approaches in visual analytics based on the dimensions of the knowledge generation process. We only discuss the most relevant methods that guide model construction and parameter refinement processes.</p><p>Dis-Function <ref type="bibr" target="#b6">[7]</ref> enables analysts to directly manipulate the data points to express the similarity by the proximity, and machines transform the interactions into the weights of distance functions. This idea has inspired a new interaction paradigm called semantic interaction <ref type="bibr" target="#b16">[17]</ref> to infer analysts' interests by manipulating visual items directly. For example, Endert et al. <ref type="bibr" target="#b17">[18]</ref> propose requireSPIRE to steer text models by spatially clustering data points. Moreover, Dowlinget et al. extend semantic interaction to a bidirectional pipeline to infer both the observation and attribute weights <ref type="bibr" target="#b14">[15]</ref>. Sacha et al. <ref type="bibr" target="#b56">[57]</ref> propose a guided visual analytics system called SOMFlow for exploratory cluster analysis using self-organizing maps. Specifically, SOMFlow provides a ranked list of recommendations and visual cues to guide analysts. <ref type="bibr">Cavallo et al.</ref> propose Clustrophile 2 <ref type="bibr" target="#b8">[9]</ref> for guided visual clustering analysis. This system guides analysts to explore the large clustering space and finetune the clustering parameters. Other techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> aim to provide suggestions to guide the visualization design or graphic design.</p><p>Our work focuses on guided supervised classification for zero-shot learning by construcing the class-attribute matrix, which has not been explored yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HUMAN-AI TEAMING FOR ZERO-SHOT CLASSIFICATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>This section briefly introduces the basic concepts of zero-shot classification and formally states the research problem, which is essential for subsequent discussion.</p><p>In zero-shot classification, the classes covered by training data are seen classes, denoted as C s = c s i |i = 1,...,N s . Contrastively, classes for unlabeled testing instances are unseen classes, denoted as C u = c u i |i = 1, ..., N u . The seen classes and unseen classes are disjoint, namely C s ∩ C u = / 0. Both training and testing data come from a D dimensional real space denoted as X ∈ R D , namely feature space. We denote D tr = (x tr i , y tr i ) ∈ X ×C s as the training dataset, in which x tr i is the labeled instance in the feature space, and y tr i is the corresponding class label in the seen classes. Similarly, we denote D te = x te i ∈ X as the testing dataset, in which x te i is the unlabeled instance in the feature space. Given the training dataset D tr , zero-shot classification aims to learn a classifier f (•) : X → C u that predicts the labels of testing dataset D te .</p><p>As there are no instances of unseen classes during training, the machine needs some auxiliary information to transfer knowledge from the seen classes to unseen classes. The most widely used auxiliary information are attributes. Attributes are human understandable words to describe objects with meaningful characteristics. These attributes construct a space namely semantic space (S ∈ R M (M &lt; D)), with each dimension being one attribute. Attributes can be binary (0/1) to symbolize their presence, or continuous to represent the degree of confidence. In this paper, we consider binary attributes as the initial step of the research. In the semantic space, each class is represented as a semantic vector namely class prototype. For example, in Fig. <ref type="figure" target="#fig_0">2</ref>, there are four visual attributes to describe classes, including black, white, striped, and water. The zebra prototype is specified as [1, 1, 1, 0]. We denote the prototypes of seen classes as</p><formula xml:id="formula_0">P s = p s i |p s i ∈ S N s i=1</formula><p>, and those of unseen classes as</p><formula xml:id="formula_1">P u = p u i |p u i ∈ S N u i=1</formula><p>. D tr , P s , P u are vital to obtain a zero-shot classifier f (•). Aligning all the class prototypes vertically, we get the class-attribute matrix, denoted as S ∈ {0, 1} N×M (N = N s + N u ). Our paper aims to build the binary class-attribute matrix interactively and train a good function f (•) as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Explainable Active Learning</head><p>Analysts without systematic domain knowledge can not easily transform the implicit knowledge to the structural class-attribute matrix. Although humans make decisions using attributes every minute, this process is usually subconscious and difficult to be characterized. To tackle this issue, we propose visual explainable active learning for zero-shot classification with four actions (ask, explain, recommend, respond) to guide the design process. Ask. Considering dialogue is a natural way for human interactions, the machine can elicit the attributes by asking questions. A naive approach is to ask for attributes given pictures of specific categories directly. However, this approach is inefficient because direct questions are not well-defined and may elicit irrelevant attributes to the classification. Considering this, we adopt contrastive questions by letting analysts compare two representative images of different categories and discover attributes that differentiate the two classes. These questions are simple enough but powerful by leveraging humans' analogical reasoning <ref type="bibr" target="#b24">[25]</ref> to discover deeper structural characteristics. Notice that we do not constrain the forms of questions, as there are still limited studies on how different kinds of questions impact the elicitation process <ref type="bibr" target="#b64">[65]</ref>. Explain. During the training process, visualization can play a critical role in enabling analysts to interpret and diagnose results from the machine. One fundamental technical problem in this setting is that features from the machine and attributes elicited from the analysts lie in the two different spaces. The semantic gap between the two spaces  The mutual mental space is then constructed by mapping both the feature space and semantic space into one shared space. The zero-shot classifier learns a mapping (ψ) to predict the visual exemplars (centers of class clusters in the mutual mental space, represented as v i ) using the class prototypes (p 1 -p 5 ) specified by the analyst in the semantic space. The predicted exemplars are denoted as v i . In this paper, the feature space is in 2048-dimensional space while the mutual mental space is in 500-dimensional space. (b) The semantic navigator closes each interaction loop with four key actions (ask, explain, recommend and respond). (c) The semantic map visualizes data distributions as contours, and each pair of the visual exemplar and prototype linked by a gray line. It supports explanations of the machine's status, label recommendations, and user feedback by interactively correcting annotations to ease the burden of specifying attributes.</p><formula xml:id="formula_2">= ≈ •• • •• • bobcat deer next iteration ••• Feature Extraction •• • = ≈ Feature Space ••• ••• ••• ••• •••</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Flow</head><note type="other">Interaction Loop</note><p>hinders the effectiveness of human-AI teaming. It is vital to bridge the two spaces using visualization to enhance the explainability of the results. We solve this problem by mapping the feature space and semantic space into one shared space, and we name the new space as mutual mental space. To explain the zero-shot classification results in the mutual mental space, we leverage explainable case-based reasoning <ref type="bibr" target="#b38">[39]</ref>.</p><p>Numerous studies <ref type="bibr" target="#b47">[48]</ref> have demonstrated that case-based reasoning involving various forms of matching and prototyping is fundamental to effective decision-making strategies. In this paper, we explain the zero-shot classifier results by visualizing the relationships between prototypes and corresponding desirable exemplars (details in Sec. 4.3). Recommend and Respond. Meanwhile, eliciting a new attribute in every interaction loop is just one step towards building the class-attribute matrix. Labeling classes using the attributes is another burden. We enable the machine to recommend labels to fill the blanks in the classattribute matrix. Moreover, analysts can respond to recommendations by interactively modifying class labels through visualization to close the interaction loop. We call the above approach containing four actions (ask, explain, recommend, respond) as visual active learning for zero-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SEMANTIC NAVIGATOR 4.1 System Overview</head><p>We instantiate the visual explainable active learning for zero-shot classification as a visual analytic approach called semantic navigator. We first discuss how we construct the mutual mental space as this is the technical fundamentals for visualization. Then we provide the overview of visualization views and interaction loop. Finally, we cover the training and testing process of the underlying zero-shot classifier, as its running process closely relates to visualization and interaction. Mutual Mental Space. We use PCA to project these features into the mutual mental space. These features are usually extracted from a pre-trained deep neural network. PCA decorates the dimensions of features so that we can predict these dimensions independently rather than jointly. Besides, we apply PCA to gain computational benefits by reducing dimensionality, considering the need for real-time interaction. We denote the PCA projection matrix as</p><formula xml:id="formula_3">M PCA ∈ R d×D (2 &lt; d &lt; D).</formula><p>Notice that we choose the dimensionality d empirically in this paper, and the projected space using PCA is still in high dimensional space. For example, in Sec. 5, we set d = 500 when the feature space is in 2048-dimensional space. Besides, M PCA is computed over all the training data and not class-specific. In the mutual mental space, the data for each class usually form a cluster. We assume that each cluster center is our target semantic representation, namely visual exemplars.</p><p>Then we map the prototypes into the new space by predicting the visual exemplars from the class prototypes. Visualization Views. The semantic map (Fig. <ref type="figure" target="#fig_2">3(c</ref>)) is the critical visualization of the semantic navigator. It explains the status of the zero-shot model, visualizes the label recommendations, and enables feedback interaction from analysts. Each contour in the semantic map presents one class cluster. The interactive zero-shot classification targets to narrow the gap between each pair of the class prototype (yellow dot) and corresponding visual exemplar (black dot). The colors of the striped contours stand for the recommended labels of the current attribute. Besides, the semantic navigator contains three more views: a hint view (Fig. <ref type="figure">1</ref> Once the analyst comes up with a new attribute, he/she can interactively specify the positive or negative status of partial classes using lasso interaction. The machine recommends the rest classes to ease the labeling burden. After specifying a new attribute, the analyst can click the submit button. The semantic navigator will pop up a window to let the analyst specify unseen classes with the new attribute. After specification, the underlying model is trained on the new class-attribute matrix, and the visualization will be updated. A new iteration begins until the gaps are small enough, or human resources are exhausted.</p><p>The interaction loop challenges the current zero-shot classification models as they need to be computationally efficient to support realtime interaction. To this end, we choose one state-of-art model called EXEM <ref type="bibr" target="#b10">[11]</ref> as its runtime depends only on the number of training classes, while most other methods can not. Notice that this model is not a must for our approach. Any model that builds a shared space between the feature space and the semantic space with efficient computation can be applied. Training and Testing. Specifically, the EXEM model learns a transformation function ψ(•) from semantic space to the mutual mental space for each class i using d support vector regressors (SVR), such that ψ(p s i ) ≈ v s i , where the p s i (p s i ∈ P s ) is the seen class prototype, and v s i ∈ R d is the visual exemplar for class i. Each SVR predicts each dimension of the visual exemplars from their corresponding class prototypes. Denote the set of visual exemplars for seen classes as</p><formula xml:id="formula_4">V s = v s i |v s i ∈ R d</formula><p>. Now we can perform zero-shot classification using a nearest neighbor classifier, once we provide the prototypes of unseen classes p u i (p u i ∈ P u ) by specifying their attributes. That is, the classifier outputs the label of the closest exemplar for each novel data point x u , namely y = arg min i dist(M PCA x u , ψ(p u i )). In the following sections, we discuss how to close the loop with four actions (ask, explain, recommend, and respond) supported by the semantic navigator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Generation</head><p>One design principle for the hint view is that it should ask questions with upgrading difficulty as the number of attributes increases, as it fits humans' cognition. Otherwise, it will discourage the analyst from making progress. Therefore at the initial stage where there are no attributes, the hint view should ask the easiest question. Specifically, we calculate the pair-wise distances of visual exemplars in the mutual mental space and rank these pairs based on the semantic distances. Then we choose the most distant classes A and B in the embedding semantic space and ask the analyst what is the most visible attribute between classes A and B. When the class-attribute matrix is not empty, the semantic navigator first builds a confusion matrix after retraining the zero-shot classifier. Each cell of the confusion matrix is the percentage of misclassification for that class in the training data. We use the percentage rather than the number to eliminate the influence of imbalanced data. When there are some classes with identical prototypes, we need to come up with a new attribute to differentiate these classes. Therefore, the semantic navigator selects a pair of classes that the zero-shot classifier confuses most from these classes. Suppose the classifier confuses classes A and B most, then the semantic navigator asks analysts what is the most visible attribute between classes A and B. When all categories have different prototypes, the semantic navigator selects a pair of classes (A and B in Fig. <ref type="figure">4(a)</ref>) that the zero-shot classifier confuses most. We then refer to the existing class-attribute matrix to find which attributes differentiate classes A and B. Suppose there are three attributes a 1 , a 2 and a 3 . Only attribute a 1 differentiates classes A and B while attributes a 2 and a 3 of classes A and B are the same (see Fig. <ref type="figure">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)). The question is provided by asking what is the most visible attribute between classes</head><p>A and B except attribute a 1 . The hint view ranks the questions by the percentage of misclassifications in the class-attribute matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualization</head><p>Design Rationale. We visualize the mutual mental space by projecting the dataset M PCA x tr i ∪V s ∪ ψ(P s ) into a 2D plane with two reasons. Firstly, the cluster structure is important to inspire analysts to define attributes since classes with similar semantics will cluster closely. Therefore, adjacent classes usually have the same attributes. Secondly, the dimension reduction can reveal the alignment of the class prototypes and the exemplars. As discussed in Sec. 4.1, the EXEM model learns a mapping function from the class prototypes to exemplars. Therefore, if class prototypes align well with the exemplars, the designed attributes have a good description of classes. We choose the t-SNE <ref type="bibr" target="#b13">[14]</ref> as the basic algorithm, as this is a popular nonlinear visualization technique to reveal the cluster structure.</p><p>The pipeline of constructing the semantic map can be viewed in Fig. <ref type="figure" target="#fig_5">6</ref>. The semantic map abstracts the scatterplot as class-level contours and then projects the prototypes every time the analyst appends a new attribute. Finally, the semantic map enhances the visualization to support case-based reasoning and interactive feedback. Visual Abstraction. The semantic map abstracts the data distribution of each class as a contour by thresholding its density. Class exemplars lie inside the contours with their class labels annotated (Fig. <ref type="figure" target="#fig_5">6</ref>). This design overcomes the overplotting problem and cognition load if all data are plotted, as we focus on the class-level semantics rather than observationlevel semantics. Besides, we do not use colors to distinguish different classes for scalability issue, because zero-shot learning usually needs many classes to learn the transferable semantics. For example, the AWA2 dataset <ref type="bibr" target="#b71">[72]</ref>, a benchmark with minimum classes for zero-shot learning, already has forty classes for training and ten classes for testing. Reprojection. Every time the analyst appends a new attribute, the semantic map needs to update the prototypes' coordinates and anchor other data points, since only prototypes change during this process, and maintaining cognitive continuity is essential. One possible method is training a surrogate model such as a neural network to map from the PCA projection to the 2D t-SNE projection <ref type="bibr" target="#b65">[66]</ref>. Then we can only update the positions of class prototypes in every iteration. However, we do not adopt this method because learning a good approximation needs to design a good neural network architecture by empirically choosing different hyperparameters such as the number of layers and neurons. Instead we adopt another strategy by fixing the coordinates of M PCA x tr i ∪V s and optimizing t-SNE loss function with stochastic gradient descent algorithm to update the positions of class prototypes ψ(P s ) in every iteration. This method enables faithful projection and maintains cognitive continuity with fast computation. Visual Enhancement. The semantic map links the class exemplar and its corresponding prototype. This simple design enables case-based reasoning using exemplars and prototypes by comparing their relationships (details in Sec. 4.4). As exemplars and prototypes do not correspond to specific images, we enable the analyst to check their nearest neighbors to estimate their semantics. By examining the nearest images around class exemplars and prototypes, analysts can understand their semantic difference and leverage their inference ability to develop a new attribute hypothesis. Besides, the prototypes will travel around the semantic map and leave different trajectories during the design process. As we will show in Sec. 5.1, the trajectories enable analysts to track the conditions of the model and understand how different attributes influence the model. For example, some prototypes will quickly converge to the visual exemplars after several iterations, which means the designed attributes describe these classes well. However, some prototypes will jump around two or three exemplars before the final convergence, which may infer that these prototypes confuse these classes easily. Therefore, we enable analysts to click class exemplars and show the trajectories of history positions. We name these trajectories as semantic trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Explanation with the Semantic Map</head><p>Now we present how semantic map supports explanations. As discussed above, each contour contains most of the data of that class. Therefore, if the class prototype comes inside the contour, it means the prototype has a high probability of having similar semantics with the exemplar inside the contour (see examples in Fig. <ref type="figure">1</ref>(a) for classes bobcat, fox, and wolf). We can sanity-check this assumption by examining the nearest neighbor images of exemplars and prototypes. This visual clue is the basic reasoning rule in our design.</p><p>When the visual exemplar comes outside the contour, we need to reason why the visual exemplar is not well aligned with its corresponding prototype. Reasoning this requires analysts to consider the context around this class and how class prototypes, contours, and exemplars interact with each other. Consider two classes A and B as the primitive case since it can be extended to cases involved multiple classes. Denote the contour of A and the class prototype of A as C(A) and P(A), respectively. The symbolism of B is the same. P(A) ∈ C(A) stands for the class prototype lies inside the contour. We first discuss the situation where contours A and B do not intersect. In terms of whether P(A) = P(B) and class prototypes inside the contours or not, there are five different visual patterns except the symmetrical cases. The above five patterns consider the conditions in which C(A) and C(B) do not overlap. However, when C(A) and C(B) mostly intersect, the situation becomes complex. Since most data points of classes A and B mix together in the feature space, it will be difficult to distinguish the two classes even though the attributes are distinguishable. In this situation, it will be better to improve the features first. Now we discuss how the machine generates contrastive questions and label recommendations, and how analysts provide feedback to the recommendations through the interaction with the semantic map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Label Recommendation</head><p>Besides providing questions, the semantic navigator highlights the contours of selected classes and provides its attribute label recommendations by coloring class contours using red or blue striped textures. If this specification mismatches with the analyst's hypothesis, he/she can provide feedback to the machine with interactions discussed in Sec. 4.6. Then the machine adjusts the hypothesis correspondingly.</p><p>There are many hypotheses given the constrain that some classes are positive and some negative. The machine should provide the most distinguishable attribute hypothesis. Therefore, we leverage the semisupervised support vector machine (S 3 V M) <ref type="bibr" target="#b0">[1]</ref> to do this. Given the training set of labeled data and a working set of unlabeled data, S 3 V M searches for the best labeling of the working set that results in a support vector machine with maximum margin.</p><p>However, solving the S 3 V M problem is very challenging for its nonconvexity and computational cost <ref type="bibr" target="#b23">[24]</ref>. It is still an open problem to scale up S 3 V M for large-scale applications. Using the whole dataset as the training and working set is unrealistic because analysts need instant feedback when generating attribute hypotheses. Considering this, we build S 3 V M using class exemplars rather than the whole datasets as they have already summarized the cluster structure. We solve this problem using quasi-Newton schemes, the state-of-art method namely QN-S 3 V M <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>.</p><formula xml:id="formula_5">A B C E D</formula><p>For example, suppose the analyst adopts the hint that asks him/her to come up with a new attribute to distinguish classes A and B except for the attributes {a i } n i=1 (see one example in Fig. <ref type="figure">4 (c</ref>)). Note that {a i } n i=1 are the existing attributes that can distinguish classes A and B. Therefore, the question asks for a new attribute to distinguish the two classes except for {a i } n i=1 . Otherwise, the analyst may come up with a re- dundant attribute specified before. Then, we use QN-S 3 V M to find a hyperplane in the mutual mental space that results in the maximum margin with class A as positive and class B as negative. The hyperplane splits the mutual mental space into two sub-spaces. All the other classes lie on the same side as class A will be labeled as positive, and the other classes labeled as negative. The semantic navigator enables analysts to reverse the status of the two groups of classes (change positive classes as negative and vice versa) using this button . If this hyperplane happens to result in the attribute a i , we choose the hyperplane with the next maximum margin until there is no conflict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">User Feedback</head><p>Analyzing the visual patterns in the semantic map guided by questions, analysts can generate an attribute hypothesis and specify it by correcting label recommendations. Specifically, the analyst can click the class contours to choose whether they are positive or negative. We use red for positive and blue for negative. Since classes with similar semantic locate closely, they usually have the same attributes. Therefore, we enable analysts to select multiple classes by drawing lassoes. The analyst can modify the label recommendations by interactively specifying partial classes. And the machine predicts the labels of the left classes at the same time. This collaboration eases the labeling burden and improves the efficiency of humans.</p><p>After generating an attribute hypothesis, the analyst finetunes the specification in the matrix view, names the attribute hypothesis, and clicks the submit button , which triggers a popup window to let him/her choose images for unseen classes with the newly added attribute. These images are the nearest neighbors for each unseen class exemplar. We show the images to ease the recall of the semantics of unseen classes for the analyst. However, it is not required as there may be no images for unseen classes in an extreme situation. In this situation, the images are replaced by their names. Once committed, the underlying EXEM model will be retrained automatically. Meanwhile, the visualization will be updated as well. A new iteration begins until the stopping condition is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We conduct case studies and controlled user studies to evaluate our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Studies</head><p>We first demonstrate the effectiveness of the semantic navigator through case studies using the Animals with Attributes2 (AWA2) dataset <ref type="bibr" target="#b71">[72]</ref> 2 , a standard benchmark for zero-shot classification. AWA2 has 37322 images of 50 kinds of mammals, along with the features extracted from a pre-trained ResNet <ref type="bibr" target="#b28">[29]</ref>. We use the standard split of the dataset (40 classes for training and 10 for testing) <ref type="bibr" target="#b71">[72]</ref> for the study. Besides, AWA2 provides a class-attribute matrix with 85 binary attributes. This class-attribute matrix is originally collected in a psychological experiment <ref type="bibr" target="#b49">[50]</ref> on attribute-based object similarity from human subjects.</p><p>Suppose one data scientist Bob wants to build an animal classifier using the AWA2 dataset. He first explores the semantic map to get familiar with the animals. Then he finds that similar animals in the semantic map are close to each other, while different animals are far apart. Semantic navigator orients the modification process of attributes. Bob checks the hint view and preview a list of recommendations. The machine first asks him what is the most visible attribute between classes otter and horse (Fig. <ref type="figure">5</ref>). Bob quickly notices that the otter lives in the water, while the horse lives on the ground. Therefore, he decides to add water concept to the empty class-attribute matrix. The semantic navigator provides an attribute hypothesis shown in the semantic map (Fig. <ref type="figure" target="#fig_7">7(a)</ref>). The machine thinks this attribute distinguishes the otter and horse most. However, this attribute does not align with the water concept of what Bob thinks. Therefore, Bob decides to modify the attribute hypothesis. He first circles the bat, mole, hamster, and mouse, and labels these animals as negative (Fig. <ref type="figure" target="#fig_7">7(a)</ref>). Then the machine quickly gives feedback to adjust the attribute hypothesis. The updated attribute hypothesis is shown in Fig. <ref type="figure" target="#fig_7">7(b)</ref>. Bob notices that the semantic navigator omits the polar bear, as the polar bear usually jumps into the sea to catch fish. Therefore, Bob circles the polar bear and marks it as positive as well (Fig. <ref type="figure" target="#fig_7">7(b)</ref>). Then the machine provides feedback and recommends labeling the grizzly bear as positive (Fig. <ref type="figure" target="#fig_7">7(c)</ref>). It seems that the machine has a different opinion of the attribute hypothesis compared with that of Bob. However, Bob is quite sure that grizzly bear usually lives in the forest. Therefore, he modifies the hypothesis  and labels the grizzly bear as negative (Fig. <ref type="figure" target="#fig_7">7(c)</ref>). Now, the machine seems to agree with Bob and does not change the hypothesis anymore (Fig. <ref type="figure" target="#fig_7">7(d)</ref>). Bob examines the specification closely, then names the hypothesis as water and summit it. Then the semantic navigator pops up a window to ask Bob to specify the unseen classes by choosing images with the attribute water. There are ten kinds of unseen animals, including chimpanzee, giant panda, leopard, persian cat, pig, hippopotamus, humpback whale, raccoon, rat, and seal. This task is effortless, and Bob quickly selects the hippopotamus, humpback whale, and seal. Finally, Bob commits the task and retrains the underlying zero-shot model with the water attribute. The final result is shown in Fig. <ref type="figure" target="#fig_7">7(e)</ref>. Semantic navigator directs the design process of the class-attribute matrix step by step. In the following iterations, Bob is guided by the machine by answering the questions in the hint view. Questions and corresponding answers of the first fifteen attributes specified by Bob are shown in Fig. <ref type="figure" target="#fig_9">8</ref>. It seems that it is getting more difficult to answer the questions when the iteration increases. For example, the machine asks Bob what is the most visible attribute between dolphin and killer whale except the white, black, and patches attributes in the final iteration. Nevertheless, with the collaboration between the machine and Bob, semantic prototypes are getting closer to their corresponding class exemplars (Fig. <ref type="figure">1(a)</ref>). For example, in Fig. <ref type="figure">1(a)</ref>, the semantic trajectory of the sheep presents that the sheep prototype transforms from squirrel and buffalo to sheep and polar bear, and finally gets to the sheep. Most of the semantic prototypes align well with their corresponding class exemplars such as siamese cat, fox, and wolf. Guided by the semantic navigator, Bob creates a better class-attribute matrix with only 15 visual attributes without losing the interpretability. This new class-attribute results in comparable training accuracy (ours: 79.612%, baseline: 89.076%) and higher testing accuracy (ours: 70.966%, baseline: 57.079%) compared with the baseline (EXEM <ref type="bibr" target="#b10">[11]</ref>) with 85 attributes (see Fig. <ref type="figure">1(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">User Studies</head><p>We conduct user studies to evaluate the semantic navigator 3 using both AWA2 and a subset of CUB200-2011 <ref type="bibr" target="#b67">[68]</ref> 4 . CUB200-2011 is a fine-grained bird dataset containing 200 bird species and binary 312 attributes with 6033 images. These attributes are collected from MTurk workers <ref type="bibr" target="#b67">[68]</ref> guided by 28 attribute questions based on an online tool for bird species identification 5 . The standard split of this dataset contains 150 training categories and 50 testing categories. We select a subset of the CUB-200-2011 for evaluation by randomly selecting 40 training classes and ten testing classes from the original training classes and testing classes, respectively. We name the selected dataset as SUB-CUB200 for short. The features for both AWA2 and SUB-CUB200 are 2048-dimensional extracted from the last pooling layer of ResNet <ref type="bibr" target="#b28">[29]</ref>.</p><p>Twenty-six participants (denote as P1-P26) in a large university volunteer the studies, including nine undergraduates, thirteen masters, and four Ph.D. students. To compare with our system, we design a system without guidance. This system contains only two views, including a matrix view (Fig. <ref type="figure">1</ref> We divide the participants into two groups (group A and group B) randomly and evenly. Before all the experiments, participants should fill in a questionnaire to provide personal information. Both groups need to conduct two experiments without and with guidance. Participants in group A build one classifier for AWA2 without guidance, while the other one for SUB-CUB200 with guidance. In contrast, group B reverses the datasets. We require participants to accomplish classification tasks with two different datasets to mitigate the anchoring bias if the same dataset is conducted sequentially. Besides, we conduct the two experiments on separate days in case the participants are exhausted.</p><p>Before each experiment, we conduct a workshop with each group to explain some functionality of the system without guidance or the semantic navigator and how to build a zero-shot classification model by describing animals using attributes. We do not explain any details of the underlying zero-shot model as participants can complete the task without understanding technical details. They only need to describe animals objectively with attributes that are visually distinguishable. Participants can explore the system and ask any questions during this session. This session lasts about thirty minutes. After each experiment, participants need to fill in a questionnaire to provide some feedback.</p><p>During each experiment, every participant has to design 15 attributes at the completion of the task. It takes each participant about 30-45 minutes to complete the task in each experiment. The whole design process is screen recorded. Notice that we adjust the system's language to their corresponding native language since the foreign words of animals can influence their cognition. We also allow analysts to name the attributes in their native language. Moreover, we translate the attributes later on. After each experiment, the class-attribute matrixes are downloaded locally for further analysis. At the end of the user studies, we interview each participant to get qualitative feedback. We summarize the results and our findings in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Quantitative results of zero-shot classification</head><p>We compare the results from three settings by calculating the testing accuracy using several state-of-art zero-shot algorithms, including EXEM <ref type="bibr" target="#b10">[11]</ref>, ESZSL <ref type="bibr" target="#b55">[56]</ref>, LisGAN <ref type="bibr" target="#b39">[40]</ref>, VAE <ref type="bibr" target="#b58">[59]</ref>, and DEM <ref type="bibr" target="#b74">[75]</ref> (Fig. <ref type="figure">9</ref>). Three settings include the method without guidance, method with guidance using the semantic navigator, and method using random strategy. The random strategy simulates 20 persons by randomly selecting 15 attributes from the class-attribute matrix benchmark. Besides testing accuracy, we also evaluate the results based on the average time Testing Accuracy (%) Testing Accuracy (%)</p><note type="other">Without Guidance With Guidance Random Baseline AWA2</note><p>SUB-CUB200 Fig. <ref type="figure">9</ref>. The box plot shows the testing accuracy of three settings (without guidance, with guidance, and random baseline) for two datasets (AWA2 and SUB-CUB200), evaluated by five state-of-art zero-shot algorithms (EXEM <ref type="bibr" target="#b10">[11]</ref>, ESZSL <ref type="bibr" target="#b55">[56]</ref>, LisGAN <ref type="bibr" target="#b39">[40]</ref>, VAE <ref type="bibr" target="#b58">[59]</ref>, and DEM <ref type="bibr" target="#b74">[75]</ref>). to specify each attribute (total time divided by the number of attributes). The total time starts when the participant enters each experiment and ends after the retraining process and updating the semantic map with the final class-attribute matrix.</p><p>Average Time (min) As shown in Fig. <ref type="figure" target="#fig_12">11</ref>, participants can spend less time coming up with new attributes for both datasets with the semantic navigator. Participants can save about 14.42 seconds to create one attribute for AWA2 and 14 seconds for SUB-CUB200 on average. As for testing accuracy (Fig. <ref type="figure">9</ref>), different algorithms produce various results in our studies. All three settings result in lower accuracy in SUB-CUB200 than AWA2 because the task of distinguishing birds in SUB-CUB200 is more challenging than the task of AWA2. Besides, participants spend more time completing the task of SUB-CUB200 than that of AWA2 regardless of whether with guidance. Moreover, participants without guidance gain comparable performance as the random baseline. However, we also find noticeable performance gain once analysts apply the semantic navigator. On average, the semantic navigator can increase the test accuracy by 4.96% for the AWA2 and 4.13% for the SUB-CUB200. This difference can become notable when participants create over five attributes (Fig. <ref type="figure" target="#fig_11">10</ref>).</p><note type="other">Without Guidance With Guidance SUB-CUB200 AWA2</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Qualitative feedback</head><p>More than half of the participants without guidance mentioned that it is hard for them to complete the task. Participant P2 said: In the beginning, I think this task is effortless as what I have to do is just to find some words to describe animals. Everybody can do it. Nevertheless, I quickly found that it is not as easy as expected. Most time, when I specify a new attribute, the test accuracy does not change much and However, most participants (18 / 26) with guidance thought that the semantic navigator is easy to understand and use. P11 mentioned that the questions helped me a lot. You just to follow its guidance to answer the questions only. P23 commented that sometimes the questions are too hard for me. For example, what is the most visible attribute between the otter and the beaver? I know they are different, but I can not find an appropriate word to distinguish them. But the good thing is that you can skip the hard question and find one that you can answer. P15 liked the design of the semantic map, he said at the beginning I did not understand what the black dots mean. But when you explained it as the destinations as the normal map, I quickly understand what I need to do is to let the yellow dots reach the corresponding destinations. I like it as it provides an overview, and I can understand the current status of the model. Meanwhile, the semantic map can tell me where I should pay attention. One participant P17 acknowledged that the label recommendations are beneficial, he said: Whenever I fix the specification, the machine can recommend the rest. It impressed me. However, sometimes we disagreed with each other so that I need to specify the attributes with several interactions. It would be better if it could read my mind more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION, DISCUSSION AND FUTURE WORK</head><p>This work focuses on the fundamental problem of designing the classattribute matrix for zero-shot classification with the mixed-initiative approach. We propose visual explainable active learning with four actions (ask, explain, recommend, and respond) to promote human-AI teaming. Besides, we design and implement a visual analytics system called semantic navigator for interactive zero-shot classification. To justify our method, we conduct case studies and controlled user studies. Results show that the semantic navigator improves analysts' efficiency for building zero-shot classifiers compared with the method without guidance.</p><p>Although the visual explainable active learning approach targets zero-shot classification in this paper, its concept has generalizability. The machine asks, and human answers using attributes can be viewed as a form of human-machine communication. At the same time, the visualization creates a shared space to bridge the low-level feature space and the high-level semantic space, which may help build the shared mental model between the human and machine. Our work contributes a new perspective on how humans and AI interact and collaborate via visual analytics and may inspire researchers to promote better human-AI teamwork. Future work can go beyond animal classification to real-world problems, such as the human-AI teaming approach to medical diagnosis <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the future, we can further improve human-AI teaming with more effective human-AI communication and further explanations. Firstly, we start our work by considering only binary attributes. Future work can extend binary attributes to relative attributes <ref type="bibr" target="#b52">[53]</ref>. For example, the classifier can capture that animal A is furrier than animal B. Using relative attributes can foster more natural human-AI communication <ref type="bibr" target="#b53">[54]</ref> and more effective human feedback <ref type="bibr" target="#b54">[55]</ref>. Secondly, we quantify the performance of zero-shot classifiers based on the model accuracy. However, one open question is how to know the classifier learns the attributes accurately rather than other related concepts? Recent explainable artificial intelligence research has shifted attention to quantifying concept-level explanations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37]</ref>, which can complement our work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The basic framework of zero-shot classification. Leveraging the class-attribute matrix using visual attributes, zero-shot classifier ( f (•)) can recognize tiger and otter even though the training data does cover these unseen classes. The class-attribute matrix is central to the zeroshot classification. However, it is very challenging to design a suitable class-attribute matrix manually without guidance.</figDesc><graphic coords="2,47.27,123.65,211.05,57.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual explainable active learning for zero-shot classification: (a) The zero-shot classifier first leverages features usually extracted from a deep neural network (DNN) as input.The mutual mental space is then constructed by mapping both the feature space and semantic space into one shared space. The zero-shot classifier learns a mapping (ψ) to predict the visual exemplars (centers of class clusters in the mutual mental space, represented as v i ) using the class prototypes (p 1 -p 5 ) specified by the analyst in the semantic space. The predicted exemplars are denoted as v i . In this paper, the feature space is in 2048-dimensional space while the mutual mental space is in 500-dimensional space. (b) The semantic navigator closes each interaction loop with four key actions (ask, explain, recommend and respond). (c) The semantic map visualizes data distributions as contours, and each pair of the visual exemplar and prototype linked by a gray line. It supports explanations of the machine's status, label recommendations, and user feedback by interactively correcting annotations to ease the burden of specifying attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a)), a matrix view (Fig.1(f)), and a line chart (Fig.1(e)). The hint view presents a ranked list of questions to elicit attributes from the analyst. Visualizing the class-attribute matrix enables the analyst to scrutinize the externalized knowledge. The line chart presents the training and testing accuracy and enables monitoring status of the current zero-shot classifier. Interaction Loop. The workflow closes the loop with four actions (ask, explain, recommend, respond), seen in Fig.3(b). The machine asks contrastive questions (what is the most visible attribute between classes A and B except existing attributes) to guide analysts to come up with new attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. An example of generating questions and label recommendations. (a) A confusion matrix whose each cell is the percentage of misclassified instances in the training dataset. Classes A and B are most confused by the zero-shot classifier. (b) The class-attribute matrix of classes A-E with attributes a 1 -a 3 . Only attribute a 1 differentiates classes A and B. (c) The question generated in this situation is what is the most visible attribute between classes A and B except attribute a 1 .</figDesc><graphic coords="5,225.35,59.81,65.90,71.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. A conceptual pipeline of constructing the semantic map. (a) The semantic map abstracts the scatterplot as class-level contours. Meanwhile, it reprojects the prototypes every time the analyst adds a new attribute. (b) The semantic map enhances the visualization with exemplar-prototype links, nearest neighbors and semantic trajectories.</figDesc><graphic coords="5,401.03,53.81,81.62,73.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>P(A) / ∈ C(A), P(B) / ∈ C(B), P(A) / ∈ C(B), P(B) /∈ C(A) and P(A) = P(B) (or exchange A and B). Leveraging projection technique in Sec. 4.3, the same data points will be projected at the same place. When class prototypes collapse into one point, it means classes A and B have the same attributes. Since both P(A) and P(B) are not inside the corresponding contours, we can infer that there is still a large gap between the prototypes and exemplars. Facing this situation, analysts should compare classes A and B to find the most distinguishable attribute that differentiates them.A B Pattern 2. P(A) ∈ C(A), P(B) ∈ C(A) and P(A) = P(B) (or exchange A and B). This pattern shows that class prototype A aligns well with exemplar A. In contrast, prototype B does not align well with exemplar B. Since prototype A and B have the same attributes, analysts should adopt the same strategy as that of pattern 1 to figure out at least one attribute that differentiates A from B. Notice that pattern 1 and pattern 2 frequently happen at the early stage of the design process. When multiple classes are involved, these patterns will result in a star graph (see the subgraph circled by the dashed line in Fig. 7(e) as an example). A B Pattern 3. P(A) / ∈ C(A), P(B) / ∈ C(B), P(A) / ∈ C(B), P(B) / ∈ C(A) and P(A) = P(B) (or exchange A and B). Now analysts find at least one attribute that distinguishes class A and class B. However, the attributes do not describe both classes well. The reason for this pattern is the incompleteness of the attributes. To solve this problem, analysts should closely examine existing attributes and the corresponding exemplar images and check their missing attributes.A B Pattern 4. P(A) ∈ C(A), P(B) ∈ C(A) and P(A) = P(B) (or exchange A and B). The difference between this pattern and pattern 2 is that the two prototypes are different. However, prototype B is still similar to class A. It means the current attributes of class B are not well described. Therefore, analysts should further explore more attributes that distinguish B from other classes. B A Pattern 5. P(A) ∈ C(B) and P(B) ∈ C(A) (or exchange A and B). This pattern shows that the machine wrongly recognizes A as B, and B as A. The reason behind this pattern may come from the errors in the class-attribute matrix. Therefore, analysts should closely examine the attributes to see whether they describe the wrong things. Hopefully, this problem can be solved by debugging the class-attribute matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The design process of water attribute: (a) The analyst annotates hamster, bat, mole, and mouse as negative using a lasso to modify the attribute specification recommended by the machine. (b) The machine adjusts the attribute specification. The analyst circles the polar bear and labels it as positive. (c) The result is updated by the machine. The analyst labels the grizzly bear as negative. (d) The machine agrees with the analyst and does not change its decision. (e) The result after retraining the model using the water attribute.</figDesc><graphic coords="7,195.11,165.77,112.82,111.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>What the most visible attribute between classA and classB except the attr-set ? iterations Hint Question: What the most visible attribute between classA and classB except the attr-set ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. The hint questions asked by the machine during the design process of the class-attribute matrix, along with answers provided by the analyst in Sec. 5.1. Notice that in the second iteration, the analyst comes up with the big attribute by himself rather than inspired by the machine.</figDesc><graphic coords="8,77.03,76.25,477.86,91.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(f)) and a line chart (Fig. 1(e)) as the semantic navigator. It enables analysts to append a new column and modify cells to specify a new attribute. Whenever the analyst adds a new attribute, the underlying zero-shot model is trained online and the line chart for accuracy is updated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The line chart compares the testing accuracy of two datasets under three settings (tested by EXEM algorithm). Each band summarizes the distribution of all corresponding performance curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The box plot shows the average time to specify each attribute for each group of participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>even decreases. It makes me crazy as I have no idea what's wrong I have done. Participant P5 agrees with it and commented: At the end of the design process, I think my brain is stuck. I do not know what I have specified before. When I want to add a new attribute, I need to go back to check if I have specified it already. Participant P8 also mentioned: It is very boring to create the matrix. When I come to a new attribute, I have to specify the animals individually to check whether they have the attribute. During the design process, I need to pay much focus and specify the matrix carefully. It exhausted me.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, the analysts (or end-users) are data scientists and domain experts who want to build zero-shot classifiers by injecting their domain knowledge with limited machine learning knowledge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">AWA2 and its images, labels, attributes, and features are publicly available in https://cvml.ist.ac.at/AwA2/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The semantic navigator is publicly available at https://bit.ly/3y4yZQl. 4 CUB200-2011 and its images, labels, and attributes are publicly available in http://www.vision.caltech.edu/visipedia/CUB-200-2011.html 5 http://www.whatbird.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported by the National Key Research and Development Program of China under Grant No.2019YFC1521200.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised support vector machines</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information processing systems</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="663" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparing visual-interactive labeling with active learning: An experimental study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeppelzauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fellner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="298" to="308" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards user-centered active learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeppelzauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vial: a unified process for visual interactive labeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeppelzauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aigner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1189" to="1207" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vizassist: an interactive user assistant for visual data mining. The Visual Computer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bouali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guettala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venturini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1447" to="1463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dis-function: Learning distance functions interactively. visual analytics science and technology</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human-centered tools for coping with imperfect algorithms during medical decision-making</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clustrophile 2: Guided visual clustering analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cavallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A review of guidance approaches in visual data analysis: A multifocal perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ceneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gschwandtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="861" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting visual exemplars of unseen classes for zero-shot learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3476" to="3485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The state of the art in enhancing trust in machine learning models with the use of visualizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chatzimparmpas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jusufi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="713" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual analytics for explainable deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sirius: Dual, symmetric, interactive dimension reductions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wenskovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Fry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="182" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Discovering localized attributes for fine-grained recognition. computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3474" to="3481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic interaction for visual analytics: toward coupling cognition and computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="15" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic interaction for sensemaking: Inferring analytical reasoning for model steering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>North</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2879" to="2888" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The human is the loop: new directions for visual analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of intelligent information systems</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="435" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse quasi-newton optimization for semi-supervised support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Fabian Gieseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods (ICPRAM 2012)</title>
				<meeting>the 1st International Conference on Pattern Recognition Applications and Methods (ICPRAM 2012)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive machine learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fails</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Olsen</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Intelligent user interfaces</title>
				<meeting>the 8th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The exploratory labeling assistant: Mixed-initiative label curation with large document collections</title>
		<author>
			<persName><forename type="first">C</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 31st Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="153" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scalable semisupervised svm via triply stochastic gradients</title>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th International Joint Conference on Artificial Intelligence (IJCAI 2019)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning and transfer: A general role for analogical encoding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gentner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loewenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">393</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards automatic concept-based explanations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9273" to="9282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast and simple gradient-based optimization for semi-supervised support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gieseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Zero-shot learning with attribute selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6870" to="6877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual classifier training for text document retrieval</title>
		<author>
			<persName><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2839" to="2848" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactive learning of ad-hoc classifiers for video visual analytics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Netzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual analytics in deep learning: An interrogative survey for the next frontiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coordinating agents: Promoting shared situational awareness in collaborative sensemaking</title>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Müller-Birn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing surrogate decision trees of convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="156" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recent research advances on interactive machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="401" to="417" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Building shared mental models between humans and ai for effective collaboration. CHI&apos;19</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lasecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-05">May 2019. 2019</date>
			<pubPlace>Glasgow, Scotland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<title level="m">terpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2668" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence for breast cancer: A visual case-based reasoning approach</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Lamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guezennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bouaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Séroussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Leveraging the invariant side of generative zero-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7402" to="7411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An interactive method to improve crowdsourced annotations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual diagnosis of tree boosting methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding hidden memories of recurrent neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<imprint>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rulematrix: Visualizing and understanding classifiers with rules</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Protosteer: Steering deep sequence model with prototypes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="238" to="248" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Designscape: Design with interactive layout suggestions</title>
		<author>
			<persName><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual ACM conference on human factors in computing systems</title>
				<meeting>the 33rd annual ACM conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1221" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Default probability</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Osherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wilkie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="269" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An approach to supporting incremental visual data classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G S</forename><surname>Paiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Minghim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="17" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Interactively building a discriminative vocabulary of nameable attributes. computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1681" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Relative attributes for enhanced human-machine communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parkash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Attributes for classifier feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parkash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="354" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Somflow: Guided exploratory cluster analysis with self-organizing maps and analytic provenance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="130" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visually analyzing and steering zero shot learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization Conference (VIS)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="251" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8247" to="8255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">User-based active learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Data Mining Workshops</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Pelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghorashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verwey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Machine teaching: A new paradigm for building machine learning systems. arXiv: Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Label-and-learn: Visualizing the likelihood of machine learning classifier&apos;s success during data labeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Intelligent User Interfaces</title>
				<meeting>the 22nd International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning attributes from the crowdsourced relative labels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning a parametric embedding by preserving local structure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="384" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Seedb: Efficient data-driven visualization recommendations to support visual analytics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment International Conference on Very Large Data Bases</title>
				<meeting>the VLDB Endowment International Conference on Very Large Data Bases</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2182</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">The caltechucsd birds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">200-2011 dataset. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dqnviz: A visual analytics approach to understand deep q-networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="288" to="298" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Voyager: Exploratory analysis via faceted browsing of visualization recommendations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="649" to="658" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Interactive correction of mislabeled training data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE VIS Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Designing category-level attributes for discriminative visual recognition. computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">an ideal human&quot; expectations of ai teammates in human-ai teaming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mcneese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Musick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">CSCW3</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
