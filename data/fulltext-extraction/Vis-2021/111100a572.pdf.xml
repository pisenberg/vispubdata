<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Direct Volume Rendering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sebastian</forename><surname>Weiss</surname></persName>
							<email>sebastian13.weiss@tum.de</email>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Üdiger Westermann</surname></persName>
						</author>
						<title level="a" type="main">Differentiable Direct Volume Rendering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">695046B62D1695D24CC59DA220EBA328</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-We present a differentiable volume rendering solution that provides differentiability of all continuous parameters of the volume rendering process. This differentiable renderer is used to steer the parameters towards a setting with an optimal solution of a problem-specific objective function. We have tailored the approach to volume rendering by enforcing a constant memory footprint via analytic inversion of the blending functions. This makes it independent of the number of sampling steps through the volume and facilitates the consideration of small-scale changes. The approach forms the basis for automatic optimizations regarding external parameters of the rendering process and the volumetric density field itself. We demonstrate its use for automatic viewpoint selection using differentiable entropy as objective, and for optimizing a transfer function from rendered images of a given volume. Optimization of per-voxel densities is addressed in two different ways: First, we mimic inverse tomography and optimize a 3D density field from images using an absorption model. This simplification enables comparisons with algebraic reconstruction techniques and state-ofthe-art differentiable path tracers. Second, we introduce a novel approach for tomographic reconstruction from images using an emission-absorption model with post-shading via an arbitrary transfer function.</p><p>Index Terms-Differentiable rendering, Direct Volume Rendering, Automatic Differentiation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Differentiable direct volume rendering (DiffDVR) can serve as a basis for a multitude of automatic optimizations regarding external parameters of the rendering process such as the camera, the transfer function (TF), and the integration stepsize, as well as the volumetric scalar field itself. DiffDVR computes derivatives of the rendered pixel values with respect to these parameters and uses these derivatives to steer the parameters towards an optimal solution of a problem-specific objective (or loss) function. DiffDVR is in particular required when using neural network-based learning tasks, where derivatives need to be propagated seamlessly through the network for training end-to-end regarding the loss function. While a number of approaches have been proposed for differentiable surface rendering <ref type="bibr" target="#b19">[20]</ref>, approaches focusing on differentiable rendering in the context of volume visualization are rare. For surface rendering, one objective is on the optimization of scene parameters like material properties, lighting conditions, or even geometric shape, to achieve matchings of synthetic and real images in computer vision tasks. Others have used implicit surface representations encoded via volumetric signed distance functions to derive analytic gradients for image-based shape reconstruction tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>. These approaches assume opaque surfaces so that in each optimization iteration the gradient descent is with respect to the encoding of a single fragment per pixel. This is different from direct volume rendering applications, where the optimization needs to consider the contributions of many samples to a pixel color. This requires considering a large number of partial derivatives of pixel colors with respect to parameter or material changes, and to propagate them back into the volumetric field.</p><p>The differentiable rendering framework Mitsuba 2 <ref type="bibr" target="#b36">[37]</ref> also provides a solution for direct volume rendering through Monte Carlo path tracing. However, Mitsuba directly applies so-called reverse-mode differentiation, which requires all intermediate derivatives to be saved for backpropagation. Thus, the memory required in direct volume rendering applications quickly exceeds the available system memory. This limits the approach to small volumetric grids and a small number of volume interactions that cannot faithfully optimize for small-scale structures. A follow-up work <ref type="bibr" target="#b35">[36]</ref> addresses this issue but limits the differentiability to volume densities and colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contribution</head><p>This work presents a general solution for DiffDVR: differentiable Direct Volume Rendering using the emission-absorption model without multiple scattering. This requires analyzing approaches for automatic differentiation (AD) with respect to the specific requirements in direct volume rendering (DVR). So-called forward-mode approaches are efficient if the number of parameters is low, yet they become computationally too expensive with an increasing number of parameters, i.e., when optimizing for per-voxel densities in a volumetric field. The so-called reverse mode or adjoint mode records the operations and intermediate results in a graph structure. This structure is then traversed in reverse order during the backward pass that propagates the changes to the sample locations. However, this requires storing O(kn) intermediate results, where n is the number of pixels and k the number of sample locations, and reversing the order of operations.</p><p>We show that a-priori knowledge about the operations performed in DVR can be exploited to avoid recording the operations in reversemode AD. We propose a custom computation kernel that inverts the order of operations in turn and derives the gradients used by AD. We further present a method for recomputing intermediate results via an analytic inversion of the light accumulation along the view rays. By this, intermediate results do not need to be recorded and the memory consumption of reverse-mode AD becomes proportional to O(n).</p><p>As our second contribution, we discuss a number of use cases in which AD is applied in volume rendering applications (Fig. <ref type="figure" target="#fig_0">1</ref>). These use cases demonstrate the automatic optimization of external parameters of the rendering process, i.e., the camera and the TF. Here the 3D density field is not changed, but the optimization searches for the external parameters that-when used to render this field-yield an optimal solution of a problem-specific loss function. In addition, we cover problems where the optimization is with respect to the densities. I.e., the field values are optimized so that an image-based loss functionafter rendering the optimized field-yields an optimal solution. We consider inverse tomography by restricting the rendering process to an absorption-only model without a TF and optimize the densities using given images of the field. For this case, we compare our method against algebraic reconstruction techniques <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> and Mitsuba 2 <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Beyond that, and for the first time to our best knowledge, we show how to incorporate TFs and an emission-absorption model into tomographic reconstruction and deal with the resulting non-convex optimization problem.</p><p>DiffDVR is written in C++ and CUDA, and it provides seamless interoperability with PyTorch for a simple embedding into existing training environments with complex, potentially network-based loss functions. The code is made publicly available under a BSD license 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Differentiable Rendering A number of differentiable renderers have been introduced for estimating scene parameters or even geometry from reference images, for example, under the assumption of local illumination and smooth shading variations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, or via edge sampling to cope with discontinuities at visibility boundaries <ref type="bibr" target="#b23">[24]</ref>. Scattering parameters of homogeneous volumes have been estimated from observed scattering pattern <ref type="bibr" target="#b11">[12]</ref>. Recently, Nimier-David et al. proposed Mitsuba 2 <ref type="bibr" target="#b36">[37]</ref>, a fully-differentiable physically-based Monte-Carlo renderer. Mitsuba 2 also handles volumetric objects, yet it requires storing intermediate results during the ray sampling process at each sampling point. This quickly exceeds the available memory and makes the approach unfeasible for direct volume rendering applications. Later, the authors have shown how to avoid storing the intermediate results <ref type="bibr" target="#b35">[36]</ref>, by restricting the parameters that can be derived to, e.g., only shading and emission. However, these methods are tailored for path tracing with multiple scattering and rely on Monte-Carlo integration with delta tracking. This makes them prone to noise and leads to long 1 https://github.com/shamanDevel/DiffDVR computation times compared to classical DVR methods without scattering. Our method, in contrast, does not require storing intermediate results and can, thus, use large volumes with arbitrary many sampling steps without resorting to a restricted parameter set. Furthermore, it does not impose restrictions on the parameters of the volume rendering process that can be differentiated.</p><p>Parameter Optimization for Volume Visualization An interesting problem in volume visualization is the automatic optimization of visualization parameters like the viewpoint, the TF, or the sampling stepsize that is required to convey the relevant information in the most efficient way. This requires at first hand an image-based loss function that can be used to steer the optimizer toward an optimal parameter setting. To measure a viewpoint's quality from a rendered image, loss functions based on image entropy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref> or image similarity <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">57]</ref> have been used. For volume visualization, the relationships between image entropy and voxel significance <ref type="bibr" target="#b4">[5]</ref> as well as importance measures of specific features like isosurfaces <ref type="bibr" target="#b44">[45]</ref> have been considered. None of these methods, however, considers the rendering process in the optimization process. Instead, views are first generated from many viewpoints, e.g., by sampling via the Fibonacci sphere algorithm <ref type="bibr" target="#b27">[28]</ref>, and then the best view regarding the used loss function is determined. We envision that by considering the volume rendering process in the optimization, more accurate and faster reconstructions can be achieved.</p><p>Another challenging problem is the automatic selection of a "meaningful" TF for a given dataset, as the features to be displayed depend highly on the user expectation. Early works attempted to find a good TF using clusters in histograms of statistical image properties <ref type="bibr" target="#b14">[15]</ref> or fitting visibility histograms <ref type="bibr" target="#b7">[8]</ref>. Others have focused on guiding an explorative user interaction <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b58">59]</ref>, also by using neural networks <ref type="bibr" target="#b2">[3]</ref>. For optimizing a TF based on information measures, Ruiz et al. <ref type="bibr" target="#b42">[43]</ref> proposed to bin voxels of similar density and match their visibility distribution from multiple viewpoints with a target distribution defined by local volume features. For optimization, the authors employ a gradientbased method where the visibility derivatives for each density bin are approximated via local linearization.</p><p>Concerning the performance of direct volume rendering, it is crucial to determine the minimum number of data samples that are required to accurately represent the volume. In prior works, strategies for optimal sampling in screen-space have been proposed, for instance, based on perceptual models <ref type="bibr" target="#b3">[4]</ref>, image saliency maps <ref type="bibr" target="#b38">[39]</ref>, entropy-based measures <ref type="bibr" target="#b55">[56]</ref>, temporal history <ref type="bibr" target="#b28">[29]</ref>, or using neural networks <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. Other approaches adaptively change the sampling stepsize along the view rays to reduce the number of samples in regions that do not contribute much to the image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>. DiffDVR's capability to compute gradients with respect to the stepsize gives rise to a gradientbased adaptation using image-based loss functions instead of gradientfree optimizations or heuristics.</p><p>Neural Rendering As an alternative to classical rendering techniques that are adapted to make them differentiable, several works have proposed to replace the whole rendering process with a neural network. For a general overview of neural rendering approaches let us refer to the recent summary article by Tewari et al. <ref type="bibr" target="#b47">[48]</ref>. For example, RenderNet proposed by Nguyen-Phuoc et al. <ref type="bibr" target="#b33">[34]</ref> replaces the mesh rasterizer with a combination of convolutional and fully connected networks. In visualization, the works by Berger et al. <ref type="bibr" target="#b2">[3]</ref> and He et al. <ref type="bibr" target="#b15">[16]</ref> fall into the same line of research. The former trained a network on rendered images and parameters of the rendering process, and use the network to predict new renderings by using only the camera and TF parameters. The latter let a network learn the relationships between the input parameters of a simulation and the rendered output, and then used this network to skip the rendering process and create images just from given input parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>In the following, we review the fundamentals underlying DVR using an optical emission-absorption model <ref type="bibr" target="#b29">[30]</ref>. Then we briefly summarise the foundation of Automatic Differentiation (AD), a method to systematically compute derivatives for arbitrary code <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Direct Volume Rendering Integral</head><p>Let V : R 3 → [0, 1] be the scalar volume of densities and let r : R + → R 3 be an arc-length parameterized ray through the volume. Let τ : [0, 1] → R + 0 be the absorption and C : [0, 1] → R + 0 the self-emission due to a given density. Then, the light intensity reaching the eye is given by</p><formula xml:id="formula_0">L(a, b) = b a g(V (r(t)))e − t a τ(V (r(u)))du dt,<label>(1)</label></formula><p>were the exponential term is the transparency of the line segment from t = a, the eye, to b, the far plane, and</p><formula xml:id="formula_1">g(v) = τ(v)C(v) is the emission.</formula><p>The transparency is one if the medium between a and b does not absorb any light and approaches zero for complete absorption. We assume that the density volume is given at the vertices v i of a rectangular grid, and the density values are obtained via trilinear interpolation. The functions τ and C define the mapping from density to absorption and emission. We assume that both functions are discretized into R regularly spaced control points with linear interpolation in between. This is realized on the GPU as a 1D texture map T with hardware-supported linear interpolation.</p><p>For arbitrary mappings of the density to absorption and emission, the volume rendering integral in Equation 1 cannot be solved analytically. Instead, it is approximated by discretizing the ray into N segments over which the absorption α i and emission L i are assumed constant. We make use of the Beer-Lambert model</p><formula xml:id="formula_2">α i = 1 − exp(−Δtτ(d i )),</formula><p>where d i is the sampled volume density, to approximate a segment's transparency. This leads to a Riemann sum which can be computed in front-to-back order using iterative application of alpha-blending, i.e.,</p><formula xml:id="formula_3">L = L + (1 − α)L i , and α = α + (1 − α)α i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Differentiation</head><p>The evaluation of any program for a fixed input can be expressed as a computation graph, a directed acyclic graph where the nodes are the operations and the edges the intermediate values. Such a computation graph can be reformulated as a linear sequence of operations, also called a Wengert list <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b54">55]</ref>,</p><formula xml:id="formula_4">x x x 0 = const x x x 1 = f 1 (x x x 0 , w w w 1 ) x x x 2 = f 2 (x x x 1 , w w w 2 ) ... x x x out = f k (x x x k−1 , w w w k ) (2)</formula><p>where the w w w i 's ∈ R p are the external parameters of the operations of size p and the x x x i 's ∈ R n refer to the state of intermediate results after the i-th operation of size n. The output x x x out ∈ R m has size m. Note here that in DiffDVR, n and k are usually large, i.e., n is in the order of the number of pixels and k in the order of the number of sampling points along the view rays. The output x x x out is a scalar (m = 1), computed, for example, as the average per-pixel loss over the image. The goal is then to compute the derivatives dx x x out dw w w i . The basic idea is to split these derivatives into simpler terms for each operation using the chain rule. For example, assuming univariate functions and w 1 the only parameter of interest, the chain rule yields</p><formula xml:id="formula_5">x 3 = f 3 ( f 2 ( f 1 (w 1 ))) ⇒ x 3 = f 3 ( f 2 ( f 1 (x))) f 2 ( f 1 (x)) f 1 (x). (3)</formula><p>There are two fundamentally different approaches to automatically evaluate the chain rule, which depend on the order of evaluations. If the product in the above example is evaluated left-to-right, the derivatives are propagated from bottom to top in Equation <ref type="formula">2</ref>. This gives rise to the adjoint-or backward-mode differentiation (see Sect. 4.3). If the product is evaluated right-to-left, the derivatives "ripple downward" from top to bottom in Equation <ref type="formula">2</ref>. This corresponds to the so-called forward-mode differentiation (see Sect. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AD FOR DIRECT VOLUME RENDERING</head><p>Now we introduce the principal procedure when using AD for DiffDVR and hint at the task-dependent differences when applied for viewpoint optimization (Sect. 5.1), TF reconstruction (Sect. 5.2) and volume reconstruction (Sect. 5.3 and Sect. 5.4). We further discuss computational aspects and memory requirements of AD in volume rendering applications and introduce the specific modifications to make DiffDVR feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Direct Volume Rendering Algorithm</head><p>In direct volume rendering, the pixel color represents the accumulated attenuated emissions at the sampling points along the view rays. In the model of the Wengert list (see Equation <ref type="formula">2</ref>), a function f i is computed for each sample. Hence, the number of operations k is proportional to the overall number of samples along the rays. The intermediate results x x x i are rgbα images of the rendered object up to the i-th sample, i.e., x x x i is of size n = W * H * 4, where W and H, respectively, are the width and height of the screen. The last operation f k in the optimization process is the evaluation of a scalar-valued loss function. Thus, the size of the output variable is m = 1. The parameters w w w i depend on the use case. For instance, in viewpoint optimization, the optimization is for the longitude and latitude of the camera position, i.e., p = 2. When reconstructing a TF, the optimization is for the R rgbα entries of the TF, i.e., p = 4R.</p><p>The DVR algorithm with interpolation, TF mapping, and front-toback blending is shown in Algorithm 1. For clarity, the variables in the algorithm are named by their function, instead of using w w w i and x x x i as in the Wengert list (Equation <ref type="formula">2</ref>). In the Wengert list model, the step size Δt, the camera intrinsics cam, the TF T , and the volume density V are the parameters w w w i . The other intermediate variables are represented by the states x x x i . Each function operates on a single ray but is executed in parallel over all pixels.</p><p>Algorithm 1 Direct Volume Rendering Algorithm Parameters: stepsize Δt, camera cam, TF T , volume V Input: uv the pixel positions where to shoot the rays 1: color i = 0 initial foreground color 2: x o , ω = f camera (uv, cam) start x o and direction ω for all rays 3: for i = 0,...,N − 1 do 4:</p><formula xml:id="formula_6">x i = x o + iΔtω</formula><p>current position along the ray 5:</p><formula xml:id="formula_7">d i = f interpolate (x i ,V ) Trilinear interpolation 6: c i = f TF (d i , T ) TF evaluation 7:</formula><p>color i+1 = f blend (color i , c i ) blending of the sample 8: end for 9: x x x out = f loss (color N )</p><p>Loss function on the output rgbα image When Algorithm 1 is executed, the operations form the computational graph. AD considers this graph to compute the derivatives of x x x out with respect to the parameters Δt, cam, T, and V , so that the changes that should be applied to the parameters to optimize the loss function can be computed automatically. Our implementation allows for computing derivatives with respect to all parameters, yet due to space limitations, we restrict the discussion to the computation of derivatives of x x x out with respect to the camera cam, the TF T and the volume densities V . In the following, we discuss the concrete implementations of forward and adjoint differentiation to compute these derivatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Forward Differentiation</head><p>On the elementary level, the functions in Algorithm 1 can be expressed as a sequence of scalar arithmetic operations like c = f (a, b) = a * b. In forward-mode differentiation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>, every variable is replaced by the associated forward variable</p><formula xml:id="formula_8">ã = a, da dw , b = b, db dw ,<label>(4)</label></formula><p>i.e., tuples of the original value and the derivative with respect to the parameter w that is optimized.</p><formula xml:id="formula_9">Each function c = f (a, b) is replaced by the respective forward function c = f ( ã, b) = f (a, b), ∂ f ∂ a da dw + ∂ f ∂ b db dw . (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>Constant variables are initialized with zero, xconst = x const , 0 , and parameters for which to trace the derivatives are initialized with one, w = w, 1 . If derivatives for multiple parameters should be computed, the tuple of forward variables is extended. Forward differentiation uses a custom templated datatype for the forward variable and operator overloading. Each variable is wrapped in an instance of this datatype, called fvar, which stores the derivatives with respect to up to p parameters along with their current values. The user has to write the functions in such a way that arbitrary input types are possible, i.e., regular floats or instances of fvar, via C++ templates. All intermediate variables are declared with type auto. This allows the compiler to use normal arithmetic if no gradients are propagated, but when forward variables with gradients are passed as input, the corresponding operator overloads are chosen.</p><p>As an example (see Fig. <ref type="figure">2</ref> for a schematics), let us assume that derivatives should be computed with respect to a single entry in a 1D texture-based TF, e.g., the red channel of the first texel T 0,red . When loading the TF from memory, T 0,red is replaced by T0,red = T 0,red , 1 , i.e., it is wrapped in an instance of fvar with the derivative for that parameter set to 1. Algorithm 1 executes in the normal way until T 0,red is encountered in the code for the TF lookup. Now, the operator overloading mechanism selects the forward function instead of the normal non-differentiated function. The result is not a regular color c i , but the forward variable of the color ci . All following functions (i.e., the blend and loss function) continue to propagate the derivatives. In contrast, if derivatives should be computed with respect to the camera, already the first operation requires tracing the derivatives with fvar.</p><p>It is worth noting that in the above example only the derivative of one single texel in the TF is computed. This process needs to be repeated for each texel, respectively each color component of each texel, by extending the array fvar::derivatives to store the required number of p parameters. Notably, for input data that is high dimensional, like TFs or a 3D volumetric field, forward differentiation becomes unfeasible. For viewpoint selection, on the other hand, where only two parameters are optimized, forward differentiation can be performed efficiently.</p><p>The computational complexity of the forward method scales linearly with the number of parameters p, as they have to be propagated through every operation. However, as every forward variable directly stores the derivative of that variable w.r.t. the parameters, gradients for an arbitrary number of outputs m can be directly realized. Furthermore, the memory requirement is proportional to O(np), as only the current state needs to be stored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adjoint Differentiation</head><p>Adjoint differentiation <ref type="bibr" target="#b30">[31]</ref>, also called the adjoint method, backward or reverse mode differentiation, or backpropagation, evaluates the chain rule in the inverse order than forward differentiation. For each variable x x x i , the associated adjoint variable</p><formula xml:id="formula_11">x x x i = ∂ x out ∂ x x x i , ŵ w w i = ∂ x out ∂ w w w i , (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>stores the derivative of the final output with respect to the current variable. Tracing the derivatives starts by setting xout = 1. Then, the adjoint variables are tracked backward through the algorithm, called the backward pass. This is equivalent to evaluating the chain rule Equation 3 from left to right, instead of right to left as in the forward method. Let c = f (a, b) be again our model function, then the adjoint variables â, b are computed from ĉ as</p><formula xml:id="formula_13">â = ∂ f ∂ a T ĉ, b = ∂ f ∂ b T ĉ. (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>This process is repeated from the last operation to the first operation, giving rise to the adjoint code. At the end, one arrives again at the derivatives with respect to the parameters ŵ w w = ∂ x out ∂ w w w . If a parameter is used multiple times, either along the ray or over multiple rays, the adjoint variables are summed up. The reverted evaluation of the DVR algorithm with the gradient propagation from Equation 7 is sketched in Algorithm 2. A schematic visualization is shown in Fig. <ref type="figure">3</ref>.</p><p>Because the adjoint method requires reversing the order of operation, simple operator overloading as in the forward method is no longer applicable. Common implementations of the adjoint method like Ten-sorFlow <ref type="bibr" target="#b0">[1]</ref> or PyTorch <ref type="bibr" target="#b39">[40]</ref> record the operations in a computation graph, which is then traversed backward in the backward pass. As it is too costly to record every single arithmetic operation, high-level </p><formula xml:id="formula_15">ĉolor i , ĉi += ∂ f blend (color i , c i ) T ĉolor N 4: di , T += ∂ f TF (d i , T ) T ĉi 5: xi , V += ∂ f interpolate (x i ,V ) T di 6: xo += xi , Δt += iω T xi , ω += iΔt xi 7: end for 8: ĉam += ∂ f camera (uv, cam) T [x o ; ω] 9:</formula><p>ĉolor 0 is ignored Output: Δt, ĉam, T , V functions like the evaluation of a single layer in neural networks are treated as atomic, and only these are recorded. Within such a high-level function, the order of operations is known and the adjoint code using Equation 7 is manually derived and implemented. We follow the same idea and treat the rendering algorithm as one unit and manually derive the adjoint code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Inversion Trick</head><p>One of the major limitations of the adjoint method is its memory consumption because the input values for the gradient computations need to be stored. For example, the blending operation (line 7 in Algorithm 1) is defined as follows: Let α,C be the opacity and rgbemission at the current sample, i.e., the components of c i , and let α (i) ,C (i) be the accumulated opacity and emission up to the current sample, i.e., the components of color i in Algorithm 1. Then, the next opacity and emission is given by front-to-back blending</p><formula xml:id="formula_16">C (i+1) = C (i) + (1 − α (i) )C α (i+1) = α (i) + (1 − α (i) )α.<label>(8)</label></formula><p>In the following adjoint code with α(i+1) , Ĉ(i+1) as input it can be seen that the derivatives again require the input values.</p><formula xml:id="formula_17">α = (1 − α (i) ) α(i+1) , Ĉ = (C − α (i) ) Ĉ(i+1) , α(i) = (1 − α) α(i+1) −C • Ĉ(i+1) , Ĉ(i) = Ĉ(i+1) .<label>(9)</label></formula><p>Therefore, the algorithm is first executed in its non-adjoint form, and the intermediate colors are stored with the computation graph. This is called the forward pass. During the backward pass, when order of operations is reversed and the derivatives are propagated (the adjoint code), the intermediate values are reused. In DVR, intermediate values need to be stored at every step through the volume. Thus, the memory requirement scales linearly with the number of steps and quickly exceeds the available memory. To overcome this limitation, we propose a method that avoids storing the intermediate colors after each step and, thus, has a constant memory requirement. We exploit that the blending step is invertible (see Fig. <ref type="figure" target="#fig_5">4</ref>): If α (i+1) ,C (i+1) are given and the current sample is recomputed to obtain α and C, α (i) ,C (i) can be reconstructed as</p><formula xml:id="formula_18">α (i) = α − α (i+1) α − 1 C (i) = C (i+1) − (1 − α (i) )C.<label>(10)</label></formula><p>With Equation <ref type="formula" target="#formula_18">10</ref>and α &lt; 1, the adjoint pass can be computed with constant memory by re-evaluating the current sample c i and reconstructing color i instead of storing the intermediate results. Thus, only  the output color used in the loss function needs to be stored, while all intermediate values are recomputed on-the-fly. Note that α = 1 is not possible in practice, since it requires the absorption stored in the TF to be at infinity. In the implementation, and indicated by the circled + in Fig. <ref type="figure">3</ref>, the adjoint variables for the parameters are first accumulated per ray into local registers (camera, stepsize, volume densities) or shared memory (TF). Then, the variables are accumulated over all rays using global atomic functions. This happens once all rays have been traversed (camera, stepsize, transfer function) or on exit of the current cell (volume densities).</p><p>Because the adjoint variables carry only the derivatives of the output, but not of the parameters, the computational complexity is largely constant in the number of parameters. For example, in TF optimization (Sect. 5.2) only the derivative of the currently accessed texel is computed when accessed in the adjoint code of TF sampling. This is significantly different from the forward method, where the derivatives of all TF entries need to be propagated in every step. On the other hand, the adjoint method considers only a single scalar output in each backward pass, requiring multiple passes to support multi-component outputs. This analysis and the following example applications show that the forward method is preferable when optimizing for a low number of parameters like the camera position, while for applications such as TF optimization, which require the optimization of many parameters, the adjoint method has clear performance advantages.</p><p>DiffDVR is implemented as a custom CUDA operation in Py-Torch <ref type="bibr" target="#b39">[40]</ref>. The various components of the DVR algorithm, like the parameter to differentiate or the type of TF, are selected via C++ template parameters. This eliminates runtime conditionals in the computation kernel. To avoid pre-compiling all possible combinations, the requested configuration is compiled on demand via CUDA's JITcompiler NVRTC <ref type="bibr" target="#b37">[38]</ref> and cached between runs. This differs from, e.g., the Enoki library <ref type="bibr" target="#b17">[18]</ref> used by the Mitsuba renderer <ref type="bibr" target="#b36">[37]</ref>, which directly generates Parallel Thread Code (PTX) for translation into GPU binary code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATIONS</head><p>In the following, we apply both AD modes for best viewpoint selection, TF reconstruction, and volume reconstruction. The results are analyzed both qualitatively and quantitatively. Timings are performed on a system running Windows 10 and CUDA 11.1 with an Intel Xeon 8x@3.60Ghz CPU, 64GB RAM, and an NVIDIA RTX 2070.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Best Viewpoint Selection</head><p>We assume that the camera is placed on a sphere enclosing the volume and faces toward the object center. The camera is parameterized by longitude and latitude. AD is used to optimize the camera parameters to determine the viewpoint that maximized the selected cost function. As cost function, we adopt the differentiable opacity entropy proposed by Ji et al. <ref type="bibr" target="#b18">[19]</ref>. Let C ∈ R H×W ×4 be the output image. We employ array notation, i.e., <ref type="figure">C[x, y, c</ref>] indicates color channel c (red, green, blue, alpha) at pixel x, y. The entropy of a vector x x x ∈ R N is defined as</p><formula xml:id="formula_19">H(x x x) = 1 log 2 N N ∑ i=1 p i log 2 p i , p i = x x x i ∑ N j=1 x x x j . (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>Then the opacity entropy is defined as OE(C) = H(C[:, :, 3]), where C[:, :, 3] indicates the linearization of the alpha channel, and the color information is unused. In a first experiment, the best viewpoint is computed for a CT scan of a human tooth of resolution 256 × 256 × 161. Eight optimizations are started in parallel with initial views from viewpoints at a longitude of {45 • , 135 • , 225 • , 315 • } and a latitude of ±45 • . In all cases, 20 iterations using gradient descent are performed. The viewpoints selected by the optimizer are shown as paths over the sphere in Fig. <ref type="figure">5a</ref>. The values of the cost function over the course of optimization are given in Fig. <ref type="figure">5b</ref>. It can be seen that the eight optimization runs converge to three distinct local minima. The best run converges to approximately the same entropy as obtained when the best view from 256 uniformly sampled views over the enclosing sphere is taken. Fig. <ref type="figure" target="#fig_0">1a</ref> shows intermediate views and the view from the optimized viewpoint. Further results on other datasets, i.e, a jetstream simulation (256 3 ), the potential field of a C60 molecule (128 3 ), and a smoke plume (178 3 ), confirm the dependency of the optimization process on the initial view (see Fig. <ref type="figure">6</ref>).</p><p>Both the adjoint and the forward method compute exactly the same gradients, except for rounding errors. As seen in Fig. <ref type="figure">5c</ref>, a single forward/backward pass in the adjoint method requires about 9.5ms/14.6ms, respectively, giving a total of 24.1ms. For the forward method, we compare two alternatives. First, forward-immediate directly evaluates the forward variables during the forward pass in PyTorch and stores these variables for reuse in the backward pass. In forward-delayed, the evaluation of gradients is delayed until the backward pass, requiring to re-trace the volume. With 21.3ms, forward-immediate is slightly faster than the adjoint method, while forward-delayed is around 30% slower due to the re-trace operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transfer Function Reconstruction</head><p>Our second use case is TF reconstruction. Reference images of a volume are first rendered from multiple views using a target TF. Given reference 0.000 0.400 5.000 Fig. <ref type="figure">7</ref>. Effect of the smoothing prior (Equation <ref type="formula" target="#formula_21">12</ref>). A small value of λ leads to "jagged" TFs which can accurately predict small details like the teeth in blue but introduce low frequency color shifts resulting in low PSNR and SSIM <ref type="bibr" target="#b51">[52]</ref>. A large smoothing prior smooths out small details.</p><p>the same volume and an arbitrary initial TF, AD is then used to optimize the TF so that the rendered images match the references. The target TF comprises of 256 rgbα entries, the target TF with R entries is initialized with random Gaussian noise. The density volume is rendered to a 512 2 viewport from eight camera positions that are uniformly distributed around the volume (the view direction always pointing toward the volume's center).</p><p>Let T ∈ R R,4 be the TF with R entries containing the rgb color and absorption, and let x x x i be the N rendered image of resolution W × H, y y y i are the reference images. In our case, N = 8,W = H = 512 and R varies. We employ an L 1 loss on the images and a smoothing prior L prior on the TF, i.e.,</p><formula xml:id="formula_21">L total = L 1 (x x x) + λ L prior (T ), L 1 (x x x) = 1 NW H ∑ i,x,y |x x x ixy − y y y ixy | L prior (T ) = 1 4(R − 1) 4 ∑ c=1 R−1 ∑ r=1 (T c,r+1 − T c,r ) 2 . (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>The Adam optimizer <ref type="bibr" target="#b21">[22]</ref> is used with a learning rate of 0.8 for 200 epochs. The use of λ to control the strength of the smoothing prior is demonstrated in Fig. <ref type="figure">7</ref> for a human head CT scan as test dataset using R = 64. If λ is too small, the reconstructed TF contains high frequencies and introduces subtle color shifts over the whole image. If the smoothing prior is too large, small details are lost. We found that a value of λ around 0.4 leads to the best results, visually using the Learned Perceptual Image Patch Similarity metric (LPIPS) <ref type="bibr" target="#b57">[58]</ref>, and is thus used in our experiments. We chose the LPIPS metric as we found that it can accurately distinguish the perceptually best results when the peak-signal-to-noise ratio (PSNR) and the structural similarity index (SSIM) <ref type="bibr" target="#b51">[52]</ref> result in similar scores. The initialization of the reconstruction and the final result for a human head dataset are shown in Fig. <ref type="figure" target="#fig_0">1b</ref>.</p><p>Next, we analyze the impact of the TF resolution R on reconstruction quality and performance (see Fig. <ref type="figure">8</ref>). For TF reconstruction, the backward AD mode significantly outperforms the forward mode. Because of the large number of parameters, especially when increasing the resolution of the TF, the derivatives of many parameters have to be traced in every operation when using the forward AD mode. Furthermore, the forward variables may no longer fit into registers and overflow into global memory. This introduces a large memory overhead that leads to a performance decrease that is even worse than the expected linear decrease. A naïve implementation of the adjoint method that directly accumulates the gradients for the TF in global memory using atomics is over 100× slower than the non-adjoint forward pass (adjoint-immediate). This is because of the large number of memory accesses and write conflicts. Therefore, we employ delayed accumulation (adjoint-delayed). The gradients for the TF are first accumulated in shared memory. Then, after all threads have finished their assigned rays, the gradients are reduced in parallel and then accumulated into global memory using atomics. As seen in Fig. <ref type="figure">8</ref>, this is the fastest of the presented methods. The whole optimization for 200 epochs requires around 5 minutes including I/O. However, as only 48kB of shared memory are freely available per multiprocessor, the maximal resolution of the TF is 96 texels. If a higher resolution is required, adjoint-immediate must be employed. At smaller values of R the reconstruction quality is decreased (see Fig. <ref type="figure">8</ref>). We found that a resolution of R = 64 leads to the best compromise between reconstruction performance and computation time.</p><p>To evaluate the capabilities of TF reconstruction to generalize to new datasets with the same hyperparameters as described above, we run the optimization on two new datasets, a CT scan of a human thorax and a smoke plume, both of resolution 256 3 . As one can see in Fig. <ref type="figure">9</ref>, the renderings with the reconstructed TF closely match the reference, demonstrating stability of the optimization for other datasets.</p><p>We envision that TF optimization with respect to losses in screen space can be used to generate "good" TFs for a dataset for which no TF is available. While a lot of research has been conducted on measuring the image quality for viewpoint selection, quality metrics specialized for TFs are still an open question to the best of our knowledge. In future work, a first approach would be to take renderings of other datasets with a TF designed by experts and transform the "style" of that rendering to a new dataset via the style loss by Gatys et al. <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Density Reconstruction</head><p>In the following, we shed light on the use of DiffDVR for reconstructing a 3D density field from images of this field. For pure absorption models, the problem reduces to a linear optimization problem. This allows for comparisons with specialized methods, such as filtered backpropagation or algebraic reconstruction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>. We compare DiffDVR to the CUDA implementation of the SIRT algebraic reconstruction algo-  rithm <ref type="bibr" target="#b13">[14]</ref> provided by the ASTRA-toolbox <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>. Furthermore, we compare the results to those computed by Mitsuba 2 <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, a general differentiable path tracer. Density reconstruction uses 64 uniformly sampled views on a surrounding sphere. Each image is rendered at a resolution of 512 2 . The reconstructed volume has a resolution of 256 3 . ASTRA and Mitsuba are used with their default optimization settings. DiffDVR performs a stepsize of 0.2 voxels during reconstruction. The Adam optimizer with a batch size of 8 images and a learning rate of 0.3 is used. To speed up convergence, we start with a volume of resolution 32 3 and double the resolution in each dimension after 10 iterations. At the highest resolution, the optimization is performed for 50 iterations. The same L total loss function as for TF reconstruction (see Equation <ref type="formula" target="#formula_21">12</ref>) is used, except that the smoothing prior is computed on the reconstructed volume densities in 3D, with λ = 0.5.</p><p>Three experiments with datasets exhibiting different characteristics are carried out. The results are shown in Fig. <ref type="figure" target="#fig_8">10</ref>. As one can see, DiffDVR consistently outperforms algebraic reconstruction via ASTRA and density reconstruction via the Mitsuba framework. In particular, Mitsuba suffers from noise in the volume due to the use of stochastic path tracing. Only for the rendering of the thorax dataset, Mitsuba shows a slightly better SSIM score than DiffDVR. For the plume dataset, intermediate results of the optimization process until convergence are shown in Fig. <ref type="figure" target="#fig_0">1c</ref>.</p><p>Note that all compared algorithms serve different purposes. Algebraic reconstruction methods (ASTRA) are specialized for absorptiononly optical models and support only such models. Mitsuba is tailored to Monte Carlo path tracing with volumetric scattering, an inher-ently computational expensive task. DifffDVR is specialized for direct volume rendering with an emission-absorption model and a TF, yet emissions and a TF were disabled in the current experiments. These differences clearly reflect in the reconstruction times. For instance, for reconstructing the human skull dataset, ASTRA requires only 53 seconds, DiffDVR requires around 12 minutes, and Mitsuba runs for multiple hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Color Reconstruction</head><p>Next, we consider an optical emission-absorption model with a TF that maps densities to colors and opacities, as it is commonly used in DVR. To the best of our knowledge, we are the first to support such a model in tomographic reconstruction.</p><p>For TFs that are not a monotonic ramp, as the absorption-only case, density optimization becomes a non-convex problem. Therefore, the optimization can be guided into local minima by a poor initialization. We illustrate this problem in a simple 1D example. A single unknown density value d 1 of a 1D "voxel" -a line segment with two values d 0 = −1 and d 1 at the end points and linear interpolation in betweenshould be optimized. A single Gaussian function with zero mean and variance 0.5 is used as TF, and the ground truth value for d 1 is −1. For varying d 1 , Fig. <ref type="figure" target="#fig_2">12</ref> shows the L 2 -loss between the color obtained from d 1 and the ground truth, and the corresponding gradients. As can be seen, for initial values of d 1 &gt; 0.4 the gradient points away from the true solution. Thus, the optimization "gets stuck" at the other side of the Gaussian, never reaching the target density of −1. This issue worsens in 2D and 3D, as the optimizer needs to reconstruct a globally consistent density field considering many local constraints.  This failure case is also shown in Fig. <ref type="figure" target="#fig_9">11b</ref>, where the tooth dataset cannot be reconstructed faithfully due to the initialization with a poorly matching initial field.</p><p>To overcome this shortcoming, it is crucial to start the optimization with an initial guess that is not "too far" from the ground truth in the high-dimensional parameter space. We account for this by proposing the following optimization pipeline: First, a pre-shaded color volume of resolution 256 3 (Fig. <ref type="figure" target="#fig_9">11c</ref>) is reconstructed from images using the same multi-resolution optimization as in the case of an absorption-only model. The color volume stores the rgb-emission and scalar absorption per voxel, instead of a scalar density value that is mapped to color via a TF. By using this color volume, trapping into local minima with nonmonotonic TFs can be avoid. Intermediate results of the optimization process until convergence are shown in Fig. <ref type="figure" target="#fig_0">1d</ref> for the tooth dataset. Then, density values that match the reconstructed colors after applying the TF are estimated. For each voxel, 256 random values are sampled, converted to color via the TF, and the best match is chosen. To avoid inconsistencies between neighboring voxels, an additional loss term penalises differences to neighbors. Let (τ T ,C T ) be the target color from the color volume and d the sampled density with mapped color τ(d),C(d), then the cost function is</p><formula xml:id="formula_23">C (d) = ||C T −C(d)|| 2 2 + α log(1 + |τ T − τ(d)|) + β ∑ i∈N (d − d i ) 2 .</formula><p>(13) Here, α and β are weights, and N loops over the 6-neighborhood of the current voxel. The logarithm accounts for the vastly different scales of the absorption, similar to an inverse of the transparency integral Equation 1. In the example, we set α = 1/ max(τ T ) to normalize for the maximal absorption in the color volume, and β = 1. This process is repeated until the changes between subsequent iterations fall below a certain threshold, or a prescribed number of iterations have been performed.</p><p>Finally, the estimated density volume is used as initialization for the optimization of the density volume from the rendered images (Fig. <ref type="figure" target="#fig_9">11d</ref>). We employ the same loss L total as before with a smoothing prior of λ = 20. The total runtime for a 256 3 volume is roughly 50 minutes. Even though the proposed initialization overcomes to a certain extent the problem of non-convexity and yields reasonable results, Fig. <ref type="figure" target="#fig_9">11</ref> indicates that some fine details are lost and spurious noise remains. We attribute this to remaining ambiguities in the sampling of densities from colors that still lead to suboptimal minima in the reconstruction. This also shows in the slice view of Fig. <ref type="figure" target="#fig_9">11d</ref>, especially for the thorax dataset. Here, some areas that are fully transparent due to the TF are arbitrarily mapped to a density value of zero, while the reference has a density around 0.5 -between the peaks of the TF -in these areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we have introduced a framework for differentiable direct volume rendering (DiffDVR), and we have demonstrated its use in a number of different tasks related to data visualization. We have shown that differentiability of the direct volume rendering process with respect to the viewpoint position, the TF, and the volume densities is feasible, and can be performed at reasonable memory requirements and surprisingly good performance.</p><p>Our results indicate the potential of the proposed framework to automatically determine optimal parameter combinations regarding different loss functions. This makes DiffDVR in particular interesting in combination with neural networks. Such networks might be used as loss functions -providing blackboxes, which steer DiffDVR to an optimal output for training purposes, e.g., to synthesize volume-rendered imagery for transfer learning tasks. Furthermore, derivatives with respect to the volume from rendered images promise the application to scene representation networks trained in screen space instead of from points in object space. We see this as one of the most interesting future works, spawning future research towards the development of techniques that can convert large data to a compact representation --a code --that can be permanently stored and accessed by a network-based visualization. Besides neural networks, we imagine possible applications in the development of lossy compression algorithms, e.g. via wavelets, where the compression rate is not determined by losses in world space, but by the quality of rendered images. The question we will address in the future is how to generate such (visualization-)task-dependent codes that can be intertwined with differentiable renderers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A fully differentiable direct volume renderer is used for a) viewpoint optimization, b) transfer function optimization, and optimization of voxel properties using c) an absorption-only model and d) an emission-absorption model with rgbα transfer functions. a), c) and d) show intermediate results of the optimization process until convergence.</figDesc><graphic coords="1,140.92,215.45,59.38,79.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>template&lt;typename T, int p&gt; struct fvar { T value; T derivatives[p]; }; Next, operator overloads are provided for all common arithmetic operations and their gradients. For example, multiplication is implemented similar to: template&lt;typename T, int p&gt; fvar&lt;T, p&gt; operator*(fvar&lt;T, p&gt; a, fvar&lt;T, p&gt; b) { fvar&lt;T, P&gt; c; //to store c = a*b and derivatives c.value = a.value * b.value; for (int i=0; i&lt;p; ++i) { //partial derivatives c.derivative[i] = a.value*b.derivative[i] + b.value*a.derivative[i]; } return c; }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 2. Schematic representation of the forward method for TF reconstruction. Gradients are stored in the forward variables (blue), and parameter values are propagated simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Algorithm 2</head><label>32</label><figDesc>Fig. 3. Schematic representation of the adjoint method for density and TF reconstruction. Gradients in the adjoint variables (red) are propagated backward through the algorithm. A circled + indicates the summation of the gradients over all steps and rays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) To compute the current contribution c i , intermediate accumulated colors color i need to be stored for every step along the ray. (b) The inversion trick enables to reconstruct color i from color i+1 . Thus, only the final color used in the loss function needs to be stored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Best viewpoint selection using maximization of visual entropy. The tooth dataset (Fig. 1a) is rendered from different viewpoints on a surrounding sphere. (a) Color coding of loss values for viewpoints on the northern and southern hemispheres, with isocontours (black lines) of the loss and local gradients with respect to the longitude and latitude of the camera position at uniformly sampled positions (black dots with arrows). Eight optimization runs (colored paths on the surface) are started at uniformly seeded positions and optimized in parallel. (b) The runs converge to three clusters of local minima. The cluster with the highest entropy (1.72) coincides with the best value from 256 sampled entropies. For the best run, the start view, as well as some intermediate views and the final result, are shown in Fig. 1a. (c) Timings and memory consumption show that forward differences approximately double the runtime, but are faster and require less memory than the adjoint method. Best viewpoint selection using maximization of visual entropy for datasets jetstream (256 3 ), potential field of a C60 molecule (128 3 ), and smoke plume (178 3 ). Comparison of DiffDVR with eight initializations against random uniform sampling over the sphere of 256 views. (a) Optimization paths over the sphere. (b) Initial view, selected view of DiffDVR, best sampled view. (c) Visual entropy of optimization results (colored points corresponding to (a)) vs. sampled images. Violin plot shows the distribution of loss values when sampling the sphere uniformly. Visual entropy of the best viewport is shown above each plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Timings and loss function values for different AD modes and resolutions of the reconstructed TF. Timings are with respect to a single epoch. initial final reference difference (10x)</figDesc><graphic coords="7,506.39,207.41,57.80,57.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Density reconstruction using an optical absorption-only model. Comparison between DiffDVR, algebraic reconstruction provided by the ASTRA-toolbox<ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> and Mitsuba's differentiable path tracer<ref type="bibr" target="#b35">[36]</ref>. For each algorithm, a single slice through the center of the reconstructed volume and a volume rendering of this volume are shown, including per-pixel differences to the reference images. PSNR values in column "slice" are computed over the whole volume, in column "rendering" they are with respect to the rendered images. Timings are given in Sect. 5.3. In the difference images, blue and red indicate under-and over-estimation, respectively.</figDesc><graphic coords="8,52.79,285.89,61.46,61.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Density optimization for a volume colored via a non-monotonic rgbα-TF using an emission-absorption model. (a) Rendering of the reference volume of a human tooth and a human thorax. (b) Local minimum of the loss function. (c) Pre-shaded color volume as initialization. (d) Final result of the density volume optimization with TF mapping. The second row shows slices through the volumes. Note the colored slice through the pre-shaded color volume in (c).</figDesc><graphic coords="9,62.51,147.29,61.34,61.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b11">12</ref>. 1D example for a density optimization with a Gaussian TF with the optimum at a density of −1.0. For a value &gt; 0.4, the gradient faces away from the optimum.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank Jakob Wenzel and Merlin Nimier-David for their help and valuable suggestions on the Mitsuba 2 framework.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} symposium on operating systems design and implementation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic differentiation of algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartholomew-Biggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Christianson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dixon</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0377-0427(00)00422-2</idno>
	</analytic>
	<monogr>
		<title level="m">IV: Optimization and Nonlinear Equations</title>
				<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="171" to="190" />
		</imprint>
	</monogr>
	<note>Numerical Analysis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A generative model for volume rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1636" to="1650" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A perceptually based adaptive sampling algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Bolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Meyer</surname></persName>
		</author>
		<idno>doi: 10. 1145/280814.280924</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;98</title>
				<meeting>the 25th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="299" to="309" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">View selection for volume rendering</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D</forename><surname>Bordoloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIS 05. IEEE Visualization</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate volume rendering based on adaptive numerical integration</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Q</forename><surname>Campagnolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Celes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>De Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th SIB-GRAPI Conference on Graphics, Patterns and Images</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An information-theoretic framework for visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jäenicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1206" to="1215" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visibility histograms and visibility-driven transfer functions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="204" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast algorithms for volume ray tracing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Danskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<idno type="DOI">10.1145/147130.147155</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1992 Workshop on Volume Visualization, VVS &apos;92</title>
				<meeting>the 1992 Workshop on Volume Visualization, VVS &apos;92<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multidimensional digital signal processing. prentice hall</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dudgeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mersereau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Merser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">19842. 1984</date>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inverse volume rendering with material dictionaries</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gkioulekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Algebraic reconstruction techniques (art) for three-dimensional electron microscopy and x-ray photography</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Herman</surname></persName>
		</author>
		<idno>doi: 10. 1016/0022-5193</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Theoretical Biology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90109" to="90117" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational analysis and improvement of sirt</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Benson</surname></persName>
		</author>
		<idno>doi: 10. 1109/TMI.2008.923696</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="918" to="924" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Volume visualization based on statistical transfer-function spaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haidacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Pacific Visualization Symposium (PacificVis)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Insitunet: Deep image synthesis for parameter space exploration of ensemble simulations</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Nashed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fundamentals of computerized tomography: image reconstruction from projections</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Herman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enoki: structured vectorization and differentiation on modern processor architectures</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jakob</surname></persName>
		</author>
		<ptr target="https://github.com/mitsuba-renderer/enoki" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic view selection for time-varying volumes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1109" to="1116" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12057</idno>
		<title level="m">Differentiable rendering: A survey</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adaptive screen-space sampling for volume ray-casting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reininghaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hotz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ZIB-Report</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Differentiable monte carlo ray tracing through edge sampling</title>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to infer implicit surfaces without 3d supervision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Opendr: An approximate differentiable renderer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abstracting attribute space for transfer function exploration and design</title>
		<author>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jänicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gaither</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="107" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spherical fibonacci point sets for illumination integrals</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ribardière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouatouch</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12190</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="134" to="143" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive temporal sampling for volumetric path tracing of medical data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martschinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hartnagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keinert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13771</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="67" to="76" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fluid control using the adjoint method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Treuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stam</surname></persName>
		</author>
		<idno type="DOI">10.1145/1015706.1015744</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="456" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient space skipping and adaptive sampling of unstructured volumes using hardware accelerated ray tracing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Morrical</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Usher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2019.8933539</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Visualization Conference (VIS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="256" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Introduction to automatic differentiation and matlab object-oriented programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Neidinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="545" to="563" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06575</idno>
		<title level="m">Rendernet: A deep convolutional network for differentiable rendering from 3d shapes</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Radiative backpropagation: An adjoint method for lightning-fast differentiable rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nimier-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Speierer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jakob</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386569.3392406</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions on Graphics (Proceedings of SIGGRAPH)</title>
				<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mitsuba 2: A retargetable forward and inverse renderer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nimier-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vicini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zeltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jakob</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355089.3356498</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<ptr target="https://docs.nvidia.com/cuda/nvrtc/index.html" />
	</analytic>
	<monogr>
		<title level="j">NVidia. Cuda nvrtc</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Antialiased ray tracing by adaptive progressive refinement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sloan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th annual conference on Computer graphics and interactive techniques</title>
				<meeting>the 16th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11149</idno>
		<title level="m">Pix2vex: Image-to-geometry reconstruction using a smooth differentiable renderer</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A versatile scene model with differentiable visibility applied to generative pose estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="765" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic transfer functions based on informational divergence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bardera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Boada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1932" to="1941" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<title level="m">Scene representation networks: Continuous 3d-structure-aware neural scene representations. NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A feature-driven approach to locating optimal viewpoints for volume visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fujishiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nishita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIS 05. IEEE Visualization</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="495" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structure-aware viewpoint selection for volume visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Clapworthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Pacific Visualization Symposium</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Similarity voting based viewpoint selection for volumes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">State of the art on neural rendering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="701" to="727" />
			<date type="published" when="2020">2020</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast and flexible x-ray tomography using the astra toolbox</title>
		<author>
			<persName><forename type="first">W</forename><surname>Van Aarle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Palenstijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Janssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bleichrodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dabravolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Beenhouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Batenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sijbers</surname></persName>
		</author>
		<idno type="DOI">10.1364/OE.24.025129</idno>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="25129" to="25147" />
			<date type="published" when="2016-10">Oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The astra toolbox: A platform for advanced algorithm development in electron tomography</title>
		<author>
			<persName><forename type="first">W</forename><surname>Van Aarle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Palenstijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Beenhouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Altantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Batenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sijbers</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ultramic.2015.05.002</idno>
	</analytic>
	<monogr>
		<title level="j">Ultramicroscopy</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="35" to="47" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Representative views and paths for volume models</title>
		<author>
			<persName><forename type="first">P.-P</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Monclús</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Navazo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Smart Graphics</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="106" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Volumetric isosurface rendering with deep learning-based super-resolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<idno>doi: 10.1109/ TVCG.2019.2956697</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning adaptive sampling and reconstruction for volume visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Is ¸ık</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3039340</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A simple automatic derivative evaluation program</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wengert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="463" to="464" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adaptive sampling for monte carlo global illumination using tsallis entropy</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational and Information Science</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="989" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep learning-based viewpoint recommendation in volume visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="991" to="1003" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Transfer function design based on user selected samples for intuitive multivariate volume exploration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Pacific Visualization Symposium (PacificVis)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
