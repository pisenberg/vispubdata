<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Language to Visualization by Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuyu</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Tang</surname></persName>
							<email>ntang@hbku.edu.qa</email>
						</author>
						<author>
							<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
							<email>liguoliang@</email>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chengliang</forename><surname>Chai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuedi</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vega</forename><forename type="middle">-</forename><surname>Lite</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">California California New York New York</orgName>
								<address>
									<postCode>2021-03-08, 2021-03-08, 2021-03-08, 2021-03-08</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>D N2</postCode>
									<country>C, D A B</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">QCRI</orgName>
								<orgName type="institution" key="instit2">Hamad Bin Khalifa University</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">American School of Doha</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Language to Visualization by Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1C0E119A2C2204EB33AD4D2B424667EA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural language interface</term>
					<term>data visualization</term>
					<term>neural machine translation</term>
					<term>chart template</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>confirmed deaths confirmed deaths 3599250 54220 1694651 48335 date states cases number D N1 C Create a bar chart showing the top 5 states with the most confirmed cases until 2021-03-08 Show me the trend of confirmed, died, and recovered cases in Utah N1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Natural language interface is a promising interaction paradigm for simplifying the creation of visualizations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref>. If successful, even novices can generate visualizations simply like a Google search. Not surprisingly, both commercial vendors (e.g., Tableau's Ask Data <ref type="bibr" target="#b45">[46]</ref>, Power BI <ref type="bibr" target="#b1">[2]</ref>, ThoughtSpot <ref type="bibr" target="#b2">[3]</ref>, and Amazon's QuickSight <ref type="bibr" target="#b0">[1]</ref>) and academic researchers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57]</ref> have investigated to support the translation from NL queries to visualizations (NL2VIS).</p><p>NL2VIS needs both natural language understanding that uses machines to comprehend natural language queries, and translation algorithms to generate targeted visualization using a visualization language. Natural language understanding is considered an AI-hard problem <ref type="bibr" target="#b55">[56]</ref>, with many intrinsic difficulties such as ambiguity and underspecification. Many tools from the NLP community, especially based on statistical phrase-based translation <ref type="bibr" target="#b25">[26]</ref> and neural machine translation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>, have been used to tackle NL2VIS.</p><p>The state-of-the-art NL2VIS methods (for example, NL4DV <ref type="bibr" target="#b39">[40]</ref> and FlowSense <ref type="bibr" target="#b56">[57]</ref>) are statistical phrase-based translation, which treats natural language understanding and machine translation as two steps. They first employ NLP toolkits (for example, NLTK <ref type="bibr" target="#b4">[5]</ref>, Stanford CoreNLP <ref type="bibr" target="#b36">[37]</ref>, and NER <ref type="bibr" target="#b11">[12]</ref>) to parse an NL query and produce a variety of linguistic annotations (for example, parts of speech, named entities, etc), based on which they then devise algorithms to generate target visualizations. They are good choices when there are not many training datasets to train deep learning models.</p><p>We present ncNet 1 , an end-to-end solution using a Transformerbased sequence-to-sequence (seq2seq) model, which translates an NL query to a visualization. It adopts self-attention to generate a rich repre-sentation (high dimensional vectors) of the input, ncNet enables smart visualization inference (e.g., guessing the missing column, selecting a chart type, etc).</p><p>Besides making smarter inferences, a system can obtain more information (or "hint") from the user, by either obtaining a one-shot hint from the user or iteratively requiring more information (a.k.a. conversational systems) <ref type="bibr" target="#b5">[6]</ref>. The hint can be of various formats, such as NL queries, tables, chart templates, with one main criterion to be easy-touse. We propose to use chart templates as additional hints, where a user can specify the output to be a pie chart or a scatter plot with a simple click. In practice, chart templates have been widely used in all commercial products, including Tableau, Excel, Google Sheets, and so on. Due to the flexibility of the seq2seq model, we just treat the selected chart template C as another sequence, together with the NL query N and the dataset D as the input X.</p><p>Contributions. In this work, we make several contributions, including:</p><p>• proposing ncNet, a Transformer-based <ref type="bibr" target="#b52">[53]</ref> seq2seq model for supporting NL2VIS; • presenting a novel visualization-grammar, namely Vega-Zero, with the main purpose to simplify the NL2VIS translation using neural machine translation techniques. Moreover, transforming it to other visualization languages are straightforward; • enhancing ncNet by allowing the user to select a chart template, which will be used to improve the translation accuracy; • devising two optimization techniques: attention forcing for incorporating pre-defined domain knowledge and visualizationaware translation for better final visualization generation; and • demonstrating that ncNet can well support NL2VIS with several use cases, as well as conducting a quantitative study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Natural Language Interface for Data Visualization</head><p>The idea of using NL as a way to create visualizations was explored around two decades ago <ref type="bibr" target="#b5">[6]</ref>, where the system interacts with the user through dialogs. During each interaction, the system tries to clarify a small part of the user specification. For example, the system asks:</p><p>"At what organizational level?", the user answers: "At the department level", and so on. At that time, the system can only map simple user inputs to pre-defined commands. Afterwards, semantic parsers (e.g., NLTK <ref type="bibr" target="#b4">[5]</ref>, NER <ref type="bibr" target="#b11">[12]</ref>, and Stanford CoreNLP <ref type="bibr" target="#b36">[37]</ref>), which can automatically add additional layers of semantic information (e.g., parts of speech, named entities, coreference, etc) to NL, have been widely adopted in the research of NL2VIS. Recent studies, such as NL4DV <ref type="bibr" target="#b39">[40]</ref> and FlowSense <ref type="bibr" target="#b56">[57]</ref>, all employ semantic parsers, which are considered as the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Natural Language Processing with Deep Learning</head><p>Closer to this work is ADVISor <ref type="bibr" target="#b26">[27]</ref> that uses BERT <ref type="bibr" target="#b9">[10]</ref> to generate the embeddings of both the NL query and the table headers, which are then used by an "Aggregation" network to select an aggregation type and a "Data" network to decide used attribute and predicatesthese SQL fragments will determine an SQL query. Then, a rule-based "Visualization" module will decide which visualization to generate. Compared with ADVISor, ncNet supports more complex data transformation types such as relational join, GroupBY, OrderBY, Or predicate in SQL WHERE clauses. Another difference is that the neural networks of ADVISor are trained using (NL, SQL) pairs, while ncNet is trained using (NL, VIS) pairs and outputs Vega-Zero queries.</p><p>In fact, the main obstacle of using deep learning for NL2VIS is not the shortage of deep learning models or techniques. Instead, it is the lack of benchmark datasets that these models can be trained on, because deep learning models are known to be data hungry <ref type="bibr" target="#b13">[14]</ref>. Fortunately, a recent work releases the first public benchmark for NL2VIS, namely nvBench <ref type="bibr" target="#b34">[35]</ref>, which can be used to try deep learning for NL2VIS. nvBench consists of 25,750 NL queries and the corresponding visualizations, i.e., 25,750 (NL, VIS) pairs, over ∼780 tables from 105 domains (e.g., sports, customers). We will discuss more details of  nvBench in Section 6.2. Another recent work <ref type="bibr" target="#b47">[48]</ref> collected 893 NL queries over three datasets. However, its number is not sufficient to train typical deep learning models. An alternative solution is NL2SQL + automatic data visualization, which is a good choice when the entire pipeline is one-shot. However, in practice, it is always iterative. That is, if the target visualization needs to be refined, the user needs to verify/refine both NL2SQL and check the result of automatic data visualization. Note that, checking whether a table is good enough is hard, even for a small table with hundreds/thousands of tuples. In this case, using end-to-end NL2VIS has an advantage that the user only sticks to one task, which is more user-friendly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN REQUIREMENTS</head><p>There are three main goals when devising solutions for NL2VIS, along the same line of other NL2VIS tools e.g., NL4DV <ref type="bibr" target="#b39">[40]</ref>.</p><p>(1) Easy-to-use. We want to allow novices to create visualizations simply like a Google-search. That is, even users without data visualization background can easily generate visualizations.</p><p>(2) End-to-end. Traditional semantic parser based translation systems typically consist of many small sub-components that are tuned separately. In contrast, we want to deliver a complete NL2VIS solution without the need of any additional steps. Besides the well-known benefits of end-to-end solutions such as increased efficiency, cost cutting and ease of learning, one particular benefit for a seq2seq model is that it is easy to maintain and upgrade. For example, upgrading a seq2seq model from using long short-term memory (LSTM) <ref type="bibr" target="#b18">[19]</ref> to Transformer <ref type="bibr" target="#b52">[53]</ref> only requires to change a few lines of code.</p><p>(3) Language-agnostic. The main benefit to be language-agnostic is that we just need to train one seq2seq model for NL2VIS, but can support multiple target visualization languages. The practical need for this is evident, because the users might use various visualization languages constrained by different applications, such as Vega-Lite, D3, ggplot2, and so forth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BACKGROUND AND PROBLEM FORMULATION 4.1 Sequence-to-Sequence Models</head><p>A sequence-to-sequence (seq2seq) model <ref type="bibr" target="#b50">[51]</ref> consists of two parts, an encoder and a decoder, where each part can be implemented by different neural networks. The task of an encoder is to understand the input sequence, and generate a smaller representation h (i.e., a high-dimensional vector) to represent the input. The task of a decoder is to generate a sequence of output by taking h as input. The network needs to be trained with a lot of training data, in the form of (Input sequence, Output sequence) pairs. Due to the flexibility of seq2seq models that allow the input and output to have different formats, they have a wide spectrum of applications including language translation <ref type="bibr" target="#b16">[17]</ref>, image captioning <ref type="bibr" target="#b37">[38]</ref>, conversational models and text summarization <ref type="bibr" target="#b38">[39]</ref>, and NL to SQL <ref type="bibr" target="#b22">[23]</ref>.</p><p>Let's first walk through a typical translation task -language translation from English to French (Figure <ref type="figure" target="#fig_0">2(A)</ref>). The { "data": {"url": "US_States.csv"}, "mark": "bar", "transform": [{ "window": [{ "op": "rank", "as": "rank" }],</p><p>"sort": [{ "field": "number", "order": "descending"}] }, { "filter": { "and": [ {"field": "datum.rank", "lte": 5}, {"field": "cases", "equal": "confirmed"}, {"field": "date", "equal": "2021-03-08"} ] }} ], "encoding": { "x": {"field": "states", "type": "nominal"}, "y": {"field": "number", "type": "quantitative", "aggregate": "sum"} } }  task is to train an English2French network with a lot of (English sentence, French sentence) pairs, such that it learns to translate from an English sentence (e.g., "natural language to visualization is important") to a French sentence (e.g., "le langage naturel à la visualisation est important").</p><p>Similar to the translation from English to French, the translation from NL2VIS is to train a NL2VIS network with a lot of (NL query, VIS query) pairs, such that it learns to translate from a natural language query (e.g., "Create a bar chart showing the top 5 ..." in Figure <ref type="figure" target="#fig_0">2(B)</ref>) to a visualization specification (e.g., "mark bar encoding x states y aggregate ...").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and Natural Language Query</head><p>Dataset. Let D be the data source that the user wants to generate visualizations on. To be simple, we consider D as a tabular dataset, which can be obtained from a JSON file, a CSV file, or a relational table from a database.</p><p>For example, Figure <ref type="figure" target="#fig_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Vega-Zero</head><p>As we have discussed in Section 4.1, the task of a seq2seq model is to translate an NL sequence to a visualization sequence, for which we need to decide which visualization grammar to use for the output visualization sequence.</p><p>Intuitively, we can use Vega-Lite <ref type="bibr" target="#b43">[44]</ref>. However, it is hard to train a seq2seq model to generate a hierarchical output (e.g., in JSON format such as Vega-Lite). In contrast, it is much easier to train a seq2seq model to generate a sequence output.</p><p>To this end, we present a visualization grammar by simplifying Vega-Lite, with the main purpose to flatten a hierarchical Vega-Lite specification to a sequence-based specification. That is, Vega-Lite is more user friendly, but Vega-Zero is more seq2seq model friendly.</p><p>Vega-Zero. Vega-Zero keeps most of the keywords of the Vega-Lite about the mapping between visual encoding channels and (transformed) data variables. It flattens a JSON object into a sequence of keywords by removing structure-aware symbols such as brackets, colons, and quotation marks. Formally, a unit specification in Vega-Zero is a fourtuple (similar to Vega-Lite but with each tuple being a sequence) as: unit = (mark, data, encoding, transform)</p><p>Naturally, as a simplification of Vega-Lite: mark denotes the chart type, including bar, line, point (for scatter chart), arc (for pie chart); data specifies the source data; encoding contains x/y-axis, aggregate function, and color based on which column; and transform defines some data transformation functions: filter, bin, group, sort, and top-k.</p><p>For example, Figures <ref type="figure" target="#fig_2">3(d</ref>) and (e) show the Vega-Zero grammar and the Vega-Lite grammar for the target histogram in Figure <ref type="figure" target="#fig_2">3(b)</ref>, respectively. It shows that Vega-Zero flattens the keywords of the Vega-Lite hierarchical JSON specification into a sequence.</p><p>Vega-Zero is Language-Agnostic. Although Vega-Zero is a simplification of Vega-Lite, we still consider it as language-agnostic, or an intermediate visualization language, because we remove all languagespecific settings such as which color to use (e.g., black or blue), what is the default width of a bar, and so on.</p><p>Converting a Vega-Zero query into the format of other visualization languages (e.g., Vega-Lite and ggplot2) is straightforward (see Section 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Chart Templates</head><p>Note that the reason to allow a user to select a chart template is to alleviate the ambiguity and underspecification intrinsic to an NL query. Besides being user friendly, it can also help novices or data scientists that are not in the visualization community to easily specify the type of desired visualization.</p><p>Chart Templates. Figure <ref type="figure" target="#fig_4">4</ref> showcases seven chart templates. Each chart template constraints the chart type, the data types of the x/y-axis, and optional sorting parameters.</p><p>For example, if a user selects a "Bar Chart" and then sets the sorting parameter as "by measure attributes in descending order", it means that the user wants to visualize a bar chart and display the bars from high to low, e.g., Figure <ref type="figure" target="#fig_2">3(b)</ref>.</p><p>Essentially, the chart template is used as a constraint to reduce the search space of possible outputs. More specifically, a chart template is selected to explicitly constrain the mark and sort part of a Vega-Zero.</p><p>For example, Figure <ref type="figure">5</ref>(a) shows an "empty" Vega-Zero template without any constraints w.r.t. the type of chart, the dataset to use, how to encode, and how to transform. Assume that a bar chart is selected together with a source CSV file "US States.csv" and sorting parameter as "by measure attributes in descending order", then they can be used to fill the mark as bar, data as "US States.csv", and sort as y desc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Problem Statement</head><p>Input. Let N be a user provided NL query, C an optional chart template, and D a source dataset. For deep learning, each input needs to be pre-processed before being fed into a deep learning model, typically in the form of vectors <ref type="bibr" target="#b13">[14]</ref>. Let X 1:n = embed(N,C, D) be a sequence of n vectors after pre-processing the three inputs N,C and D (see Section 5.2 "Input Embedding" for more details).</p><p>Output. Let Y 1:m be the target visualization w.r.t. the given input, which is a sequence of m vectors. (b) A selected chart template in Vega-Zero Fig. <ref type="figure">5</ref>: Using a chart template as a constraint.</p><p>We will simply write X 1:n as X and Y 1:m as Y, when it is clear from the context.</p><p>Problem. The problem of using a seq2seq model for NL2VIS is to learn a function f θ () such that Y = f θ (X), where f θ is a deep neural network parameterized by θ .</p><p>More specifically, a seq2seq model consists of two neural networks, an encoder that understands the input sequence and create a hidden representation h (i.e., a high-dimensional vector), and a decoder that generates a target output from h. In a finer granularity, the problem of learning f θ () is to learn two connected networks: the encoder network h = encode(X), and the decoder network Y = decode(h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NCNET</head><p>In this paper, we propose to use a Transformer-based seq2seq model for NL2VIS, namely ncNet, where "n" stands for natural language, "c" for chart template. and "Net" for neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture</head><p>The architecture of ncNet is shown in Figure <ref type="figure" target="#fig_6">6(a)</ref>. ncNet adopts a Transformer-based <ref type="bibr" target="#b52">[53]</ref> encoder-decoder model that consists of an encoder and a decoder, which are both stacks of self-attention blocks. That is, an encoder (or decoder) is composed of multiple encoder (or decoder) blocks, where an encoder block is depicted in Figure <ref type="figure" target="#fig_6">6(c)</ref>.</p><p>Why Transformer. In retrospect, recurrent neural network (RNN) <ref type="bibr" target="#b14">[15]</ref> has been widely used as encoder/decoder blocks. The key innovation of Transformer <ref type="bibr" target="#b52">[53]</ref> is that it can process the entire input sequence X 1:n of variable length n without exhibiting a recurrent structure as RNN does, which allows Transformer-based encoder-decoder models to be highly parallelizable. Moreover, this allows the encoder to compute a better contextualized encoding (e.g., when processing a token x i , it can attend on any input token before or after x i ). In contrast, an RNN can only process tokens sequentially, i.e., when processing a token x i , it can only see the input tokens before x i . Due to the advantage of Transfomer to process the input holistically, not sequentially, it can successfully support a wide range of applications including NL2SQL, which suggests that it may also be a good fit for NL2VIS in the current age. Moreover, explainability on Transformers for neural machine translation has been studied <ref type="bibr" target="#b24">[25]</ref>, which can help explain how it works to the users. Another advantage is that we can further pre-train Transformers on a large set of NL corpus such as Wikipedia such that it can obtain general knowledge such as "MA" in "Cambridge, MA, USA" is a state, and then fine-tune on NL2VIS benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Working Mechanism of ncNet</head><p>Next, let's work through an example to understand how ncNet works.</p><p>Consider the three inputs: the NL query N, the selected chart C, and the table D as shown in Figure <ref type="figure" target="#fig_6">6(a)</ref>.</p><p>Note that, if the chart template is not specified, we will use the "empty" chart template (see Figure <ref type="figure">5(a)</ref>) as the input sequence; otherwise, the chart template is selected, we will use a a partially specified Vega-Zero specification as the input sequence (e.g., see Figure <ref type="figure">5(b)</ref>).</p><p>Input Tokenization. We have three types of inputs. Each one will be converted into a sequence, and the three sequences will be concatenated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">N:</head><p>The NL query is naturally a sequence of words, where each word will be treated as a token. We add a special token N (or /N ) to denote the start (or end) of an NL sequence. For example, the NL query will be tokenized as: (see Figure <ref type="figure">5</ref>) -these are called masked tokens. The main task of ncNet is to fill these masked tokens. Moreover, we add a special C (or /C ) token to denote the start (or end) of a chart sequence. For example, the selected line chart is tokenized as:</p><formula xml:id="formula_0">T N = N show</formula><formula xml:id="formula_1">T C = C mark line encoding x [x] y aggregate ... /C</formula><p>3. D: For simplicity, we consider a single table, which consists of a set of rows with columns. We linearize the D into a sequence of tokens by concatenating the column names and scanning the table content row by row. Similarly, we add a special D (or /D ) to denote the start (or end) of a table, the columns (or cell values) tokens will be filled between special token COL (or VAL ) and /COL (or /VAL ). For example, the table will be tokenized as:</p><formula xml:id="formula_2">T D = D table name COL ... /COL VAL ... /VAL /D</formula><p>The above three tokenized sequences will be concatenated as one sequence X = T N ⊕T C ⊕T D , which is the input of the "Input Embedding" model, shown in Figure <ref type="figure" target="#fig_6">6(b)</ref>.</p><p>Input Embedding. Each token x i in the input token sequence X will be converted into a vector embedding x i as follows:</p><p>1. Token embedding: Each x i will be converted to a token embedding, which produces a vector, denoted as x token i . 2. Type embedding: Each x i will also be converted to a type embedding, which is responsible for distinguishing the type of token, e.g., an NL token, a chart template token, or a data token. It also produces a vector, denoted as x type i . 3. Position embedding: Finally, we also compute for each x i a positional embedding, which hints the position of the token x i in the sequence, denoted as x position i .</p><p>Finally, the input embedding of x i , denoted as x i , is the summation of the above three embeddings as:</p><formula xml:id="formula_3">x i = x token i + x type i + x position i = embed(x i )</formula><p>After each token is processed as discussed above, it outputs X as shown in Figure <ref type="figure" target="#fig_6">6</ref>(b), i.e., X = embed(N,C, D). This X will serve as the input to the encoder as shown in Figure <ref type="figure" target="#fig_6">6(a)</ref>.  Here, " " is matrix multiplication, and " " denotes Hadamard product. Discussion: Handling Large D. When D is small, we can tokenize the entire D as a sequence as discussed above. However, when D is large, due to the limitation of the length of the input vector of a Transformer (e.g., up to 512 tokens), it is not allowed to incorporate all values of D into the sequence. We propose to only add those values that are very possibly mentioned by the user input NL query to the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⊙ × Attention Forcing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Attention</head><formula xml:id="formula_4">Feed Forward Add &amp; Norm Add &amp; Norm Q K V X E One Transformer-based Encoder Block softmax Q T K V E ⊙ d k ( ) × × Vega-Lite</formula><p>To this end, we first measure the similarity between a text value in the NL query N and values in D, which is achieved by string-based similarity functions <ref type="bibr" target="#b7">[8]</ref>, either through handcrafted patterns, or by word embeddings <ref type="bibr" target="#b21">[22]</ref>. In this paper, we equip an efficient and effective string similarity search algorithm <ref type="bibr" target="#b7">[8]</ref> to extract those values in D that are similar to the text values in the user input NL query N, which are typically small enough to be fed to a Transformer. After the encoder processes the complete input sequence X, it will generate a contextualized encoded sequence h, as h = encode(X).</p><p>Output Sequence Generation. Next we describe how the decoder can auto-regressively generate the output (i.e., the output tokens are sequentially generated) and thus a mapping of an input sequence X to an output sequence Y.</p><p>First, the input encoding h together with a special "start-of-sentence" SOS vector, i.e., y 0 , is fed to the decoder. The decoder processes the inputs h and y 0 to compute the first target vector y 1 . The first target vector y 1 = "mark" is selected.</p><p>Next, the decoder now processes both y 0 = SOS and y 1 = "mark" to compute the second target vector y 2 (e.g., "line").</p><p>The decoder will then process y 0 , y 1 and y 2 , and so on, done in an auto-regressive fashion, until a special EOS token is generated as a termination condition.</p><p>By doing so, it generates an output sequence as Y = decode(h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training</head><p>ncNet is trained using an existing NL2VIS benchmark called nvBench, which consists of the mapping from thousands of (N, D) pairs to visualizations, such as (N 1 , D) and the bar chart in Figure <ref type="figure">1</ref>. We insert the chart template information C into (N, D) as (N,C, D) with two modes:</p><p>(1) empty chart template, as shown in Figure <ref type="figure">5</ref> The training is to optimize a reconstruction loss -the cross-entropy between the model output (i.e., the predicted visualization) and the ground truth visualization provided by the benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Attention Forcing</head><p>A major innovation of Transformer <ref type="bibr" target="#b52">[53]</ref> is attention, which allows the model to focus on the relevant parts of the input sequence as needed, and thus highly improves the quality of machine translation systems. Next, let's first review the concept of attention, followed an optimization technique, called attention forcing, to further improve the quality.</p><p>Let's Pay "Attention". Figure <ref type="figure" target="#fig_6">6</ref>(c) shows a Transformer-based encoder block, where the "Multiple-Head Attention" module gives the encoder greater power to estimate the relevance of one token to other tokens. For simplicity, in the following, we only illustrate "One-Head Attention" (i.e., one attention unit), and "Multiple-Head Attention" just repeats the computation of one attention unit multiple times in parallel. A sample attention matrix is shown in Figure <ref type="figure" target="#fig_8">7</ref>(a), which shows the encoder attention weights of pairs of tokens in the range [0, 1] that correspond to colors from light chartreuse to dark blue. A light color (or a dark color) indicates that the corresponding two tokens are less (or more) correlated.</p><p>Next we briefly discuss how an attention matrix is computed (see Figure <ref type="figure" target="#fig_6">6(c)</ref>). Recall the input X is a sequence of vectors, which forms a matrix by packing these vectors. The Transformer will learn three weight matrices: the query weight matrix W Q , the key weight matrix W K and the value weight matrix W V . It then computes three matrices as Q = X ×W Q , i.e., a matrix that contains the query (vector representation of one word in the sequence); K = X ×W K , i.e., a matrix with all the keys (vector representations of all the words in the sequence); and V = X × W V i.e., a matrix with all the values, which are again the vector representations of all the words in the sequence, where "×" means matrix multiplication. The three matrices, Q, K and V, will be fed to the attention unit. The output attention matrix is computed as:</p><formula xml:id="formula_5">Attention(Q, K, V) = softmax Q × K T √ d k × V,</formula><p>where K T is the transpose of matrix K, √ d k is to stabilize gradients during training, and softmax normalizes the weights to sum to 1. Please refer to the Transformer paper <ref type="bibr" target="#b52">[53]</ref> for more details.</p><p>Attention Forcing. It can be observed from Figure <ref type="figure" target="#fig_8">7</ref>(a) that there is no need to have an attention value between [X] and cell values recovered, confirmed, died, and utah, because these cell values should not be encoded as the x-axis of a visualization. If we have such prior knowledge, we can explicitly tell the attention unit about what should not be attended, which is known as attention forcing <ref type="bibr" target="#b10">[11]</ref>.</p><p>More specifically, we explicitly encode the prior knowledge as a forcing matrix E with boolean values 0 and 1, where 0 means do not attend. In the case of NL2VIS, the values (i.e., Next, we discuss how to force the attention to be aware of the forcing matrix E. Essentially, the forcing matrix E acts as an attention mask so that some attention scores of the irrelevant tokens are set to zero. This is achieved by an element-wise multiplication as below:</p><formula xml:id="formula_6">Attention(Q, K, V, E) = softmax Q × K T √ d k E × V,</formula><p>where " " denotes Hadamard product. The process of attention forcing is also depicted in Figure <ref type="figure" target="#fig_6">6</ref>(c) to help illustrate the difference with/without the forcing matrix E.</p><p>The computed attention matrix, after attention forcing, is shown in Figure <ref type="figure" target="#fig_8">7</ref>(c). Comparing with Figure <ref type="figure" target="#fig_8">7</ref>(a), we can see that the attention scores in red rectangles of Figure <ref type="figure" target="#fig_8">7</ref>(c) are set to zero. In contrast, the attention scores of ([X], date) , ([Y], number), ([Z], cases), ([F], states), ([F], utah) become higher, which indicate that the model better capture token correlation after attention forcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Visualization-aware Translation</head><p>In the decoding phase, the model may make false prediction due to various reasons. We detail some reasons as follows. The first reason is the inherent ambiguity of the NL query. The second reason is that the NL2VIS model may work poorly in some situations. For example, when it predicts the encoding parts, the tokens should be selected from the column names of the table D instead of the cell values of the D.</p><p>To alleviate the above issues, we propose a simple yet effective method, namely visualization-aware translation, that incorporates visualization knowledge (i.e., rules of thumb for visualization) and chart templates to validate the output tokens of the ncNet and correct those incorrect tokens (Figure <ref type="figure" target="#fig_6">6(d)</ref>). Our visualization-aware translation method is a variant of the Beam Search <ref type="bibr" target="#b50">[51]</ref>, a well-known algorithm for neural machine translation task. In a nutshell, the beam search algorithm <ref type="bibr" target="#b46">[47]</ref> maintains a set of k candidate partial outputs and their cumulative probabilities at each step, and finally select k candidates with highest cumulative probabilities.</p><p>The basic idea of visualization-aware translation is to maintain the top-k possible tokens predicted by the ncNet at some specific steps, and then we use visualization rules and the constraints of a selected chart template to select the "most possible" token from the top-k candidates. More concretely, the specific steps are those steps for predicting the empty parts of the Vega-Zero grammar (e.g., the [T], [X] in Figure <ref type="figure">5</ref>(a)). For steps of predicting the following types of tokens, by default we maintain top-5 tokens given by the model, and choose the "best" with the following heuristic rules:</p><p>1. For the [T]/[S] tokens: if a chart template is specified, we use the parameters from the chart template as the target outputs; otherwise, we take the top-1 candidate as the output token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For the [X]/[Y]/[Z] tokens: we choose the candidate token t</head><p>with the highest probability under the condition that the t is a column of the dataset D and the column type should suitable for the chart type, e.g., two quantitative columns for scatter chart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">For the [G] (or [B]</head><p>) tokens: for the group (or bin) key, we choose the candidate token t with the highest probability and it must satisfy: (i) the t is temporal/categorical (or temporal/quantitative) types, and (ii) the t should not be the tokens from the [Z] part. 4. For the [K] token: we keep the candidate token t with the highest probability under the condition that the t is a valid number.</p><p>For example, Figure <ref type="figure" target="#fig_10">8</ref> shows an example to demonstrate the working mechanism of the visualization-aware translation algorithm. The algorithm follows heuristic rules mentioned before to process output tokens in those specific steps. For example, for the [T] token, since the token "line" has the highest probability and is a right chart type specified by the chart template, the algorithm keeps it as the target output. For the [Y] token, it selects the rank-2 token "number" because the rank-1 token "numbers" is not a column of the dataset D. Remark: Visualization Recommendation. Although the above process will output only one visualization, it is readily to be extended to provide top-k visualizations, e.g., by ranking different branches in Figure <ref type="figure" target="#fig_10">8</ref>, which can be used for visualization recommendation.</p><p>We have empirically verified that using this optimization can increase ∼3% accuracy, which will be used by ncNet by default. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Language-aware Rendering</head><p>As shown in Figure <ref type="figure" target="#fig_6">6</ref>(e), the Vega-Zero specification can be converted to popular visualization languages (e.g., Vega-Lite, ggplot2) for rendering the visualization result. The translation from a Vega-Zero specification to a targeted visualization language is hard-coded. Currently, we write ∼240 and ∼333 lines Python3 code to support the translation from Vega-Zero to Vega-Lite and ggplot2, respectively. The code is available at https://github.com/Thanksyy/Vega-Zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details of ncNet</head><p>We implement the ncNet with PyTorch <ref type="bibr" target="#b40">[41]</ref>, with three Transformerbased encoder blocks and three Transformer-based decoder blocks. For the multi-head attention layer, we set the number of heads to 8. The embedding dimension for word embedding, token types embedding and position embedding are set to 256. We limit the input length to 512 tokens -inputs that exceed this limit are truncated. We use the Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with a static learning rate (i.e., 0.0005) instead of the one with warm-up and cool-down steps. We use a learned positional encoding. We set the dropout rate as 0.1, for both encoder and decoder. The batch size is set to 64. The size of the trained model is about 40 MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">NL2VIS Benchmark</head><p>For training and testing ncNet, we use a public benchmark for the NL2VIS task, namely nvBench <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>NVBench Statistics. Figure <ref type="figure">9</ref> overviews the statistics of nvBench from the data sources and (NL, VIS) perspectives. As shown in Figure <ref type="figure">9(a)</ref>, nvBench has 153 databases along with 780 tables in total and covers 105 domains (e.g., sports, customers). The average number of columns/rows of the 780 tables is 5.26/1,309.65, and the maximum/minimum number of columns (rows) is 48/2 (183,978/1). Among the columns, 68.78% of columns are categorical columns, 11.58% of columns are temporal columns, and 19.64% of columns are quantitative columns. Figure <ref type="figure">9</ref>(a) also depicts the distributions of columns and rows, which tells us that most of the tables have 2 to 9 columns.</p><p>Given 153 databases, as shown in Figure <ref type="figure">9</ref>(b), nvBench contains 7,274 visualizations on seven types of charts. For each visualization, nvBench provides one to several NL queries because different users might provide different NL queries for the same visualization. In total, nvBench consists of 25,750 (NL, VIS) pairs. nvBench further defines the four-level complexities, i.e., easy, medium, hard, and extra hard, of the visualizations based on the hardness of the visualization query (i.e., similar to the Vega-Zero). For example, a Vega-Zero with filter, bin and aggregations may be categorized as a Hard visualization. The heatmap in Figure <ref type="figure">9</ref>   First, since nvBench contains some visualizations generated from multiple tables with the join operations, we remove such cases so as to focus on non-join cases in this evaluation. In average, the accuracy join cases is lower than non-join cases for ∼ 3.6%.</p><p>Second, we have to inject the chart templates into the benchmark to train ncNet for generating visualization result with NL query and chart template as inputs, because nvBench only provides NL query and the corresponding visualization. This step is straightforward: given a (NL, VIS) pair, we assign this (NL, VIS) pair a chart template (e.g., Figure <ref type="figure">5(b)</ref>) based on its chart type and sorting parameter. Hence, each (NL, VIS) pair will result in two (NL, VIS) pairs, one with an empty chart template, and the other with the desired chart template (e.g., a bar chart with sorting y-axis in descending order).</p><p>Finally, we have training/test sets with 25,238/4,920 (NL, VIS) pairs. More specifically, we show the distribution of training and testing (NL, VIS) pairs in Figure <ref type="figure" target="#fig_13">10</ref>. We can see that they have similar distributions on chart types and the hardness of visualization queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Usage Scenarios</head><p>We first present two common usage scenarios to demonstrate how developers (or lay users) can use ncNet. ncNet in Jupyter Lab. Data science practitioners often perform interactive data visualization in Jupyter Lab (or Jupyter Notebook). To make ncNet easy-to-use to for this type of users, we have developed a Python package to be used in the Jupyter Lab ecosystem.</p><p>Figure <ref type="figure">11</ref> is a screenshot that demonstrates how to create a desired grouping line chart with an NL query and a chart template for a COVID-19 dataset. First, the user imports the ncNet package and   ncNet as a Toolkit. In addition to using ncNet in an interactive Python environment, we also regard ncNet as a Python toolkit for the NL2VIS task. It means developers can build their visualization systems or dashboards <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref> via using ncNet package through pre-defined APIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">User Study</head><p>We conducted a user study on ncNet with two goals: (1) whether ncNet works well in real-world datasets with users from multiple domains? and (2) collect qualitative feedback on ncNet's design criterion.</p><p>Tasks. To achieve the above goals, we design the following tasks. Task 1: Given a table and a target visualization, we asked the participants to express an NL query for creating such a visualization. Task 2: We asked participants to provide their comments about ncNet.</p><p>Participants. We invited 4 experts (2 VISers, 1 NLPer, and 1 DBer) and 2 non-experts (1 financial staff and 1 operational staff) to participate in our user study. The four experts have expertise in data analysis/visualization and have more than six years of experience in Python, SQL, and Tableau. The latter two users are familiar with Excel and creating simple visualizations (i.e., plotting two columns as a bar chart in Excel).</p><p>Procedures. We asked participants to vote for 5 databases with domains they are familiar with out of 153 databases in nvBench. The 5 databases selected by majority voting were used in the user study. We then taught participants to use ncNet in Jupyter Lab. The experts took about 5 minutes to understand how to use ncNet, while the two nontechnical participants took about 20 minutes to understand the usage of ncNet. Non-technical participants had longer learning time mainly because they need to be familiar with the Jupyter Lab environment, including inputting/rephrasing input (NL queries) and executing cell. Third, we randomly sampled 10 visualizations for each database as the targets. Totally, we have 50 target visualizations for the user study. For task 1, we provided descriptions (i.e., what is expected) for each target visualization to guide the participants to formulate their NL queries (i.e., how). We recorded the interaction logs, including NL queries, input records, and task situations, when the participants interacted with ncNet. For task 2, we collected their comments about ncNet.</p><p>Results. Figure <ref type="figure" target="#fig_16">12</ref>(a) samples 4 target visualizations with userprovided NL queries. We also denote (using ) which NL queries can successfully generate the target visualization using ncNet. Figure <ref type="figure" target="#fig_16">12(b</ref>) reports the average accuracy; we can see that the experts achieve better results than non-experts. By rephrasing NL query several times, the accuracy rate is also improved. We also analyze the interaction logs from Task 1. Figure <ref type="figure" target="#fig_16">12(c)</ref> shows the time for non-experts to perform a NL2VIS task is about twice than experts. We report more details in Figure <ref type="figure" target="#fig_16">12(d)</ref>. It depicts the composition of a round of user time, in which thinking time and typing time account for almost half, and the system response time is around 1 second.</p><p>The participants comment that that ncNet is easy-to-use and can generate the target visualization in easily. Specifically, the two nontechnical users think that the NL2VIS tool is easier than Excel because it only needs to express the intent of visualization using natural language queries instead of conducting a series of operations in the Excel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Quantitative Evaluation</head><p>We now present quantitative assessments of ncNet using the nvBench testing dataset mentioned in Section 6.3. We use accuracy as the evaluation metrics. To be conservative, the accuracy measures whether the output Vega-Zero sequence exactly matches the ground truth Vega-Zero sequence. We define the accuracy = M/N, where M is the number of the output Vega-Zero sequences that are equivalent to the ground truth Vega-Zero sequences, and N is the total number of testing cases.</p><p>We evaluate the following three cases on the same testing dataset:</p><p>(1) Neural Network-based Baseline: we adapt the state-of-the-art NL2SQL model <ref type="bibr" target="#b17">[18]</ref> for NL2VIS task (Figure <ref type="figure" target="#fig_2">13</ref>(a)). ( <ref type="formula">2</ref>) ncNet without chart template (Figure <ref type="figure" target="#fig_2">13(b)</ref>).</p><p>(3) ncNet with a selected chart template (Figure <ref type="figure" target="#fig_2">13(c)</ref>).</p><p>Note that, we only consider the first visualization result (i.e., rank-1) outputted by baseline model and ncNet. Figure <ref type="figure" target="#fig_2">13</ref> summarizes the evaluation results. More concretely, the heatmap details the accuracy under different visualization types and difficult levels of the NL query. The bar charts colored in red report the average accuracy by chart types, while the green bar charts show the average accuracy across different hardness of NL queries. Next, we will elaborate the evaluation results. ncNet: Overall Performance. Figures <ref type="figure" target="#fig_2">13(b</ref>) and (c) summarize the performance of ncNet without and with chart template, respectively. The ncNet performs well on different hardness levels of visualization queries (shown in the green bar chart) and works effectively on different types of visualizations (the red bar chart). More specifically, as shown in the heatmaps of Figures <ref type="figure" target="#fig_2">13(b</ref>) and (c), we can see that ncNet achieves 100% accuracy on some cases (e.g., (S, Hard) in Figure <ref type="figure" target="#fig_2">13</ref>(b) and (c)). Overall, ncNet shows its effectiveness by achieving the accuracy of 77.8% and 79.6% on average for the cases without and with a chart template, respectively. ncNet: With/Without Chart Template. Next, we give a closer look at the performance difference between two cases of ncNet with and without chart template in Figure <ref type="figure" target="#fig_2">13</ref>(b) and (c), respectively. In general, it shows that the accuracy of nearly all cases is higher when an 8.1 Limitations (L1) Limited benchmarks. In retrospect, benchmarks have played a key role in spawning the boom in different research communities, such as ImageNet <ref type="bibr" target="#b8">[9]</ref> for image processing, GLUE <ref type="bibr" target="#b54">[55]</ref> and SuperGLUE <ref type="bibr" target="#b53">[54]</ref> for the NLP community, and TPC benchmarks <ref type="bibr" target="#b15">[16]</ref> for the database community. However, large public benchmarks, such as VizNet <ref type="bibr" target="#b20">[21]</ref> and nvBench <ref type="bibr" target="#b34">[35]</ref>, that can be used for deep learning on data visualization tasks are quite limited.</p><p>(L2) Supporting only one-shot NL queries. Different from applications that only need one-shot queries, data analytics often needs to issue a sequence of (or iterative) queries. End-to-end approaches (e.g., NL2VIS) have the advantage that the user only needs to check the visualization and refine the NL query. In contrast, non end-to-end approaches might require the user to be involved in different tasks, which is less user friendly. Even though, however, our current NL2VIS model only translates one (possibly revised) NL to a visualization, instead of translating a sequence of (or conversational) NL pieces into a visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Future Work</head><p>(F1) The quest for more benchmarks. In response to limitation L1, a promising future direction for pushing deep learning for data visualization, is to contribute to new benchmarks that cover more diversified tasks, such as conversational NL2VIS benchmarks, annotations for chart to description mapping, annotation for similar charts, and so on.</p><p>(F2) Supporting conversational NL queries. To lift limitation L2, an interesting direction is to extend ncNet to support conversational NL queries. The good news is that there are text-to-SQL benchmarks such as CoSQL <ref type="bibr" target="#b57">[58]</ref>. Naturally, it is interesting to explore how to leverage these benchmarks to extend and train ncNet such that it can support conversational NL2VIS cases, which have lots of practical applications.</p><p>(F3) Chart2vec. Similar to word2vec that learned a universal embedding of words based on the word associations from a large corpus of text, an interesting future work is to learn a universal embedding of charts, so as to enable other downstream applications (e.g., recommendations, story telling, guideline generation, and so forth.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>We present ncNet, a research attempt using deep neural networks to support NL2VIS. This approach is built upon the latest NLP models, namely Transformer-based seq2seq models. We further propose to use chart templates to help enhance the translation accuracy. We also demonstrate the effectiveness of ncNet on a NL2VIS benchmark over 105 domains. We hope that our proposal ncNet, along with recent advances in NLP, can shed some light on NL2VIS and justify the potential of deep neural networks for NL2VIS. However, the journey just starts and a lot need to be done such as handling non-friendly column header names (or even foo bar) and work for big data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Sample seq2seq tasks. (A) Translation from English to French. (B) Translation from NL queries to visualization specifications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Create a bar chart showing the top 5 states with the most confirmed cases until 2021-03-08 mark bar data US_States.csv encoding x states y aggregate sum number transform filter date = "2021-03-08" and cases = "confirmed" group x sort y desc topk 5 (a) A sample table US_States of COVID-19. (b) A bar chart. (c) A natural language query. (d) A Vega-Zero specification of the bar chart. A Vega-Lite specification of the bar chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Sample input, output, and the corresponding Vega-Lite query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) shows a sample data source about the COVID-19 statistical data in the United States. Natural Language Query. Let N be an NL query that specifies what a user wants to visualize on the dataset D. For example, Figure 3(c) is an NL query over the dataset in Figure 3(a), which corresponds to a histogram depicted in Figure 3(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The chart templates that users can select from.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The architecture of ncNet: a Transformer-based encoder and a Transformer-based auto-regressive decoder. Note that the Inputs to the encoder need not be aligned with decoder outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The architecture of ncNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a); and (2) selected chart template, as shown in Figure 5(b). In both cases, they are uniformly treated as partially completed visualization specifications with many masked tokens, such as [X], [Y], [F], and [G], to be filled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Examples of attention forcing. Plotting attention scores for all tokens are cluttered, we sample some tokens for visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>[X]/[Y]/[Z]) of the encoding part, grouping [G], binning [B], and sorting [S] should be selected from the columns of the relational table D, while the values (i.e., [F]) of the filter part can be filled by both the columns and cell values of the D. A sample forcing matrix is shown in Figure 7(b), which shows that it does not attend the tokens between [X]/[Y]/[Z] and cell values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: An example of visualization-aware translation. It shows how to select the "best" token from top-3 candidates. The blue path is selected by our algorithm, while the gray paths are ignored by the algorithm. Tokens in specific steps are listed by the probability in descending order. It selects the "most possible" token in specific steps (e.g., line for [T]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 Fig. 9 :</head><label>19</label><figDesc>Fig. 9: The statistics of the nvBench benchmark.</figDesc><graphic coords="7,316.31,158.81,250.94,197.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(b)  shows the distribution of visualizations in different chart types and hardness of visualization queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: The distribution of training and test set. B (Bar Chart), P (Pie Chart), L (Line Chart), S (Scatter Chart) SB (Stacked Bar Chart), GL (Grouping Line Chart), GS (Grouping Scatter Chart).</figDesc><graphic coords="7,317.99,159.65,141.62,97.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 2 3 Fig. 11 :</head><label>1311</label><figDesc>Fig. 11: Using ncNet in Jupyter Notebook. 6.3 Training ncNet nvBench splits the 25,750 (NL, VIS) pairs into a training set with 20,598 pairs, a development set consisting of 1,162 pairs, and a test set with 3,990 pairs. Based on the original training, development, and test sets provided by nvBench, we further process them to satisfy our requirement, as discussed below.First, since nvBench contains some visualizations generated from multiple tables with the join operations, we remove such cases so as to focus on non-join cases in this evaluation. In average, the accuracy join cases is lower than non-join cases for ∼ 3.6%.Second, we have to inject the chart templates into the benchmark to train ncNet for generating visualization result with NL query and chart template as inputs, because nvBench only provides NL query and the corresponding visualization. This step is straightforward: given a (NL, VIS) pair, we assign this (NL, VIS) pair a chart template (e.g., Figure5(b)) based on its chart type and sorting parameter. Hence, each (NL, VIS) pair will result in two (NL, VIS) pairs, one with an empty chart template, and the other with the desired chart template (e.g., a bar chart with sorting y-axis in descending order).Finally, we have training/test sets with 25,238/4,920 (NL, VIS) pairs. More specifically, we show the distribution of training and testing (NL, VIS) pairs in Figure10. We can see that they have similar distributions on chart types and the hardness of visualization queries.</figDesc><graphic coords="7,459.23,288.65,106.34,64.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: User study. creates an instance of ncNet by setting the dataset D variables (Figure 11-). Next, the user can initialize an input interface for the NL query N and the chart template C (Figure 11-). Alternatively, the user can also directly pass the N and C variables to the function input interface(nl query, chart template) . Finally, ncNet will render the best inferred visualization relevant to N and C in Jupyter Lab using Vega-Lite library (Figure 11-).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Input embedding. The three inputs N, C, and D will be converted to a sequence of vectors X (X = embed(N, C, D)) which is the input of the encoder.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Position Embeddings Embeddings Token Embeddings Type</cell><cell>Ep0 E&lt;N&gt; + EtN +</cell><cell>Ep1 Eshow + EtN +</cell><cell>Epi E&lt;/N&gt; + EtN +</cell><cell>Epi+1 Epi+2 E&lt;C&gt; Emark + + EtC EtC + +</cell><cell>Epj E&lt;/C&gt; E&lt;D&gt; Epj+1 + + EtC EtD + +</cell><cell>Epj+2 Etable + EtD +</cell><cell>Epn+1 E&lt;EOS&gt; + E&lt;EOS&gt; +</cell></row><row><cell></cell><cell>&lt;N&gt;</cell><cell>show</cell><cell>&lt;/N&gt;</cell><cell>&lt;C&gt; mark</cell><cell>&lt;/C&gt; &lt;D&gt;</cell><cell>table</cell><cell>&lt;EOS&gt;</cell></row><row><cell></cell><cell></cell><cell>N</cell><cell></cell><cell>C</cell><cell></cell><cell>D</cell><cell></cell></row><row><cell cols="3">(b) Output Probabilities</cell><cell cols="2">softmax</cell><cell>Linear</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">Visualization-aware Translation</cell><cell></cell></row><row><cell cols="4">(d) Visualization-aware translation.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Language-aware Rendering</cell><cell></cell></row><row><cell cols="7">(e) Language-aware rendering to multiple target visualization languages</cell><cell></cell></row></table><note>Inputs(c) A transformer-based encoder block, and an attention forcing technique.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table : candidates</head><label>:</label><figDesc></figDesc><table><row><cell>1 2 3 4 id …</cell><cell>2018-03-15 21:13:57 2018-03-13 13:27:46 2018-03-03 01:50:25 2018-03-10 13:46:48 transaction_date …</cell><cell>613.96 368.46 1598.25 540.73 transaction_amount …</cell><cell>None None None None comment …</cell><cell>details None None None None …</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table : financial_transactions</head><label>:</label><figDesc></figDesc><table><row><cell>transaction_type Payment Payment Refund Payment …</cell></row><row><cell>Expert:</cell></row><row><cell>create a bar chart about the number of candidates by sex with date of birth large than 07.01.1976</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available at https://github.com/Thanksyy/ncNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This project is supported by NSF of China (61925205, 61632016), Beijing National Research Center for Information Science and Technology (BNRist), Huawei, TAL Education, China National Postdoctoral Program for Innovative Talents (BX2021155), China Postdoctoral Science Foundation (2021M691784), and Zhejiang Lab's International Talent Fund for Young Professionals.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>additional chart template is specified, which can be observed from the red and green bar charts in Figure <ref type="figure">13</ref>(b) and (c) that the accuracy in Figure <ref type="figure">13</ref>(c) is higher than Figure <ref type="figure">13</ref>(b). This is obvious especially for those cases of bar, pie and scatter charts.</p><p>ncNet vs. Neural Network-based Baseline. The evaluation of the neural network-based baseline is shown in Figure <ref type="figure">13</ref>(a). The baseline solution achieves an average 65.0% accuracy, while our approaches outperform it by 12.8% and 14.6%, respectively. More concretely, we can compare the performances across different solutions in the heatmaps (Figure <ref type="figure">13</ref>). It shows that ncNet is better than the neural network-based baseline in almost all cases. The result is expected, because ncNet uses several optimization techniques specifically designed for the task of NL2VIS.</p><p>Error Analysis. We conduct an error analysis on those failed cases.</p><p>There are four main causes of evaluation error of ncNet.</p><p>(1) Fail on predicting columns or transform parts. This is the main cause of failure. For mispredicted columns, it is mainly because the NL query mentions desired columns in an ambiguous or implicit way. In some cases, it fails due to the "unperfect" table header (e.g., "StuId" vs. "Student ID"). Incorporating some pre-trained language models (e.g., BERT <ref type="bibr" target="#b9">[10]</ref>) or processing the table headers might help to tackle this issue. The main reason of mispredicting transform parts (e.g., filter) is mainly regard to problem of natural language understand. That is how to successfully convert the intend into right logic operations. For example, as shown in Figure <ref type="figure">12</ref>(a), the NL query mentions "... candidates under the age of 45 ...", it should construct the [F] as "... filter Date of Birth &gt; 07.01.1976 ...". The above issues could be alleviated by developing more advanced natural language understanding models and designing smart strategies for jointly understanding the user intends incorporating database information and visual analysis knowledge.</p><p>(2) Fail on predicting chart type and sorting parameters. This case usually appears when ncNet does not take a chart template as an additional input.</p><p>(3) True result in the rank-k output. Since our quantitative evaluation only considers the rank-1 Vega-Zero returned by ncNet. However, we observe that the ground truth Vega-Zero can be generated at the rank-k (e.g., rank-4) result by ncNet, in some cases. Therefore, this observation points out that the performance of NL2VIS can be further improved by incorporating visualization recommendation techniques, i.e., suggesting top-k visualization results relevant to the NL query.</p><p>(4) Vega-Zero unmatched but visualization result match. Some of predicted Vega-Zero sequences are unmatched with the ground truth Vega-Zero sequences, but their visualization results are in fact equivalent. For example, "mark bar encoding x cars y aggregate count cars ..." equals "mark bar encoding x cars y aggregate count cylinders ..." because they have the same aggregate results used for showing the number of cars in a bar chart.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Amazon's Quicksight</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/cn/blogs/aws/amazon-quicksight-q-to-answer-ad-hoc-business-questions/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Microsoft</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Q&amp;a</surname></persName>
		</author>
		<ptr target="https://docs.microsoft.com/en-us/power-bi/create-reports/power-bi-tutorial-q-and-a" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ai-Driven</forename><surname>Spotiq</surname></persName>
		</author>
		<author>
			<persName><surname>Insignts</surname></persName>
		</author>
		<ptr target="https://www.thoughtspot.com/resources#white_paper" />
		<imprint/>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">NLTK: the natural language toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<editor>N. Calzolari, C. Cardie, and P. Isabelle</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-modal natural language interface to an information visualization environment</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Grinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hibino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Jagadeesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mantilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Speech Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="297" to="314" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text-to-viz: Automatic generation of infographics from proportion-related natural language statements</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="906" to="916" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A pivotal prefix based filtering algorithm for string similarity search. SIGMOD &apos;14</title>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="673" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention forcing for speech synthesis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Efiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event</title>
				<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2020-10-29">25-29 October 2020. 2020</date>
			<biblScope unit="page" from="4014" to="4018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Datatone: Managing ambiguity in natural language interfaces for data visualization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep Learning. Adaptive computation and machine learning</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Introduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Benchmark Handbook for Database and Transaction Systems</title>
				<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
	<note>1st Edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint semantic utterance classification and slot filling with recursive neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLT</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards complex text-to-sql in crossdomain database with intermediate representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Applying pragmatics principles for interaction with visual analytics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dykeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Viznet: Towards A large-scale visualization learning and benchmarking repository</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N S</forename><surname>Gaikwad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Short text similarity with word embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1411" to="1420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Natural language to SQL: where are we today? Proc. VLDB Endow</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1737" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is not only a weight: Analyzing transformers with vector norms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuribayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7057" to="7075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the ACL</title>
				<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the ACL</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Advisor: Automatic visualization answer for natural-language question on tabular data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE Pacific Visualization Symposium, PacificVis 2021</title>
				<meeting><address><addrLine>Tianjin, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">April 19-21, 2021. 2021</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interactive cleaning for progressive visualization through composite questions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th IEEE International Conference on Data Engineering</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>ICDE. IEEE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visclean: Interactive cleaning for progressive visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Empowering natural language to visualization neural translation using synthesized benchmarks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIS</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeptrack: Monitoring and exploring spatio-temporal data -A case of tracking COVID-19</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2841" to="2844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Steerable selfdriving data visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2020.2981464</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepeye: Towards automatic data visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepeye: Creating good data visualizations by keyword search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Synthesizing natural language to visualization (NL2VIS) benchmarks from NL2SQL benchmarks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;21: International Conference on Management of Data, Virtual Event</title>
				<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">June 20-25, 2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepeye: A data science system for monitoring and exploring COVID-19 data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating sequenceto-sequence models for handwritten text recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Labahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grüning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zöllner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence rnns for text summarization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.06023</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">NL4DV: A toolkit for generating analytic specifications for data visualization from natural language queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narechania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VIS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Bradbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepeye: Visualizing your data by keyword search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="441" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Making data visualization more efficient and effective: a survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="117" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Vega-lite: A grammar of interactive graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="341" to="350" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Eviza: A natural language interface for visual analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Battersby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Inferencing underspecified natural language utterances in visual analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Djalali</surname></persName>
		</author>
		<editor>W. Fu, S. Pan, O. Brdiczka, P. Chau, and G. Calvary</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improved beam search diversity for neural machine translation with k-dpp sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collecting and characterizing natural language utterances for specifying data visualizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nyapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Orko: Facilitating multimodal interaction for visual exploration and analysis of networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="511" to="521" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Natural language interfaces for data analysis with visualization: Considering what has and could be asked</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Eurographics Association</publisher>
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
	<note>In EuroVis</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. December 8-13 2014. 2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards democratizing relational data visualization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Management of Data, SIGMOD Conference</title>
				<meeting>the 2019 International Conference on Management of Data, SIGMOD Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>abs/1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Turing test as a defining feature of ai-completeness</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Yampolskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence, Evolutionary Computing and Metaheuristics -In the Footsteps of Alan Turing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Flowsense: A natural language interface for visual data exploration within a dataflow system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TVCG</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1962" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
