<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyeok</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhraneel</forename><surname>Sarma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dominik</forename><surname>Moritz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jessica</forename><surname>Hullman</surname></persName>
						</author>
						<title level="a" type="main">An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">36BB450393C691D9A545FF14B5D86C73</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Task-oriented insight preservation, responsive visualization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visualization authors often transform their designs to accommodate different audience, styles, or display types. For example, authors might simplify charts for audiences with different graphical literacy, altering information conveyed in the new design <ref type="bibr" target="#b14">[14]</ref>. Authors of responsive visualizations create multiple designs for different screen sizes and interactivity <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b43">43]</ref>. However, transforming visualizations may invoke trade-offs between desirable design criteria. For instance, in Fig. <ref type="figure" target="#fig_0">1</ref>, proportionate rescaling (a) makes it harder to compare values along the y-axis due to the reduced absolute height, while increasing the relative height (b) or transposing (c) can distort the shape of the represented distribution. Increasing the bin size (d) reduces possible comparisons and a viewer's ability to see distributional detail.</p><p>Unfortunately, design trade-offs make it difficult to reason about preserving takeaways or "insights" under visualization design transformations. Designers may need to iteratively try out different combinations of strategies like those in Fig. <ref type="figure" target="#fig_0">1</ref> and compare them with the original design <ref type="bibr" target="#b30">[30]</ref>. Kim et al. <ref type="bibr" target="#b43">[43]</ref> identify trade-offs in designing responsive visualization where authors try to strike a balance between maintaining graphical density (i.e., an appropriate number of marks per unit screen size) and the preservation of a user's ability to arrive at certain insights. For example, authors need to decide either to preserve distributional characteristics with higher visual density by rescaling proportionately (a) or to adjust visual density while losing distributional details by changing the bin size (d). Currently these decisions are made manually.</p><p>We contribute an automated approach to approximating the amount of change to task-oriented insights-insights that viewers are likely to be able to obtain from a visualization by performing visual tasksunder design transformation. We define measures that approximate a visualization's support for three low-level visual analytic tasks discussed in the literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>: the viewer's ability to identify a datum, to  compare pairs of data points, and to perceive a multivariate trend. We demonstrate the use of our measures for choosing between alternative transformations of a source large screen visualization in a responsive design context. We provide a prototype automated design recommender for responsive visualization that enumerates responsive design transformations based on an input source view and reasons about changes in task-oriented insights from the source design to each transformed design. Our recommender supports scatterplots, bar charts, line graphs, and heatmaps with position, color, size, and shape encoding channels. We train and test different machine learning (ML) models based on our measures to evaluate their utility for automatically ranking small screen visualization design alternatives given a large screen view. We achieve up to 84% accuracy (via a random forest model) in ranking a set of responsive transformations across a set of six source large screen views spanning different encoding channels. Models trained with our measures outperform two baseline models based on simple heuristics related to chart size changes (59%) and transposing of axes (63%). We discuss implications of our work for future research, including recommender-driven responsive visualization authoring and generalizing our approach to further visualization design domains such as simplification and style transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Responsive Visualization and Design Transformations</head><p>We use visualization design transformation to refer to the transformation of a source visualization specification to a new visualization specification intended to better achieve certain context-specific constraints. These might be screen size limitations for responsive visualization, audience-related constraints in visualization simplification or audience retargeting (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">40]</ref>), or style constraints in style transfer <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref>, for example. Visualization design transformation differs from creating multiple different views from the same dataset (e.g., a visualization sequence or dashboard) in that transformations of an original source view typically are intended to preserve many properties of the source while changing select properties.</p><p>Prior work on responsive visualization, which tends to focus on Webbased communicative visualization, or scalable visualization <ref type="bibr" target="#b13">[13]</ref> more broadly, emphasizes the importance of maintaining intended takeaways between source and transformed views. Analyzing 378 responsive visualization pairs on desktop and mobile devices, Kim et al. <ref type="bibr" target="#b43">[43]</ref> identify density-message trade-offs in responsive visualization where authors need to balance adjusting visual density or complexity for different screen types while maintaining patterns, trends or other important information conveyed in the source view. Focusing on maintaining key information at different scales, earlier work on visualization resizing introduces algorithms that repeatedly remove the pixels determined to be least important <ref type="bibr" target="#b18">[18]</ref> and iteratively minimize scaling in more salient regions <ref type="bibr" target="#b75">[76]</ref>, for example. We extend prior approaches by proposing approximation methods for task-oriented visualization insights.</p><p>Defaulting to simpler views over complex, over-encoded plots is often recommended when exploring or publicizing complex data <ref type="bibr" target="#b41">[41]</ref>. Authors accomplish this through data-level transformations, such as data abstraction, clutter reduction, filtering, or clustering. For example, data abstraction studies have attempted to enhance the simplicity of a view while preserving original structure or insights (e.g., aggregating a large movement dataset <ref type="bibr" target="#b0">[1]</ref>, using interactive dimensionality reduction <ref type="bibr" target="#b39">[39]</ref>, using hierarchical aggregation <ref type="bibr" target="#b19">[19]</ref>, and measuring the quality of an abstraction <ref type="bibr" target="#b14">[14]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization Recommendation</head><p>We discuss two approaches in visualization recommendation-insightbased and similarity-based-that are relevant to our goal of approximating changes in task-oriented insights. Prior work on visualization recommendation employs statistical calculations to characterize properties of a visualization thought to relate to the insights a user can draw from it. Often these 'insights' are intended to capture how well a user can perform analytic tasks, such as recognizing trends or identifying and comparing data points. Tang et al. <ref type="bibr" target="#b65">[65]</ref> suggest detecting 'top-k insights' from data using statistical significance testing (e.g., low pvalue of a linear regression coefficient for slope insight). Similarly, Foresight <ref type="bibr" target="#b17">[17]</ref>, DataSite <ref type="bibr" target="#b15">[15]</ref>, and Voder <ref type="bibr" target="#b61">[61]</ref> use statistics calculated on the data, such as correlation coefficient and interquartile range, and recommend visualization types predicted to better support extracting such information. However, statistics on data are invariant for views sharing the same data set and hence of limited use for comparing different ways of visualizing the same underlying data. Our work instead considers statistics calculated on the rendered visualization.</p><p>Several prior visualization recommenders model similarity between views, but assume a scenario where the underlying dataset changes. GraphScape <ref type="bibr" target="#b45">[45]</ref> offers a view similarity model that assigns costs to visualization pairs that are intended to approximate the cognitive cost of transitioning from one view to another in a visualization sequence. GraphScape applies an a priori cost model in which data transformation (e.g., binning, modifying scales) is always less costly than changes in encoding. Hence, filtering data has a lower cost than transposing axes. However, filtering operations like removing a bar from a bar chart or rescaling a y-axis can significantly change the presumed "takeaways" of a chart (e.g., <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b31">31]</ref>). The space of transformations covered by GraphScape also does not include view size transformations, so it cannot assign costs to changes in aspect ratio common to responsive visualization. Although Dziban <ref type="bibr" target="#b47">[47]</ref> extends GraphScape to suggest a view that is 'anchored' to the previous view for an exploratory data analysis process, it also assumes different subsets of data between the previous and current views and focuses more on similar chart encodings than on preserving task-oriented insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparing Visual Structure by Processing Signal</head><p>Signal processing-based approaches analyze the underlying visual or perceptual structure of a visualization to enable multi-scale visualizations (i.e., providing different insights at different scales) and to Output: ranked targets Fig. <ref type="figure">2</ref>. A pipeline for a responsive visualization recommender enhance visualization effectiveness. Prior work has attempted to enable multi-scale views through perceptual organization analysis of a information graphic at each scale <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73]</ref> and hybrid-image visualization that displays different aggregation levels at different viewing distances <ref type="bibr" target="#b35">[35]</ref>, for example. Signal processing approaches have also been applied to improve the effectiveness of a visualization, for instance, by measuring the difference between the visual salience of a representation and salience of signals in data <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b46">46]</ref>, comparing kernel density estimations between a LOESS curve and different representations <ref type="bibr" target="#b70">[71]</ref>, and extending a structural similarity index for image compression to data visualization <ref type="bibr" target="#b67">[68]</ref>. Signal processing-based approaches have typically been applied to single views, and are generally confined to a predefined set of marks and visual variables (e.g., a line chart, a scatterplot), restricting their applicability for settings like ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>We propose formulating responsive visualization as a search problem from an input source view to transformed target views, following the characterization proposed by Kim et al. <ref type="bibr" target="#b43">[43]</ref>. Consider a recommender that takes a source desktop view as input and returns a ranked set of targets as illustrated in Fig. <ref type="figure">2</ref>. The first step in creating such a recommender is to define a search space that can enumerate well-formed responsive targets. To generate useful target views from a source (large screen) visualization, a search space should cover common transformation strategies in responsive visualization, such as rescaling, aggregating, binning, and transposing <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b43">43]</ref>. After enumerating target views, a responsive visualization recommender should evaluate how well each target preserves certain information or "insights." While the term insight can be overloaded <ref type="bibr" target="#b76">[77]</ref>, a relatively robust way to define insights comes from typologies for describing visualization judgments or patterns <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. These typologies suggest defining insights around common low-level visual analysis tasks like identifying and comparing data. In an automated design recommendation scenario, these task-oriented insights can be approximated by objective functions (i.e., loss measures) that capture support for common tasks, applied to both the source and target view. Finally, the recommender returns the set of target designs based on how well they minimize these loss measures. We formalize this problem and motivate and define three loss measures that we call task-oriented insight preservation measures. In Sect. 5, we describe a prototype visualization recommender in which we implemented the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We define a visualization (or a view), V , as a three tuple</p><formula xml:id="formula_0">V = [D V ,C V , E V ],<label>(1)</label></formula><p>where D V is the data used in V , C V is a visualization specification (defining encodings, chart size, mark type, etc.), and E V is a set of rendered values that we compute our measures on. For example, suppose a bivariate data set with GDP and GNI fields (i.e., D V = {x as a vector of field values and E V .channel as a vector of rendered values in channel. Our notation is also illustrated in Fig. <ref type="figure">3</ref> Given a source view S and a transformation (or target) T, we represent the loss of insight type M from S to T as below:</p><formula xml:id="formula_1">Loss(S → T; M) (2)</formula><p>For example, Loss(S → T; Trend) indicates trend loss from S to T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TASK-ORIENTED INSIGHT PRESERVATION MEASURES</head><p>High level criteria for preserving task-oriented insights of a visualization include preserving datum-level information, maintaining comparability of data points, and preserving the aggregate features <ref type="bibr" target="#b6">[7]</ref>. We use these distinct classes of information to define task-oriented insight loss measures for approximating how well a responsive transformation preserves support for low-level tasks of identifying data, comparing data, and identifying trend. Our goal is to define a small set of measures that capture important types of low-level tasks a designer might wish to preserve in responsive transformation. Each measure should be distinct (i.e., mostly independent of the others) and should improve accuracy when combined with the others (such as through regression or ML modeling) to predict human judgments about how visualization transformations rank. Together, the measures should outperform reasonable baseline approaches based on simple heuristics. While chosen to cover three important classes of low-level analytic task, the measures we describe are not meant to be exhaustive, as there are many ways one could approximate support for task-oriented insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Identification Loss</head><p>Responsive visualization strategies often alter the number of visual attributes of marks that viewers can identify (affecting a low-level identification task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>). As illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>, when the number of bin buckets of a histogram is decreased in a mobile view (a), each bar encodes more information on average than in the desktop view, such that some information about the distribution is lost. Similarly, strategies to adjust graphical density, like aggregating distributions (b) and filtering certain data (c), also reduce the number of identifiable attributes. We use identification loss to refer to changes to the identifiability of rendered values between a source view and a target. Information theory, and in particular Shannon Entropy (entropy, hereafter) captures the information in a signal by measuring the minimum number of bits needed to encode it <ref type="bibr" target="#b59">[59]</ref>. Given a random variable X, entropy is defined as H(X) = − ∑ x∈X P(x) log 2 P(x). Applying this to visualization, suppose that for a source visualization S, a vector for data field f , D S .f = {x 1 .f,...,x n .f}, is mapped to an encoding channel c. The corresponding rendered values E S .c = {e 1 .c,...,e n .c} compose a random variable U S .c that takes the set of unique values of E S .c as its outcome space, where the probability of U S .c taking x is defined as the relative frequency of x in E S .c, formalized as</p><formula xml:id="formula_2">P(U S .c = x) = Count i (e i .c = x)/n (3) H(E S .c) = − ∑ x P(U S .c = x) log 2 P(U S .c = x) (4)</formula><p>We can similarly compute the probabilities of rendered values, P(U T .c), and the entropy of an encoding channel, H(E T .c), for a target view T. Finally, we can calculate the identification loss for the channel as the absolute difference in entropy (i.e., |H(E S .c) − H(E T .c)|), where 0 difference is the identity. The final identification loss from S to T is the sum of absolute differences in entropy for each encoding channel c between the two views:</p><formula xml:id="formula_3">Loss(S → T; Identification) = ∑ c |H(E S .c) − H(E T .c)|, (<label>5</label></formula><formula xml:id="formula_4">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison Loss</head><p>Responsive transformations like resizing or scaling a view or aggregating data can alter the number of possible data comparisons that a user can make and how perceptually difficult they are (affecting a low-level comparison task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>). For instance, in Fig. <ref type="figure">5</ref>, resizing (a) diminishes the magnitude of difference between two highlighted data points in the small screen design. In a mobile design with aggregation (b), viewers are no longer able to make each comparison that is available in the large screen view. This motivates estimating how similarly viewers are able to discriminate between pairs of points in a target view compared to the source view, which we refer to as comparison loss.</p><p>Empirical visualization studies (e.g., <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b64">64]</ref>) often operationalizes accuracy as the viewer's ability to perceive relationships between pairs of values. While simpler scalar statistics like a sum or mean might suffice under some transformations, a method that preserves the distribution of distances will be more robust to transformations that change the number of data points or scales (e.g., log-scale). We operationalize comparison loss as the difference in pairwise discriminability, measured using Earth Mover's Distance (EMD), between the source and a target in each encoding channel used in a visualization:</p><formula xml:id="formula_5">Loss(S → T; Comparison) = ∑ c EMD(B S .c, B T .c ),<label>(6)</label></formula><p>where B S .c and B S .c are the discriminability distributions of the source and target views in encoding channel c and c , respectively, that encode the same data field. Given a source visualization S, we define the discriminability distribution B S .c, of an encoding channel c for a view S, as the set of distances between each pair of rendered values (E S .c) of S in terms of c. This is formalized as</p><formula xml:id="formula_6">B S .c = {d c (e i .c, e j .c) : e i .c, e j .c ∈ E S .c}, (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where d c (•, •) is a distance metric for the encoding channel c. Distance metrics: Ideally, comparison loss should account for differences in how well visual channels support perception of numerical values. Informed by visual perception models, we select several distance metrics intended to provide a rough proxy of the perceptual difference between two visual signals. While visual variables can have interaction effects <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b64">64]</ref>, for simplicity in demonstrating our approach, we limit our use of perceptual distance metrics to encoding channel specific measures. However, as the state-of-the-art in predicting effects of visual variable interactions develops, our approach could be amended to consider combinations.</p><p>For position channels, we use the absolute difference between two position values (in pixel space), as human vision is highly accurate in discriminating positions according to Stevens' power law <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b63">63]</ref> and empirical studies <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>:</p><formula xml:id="formula_8">d position (e i .</formula><p>position, e j .position) = |e i .position − e j .position| <ref type="bibr" target="#b7">(8)</ref> We measure distance in a size channel using the absolute difference between two size values (in pixel) raised to the estimated Stevens' exponent of 0.7 <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b63">63]</ref>:</p><formula xml:id="formula_9">d size (e i .size, e j .size) = |e i .size − e j .size| 0.7<label>(9)</label></formula><p>We calculate the Euclidean distance in the perceptual color space CIELAB <ref type="bibr" target="#b20">[20]</ref>  Lastly, for shape encodings, we employ a perceptual kernel <ref type="bibr" target="#b16">[16]</ref>, a (symmetric) matrix of pairwise distances between visual attributes. The i, j-th element in the perceptual kernel for shape is the empirical probability of discriminating shape i from shape j based on an online crowdsourced experiment in which workers completed a triplet discrimination task where they chose the most dissimilar shape out of three shapes. Formally, our shape distance metric can be stated as: d shape (e i .shape, e j .shape) = P(e i .shape is discriminated from e j .shape) <ref type="bibr" target="#b10">(11)</ref> Comparing discriminability distributions: To quantify the discrepancy between the discriminability distributions of encoding channel c and c (mapping the same field) for the source S and target T (i.e., B S .c and B T .c , respectively), we compute Earth Mover's Distance <ref type="bibr" target="#b68">[69]</ref> (EMD or Wasserstein distance). We use EMD, which measures the minimum cost to transform a distribution to another distribution, because it is non-parametric, symmetric, and unbounded. An EMD of 0 is the identity, and the greater the EMD is, the more different the two distributions are. Thus, the comparison loss between the source view S and a target view T is the sum of the EMD between their discriminability distributions in each encoding channel, formalized in Equation <ref type="formula" target="#formula_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Trend Loss</head><p>Responsive transformations like disproportionate rescaling and changes to binning may impact the implied relationship (or trend) between two or more variables represented in a target view compared to the source view (affecting low-level trend identification <ref type="bibr" target="#b6">[7]</ref>). As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, different aspect ratios can alter the magnitude of the slope of a trend, and modifying bin size affect the amount of distributional information available. We use trend loss to refer to changes in the implied trend from the source to a target.</p><p>To capture representative data patterns while avoiding influences of noise, our trend loss first estimates trend models between the source and target views using LOESS. We then compare the area (or volume) of the estimated trends because it is more sensitive to details that simpler methods (e.g., the difference between regression coefficients) might ignore. We define trend models for the quantitative encoding channels in our scope (position, color and size): • e y ∼ e x : a 2D trend of y on x as appears in a simple scatterplot, line chart, or bar graph.</p><p>• e color ∼ e x + e y : a 3D trend of color on x and y like a heatmap or a scatterplot with a continuous color channel</p><p>• e size ∼ e x + e y : a 3D trend of size on x and y (e.g., a scatterplot with a continuous size encoding)</p><p>After calculating trend models for a source and target, we can define trend loss as the sum of the relative area between curves (or volume between surfaces) of the estimated trends in each trend model (m). This is formalized as:</p><formula xml:id="formula_10">Loss(S → T; Trend) = ∑ m A(LOESS(m S ), LOESS(m T ))<label>(12)</label></formula><p>where A stands for the relative area between curves (ABC) between the source and target trends (m S and m T ), normalized by dividing by the area under the curve of the source trend for a 2D model. For a 3D model, A is the relative volume between surfaces (VBS), which is the VBS of the source and target trends divided by the volume under the surface of the source trend. We estimate the trend models using LOESS regression <ref type="bibr" target="#b12">[12]</ref> as it is non-parametric. We use uniform weights and bandwidth of 0.5 <ref type="bibr" target="#b12">[12]</ref>. LOESS regression returns an estimate at each observed value of the independent variable(s) (as an array of coordinates): an estimated curve for a 2D model and an estimated surface for a 3D model. Thus, when source and target views have different chart sizes or different sets of rendered values for the independent variable(s), it is difficult to directly compare the LOESS estimations. As shown in Fig. <ref type="figure" target="#fig_6">7a</ref>, we first standardize the chart sizes of two views by rescaling an estimated LOESS curve or surface in a target view to have the same chart width with the source. Then, we interpolate the LOESS curve to have equal When a nominal encoding divides points into subgroups, we match those subgroups in source and target views.</p><p>We recursively accumulate the distances between each consecutive pair of rendered values. LS is large screen, and SS is small screen. data("economy.csv") fieldtype(gdpc,number). mark(bar). width(600). height(300). encoding(e0). field(e0,gdpc). bin(e0,10). channel(e0,x). encoding(e1). aggregate(e1,count).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source (large screen) 1</head><p>Enumerate targets (small screen) 2</p><p>Evaluate targets 3 Outcome 4 data("economy.csv"). fieldtype(gdpc,number). encoding(e0). field(e0,gdpc). encoding(e1). aggregate(e1,count). mark_source(bar). bin_source(e0,10). channel_source(e0,x). channel_source(e1,y). distances between two consecutive coordinates for a 2D model (Fig. <ref type="figure" target="#fig_6">7b</ref>). We interpolate on 300 breakpoints in a 2D model by default, where one breakpoint corresponds to one to three pixels in many Web-based visualizations. For a 3D model, we interpolate 300 × 300 breakpoints from a LOESS surface in a similar way. Given these interpolations for the LOESS curves (or surfaces) in the source and target, we obtain the ABC (or VBS) segment at each breakpoint. Subgroups: When a nominal variable encoded by color or shape divides the data set into subgroups, viewers might naturally consider each subgroup's trend independently. To distinguish trends implied by subgroups, we first identify and match subgroups which occur in both the source and target views by looking at their nominal data values, as depicted in Fig. <ref type="figure" target="#fig_6">7b</ref>. Then, we compute the relative ABC (or VBS) of each subgroup and combine them by taking their average. Color scale linearization: Although a continuous color scale encodes a unidimensional vector, color is often modeled on a multi-dimensional space (e.g., RGB, CIELAB), which makes it complex to estimate a LOESS surface. Similar to how common color schemes such as viridis or magma are designed to be perceptually uniform by keeping equidistance in a perceptual color space between two consecutive color points <ref type="bibr" target="#b66">[67]</ref>, we can make use of the Euclidean distance between rendered color values in CIELAB to linearize a 3D color scheme. Specifically, we recursively accumulate the distances between each consecutive pair of rendered values to create a unidimensional vector. In Fig. <ref type="figure" target="#fig_6">7c</ref>, we show how the linear value of i-th color is computed from that of i − 1-th point; we take the calculated value of the i − 1-th point and add to it the distance between the i − 1-th and i-th points. The first color point is assigned as zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vega-Lite Rendering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PROTOTYPE RESPONSIVE VISUALIZATION RECOMMENDER</head><p>To implement our task-oriented insight preservation measures, we developed a prototype responsive visualization recommender that enumerates and evaluates responsive designs (or targets). As shown in Fig. <ref type="figure" target="#fig_7">8</ref>, given an input source (large screen) view, our recommender first converts it to a partial specification, and then generates a search space of small screen targets based on the partial specification. We adopt the desktop-first approach that visualization authors have described using <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b43">43]</ref>. Finally, the recommender computes our measures between the source view and each target to rank those targets using an ML model trained on human-labeled rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Enumerating Target Views</head><p>To enumerate target views, we need a formal grammar for representing visualization specifications and formulating a search space. We use Answer Set Programming (ASP) <ref type="bibr" target="#b7">[8]</ref>, particularly by modifying Draco <ref type="bibr" target="#b50">[50]</ref>. ASP is a declarative programming language for complex search problems (e.g., satisfiability problems) that encodes knowledge as facts, rules, and constraints. Rules generate further facts, and constraints prevent certain combinations of facts. Formalized in ASP, for example, Draco has a rule that if an encoding is binned, then it is discrete, and a constraint that disallows logarithmic scale on a discrete encoding <ref type="bibr" target="#b50">[50]</ref>. A constraint solver then solves an ASP program (the partial specification of a source view and our search space), returning stable sets of non-conflicting facts (enumerated target views with different transformation strategies). We use Clingo <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref> as our solver.</p><p>Converting to a partial specification: Our recommender converts the full specification of an input source view to a partial specification to allow applying responsive transformation strategies. We maintain the data specification (data file, data field definitions, and the number of rows) and encoding information (e.g., count aggregation, association of data field) that are not changed under transformation. We indicate the rest of the specification (mark type, chart size, and encoding channels) as information about the source view to constrain responsive transformation strategies (e.g., constraining possible mark type replacement, allowing for swapping position encodings for axis-transpose). Generating a search space: Our goal in generating a search space is to produce a set of reasonable targets that a responsive visualization author might consider given a source view. We generate a search space by automatically applying responsive visualization transformations recently observed in an empirical study of common responsive visualization design strategies <ref type="bibr" target="#b43">[43]</ref> to a source visualization. Our prototype implements rescaling, aggregation, binning, transposing, and select changes to marks and encodings. For rescaling, we fix the width of target views and vary heights, in the range from the height resulting from proportionate rescaling to the height that forms the inverse aspect ratio with an increment of 50 px. For example, if the source view has a width of 600 px and a height of 300 px (an aspect ratio of 2:1) and the width of target views is fixed at 300 px, then the height varies from 150 px (2:1) to 600 (1:2) by 50 px (i.e., 150, 200, . . . , 550, 600 px). Given a disaggregated source view, we generate alternatives by applying binning (max bin buckets of 25, 15, and 5) and aggregation (count, mean, median, sum) as graphical density adjustment strategies. We also generate alternatives by transposing axes (i.e., swapping x and y position channels). Finally, in line with the observation of prior work that responsive visualization authors occasionally substituted mark types when adding an encoding channel for aggregation, we allow a mark type change in scatterplots from a point mark to a rectangle (heatmap). We formulate these strategies in ASP format and add them to Draco <ref type="bibr" target="#b50">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating and Ranking Targets</head><p>To evaluate enumerated targets, we calculate our loss measures on rendered values after rendering source and target views using Vega-Lite <ref type="bibr" target="#b56">[56]</ref>. Then, we obtain rendered values, E V , of a visualization V by gleaning Vega <ref type="bibr" target="#b57">[57]</ref> states (a set of raw rendered values [66]). We implemented the loss measures in Python using SciPy <ref type="bibr" target="#b69">[70]</ref>'s stats.entropy and stats.wasserstein distance methods for entropy and EMD, respectively. To compute LOESS regression, we use the LOESS package <ref type="bibr" target="#b10">[11]</ref>. Finally, to rank the enumerated targets, we combine the computed loss values by training ML models, which we detail in Sect. 6. We use ML models for ranking instead of formalizing them in ASP because our measures are not declarative (not rule-based).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Examples</head><p>We introduce two example cases of transformations generated by our prototype and describe how our measures distinguish target views. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Case 1: Simple scatterplot</head><p>In the source scatterplot (Fig. <ref type="figure" target="#fig_8">9a</ref>), each point mark represents a country, and x and y positions encode Gini coefficients and annual growth rate of GDP per capita of different countries, respectively. The first example transformation (Ta1) is simple resizing. The second target view (Ta2) is transposed from the source view while keeping the size. The third and fourth target views (Ta3 and Ta4) are resized, binned in x and y scales, and aggregated by count, so the size of each dot represents the number of data points in the corresponding bin bucket. In the fifth target (Ta5), the mark type is changed from point to rectangle in addition to resizing, binning, and aggregating, and the color of each rectangle encodes the number of data points in that cell.</p><p>Because Ta1 and Ta2 perfectly preserve the number of identifiable rendered values, identification loss is zero. Ta4 and Ta5 have more identifiable points than Ta3 (due to their smaller bin size), so they have smaller identification loss. While Ta1 has disaggregated values, Ta4 better preserves the distances between points in terms of position encoding, so it has smaller comparison loss. Compared to the source view, the implied trend given x and y positions in Ta1 has a more similar slope and hence smaller trend loss than Ta2, whereas Ta2 preserves the differences in the position encodings, resulting in zero comparison loss. Similarly, Ta3 has a smaller trend loss than Ta4 because Ta3 better preserves the visual shape of the distribution in the source view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Case 2: Histogram</head><p>The source histogram in Fig. <ref type="figure" target="#fig_8">9b</ref> shows the distribution of GDP per capita of different countries. There are 23 bins along the x axis and each bar height (y position) represents the number of countries in the corresponding bin. The first target view (Tb1) is resized. The second and third target views (Tb2 and Tb3) are transposed with different resizing. In the fourth and fifth target views (Tb4 and Tb5) bin sizes are changed from (23 to 10 and 5, respectively), with Tb5 transposed.</p><p>As Tb1, Tb2, and Tb3 have no changes in binning, they have zero identification loss, whereas Tb4 and Tb5 has greater identification loss proportional to their bin sizes. While Tb1, Tb2, and Tb3 have the same binning, Tb3 has the most similar differences between bar heights and bar intervals in pixel space, so it has the smallest comparison loss among them. Transposing axes (Tb3) better preserves the resolution for comparison (i.e., chart height and width), often resulting in the smaller comparison loss than other similarly transformed targets. Tb5 has smaller trend loss than Tb4 as it shows a similar aspect ratio to the source view, though inverted, as implied by x and y positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MODEL TRAINING AND EVALUATION</head><p>A responsive visualization recommender should combine loss measures to rank a set of targets by how well they preserve task-oriented insights. For our prototype recommender, we train machine learning models to efficiently combine our loss measures and rank enumerated targets. We describe training data collection, model specification, and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Labeling</head><p>We obtained training and test data consisting of ranked target views for a set of source views using a Web-based task completed by nine visualization experts. As shown in Fig. <ref type="figure" target="#fig_0">10a</ref>, each labeler was assigned one out of three trial sets and performed 36 trials, with each trial asking them to rank five target transformations (small screen) given a source visualization (large screen). Task materials: To create instances for labeling, we selected six desktop visualizations (source views) as shown in Fig. <ref type="figure" target="#fig_8">9c</ref>. Our goal was to include different chart types, multiple encoding channels for identification and comparison losses, and different types of examples for trend loss (e.g., 2D/3D models, subgroups, color scale linearization). Our prototype generates 60 to 620 target transformations (2,120 in total) for these six source views. We generated three sets of 30 target views per source view for labeling, using quintile sampling per preservation (loss) measure, to ensure relatively diverse sets of targets. After sorting targets in terms of each of our three measures, we sampled two targets from each quintile of the top 100 targets per measure, as depicted in Fig. <ref type="figure" target="#fig_0">10c</ref>. We took the top 100 targets after inspecting the best ranked views per measure for each source view, to avoid labeling examples that might be obviously inferior. Because identification loss is measured using entropy and is primarily affected by how data are binned, certain source views had fewer than five unique discrete values within top 100 targets. In this case, we proportionately sampled each discrete value.</p><p>After sampling 30 targets for a source view in a trial set, we randomly divided them into six trials (but fixed these trials between labeler in the same trial set), so we had 1,080 pairs (1,077 unique pairs 1 ) labeled by three people each. We randomly assigned each trial set to labeler (3 trial sets (between) × 6 source views × 30 targets (within), Fig. <ref type="figure" target="#fig_0">10a</ref>).</p><p>Labelers: All five authors, who have considerable background in visualization design and evaluation, and an additional convenience sample of four visualization experts (representing postdoctoral researchers and graduate students in visualization) participated in labeling. All labelers worked independently.</p><p>Labeling task: Each labeler was asked to imagine that they were a visualization designer for a responsive visualization project, tasked with ranking a set of small screen design alternatives created by transforming the source. Their goal was to consider what would be an appropriate small screen design that would also preserve insights or takeaways conveyed in the desktop version as much as possible.</p><p>The study interface is shown in Fig. <ref type="figure" target="#fig_0">10b</ref>. Each labeler completed 36 trials (6 desktop visualizations × 6 sets of 5 smartphone design candidates). In each trial, the desktop visualization and five smartphone design candidates were shown, and labeler ranked the candidates by dragging and dropping them into an order. Trial order was randomized. Aggregating labels: From the task, we collected human-judged rankings of 1,080 pairs each of which was ranked by three labelers. To produce our training data set, we aggregated the three labels obtained from the three labelers of each pair into a single label representing the majority opinion, such that that for the i-th pair</p><formula xml:id="formula_11">x i = (x (1) i , x (2) i ), the label y i is 1 if x (1)</formula><p>i is more likely to appear higher than x (2) i , and −1 otherwise.</p><formula xml:id="formula_12">y i = 1, if x<label>(1)</label></formula><p>i more often appears higher than x</p><formula xml:id="formula_13">(2) i −1, otherwise<label>(13)</label></formula><p>To avoid a biased distribution of training data as well as minimize the ordering effect within each pair, we randomized the order of pairs so that half of the pairs are labeled as 1 and the other half as −1, which naturally sets the baseline training accuracy of 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model Description</head><p>Prior approaches to visualization ranking problems (e.g., Draco-Learn <ref type="bibr" target="#b50">[50]</ref>, DeepEye <ref type="bibr" target="#b48">[48]</ref>) utilize ML methods that convert the ranking problem to a pairwise ordering problem, such as RankSVM (Support Vector Machine) <ref type="bibr" target="#b29">[29]</ref> and the learning-to-rank model <ref type="bibr" target="#b9">[10]</ref>; we adopt a similar approach. A model, f , takes as input a pair of objects, x = (x (1) , x (2) ), and returns their orders (i.e., either x (1) or x (2) ranks higher).</p><p>f (g(x (1) , x (2) )) = 1, if x (1) appears higher in the ranking −1, if x (2) appears higher in the ranking , ( where g(•, •) is a mapping function that combines the features from a pair of objects. We consider vector difference and concatenation for g. Our models take two target views representing transformations of the same source view and return the one with higher predicted ranking, as depicted in Fig. <ref type="figure" target="#fig_7">8</ref>.3.</p><p>Features: We define the feature matrix X ∈ R n×d where each row corresponds to a pair of target visualizations and columns represent the features (converted by g). We use our proposed loss measures as the features (Table <ref type="table" target="#tab_3">1</ref>). Aggregated features (A) refer to our three loss measures: identification, comparison, and trend loss, as described in Equations 5, 6, and 12 (Sect. 4). Disaggregated features (D) refer to the components of the aggregated features (e.g., the EMD value in each encoding channels for comparison loss). We standardized all features.</p><p>Model training: We train SVM with a linear kernel, K-nearest neighborhood (KNN) with k = 1, 10, logistic regression, decision tree (DT), and a Multilayer Perceptron (MLP) with four layers and 128 perceptrons per layer, similar to other recent applications of ML in data visualization (e.g., Hu et al. <ref type="bibr" target="#b32">[32]</ref>, Luo et al. <ref type="bibr" target="#b48">[48]</ref>). We also train ensemble models of DTs: random forest (RF) with 50, and 100 estimators, Adaptive Boosting (AB), and gradient boosting (GB). Given the moderate number of observations (1,067) in our data set, we use leave-one-out (LOO) as a cross validation iterator to obtain robust training results. We used Scikit-Learn <ref type="bibr" target="#b51">[51]</ref> for training.</p><p>Baselines: In addition to the natural baseline of 50% (random), we include two simple heuristic-based baselines to evaluate the performance of our models. The first baseline (B1) includes the changes in chart width and height between a target and its source, capturing an intuition about maintaining size and aspect ratio. The second baseline (B2) is whether x and y axes are transposed, capturing an intuition that, of the strategies in our search space, transposing is the most drastic change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>All the experimental materials, and files used for analysis are included in the supplementary materials, available at https://osf.io/jcvbx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Rank correlation between loss measures</head><p>To ensure that our loss measures capture different information about transformations, we compute and inspect rank correlations between each pair of aggregated, and each pair of disaggregated measures. If two different loss measures produce highly similar rankings of target views, then one of them might be redundant. Our measures tend to be orthogonal to each other (see Fig. <ref type="figure" target="#fig_10">11</ref>), with Kendall rank correlation coefficients <ref type="bibr" target="#b42">[42]</ref> between −0.41 and 0.47. The same pattern is observed for the disaggregated measures with overall correlation coefficients mostly between −0.5 and 0.5 (see supplementary material).</p><p>When the chart type of a source view allows a few, limited responsive transformation strategies due to its own design constraints  (e.g., line chart, heatmap), the correlation between measures appear slightly higher than the other chart types. For example, it is often impossible to add a new encoding channel through aggregation or binning in a line chart. This makes the line chart more sensitive to chart size changes, resulting in relatively higher negative rank correlation between comparison and trend loss. Similarly, different binning levels in a heatmap can affect both one's ability to identify data points in different encoding channels and to recognize a trend implied by x and y on color channels (i.e., e color e x + e y ), leading to a slightly higher positive rank correlation between identification and trend loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Monotonicity</head><p>Ranking problems through pairwise comparison assume that the partial rankings used as input are consistent with the full ranking (monotonicity of rankings) <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b36">36]</ref>. In other words, we need to ensure that the partial pairwise rankings that we calculate based on aggregated expert labels can yield a monotonic full ranking. Comparison sorting algorithms can be used to determine whether a monotonic full ranking can be obtained from pairwise rankings, as a comparison sort will only result in a monotonic ranking if the principle of transitivity (a &gt; b ∧ b &gt; c =⇒ a &gt; c) and connexity (∀a and b, a ≤ b ∨ b ≤ a) hold.</p><p>To confirm whether our expert labels satisfy the monotonicity assumption, we first sort the five target views in each of our 108 trials, using the ten aggregated pairwise rankings as a comparison function. Next, we check whether each consecutive pair in the reproduced ordering conflicts with the aggregated expert labels, because if a pair in the reproduced ordering is not aligned with the aggregated label, that trial violates the monotonicity assumption. 102 out of 108 trials (94.44%) in our data set had fully monotonic orderings. Of the six orderings which are not fully monotonic, five are partially monotonic with only one misaligned pair each (out of the ten ordered pairs). The other non-monotonic ordering (a trial with line chart as the source view) had multiple conflicts; we dropped this ordering from our training data, resulting in 1,070 training pairs (1,067 unique training pairs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Training results</head><p>Model performance: Overall, our models with disaggregated (D) and aggregated features (A) achieved prediction accuracy greater than 75% (Table <ref type="table" target="#tab_4">2a</ref>), showing the utility of our measures in ranking responsive design transformations. Ensemble models (RF, AB, and GB) with D features resulted in the highest overall accuracy (above 81%) because they iterate over multiple different models and we have a relatively small number of features. In particular, RF with D and 100 estimators showed highest accuracy of 84%. Our neural network model (MLP) also provided comparable performance to the ensemble models.</p><p>Models with D features in general obtained higher accuracy than A features, and combining them (D + A) did not provide significant gain in accuracy. Although they had only three features, our models with A features showed reasonable accuracy of up to 77.4% (g = concatenate) and 76.4% (g = difference). For mapping functions, concatenation performed slightly better than difference for our best performing models (RF). A features performed much higher with concatenation than difference possibly due to the very small number of features.</p><p>Our models all outperformed those with both baselines features (B1 and B2), indicating that our loss measures capture information that simple heuristics, such as changes in chart size or axes transposition, are unable to capture. When we trained the best performing model (RF) with the features of only a single loss criterion (e.g., only trend), accuracy ranged from 52.6% to 79.7%, implying that our measures are more useful when combined than when used individually. As hypothetical upper bounds for accuracy, training and testing the model on the same data set resulted in accuracy from 84% (KNN) to 100% (RF). Feature importance: To understand how the different loss measures function in our models, we inspected the importance of each disaggregated feature (mapping function g = difference) using the impuritybased importance measure (average information gain) by training a random forest model with 50 estimators (average over 10 training iterations). As shown in Table <ref type="table" target="#tab_4">2b</ref>, features related to position encodings (x, y, e x ∼ e y ) in general seem to have higher importance, which makes sense given their ubiquity in our sample. Predicted rankings of example cases: Using the best prediction model (RF with 100 estimators), we predicted the rankings of example cases described in Sect. 5.3. Transformations from the simple scatterplot example (Fig. <ref type="figure" target="#fig_8">9a</ref>) are ranked as: Ta1 (resizing), Ta2 (transposing axes), Ta4, Ta3 (binning, resizing, aggregation), and Ta5 (binning, resizing, aggregation, mark type change). Ta1 appears higher in ranking than Ta2 because Ta2 has higher trend loss, while Ta1 slightly sacrifices comparison loss. Ta4 is ranked in a higher position than Ta5 because Ta5 has higher comparison and trend loss. Responsive transformations from the histogram example (Fig. <ref type="figure" target="#fig_8">9b</ref>) are ranked as: Tb2 (transposing axes, resizing), Tb3 (transposing axes, resizing), Tb1 (resizing), Tb5 (resizing, changing bin size), and Tb4 (resizing, changing bin size). The transposed views (Tb2 and Tb3) are ranked higher than Tb1 probably because the model has more emphasis on comparison loss as the feature importance (Table <ref type="table" target="#tab_4">2b</ref>) shows. The ordering between Tb5 and Tb4 can be backed by the smaller trend loss of Tb5 while the difference in comparison loss between them appears subtle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND FUTURE WORK 7.1 Extending and Validating Our Preservation Measures</head><p>We devised a small set of measures for three common low-level tasks in visualization use and found that they can be used to build reasonably well-performing ML models for ranking small screen design alternatives given a large screen source view. Our measures are not strongly correlated, and removing some of the measures results in lower predictive accuracy. However, there are other forms of prominent taskoriented insights that could extend our approach if approximated well, such as clustering data points or identifying outliers. As our measures lose information by processing rendered values, future work could estimate task-oriented insights with different methods, such as extracting and directly comparing image features from rendered visualizations.</p><p>There are also opportunities to strengthen and extend our measures through human subject studies. These include more formative research with mixed methods to understand heuristics and other strategies that visualization authors and users employ to reason about how well a design transformation preserves important takeaways. In addition, future work could conduct perceptual experiments that more precisely estimate human baselines for identification, comparison, and trend losses. We also used simple approximations of perceptual differences in position, size, and color channels which could be improved through new experiments specifically designed to understand how perception is affected on smaller screen sizes, adding to work like examining task performance on smaller screens by different chart types <ref type="bibr" target="#b5">[6]</ref> and comparing task performance between small and large screens <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. A limitation is that our experiment was conducted on desktop devices. Future work could test on mobile devices, as well as explore mobilefirst design contexts, as our measures are designed to be symmetric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Responsive Visualization Authoring Tools</head><p>Our work demonstrates how task-oriented insight preservation can be used to rank design alternatives in responsive visualization. To do so, we formulated and evaluated our insight preservation measures on a search space representing common responsive visualization design strategies and mark-encoding combinations. However, our work should be extended in several important ways to support a responsive visualization authoring use case.</p><p>First, while more drastic encoding changes than those supported by our generator are rare in practice <ref type="bibr" target="#b43">[43]</ref>, this might be because responsive visualization authoring is currently a tedious process and authors satisfice by exploring smaller regions of the design space. There are many strategies that could be added to a search space like the one we defined, and used to evaluate our measures as well as to learn more about how authors react when confronted with more diverse sets of design alternatives. For example, while we mainly consider singleview, static visualizations, many communicative visualizations employ multiple views and interactivity <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b58">58]</ref>. Ideally a responsive visualization recommender should be able to formulate related strategies (e.g., rearranging the layout of multiple views, omitting an interaction feature, editing non-data ink like legends). Recommenders may need to consider further conditions such as consistency constraints for multiple views <ref type="bibr" target="#b52">[52]</ref>, effectiveness of visualization sequence <ref type="bibr" target="#b45">[45]</ref>, semantics of composite visualizations <ref type="bibr" target="#b38">[38]</ref>, and effectiveness of interactive graphical encoding <ref type="bibr" target="#b55">[55]</ref>. As indicated in Kim et al. <ref type="bibr" target="#b43">[43]</ref>, loss measures should be able to address concurrency of information because rearranging multiples views (e.g., serializing) can make it difficult to recognize data points at the same time on small screen devices. In addition, they should also account for loss of information that can only be obtained via user interaction (e.g., trend implied by filtered marks).</p><p>We envision our measures, and similar measures motivated to capture other task-oriented insights, being surfaced for an author to specify preferences on in a semi-automated responsive visualization design context. Because what our measures capture is relatively interpretable, authors may find it useful to customize them for certain design tasks, such as prioritizing one measure or changing how information is combined to capture identification, comparison, or trend loss. This is a strength of our approach relative to using a more "black-box" approach where model predictions might be difficult to explain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Extending ML-based approaches</head><p>The human labelers in our experiment, including the authors, seemed to at times use strategies or heuristics such as preferring non-transposed views in their rankings or trying to minimize changes to aspect ratio for some chart types. However, models with our loss measures as features perform better than heuristic approaches like detecting axes-transpose and chart size changes, implying that task-oriented insights may be the right level at which to model rankings. As an extension, future work might learn pre-defined costs for different transformation strategies to reduce the time complexity of evaluating task-oriented insights preservation, similar to the approach adopted by Draco-Learn <ref type="bibr" target="#b50">[50]</ref> which obtained costs for constraint violation. Learning such pre-defined costs may also enable better understanding how each responsive design strategy contributes to changes in task-oriented insights. An alternative approach could be to use our loss measures as cost functions and optimize different strategies to reduce them as MobileVisFixer <ref type="bibr" target="#b73">[74]</ref> fixes a non-mobile-friendly visualizations for a mobile screen by minimizing heuristic-based costs. As recent deep learning models <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b74">75]</ref> have performed well in visualization ranking problems, future work may further elaborate on those models. In doing so, one could combine our measures with image features (e.g., ScatterNet <ref type="bibr" target="#b49">[49]</ref>) or chart parameters (e.g., aspect ratio, orientation <ref type="bibr" target="#b74">[75]</ref>).</p><p>As noted in Sect. 6.3.2, there were a few partial and not fully monotonic orderings in our data set. A better model might ignore this assumption and try to identify highly recommendable transformations or classify them into multiple ordinal classes, yet this might come up with lower interpretability about recommendations due to a lack of explicit ordinal relationship between transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Generalizing Our Measures to Other Design Domains</head><p>Our approach to task-oriented insight preservation is likely to be useful in visualization design domains beyond responsive visualization, like style transfer and visualization simplification, although other domains may also require different transformation strategies. Style transfer, for instance, may involve techniques like aggregation but is more likely to change visual attributes of marks or references. While our loss measures are designed to be low-level enough to apply relatively generically to visualizations, their precise formulation and the combination strategy might warrant changes in other domains. For example, in visualization simplification, minimizing trend loss is likely to be more important than preserving identification and comparison of individual data points. Style transfer often focuses on altering color schemes, size scales, or mark types <ref type="bibr" target="#b24">[24]</ref> which can result in different discriminability distributions, so it might put more emphasis on comparison loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Responsive visualization transformations often alter task-oriented insights obtainable from a transformed view relative to a source view. To enable automated recommenders for iterative responsive visualization design, we suggest loss measures for identification, comparison, and trend insights. We developed a prototype responsive visualization recommender that enumerates transformations and evaluates them using our measures. To evaluate the utility of our measures, we trained ML models on human-produced orderings that we collected, achieving accuracy of up to 84.1% with a random forest model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example responsive transformations for small screen generated from a large screen design: (a) proportionate rescaling, (b) disproportionate rescaling, (c) transposing axes, and (d) increasing bin size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Responsive transformations that may cause identification loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(c) 15 Fig. 5 .</head><label>155</label><figDesc>Fig.<ref type="bibr" target="#b4">5</ref>. Responsive transformations that may cause comparison loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(CIELAB 2002): d color (e i .color, e j .color) = (e i .L − e j .L) 2 + (e i .a − e j .a) 2 + (e i .b − e j .b) 2 , (10) where L, a, and b represent L * , a * , and b * in CIELAB space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Responsive transformations motivating trend loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Components of computing trend loss. (a) Calculating area between curves by standardizing chart size and interpolating break points. (b) Dividing and matching subgroups. (c) Linearizing color scale.LS is large screen, and SS is small screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Prototype pipeline. (1) The full specification of an input source view in ASP. (2) Enumerating targets by extracting a partial specification of the source view and generating a search space using an ASP solver. (3) Evaluating targets by computing our loss measures and ranking them using a model trained on human-produced rankings. (4) Ranked targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. (a, b) Example target transformations enumerated by our prototype responsive visualization recommender (total size of search space per source given as #Targets). indicates rankings of each five targets per source view predicted by our best model (see Sect. 6.3). (c) Source visualizations for our user study (also includes a and b). Sources views have width of 600px and height of 300px. The width of targets is fixed as 300px. Data sets are from Our World in Data [26, 53, 54]. Continuous, Nominal, Temporal, Identification loss, Comparison loss, and Trend loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 Fig. 10 .</head><label>110</label><figDesc>Fig. 10. (a) Study design. (b) Task interface. (c) Quintile sampling of targets for task materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. (a) Joint distributions of rankings of target views in each pair of aggregated features (the source visualization is shown in Fig. 9a). (b) Kendall rank correlation coefficients for targets of our source views in Fig. 9. Cmp (comparison).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>x .GNI x i .GDP e .y e i .x e i</head><label></label><figDesc>1 , x 2 ,...,x n }, where x i = (x i .GDP, x i .GNI)). C V maps GDP and GNI to x and y positions of point marks, respectively, producing a scatterplot. The corresponding set of rendered values is a set of Cartesian coordinates on the XY-plane (i.e., E V = {e 1 , e 2 ,...,e n }, where e i = (e i .x, e i .y) is the tuple of rendered values for x i ). Similarly, for a data set containing a field CO2 (emission) that is mapped to color, e i .color would correspond to the rendered value of x i .CO2. For brevity, we define D V .field</figDesc><table><row><cell>V</cell><cell></cell><cell></cell><cell>D V</cell><cell>C V</cell><cell>E V</cell></row><row><cell>y=75</cell><cell>GDP x=73 e 3</cell><cell>GNI =</cell><cell>x i x 1 x 2 x 3 ...</cell><cell cols="2">D V .GDP = {3.5B, 2.5B, 2.8B, ...} E V .x = {87, 67, 73, ...} + + 3.6B 3.5B 2.4B 2.5B 2.9B 2.8B ... ... 89 87 × e 1 65 67 e 2 75 73 e 3 ... ... ... x: GDP ... y: GNI</cell></row><row><cell cols="6">Fig. 3. Our notation for a visualization. Rendered values are defined in</cell></row><row><cell cols="6">the space implied by the visual variable (e.g., pixel space for position or</cell></row><row><cell cols="5">size, color space for color).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>The set of features for our ML models by each chart type. These features are either concatenated or differentiated for each pair of targets. Aggregated features are the sum of the corresponding Disaggregated features. Pink, bold-bordered circles represent required features, and yellow, light-bordered circles optional encoding-specific features.</figDesc><table><row><cell>Features</cell><cell></cell><cell>Chart types</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Aggregated</cell><cell>Disaggregated</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Insight type</cell><cell>Enc./Model</cell><cell>Scatterplot</cell><cell>Bar graph</cell><cell>Line chart</cell><cell>Heatmap</cell></row><row><cell></cell><cell>x</cell><cell>Required</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>y~x</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>x+y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>x+y</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>(a) Prediction accuracy of our models, averaged over LOO cross validation. Other performance measures (AUC score and F1-score) appeared similarly to accuracy. e stands for the number of estimators of each random forest model. (b) Average importance of Disaggregated features (g = difference) measured by impurity-based importance from training a random forest model (e = 50) 10 times.</figDesc><table><row><cell cols="3">(a) Prediction accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Feature importances</cell></row><row><cell>Models</cell><cell>Features</cell><cell></cell><cell cols="3">Disaggregated</cell><cell></cell><cell></cell><cell cols="2">Aggregated</cell><cell></cell><cell></cell><cell cols="2">Disagg. + Agg.</cell><cell>B1 (chart size change)</cell><cell>B2 (axes-transpose)</cell><cell>Features</cell><cell>Importance*</cell></row><row><cell></cell><cell cols="2">Mappings</cell><cell cols="2">concatenate</cell><cell cols="2">difference</cell><cell cols="2">concatenate</cell><cell cols="2">difference</cell><cell cols="2">concatenate</cell><cell>difference</cell><cell>concatenate</cell><cell>difference</cell><cell>concatenate</cell><cell>difference</cell><cell>Ident.</cell><cell>y .159</cell></row><row><cell cols="3">1-Nearest Neighborhood</cell><cell>78.73</cell><cell></cell><cell>77.88</cell><cell></cell><cell cols="2">71.60</cell><cell cols="2">67.48</cell><cell></cell><cell>77.79</cell><cell>77.13</cell><cell>51.08</cell><cell>49.02</cell><cell>62.32</cell><cell>59.70</cell><cell>x .153</cell></row><row><cell cols="3">K-Nearest Neighborhood</cell><cell>76.48</cell><cell></cell><cell>77.79</cell><cell></cell><cell cols="2">72.73</cell><cell cols="2">72.26</cell><cell></cell><cell>76.01</cell><cell>77.32</cell><cell>55.11</cell><cell>50.42</cell><cell>62.61</cell><cell>62.32</cell><cell>color .067</cell></row><row><cell cols="2">Logistic Regression</cell><cell></cell><cell>78.63</cell><cell></cell><cell>78.07</cell><cell></cell><cell cols="2">75.91</cell><cell cols="2">76.38</cell><cell></cell><cell>78.63</cell><cell>77.98</cell><cell>53.70</cell><cell>54.08</cell><cell>62.42</cell><cell>62.42</cell><cell>size .026</cell></row><row><cell>SVM Linear</cell><cell></cell><cell></cell><cell>78.35</cell><cell></cell><cell>78.82</cell><cell></cell><cell cols="2">75.35</cell><cell cols="2">76.01</cell><cell></cell><cell>78.44</cell><cell>78.35</cell><cell>55.01</cell><cell>59.04</cell><cell>62.89</cell><cell>63.17</cell><cell>Cmp.</cell><cell>y .141</cell></row><row><cell>Decision Tree</cell><cell></cell><cell></cell><cell>76.19</cell><cell></cell><cell>75.91</cell><cell></cell><cell cols="2">70.20</cell><cell cols="2">67.95</cell><cell></cell><cell>76.19</cell><cell>74.70</cell><cell>50.05</cell><cell>51.08</cell><cell>63.17</cell><cell>63.17</cell><cell>x .114</cell></row><row><cell cols="2">Rrandom Forest (e=50)</cell><cell></cell><cell>83.04</cell><cell></cell><cell>82.38</cell><cell></cell><cell cols="2">76.66</cell><cell cols="2">74.70</cell><cell></cell><cell>82.19</cell><cell>82.57</cell><cell>50.89</cell><cell>50.61</cell><cell>61.20</cell><cell>62.89</cell><cell>color .092</cell></row><row><cell cols="2">Random Forest (e=100)</cell><cell></cell><cell>84.07</cell><cell></cell><cell>82.29</cell><cell></cell><cell cols="2">77.41</cell><cell cols="2">74.32</cell><cell></cell><cell>82.76</cell><cell>82.38</cell><cell>51.55</cell><cell>50.98</cell><cell>62.04</cell><cell>63.07</cell><cell>size .038</cell></row><row><cell cols="2">Adaptive Boosting</cell><cell></cell><cell>81.16</cell><cell></cell><cell>79.01</cell><cell></cell><cell cols="2">72.73</cell><cell cols="2">73.01</cell><cell></cell><cell>79.94</cell><cell>78.73</cell><cell>56.42</cell><cell>53.05</cell><cell>62.42</cell><cell>63.17</cell><cell>Trend</cell><cell>y~x .136</cell></row><row><cell cols="2">Gradient Boosting</cell><cell></cell><cell>81.82</cell><cell></cell><cell>80.88</cell><cell></cell><cell cols="2">77.32</cell><cell cols="2">75.26</cell><cell></cell><cell>82.94</cell><cell>80.04</cell><cell>53.51</cell><cell>53.05</cell><cell>63.17</cell><cell>63.17</cell><cell>color~x+y .055</cell></row><row><cell cols="2">Multilayer Perceptron</cell><cell></cell><cell>81.72</cell><cell></cell><cell>82.10</cell><cell></cell><cell cols="2">77.23</cell><cell cols="2">76.29</cell><cell></cell><cell>80.97</cell><cell>81.07</cell><cell>54.92</cell><cell>53.61</cell><cell>61.95</cell><cell>62.04</cell><cell>size~x+y .020</cell></row><row><cell cols="9">(a) Example case: Simple scatterplot (Fig. 9a) /Trend</cell><cell cols="4">Comparison/Trend</cell></row><row><cell>400</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>200</cell><cell cols="2">400</cell><cell>0</cell><cell>200</cell><cell></cell><cell>400</cell><cell></cell><cell>0</cell><cell cols="2">200</cell><cell>400</cell></row><row><cell cols="4">(b) Rank correlation Scatter</cell><cell cols="2">Scatter +color</cell><cell cols="2">Scatter +size</cell><cell cols="2">Histogram</cell><cell cols="2">Line</cell><cell cols="2">Heatmap</cell></row><row><cell></cell><cell></cell><cell></cell><cell>.27</cell><cell></cell><cell>.19</cell><cell></cell><cell>.13</cell><cell cols="2">.37</cell><cell>.16</cell><cell></cell><cell>.30</cell></row><row><cell></cell><cell cols="2">/Trend</cell><cell>-.12</cell><cell></cell><cell>-.22</cell><cell></cell><cell>.08</cell><cell cols="2">.21</cell><cell>.08</cell><cell></cell><cell>.47</cell></row><row><cell cols="3">Comparison/Trend</cell><cell>-.14</cell><cell></cell><cell>-.14</cell><cell></cell><cell>-.18</cell><cell cols="2">-.03</cell><cell cols="2">-.41</cell><cell>-.01</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Each source view had 180 pairs, but the histogram source view with 60 transformations has 177 unique pairs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Jessica Hullman thanks NSF (#1907941) and Adobe.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatial generalization and aggregation of massive movement data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Adrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Adrienko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2010.44</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno>doi: 10. 1109/INFVIS.2005.1532136</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization, 2005, InfoVis &apos;05</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Glanceable visualization: Studies of data comparison performance on smartwatches</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besanc ¸on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bezerianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2865142</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A replication study on glanceable visualizations: Comparing different stimulus sizes on a laptop computer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<idno type="DOI">10.5220/0010328501330143</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Joint Conference on Computer Vision</title>
				<meeting>the 16th International Joint Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IVAPP,. INSTICC, SciTePress</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reaching Broad Audiences from a Research Institute Setting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Böttinger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-34444-317</idno>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparative evaluation of animation and small multiples for trend visualization on mobile phones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Choe</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934397</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multi-level typology of abstract visualization tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.124</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Answer set programming at a glance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Brewka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Truszczyński</surname></persName>
		</author>
		<idno type="DOI">10.1145/2043174.2043195</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effect of spatial distance on the discriminability of colors in maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brychtová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>¸öltekin</surname></persName>
		</author>
		<idno type="DOI">10.1080/15230406.2016.1140074</idno>
	</analytic>
	<monogr>
		<title level="j">Cartography and Geographic Information Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
		<idno type="DOI">10.1145/1102351.1102363</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning, ICML &apos;05</title>
				<meeting>the 22nd International Conference on Machine Learning, ICML &apos;05</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Cappellari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mcdermid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alatalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bournaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Crocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>De Zeeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Emsellem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khochfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krajnović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuntschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oosterloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sarzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The atlas 3D project -xx. mass-size and massσ distributions of early-type galaxies: bulge fraction drives kinematics, mass-to-light ratio, molecular gas fraction and stellar initial mass function</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Weijmans</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stt644</idno>
	</analytic>
	<monogr>
		<title level="j">MNRAS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust locally weighted regression and smoothing scatterplots</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1979.10481038</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Illuminating the path: The research and development agenda for visual analytics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<ptr target="https://www.hsdl.org/?abstract&amp;did=485291" />
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>National Visualization and Analytic Center</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Measuring data abstraction quality in multiresolution visualizations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rundensteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2006.161</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Datasite: Proactive visual data exploration with computation of insight-based recommendations. Information Visualization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Yalc ¸in</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<idno type="DOI">10.1177/1473871618806555</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning perceptual kernels for visualization design</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346978</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Foresight: Recommending visual insights</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pedapati</surname></persName>
		</author>
		<idno type="DOI">10.14778/3137765.3137813</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Network visualization retargeting</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Di</forename><surname>Giacomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Didimo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Montecchiani</surname></persName>
		</author>
		<idno type="DOI">10.1109/IISA.2015.7388095</idno>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Information, Intelligence, Systems and Applications (IISA)</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical aggregation for information visualization: Overview, techniques, and design guidelines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2009.84</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Color appearance models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Fairchild</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Wiley Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Gebser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaminski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaub</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1405.3694" />
	</analytic>
	<monogr>
		<title level="j">Clingo = ASP + control</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Preliminary report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Potassco: The potsdam answer set solving collection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gebser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaminski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<idno type="DOI">10.3233/AIC-2011-0491</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>AI Commun</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Statistics as squid ink: How prominent researchers can get away with misrepresenting data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guzey</surname></persName>
		</author>
		<idno type="DOI">10.1080/09332480.2020.1754069</idno>
	</analytic>
	<monogr>
		<title level="j">CHANCE</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deconstructing and restyling d3 visualizations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1145/2642918.2647411</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology, UIST &apos;14. ACM</title>
				<meeting>the 27th Annual ACM Symposium on User Interface Software and Technology, UIST &apos;14. ACM</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Converting basic d3 charts into reusable style templates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2659744</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Which countries have protected both health and the economy in the pandemic?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hasell</surname></persName>
		</author>
		<ptr target="https://ourworldindata.org/covid-health-economyLastaccessed" />
		<imprint>
			<date type="published" when="2020-03-01">2020. March 1, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowdsourcing graphical perception: Using mechanical turk to assess visualization design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<idno type="DOI">10.1145/1753326.1753357</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;10. ACM</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;10. ACM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sizing the horizon: The effects of chart size and layering on the graphical perception of time series visualizations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1145/1518701.1518897</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;09</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Support vector learning for ordinal regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<idno type="DOI">10.1049/cp:19991091</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Artificial Neural Networks, ICANN &apos;99</title>
				<meeting>the 9th International Conference on Artificial Neural Networks, ICANN &apos;99</meeting>
		<imprint>
			<publisher>Institution of Engineering and Technology</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Techniques for flexible responsive visualization design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhicheng</surname></persName>
		</author>
		<idno>doi: 10. 1145/3313831.3376777</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI &apos;20. ACM</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems, CHI &apos;20. ACM</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How visualizing inferential uncertainty can mislead readers about treatment effects in scientific results</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376454</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI &apos;20</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems, CHI &apos;20</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vizml: A machine learning approach to visualization recommendation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hidalgo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300358</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualization rhetoric: Framing effects in narrative visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Diakopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.255</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A deeper understanding of sequence in narrative visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<idno>doi: 10.1109/ TVCG.2013.119</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hybridimage visualization for large viewing environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bezerianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.163</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization &amp; Computer Graphics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active ranking using pairwise comparisons</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A salience-based quality metric for visualization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jänicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2009.01667.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring the design space of composite visualization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<idno type="DOI">10.1109/PacificVis.2012.6183556</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Pacific Visualization Symposium, PacificVis &apos;12</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Interactive dimensionality reduction through user-defined combinations of quality metrics. IEEE Transactions on Visualization and Computer Graphics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johansson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2009.153</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A guide to the visual analysis and communication of biomolecular structural data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hertig</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrm3874</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Molecular Cell Biology</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ten guidelines for effective data visualization in scientific publications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wagener</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.envsoft.2010.12.006</idno>
	</analytic>
	<monogr>
		<title level="m">Environmental Modelling &amp; Software</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Rank correlation methods</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948">1948</date>
			<pubPlace>Griffin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Design patterns and trade-offs in authoring responsive visualization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14321</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Assessing effects of task and data distribution on the effectiveness of visual encodings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13409</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graphscape: A model for automated reasoning about visualization similarity and sequencing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025866</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An algebraic process for visualization design</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346325</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dziban: Balancing agency &amp; automation in visualization design via anchored recommendations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376880</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI &apos;20</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems, CHI &apos;20</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deepeye: Towards automatic data visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2018.00019</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 34th International Conference on Data Engineering, ICDE &apos;18</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scatternet: A deep subjective similarity model for visual analysis of scatterplots</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2875702</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1562" to="1576" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Formalizing visualization design knowledge as constraints: Actionable and extensible models in draco</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2865240</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Duchesnay</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v12/pedregosa11a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evaluating visualization sets: Trade-offs between local effectiveness and global consistency</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1145/2993901.2993910</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization, BELIV &apos;16</title>
				<meeting>the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization, BELIV &apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Human development index (hdi)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
		<ptr target="https://ourworldindata.org/human-development-index.Lastaccessed" />
		<imprint>
			<date type="published" when="2014-03-01">2014. March 1, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Income inequality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ortiz-Ospina</surname></persName>
		</author>
		<ptr target="https://ourworldindata.org/income-inequalityLastaccessed" />
		<imprint>
			<date type="published" when="2013-03-01">2013. March 1, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Evaluating interactive graphical encodings for data visualization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Ragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2680452</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Vega-lite: A grammar of interactive graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2599030</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Reactive vega: A streaming dataflow architecture for declarative interactive visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467091</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Narrative visualization: Telling stories with data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Segel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2010.179</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. The Bell System Technical Journal</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1948.tb01338.x</idno>
		<imprint>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Measuring the separability of shape, size, and color in scatterplots</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<idno>doi: 10. 1145/3290605.3300899</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19. ACM</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems, CHI &apos;19. ACM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Augmenting visualizations with interactive data facts to facilitate interpretation and communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2865145</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the psychophysical law</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0046162</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="page">1957</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Psychophysics: Introduction to its perceptual, neural and social prospects. Routledge, 2 ed</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Modeling color difference for visualization design</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2017.2744359</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Extracting top-k insights from multi-dimensional data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3035918.3035922</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD &apos;17</title>
				<meeting>the 2017 ACM International Conference on Management of Data, SIGMOD &apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://bids.github.io/colormap/.Lastaccessed" />
		<imprint>
			<date type="published" when="2015-03-01">2015. March 1, 2021</date>
		</imprint>
	</monogr>
	<note>mpl colormaps</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Discriminability tests for visualization effectiveness and scalability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Veras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934432</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The Wasserstein distances</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-71050-96</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">İ</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Mulbregt</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0686-2</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Line graph or scatter plot? automatic selection of methods for visualizing trends in time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2017.2653106</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A model of multi-scale perceptual organization in information graphics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<idno>doi: 10. 1109/INFVIS.2003.1249005</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization 2003, InfoVis &apos;03</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Analyzing perceptual organization in information graphics. Information Visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="DOI">10.1057/palgrave.ivs.9500070</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Mobilevisfixer: Tailoring web visualizations for mobile phones leveraging an explainable reinforcement learning framework</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030423</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning to automate chart layout configurations using crowdsourced paired comparison</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI &apos;21</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems, CHI &apos;21</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Visizer: A visualization resizing framework</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2012.114</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Investigating the effect of the multiple comparisons problem in visual analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zeleznik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3174053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI &apos;18</title>
				<meeting>the 2018 CHI Conference on Human Factors in Computing Systems, CHI &apos;18</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
