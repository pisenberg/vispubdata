<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M 2 Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xingbo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianben</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhihua</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Muqiao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
						</author>
						<title level="a" type="main">M 2 Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AA83C6030EA850341CDE203688B3F082</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra-and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M 2 Lens, to visualize and explain multimodal models for sentiment analysis. M 2 Lens provides explanations on intra-and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M 2 Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.</p><p>Index Terms-Multimodal models, sentiment analysis, explainable machine learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>• Xingbo Wang, Jianben He, Zhihua Jin, and Huamin Qu are with the Hong Kong University of Science and Technology. E-mail: {xingbo.wang, jhebt, zjinak, huamin}@ust.hk. Sentiment analysis aims to use computational approaches to identify people's attitudes, opinions, and other subjective information in human communication. It can benefit various applications, such as customer analysis, social robots, and political campaigns. Prior research on sentiment analysis is mainly based on a single communication channel (i.e., text or facial expression) <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b82">82]</ref>, which is often referred to as unimodal sentiment analysis. However, human communication is often multimodal. For example, people can show their happiness through positive words and tones, along with a wild smile. With the thriving of social media, a large number of multimodal communication datasets can be collected and studied, e.g., TV series and vlogs showing people's sentiment towards different topics and objects. This has greatly boosted the development of multimodal sentiment analysis techniques, and it has already become a vibrant and important research topic. Unlike the long-established unimodal sentiment analysis, multimodal sentiment analysis combines the heterogeneous data and captures two primary forms of interactions in different modalities: intra-modal and inter-modal interactions. Intra-modal interactions refer to the dynamics of one modality, which is the same as the unimodal analysis based on the single communication channel. Inter-modal interactions consider the correspondence between different modalities across time, e.g., the co-occurrences of a happy tone and a smile or a sudden pause after a humorous punchline. In practice, people's communication styles are highly complex and idiosyncratic. For example, a sentence may seem semantically positive, but people can express it with a sarcastic tone to reveal their dissatisfaction. In such cases, unimodal sentiment analysis is not reliable, while multimodal models can offer the opportunities to explore vocal and visual expressions besides texts. In addition, previous research <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b63">64]</ref> has confirmed that multimodal models are more accurate and robust in various downstream tasks.</p><p>Currently, deep-learning-based models achieve superior performance over the traditional methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b75">75]</ref> in multimodal sentiment analysis. Representative examples include transformers <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b69">69]</ref>, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b72">[72]</ref>, and Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b78">78]</ref>. However, these models often work like black-boxes, hindering users from understanding the underlying model mechanism and fully trusting them in decision-making. Enhancing the explainability of deep learning models has become critical for both model developers and users, and received increasing attention in the past few years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. For example, post-hoc explainability techniques, such as LIME <ref type="bibr" target="#b53">[54]</ref>, SHAP <ref type="bibr" target="#b37">[38]</ref>, and IG <ref type="bibr" target="#b67">[67]</ref>, help identify important features (e.g., words or image patches) that influence model predictions. However, these methods often target providing local explanations on instances (e.g., sentences) in unimodal scenarios. They do not scale well to produce global explanations on how intra-and inter-modal interactions influence the model decisions, for example, how the models will behave when positive words and sad voices are presented.</p><p>It is challenging to explain multimodal models for sentiment analysis. First, it is necessary to relate the model performance back to the multimodal input data <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b52">53]</ref>. The heterogeneity and high dimensionality of multimodal human behaviors make it difficult for users to easily interpret the input features or data, as well as how they affect model decisions. Compact and human-friendly summaries of multimodal data are highly desired, but little research (if not no) has been done on it. Second, it is non-trivial to explain inter-modal interactions between different modalities explicitly, which, however, are the unique characteristics of multimodal sentiment analysis models. For example, when a person says something positive with a neutral voice and facial emotion, users may feel interested in whether the models can discern positive sentiment in the language modality (i.e., the text).</p><p>In this paper, we propose M 2 Lens, a novel explanatory visual analytics tool to help both developers and users of multimodal machine learning models better understand and diagnose Multimodal Models for sentiment analysis. By considering the feature importance measured by post-hoc explainability techniques, M 2 Lens interprets intra-and inter-modal interactions learned by a multimodal language model from the global, subsets, and local levels. Particularly, we focus on interpreting three typical types of interactions, i.e., dominance, complement, and conflict. Moreover, it facilitates a multi-faceted exploration of the multimodal features and their influences on the model decisions for sentiment analysis. M 2 Lens consists of four major views. Specifically, the Summary View features an augmented tree-like layout for global explanations of the impacts of individual modalities and their interplay. The Template View summarizes influential and frequent multimodal features with compact templates. The Projection View enables multifaceted exploration of the features of user interest using different glyph designs. The Instance View visualizes individual multimodal instances and their explanations with details.</p><p>In summary, our major contributions are:</p><p>• M 2 Lens, a visual analytics system to produce multi-level and multi-faceted explanations on intra-and inter-modal interactions that are learned by the multimodal models. • Case studies and expert interviews that demonstrate the effectiveness of our approach in helping users gain deep insights into two state-of-the-art multimodal models for sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This section discusses the relevant research of our approach, including multimodal language analysis, post-hoc explainability techniques, and machine learning interpretation with visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multimodal Sentiment Analysis</head><p>Multimodal sentiment analysis is a vibrant topic in natural language processing (NLP). It automatically extract people's attitudes or affective states from multiple communication channels (e.g., text, voice, and facial expressions). Moreover, it has various applications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b81">81]</ref>. The core challenge is modeling the complex intra-modal and intermodal interactions, where multimodal features are being fused.</p><p>Early work <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref> concatenated features from different modalities before being input to a learning model. Conversely, some work adopted late-fusion approaches that combine the decision values from individual unimodal models using a voting scheme <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b48">49]</ref> or a learning model <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b51">52]</ref>. However, these methods ignore the cross-modal interactions. To address such issues, some work explicitly computed the unimodal, bimodal, and trimodal features and fused them with tensor product <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b76">76]</ref> and dynamic routing <ref type="bibr" target="#b70">[70]</ref>. Recently, neural network methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b78">78]</ref> are popular to model the complex interplay between modalities. For example, researchers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref> have extended LSTM cells and gates to learn temporal interaction patterns among multimodal sequences. Pham et al. <ref type="bibr" target="#b45">[46]</ref> proposed attention-based RNNs to learn multimodal representations with a cyclic translation loss among modalities. Zadeh et al. <ref type="bibr" target="#b77">[77]</ref> designed a multiview gated memory unit that is controlled by neural networks. It stores and predicts temporal cross-modal interactions. Tsai et al. <ref type="bibr" target="#b69">[69]</ref> utilized transformer attention mechanisms to learn both cross-modal alignment and interactions. Although neural networks greatly improve the performance over traditional methods, their complex architecture seriously affects the model interpretability. This paper presents an explanatory interface to diagnose black-box models for sentiment analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Post-hoc Explainability Techniques</head><p>Post-hoc explainability techniques interpret models after the training process <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>. They generally include model-specific and modelagnostic approaches <ref type="bibr" target="#b36">[37]</ref>. Model-specific methods explain particular models ranging from shallow models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b68">68]</ref> to sophisticated neural networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b60">61]</ref>. In contrast, model-agnostic methods are flexible enough to be applied to any machine learning model. Here, we discuss two main types of model-agnostic approaches: explanation by simplification and feature relevance explanation <ref type="bibr" target="#b2">[3]</ref>.</p><p>For simplification techniques, researchers often built surrogate models (e.g., rule-based learners <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b54">55]</ref>, decision trees <ref type="bibr" target="#b4">[5]</ref>, and linear models <ref type="bibr" target="#b53">[54]</ref>) to imitate the original model behaviors with reduced complexity. One of the most representative methods is LIME <ref type="bibr" target="#b53">[54]</ref>, which builds locally linear models to approximate individual predictions based on neighbors of instances of interest. Feature relevance explanation quantifies the feature contributions to model predictions. One popular example is SHAP <ref type="bibr" target="#b37">[38]</ref>, whose mathematical root is Shapley Value [62]a method from cooperative game theory. SHAP computes an additive importance score for each feature to describe its influence, given a prediction result. It has desirable properties (local accuracy, missingness, and consistency) and is proved to be aligned with human intuitions. Other work used local gradients <ref type="bibr" target="#b56">[57]</ref>, randomized feature permutations <ref type="bibr" target="#b20">[21]</ref>, or influence functions <ref type="bibr" target="#b27">[28]</ref> to disclose feature relevance.</p><p>However, the methods above are often used to interpret specific instances of one modality (e.g., sentences, images), which cannot be directly applied to multimodal sentiment analysis. This paper aims to fill the gap by enabling multi-level explanations on the learned intraand inter-modal interactions from global, subsets, and local levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Machine Learning Interpretation With Visualization</head><p>With the increasing complexity of both data and machine learning models, various visual analytics systems have been proposed to assist in understanding the model behaviors. Besides measuring the model performance with computational metrics, users also need to explore when and why a model makes specific decisions <ref type="bibr" target="#b22">[23]</ref>. One of the most common and important interpretation strategies in previous work is to reveal the relationship between the input data and model predictions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>. They can be categorized into two groups: instance exploration and feature &amp; subset exploration.</p><p>Instance visualization shows model behavior towards individual data samples. Amershi et al. <ref type="bibr" target="#b1">[2]</ref> presented ModelTracker to support performance debugging with a visual summary of binary classification instances. Ren et al. <ref type="bibr" target="#b52">[53]</ref> extended the performance visualization to multi-class scenarios with aligned vertical axis designs, while Kahng et al. <ref type="bibr" target="#b25">[26]</ref> and Alsallakh et al. <ref type="bibr" target="#b5">[6]</ref> adopted a matrix-like design for instance summary. Apart from visualizing instance distributions, Kulesza et al. <ref type="bibr" target="#b33">[34]</ref> built an exploratory debugging prototype to enable users to explain corrections back to models. In addition, there are tools <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b62">63]</ref> that allow users to interactively probe models with provided inputs.</p><p>Feature and subset visualization investigates how to surface the patterns groups of features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> and instances <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b83">83]</ref> that affect model decisions. Brooks et al. <ref type="bibr" target="#b6">[7]</ref> developed FeatureInsight, which supports the feature ideation process with a visual summary of set errors. Krause et al. <ref type="bibr" target="#b30">[31]</ref> enabled exploration of the predictive power of feature candidates across different feature selection algorithms. For specific applications in CV and NLP, features are often visualized as image patches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b64">65]</ref> or text segments <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. Besides, researchers built interactive tools to facilitate group-level exploration. Zhang et al. <ref type="bibr" target="#b83">[83]</ref> conducted feature attribution comparisons to inspect discrepancies across different data subsets. Some work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b74">74]</ref> used fairness metrics to partition data into groups for model diagnosis.</p><p>However, these methods do not consider exploring multimodal features and determining how much they affect model decisions. Our system facilitates multi-faceted exploration of multimodal features and generates multi-level visual explanations on their influences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>In multimodal sentiment analysis, a machine learning model predicts sentiment based on the visual, acoustic, and language features extracted from the raw video data. This section introduces the related background about multimodal datasets, feature engineering techniques, performance metrics, and intra-and inter-modal interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>There is a wide range of multimodal datasets in the community. For example, IEMOCAP <ref type="bibr" target="#b7">[8]</ref> contains 151 videos of dialogues with different emotion labels. YouTube <ref type="bibr" target="#b39">[40]</ref> consists of videos of product reviews extracted from the social media website, YouTube. Without loss of generality, our work focuses on the largest and widely-used benchmark dataset for multimodal sentiment analysis, i.e., CMU-MOSEI <ref type="bibr" target="#b79">[79]</ref>. It consists of 23,454 monologue movie review video clips from 1,000 speakers and 250 topics in YouTube. The sentiment of each video clip is labeled by three annotators with a Likert scale of [−3, 3], where 3 indicates strongly positive, −3 represents strongly negative, and 0 means neural. Besides the sentiment label, each video is associated with the information from the three communicative channels-transcripts for language resources (l), facial expressions for the visual (v), and voice of speakers as the acoustic modalities (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal Feature Engineering</head><p>Prior research on multimodal models mostly uses different feature engineering techniques for all three modalities in sentiment analysis. Here, we follow the common practice of multimodal feature extraction (also provided by CMU-MOSEI). For language features, transcripts are encoded by high-dimensional word vectors. We leverage Glove embeddings <ref type="bibr" target="#b44">[45]</ref> to represent each word, where each word is transformed to a 300-dimension vector. For visual modality, most work focuses on facial expressions, which are often encoded by Facial Action Coding System (FACS) <ref type="bibr" target="#b14">[15]</ref>. FACS encodes the facial muscle movement with 35 facial action units. We deploy it to extract frame-level facial features. The acoustic features are engineered through a speech processing framework, COVAREP <ref type="bibr" target="#b12">[13]</ref>. The extracted features have 74 dimensions, and all of them are related to speech emotions and tones. To help users gain a quick overview of these fundamental features, we further group them into different classes, which will be introduced in Sect. 5.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metrics for Multimodal Sentiment Analysis</head><p>Prior work applies several metrics to evaluate the model performance for multimodal sentiment analysis, including mean absolute error (MAE), the correlation between the model predictions and human labels (Corr.), F1 score (F1), 7-class accuracy (Acc 7 ), and 2-class accuracy (Acc). Note that Acc 7 considers all of the sentiment scores Z ∈ [−3, 3], while Acc is a binary classification score that only predicts whether this video clip is positive or negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Intra-and Inter-modal Interactions</head><p>In practice, sentiment analysis relies on multimodal language signals (e.g., language, facial expressions, and tones). A successful multimodal sentiment analysis requires the understanding of the combinations of these signals, where two primary forms of interactions exist-intraand inter-modal interactions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>When modeling intra-and inter-modal interactions, three typical situations arise <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b77">77]</ref>:</p><p>• One modality is dominant for sentiment analysis. For example, people may show agreement by nodding their heads, where the vision modality dominantly indicates their positive attitudes. • More than one modalities complement each other when people are expressing their sentiment. For example, people's positive attitudes in words can be enhanced by a happy tone. • More than one modalities conflict with each other. For example, people may tell sad stories with smiles on their face. Researchers have tried to build models to analyze the situations above for better sentiment analysis. However, most state-of-the-art models are deep-learning-based techniques with little interpretability. Model developers and users are not aware of how exactly the model utilizes information in multiple modalities in situations of dominance, complement, or conflict. Explaining multimodal model behaviors not only provides insights into the multimodal language characteristics, but also reveals the model errors and inspires new model designs. In our work, we explicitly provide global explanations on intra-and inter-modal interactions with a compact visual summary. Specifically, we categorize instances into dominance, complement, and conflict groups based on the importance of each modality computed by SHAP <ref type="bibr" target="#b37">[38]</ref>. Furthermore, we summarize influential feature sets for each group with templates to provide finer-grained explanations on model behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN REQUIREMENTS</head><p>Our goal is to develop a visual analytics system to help users (e.g., model developers and model users) understand and diagnose the behaviors of multimodal models for sentiment analysis. Similar to the general black-box explanation tools <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b83">83]</ref>, interpreting multimodal models helps target users gain insights into the connection between the model performance (e.g., model errors) and the characteristics of multimodal data. For example, model users can examine whether a model has a bias or poor performance on some types of data and further decide if it is a proper fit for target applications. Furthermore, given the critical aspects of multimodal sentiment analysis (in Sect. 3.4), it is beneficial to explain the intra-and inter-modal relationships learned by the model. For instance, model developers can adjust the fusion weights of different modalities based on their relative importance to achieve better sentiment predictions. However, it is challenging to interpret multimodal models due to the high complexity of multimodal data and inter-modal relationships.</p><p>To understand users' general needs and formulate design requirements, we surveyed prior visualization techniques for interpreting machine learning models [2, 3, 7, 10, 26, 31, 32, 39, 53, 83] and multimodal language analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b79">79]</ref>. Also, we worked closely with a researcher in NLP and multimodal machine learning (who is also a coauthor of this paper) for about five months to collect his feedback and iteratively refine the design requirements. We summarize the design requirements as follows.</p><p>R1: Show the model performance. Performance metrics are crucial for guiding the model analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b52">53]</ref>. They provide quantitative measures of how accurate the predictions are and can help users pinpoint where the model is likely to fail. The users often want to evaluate models at different levels:</p><p>Q1: What are the overall error distributions for model predictions? Q2: What are the instances that are predicted with large/small errors? R2: Reveal the contributions of modalities to the model predictions. Besides performance metrics, the system should provide global explanations on how the model generally works, especially when working with huge datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>. In multimodal sentiment analysis, intra-and inter-modal interactions are crucial for understanding the model behaviors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>. Thus, it is essential to summarize the influences of individual modalities and their interplay for predictions. Specifically, the system should help users answer the following questions:</p><p>Q3: How does each modality influence the model predictions? Displaying the contributions of each modality helps users prioritize their efforts in diagnosing a particular modality for model predictions <ref type="bibr" target="#b76">[76]</ref>. Q4: Which modalities dominate the model predictions? Also, which modalities complement or conflict with each other for model predictions? To better reveal the characteristics of multimodal interactions captured by the model, the system should further summarize the instances according to the interaction types <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b79">79]</ref>. Specifically, dominant, complementary, and conflicting modalities, which depict typical interaction types, are the targets for analysis. Q5: How do dominant/complementary/conflicting modalities influence the model predictions? Besides recognizing the learned interaction types, it is also essential to connect them to the model predictions for a comprehensive understanding of model behaviors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b83">83]</ref>. For example, the dominance of language modality can contribute to positive or negative sentiment for different instances.</p><p>R3: Identify the influences of multimodal features for the model predictions. With a global understanding of how the model work on individual modality (R2), users need to drill down to finer-level inspection on model behaviors. Feature-based exploration is a common and effective approach for explaining machine learning models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>. Accordingly, the system should connect high-level modality interactions with the corresponding multimodal features. For example, users may want to know when the language modality dominates the predictions and what words people use to express their sentiments.</p><p>Q6: What are the feature sets that significantly contribute to positive/negative sentiment predictions? Exploring all the features of instances individually is tedious given the high volume and dimensionality of multimodal data. Summarizing the set of features with a significant predictive contribution helps reduce the efforts in exploration <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. In addition, it helps users develop a high-level concept about model predictions. For example, users may want to know what types of words or facial expressions are considered important to models when dealing with positive sentiment cases. Q7: What features are considered important by the model? Are they plausible for prediction? To help users analyze the individual predictions, features with a significant influence on the model performance should be presented to users and allow them to judge whether they align well with the observation of the original data.</p><p>R4: Support multi-level and multi-faceted exploration of the multimodal model behaviors. Given the multimodal settings of sentiment analysis, the visualization should empower users to explore the relationships between the model and input data from multiple aspects (e.g., language, facial expressions). To facilitate a comprehensive understanding of multimodal models, explanations should be offered on different levels, including the influences of individual modalities and their interplay, and the importance of multimodal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">M 2 LENS</head><p>Based on the derived design requirements (Sect. 4), we develop a visual analytics system, M 2 Lens (Fig. <ref type="figure" target="#fig_0">1</ref>), for understanding and diagnosing how models utilize multimodal information for sentiment prediction. In this section, we first provide an overview of the system architecture. Then, we will illustrate the methods for generating explanations of multimodal model behavior. Next, we describe the visual designs and interactions in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Overview</head><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows the system architecture. First, speakers' opinion videos are transformed into visual, acoustic, and language features. The storage module saves users' model and data with processed features. Then, the explanation engine inputs the features into the model and generates multi-level explanations of model behaviors based on the feature attribution methods (e.g., SHAP). The visual analysis module enables interactive exploration of the explanations through five main views.  The User Panel is the entry point of the whole interface, where the descriptive statistics about the model performance and dataset (Q1) are shown. Then, Summary View, Template View, Projection View, and Instance View provide multi-level model explanations from language, visual, and acoustic modalities (R4). The Summary View presents a global summary of the influences of individual modalities and their interplay for the sentiment predictions (R2). The Template View and Projection View complement each other for subset-level explanations (R3). Specifically, Template View uses templates to summarize feature sets that frequently and significantly contribute to the model predictions. Projection View supports the multi-faceted exploration of instances that have features of interest, along with their prediction errors. The Instance View summarizes instance-level prediction information (e.g., errors) (Q2) and offers local explanations on the importance of each modality and its features (R4). In addition, it adds the audio and vision features along the spoken words and provides the corresponding raw video clips with feature annotations for further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-level Explanations</head><p>To facilitate users with a comprehensive understanding of multimodal behavior, we propose methods to generate global and subset-level explanations (R2, R3). They supplement the local explanations computed by feature attribution methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Global Explanations</head><p>Since the intra-and inter-modal interactions lie at the heart of multimodal sentiment analysis, they are essential for users to understand how the multimodal model utilizes the information from different modalities (i.e., language, audio, and vision) (R2). In our work, we characterize three typical types of interactions among modalities-dominance, complement, and conflict (details are in Sect. <ref type="bibr">3.4)</ref>.</p><p>The dominance suggests that the influence of one modality dominates the polarity (i.e., positive or negative) of a sentiment prediction. The complement indicates that two or all three modalities affect a model prediction in the same direction (i.e., positively or negatively). Conversely, the conflict reveals that the influences of modalities differ from each other. According to the definitions above, we formulate a set of rules to identify them (Algorithm 1). Specifically, The influence of the interactions on the model output is based on the importance of each modality (I l , I a , I v ), which is the summation of the importance of all its features. Then, we extract and summarize the interactions (L) with strong influences for all the predictions. The thresholds for our rules are determined by maximizing the distances between the interaction types while minimizing the average influences of interactions that do not belong to dominance, complement, or conflict (i.e., others):</p><formula xml:id="formula_0">arg max {T h sig ,T h dom ,T h con f l } 1 |L| 2 L ∑ i L ∑ j dist(L i , L j ) − Lothers (1)</formula><p>where L i (i ∈ {dominance, conflict, complement, others}) is the interaction types output by Algorithm 1 for all the instances, dist is the Euclidean distance between the average influences of L i and L j .</p><p>Algorithm 1 Rules for extracting important relationships of modalities. Input: {I l , I a , I v }; T h sig , T h dom , T h con f l (∈ (0, 1)); Output: Label for the interaction types, l;</p><formula xml:id="formula_1">1: if ∀i ∈ {l, a, v}, |I i | &gt; T h sig then 2: /* important interactions */ 3: if ∃i, j ∈ {l, a, v}, I i • ∑ I j &gt; 0, |I i | I ≥ T h dom then 4: l = dominance; 5: else if ∃i, j ∈ {l, a, v}, I i • I j &lt; 0, ∑ I i I ≤ T h con f l then 6: l = conflict; 7:</formula><p>else if ∃i, j ∈ {l, a, v}, I i • I j &gt; 0 then </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Feature Templates</head><p>Compared with inspecting the impacts of individual features, exploring feature groups is more effective for analyzing complex model behaviors and data characteristics <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>. It helps users develop a mental model about the model decisions (Q6). For example, what types of words (e.g., adjectives) are considered important indicators for positive sentiment. To ease the exploration of influences of high-dimensional features, we organize the model's input features introduced in Sect. 3.2 into several meaningful groups. Then, we summarize frequent and influential groups with compact templates (Fig. <ref type="figure" target="#fig_0">1C</ref>).</p><p>To promote the understanding of model behaviors, we first identify several feature sets based on the sentence structures for the language modality, emotion-related features for the acoustic modality, and facial expressions for the visual modality:</p><p>• Language: part of speech (POS) 1 (e.g., noun, adjective, verb);</p><p>• Audio: pitch, amplitude, glottal/voice quality, and phase;</p><p>• Vision (i.e., Face): face parts (i.e., brow, eye, nose, lip, and chin), head movement, and face emotions. For language modality, POS features provide a compact summary of the structure of language use. They have been widely used as a probe for natural language models <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b65">66]</ref>. The audio features are grouped according to a state-of-the-art speech processing framework, COVAREP <ref type="bibr" target="#b12">[13]</ref>, and speech applications <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b73">73]</ref>. These sets generally relate to the emotions and tones of speech. For face-related features, we divide them into the face parts, head movement, and face emotions. They are the representative components in the facial action coding system (FACS) <ref type="bibr" target="#b13">[14]</ref> for describing facial expressions. For the mapping between low-level multimodal features and the feature sets, please refer to the supplementary material. After grouping the low-level features for each modality, we construct templates for both the frequent feature sets (e.g., "ADJ") and features (e.g., word "good") that have a strong influence on predictions (Q6). Specifically, we create itemsets of important features and feature sets for all predictions. Then, we build FP trees <ref type="bibr" target="#b18">[19]</ref> to find frequent patterns within the itemsets. For example, if "PRON" and "PART" or the word "not" constantly appear, they will be recorded in the templates (Fig. <ref type="figure" target="#fig_0">1C</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">User Interface</head><p>Based on the generated explanations, the user interface of M 2 Lens facilitates multi-level exploration of model behavior from the perspective of language, acoustic, and visual modalities (R4). All the views are tightly integrated with interactions to ensure a smooth transition between different levels of explanations. They share the same color encoding scheme where dark red means strong positive sentiment and dark blue represents strong negative sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Summary View</head><p>The Summary View presents an overview of the intra-and inter-modal interactions that are learned by the selected model in the User Panel (R2). The influences of individual modalities and their interplay are visualized in a three-layer augmented tree-like layout (Fig. <ref type="figure" target="#fig_3">3B</ref>).</p><p>Visual designs. In the parent node, a barcode chart and a line chart show the distributions of the ground truths and model prediction errors, respectively (Q1). The vertical height of the barcode represents the total number of instances, and the color displays the sentiment. Meanwhile, the horizontal position of the line chart suggests the absolute error, and the mean error is represented as a dashed line.</p><p>The second layer presents the importance of individual modalities in bee swarm plots (Q3). They are arranged according to the influences of modalities in descending order. For each node in the layer, a blue bar is put to the left, whose horizontal length summarizes the total influences of the modality. Besides, the dots in the bee swarm plot and their projections (i.e., the barcode below) demonstrate the distribution of the influences of that modality for all the instances. The color and horizontal position of the dots encode the importance values, while the two gray lines indicate the magnitude of mean absolute importance.</p><p>The last layer summarizes the information about the four types of interactions (Sect. 5.2), where the most influential one is shown at the top (Q4, Q5). For each interaction, the horizontal range of all its charts marks the number of instances in that group. To better surface the patterns of how the combinations of modalities affect the model predictions, we put the data instances close to each other if all their three modalities share similar influence patterns. Specifically, the similarity is measured by the farthest distances among three modalities between the instances. Then, a line chart and a barcode chart at the top summarize the error and prediction patterns, which are similar to the parent node. In addition, three barcode charts are attached below to present the Besides, between two neighboring layers, links are drawn from the parent nodes to their child nodes. The width of a link is proportional to the importance of the child node to the model predictions.</p><p>Design choices. We have considered an alternative design (Fig. <ref type="figure" target="#fig_3">3A</ref>) based on the Sankey diagram to reveal the intra-and inter-modal interactions and their importance to the predictions. It consists of three parts, the ground truth information at the left, the influences of individual modality at the center, and the inter-modal interactions at the right. The width of a flow is proportional to the importance of the target node of the flow. The barcode chart of each node further displays the importance distribution. In addition, the orange lines of the nodes show the error distribution to guide the exploration. However, one expert commended that it would be necessary to demonstrate more detailed information on each node. For example, what modalities dominate the predictions, and what is the frequency? Therefore, we augment the nodes with graphs and further convert the Sankey diagram into a compact tree-like layout, which leads to the current design (Fig. <ref type="figure" target="#fig_3">3B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Template View</head><p>To facilitate the exploration of feature sets and their influences, the Template View (Fig. <ref type="figure" target="#fig_0">1C</ref>) summarizes frequent and influential templates of multimodal features in a table (Q6).</p><p>Visual designs. The Template View has four columns describing information about the template types, support, importance, and predictions and errors (R1, Q6). The first column records the names of feature sets by default. If a feature set contains frequent and important features, a green bar will be placed to the right denoting the number of children for the feature set. Users can collapse the corresponding row for detail by clicking the . The second table column displays the frequency for the templates. The distribution of the templates' importance and prediction information is visualized in the third and fourth columns. They share the same visual representations with the Summary View (Sect. 5.3.1). Users are enabled to sort the templates according to their support, importance, and errors. In such a way, they can prioritize their efforts in diagnosing the complex model behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Projection View</head><p>To further support the subset-level exploration of model behavior (R3, Q6), the Projection View (Fig. <ref type="figure" target="#fig_0">1D</ref>) connects the multimodal feature templates in the Template View with the instances. It allows users to examine the detailed information (e.g., feature values, prediction errors) about features across the instances. For example, after users select the "ADJ" template in the Template View, they may feel intrigued by what adjectives associate with large errors or with positive predictions. Then, they need to further inspect the individual instances.</p><p>Visual designs. To summarize the feature sets of a group of instances, we project the high-dimensional features onto a 2D plane using t-SNE <ref type="bibr" target="#b71">[71]</ref>. Thus, instances with similar features will be placed close to each other. Given textual, acoustic, and visual features are heterogeneous, we design three different glyphs to encode the feature sets of the instances. Users can switch between views to see the feature distribution of each modality. Moreover, to help diagnose the model behavior (e.g., errors), we add a heatmap as the background to display the distribution of prediction errors or template importance.</p><p>• Language: since words already carry semantic meanings, we use them to represent the textual features. In addition, we add a circle for each word, whose color encodes the sentiment prediction. • Vision: our glyph designs for facial features (Fig. <ref type="figure" target="#fig_4">4A</ref>) are inspired by Chernoff face <ref type="bibr" target="#b11">[12]</ref>, which is popular for displaying facial expressions. However, the original Chernoff face cannot reflect information such as head movement. Therefore, we add three sticks around the face to indicate the head movement in the yaw, pitch, and roll axis, respectively. The outer ring encodes the whole face information (e.g., emotion in our case), where the dark color suggests large feature values. Moreover, the stroke width of face parts (e.g., nose) and sticks mean movement intensity. The sentiment prediction is revealed by the face's background color. • Audio: to help understand acoustic features, we group them into higher-level classes (Sect. 5.2.2). As shown in Fig. <ref type="figure" target="#fig_4">4B</ref>, each colored sector represents the features of a class, where the radius relates to feature values. The sectors at the front summarized the average values of normalized features, while the small ones at the back display detailed feature values of the classes. Additionally, the inner circle color shows the sentiment prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Instance View</head><p>The Instance View (Fig. <ref type="figure" target="#fig_0">1E</ref>) provides local explanations by visualizing the important multimodal features and the context (i.e., transcripts and videos) of individual instances (Q7).</p><p>Visual designs. The left column presents a visual summary of the influences of modalities on the model predictions, as well as the prediction errors. Users can sort the instances according to different criteria (e.g., error) at the header and prioritize their efforts in instancelevel exploration. In each row, the horizontal axes demonstrate the sentiment range, where the prediction and ground truth are marked. Between the two values, the thick red line suggests the error. Below the prediction mark, three colored rectangles represent the aggregated feature importance values of the three modalities. The length and color of each rectangle encode the magnitude and sign of importance. For example, the modality with negative influences on the prediction will be encoded by a blue rectangle and placed at the right. In addition, the feature table below allows users to sort and search for the importance values of features or modalities.</p><p>To promote a comprehensive understanding of the context of individual instances, the right column highlights the important features of the instances. Unlike intuitive texts, the acoustic and visual features are harder to recognize. Thus, we align them with the spoken words and draw the most important ones using orange lines. The lines above the words correspond to acoustic features, while the lines below represent the visual features. The vertical offset of the lines denotes the feature values, and hence the fluctuations indicate the feature variation. In addition, the backgrounds of texts or feature lines reflect the importance of multimodal features at a word level.</p><p>The Instance View also provides video context for instance-level exploration. When users click on the rows of the table, the corresponding video clips will pop up and play. To make the visual features more intuitive, the top-ranked facial features (sorted according to importance value) are highlighted with bounding boxes that cover the corresponding parts of the face. Users can further find the detailed facial action units and their concrete meanings by hovering on the boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">User Interactions</head><p>The M 2 Lens provides a rich set of interactions, which help unify the different views and facilitate multi-level and multi-faceted exploration with details on demand.</p><p>Brushing. Users can brush the barcodes in the last layer of Summary View to emit a query on the specific data instances of an interaction type. Then, the Template View and Instance View will show the related templates and local explanations, respectively.</p><p>Clicking. Many interactions in the system can be triggered and undone by clicking. For example, clicking the table rows in the Tem-plate View will filter the irrelevant instances in the Projection View and Instance View. Users can switch between feature projections of different modalities by clicking the radio buttons in the Projection View. When clicking the table rows in the Instance View, the corresponding instances in the Projection View will be shown, and its video clips will pop up and play. In addition, users can click on the header of the Template View and Projection View to undo the previous selections.</p><p>Lasso and semantic zooming. To facilitate scalable exploration, users can use lasso or semantic zoom to focus on specific instances of interest in the Projection View. Then, the detailed information will be displayed in the Instance View.</p><p>Searching, sorting, and filtering. To narrow down the exploration space, users can sort and search for the instances or features in the table of Template View and Instance View. By adjusting the sliders in the Projection View, users can filter the instances according to the sentiment predictions and the feature importance of specific modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>In this section, we demonstrate how M 2 Lens helps users understand and diagnose multimodal models for sentiment analysis through two case studies and interviews with three domain experts (E1, E2, and E3) using the CMU-MOSEI dataset. E1 and E2 are NLP researchers who have multiple top research publications on multimodal language analysis (e.g., emotion recognition). E3 is a senior software engineer who has five years' experience in developing affective computing applications. The two cases are discovered by E1 and E2 during the system exploration in the interviews. The detailed feedback from all the experts is also collected and summarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Case One: Multimodal Transformer</head><p>In the first case, the expert E1 explored and diagnosed a state-of-the-art model, Multimodal Transformer (MulT) <ref type="bibr" target="#b69">[69]</ref>, for sentiment analysis using the CMU-MOSEI dataset (Sect. 3.1). MulT fuses multimodal inputs with cross-modal transformers for all pairs of modalities, which learn the mappings between the source modality and target modality (e.g., vision → text). Then, the results are passed to sequence models (i.e., self-attention transformers) for final predictions. All the multimodal features of the input data are aligned at the word level based on the word timestamps. Following the settings of previous work <ref type="bibr" target="#b69">[69]</ref>, we trained, validated, and evaluated MulT with the same data splits (training: 16,265, validation: 1,869, and testing: 4,643). The details about the MulT are included in the supplementary material.</p><p>During the exploration, E1 observed that the language modality often dominates the predictions, and the model cannot handle the negations in sentiment analysis very well. He further investigated the dominance of visual modality, where "Joy" and "Sadness" (two facial emotions) frequently co-occur. It was thought to be caused by the intense facial muscle movement, which was also captured by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Dominance of Language Modality</head><p>Global summary (R1, R2) After selecting the MulT and valid set in the User Panel, E1 felt interested in how individual modalities and their interplay contribute to the model predictions. By looking at the second layer of the Summary View (Fig. <ref type="figure" target="#fig_0">1B</ref>), E1 found that the language modality (indicated by the letter "L") has the largest influence among the three modalities since it has the longest bar to the left and widest range of dots in the bee swarm plot. On the contrary, the acoustic modality (indicated by the letter "A"), which ranks at the bottom, has the least influence. Then, E1 examined the last layer, where the dominance group with the widest barcode charts is shown at the top. Within the group, he discovered that the longest bars attach to the language modality, and the color of the prediction barcode aligns well with that of the language barcode. Thus, E1 concluded that the language also plays a leading role in the dominance relationship. Furthermore, he noticed that there are a group of dense blue bars appearing at the end of the language barcode, where the errors are relatively large (as indicated by the yellow curve above the dashed line). He wondered what features or their combinations cause the high errors. Therefore, he brushed the corresponding area of the blue bars. Subset exploration (R1, R3, R4) The Template View (Fig. <ref type="figure" target="#fig_0">1C</ref>) lists all the frequent and important feature templates for the brushed instances in the Summary View. By sorting them in descending order of error, E1 found that the "PRON + PART" appears at the top with one child feature. Then, he collapsed the row and found that 21 instances contain the word "not", where it negatively influences the predictions (blue dots in the bee swarm plot in the "importance" column). Next, he clicked "not" to see the details about this feature in the Projection View. Zooming in on the word "not", several similar negative words (e.g., "isn't", "wouldn't") were observed (Fig. <ref type="figure" target="#fig_0">1D</ref>). They were all located in a red area, indicating large errors. E1 speculated that the model cannot deal well with negations. Subsequently, he lassoed these words to closely examine the corresponding instances in the Instance View.</p><p>Instance exploration (R1, R3, R4) To further evaluate how the model handles negations, E1 started with the instances with large errors in the table (Fig. <ref type="figure" target="#fig_0">1E</ref>). When exploring the top-listed examples, E1 observed that negations always have significant negative influences on the predictions, and the model fails to interpret the true sentiment. For example, E1 found a case where the language modality dominates the negative sentiment prediction, and the word "not" is highlighted in blue (Fig. <ref type="figure" target="#fig_0">1E</ref>). However, the true sentiment of this sentence is positive, where the starting phrase "I really like" demonstrates the positive attitude. However, the model fails to extract the keywords and relies on the negation (i.e., "not") to predict the negative sentiment. Moreover, E1 noticed that when double negations appear in a sentence (Fig. <ref type="figure" target="#fig_5">5</ref>), the model tends to treat them separately and regards both of them as indicators for negative sentiment. However, in fact, these double negations reflect sentiments that are slightly positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Dominance of Visual Modality</head><p>Global summary (R1, R2) E1 referred back to the "dominance" group in the Summary View, where a collection of red bars from the prediction barcode conform with the ones from the visual modality (highlighted red in Fig. <ref type="figure" target="#fig_0">1B</ref>). The visual modality dominates the predictions, and the error line chart above suggests a low error rate in contrast with the previous case in Sect. 6.1.1. Motivated by this observation, E1 brushed the red bars to investigate the patterns in the visual features.</p><p>Subset exploration (R1, R3, R4) In the Template View, "Face Emotion" has the largest support (Fig. <ref type="figure">6A</ref>). After unfolding the row, E1 found that "Joy + Sadness" is a frequent and important combination. This intrigued him to find out how a contrary emotion pair co-occurs. After clicking the template, the corresponding glyphs are highlighted in the Projection View (Fig. <ref type="figure">6B</ref>). Most of them are found outside the red area, which verifies that the instances with"Joy + Sadness" often have small prediction errors. He decided to inspect these instances.</p><p>Instance exploration (R1, R3, R4) Through browsing the instances and their videos in the Instance View, "Joy" and "Sadness" are often considered important visual features with positive influences. Additionally, E1 found their co-occurrences may be due to the presence of intense and rich facial expressions in the videos. These expressions generally involve the movement of the related facial action units in "Joy" and "Sadness". For example, after E1 clicked on the instances, he noticed all the face parts (i.e., nose, eyes, brows, mouth, and chin) of the corresponding glyphs (Fig. <ref type="figure">6B</ref>) in the Projection View has thick strokes, which suggests intense movements. When he watched the original videos, the bounding boxes of "Joy" and "Sadness" always popped up as important visual features. Hovering on the boxes and A B Fig. <ref type="figure">6</ref>. "Joy + Sadness" co-occurrence patterns. A: "Joy + Sadness" is a frequent and important feature template in the table. B: The raw video information and corresponding glyphs of three representative instances of the "Joy + Sadness" template.</p><p>examining the facial expressions and their explanations, E1 concluded that the extreme facial expressions triggered the movement of the action units in "Joy" and "Sadness", and the model seemed to capture these important visual facial expressions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case Two: EF-LSTM</head><p>In this case, the expert E2 explored the popular RNN-based model, EF-LSTM <ref type="bibr" target="#b21">[22]</ref>, for multimodal sentiment analysis using the CMU-MOSEI dataset. The dataset setup and feature processing are the same as Case One (Sect. 6.1). EF-LSTM concatenates textual, acoustic, and visual features at each word. Then, it uses an LSTM model to derive the input representations for the predictions. The details of the model are provided in the supplementary material.</p><p>Through interactive explorations with M 2 Lens, E2 was surprised to find that EF-LSTM does not learn sentiment in text. Also, he noticed that the acoustic modality has the largest influence on the sentiment prediction results among the three modalities, and the voice pitch always plays a negative role in the sentiment predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">No Meaningful Information Learned in Text</head><p>Global summary (R2) After selecting the valid set and EF-LSTM, E2 started with the Summary View to gain an overview of the impacts of the modalities (Fig. <ref type="figure" target="#fig_3">3B</ref>). By comparing the range of dots in the three bee swarm plots, he was surprised to find that acoustic modality is the most influential modality, then comes the language modality. In addition, the language modality always exhibits a positive impact on the sentiment. These findings are quite counter-intuitive. Thus, E2 first explored text-related interactions by tracking the thickest links from the language modality to the third layer. He noticed that "complement" group shows at the top, and the text plays a leading role within the group. Then, he brushed the whole group to see textual feature patterns.</p><p>Subset exploration (R3, R4) The strange thing is that no textual templates and text glyphs were spotted in the Template View and the Projection View, respectively. E2 suspected that the model does not learn any important language features (i.e., words) for sentiment analysis. Then, he referred to the Instance View to validate his doubt.   Instance exploration (R1, R3, R4) When exploring the instances in the Instance View, E2 found that the model fails to recognize potentiallyimportant words for sentiment analysis, such as "fantastic" (in line #1), "excellent" (in line #2). None of them is highlighted with colors in the Instance Detail. E2 also noticed every word of the sentences in the feature table has evenly low positive importance scores (less than 0.1). This explains why the language modality always has positive influences and further proves that the model does not capture the sentiment in text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Negative Influences of Voice Pitch</head><p>Global summary (R1, R2) E2 paid attention to the most influential modality (i.e., the acoustic modality) in the Summary View (Fig. <ref type="figure" target="#fig_3">3B</ref>), where a negatively-skewed distribution of dots was shown. In addition, he noticed that within the"conflict" group, the acoustic modality plays a negative role (blue bars) throughout the time. Thus, E2 brushed this group to investigate the negative influence of acoustic features.</p><p>Subset exploration (R1, R3, R4) E2 found the "pitch" is the most frequent acoustic template in the Template View (Fig. <ref type="figure" target="#fig_7">7A</ref>). Moreover, E2 noticed that pitch always has a negative impact given the negativelyskewed distribution of dots in the third column. After clicking the row, he switched to the Projection View to see the pitch value distribution (Fig. <ref type="figure" target="#fig_7">7B</ref>). He discovered that the acoustic glyphs are spread along a left-slanting line, where the radius of the blue sectors (i.e., pitch values) generally increases from left to right. Then, he selected a group of instances with the large pitch at the right corner for further inspection.</p><p>Instance exploration (R1, R3, R4) By browsing the instances and videos in the Instance Summary (Fig. <ref type="figure" target="#fig_7">7C</ref>), E2 observed that pitch is always the top important acoustic feature and is associated with negative influences. Although some important pitch variation signals in the videos are captured by the model, he believed that the model is not reliable since it always regards the pitch as a strong negative sentiment indicator and he found many counterexamples. To name a few, in two cases (Fig. <ref type="figure" target="#fig_7">7C</ref>), he found pitch ranks the first with negative importance in the feature table. And he noticed that some backgrounds of the orange lines (i.e., pitch values) are colored light blue (i.e., negative). By examining the offsets of all the orange lines, he thought the highlighted ones seem to be the turning points of pitch values. He speculated that the model captures the important signals in audio. He further checked the original video and verified the observations. However, the speakers sound high-spirited, and the pitch should reflect positive sentiment.</p><p>Conclusions. Through the case study, E2 found that EF-LSTM seems not able to capture the sentiment in text. He reasoned that the simple early feature fusion may lead to textual information loss. He speculated that some more advanced model designs (e.g., transformer) can be incorporated into the model to facilitate text understanding. Given the negative impacts of voice pitch, E2 thought that removing the pitch feature may increase the model accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Expert Interviews</head><p>We collected the feedback from the one-on-one interviews with the aforementioned three domain experts (E1, E2, E3). None of them have tried the system before the interviews. We first introduced the background and system designs. Then we asked the experts to use M 2 Lens to diagnose two state-of-the-art models (i.e., multimodal transformer and EF-LSTM) on the CMU-MOSEI dataset. After a 50-minute exploration, we collected their feedback about the system workflow, system designs, application scenarios, and improvement suggestions.</p><p>System workflow. All the experts confirmed the effectiveness of the system workflow of M 2 Lens in providing explanations for multimodal sentiment analysis models. They mentioned that they usually rely on performance metrics or instance-level feature importance measures for model evaluation, which does not provide many details and is unable to support an in-depth analysis. Our system supplements them with globaland subset-level explanations, which facilitates a comprehensive and systematic understanding of model behaviors. E1 and E3 praised that the interaction summaries (i.e., dominance, complement, and conflict) are impressive and very useful for revealing both the model behaviors and the multimodal data characteristics. E3 mentioned if he finds some modalities are influential in predicting sentiment using M 2 Lens, he can consider reducing the number of modalities without losing much performance when deploying the model to low-end devices. E1 added that the feature templates help generalize the model error patterns. E2 summarized that the system assisted him in discovering interesting insights into the models. For example, he was surprised that EF-LSTM seems to not capture any sentiment information from the text.</p><p>Visual designs and interactions. Overall, the experts confirmed that the visualizations are useful and still easy to understand, and interactions are smooth. The Summary View is most favored by the experts for a quick overview of the learned intra-and inter-modal interactions. The designs of Projection View are also appreciated by the experts. E3 really liked the heatmap for showing the error and feature importance patterns. E1 thought the face glyphs are very intuitive, and the interactions such as lasso and zoom are really helpful for the exploration of a large amount of data. Moreover, he valued the video playback and the realtime highlighting of face parts for raw video browsing. Nevertheless, E1 and E2 said that the Instance View is a little complex, visualizing lots of information. Additionally, the experts responded that it took them a while (about 20 minutes) to fully grasp all the components and functions in the system.</p><p>Improvements. The experts offered constructive suggestions for improvements. E3 requested a bookmark function to save user interaction histories (e.g., selection of templates) for further review. E1 suggested that the system can add a comparison module for exploring and comparing different models at the same time. During the exploration, E2 and E3 observed that some large model errors are caused by dataset errors (e.g., a mismatch between the video and transcript). They recommended that the system should support correcting dataset errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Here, we discuss M 2 Lens regarding generalizability, scalability, multilevel and multi-faceted exploratory analysis, and learning curve.</p><p>Generalizability. M 2 Lens was developed to visualize and explain multimodal models for sentiment analysis. We demonstrated our system through case studies on two state-of-the-art models using the CMU-MOSEI dataset. However, M 2 Lens can also be used to explain other multimodal models on different sentiment datasets based on the feature importance computed by post-hoc explainability techniques. Furthermore, the interaction types (i.e., dominance, complement, and conflict) and feature templates can summarize multimodal features from the global and subset levels in other multimodal language analyses. For example, for the multimodal emotion recognition task, the system can explain what are the dominant modalities when "angry" is predicted. The feature templates can summarize the frequent and influential feature sets for "angry" and facilitate the exploration of model behaviors.</p><p>Scalability. Our approach also has some scalability issues, which come from the automated algorithms and visual designs. The bottleneck of our computational cost is the feature attribution methods. We use SHAP to compute the feature importance. It took about 25 minutes to process 2,000 instances of the CMU-MOSEI validation set. To speed up the process, we can employ techniques such as feature clustering, data sampling, and parallel computing. For the visual designs, the visual clutter can occur in the Projection View, where multimodal instances are encoded with different glyphs. To reduce this issue, M 2 Lens enables filtering instances according to the feature importance and sentiment predictions. Moreover, users can use semantic zoom to focus on instances of interest, which alleviates the overlapping issues.</p><p>Multi-level and multi-faceted exploratory analysis. M 2 Lens provides multi-level and multi-faceted explanations on the behaviors of multimodal models for sentiment analysis. A general workflow for our target users (e.g., model users and researchers) starts with the Summary View, where the global summary of the influences of individual modalities and their interplay is displayed. Then, users can specify an interaction type. Its influential and frequent multimodal features will be summarized in the Template View and Projection View. Users can examine their error and importance patterns, which helps prioritize their efforts for the instance exploration in the Instance View.</p><p>Learning curve. According to the feedback from the expert interviews, the experts pointed out that it took them some time (usually a 20-min trial) before smoothly using our system since our system contains a few components. However, they said that M 2 Lens is very helpful for them to explore the models. Moreover, they have derived comprehensive insights into the model behaviors and are eager to use M 2 Lens for model understanding and diagnosis in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In this paper, we presented M 2 Lens, a visual analytics system to help users understand and diagnose multimodal models for sentiment analysis. M 2 Lens provides multi-level explanations on model behaviors from language, acoustic, and visual modalities. It features an augmented tree-like layout for a global understanding of learned intra-and intermodal interactions. Moreover, the feature templates and visualization glyphs of multimodal features facilitate the exploration of a group of frequent and influential feature sets. Through two case studies and expert interviews, we demonstrated M 2 Lens can provide deep insights into the state-of-art multimodal models for sentiment analysis.</p><p>In the future, we plan to enhance our system usability by adding functions, such as model comparison, data error correction. Also, we would like to extend our system to other multimodal applications (e.g., emotion recognition). Further, more domain experts can be invited to further validate the usability and effectiveness of M 2 Lens with more datasets and models for sentiment analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The explanatory interface of M 2 Lens consists of five views. The User Panel (A) displays the descriptive statistics about the model and dataset. The Summary View (B) presents a global summary of the importance of individual modalities, as well as their interactions using a three-layer augmented tree-like layout. The Template View (C) and Projection View (D) complement each other for subset-level explanations. Specifically, Template View (C) summarizes frequent and influential templates of feature sets in a table. The Projection View (D) supports multi-faceted explorations of instances that have features of interest. The Instance View (E) provides local explanations by visualizing the important features and the context of individual instances.</figDesc><graphic coords="1,109.75,138.53,362.12,233.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. M 2 Lens consists of a storage module, an explanation engine, and a visual analysis interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Design choices for the Summary View. A: An augmented Sankey diagram. B: Our current design of augmented tree-like layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The glyph designs in the Projection View. A: Chernoff face glyph designs. The left one with darked colored rings and thick strokes of face parts indicates intense facial movement, while the right one suggests little facial movement. B: Audio glyph designs. The left one with big blue sectors indicates high pitch, while the right one suggests low pitch. distribution of importance of all three modalities. Their vertical orders show the total influences of the corresponding modalities, which are summed up by the blue bars to the left. The color of the bars inside the barcodes represents the importance values.Besides, between two neighboring layers, links are drawn from the parent nodes to their child nodes. The width of a link is proportional to the importance of the child node to the model predictions.Design choices. We have considered an alternative design (Fig.3A) based on the Sankey diagram to reveal the intra-and inter-modal interactions and their importance to the predictions. It consists of three parts, the ground truth information at the left, the influences of individual modality at the center, and the inter-modal interactions at the right. The width of a flow is proportional to the importance of the target node of the flow. The barcode chart of each node further displays the importance distribution. In addition, the orange lines of the nodes show the error distribution to guide the exploration. However, one expert commended that it would be necessary to demonstrate more detailed information on each node. For example, what modalities dominate the predictions, and what is the frequency? Therefore, we augment the nodes with graphs and further convert the Sankey diagram into a compact tree-like layout, which leads to the current design (Fig.3B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of double negations. "not...sin" (in A) and "not..bad" (in B) are considered as indicators for negative sentiment by the model. However, these phrases reflect sentiments that are slightly positive.</figDesc><graphic coords="7,266.20,104.95,274.50,84.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Conclusions.</head><label></label><figDesc>During exploration, E1 discovered that MulT cannot handle double negations very well, though it is a state-of-the-art model. He commented augmenting double negation examples or preprocessing them into positive forms can further improve the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. Negative influences of voice pitch. A: "pitch" is the most frequent acoustic template, and it always has a negative impact (as indicated by the dots in the bee swarm plot). B: The selected group of instances with large pitch values (as indicated by the large radius of the blue sectors). C: Two high-error cases where the model captures the turning points of the pitch but wrongly associates pitch with negative influences.</figDesc><graphic coords="8,325.68,284.45,219.53,68.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>It's run by a fantastic team of professors; they are always available for you. (Umm) this movie was excellent.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank anonymous reviewers for their feedback. This research was supported in part by grant FSNH20EG01 under Foshan-HKUST Projects.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fairsight: Visual analytics for fairness in decision making</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1086" to="1095" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeltracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gil-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benjamins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Interpretability via model extraction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bastani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09773</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Featureinsight: Visual support for error-driven feature ideation in text classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fairvis: Visual analytics for discovering intersectional bias in machine learning</title>
		<author>
			<persName><forename type="first">Á</forename><forename type="middle">A</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Epperson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
		<title level="m">Machine learning interpretability: A survey on methods and metrics. Electronics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">832</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with word-level fusion and reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The use of faces to represent points in k-dimensional space graphically</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chernoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">342</biblScope>
			<biblScope unit="page" from="361" to="368" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">COVAREP -A collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName><forename type="first">G</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Facial action coding system: A technique for the measurement of facial movement</title>
		<author>
			<persName><forename type="first">E</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Consulting Psychologists Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GLTR: Statistical detection and visualization of generated text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple classifier systems for the classification of audio-visual emotional states</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tschechne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Layher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kächele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="359" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature space interpretation of svms with indefinite kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Haasdonk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="482" to="492" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining frequent patterns without candidate generation: A frequent-pattern tree approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="87" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An interactive node-link visualization of convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="867" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A peek into the black box: exploring classifiers by randomization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Henelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Puolamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Boström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Asker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Papapetrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1503" to="1529" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual analytics in deep learning: An interrogative survey for the next frontiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis to explore the structure of emotions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="350" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accuracy vs. comprehensibility in data mining models</title>
		<author>
			<persName><forename type="first">U</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>König</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="295" to="300" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Activis: Visual exploration of industry-scale deep neural network models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A versatile framework for evolutionary data mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Konig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName><surname>G-Rex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops Proceedings of the 8th IEEE International Conference on Data Mining</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="971" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Increasing the interpretability of recurrent neural networks using hidden markov models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Krakovna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05320</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Infuse: Interactive feature selection for predictive modeling of high dimensional data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1614" to="1623" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interacting with predictions: Visual inspection of black-box machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5686" to="5697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supporting iterative cohort construction with visual temporal queries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stavropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Principles of explanatory debugging to personalize interactive machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Intelligent User Interfaces</title>
				<meeting>the 20th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter</title>
				<meeting>the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence: Concepts, applications, research challenges and visions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lecue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Interpretable Machine Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://christophm.github.io/interpretable-ml-book/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Multimodal Interfaces</title>
				<meeting>the 13th International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
				<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for persuasiveness prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="284" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Feature visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Investigating statistical machine learning as a tool for software development</title>
		<author>
			<persName><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6892" to="6899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Integrating multimodal information in large pretrained transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2359" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Extending long short-term memory for multi-view structured learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="338" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling latent discriminative dynamic of multi-dimensional affective signals</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="396" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Anchors: High-precision modelagnostic explanations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1527" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Beyond accuracy: Behavioral testing of nlp models with checklist</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4902" to="4912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Explaining classifications for individual instances</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Šikonja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="589" to="600" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A primer in bertology: What we know about how bert works</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Contentbased tools for editing audio stories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berthouzoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 26th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial affect: A survey of registration, representation, and recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1113" to="1133" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A value for n-person games</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to the Theory of Games</title>
				<imprint>
			<date type="published" when="1953">1953</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03788</idno>
		<title level="m">Direct-manipulation visualization of deep networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A survey of multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (Workshop Track)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Interpretable predictions of tree-based ensembles via actionable feature tweaking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tolomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="465" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6558" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multimodal routing: Improving local and global interpretability of multimodal language analysis</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1823" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Select-additive learning: Improving generalization in multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="949" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Voicecoach: Interactive evidence-based training for voice modulation skills in public speaking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The what-if tool: Interactive probing of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5634" to="5641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5642" to="5649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Emotioncues: Emotion-oriented visual summarization of classroom videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3168" to="3181" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Emoco: Visual analysis of emotion coherence in presentation videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="927" to="937" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Manifold: A modelagnostic framework for interpretation and diagnosis of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
