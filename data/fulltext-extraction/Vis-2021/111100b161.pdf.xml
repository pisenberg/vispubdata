<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Support: Modeling Causal Inferences with Visualizations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Kale</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jessica</forename><surname>Hullman</surname></persName>
						</author>
						<title level="a" type="main">Causal Support: Modeling Causal Inferences with Visualizations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB5ADB4CEDD638F60B9485A657711ED9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal inference</term>
					<term>visualization</term>
					<term>contingency tables</term>
					<term>data cognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>: Modeling causal inferences with visualizations: A Users view and may interact with data visualizations; B Ideally, users reason through a series of comparisons that allow them to allocate subjective probabilities to possible data generating processes; and C We elicit users' subjective probabilities as a Dirichlet distribution across possible causal explanations and compare these causal inferences to a computed benchmark of causal support, which we derive from Bayesian inference across possible causal models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Data analysts engaged in sensemaking <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b49">48]</ref> infer the compatibility of different causal explanations with their data. During exploratory analysis or provisional statistical modeling, analysts implicitly or explicitly compare their data to patterns they expect as logical consequences of hypothesized data generating processes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Visualizations play a critical role in causal inference both because externalization reduces cognitive load <ref type="bibr" target="#b11">[12]</ref> and because human capabilities for inference rely heavily on sensory expectations (e.g., mental imagery) and comparisons between expectations and experiences <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Data analysts and software designers need to anticipate how human capabilities for causal inference may be error prone. For instance, perceptual biases such as underestimation of sample size (e.g., <ref type="bibr" target="#b32">[33]</ref>) contribute to errors in causal inferences insofar as perceived associations seem to be the basis for causal inferences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b60">59]</ref>. Analysts also err in their causal interpretations of data when the mapping between a potential causal explanation and an expected pattern in the data is unclear <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b61">60]</ref>. For example, imagine an analyst trying to detect confounding in experiment results on the effectiveness of a treatment at preventing disease (Fig. <ref type="figure">1 A</ref> ). To detect that 'gene' is a confounding factor, the analyst must see effects of gene on both treatment effectiveness (i.e., a difference between the top and bottom cells in the right column of table A ) and overall rate of disease (i.e., a difference be- tween the top and bottom rows of A ). Attributing these signals in the data to confounding requires the analyst to know what they are looking for, rather than passively detecting the appropriate patterns.</p><p>To assess how well visual analytics (VA) tools support such causal inferences, we need to compare analysts' inferences to a benchmark that is roughly 'normative', that captures important aspects of good causal inference. We adopt causal support, a model from mathematical psychology <ref type="bibr" target="#b20">[21]</ref>, as a benchmark for evaluating causal inferences from visualizations. Causal support models one's belief in a set of possible data generating processes as a Bayesian update. Causal support is a good normative model for three reasons. <ref type="bibr" target="#b0">(1)</ref> Causal support has numerous proprieties of valid statistical inference in light of the analyst's prior knowledge. It captures the fact that belief in evidence should be stronger as sample size increases. It accounts for unknown unknowns about the space of possible models, such that causal support assigns no posterior probability to models that the analyst does not explicitly consider. <ref type="bibr" target="#b1">(2)</ref> Prior work <ref type="bibr" target="#b20">[21]</ref> shows through a system of experiments that causal support accounts for otherwise hard-to-explain patterns in human causal inferences, e.g., that subjective belief in a causal relationship varies as a function of the potential to detect that relationship in a given data set. (3) Since causal support is extensible to any generative model (i.e., models that can assign likelihood to data), it can be applied in a wide range of VA applications to evaluate causal inferences.</p><p>We contribute two experiments using causal inference problems involving count and proportion data to <ref type="bibr" target="#b0">(1)</ref> study the utility of causal support for gaining insight into inferences from visualizations and (2) evaluate how well visualizations common in visual analytics (VA) software support causal inferences about possible data generating processes. We compare three common visual encodings for count and proportion data-bar charts, icon arrays, and text tables as a baseline-and we investigate how the ability to interactively aggregate or cross-filter data in bar charts impacts causal inferences. In Experiment 1, we ask participants to differentiate whether a treatment is effective by allocating probability across two different data generating processes. We find that chart users' causal inferences are far from normative with all visualizations we tested, and interacting with visualizations can improve sensitivity to signal in predictable ways. Ultimately, however, no visualization reliably outperforms text tables. We also see that chart users are more sensitive to evidence against a treatment effect than evidence in favor of one, suggesting an unequal weighting of falsifying versus verifying evidence. In Experiment 2, we replicate the main findings Experiment 1 but with a confounding detection task where participants allocate probability across four alternative data generating processes. Experiment 2 demonstrates how casual support can be extended to study causal inferences about more complex data generating processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visualization for casual inference</head><p>Much of the psychology and statistics literature on visual aids for causal reasoning focuses on contingency tables (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">49]</ref>). Contingency tables support causal inferences by using layout to encode conditional probabilities, the same way trellis plots afford grouping by factors during visual data analysis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b54">53]</ref>. Whether or not a factor seems to be collapsible-whether or not patterns the data seem to change depending on whether the data are grouped by that factor-can be a visual signal for reasoning about causal relationships such as confounding <ref type="bibr" target="#b19">[20]</ref>. However, empirical research on interpretation strategies for contingency tables <ref type="bibr" target="#b3">[4]</ref> suggests that analysts often misinterpret signals like collapsability because they don't ascertain the mapping between these visual signals and hypothesized causal relationships. Tools like Tableau enable users to explore collapsability by interactively grouping data. We investigate whether the ability to interactively control data aggregation improves the quality of causal inferences.</p><p>Research on visual analytics (VA) employs a broader range of representations to support causal reasoning, including parallel coordinates <ref type="bibr" target="#b56">[55,</ref><ref type="bibr" target="#b57">56]</ref>, bar charts <ref type="bibr" target="#b61">[60]</ref>, "diff bar charts" showing counterfactual outcomes under different conditions as layered bars <ref type="bibr" target="#b59">[58]</ref>, and novel techniques using animation to show event sequences (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>).</p><p>Some of these tools also incorporate directed acyclic graphs (DAGs) as interfaces to models and visualizations (e.g., <ref type="bibr" target="#b56">[55,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b61">60]</ref>). DAGs are devices for causal reasoning which have garnered attention in recent years <ref type="bibr" target="#b45">[44]</ref>. DAGs encode hypothesized relationships among variables (e.g., Fig. <ref type="figure">1 B</ref> ), making causal relationships and the assumptions they entail explicit and in some cases testable <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">[41]</ref><ref type="bibr" target="#b43">[42]</ref><ref type="bibr" target="#b44">[43]</ref>. We use DAGs to present differences between alternative causal explanations for data sets that we ask participants to judge (Figs. <ref type="bibr">2 &amp; 9)</ref>.</p><p>VA systems frequently use interaction techniques such as crossfiltering linked views of data (e.g., <ref type="bibr" target="#b56">[55,</ref><ref type="bibr" target="#b57">56]</ref>) and click-or drag-anddrop-based chart construction (e.g., <ref type="bibr" target="#b53">[52,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b61">60]</ref>). The most similar prior research to our study 1 tests whether constructing charts by clicking on variables versus dragging variables onto a DAG makes a difference in analysts' ability to differentiate between different kinds of causal relationships <ref type="bibr" target="#b61">[60]</ref>, specifically identifying mediating variables. Although they do not find an effect of interaction method on causal inferences, the authors provide a detailed strategy analysis extending evidence from psychological studies <ref type="bibr" target="#b3">[4]</ref> that analysts struggle to reason about the exact set of visual signals they should look for to verify or falsify a causal relationship. We extend this line of work by studying whether the ability to (un)facet charts or cross-filter coordinated multiple views impacts the quality of untrained analysts' causal inferences.</p><p>Prior work in visualization <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">38]</ref> and risk communication <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b51">50]</ref> suggests that icon arrays can improve Bayesian inferences, perhaps in part because of cognitive benefits of framing probabilities as frequencies of events <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref>. We compare icon arrays to text tables and bar charts since these visualizations span the design space for showing count data and are also easy to create in VA software like Tableau.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modeling causal reasoning</head><p>In the present study, we draw on and extend a model of causal reasoning called causal support, first proposed by Griffiths and Tenenbaum <ref type="bibr" target="#b20">[21]</ref>. Causal support formulates causal inferences as a Bayesian update on 1 Also see https://logical-interactions.github.io/causal2020/ the log odds of a finite set of causal explanations given some observed data. Mathematically, causal support has similar properties to a Chisquared test (i.e., Are the data in each cell of a contingency table likely generated by the same process?), which prior work analogizes to the kind of comparisons between data and model predictions that analysts visualize in "model checks" <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref> such as QQ-plots. However, unlike a Chi-squared test, causal support relies on Monte Carlo simulations to assign likelihoods under alternative causal explanations, making causal support extensible to any finite set of generative causal models. For instance, Pacer and Griffiths extend causal support to handle continuous data <ref type="bibr" target="#b41">[40]</ref> and event streams <ref type="bibr" target="#b40">[39]</ref>. Similarly, in Experiment 2, we present an extension of causal support to evaluate inferences about more than two possible data generating models.</p><p>Previous cognitive models of causal inference explored in psychology share more in common with parameter estimation than statistical inference per se, a subtle but important distinction. One such model delta p posits that that people judge differences in conditional proportions of observed events when making causal inferences about count data <ref type="bibr" target="#b0">[1]</ref>. Another such model causal power posits that people judge the magnitude of effect size when making causal inferences <ref type="bibr" target="#b10">[11]</ref>. Both of these predecessors to causal support make the assumption that causal inferences are fundamentally a perceptual judgment, however, causal power rescales delta p based on the potential to detect any signal whatsoever within the observed data. In contrast, causal support assumes that the signal for causal inferences depends on the possible data generating models that the analyst has in mind and represents these alternative models explicitly. This makes causal support more flexible, with higher predictive validity for human judgments than delta p, causal power, and even Chi-squared <ref type="bibr" target="#b20">[21]</ref>. Causal support reflects analysts' natural tendency to dichotomize, for better or worse, reasoning about whether or not causal relationships exist rather than how strong they are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENT 1</head><p>How well do different visualization designs that are common in visual analytics (VA) software support causal inferences about possible data generating processes? We evaluate visualizations of count data including text contingency tables, icon arrays, grouped bar charts, bars that users can interactively aggregate, and linked bars that users can interactively cross-filter. In Experiment 1, we investigate chart users' ability to infer whether a treatment prevents a disease. By asking chart users about a treatment effect in count data, we build on the task and structural equation models used by Griffiths and Tenenbaum <ref type="bibr" target="#b20">[21]</ref> to propose and validate causal support. Count data are also ideal for evaluating bar charts. The design requirements for supporting our causal inference task with count data are that visualizations should express both the proportion of people with disease and sample size. Based on these requirements, we ruled out testing pie charts and heatmaps, common ways of encoding proportions that do not encode sample size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>We set out to study how well different visualizations support causal reasoning by using causal support as a benchmark for causal inferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Task scenario &amp; response elicitation</head><p>Participants played the role of an analyst hired by a company to interpret samples of data on the effectiveness of experimental treatments at preventing various diseases. We showed participants visualizations of the number of people in each sample who did or did not receive treatment, get a disease, and have a gene known to cause the disease.</p><p>We asked participants to judge the underlying causal relationships in the data, rating their degree of belief that treatment protects against disease by allocating probability across the two DAGs in Figure <ref type="figure" target="#fig_0">2</ref>. We chose to study causal inferences about treatments, genes, and diseases in order to create a scenario where users would find the possible causal explanations feasible, coherent, and memorable. Question &amp; elicitation. We asked participants the following question:</p><p>How much do you believe in each of the causal explanations described below? Imagine you have 100 votes to allocate across the two possible explanations. Split your 100 votes between explanations based on your degree of belief. For example, if you think one explanation is twice as likely as the other, you might give 67 votes (roughly two thirds) to that explanation and 33 votes (roughly one third) to the other. Assume no other explanations are possible. Participants responded with two complementary probabilities. We used form validation to make sure their responses were both numbers between 0 and 100 that summed to 100. Following prior work on eliciting Dirichlet distributions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> (i.e., probabilities allocated across alternatives), when participants gave their first response, we imputed what the second response would need to be in order for their responses to sum to 100. This imputed value and a corresponding prompt, "Adjust your responses until both numbers reflect your beliefs.", were both highlighted with the same color to indicate this imputation. We elicited probabilities as "votes out of 100" because frequency framing tends to reduce bias in probability estimates <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>. Participants received no feedback on their responses. We transformed these responses into perceived causal support, which we compared to our benchmark. Perceived causal support. The dependent variable in our study was a measure of the perceived log odds of a target explanation over other possible causal explanations. Specifically, in Experiment 1 we targeted explanation A, which posited a treatment effect, requiring us to transform participants' responses into a log response ratio (lrr A ), lrr A = log response A response B where response A and response B were the probabilities participants allocated to causal explanations A and B, respectively, on each trial. We used a log odds scale in order to make participants' perceived causal support comparable to our normative benchmark of causal support. Payment. Participants received a guaranteed reward of $2 plus a bonus of $0.25 for every trial where their estimate of the probability of causal explanation A 2 was within 5 percentage points of the ground truth. Apparatus. We collected data using a Flask application deployed on Heroku with a Firebase database and visualizations created with D3. 3   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Visualization conditions</head><p>Our visualizations show the number of people with and without disease in each cell of a 2x2 contingency table faceted by treatment and gene, with the exception of cross-filter bars which use a different layout. We aimed to test visualizations of count data similar to what an analyst could produce using visual analytics software like Tableau.  Text contingency tables showed the number of people with disease as a fraction of the total number of people in each cell of a faceted table (Fig. <ref type="figure" target="#fig_3">3 A</ref> ). Text tables, which have been studied in prior research on causal support, served as a baseline comparison for other visualizations. Icon arrays showed counts of people with and without disease as filled and open circles, respectively (Fig. <ref type="figure" target="#fig_3">3 B</ref> ). We set the number of dot columns to minimize the aspect ratio on each trial, similar to how analysts might create roughly square icon arrays in Tableau. Icon arrays express both proportion and sample size as natural frequencies, which prior work finds beneficial for statistical reasoning (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>). Bar charts showed counts of people with and without disease using a length/position encoding on a common scale (Fig. <ref type="figure" target="#fig_3">3 C</ref> ). On each trial, we set the y-axis scale to the maximum count of the data in view, allowing scales to change from trial to trial as they do when users load a new data set in Tableau. Bar charts are ubiquitous for count data. Aggregating bars (aggbars) were similar to bar charts, however, users could interactively toggle faceting by treatment or gene by clicking on the table headers (Fig. <ref type="figure" target="#fig_2">4</ref>). On each trial, we set the y-axis scale to the maximum count of the fully aggregated data. We designed aggbars to roughly mirror Tableau's shelf interactions, where users control faceting by direct manipulation of table headers. Interactive faceting may facilitate causal inferences by enabling users to explore whether "collapsing" <ref type="bibr" target="#b19">[20]</ref> over a factor changes patterns in the data. Cross-filtering bars were three bar charts showing the number of people with and without treatment, the gene, and disease, respectively (Fig. <ref type="figure">5</ref>). We linked these bar charts such that clicking on one bar cross-filtered the rest of the data in view. When users applied a filter (e.g., only show people who received treatment), the corresponding axis label became bold and gray bars persisted in the background, so chart users could compare filtered and unfiltered views without relying on working memory. Users "reset filters" by clicking a button below the charts. Filterbars emulated coordinated multiple views such as a Tableau dashboard. Interactive cross-filtering might assist in causal inferences because conditioning the data in view based on a specific event is analogous to Pearl's do operator (e.g., do(treatment = yes)), a notation for reasoning about counterfactuals in a causal networks <ref type="bibr" target="#b45">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Experimental design</head><p>We manipulated both the visualizations participants used for the task and the data sets that we showed. We randomly assigned each participant to use one of five visualizations (see Section 3.1.2), making comparisions of visualizations between-subjects. We showed each participant a total of 18 trials, which included 16 data conditions (see Section 3.1.4) presented within-subjects and two attention checks which we used for exclusion criteria (see Section 3.1.6). We randomized trial order for each participant, inserting attention checks on trials 7 and 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Stimulus generation</head><p>We evaluated causal inferences on realistic data sets, which spanned a range of ground truth causal support. Generating data sets required (1) Fig. <ref type="figure">5</ref>: Cross-filtering bars mimic coordinated multiple views.</p><p>manipulating data attributes which signaled causal support to participants, and (2) labeling each data set with ground truth causal support.</p><p>Our goal was to generate 16 data conditions (i.e., trials in our experiment) that varied delta p and sample size, two data attributes which in turn manipulate ground truth causal support. Delta p described the difference in the proportion of people with disease in each data set depending on whether they received treatment. Positive values of delta p indicated evidence that the treatment protected against disease; negative values indicated evidence against treatment effectiveness. Sample size was the number of people in each data set we showed participants. Data conditions. To generate our 16 data conditions, we simulated data from structural models with one parameter per DAG arrow in Figure <ref type="figure" target="#fig_0">2</ref>. We manipulated both the probability that treatment prevents disease (4 levels: {0, 0.1, 0.2, 0.4}) and sample size (4 levels: {100, 500, 1000, 1500}). We controlled the probability of disease due to the gene (0.5), probability of disease due to unobserved causes (0.2), base rate of the gene (0.4), and the proportion of each sample with treatment (0.5). We selected these parameters iteratively by sampling data sets and labeling ground truth until half of the trials had greater than a 50% chance of being generated by causal explanation A.</p><p>For each of the resulting 16 data conditions, we simulated many data sets using a binomial random number generator to approximate realistic sampling error. By simulating sampling error, we prevented the count data from appearing contrived. This sampling error resulted in a distribution of ground truth causal support under each data condition, with more variability in the ground truth at smaller sample sizes. To guarantee that each participant saw trials spanning a consistent range of causal support, we selected 16 data sets representing 16 quantiles of the ground truth distribution per data condition, and we counterbalanced the quantile shown for each data condition across participants within each visualization condition using a balanced latin square. For our attention check trials, we selected the two simulated data sets that had the minimum and maximum ground truth causal support. Labeling ground truth causal support for each data set. We operationalized the ground truth for causal inferences using Griffiths and Tenenbaum's causal support, a Bayesian cognition model that estimates the posterior log odds of a target data generating model over a set of alternative data generating models, given a data set. In Experiment 1, we targeted causal support for explanation A over explanation B:</p><formula xml:id="formula_0">cs A = log Pr(C|model A ) Pr(C|model B ) + log Pr(model A ) Pr(model B )</formula><p>where C is the data set we label with ground truth, and models model A and model B correspond to causal explanations A and B (Fig. <ref type="figure" target="#fig_0">2</ref>).</p><p>The first term in the formula for cs A is a log likelihood ratio representing the relative compatibility of a given data set with causal explanations A and B. We computed the log likelihood of each data set given model A and model B using Monte Carlo simulations (Alg. 1, lines 28-30), based on structural models similar to those we used to generate data sets. In practical scenarios, we would not know the true data generating parameters, so we used Monte Carlo simulations of possible parameter values under each model to calculate likelihoods without needing to know the ground truth a priori. Under model A we sampled all three parameters uniformly on the interval [0, 1], representing the assumptions that there is a treatment effect and that both gene and unobserved factors cause disease. Under model B we sampled Pr(D|G) and Pr(D) uniformly, but we fixed Pr(¬D|T ) at zero, representing an assumption of no treatment effect (i.e., omitting the DAG arrow between treatment and gene in Fig. <ref type="figure" target="#fig_0">2</ref>). In each simulation, we averaged log likelihood of a given data set over m = 10000 Monte Carlo iterations (Alg. 1, lines 25-26), marginalizing over sampled parameter values.</p><p>The second term in the formula for cs A is a log ratio of the prior probability of explanations A versus B. Following Griffiths and Tenenbaum <ref type="bibr" target="#b20">[21]</ref>, we assume a uniform prior to be normative, assigning 50% probability to both explanations A and B (Alg. 1, lines <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. The prior encodes a bias in belief allocation across a finite set of alternative causal explanations. We assume a uniform prior because we want our benchmark to reflect no a priori bias toward causal explanations. 4   4 Uniform priors follow a convention of psychometric models that assume Algorithm 1 Monte Carlo simulation to calculate causal support in Experiment 1. Algorithm for Experiment 2 is similar.</p><p>Input: (8, 1) vector of contingency table counts (no disease vs disease, no gene vs gene, no treatment vs treatment) C, Monte Carlo iterations m, set of parameters to fix at zero θ 0 (i.e., parameters representing DAG arrows to omit from the data generating process) Output: MONTE CARLO returns log likelihood of the given data generating process log lik; Main returns causal support for the target explanation (Fig. <ref type="figure" target="#fig_0">2</ref>, Explanation A) cs A 1: # Monte Carlo simulation to calculate likelihood 2: function MONTE CARLO(C, m, θ 0 ): for parameter θ ∈ θ P do # assign parameters 10:</p><p>if θ ∈ θ 0 then Fix parameter at zero: θ = Zeros(m)</p><p>11:</p><p>else Uniformly sample probabilities: θ = Random(0, 1, m)</p><p>12:</p><p>Calculate probabilities corresponding to contingency table:</p><p>13: P = [ # p no disease vs p disease given... • 1 − Pr(¬D|T )</p><p>24:</p><p>] 25:</p><p>return average log likelihood of data:   is zero such that an intercept of 50% indicates no bias. One can think of intercepts as the average prior probability that participants allocate to explanation A when there is no signal in the data.</p><p>Approach. We used the brms package <ref type="bibr" target="#b7">[8]</ref> in R to fit Bayesian hierarchical models on perceived causal support. We adopted a Bayesian workflow called model expansion <ref type="bibr" target="#b13">[14]</ref>, where we started with a simple model and iteratively added predictors to build up to more complex models, running prior predictive checks, model diagnostics, posterior predictive checks, and leave-one-out cross validation for each version of the model. We centered each prior to reflect a null hypothesis of ideal performance and no bias, and we scaled each prior to be weakly informative while providing sufficient regularization for models to converge. We provide more details about our modeling workflow in our preregistrations 5 and Supplemental Materials. 6  Model specification. We used the following model (Wilkinson-Pinheiro-Bates notation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b58">57]</ref>) to evaluate participants' responses: lrr A ∼Normal(μ A , σ A ) μ A =cs A * delta p * n * vis + cs A * delta p + cs A * n worker id where lrr A was perceived causal support for a treatment effect, cs A was our normative benchmark, delta p was the difference in the proportion of people with disease given treatment versus no treatment, n was the sample size as a factor, vis was a dummy variable for visualization condition, and worker id was a unique identifier for each participant.</p><p>We primarily modeled effects on the mean of perceived causal support μ A , but our model also learned the residual standard deviation σ A . Both σ A and the random effects in the μ A submodel helped account for the empirical distribution, differentiating between response noise and effects of interest. The term cs A * delta p * n * vis enabled our model to 5 See preregistrations for Experiment 1 (https://osf.io/vzmhu) and for Experiment 2 (https://osf.io/y46nw) 6 https://github.com/kalealex/causal-support learn how the slope on causal support varies as a function of the visual signal on each trial (delta p and n) and visualization condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Participants &amp; exclusions</head><p>We recruited participants on Amazon Mechanical Turk. Workers had a HIT acceptance rate of at least 97% and were located in the US. We aimed to recruit a total of 400 participants after exclusions using our attention check trials, 80 per visualization condition. We determined this target sample size using a heuristic power analysis based on pilot data and the assumption that the width of confidence intervals would be inversely proportional to √ N. We recruited a total of 548 participants, and after exclusions we used data from 408 participants in our analysis. We slightly overshot our target sample size because we could not anticipate perfectly how many participants would miss our attention checks (see Section 3.1.4). Although we preregistered that we would exclude participants who failed to allocate at least 50% subjective probability to the most likely causal explanation on either attention check, this criterion proved too strict and would have excluded 48% of our sample. Instead, we opted to use only the easier of the two attention checks for exclusions, resulting in the exclusion of 26% of our sample. All participants were paid regardless of exclusions. We compensated the average participant $2.50 for about 9 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We evaluate chart users' causal inferences using a linear in log odds (LLO) model to assess sensitivity to the ground truth and bias in probability allocations when each causal explanation is equally likely. Sensitivity. A LLO slope of one indicates one-to-one correspondence between the ground truth and users' probability allocations. In all visualization conditions (Fig. <ref type="figure" target="#fig_6">7</ref>), we see slopes far below one, indicating that users are much less sensitive than ideal. The only reliable differences between visualization conditions are that filterbars users who do not interact are less sensitive than users in other conditions.</p><p>When filterbars users do not interact with the charts, slopes are approximately zero indicating that users are insensitive to signal. Performance improves reliably when users interact with the visualization by applying cross-filters to coordinated multiple views. This is expected because filterbars hide visual signal for the task behind interactions.</p><p>Surprisingly, when aggbars users interact with the charts to group by gene or treatment, this leads to lower sensitivity, though this difference is not reliable. To make sense of this result, we analyze interaction log data to see which variables chart users condition on. We find that aggbars users group the data by gene almost as often as treatment. Compare this to filterbars users, who condition on treatment much more often than gene (see Supplemental Material). This suggests that interacting with visualizations only improves sensitivity to causal support when users deliberately generate views of the data which show counterfactual predictions that can distinguish competing causal explanations. Visual signal effects on sensitivity. We examine sensitivity in each visualization as function of attributes of the visual signal for our task. In Experiment 1, the signal breaks down into two data attributes, delta p and sample size (see Section 3.1.4). Normatively, LLO slopes equal one regardless of delta p and sample size, however, our model measures differences in sensitivity depending on these data attributes.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows that in the conditions where slopes are largest-text, icons, bars, aggbars without interaction, and filterbars with interactionusers are more sensitive to causal support at negative values of delta p (e.g., Fig. <ref type="figure" target="#fig_5">6</ref>, top inset). The average user in these conditions responds more to evidence against treatment (i.e., falsification) than evidence in favor of a treatment effect (i.e., verification). At positive values of delta p (e.g., Fig. <ref type="figure" target="#fig_5">6</ref>, top inset), LLO slopes are similar across conditions, suggesting that differences in performance between conditions are driven in part by differences in sensitivity to falsifying evidence.</p><p>We also see in Figure <ref type="figure" target="#fig_5">6</ref> that users of icons, bars, and aggbars are more sensitive to signal when sample size is smaller. This finding is consistent with prior work showing that chart users tend to underestimate sample size when making inferences with data <ref type="bibr" target="#b32">[33]</ref>, which may be driven by logarithmic perception <ref type="bibr" target="#b55">[54,</ref><ref type="bibr" target="#b62">61]</ref>. Alternatively, we could interpret this result as a cognitive bias where users are unwilling to be certain even when sample size is large enough to support unambiguous inferences, related to non-belief in the law of large numbers <ref type="bibr" target="#b5">[6]</ref>. Bias. Intercepts in the LLO model describe bias in users' probability allocations when ground truth causal support indicates that explanation A (i.e., treatment effect) is just as likely as explanation B (i.e., no treatment effect). Under this condition, a normative observer would allocate equal probability to both causal explanations. We derive expected probability allocated to explanation A based on a logistic transform of LLO intercepts, and compare this to the normative benchmark of 50%.</p><p>With all visualizations except for filterbars, probability allocations are far below 50% indicating substantial bias (Fig. <ref type="figure" target="#fig_7">8</ref>). On average when causal support is zero, users of text tables, icons, bars, and aggbars allocate too little probability to causal explanation A. Users of filterbars, on the other hand, allocate approximately 50% to explanation A. We see the most extreme bias of up to 20% with icons arrays.</p><p>Unfortunately, we can only speculate about possible reasons for these biases. We expected that LLO intercepts would indicate average responses near 50% in the absence of signal for all conditions (i.e., a uniform prior), simply because this follows from the structure of the task. Because this pattern of biases across visualizations results from a non-preregistered exploratory comparison, we investigate in Experiment 2 whether these biases replicate for a more complex task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 2</head><p>In Experiment 2, we evaluate the same visualization designs on a more difficult task. We asked participants to detect confounding in the presence of a known treatment effect by allocating probability across four possible "backdoor paths" <ref type="bibr" target="#b45">[44]</ref> (Fig. <ref type="figure" target="#fig_8">9</ref>). We extend causal support to handle more than two alternative causal explanations, demonstrating how causal support can be employed in more complex analyses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method</head><p>Experiment 2 was the same as Experiment 1 except for the following changes to response elicitation, modeling, and experimental design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Task scenario &amp; response elicitation</head><p>Participants judge the influence of a gene on both disease and treatment effectiveness by allocating probability across the four DAGs in Figure <ref type="figure" target="#fig_8">9</ref>, separately assessing each DAG arrow in a confounding relationship. Question &amp; Elicitation. We asked participants a similar question as in Experiment 1, where participants allocated 100 votes (i.e., subjective probability) across alternative causal explanations. However, in Experiment 2 we elicited a Dirichlet distribution with four alternatives. Following Chalone et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> and extending our interface from Experiment 1, each time participants allocated a number of votes between 0 and 100 to an option, the remaining votes out of 100 were uniformly distributed across unused response options. These imputed responses were highlighted along with a prompt to, "Adjust your responses until all the numbers reflect your beliefs." Participants iteratively set and adjusted their probability allocations. We combined these responses into perceived causal support, which we compared to our benchmark. Perceived causal support. When estimating perceived causal support in Experiment 2, we separately evaluated multiple target explanations. Primarily, we targeted belief in explanation D (llr D ), confounding:</p><formula xml:id="formula_1">lrr D = log response D ∑ i={A,B,C} response i</formula><p>where response A , response B , response C , and response D were participants' probability allocations to causal explanations A through D, respectively, on each trial. We also separately targeted belief in both of the component DAG arrows that constitute a confounding relationship (Fig. <ref type="figure" target="#fig_8">9</ref>): (llr BD ) the effect of gene on disease, which appears in explanations B and D; and (llr CD ) the effect of gene on treatment effectiveness, which appears in explanations C and D. We define llr BD as follows,</p><formula xml:id="formula_2">lrr BD = log ∑ i={B,D} response i ∑ i={A,C} response i</formula><p>and we define llr CD similarly. We compare log response ratios llr D , llr BD , and llr CD to corresponding causal support cs D , cs BD and cs CD .</p><p>Strategy. At the end of the experiment, we asked participants,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How did you use the charts to complete the task? Please tell us what patterns you looked for in the data and what comparisons you made.</head><p>We analyzed these qualitative responses to assess whether participants understood how to use the charts for the confounding detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Experimental design</head><p>We manipulated both the visualizations (between-subjects) and the data sets we showed (within-subjects). We showed each participant 19 trials, 18 data conditions (see Section 4.1.3) and one attention check used for exclusions (see Section 4.1.5). We randomized trial order for each participant, inserting the attention check on trial 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Stimulus generation</head><p>We generated data sets that spanned a range of ground truth causal support for confounding. Creating these data sets required (1) manipulating data attributes which signaled whether the gene was a confounding factor, and (2) labeling each data set with ground truth causal support.</p><p>Our goal was to generate 18 data conditions that varied delta p disease, delta p treatment, and sample size, data attributes which manipulate causal support for confounding. Delta p disease described the difference in the proportion of people with disease in each data set depending on whether they had the gene. Negative values of delta p disease indicated evidence that the gene caused disease, whereas values near zero indicated evidence against a gene effect on disease. Delta p treatment described the difference in the proportion of people with disease within the treatment group depending on whether they had the gene. Negative values of delta p treatment indicated evidence that the gene stopped the treatment from preventing disease, whereas values near zero indicated evidence against a gene effect on treatment. Sample size was the number of people in each data set we showed chart users. Data conditions. To generate 18 data conditions, we simulated data from structural models with one parameter per DAG arrow in Figure <ref type="figure" target="#fig_8">9</ref>. We manipulated the probability that gene causes disease (3 levels: {0, 0.35, 0.7}), the probability that gene prevents treatment from working (3 levels: {0, 0.35, 0.7}), and sample size (2 levels: {100, 1000}). We controlled the probability that treatment prevents disease (0.8), probability of disease due to unobserved causes (0.2), base rate of the gene (0.4), and the proportion of each sample with treatment (0.5). We selected these parameters iteratively by sampling data sets and labeling ground truth until half of trials had greater than a 25% chance of having been generated by causal explanation D.</p><p>As in Experiment 1, we simulated many data sets for each data condition, and we counterbalanced quantiles of sampling error across participants (see Section 3.1.4). For our attention check trial, we selected the simulated data set that maximized causal support for confounding. Labeling ground truth causal support. We extended Griffiths and Tenenbaum's model of causal support <ref type="bibr" target="#b20">[21]</ref> to account for more than two alternative causal explanations. We primarily targeted causal support for causal explanation D over explanations A, B or, C,</p><formula xml:id="formula_3">cs D = log Pr(C|model D ) ∑ i={A,B,C} Pr(C|model i ) + log Pr(model D ) ∑ i={A,B,C} Pr(model i )</formula><p>where C is the data set we label with ground truth, and model A , model B , model C , and model D correspond to causal explanations A through D (Fig. <ref type="figure" target="#fig_8">9</ref>), respectively. Since we separately targeted belief in both of the component DAG arrows that constitute a confounding relationship (see Section 4.1.1, perceived causal support), we needed to calculate (cs BD ) ground truth causal support for explanations B or D over A or C:</p><formula xml:id="formula_4">cs BD = log ∑ i={B,D} Pr(C|model i ) ∑ i={A,C} Pr(C|model i ) + log ∑ i={B,D} Pr(model i ) ∑ i={A,C} Pr(model i )</formula><p>We similarly calculated (cs CD ) causal support for explanations C or D. The first terms in the formulae for cs D , cs BD , and cs CD are log likelihood ratios representing the relative compatibility of a given data set with causal explanations A, B, C, and D. We calculated the log likelihood of each data set we showed participants given model A , model B , model C , and model D using Monte Carlo simulations similar to Algorithm 1. In Experiment 2, we introduced one more parameter Pr(¬T |G) to our structural models, representing the probability that the gene prevents the treatment effect. We incorporate this parameter into our Monte Carlo simulations (Alg. 1) by making the following substitutions: The second terms in the formulae for cs D , cs BD , and cs CD are log ratios of the prior probabilities of the target explanation(s) versus other possible explanations. Again, we assumed a uniform prior to create an unbiased benchmark for our task such that 25% was the normative prior probability for each causal explanation A, B, C, and D, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Performance evaluation</head><p>Again, we used linear in log odds (LLO) models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b62">61]</ref> to describe discrepancies between perceived and normative causal support. We also conducted a qualitative analysis of participants' reported strategies. Model specification. We used three inferential models because we had three dependent variables, representing perceived causal support for confounding (lrr D ) and for the two constituent effects of confounding (lrr BD and lrr CD ). Here, we show only the models on lrr D and lrr BD because the models on lrr CD and lrr BD are identical in form, with cs CD and delta p t replacing cs BD and delta p d as predictors: , and lrr CD were perceived causal support for a confounding, the gene effect on disease, and the gene effect on treatment, respectively, cs D , cs BD , and cs CD were our normative benchmarks corresponding to each log response ratio, delta p d was the difference in the proportion of people with disease given gene versus no gene, delta p t was the difference in the proportion of people with disease among those in the treatment group given gene versus no gene, n was the sample size as a factor, vis was a dummy variable for visualization condition, and worker id was a unique identifier for each participant.</p><formula xml:id="formula_5">lrr D ∼Normal(μ D , σ D ) μ D =cs D *</formula><p>We primarily modeled effects on the mean of perceived causal support μ D , μ BD , and μ CD , but our models also learned residual standard deviations σ D , σ BD , and σ CD . The residual standard deviations and random effects in each model helped us separate patterns in responses from noise and individual differences. In the first model, we used the term cs D * delta p d * delta p t * n * vis to learn how sensitivity to causal support for confounding varies as a function of sample size n and visualization vis. In the second and third models, we used the terms cs BD * delta p d * n * vis and cs CD * delta p t * n * vis to learn how sensitive users in each visualization condition were to the gene effects on disease delta p d and treatment delta p t, respectively. Qualitative analysis. We wondered how well participants would intuit how to perform the confounding detection task, considering it was more difficult than the task in Experiment 1, and we provided no training. To address this we applied a deductive coding scheme. We coded participants' strategy descriptions as uninformative if they didn't describe a strategy. Otherwise, we coded whether or not participants described adequate strategies for judging delta p disease, delta p treatment, or sample size (see Section 4.1.3), and we coded confusion if they stated they were confused or described an incorrect strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Participants &amp; exclusions</head><p>We used a similar approach to power analysis as in Experiment 1 to determine a target sample size of 500 participants after exclusions. We recruited a total of 703 participants, and after exclusions we used data from 519 participants in our analysis. Although we preregistered that we would exclude participants who allocated less than 25% probability to confounding on an attention check trial where confounding was very likely (see Section 4.1.3), this criterion would have excluded 39% of our sample. We relaxed the cutoff to than 20% probability of  confounding to allow for additional response error, resulting in a 26% exclusion rate. We paid participants $3.04 for 14 minutes on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We use a linear in log odds (LLO) model to describe performance in terms of sensitivity to ground truth causal support and bias in probability allocations when all four causal explanations are equally likely. Sensitivity. A LLO slope of one indicates ideal sensitivity to the log likelihood of the data given a set of causal explanations Similar to Experiment 1, slopes in all visualization conditions are closer to zero than one (Fig. <ref type="figure" target="#fig_11">11</ref>), indicating under-sensitivity to the ground truth.</p><p>Interacting with filterbars seems to improve sensitivity, while interacting with aggbars seems to decrease sensitivity, although these differences are not reliable. It is surprising to see a similar pattern of results for interactive visualizations in both experiments, since we expected interactive visualizations to be more helpful for detecting confounding than for detecting a treatment effect. Detecting confounding requires users to look for more complex counterfactual patterns in order to distinguish between causal explanations, and manipulating data aggregation and filtering should help users to query visualizations for these patterns. When we analyze interaction logs (see Supplemental Materials), we see that filterbars users interacted with the visualizations more frequently and created more task-relevant views of the data than Fig. <ref type="figure" target="#fig_0">12</ref>: LLO intercepts per visualization condition. aggbars users, which may help to explain why interacting with filterbars was somewhat more helpful than interacting with aggbars. Visual signal effects on sensitivity. We examine sensitivity in each visualization condition to the three visual signals for confounding in our task (delta p disease, delta p treatment, and sample size; see Section 4.1.3). Normatively, slopes are one regardless of these visual signals.</p><p>Figure <ref type="figure" target="#fig_10">10</ref> shows users are more sensitive to causal support at values of delta p disease and delta p treatment near zero, with the exception of filterbars users who don't interact. This pattern is consistent with the findings of Experiment 1 in that chart users respond more to evidence against a given causal effect than evidence in favor of an effect.</p><p>In Figure <ref type="figure" target="#fig_10">10</ref>, we also see that users of every visualization but filterbars are more sensitive when sample size is smaller. This pattern is consistent with prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> and the results of Experiment 1. Bias. LLO intercepts describe bias in probability allocations when the data are equally likely under each alternative causal explanation. We derive expected probability allocated to explanation D based on LLO intercepts and compare this to the normative benchmark of 25%.</p><p>Figure <ref type="figure" target="#fig_0">12</ref> shows that, with all visualizations but text tables, users underestimate the probability of confounding in the absence of signal. The fact that biases for each visualization condition differ between Experiments 1 and 2 suggests that these results are task-specific. Future work should study reasons for these biases and what visual analytics software can do to help calibrate analysts' probability allocations. Strategies. We assess users' self-reported strategy descriptions. 235 of 519 (45%) users included in our analysis gave uninformative responses and were excluded from further analysis. 42 of 284 (15%) remaining users either stated they were confused or described an incorrect strategy.</p><p>However, many users intuited the important signals in the data: "I relied more on the 'no treatment' cells to consider whether the gene causes the [disease], trying to look at ratio of 'disease' and 'no disease' within those two quadrants... [I] tried to consider the actual counts remembering that small numbers mean loose estimates but this was easy to overlook. Then I compared the two purple bars in the 'gene no' top-half of graph to estimate the treatment effect... and did the same for the two lower purple bars to see if treatment equally effective in those with the gene." 222 of 284 (79%) described an adequate strategy for inferring the gene effect on disease. 81 of 284 (29%) mentioned sample size information. 168 of 284 (59%) described an adequate strategy for inferring the gene effect on treatment effectiveness. These results suggest that much of our data represent a reasonable understanding of the task, yet participants still appeared to struggle to use the visualizations effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We demonstrate the utility of causal support for evaluating inferences with visualizations, successfully measuring expected patterns in the quality of chart users' causal inferences. For example, filterbars users should not have been able to perform either task without interacting because the visual signals required to perform the tasks were hidden behind interactions. Our method shows that filterbars users were completely insensitive to the signal in data when they did not interact. Similarly, our models corroborate prior work suggesting that chart users underweight sample size when making inferences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref>. Findings like these reassure us that causal support can help us understand how users struggle to use visualizations to evaluate causal hypotheses.</p><p>Our findings point to unsolved design challenges for supporting causal inferences with visual analytics (VA) tools. Contrary to what we might expect given the emphasis of visualization research on evaluating encodings and interaction techniques, using different encodings for count data doesn't appear to improve sensitivity to evidence for causal inferences beyond text contingency tables. Similarly, common interaction techniques in VA tools, such as manipulating data aggregation or cross-filtering coordinated multiple views, don't seem to improve causal inferences beyond what users can achieve with simpler static visualizations. Interacting with visualizations seems to help or hurt sensitivity depending on how deliberately signal-seeking users are and whether interacting is necessary in order to expose the visual signal in the data. This suggests that VA tools designed to optimize easy exposure of data are not sufficient for supporting causal inferences.</p><p>We also find systematic biases in the way that chart users respond to specific visual signals in charts. Chart users seem ubiquitously more sensitive to falsifying evidence than they are to verifying evidence. This may reflect a cognitive bias where analysts are more responsive to discrepancies, between observed data and the counterfactual patterns expected under a given causal explanation, than they are to similarities between observed data and counterfactual patterns. Interestingly, this bias may be somewhat rational to the extent that verifying an inference is probabilistic, whereas the logic of falsification is deductive and thus "more powerful" in that it can definitively rule out an explanation <ref type="bibr" target="#b48">[47]</ref>.</p><p>Insensitivity to sample size remains a major challenge for informal statistical inferences, and it appears not to be sufficiently addressed by common chart types for showing count data. Even icon arrays, which emphasize sample size as the number of equal-sized dots, don't seem to mitigate this problem. Prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> suggests this may be due to perceptual underestimation of sample size and cognitive bias against claiming certainty in inferences. Additionally, our qualitative results suggest chart users may not intuitively pay as much attention to sample size as they do to other signals when making causal inferences.</p><p>Consistent with an aversion to believing causal relationships exist, we find that chart users tend to underestimate the probability of a given DAG arrow. In the absence of any signal differentiating between causal explanations, chart users allocate more probability to explanations that posit fewer relationships, rather than allocating probability uniformly across alternatives. Though this tendency interacts with task and visualization in ways that warrant further study, it may reflect an overall cognitive bias toward believing in simpler causal explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Limitations &amp; future work</head><p>We set out to run a proof-of-concept study establishing causal support as an evaluation method for VA tools, and our study raises many unanswered questions. A primary limitation of this work is that we recruited participants on Mechanical Turk, who may be less sensitive to causal support than real data analysts to the extent that they may use VA tools less deliberately. However, our qualitative analysis suggests that many participants understood the task and used reasonable strategies. Future work may find causal support helpful in evaluating current practices or novel interfaces with smaller pools of participants, insofar as real data analysts give less noisy responses than crowdworkers. Questions remain about whether our findings generalize for other data types (e.g., continuous <ref type="bibr" target="#b41">[40]</ref> and event stream data <ref type="bibr" target="#b40">[39]</ref>), for domains outside of medicine, and for analysis scenarios with more complex possible data generating models. Though we suspect our findings will persist in some form across user populations and analysis scenarios, visualizations probably will support some other causal inference tasks better than they support differentiating possible data generating processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Improving visual analytics for causal inference</head><p>A theme in visual causal inference is that analysts do not always know what to look for in data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b61">60]</ref>. Causal inferences differentiating between possible data generating processes (DGPs) require comparisons between patterns in observed data and counterfactual patterns under a specific DGP <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">44]</ref>. Users of VA software may struggle with causal inferences insofar as they fail to imagine counterfactual predictions.</p><p>Prior work in statistics and visualization argues for model checks that make comparisons between data and model predictions explicit <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. For example, workflows in Bayesian statistics frequently employ prior and posterior predictive checks <ref type="bibr" target="#b13">[14]</ref>.Visualizing model predictions alongside empirical data could support causal inference by externalizing discrepancies and similarities between observed and expected patterns.</p><p>We envision a VA workflow where analysts cycle between interactively specifying models (e.g., <ref type="bibr" target="#b34">[35]</ref>) and generating model checks to gauge model compatibility with their data. This echos calls to make models themselves a primary goal of visual data analysis <ref type="bibr" target="#b1">[2]</ref>. Causal support solves an important problem in realizing this vision, defining a "good" model check as one which supports sensitive inferences among a set of candidate DGPs. Though it may be difficult to come up with an exhaustive set of DGPs in many real world applications, we think that this approach would be fruitful even with a relatively simple set of models that a knowledgeable analyst might provisionally entertain. Causal support cannot guard against analysts ignoring possible models, but it can be used to evaluate visualization and interaction designs intended to help analysts collate and compare alternative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We contribute two crowdsourced experiments demonstrating an approach to evaluating causal inferences with visual analytics (VA) tools. No visualization or interaction designs we tested lead to reliably better causal inferences than text contingency tables, suggesting that common VA tools designed for data exposure may not be sufficient for supporting causal inferences. We point to perceptual and cognitive biases which seem to make visual causal inferences difficult, including tendencies to underweight both evidence verifying a causal relationship and evidence from large samples. We discuss how formal models of causal support can be used to evaluate VA systems that place an emphasis on helping users reason about possible data generating processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: DAGs representing possible causal explanations participants were asked to consider in Experiment 1.</figDesc><graphic coords="3,53.99,49.37,250.49,83.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Non-interactive visualizations evaluated in our study: A text contingency tables; B faceted icon arrays; and C faceted bar charts.</figDesc><graphic coords="3,53.75,605.45,143.54,117.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Aggregating bars mimic shelf construction and faceting.</figDesc><graphic coords="3,316.43,49.25,250.82,77.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>Parameters corresponding to each DAG arrow in Fig.2:4: θ P = { # initialize parameters 5: Pr(D), # p disease due to unknown causes 6: Pr(D|G), # p disease due to gene 7:Pr(¬D|T ) # p no disease due to treatment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>14 : 1 − 1 − 1 −•</head><label>14111</label><figDesc>Pr(D) , # no treat ¬T , no gene ¬G Pr(D) + Pr(D) • Pr(¬D|T ), # T , ¬G 17: Pr(D) • 1 − Pr(¬D|T ) , 18: Pr(D | G) • 1 − Pr(D) , # ¬T , G 19: Pr(D | G) + Pr(D) − Pr(D | G) • Pr(D), 20: Pr(D | G) + Pr(D) − Pr(D | G) • Pr(D) # T , G 21: Pr(¬D|T ) + 1 − Pr(D | G) • 1 − Pr(D) , 22: Pr(D | G) + Pr(D) − Pr(D | G) • Pr(D) 23:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Sensitivity (y-axes) conditioned on two attributes of visual signal for treatment effectiveness (rows, x-axes) and visualizations (columns).</figDesc><graphic coords="5,53.75,142.97,137.18,94.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Linear in log odds (LLO) slopes per visualization condition.</figDesc><graphic coords="5,196.79,249.53,107.78,156.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: LLO intercepts per visualization condition.</figDesc><graphic coords="6,187.91,49.25,107.66,150.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: DAGs for possible causal explanations in Experiment 2. 4.1 Method</figDesc><graphic coords="6,307.43,118.85,71.98,69.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>20 :</head><label>20</label><figDesc>(Pr(D|G) + Pr(D) − Pr(D|G) • Pr(D)) • (Pr(¬D|T ) 21 : • (1 − Pr(¬T |G)) + (1 − Pr(D|G)) • (1 − Pr(D)), 22 : (Pr(D|G) + Pr(D) − Pr(D|G) • Pr(D)) 23 : • ((1 − Pr(¬D|T )) + Pr(¬D|T ) • Pr(¬T |G)) Under model A we sampled Pr(D) and Pr(¬D|T ) uniformly on the interval [0, 1] and fixed Pr(D|G) and Pr(¬T |G) at zero, representing assumptions that the gene impacts neither disease or treatment. Under model B we sampled Pr(D), Pr(D|G), and Pr(¬D|T ) uniformly and fixed Pr(¬T |G) at zero, representing the assumption that the gene has no effect on treatment. Under model C we sampled Pr(D), Pr(¬T |G), and Pr(¬D|T ) uniformly and fixed Pr(D|G) at zero, representing the assumption that the gene has no effect on disease. Under model D we sampled all four parameters uniformly to represent confounding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Sensitivity (y-axes) conditioned on three attributes of visual signal for confounding (rows, x-axes) and visualization conditions (columns).</figDesc><graphic coords="8,455.27,191.57,102.98,142.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Linear in log odds (LLO) slopes per visualization condition.</figDesc><graphic coords="8,187.79,343.73,107.78,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Alex Kale is with the University of Washington. E-mail: kalea@uw.edu. • Yifan Wu is with the University of California at Berkeley. E-mail: yifanwu@berkeley.edu.</figDesc><table /><note>• Jessica Hullman is with Northwestern University. E-mail: jhullman@northwestern.edu. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The linear in log odds model &amp; causal support. By choosing to model perceived causal support lrr A (see Section 3.1.1) as a function of ground truth causal support on a log odds scale, we leverage a linear in log odds (LLO) model to extend causal support from a normative cognitive model into a descriptive one. Prior work shows that the LLO model accurately describes natural distortions in mental representations of probability<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b62">61]</ref>. For example, visualization researchers<ref type="bibr" target="#b30">[31]</ref> used the LLO model to measure perceptual distortions in probabilistic judgments about intervention effectiveness. Our normative model of causal support itself (see Section 3.1.4) is a sum in log odds units. Derived measures. Using a LLO model to measure the correspondence between normative and perceived causal support enables us to estimate (1) participants' sensitivity to changes in ground truth causal support and (2) bias in perceived causal support. From our model, we</figDesc><table /><note>26:log lik = ∑ i∈{1,...,m} ∑ C • log(P) − log(m) 27: # Main: causal support calculation 28: Calculate likelihood of data given causal explanations A and B: 29: log lik A = MONTE CARLO(C, 10000, {}) 30: log lik B = MONTE CARLO(C, 10000, {Pr(¬D|T )}) 31: return causal support for explanation A: # Bayesian update 32: cs A = log lik A − log lik B + log(0.5) − log(0.5)3.1.5 Performance evaluationWe wanted to measure how much participants' causal inferences deviated from our normative benchmark, causal support. derive sensitivity and bias per condition as LLO slopes and intercepts, respectively. LLO slopes describe sensitivity to ground truth causal support such that a slope of one indicates ideal sensitivity. One can think of slopes as the weight participants assign to changes in the ground truth log likelihood ratio of explanations A versus B. LLO intercepts describe bias in participants' probability allocations when causal support guessing responses are informed by the number of response alternatives<ref type="bibr" target="#b33">[34]</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>delta p d * delta p t * n * vis + cs D * delta p d + cs D * delta p t + cs D * n worker id lrr BD ∼Normal(μ BD , σ BD ) μ BD =cs BD * delta p d * n * vis + cs BD worker id where lrr D , lrr BD</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Bonuses in Experiment 2 were based on the probability of explanation D. 3 E.g., see Experiment 2 interface at https://bit.ly/3rDcxfn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the UW IDL and the NU MU Collective for their feedback. We thank NSF (#1930642) for funding this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causal inferences as perceptual judgments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Sheu</surname></persName>
		</author>
		<idno type="DOI">10.3758/BF03197251</idno>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="510" to="524" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Viewing Visual Analytics as Model Building</title>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lammarsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rind</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13324</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="275" to="299" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Causal inference and the data-fusion problem</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1510507113</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7345" to="7352" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intuitive Strategies and Preconceptions about Association in Contingency Tables</title>
		<author>
			<persName><forename type="first">C</forename><surname>Batanero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Estepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Godino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal for Research in Mathematics Education</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="169" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The visual design and control of trellis display</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Shyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="155" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Model of Nonbelief in the Law of Large Numbers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
		<idno type="DOI">10.1111/jeea.12139</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the European Economic Association</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="515" to="544" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative Feedback Explains Distinct Brain Activity Codes for Seen and Mental Images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Breedlove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>St-Yves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Olman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naselaris</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cub.2020.04.014</idno>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">brms: Bayesian Regression Models using &apos;Stan</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Bürkner</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Readings in Information Visualization Using Vision to Think</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Some properties of the dirichletmultinominal distribution and its use in prior elicitation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chalone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Duncan</surname></persName>
		</author>
		<idno>doi: 10. 1080/03610928708829384</idno>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics -Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="523" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From covariation to causation: A causal power theory</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1037//0033-295x.104.2.367</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="405" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Representation construction, externalised, cognition and individual differences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cox</surname></persName>
		</author>
		<idno>doi: 10. 1016/S0959-4752</idno>
	</analytic>
	<monogr>
		<title level="j">Learning and Instruction</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="51" to="56" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Causality visualization using animated Growing Polygons</title>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFVIS.2003.1249025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE Symposium on Information Visualization, INFO VIS</title>
				<meeting>-IEEE Symposium on Information Visualization, INFO VIS</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualization in Bayesian workflow</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<idno type="DOI">10.1111/rssa.12378</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series A: Statistics in Society</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="402" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using Icon Arrays to Communicate Medical Risks: Overcoming Low Numeracy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Galesic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia-Retamero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0014474</idno>
	</analytic>
	<monogr>
		<title level="j">Health Psychology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="216" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Bayesian formulation of exploratory data analysis and goodness-of-fit testing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1751-5823.2003.tb00203.x</idno>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="382" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploratory data analysis for complex models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="755" to="779" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How to Improve Bayesian Reasoning Without Instruction: Frequency Formats</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hoffrage</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.102.4.684</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="684" to="704" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the Shape of the Probability Weighting Function</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-89824-785</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="129" to="166" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Confounding and collapsibility in causal inference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greenland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno>doi: 10.1214/ ss/1009211805</idno>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="46" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure and strength in causal induction</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno>doi: 10.1016/j. cogpsych.2005.05.004</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="334" to="384" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A cognitive interpretation of data analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Grolemund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<idno type="DOI">10.1111/insr.12028</idno>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using natural frequencies to improve diagnostic inferences</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hoffrage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gigerenzer</surname></persName>
		</author>
		<idno type="DOI">10.1097/00001888-199805000-00024</idno>
	</analytic>
	<monogr>
		<title level="j">Academic medicine: Journal of the Association of American Medical Colleges</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="538" to="540" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bias in Proportion Judgments: The Cyclical Power Model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Hollands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Dyre</surname></persName>
		</author>
		<idno>doi: 10. 1037//0033-295X.107.3.500</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="524" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Why Authors Don&apos;t Visualize Uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<publisher>Institute of Electrical and Electronics Engineers</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">To design interfaces for exploratory data analysis, we need theories of graphical inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Data Science Review</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hypothetical Outcome Plots Outperform Error Bars and Violin Plots for Inferences about Reliability of Variable Ordering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0142444</idno>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual Causality Analysis of Event Sequence Data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvcg.2020.3030465</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing causal semantics using animations</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Kadaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leboe</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70528</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1254" to="1261" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptation and learning priors in visual inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VisXVision Workshop at IEEE VIS 2019</title>
				<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10">Oct 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual reasoning strategies for effect size judgments and decisions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030335</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="272" to="282" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">When (ish) is My Bus? User-centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM annual conference on Human Factors in Computing Systems</title>
				<meeting>the 2016 ACM annual conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Bayesian Cognition Approach to Improve Data Visualization</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krafft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Human Factors in Computing Systems (CHI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Psychophysics: A Practical Introduction</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Kingdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Prins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Elsevier Ltd</publisher>
		</imprint>
	</monogr>
	<note>first edition ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Northstar: An interactive data science system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<idno type="DOI">10.14778/3229863.3240493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2150" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Assessing the effect of visualizations on Bayesian reasoning through crowdsourcing to cite this version</title>
		<author>
			<persName><forename type="first">L</forename><surname>Micallef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fekete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2536" to="2545" />
			<date type="published" when="2012">2012</date>
			<publisher>Institute of Electrical and Electronics Engineers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>O'hagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Daneshkhah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Eiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Garthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oakley</surname></persName>
		</author>
		<author>
			<persName><surname>Rakow</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-985x.2007.0050614.x</idno>
		<title level="m">Uncertain Judgements: Eliciting Experts&apos; Probabilities</title>
				<meeting><address><addrLine>Chichester, West Sussex, England</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons Inc</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">170</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Afergan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ziemkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2015.2467758</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="529" to="538" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Upsetting the contingency table : Causal induction over sequences of point events</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pacer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Conference of the Cognitive Science Society (CogSci&apos;15)</title>
				<meeting>the 37th Annual Conference of the Cognitive Science Society (CogSci&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A rational model of causal induction with continuous causes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Pacer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011. 2011. 2011</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1214/09-SS057</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics Surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="96" to="146" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Generalizing Experimental Findings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1515/jci-2015-0025</idno>
		<imprint>
			<date type="published" when="2015-07">July. 2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="259" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">External Validity: From Do-Calculus to Transportability Across Populations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno type="DOI">10.1214/14-STS486</idno>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="595" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The book of why: the new science of cause and effect</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Basic Books</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Debroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Authors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heisterkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Willigen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>and R-core. nlme: Linear and Nonlinear Mixed Effects Models</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pirolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Intelligence Analysis</title>
				<meeting>International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005-05">2005. May 2014. 2005</date>
			<biblScope unit="page" from="2" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The problem of induction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Popper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The logic of discovery</title>
				<meeting><address><addrLine>Hutchinson, London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1959">1959</date>
			<biblScope unit="page" from="27" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Stefii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pirolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<title level="m">The Cost Structure of Sensemaking. CHI &apos;93 Proceedings of the INTERACT &apos;93 and CHI &apos;93 Conference on Human Factors in Computing Systems</title>
				<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The Analysis of Contingency Tables</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Sobel</surname></persName>
		</author>
		<idno type="DOI">10.2307/3617686</idno>
	</analytic>
	<monogr>
		<title level="m">Handbook of Statistical Modeling for the Social and Behavioral Sciences, chap. Chaper 5 T</title>
				<editor>
			<persName><forename type="first">G</forename><surname>Arminger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Clogg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Sobel</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page">251</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Surgical Audit: Statistical Lessons from Nightingale and Codman</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="58" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On the psychophysical law</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="153" to="181" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Polaris: A system for query, analysis, and visualization of multidimensional relational databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="65" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Exploratory data analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Addison-Wesley Pub</publisher>
			<pubPlace>Reading, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Why do we perceive logarithmically? Significance</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-02">February. 2013</date>
			<biblScope unit="page" from="28" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The Visual Causality Analyst: An Interactive Interface for Causal Reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467931</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="230" to="239" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Visual Causality Analysis Made Practical</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST.2017.8585647</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visual Analytics Science and Technology</title>
				<imprint>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
	<note>VAST 2017 -Proceedings, (October)</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Symbolic Description of Factorial Models for Analysis of Variance</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="392" to="399" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A visual analytics approach for exploratory causal analysis: Exploration, validation, and applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<title level="m">Illusion of causality in visualized data. arXiv</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An exploratory user study of visual causality analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H E</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13680</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="173" to="184" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Ubiquitous log odds: A common representation of probability and frequency distortion in perception, action, and cognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Maloney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lange</surname></persName>
		</author>
		<idno>doi: 10. 3389/fnins.2012.00001</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2012-01">January. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
