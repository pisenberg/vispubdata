<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VisQA: X-raying Vision and Language Reasoning in Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Th</forename><surname>Éo Jaunet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Corentin</forename><surname>Kervadec</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Romain</forename><surname>Vuillemot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Grigory</forename><surname>Antipov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Wolf</surname></persName>
						</author>
						<title level="a" type="main">VisQA: X-raying Vision and Language Reasoning in Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7D3462D8AD704C7BCD0A9A751DB61663</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transformers</term>
					<term>Visual Question Answering</term>
					<term>Visual analytics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Opening the black box of neural models for vision and language reasoning: given an open-ended question and an image , VISQA enables to investigate whether a trained model resorts to reasoning or to bias exploitation to provide its answer. This can be achieved by exploring the behavior of a set of attention heads , each producing an attention map , which manage how different items of the problem relate to each other. Heads can be selected , for instance, based on color-coded activity statistics. Their semantics can be linked to language functions derived from dataset-level statistics , filtered and compared between different models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual Question Answering (VQA) systems <ref type="bibr" target="#b4">[5]</ref> attempt to answer questions provided as input in textual form together with a corresponding image. As an example, asking the question "Is the knife in the top part of the photo?" associated with the input image shown in Figure <ref type="figure">1</ref> should yield the answer "No". Direct applications of such systems are support for the visually impaired, semi-autonomous robot navigation through language instructions, and, more generally, AI tools covering a broad spectrum of tasks guided through language input. In particular, VQA serves as a testbed for learning high-level reasoning from data, as the performance of targeted models and methods relies on advances in Computer Vision (CV), Natural Language Processing (NLP), and Machine Learning (ML). The task deals with large varieties, and solving an instance can involve visual recognition, logic, arithmetic, spatial reasoning, intuitive physics, causality, and multi-hop reasoning. It also requires to combine two modalities of different nature, images and language.</p><p>Recent VQA models are based on a powerful type of deep neural networks called transformers <ref type="bibr" target="#b55">[56]</ref>. Originally developed for NLP tasks, these models have been extensively applied to VQA <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62]</ref> and recently even on pixel-level in pure image based problems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b64">65]</ref>. Transformers are conceptually simple models, which, however, can learn very complex relationships between the items of unordered sets, each of which is represented as a (learned) embedding in a vector space. Making sense of a learned neural model and verifying its inner workings is a difficult problem, which we address in this work.</p><p>In this paper, we focus on a typical and important problem arising with trained neural models, and in particular models for vision and language reasoning: as they are trained with supervision to provide correct answers, they often tend to find shortcuts in learning and learn to exploit spurious biases in training data instead of the desired reasoning a human would apply in a similar situation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44</ref>]. To provide an example, if a model is asked "What is the color of the banana"? with an image of a "banana", it might learn to answer "yellow" whatever the real color of the image is, as this is probably the correct answer for a large majority of input images -solving the correct reasoning problem is a much harder task. An exact definition of the term "correct reasoning" is difficult, we refer to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref> and define it as algebraically manipulating words and visual objects to answer a new question. In particular, we interpret reasoning as the opposite of exploiting spurious biases in training data.</p><p>Existing work on bias reduction and on the evaluation of bias origins tends to focus on statistical techniques, whose power lies in quantitative evaluation and visualizations on dataset-level, showing full or marginal distributions of inputs, features, and outputs, and resort to dimensionality reduction. While these techniques are very useful, their power is limited when we search for insights into detailed inner workings of neural models, for which an investigation per sample is more helpful. Only for a single instance, it is possible to observe the origins for lack of reasoning, which can include, aside from errors in the trained reasoning module itself, also problems in the input pipeline (the object detection module) and wrong annotations of ground truth data.</p><p>We introduce VISQA, an instance-based visual analytics tool designed to help domain experts, referred to as model builders <ref type="bibr" target="#b26">[27]</ref>, investigate how information flows in a neural model and how the model relates different items of interest to each other in vision and language reasoning. Attention maps are at the heart of transformer-based deep networks, and as such are the primary object studied by VISQA. It allows an expert to browse through image and question pairs sorted by an automatic estimate of the amount of reasoning that went into answering each sample. Once a pair is selected, users can explore the different attention maps represented as heatmaps. The exploration is guided by their position in the model, but also by color codes which convey the intensity of each head, i.e. whether they focus attention narrowly on specific items, or broadly over the full input set. Complementary dataset-wide statistics are provided for each selected attention head, either globally, or with respect to specific reasoning modes or language functions, e.g. "What is", "Where is", "What color" etc. While the tool is post-hoc, it is also interactive and allows certain modifications to the internal structure of the model. At any time, attention maps can be pruned to observe their impact on the output answer.</p><p>VISQA is the result of a collaboration between experts in visual analytics, and with experts in Visual Question Answering systems and Machine Learning. As will be detailed in section 4, this collaboration, and data gathered using VISQA, led to improvements of the reasoning capabilities of transformer-based models by introducing new methodological contributions in machine learning and computer vision, which we also recently reported in a different associated publication <ref type="bibr" target="#b32">[33]</ref>. The usability of VISQA has been evaluated by different experts in deep learning, who were not involved in the project or its design. We report experiments with qualitative interviews, and results in section 8.</p><p>In this work, we contribute to a better understanding of bias in VQA models as follows:</p><p>• VISQA an interactive visual analytics tool which helps experts to explore the inner workings of transformers models for VQA by displaying models' attention heads in an instance-based fashion.</p><p>• A set of visualizations to address bias in VQA systems designed to explore models' performances in real-time with altered attention, and/or by asking free-text questions.</p><p>• Insights on the emergence bias in transformers for VQA gained by experts through an in-depth analysis using VISQA, along with an evaluation of its usability to estimate models' predictions and eventually bias exploitation.</p><p>VISQA is available online as an interactive prototype https://visqa. liris.cnrs.fr, and our code and data are available as an open-source project: https://github.com/Theo-Jaunet/VisQA .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We first introduce some background on understanding neural networks in vision and language reasoning, the context of this paper, and we will provide a short and concise introduction into transformers, the type of neural networks which currently dominates academic and industrial research in language reasoning, and in vision-based language problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformers and Attention</head><p>Following the introduction and success of transformers applied to natural language processing tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">56]</ref>, transformer-based models were also proposed for VQA <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref>. Their key strength is the ability to contextualize input representations, i.e. to take input items like words and objects, each one represented in a vectorial form called "embedding", and to enrich them, adding information on relationships. This is achieved by series of transformations of the input vectors, which effectively encodes the reasoning process, and the underlying key mechanism is attention (self-attention). We start with a brief overview of how a typical language transformer works by applying it to encode the question "What is the name of the clothing item that is white?".</p><p>Step 1: Preparation: Sentence Tokenization. We split the question into elementary language items (called "tokens") with the Word-Piece tokenizer <ref type="bibr" target="#b59">[60]</ref>. Two special tokens are added at the beginning and at the end of the sentence (respectively 'CLS' and 'SEP'). While the latter encodes the end of the sentence, the former is of the uttermost importance; it is transformed by the model as are the other tokens, with the difference that the 'CLS' token is transformed to encode the task-specific information, and the answer. In the given example, at the end of the transformation, it is expected to contain the information required to predict the name of the white clothing. Each token (including special tokens) is then projected into a high-dimensional vector space through a learned dictionary, resulting in a sequence of N token embeddings:</p><formula xml:id="formula_0">L = [l CLS , l 1 ,...,l i ,...,l N−2 , l SEP ], l i ∈ R n , n being the (chosen) embedding dimension.</formula><p>Step 2: Attention Maps. Transformers progressively contextualize each of the N input embeddings by a sequence of self-attention operations (layers), with the objective of making each token embedding "aware" of the neighboring embeddings. In our example, it might be helpful to combine the information from the embeddings of tokens 'item', 'clothing' and 'white' into one "enriched" embedding describing the referred object, as the three words are semantically related. More generally, the so-called "self-attention operations" (layers) are the key elements of this type of model. They are implemented via the calculation of attention maps A = {α i j }, which reflect the N 2 pairwise interactions between the tokens, each α i j being the similarity between token embeddings i and j. The similarity function, here, the scaled dotproduct, is calculated between a trainable projection of the embedding i, called query, and a trainable projection of the embedding j, called key. The per-token attention energy is normalized into a probability distribution using a row-wise softmax. In our example, the row vector Question and image are first tokenized and then encoded using vision (in green) and language (in blue) only transformers <ref type="bibr" target="#b55">[56]</ref>, followed by (bidirectional) inter-modality transformers <ref type="bibr" target="#b53">[54]</ref>. The answer is predicted from the "CLS" token. Yellow and orange rectangles represent, respectively, inter-and intra-modality attention heads. i and j are the layer and head indices used for naming attention heads through the paper.</p><p>A 7 = {α 7 j } j∈{0,...,N−1} encodes the N interactions between 'clothing' and the other words.</p><p>Step 3: Token Updates. Each token embedding is updated as a linear combination of a trained function of the input embeddings, called values, weighted by the attention map A. Hence, the model transforms each token by learning a strategy for looking at specific other words.</p><p>Multi-headed Attention. As more classical neural networks, Transformers are organized into a sequence of layers. Each of these layers bundles together multiple attention heads working in parallel, which allows the model to learn different cooperating strategies. Different heads might learn different syntactic or semantic language functions, as shown in <ref type="bibr" target="#b57">[58]</ref> for language models, and as we will show in the experimental section for vision and language reasoning. The outputs of the attention heads are combined with standard neural network blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision-Language (VL)-Transformers</head><p>Transformer have been extended to reasoning on multiple modalities, in particular vision and language, through different types of layers: Language-only and vision-only layers, referred to as intra-modal layers, and language-vision layers, referred as the inter-modal ones. Fig. <ref type="figure" target="#fig_0">2</ref> depicts the transformer architecture designed for VQA which we call "VL-Transformer". Each layer is named as X i j, where X denotes the layer type (e.g. vision-only intra-modal layer, vision-language inter-modal layer, etc.) and i and j index, respectively, layer and head.</p><p>Intra-modality. Both modalities are first processed in two independent streams (cf. Fig. <ref type="figure" target="#fig_0">2</ref>): heads lang i j encode question words, and vis i j heads encode visual objects detected in the image by an off-the-shelf object detector, a mainstream approach in VQA <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">54]</ref>. Visual embeddings are the concatenation of 2048-dimensional object appearance embeddings and 4-dimensional bounding box coordinates.</p><p>Inter-modality. Subsequent layers combine information between both modalities, c.f. Fig. <ref type="figure" target="#fig_0">2</ref>, in a bidirectional way: from question words to visual objects in lv i j, and vice-versa in vl i j (lv means 'language to vision' while the opposite vl means 'vision to language'). This requires a minor, but essential, modification of the attention mechanism. Intuitively, and a bit simplified, in vision-to-language heads, the language (word) embeddings are transformed by taking each word and checking its similarity to the full set of visual input objects, and vice-versa for language-to-vision heads. Details are given in supplementary materials. As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, each lv or vl attention head is immediately followed by an intra-modal attention head called, respectively, vv or ll.</p><p>Predicting the answer. The answer is produced by decoding the final representation of the "CLS" token using a 2-layered neural network. It predicts a probability vector over the most frequent answers found in the training set, the answer with the highest score is then chosen.</p><p>Training details. For the experiments in this paper, we set the embedding size to d=128 and the number of heads per-layers to h=4. Following <ref type="bibr" target="#b53">[54]</ref>, our model is composed of 9 language only and 5 vision only intra-modality transformers layers, and 5 language → vision and vision → language layers. In addition to the VQA objective, we train the model parameters also on MS-COCO <ref type="bibr" target="#b38">[39]</ref> and Visual-Genome <ref type="bibr" target="#b34">[35]</ref> images following the semi-supervised BERT <ref type="bibr" target="#b15">[16]</ref>-like strategy introduced in <ref type="bibr" target="#b53">[54]</ref>. In particular, the model is trained to perform simple tasks such as recognizing masked words and visual objects, or predicting if a given sentence matches the question. After pre-training on these auxiliary tasks, the model is fine-tuned on the GQA <ref type="bibr" target="#b28">[29]</ref> dataset with the VQA objective. Our VL-Transformer is a variant of the LXMERT model <ref type="bibr" target="#b53">[54]</ref>, in line with the many works adapting BERT <ref type="bibr" target="#b15">[16]</ref>like pre-training to vision and language tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Discussion. In this paper, we focus on the interpretation of the attention maps, as they contain crucial cues on the internal reasoning in transformers. These maps highlight to what extent a given token has been contextualized by which neighbors, high attention α i j indicating strong interaction between tokens i and j. We argue, that attention maps provide strong insights on how our the model handles interactions between the question the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Our work is related to building visual analytics tools for interpretability of Deep Learning. Our design targets in particular the study of attention maps from transformers models to grasp insights on their potential exploitation of bias. This section reviews previous work on the visual analysis of deep learning models, and a review of previous work from machine learning communities to tackle bias in VQA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Analytics for Interpretability</head><p>In recent year, the visual analytics community has joined forces with machine learning experts and provided contributions improving the interpretability of deep neural networks <ref type="bibr" target="#b26">[27]</ref>, by providing insights on their inner workings. Those models are often considered as black-boxes due to the large amount of parameters and data they manipulate to reach a decision. Prior work focused on the analysis of image processing models, known as convolutional neuronal networks, by exposing their gradients over the inputted images <ref type="bibr" target="#b62">[63]</ref>. This approach, enhanced with visual analytics <ref type="bibr" target="#b40">[41]</ref>, and provided glimpses on how the neurons of those models are sensible do different patterns in the input. More recently, CNNs have been analyzed through the prism of attribution maps in works such as Activation-atlas <ref type="bibr" target="#b9">[10]</ref> and attribution graphs <ref type="bibr" target="#b25">[26]</ref>.</p><p>In the other side, natural language processing (NLP) with recurrent neural networks, have also been explored through static visualization <ref type="bibr" target="#b29">[30]</ref> which provided insights, among others, on how those models can learn to encode patterns in sentences beyond their architectures in capacities. Interactive visual analytics, works such as LSMTViz <ref type="bibr" target="#b51">[52]</ref>, and RetainVis <ref type="bibr" target="#b35">[36]</ref> have also addressed the interpretability of those models through visual encoding of their inner parameters, which can then be filtered and completed with additional information. Those parameters are collected during forward pass on models, as opposed to RNNbow <ref type="bibr" target="#b11">[12]</ref>, which has the particularity to focus on visualizing gradients of those models thought back-propagation during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interpretability of Attention</head><p>More recently, models with attention <ref type="bibr" target="#b55">[56]</ref> increasingly gained popularity due to their improvement of state-of-the-art performance, and their attention mechanisms which may be more interpretable than CNNs and RNNs. The interpretability of attention models similar to the transformer models used in this work, initially designed for NLP, has also been addressed by visual analytics contributions. Commonly, in works such as <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57]</ref>, the attention of those models is presented, in instance-based <ref type="bibr" target="#b26">[27]</ref> interfaces as graphs with bipartite connections that can be inspected to grasp how input words are associated to each-other. Attention Flows <ref type="bibr" target="#b14">[15]</ref> addresses the influence of BERT pre-training on model predictions by comparing two transformers models applied to NLP. Similarly to our work, such a tool displays an overview of each attention heads with a color encoding their activity. Those methods are specific to NLP tasks. In this work, we address the challenges provided by the bi-modality of vision and language reasoning, and expend the interpretability of VQA systems which can rely on visual cues or dataset biases. Current practices of VQA visualization include attention heatmaps of selected VL heads based on their activation <ref type="bibr" target="#b36">[37]</ref> to highlight word/key-object associations, global overview heatmaps of attention heatmaps towards a specific token <ref type="bibr" target="#b8">[9]</ref>, and guided backpropagation <ref type="bibr" target="#b24">[25]</ref> to highlight the most relevant words in questions. Following those works, VISQA provides a visualization of every head's attention heatmaps and word/object associations, along with an overview of their activations.</p><p>Our focus is on post-hoc interpretability <ref type="bibr" target="#b39">[40]</ref>, i.e., the analysis of a trained model's decision policy after-the-fact. Our approach rely on instance-based analysis, which displays inner model parameters with respect to a current input. Such analysis is often combined with direct manipulation mechanisms designed to let users experiment with desired input conditions, like drawing the input <ref type="bibr" target="#b10">[11]</ref>. Our working hypothesis is that a transformer-based model's mode of operation, i.e. whether it is reasoning or exploiting dataset biases, is observable from its trained parameters, and in particular from attention maps, an intermediate representation dependent on parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bias Reduction in VQA</head><p>Bias reduction has been addressed on the data side through cleaning and balancing. In particular, GQA <ref type="bibr" target="#b28">[29]</ref> focuses on semantics with the help of human-annotated scene graphs and automatically generated questions. On the contrary, VQAv2 <ref type="bibr" target="#b23">[24]</ref> dataset is crowdsourced, which lead to more natural questions, but includes cognitive and/or social biases <ref type="bibr" target="#b18">[19]</ref> as well as annotation mistakes. In addition, evaluation benchmarks have been specifically designed to identify the presence of bias dependencies. VQA-CP <ref type="bibr" target="#b2">[3]</ref> proposes to evaluate the models bias dependency by introducing distribution shifts between training and testing sets. However, this approach has limitations to evaluate the decisions and biases exploitation of models as it emphasizes the diversity in answers rather than the reasoning itself. As result, random predictions can also contribute to improving a model's score on this benchmark <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b54">55]</ref>. On the other side, GQA-OOD <ref type="bibr" target="#b31">[32]</ref> undertakes a similar strategy while keeping the training set distribution untouched. Instead, the authors propose to evaluate the VQA performances on question-answer pairs regarding their frequency in the dataset. This makes it possible to evaluate both in-and out-of-distribution performances and, at the same time, estimating the reasoning ability of the system. Indeed, if a model's prediction is correct while the questionanswer pair is rare, there are chances that the model has not used statistical biases. These datasets and evaluation benchmarks have lead to the conception of diverse bias-reduction methods. While an exhaustive survey of these methods is out of the scope of this paper, one can mention families of approaches such as training regularization <ref type="bibr" target="#b49">[50]</ref>, or counterfactual learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In this work, we define bias as the way how a model learns regularities or shortcuts from its training dataset, which may push it to provide answers which it frequently encountered as correct during training, without even considering information from the input image. Technically, we evaluate bias as introduced in <ref type="bibr" target="#b31">[32]</ref>. Hence, every question from the dataset is grouped using their topic (e.g., "furniture") and function (i.e., task extracted from the semantic of the question). Both the semantic and topic metadata are provided by the GQA dataset. Then for each kind of question, the frequency of their answers is computed. We estimate that the model exploits bias when it incorrectly predicts an answer which is among the 20% most frequent answers for the given question, while its ground-truth answer is among the 20% least frequent. While our proposed tool allows to partially open the black-box of transformer-based neural vision and language models in a very general sense, making it possible to inspect how they handle relationships between data items, we nevertheless specifically focus on the important problem of bias reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MOTIVATING CASE STUDY</head><p>This work was primarily motivated by growing concerns in the field over bias exploitation of models trained in large scale settings, in particular when trained on very broad problems like vision and language reasoning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref>. Our own contributions in this area include new benchmarks <ref type="bibr" target="#b31">[32]</ref> and additional supervision and regularization for neural models <ref type="bibr" target="#b30">[31]</ref>. Here we extend our previous efforts by providing a tool for instance-level visualizations and performing visual analytics on Fig. <ref type="figure">3</ref>. When asked "Is the knife in the top part of the photo" x the tiny-LXMERT model, with the image of a knife at the bottom y, incorrectly outputs "yes" z with more than 95% confidence. While an exploitation of bias can be considered, we can observe that the answer "yes" represents only 33% of answers of similar questions over the complete dataset. Thus in-depth analysis of the attention of the model may be required to grasp what led to such a mistake. a single sample. This choice was ultimately taken when comparisons of different trained model through statistics failed to provide concrete answers on the sources of confusions and errors. Statistical models enabled us to drill down examinations to a certain minimal level of aggregation, for instance linguistic function groups, but this kind of analysis was not fine-grained enough.</p><p>We illustrate the advantages and the power of instance-level visualizations with our contribution, VISQA on the following case study, which is also available in video form in the supplementary material. It is based on the exploration of a tiny version of the state-of-the-art neural model LXMERT <ref type="bibr" target="#b53">[54]</ref> as described in sec. 2.2. We provide it with the following input instance, i.e., the image given in Fig. <ref type="figure">3</ref>(y), and associated question "Is the knife in the top part of this photo?" x. The correct ground truth answer is of course "No", but the baseline tiny-LXMERT model incorrectly answers "Yes" z. We see the frequency of the different possible answers provided in the interface, and observe that the wrong answer "Yes" is not the most frequent one for this kind of question as "No" is the correct answer 67% of the time, which does not provide evidence for bias exploitation. The objective is to use VISQA to dive deeper into the inner workings of the model.</p><p>A first step is to analyze whether the model is provided with all necessary information. While the input image itself does contain all clear picture of the answer, the neural transformer model reasons over a list of objects detected by a first object detection and recognition module (Faster R-CNN <ref type="bibr" target="#b47">[48]</ref>) which may output errors. In the rest of this paper, we will refer to this tiny-LXMERT as the noisy model. Is the knife detected by the vision module? VISQA provides access to the bounding boxes of the objects detected by the input pipeline. Each bounding box can be displayed superimposed over the input image along with the corresponding object label predicted by the object recognition module. We can observe that the key object "knife" lacks a suitable bounding box or class label, it has not been detected. Since this object is required to answer the question for this image, the model cannot predict a coherent answer. However, the question remains why the wrong answer is "yes", corresponding to the presence of a knife. Can attention maps provide cues for reasoning modes? VISQA focuses on attention maps, which are a key feature of transformer-based neural models, as they fully determine relationships between input items. Users can select different heads and explore the corresponding attention maps. For the example case, we are interested in checking the correspondence between the question word "knife" and the set of bounding boxes, which should provide us with evidence whether the model was capable of associating the concept with the visual object in the scene, which is, of course, not sufficient for correctly answering, but a necessary step. This verification is non-trivial, however, since the model is free to perform this operation in any of the inter-modality layers and heads. VISQA allows to select the different heads, and we could observe that none of the heads provides a correct association.</p><p>As an example we can see the behavior of a head in Fig. <ref type="figure">4</ref> x, which associates the word "knife" to various objects, mostly fruits. No other Fig. <ref type="figure">4</ref>. Visualization of a selected vision-to-language head and attention map for two different models. x the noisy model associates the "knife" word with a large number of different objects, including fruit. y the oracle model learns a perfect association between the word "knife" and the "knife" object; z the oracle transfer model associates the word "knife" with two different bounding boxes of type knife handle, whose embeddings are sufficiently close for correct reasoning. Head selections are not comparable between models, we therefore checked for permutations.</p><p>head is found indicating a more promising relationship. Is computer vision the bottleneck? From the example above, as well as similar observations in other instances, we conjecture that the computer vision input pipeline (notably, the imperfect object detector) is one of the main bottlenecks in preventing correct reasoning. To validate this hypothesis, we explored training an Oracle model with perfect sight, which thus takes as input the ground truth objects provided by human annotation instead of the noisy object detections by a trained neural model. This improves the performance of the model considerably, reaching ∼80% accuracy on the difficult questions with rare ground truth answers, compared to ∼20% for the standard model reasoning on noisy input. This particularly high difference in performance for questions with rare answers suggests a higher performance in correct reasoning of the oracle model. By loading this model into VISQA, we observe in Fig. <ref type="figure">4</ref> y, that there exists an attention map which associates the word "knife" to a visual object "knife", which, we recall, is an object indicated through human annotation. This correct association is reassuring, but by itself does not yet guarantee correct reasoningfurther exploration is possible, but we will now concentrate on this problem of finding correspondences between words and visual objects and explore this question further. Transferring reasoning modes. Given these observations, we conjecture that a transformer model with perfect sight can learn modes of reasoning which are less biased than models trained on real but noisy input data, which is an insight we gathered from using VISQA as a tool for visual analytics. Oracle models, on the downside, are not deployable to real life problems, as they work on human annotations only, per definition. We explore a solution to transfer reasoning modes from oracle models to noisy input data, which can be done using knowledge transfer by parameter initialization <ref type="bibr" target="#b60">[61]</ref>. In more detail, we pretain the Oracle model on ground truth data, once it reaches convergence, use its weights as initialization for the model using noisy inputs.</p><p>We load this model, which we call Oracle Transfer, into VISQA, with the objective of exploring whether reasoning modes have been transferred into a deployable model capable of providing answers given real input. Fig. <ref type="figure">4</ref> shows the vision-to-language attention maps from the same layer as the ones explore above (Note: Each layer contains several attention heads, and these heads are not ordered. Reasoning associated with a given head in one model can correspond to the reasoning mode of a different head in another model. We cope for these possible permutations in our experiments by searching over all possible heads of a layer). We can observe that the model's attention is drawn towards "handle" visual objects, which are parts of the only knife in the picture. While the object classes "knife" and "handle" are logically different, we can conjecture that their vectorial feature embeddings are different, but sufficiently similar to allow reasoning. More importantly, we can deduce that the model adapted to the absence of the knife object by relying on what is available, i.e. knife handles. This leads the model to correctly answering "No", as did the Oracle model on ground truth data, and in contrast to the baseline noisy model without transfer -see also the illustration in Fig. <ref type="figure">1</ref>. We further describe this Oracle knowledge transfer, and improvements it provides to transformer-based VQA in a different associated publication <ref type="bibr" target="#b32">[33]</ref>. This model will be used in evaluations performed by deep learning experts in section 8. All models, noisy, oracle and oracle transfer, can be tested and explored online in a prototype available at: https://visqa.liris.cnrs.fr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN GOALS</head><p>Prior to the design of VISQA, when working on an associated publication <ref type="bibr" target="#b32">[33]</ref>, we conducted discussions with experts (co-authors of this paper), about their workflow to analyze bias in VQA systems. From those discussions, and literature review introduced in sec. 3, we distill their needs in the following four main themes of design goals for instance-based analysis.</p><p>G1 Examine the performances of each instance for a given model. To investigate bias in VQA systems as introduced in sec. 3.3, it is first important to examine the model predictions, along with its confidence score with respect to its inputs. In order to be useful, those predictions need to be combined with the ground-truth, to estimate if the model is wrong, and how frequent is the ground-truth answer and predictions. Inputs need to be inspected as well as they may convey ambiguities that may be at the beginning of an explanation for a mistake. Finally, due to a large amount of data available for inspection, experts may prioritize inputs the more likely to be biased, i.e., those with infrequent ground-truth answers and frequent predictions. G2 Browse the attention of the model for an instance. Analyzing the attention maps of LXMERT models is crucial for understanding what factors influenced its decision, and eventually whether or not the model attends to both language and vision. While visualizing individually each attention map is feasible, we aim at improving such an exploration, by contextualizing each attention head with their neighborhood (i.e., other attention heads directly connected to it), and position within the model. This is relevant as attention heads get closer to the output, they both encapsulate previous attention, and may be more influential on models' decisions. In addition, experts may need to prioritize heads conveying salient attention, thus those heads need to be summarized and/or emphasized. Finally, for in-depth analysis, experts need to visualize the complete attention map and link each of their elements to human-understandable information, i.e., words of the question and bounding boxes within the input image. G3 Link attention to language tasks. Once a relevant attention head is observed by an expert, the user should be able to contextualize it with the rest of the dataset. In particular, experts are interested in evaluating whether or not this head is responsive to certain tasks provided by the semantics of questions (e.g., find a color), or rather if the head is responsive to certain topics (e.g., clothing). Such information can be provided in VQA training dataset, considered as categories. G4 Explore alternative scenarios. Once cues on how the model uses its attention to output a decision are gathered, the next step, is to test this knowledge by querying the model on altered input or parameters. Ultimately the experts desire to answers questions such as: "would the model have a similar attention if the question were on another object of the image?", or "is this head or group of heads relevant for the final decision?". This can be regrouped into two categories first, the possibility to ask free-from questions, and second the possibility to modify the model's attention. In order to be usable, and due to the number of queries an expert may need to execute, both those manipulations require to interact with the model in a reasonable amount of time, e.g., less than a couple of seconds.</p><p>We designed VISQA, an visual analytics tool designed to facilitate indepth analysis of the internal structure of transformers models applied to visual question answering. VISQA implements attention heads and interactive heatmaps visualizations, and other interactions such as freeform question and pruning mechanisms, to assess whether a model resorts to reasoning or bias exploitation when answering questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Workflow</head><p>Through iterative design resulting from frequent meetings with VQA experts co-authors of this work, we extracted the following workflow of use of VISQA, based on their experience analyzing VQA systems, and the mantra overview first, zoom and filter, then details on demand <ref type="bibr" target="#b48">[49]</ref>:</p><p>1. the user picks and loads into the model an instance, informed by the likeliness of the question to be answered with bias; 2. the result of the model is presented both as attention heads (internal structure of the model), as well as the top-5 predicted answers; 3. the user then may interact with the model internal structure (e.g., heads intensity, attention maps, etc.) which triggers updates to the statistical views; 4. bounding boxes are displayed on the input image to reflect based on attention heat-map selections, showing how the model associates words and visual objects.</p><p>VISQA can also be used beyond this typical workflow for in realtime query of the model by asking user-defined questions, or pruning attention heads, as further detailed in Sec. 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Visualization of Instances</head><p>At its core, VISQA is organized following an end-to-end approach, from model input to output. This is done to contextualize attention heads with their neighborhood and position with the model (G2). We added input selection to guide users in their exploration, and a visual summary of the attention of the model to keep it visually compact. Image ranking-by-feature (Fig. <ref type="figure">1</ref> x). In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in <ref type="bibr" target="#b31">[32]</ref>: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. "Yellow bananas"). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly. Answers and their frequencies are displayed in (Fig. <ref type="figure">1</ref> y right) following design goal (G1).</p><p>Instance view (Fig. <ref type="figure">1 y</ref>). This view, inspired by VL-Transformer representations illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>, is the root of any analysis of attention maps using VISQA (G2). It matches the internal data flow through the internal structure of the model, from left to right: the input image/question pair, layers and heads with intra-modality layers first, and finally the answer output distribution (encoded as horizontal bars). A particular design decision was to display all attention maps at once, using a single colored rectangle encoding the attention intensity as k-number <ref type="bibr" target="#b46">[47]</ref> (see next paragraph for details). In the Instance view, attention heads can be selected with a mouse-over interaction as illustrated in Fig. <ref type="figure">1</ref> z in order to provide details on demand, by displaying the corresponding attention heat-map Fig. <ref type="figure">1</ref> { and head statistics. Visual summary. Our model uses 136 attention maps with dimensions varying with respect to the number of words in the question and bounding boxes provided. As displaying all of those matrices would prevent experts to analyze them in a reasonable amount of time, we rely on summarizing each of them to a single scalar. Such a scalar, referred to as k-number <ref type="bibr" target="#b46">[47]</ref>, represents the normalized amount of tokens per row summed up to reach a threshold of 90% of energy. A k-number close to 0 indicates that the corresponding row has peaky attention focusing on only one column (as seen in Fig. <ref type="figure">4</ref> x), and a high k-number encodes a uniform attention (as in Fig. <ref type="figure">4 y</ref>). Then we combine each of those k-number together using either min, median, or max functions. Such functions can be selected in VISQA by users, depending on the attention maps intensity they want to investigate. VISQA provides this interaction because for a head to have a low k-number, the majority of its rows needs to be highly activated. This can shadow attention maps with less than half of their rows with peaky attention. In VISQA, the k-number is discretized and color encoded in 4 categories as it follows:</p><p>. The decision to use a logarithmic discretization and color encoding, as introduced in <ref type="bibr" target="#b46">[47]</ref>, was done to emphasize peaky attention maps (k &lt; 12) that need to be prioritized for analysis. In addition, this discretization, used throughout VISQA, is particularly useful to regroup instances in head-stat view Fig. <ref type="figure">1</ref> { and select them by clicking on the corresponding square (legend on the right of y in Fig. <ref type="figure">1</ref>) for Head pruning further detailed in section 6.4. Finally, this reduces the learning curve of VISQA, as experts are familiar with this discretization of k-numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Visualization of Selected Heads</head><p>VISQA provides details for a selected head in the Instance view using the attention map and head statistics. Attention Maps (Fig. <ref type="figure">1 |</ref>). are represented as heat-maps, with cell colors encoding the attention intensity over a sequential, single hue scale from no attention ( i.e., 0) in beige, to full attention (i.e., 1) in brown. This matrix-based approach contrasts with bipartite connections representations found in Seq2Seq-Vis <ref type="bibr" target="#b50">[51]</ref> and BertViz <ref type="bibr" target="#b56">[57]</ref>. We use matrices as they provide a clutter-free representation of attention and they can display multiple heads at once by aggregating connections. However, it may suffer from visual clutter issues as the number of words grows. Seq2Seq-Vis tackles this issue by using interactions to hide graph lowest connections (i.e., lowest attention). In our case, we argue that visualizing the attention of one head at a time can lead to a better understanding of its function in the model. Such an exploration of head functions is further detailed in Sec.8. Head Statistics (Fig. <ref type="figure">1 {</ref>). are represented using three charts. A vertical area-chart (leftmost chart) represents the distribution of knumbers of the selected head over the complete validation dataset (around 1500 image/question pairs). The vertical axis encodes the values of k-numbers, while the horizontal axis encodes the density of the corresponding k-number. The current k-number, for the selected head, which corresponds to the image/question pair loaded in VISQA, is represented as a horizontal red bar positioned on the vertical axis. This area-chart provides insights such as the detection of useless heads with constant high k-number which can be reduced to calculation on average overall items instead of selecting specific items. In contrast, heads with constant low k-number can be interpreted as conveying key information. More specialized heads, with bi-modal k-number distributions, can also be observed. Two stacked bar-charts represent the k-numbers of the selected head grouped by question operations (G3). Question operations are ground truth information provided by the GQA dataset <ref type="bibr" target="#b28">[29]</ref> describing the semantic reasoning operation of asked questions (e.g., select, query..). Each bar is associated with an operation, and its length encodes how many questions in the dataset are using the corresponding operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Interacting with Models</head><p>VISQA also includes features, complementary to the usual workflow introduced in Sec.6.4, designed to investigate hypotheses on reasoning. Free-form questions. By default, VISQA loads the GQA dataset <ref type="bibr" target="#b28">[29]</ref> to provide images and questions. But at any time, users can type and ask free-form open-ended questions (G4). Such an interaction allows investigating the model's bias exploitation. For instance, when asked the following question from the GQA dataset "Is this a mirror or a sofa", the model correctly outputs "mirror". However, when asked the following user-inputted question "Is there a mirror in this image?", the model fails and outputs "no". This suggests that the model might have exploited biases when it answered the first question, which is supported by the fact that in the GQA dataset, "mirror" is the correct answer to the question "Is this a mirror or a sofa" in 85% of all cases. Head filtering (Fig. <ref type="figure" target="#fig_1">5</ref>). As shown in Fig. <ref type="figure" target="#fig_1">5</ref> x, attention heat-maps feature two interactions. First, hovering with the cursor a row, a column, a cell, which corresponds to an object in the image, automatically displays its corresponding bounding box along with its label over the input (Fig. <ref type="figure" target="#fig_1">5 y</ref>). The user can also, by clicking on a row column or cell, filter attention heads to only keep the ones in which the corresponding clicked element has an attention above a threshold. For rows and columns which contain multiple attention values, such a filtering process will merge those values by using one of three functions min, median, and max depending on a user selection. The result of such a filtering, as displayed in Fig. <ref type="figure" target="#fig_1">5</ref> z, occurs in the instance view in which the size and opacity heads that do not match with the user's query are reduced while the others are preserved. Such interactions facilitate seeking for heads in which a specific association is expected e.g., a word in the question with an object of the image required to answer. Head pruning. Users can select attention heads by clicking on them in the instance view, or by their k-number category. Such a selection can then be used to prune the corresponding head for the next forward of the model. Pruning here means that the attention head does not perform any focused attention, but uniformly distributes attention over the full set of items (objects or words). Each row of a pruned attention map is thus the equivalent of an average calculation. At any time, users can request a new forward pass of the model by clicking on the top left button "ask" (G4), which allows to see the effect of the configured pruning on the model's predictions. This can be used in order to test hypotheses on attention head interpretations as explored in Sec. 8.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">IMPLEMENTATION</head><p>VISQA is available online as an interactive prototype https:// visqa.liris.cnrs.fr, and its code is open-source https://github.com/ Theo-Jaunet/VisQA . The GQA <ref type="bibr" target="#b28">[29]</ref> standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions. By default, VISQA provides around 1500 question/image pairs, but as images are loaded progressively when users request it, such a quantity can be increased without affecting performances. The models have been trained on a significantly larger amount of training data (about 9M image/sentence pairs), which is different from the validation data on which the performance is evaluated. The user interface of VISQA is implemented using D3 <ref type="bibr" target="#b5">[6]</ref>, and directly interacts with transformer models implemented in Pytorch <ref type="bibr" target="#b45">[46]</ref>, using JSON files through a python Flask server. As the possibility to ask free-form questions offers an infinity of possible combinations, each question/image pair is forwarded through the model in a plug-in fashion, i.e., without altering the model and its performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EVALUATION WITH DOMAIN EXPERTS</head><p>We conducted a user study with 6 experts with experience in building deep neural networks, who were not involved in the project or the design process of VISQA. We report on their feedback using VISQA to evaluate the decision process of the Oracle transfer model, with 57.8% accuracy on GQA, introduced in Sec.4 on several provided problem instances, as well as insights they received from this experience. Hypotheses drawn from single instances cannot be confirmed or denied, but as illustrated in the following sections, such a fine-grained analysis aims to provide cues (often unexpected) that can later be explored through statistical evidence outside of VISQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Evaluation Protocol</head><p>For each expert, we conducted an interview session lasting on average two hours. Sessions were organized remotely and began with a training on VISQA, showing step-by-step how to analyze attention maps. During this presentation, experts were able to ask questions. The study then began with questions on 6 problem instances, i.e., image/question pairs loaded into VISQA in a browser window on participants' workstations. Those instances were balanced between the prediction failures and successes, head or tail distributions of question rarity as described in sec. 3.3, as well as our estimation on whether the model resorts to bias for this instance grasped using VISQA. VISQA, configured as conditioned during evaluations is accessible online at: (https://theojaunet.github.io/visqEval/). The model outputs were hidden and the experts were asked to use VISQA to provide an estimate for two different questions: (1) will the model predict a correct answer, ( <ref type="formula">2</ref>) what will it be?, and (3) does it exploit biases for its prediction, or does it reason correctly? During this part of the interview, experts were asked to explain out loud what lead them to each decision. Once those questions were completed, post-study questions were asked on the usability of VISQA, such as "Which part of VISQA is the least useful?", and "What was the hardest part to understand?". Results. The ability of users to predict failures and specific answers of VQA systems has already been addressed through evaluation <ref type="bibr" target="#b12">[13]</ref> under different conditions. The experiment closest to ours is question+image attention <ref type="bibr" target="#b42">[43]</ref> with instant feedback -similarly to ours, users were asked to estimate whether a model will predict a correct answer when provided with attention visualizations of the model, and reaching a similar score of ∼75% accuracy. The difference is that in <ref type="bibr" target="#b42">[43]</ref> attention is overlaid over the visual input, whereas our attention maps allow to inspect reasoning in a more detailed and fine-grained manner, and not necessarily tied to the visual aspects. The similarity in results changes when users are asked to provide the specific answer predicted by the model: this accuracy drops to 61% in our case, and to 51% in <ref type="bibr" target="#b12">[13]</ref>. While our results are promising, they cannot be directly compared to their results due to the different pool and amount of users. Future work will address studies on a larger number of human experts.</p><p>More importantly, our work focuses on qualitative results of bias estimation in which experts obtained a precision of 75% on whether the model exploited any bias. We extracted the ground truth estimate by comparing the rarity of the question, following <ref type="bibr" target="#b31">[32]</ref>. Supplementary material provides more details on the evaluation protocol and experts' performances. These results are encouraging, as they provide a first indication that the reasoning behavior of VL models can be examined and estimated by human users with VISQA. While 75% of performance reasoning vs. bias is not a perfect score, it is also far away from the random performance of 50%, which is important given the large capacity of these models, which contain millions of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Object Detection and Attention</head><p>To provide an answer, a model must first grasp which objects from the image are requested and thus are essential to focus on. Such an association needs to occur early in the model as those objects are needed for further reasoning. The experts widely observed high intensity in the first language-to-vision (LV) layer. As illustrated in Fig. <ref type="figure">6</ref>, when asked "Is the person wearing shorts?", the attention map LV 0 1 has peaky activations in the columns "person" and "shorts". This can be interpreted as the model correctly identifying with its self-attention for language that those two words are essential to answer the given question. In Fig. <ref type="figure">6</ref>, the word "person" is associated with the bounding boxes labeled as "woman","shirt","shoe","leg", while the word "shorts" is associated with the "shorts" bounding box. Based on this observation, Fig. <ref type="figure">6</ref>. When asked "Is the person wearing shorts?", the oracle transfer model successfully answers "yes". It can be observed in its first Language-to-Vision attention maps, that the word "shorts" (column) is strongly associated with the object "shorts" (row). The same phenomenon is also observed for the word "person', strongly associated with objects labeled as "woman" among others.</p><p>all experts concluded that the model correctly sees the required objects, and more broadly over the evaluation instances, that the first LV layer might be responsible for the recognition of objects with respect to the question. One of the experts mentioned that therefore, "if we don't see a good word/bounding-box association here, the model can hardly cope with such a mistake and might exploit dataset biases". In order to verify such a statement, we pruned the four heads in this LV layer, to observe how the model would behave with no association in them. From such pruning, we observe that the following vision-to-language (VL) layers have lower attention distributions, close uniform in some cases. In addition, after pruning, the model's prediction wrongly switched from "Yes", a rare answer (in Tail), to "No", the most frequent one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Questions with Logical Operators</head><p>During the evaluation, experts were shown two instances with questions containing the word "and". Such instances are interesting because, as one of the experts mentioned, "this word has a lot of importance is this question". To answer correctly, the model needs to grasp that it must analyze the image over two different aspects. With the image, illustrated in Fig. <ref type="figure">7</ref>, and asked "Are there both knives and pizzas in this image?", the model fails and answer "yes", the most frequent answer despite having no knife in the picture nor provided bounding-boxes. However, when asked "Are there knives in this image?" the model correctly answers "no". This suggests that the model failed to grasp the meaning of the keyword "and", and thus that the self-attention language heads might associate wrong words. Also, swapping the terms "knives" and "pizzas" in the question, yields the correct answer, i.e., "no". This indicates that the model ignores the first term when questions contain the operator "and". Using the head-filtering interaction, we can observe that in self-attention heads, the word "and" has little to no attention. Instead, the word "both" has peaky attention scattered across most of self-language layers, and some language-to-language heads. Pruning those 19 heads makes the model correctly yield "no", regardless of the order the words "knives" and "pizzas" are in the question. Such a behavior can be observed over our evaluation dataset, in which 34 questions have the keyword "and". On those questions the model, without pruning, can provide a correct answer 62% of cases, up to 64% with the two words around "and" are swapped. In opposition, while having the 19 attention-heads with peaky attention for the word "both" pruned, the model reached an accuracy of 76%, down to 74% with words around "and" swapped. Thus, in the worst case, this pruning of the 19 attention heads illustrated in Fig. <ref type="figure">7</ref> is responsible for an improvement of 10% on question contain the operator "and".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Vision to Vision Contextualization</head><p>When asked "What is the woman holding?", with the image in Fig. <ref type="figure">8</ref>, the Oracle transfer model fails and outputs "remote-control", a frequent answer, instead of "hair dryer". This can be interpreted as the model exploiting a statistical bias from its training dataset. However, in such a dataset, "remote-control" is not among the 10 most common answers to this question. This raises the question of what leads the model Fig. <ref type="figure">7</ref>. When asked "Are there both knives and pizzas in this image?", the oracle transfer model fails and answers "yes". By filtering heads associated with a selected word, we can observ that language selfattention heads are more responsive to the word "both" x, as opposed to the word "and" y. Fig. <ref type="figure">8</ref>. Without any"hair dryer" provided by the object detector, the oracle transfer associates in its vision-to-language x the object "hand" with the words {"[CLS]","is","?","[SEP]"}. While vision-to-vision focuses on a "knob" object y.</p><p>to output such an answer. During evaluation on this instance, experts noticed that the object detector failed to provide a "hair dryer" object. Similar to the use case given in Sec. 4, such a mistake forces the Oracle transfer to draw its attention towards other bounding boxes related to the missing object. In this case, as observed by experts, a majority of the vision-to-language reached their highest association between the word "holding" and bounding boxes labeled as "hands". Such an association is expected as held objects are directly related to hands, and no "hair dryer" bounding box is provided. Among those bounding boxes, we can observe the presence of one labeled as "television", and another as "knob" which are associated to "holding" and "woman" in both vision-to-vision 2 2 and early vision-to-language layers. This suggests that those heads might have influenced the model's predictions towards "remote-control" instead of the most common dataset bias. This can be confirmed by pruning those heads which yields a more frequent answer: "cell phone". In addition, one of the experts also highlighted that those attention heads had a high association with the tokens "[CLS]", "is", "?", and "[SEP]". Which the expert interpreted as "the model correctly transferred the context of the question".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">DISCUSSIONS, LIMITATIONS AND FUTURE WORK</head><p>Usability of VISQA. Overall, VISQA was positively received by experts, during discussion at the end of interviews, they expressed that VISQA is "well designed", "complete", and "particularly useful as VQA transformers are hard to interpret". Two out of the six experts confessed that they felt "overwhelmed at first", but gradually "grasped where to look", and getting "used to interactions". Four out of six experts stated that the head-statistics 1 { is the least useful feature implemented in VISQA, as it felt "hard to understand". One expert mentioned that such view is "useful in theory, but less usable in practice". However, the rest of the experts mentioned that the headstatistics view helped them while analyzing attention to "see if heads acted out of their distribution". Experts were unanimous, the attention heat-map, and in particular its interactions, is the most useful feature of VISQA, as it "provides a lot of information', and head filtering "speeds up the analysis". One of the expert, used this interaction on the first language to vision layer as entry-point to grasp if the model had every information required to correctly answer the given question. Expert Suggestions. As expressed by experts during evaluation, the main limitation of VISQA is how instances can be selected by users. Currently such selections are handled by a bar at the top of the tool (Fig. <ref type="figure">1</ref> x), displaying images ordered from left to right based on the likelihood of their questions to be answered using statistical biases. However, during interviews, experts mentioned their desire to quickly switch between similar instances in order to grasp if a behavior can be seen across different cases. To do so, experts suggested that such a similarity could be measured at three levels: switch to an instance with the exact same question, switch to a similar image, and switch to an instance in which a selected head has similar attention distribution. In addition, currently in VISQA, it is difficult to evaluate how influential a head is over the model's output. To tackle such an issue, experts proposed to encode this information in the representation size of the instance-view attention heads. The impact of each head over the model's input could be retrieved trough back-propagation, in particular by calculating the gradient of the model outputs with respect to a statistic of the attention head, for instance its k-number. In addition to addressing those limits and experts suggestions, we plan as future works to adapt the usage of VISQA on other problems involving transformers, such as machine translation, and to further investigate the role of each attention heads as done in <ref type="bibr" target="#b57">[58]</ref>.</p><p>Scalability. To date, the largest model loaded in VISQA is LXMERT with 12 attention heads per block and 768 hidden dimensions. Using VISQA, we noted that this model has a similar behavior as the tiny-LXMERT (see supplementary material), and can outperform LXMERT with auxiliary losses <ref type="bibr" target="#b33">[34]</ref>. In addition, LXMERT is less accurate than the tiny-oracle model we used for evaluation, on questions with rare answers-i.e., those that may be the most susceptible to convey biases. While inspecting this model with VISQA, we noted that the more the number of heads increases, the more summary and filtering of heads became relevant for faster analysis. However, our visual encoding of heads may become tedious to use as the number of heads increases, and the amount of pixels allocated per head decreases. A workaround can be to increase the designated space of the model in instance-view, but ultimately, VISQA will be limited by screen realestate. Thus, larger models (e.g., UNITER-large <ref type="bibr" target="#b13">[14]</ref>), may need their heads to be filtered (e.g., by k-numbers) to avoid being overwhelming, before being displayed in VISQA. Similarly, heatmaps of attention may suffer from the same limitation. However, we decided to use them for attention as the maximum visual object (36 in most VQA systems using a Faster R-CNN), and the average of words per question are known beforehand. For larger sets of inputs, alternative designs such as bipartite graphs combined with aggregation methods to hide elements under a threshold of attention may be more space-efficient. However, we argue that this is a trade-off as displaying every bounding boxes and their attention may be relevant to evaluate a prediction (as depicted in sec. 4).</p><p>Generalization. This work focuses on detector-based VL transformers such as LXMERT, however other VQA systems may include single stream models such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b52">53]</ref>. While VISQA is applicable to both single and 2-stream models, we decided to focus on the latest during evaluation. Those models are considered by experts as more interpretable because self-attention layers for language and vision can be observed separately, and there is no significant differences in performance between both architecture <ref type="bibr" target="#b7">[8]</ref>. VQA systems' state-of-the-art often alternate between detector-based approaches (e.g., VinVL <ref type="bibr" target="#b63">[64]</ref>) and pixel models <ref type="bibr" target="#b27">[28]</ref>. However, at its current state, VISQA is not applicable to the latter type, as it would only require adapting to the higher number of visual tokens (depending on the granularity of the method) which raises the challenge of scalability of heatmaps discussed above. Future works can address this through aggregation methods to reduce the number of tokens displayed (e.g., by dividing inputted images in regions). Finally, our work mainly focuses on insights on reasoning skills grasped from models on the GQA dataset. This decision was taken because it has been argued <ref type="bibr" target="#b28">[29]</ref> that it involves a larger variety of reasoning skills (spatial, logical, relational and comparative) than in a dataset with where questions were annoted by humans directly (e.g., VQA2 <ref type="bibr" target="#b23">[24]</ref>) which contains questions targeting difficult external knowledge (e.g., a name of baseball team). Despite not being presented in this work, VISQA can directly be used on the VQA2 dataset.</p><p>Comparison of instances (Fig. <ref type="figure">9</ref>). Comparison between models, Fig. <ref type="figure">9</ref>. Difference between the attentions of Head LV 1 0, when asked "is the the train blue?", and "is the the train red?". We can observe that in this head, the attention focuses on different objects (row) depending on the color asked (column).</p><p>and instances can also yield interesting insights on how a model behaves <ref type="bibr" target="#b14">[15]</ref>. Currently, to this end, VISQA memorizes inputs and all intermediate and final results including k-numbers and attention maps. This state can be saved, and then used at any time and compared to a new current instance through the compare button at the top of the instance-view. The comparison itself is obtained by computing the difference of k-numbers, and complete attention maps. As a result, head representations in instance-view now encode, with a single hue color scale, the difference between the two attention maps. Besides, as shown in Fig. <ref type="figure">9</ref>, attention heat-maps also encodes such a difference using a diverging color scale from dark blue for values close to −1, i.e., a cell with high attention in the previous instance but not in the current one, to brown for values close to 1, the opposite. Comparisons are particularly useful when combined with the possibility to ask freeform questions. As it can be observed in fig <ref type="figure">9</ref>, the manually asked questions "Is the train blue?" and "Is the train red?" are responsible for different attention modes between the word representing the color and bounding boxes of the image. By browsing those bounding boxes, it can be observed that the selected head LV 1 00 associates them, in this case, to the word color only if their color matches. As example, in Fig <ref type="figure">9</ref> (right), the third row, which corresponds to the bounding box of a blue train, is associated through attention with the word "blue" but not the word "red". In order for such a comparison to be relevant, shifts between two instances must be on a few words of the question, as otherwise, attention maps rows and columns would not align between the instances, and thus any difference would occur for wrong reasons. We plan for future work to enhance this functionality of VISQA through better visual encoding, more intuitive interactions, and an evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>We introduced VISQA, an interactive visual analytics tool designed to perform an instance-based in-depth analysis of the reasoning behavior transformer neural networks for vision and language reasoning, in particular visual question answering. VISQA allows users to select display VQA instances based on the likelihood of bias exploitation; to display attention head intensities; to inspect attention distributions; to prune attention heads; and to directly interact with the model by asking free-form questions. Our quantitative evaluations are encouraging, providing first evidence that human users can obtain indications on the reasoning behavior of a neural network using VISQA, i.e. estimates on whether it correctly predicts an answer, and whether it exploits biases. VISQA received positive feedback from these experts, who also provided additional qualitative feedback on the nature of the information they extracted on the behavior of different neural models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An Illustration of the VL-Transformer architecture used in the paper.Question and image are first tokenized and then encoded using vision (in green) and language (in blue) only transformers<ref type="bibr" target="#b55">[56]</ref>, followed by (bidirectional) inter-modality transformers<ref type="bibr" target="#b53">[54]</ref>. The answer is predicted from the "CLS" token. Yellow and orange rectangles represent, respectively, inter-and intra-modality attention heads. i and j are the layer and head indices used for naming attention heads through the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Hovering the mouse over a cell of the attention maps x filters the corresponding object bounding box in the input image y. While clicking on this cell filters attention heads in instance-view to display those within which the selected cell is highly activated.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank reviewers from the previous EuroVis'21 submission, as well as coworkers for their valuable discussions, and proofreading: Nicolas Jacquelin, Aurélien Tabard, and Antoine Coutrot. We finally thank our experts for their time and enthusiasm during the evaluation of VISQA. We also acknowledge grant ANR-20-CHIA-0018 "Remember" of call "ANR AI chairs hors centres".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Counterfactual vision and language learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parvaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10044" to="10054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analyzing the behavior of visual question answering models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07356</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4971" to="4980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">D3: Data-Driven Documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Visualization &amp; Comp. Graphics (Proc. InfoVis)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">From machine learning to machine reasoning. Machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="133" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal pretraining unmasked: A meta-analysis and a unified framework of visionand-language BERTs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Behind the scene: Revealing the secrets of pre-trained vision-and-language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Activation atlas. Distill</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<idno>doi: 10.23915/ distill.00015</idno>
		<ptr target="https://distill.pub/2019/activation-atlas" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experiments in Handwriting with a Neural Network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00004</idno>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rnnbow: Visualizing learning via backpropagation gradients in rnns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Do explanations make VQA models more predictable to a human</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention flows: Analyzing and comparing attention mechanisms in language models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cognitive biases in crowdsourcing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on web search and data mining</title>
				<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6616" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video Action Transformer Network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mutant: A training paradigm for out-of-distribution generalization in visual question answering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="878" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards transparent ai systems: Interpreting visual question answering models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08974</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics (TVCG)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2018.2843369</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weak supervision helps emergence of word-object alignment and improves vision-language tasks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Roses are red, violets are blue</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">but should vqa expect them to? IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How Transferable are Reasoning Patterns in VQA</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Supervising the transfer of reasoning patterns in vqa</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nadri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05597</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Retainvis: Visual analytics with interpretable and interactive recurrent neural networks on electronic medical records</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="309" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">What does bert with vision look at</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (short)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for visionlanguage tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03490</idno>
		<title level="m">The mythos of model interpretability</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
				<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explicit bias discovery in visual question answering models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9562" to="9571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention and augmented recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00001</idno>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schäfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Greiff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02217</idno>
		<title level="m">Hopfield networks is all you need</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The eyes have it: A task by data type taxonomy for information visualizations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The craft of information visualization</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="364" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A negative case analysis of visual grounding methods for vqa</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05704</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Seq2seq-vis: A visual debugging tool for sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">On the value of out-of-distribution testing: An example of goodhart&apos;s law</title>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09241</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05714</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Analyzing multihead self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName><forename type="first">E</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? In NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploring Self-attention for Image Recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
