<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STNet: An End-to-End Generative Framework for Synthesizing Spatiotemporal Super-Resolution Volumes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<addrLine>Notre Dame</addrLine>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
							<email>hzheng3@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<addrLine>Notre Dame</addrLine>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
							<email>dchen@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<addrLine>Notre Dame</addrLine>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Chaoli</forename><surname>Wang</surname></persName>
							<email>chaoli.wang@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<addrLine>Notre Dame</addrLine>
									<postCode>46556</postCode>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STNet: An End-to-End Generative Framework for Synthesizing Spatiotemporal Super-Resolution Volumes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">279508C5BFC4FEA2A30B93D0CE781E2D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time-varying data</term>
					<term>generative adversarial network</term>
					<term>spatiotemporal super-resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present STNet, an end-to-end generative framework that synthesizes spatiotemporal super-resolution volumes with high fidelity for time-varying data. STNet includes two modules: a generator and a spatiotemporal discriminator. The input to the generator is two low-resolution volumes at both ends, and the output is the intermediate and the two-ending spatiotemporal superresolution volumes. The spatiotemporal discriminator, leveraging convolutional long short-term memory, accepts a spatiotemporal super-resolution sequence as input and predicts a conditional score for each volume based on its spatial (the volume itself) and temporal (the previous volumes) information. We propose an unsupervised pre-training stage using cycle loss to improve the generalization of STNet. Once trained, STNet can generate spatiotemporal super-resolution volumes from low-resolution ones, offering scientists an option to save data storage (i.e., sparsely sampling the simulation output in both spatial and temporal dimensions). We compare STNet with the baseline bicubic+linear interpolation, two deep learning solutions (SSR+TSR, STD), and a state-of-the-art tensor compression solution (TTHRESH) to show the effectiveness of STNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We have witnessed the success of deep learning in scientific visualization tasks, such as volume upscaling <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b58">59]</ref>, data reconstruction <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref>, variable and ensemble generation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22]</ref>, rendering image synthesis <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11]</ref>, and representative selection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b17">18]</ref>. However, the end-to-end spatiotemporal super-resolution (STSR) task for time-varying data is unexplored. That is, given a set of sparsely sampled low-resolution volumes (e.g., 128 × 128 × 128 × 50), we aim to generate STSR volumes (e.g., 512 × 512 × 512 × 200). STSR is meaningful for scientific visualization due to its potential applications in data reduction and data recovery. A straightforward solution for generating spatiotemporal solution is to train a spatial super-resolution (SSR) network (e.g., <ref type="bibr" target="#b19">[20]</ref>) and a temporal super-resolution (TSR) network (e.g., <ref type="bibr" target="#b20">[21]</ref>) in sequence, i.e., SSR+TSR or TSR+SSR. An example is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The results show that both solutions do not yield high-quality STSR volumes compared with the ground-truth (GT). The is because the errors in the first stage (e.g., SSR) could accumulate and amplify in the second stage (e.g., TSR). Thus, such sequential solutions may not guarantee to synthesize STSR volumes with high fidelity.</p><p>Three challenges remain for the STSR task. First, although SSR and TSR have been studied independently, merely concatenating the two solutions cannot guarantee satisfactory STSR volumes since the rendering quality is far away from GT. Designing an end-to-end STSR architecture is critical for avoiding error accumulation and amplification. Second, deep learning's success depends heavily on large input data, which is often challenging to acquire in scientific visualization. The limited training data will prevent the network from a better generalization during inference. How to utilize inadequate data samples to improve the generalization power should be considered in the optimization. Third, the computational cost is high as it usually requires days to train a generative adversarial network (GAN) on 3D data. In addition, the synthesized volumes should maintain similar spatial (e.g., structure and texture) and temporal (e.g., close similarity among neighboring time steps) coherence compared with GTs.</p><p>To respond, we design STNet, an end-to-end spatiotemporal generative network for STSR. STNet encompasses two stages: pre-training and fine-tuning. During pre-training, STNet accepts all available lowresolution data as input, generates synthesized low-resolution volumes, and applies cycle loss for optimization. During fine-tuning, STNet takes low-resolution data at early time steps as input, produces STSR volumes, and leverages volumetric and adversarial losses for training. Specifically, we first investigate popular framework designs in both SSR and TSR tasks and design an end-to-end spatiotemporal model with post-upsampling for spatial upscaling and feature interpolation for temporal upscaling. That is, STNet interpolates the feature of each low-resolution volume and upscales the features into the data (super-resolution) space. Second, we customize a pre-training task for STSR by only leveraging the information from low-resolution volumes. The goal is to explicitly promote a better generalization for producing spatiotemporal volumes for time-varying data. Third, we design a spatiotemporal discriminator to guarantee spatial and temporal coherence of the synthesized spatiotemporal volumes. We apply a two-stage optimization procedure to cut the computational cost and boost the stability of GAN training.</p><p>For evaluation, we apply STNet to different time-varying data sets with various characteristics. Volume and isosurface rendering results show that STNet achieves better visual quality than bicubic+linear interpolation, SSR+TSR, STD (a variant of STNet), and TTHRESH <ref type="bibr" target="#b0">[1]</ref> (a state-of-the-art tensor compression algorithm). Moreover, qualitative analysis results also confirm the effectiveness of STNet using three metrics at the data, image, and feature levels.</p><p>The contributions of STNet are as follows. First, we design STNet, a novel end-to-end deep learning model that applies GANs to simultaneously upscales volumes at both spatial and temporal dimensions. Second, we establish a pre-training algorithm that can improve the network's generalization ability. Third, we perform a comprehensive study to investigate the potential impact factors for STSR.</p><p>Deep learning for volume visualization. With the noticeable success of deep learning in computer vision, robotics, and NLP, researchers have explored neural networks' possibility in solving volume visualization problems. Such examples include SSR for volume <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b19">20]</ref> and isosurface <ref type="bibr" target="#b57">[58]</ref>, TSR for volume <ref type="bibr" target="#b20">[21]</ref>, variable selection and translation <ref type="bibr" target="#b21">[22]</ref>, ensemble generation <ref type="bibr" target="#b24">[25]</ref>, volume rendering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11]</ref>, and viewpoint estimation <ref type="bibr" target="#b51">[52]</ref>. Our work is closely related to SSR-TVD <ref type="bibr" target="#b19">[20]</ref> and TSR-TVD <ref type="bibr" target="#b20">[21]</ref>. Instead of only focusing on either SSR or TSR, we upscale volumes at both spatial and temporal dimensions simultaneously in an end-to-end style.</p><p>Deep learning for super-resolution. Deep neural networks have been widely applied in SSR,TSR, and STSR tasks. The examples of SSR include SRCNN <ref type="bibr" target="#b9">[10]</ref>, SRGAN <ref type="bibr" target="#b31">[32]</ref>, ZSSR <ref type="bibr" target="#b52">[53]</ref>, SRFBN <ref type="bibr" target="#b32">[33]</ref>, SRNTT <ref type="bibr" target="#b63">[64]</ref>, and NatSR <ref type="bibr" target="#b53">[54]</ref>. As for TSR, examples are phasebased interpolation <ref type="bibr" target="#b37">[38]</ref>, DVF <ref type="bibr" target="#b35">[36]</ref>, SepConv <ref type="bibr" target="#b41">[42]</ref>, DeepLLE <ref type="bibr" target="#b40">[41]</ref>, and SloMo <ref type="bibr" target="#b28">[29]</ref>. The works of STSR include Xiang et al. <ref type="bibr" target="#b59">[60]</ref>, Shechtman et al. <ref type="bibr" target="#b50">[51]</ref>, Mudenagudi et al. <ref type="bibr" target="#b38">[39]</ref>, Takeda et al. <ref type="bibr" target="#b54">[55]</ref>, and Shahar et al. <ref type="bibr" target="#b49">[50]</ref>. Our work is different from these works in three ways. First, compared with SSR and TSR, we propose a framework with end-toend training for STSR. Second, unlike these computationally expensive STSR solutions with limited capacity to capture complex spatiotemporal patterns, we build a fast and accurate STSR framework. Third, instead of training from scratch in these works, we design a pre-training algorithm to improve the network's generalization ability.</p><p>Network pre-training. Pre-training in deep learning models aims to provide a good parameter initialization for better generalization in a particular task (e.g., classification). Based on different tasks, pretraining examples include inpainting <ref type="bibr" target="#b44">[45]</ref>, colorization <ref type="bibr" target="#b30">[31]</ref>, synthesis <ref type="bibr" target="#b8">[9]</ref>, feature agreement <ref type="bibr" target="#b14">[15]</ref>, rotation prediction <ref type="bibr" target="#b12">[13]</ref>, context prediction <ref type="bibr" target="#b7">[8]</ref>, and feature contrast <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref>. Unlike these pre-training approaches, which are tailored for classification, detection, or segmentation, we propose a novel unsupervised pre-training method for STSR using cycle loss <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b61">62]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STNET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><formula xml:id="formula_0">Let V L = {V L 1 , V L 2 , V L 3 , ••• , V L n } and V H = {V H 1 , V H 2 , V H 3 , ••• , V H</formula><p>n } be low-resolution and high-resolution time-varying volumetric sequences, respectively, where n denotes the number of time steps.</p><formula xml:id="formula_1">V L = {V L 1 , V L 2+t , V L 3+2t , ••• , V L n }</formula><p>is a sparsely sampled low-resolution time-varying volumetric sequence, where t denotes the number of intermediate time steps and t = t + 1 is the temporal upscaling factor (i.e., the sequence is upscaled t times at the temporal dimension). V L is also the pre-training data in STNet (Figure <ref type="figure" target="#fig_1">2 (a)</ref>). In this paper, we set k = 0.2n.</p><formula xml:id="formula_2">V T = {(V L 1 , V H 1 ), (V L 2 , V H 2 ), ••• , (V L k , V H k )}</formula><formula xml:id="formula_3">V S = {V S 1 , V S 2 , V S 3 , ••• , V S n</formula><p>} is a superresolution time-varying volumetric sequence that we aim to generate via STNet. Namely, V H ≈ V S = STNet(V L ). s is the spatial upscaling factor (i.e., each volume is upscaled s times at each spatial dimension). Note that we purposefully refer to the synthesized data as super-resolution data and the original data as high-resolution data for differentiation. Fig. <ref type="figure">3</ref>: Overview of STNet. The network consists of several feature extraction and interpolation (FEI) modules for representing spatiotemporal features and one feature upscaling (FU) module for generating super-resolution volumes. After that, a spatiotemporal discriminator is utilized to discern the spatial and temporal realness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview</head><p>As shown in Figure <ref type="figure">3</ref>, given two-ending low-resolution volumes V L i+t and V L i+1+2t , STNet first leverages t + 1 FEI modules to learn features of the intermediate and the two-ending volumes. One FEI module is tailored for representing the two-ending volumes, and additional t modules are for learning the t intermediate volumes. Once the features are learned, a FU module transforms the spatiotemporal features into a high-dimensional space for generating high-fidelity super-resolution volumes. To discern the spatial and temporal realness of these synthesized volumes, we apply a spatiotemporal discriminator (D) based on convolutional long short-term memory (ConvLSTM) <ref type="bibr" target="#b20">[21]</ref>. D accepts a volume sequence as input and scores each volume's realness through its spatial (the volume itself) and temporal (its previous time steps) information. To optimize STNet, we propose a two-stage pre-training and fine-tuning algorithm. During pre-training, we only utilize the low-resolution volumes (i.e., V L ) to optimize STNet using cycle loss. This stage aims to furnish a proper parameter initialization for STNet, which can boost its generalization ability. During fine-tuning, we use V T as training samples to fine-tune STNet for performance improvement. In the following, we discuss the criteria and rationales for designing spatial and temporal modules. Then, we provide optimization details for the pre-training and fine-tuning stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Framework</head><p>STNet follows a post-upsampling architecture for spatial upscaling and performs interpolation in the feature space for temporal upscaling. The rationales are provided as follows.</p><p>Why choose post-upsampling for SSR? Considering SSR architectures, the most widely used ones are pre-sampling and postupsampling frameworks <ref type="bibr" target="#b56">[57]</ref>. Pre-sampling applies common upscaling approaches (e.g., bicubic and trilinear) for upscaling and follows a series of Conv layers to refine the upscaled data. In contrast, postsampling leverages Convs to represent low-resolution data and upscales the representations to super-resolution data using deconvolutional or shuffle layers. Compared with pre-sampling, post-sampling brings two benefits: speed and performance. First, since most operations perform in the low-dimensional space and only a few operations occur in the high-dimensional space in post-sampling, the computational cost is low. Second, Convs cannot completely eliminate the noises and artifacts introduced by common upscaling approaches in pre-upsampling, while post-upsampling has no such issue in upscaling because Convs already distill data in the low-dimensional space.</p><p>Why perform feature-space temporal interpolation? For TSR architectures, two common options are performing interpolation in the feature or data (super-resolution) space. The feature space refers to the hidden representations of low-resolution volumes, which CNNs usually extract. The data space refers to the space composed of the original volumes. The examples are sketched in Figure <ref type="figure" target="#fig_3">4</ref>. For feature-space interpolation, give two time steps at both ends, we leverage feature extraction and interpolation to generate the feature of each intermediate time step and the two-ending time steps individually. We then use a FU module to generate the super-resolution volumes from these features. For data-space interpolation, a unified representation is learned from all intermediate and the two-ending time steps. Then the feature is upscaled and interpolated in the data (super-resolution) space. Taking into account the involvement of SSR, feature interpolation is a more suitable solution due to the following reason. Applying data-space interpolation requires a powerful FEI module to learn a representation with rich spatiotemporal information for all intermediate and the twoending volumes. It also demands a powerful FU module to transform one feature into t + 2 time steps. This would be difficult, especially in a low-dimensional space, since the information in low-resolution data is limited. For feature-space interpolation, the difficulties of extracting spatiotemporal information and the FU module's demanding capability are mitigated through interpolating multiple features. Generator. The core of generator lies in the feature extraction and interpolation (FEI) and feature upscaling (FU) modules. The FEI module comprises four dense blocks (DBs) <ref type="bibr" target="#b27">[28]</ref>. As shown in Figure <ref type="figure" target="#fig_4">5</ref> (a), in each DB, it includes three Conv layers. Each Conv accepts all previous outputs stacked together as input. In particular, we utilize t + 1 FEI modules to interpolate features of the intermediate and the two-ending volumes. One module accepts the low-resolution volumes as input and produces the corresponding features. The rest of the t modules take the two ending volumes as input and interpolate t features of the intermediate volumes. As sketched in Figure <ref type="figure" target="#fig_4">5</ref> (b), in the FU module, we first separate the input into two branches. In each branch, one voxel shuffle (VS) layer <ref type="bibr" target="#b20">[21]</ref> is used to upscale the input. Then in the second branch, after VS, a Conv and a sigmoid activation function follow. This result is multiplied by the output from the first branch. After merging, two Conv layers and skip connection <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b48">49]</ref> are utilized to produce the final output <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">43]</ref>. The motivation of using this two-branch-based FU module is that the network can estimate the importance of each neuron in the feature maps, and the more important neurons will offer a larger weight in the following convolution computation. This design forces the network to pay more attention to interesting volumetric regions instead of treating interesting (e.g., features) and uninteresting (e.g., background) regions equally. We have only a FU module in the generator, which upscales the intermediate features of all time steps. The architecture detail is given in the Appendix. Note that for generating STSR volumes, we need t + 1 FEI modules: one for representing the two-ending volumes and t for the t intermediate volumes. Rectified linear unit (ReLU) <ref type="bibr" target="#b39">[40]</ref> is applied after each Conv layer except the final output layer. No activation function follows after the output layer. Adding tanh or sigmoid will significantly hurt the performance for some specific data sets (e.g., supercurrent) since tanh will saturate at the tails of −1 and 1 and sigmoid will saturate at the tails of 0 and 1, which could kill the gradient and prevent the network from continuous learning. Spatiotemporal discriminator. We build a spatiotemporal discriminator to judge the spatial and temporal realness of the volumes generated by STNet. As displayed in Figure <ref type="figure" target="#fig_4">5</ref> (c), two Conv layers are utilized to extract spatial information from the input volumes. Each Conv decreases the dimension by half while doubling the channels. Then, temporal coherence is evaluated by incorporating ConvLSTM that accepts the features of the previous and current time steps as inputs. Finally, a Conv and global average pooling (GAP) <ref type="bibr" target="#b33">[34]</ref> layer compresses the feature into a single value, which scores the realness of the input volume. ReLU is picked as the activation function, excluding the ConvLSTM and GAP layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>To optimize STNet, we design a two-stage training algorithm: pretraining and fine-tuning. Pre-training offers an appropriate starting point for training STNet and provides the network with better generalization ability during inference. Fine-tuning fits the network in the downstream STSR task. Pre-training. To pre-train in an unsupervised fashion, we leverage cycle loss to optimize STNet. The cycle loss for V S i is defined as</p><formula xml:id="formula_4">L cyc = ||D(V S i ) − V L i || 2 , (<label>1</label></formula><formula xml:id="formula_5">)</formula><p>where D denotes a downsizing operation (e.g., trilinear) and || • || 2 is L 2 norm. The rationale for designing this loss is that once the superresolution volumes are generated and if we downsize them again to the low-dimensional space, the downsized version (i.e., D(V S i )) should be consistent with the original low-resolution volumes (i.e., V L i ). Fine-tuning. To fine-tune STNet in the STSR task, we leverage volumetric loss for the closeness to high-resolution volumes, and adversarial loss for the realness, to train STNet. The volumetric loss for V S i is defined as</p><formula xml:id="formula_6">L G vol = ||V S i − V H i || 2 . (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>The adversarial losses of generator G and discriminator D for</p><formula xml:id="formula_8">{V S i , ••• , V S i+t+1 } are defined as L G adv = t+1 ∑ j=0 1 − D V S i+ j |V S i+ j−1 , ••• , V S i , (<label>3</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">L D adv = t+1 ∑ j=0 D V S i+ j |V S i+ j−1 , ••• , V S i + t+1 ∑ j=0 1 − D V H i+ j |V H i+ j−1 , ••• , V H i . (<label>4</label></formula><formula xml:id="formula_11">)</formula><p>Considering both volumetric and adversarial losses, we define the total loss of G as</p><formula xml:id="formula_12">L G = λ vol L G vol + λ adv L G adv ,<label>(5)</label></formula><p>where λ vol and λ adv control the weights of these two losses.</p><p>The training algorithm of STNet is listed in Algorithm 1. In the pre-training stage, we first use the sparsely sampled low-resolution sequence V L , as sketched in Figure <ref type="figure" target="#fig_1">2</ref> (a), to train STNet. After training T P epochs, we begin to fine-tune STNet using the low-resolution and high-resolution pairs at the early time steps V T , as shown in Figure <ref type="figure" target="#fig_1">2</ref> (b). Following Wang et al. <ref type="bibr" target="#b55">[56]</ref>, the fine-tuning stage contains two steps. First, only volumetric loss is applied to optimize STNet for training stabilization and computational cost reduction. Second, the spatiotemporal discriminator D is involved in the training procedure for enhancing spatial and temporal coherence. We tested STNet using the data sets reported in Table <ref type="table" target="#tab_0">1</ref>. Note that half-cylinder is an ensemble data set with different Reynolds numbers (i.e., 320, 640, and 6, 400). PyTorch <ref type="bibr" target="#b43">[44]</ref> was used for implementation. Training and inference were performed on an NVIDIA TESLA V100 GPU. The low-resolution data were obtained by applying bicubic kernel with reflection padding. We scaled the range of V L and V H to [−1, 1]. We initialized parameters following He et al. <ref type="bibr" target="#b23">[24]</ref> for optimization and utilized the Adam optimizer <ref type="bibr" target="#b29">[30]</ref> for parameter update. In each mini-batch, one training sample is used. The learning rates for G and D are 10 −4 with β 1 = 0.9, β 2 = 0.999. λ vol = 1 and λ adv = 10 −3 . T P , T F </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Baselines. We compare STNet with three baseline solutions:</p><p>• BL: Bicubic interpolation is used for SSR and linear interpolation for TSR. BL stands for bicubic+linear interpolation. • SSR+TSR: SSR <ref type="bibr" target="#b19">[20]</ref> is a GAN solution for time-varying data SSR, and TSR <ref type="bibr" target="#b20">[21]</ref> is a recurrent generative solution for TSR. We train SSR for 400 epochs and TSR for 400 epochs. • STD: STD is a variant of STNet. Instead of performing temporal interpolation in the feature space, STD directly interpolates the volumes in the data space. Namely, given two volumes at both ends, STD leverages a FEI module to simultaneously learn spatiotemporal features of all intermediate and the two-ending time steps and applies a FU module to generate super-resolution volumes.</p><p>Note that existing STSR works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b49">50]</ref> are not suitable for 3D data since they could not capture complicated spatiotemporal patterns. As for Xiang et al. <ref type="bibr" target="#b59">[60]</ref>, it leverages deformable Conv for spatiotemporal super-resolution. However, it is difficult to extend this architecture to handle 3D data sets for two reasons. First, deformable Conv needs to learn offsets to perform Convs, which requires additional parameters and memories. Second, the offsets are learned from the whole data, not a subregion. Thus, the computational cost is extremely high when the volume is large.</p><p>Section 1 in the Appendix provides a detailed discussion of the two deep learning baselines. The accompanying video shows the frameto-frame comparison results. For the same data set, all visualization results follow the same rendering parameters for lighting, viewpoint, transfer function (used in volume rendering), and isovalue (used in isosurface rendering). Except for the GT results, all results from STNet and baseline solutions are rendered using inferred data from a later time step (refer to Figure <ref type="figure" target="#fig_1">2 (b)</ref>).</p><p>Evaluation metrics. We utilize three metrics, including data-level peak signal-to-noise (PSNR), image-level structural similarity index (SSIM), and feature-level isosurface similarity (IS) <ref type="bibr" target="#b2">[3]</ref>, for quantitative evaluation.</p><p>Quantitative and qualitative analysis. Figure <ref type="figure" target="#fig_6">7</ref> shows volume rendering results produced from BL, SSR+TSR, STD, STNet, and GT using the five jets, half-cylinder (640), and vortex data sets. For the five jets data set, both BL and SSR+TSR produce more cyan parts at the cap, and the rendering results are overly smooth at the legs. STD and STNet generate similar results, but taking a close comparison, STNet synthesizes finer details at the green part (refer to the zoom-ins on the   STNet has more FU modules than STD (t + 1 vs. 1), it demands more time to compute gradients and optimize. However, there is no significant difference for the inference time.</p><p>Figure <ref type="figure" target="#fig_7">8</ref> shows isosurface rendering results among BL, SSR+TSR, STD, STNet, and GT using the five jets, ionization (H), Tangaroa, and supercurrent data sets. For each data set, we pick one isovalue for comparison. For the five jets data set, both BL and SSR+TSR do not capture the isosurface details (refer to the zoom-ins on the right), and the lighting on the isosurface generated by STD shifts too much compared with GT (refer to the zoom-ins on the left). For the ionization (H) data set, STNet can capture finer structures (refer to the zooms-in on the right) compared with other solutions. For the Tangaroa data set, both BL and SSR+TSR do not produce isosurfaces with fine details. In addition, the isosurfaces generated by SSR+TSR contain noticeable noises and artifacts. For STD and STNet, both can synthesize similar isosurfaces compared with GT. However, STNet can extract more details. For example, it produces close isosurfaces at two corners (refer to the zoom-ins). For the supercurrent data set, BL does not extract close isosurfaces compared with GT, while SST+TSR, STD, and STNet pro-duce similar results, and all of them are comparable to GT. In terms of quantitative comparison, Table <ref type="table" target="#tab_3">3</ref> reports the average IS score for BL, SSR+TSR, STD, and STNet. STNet achieves the best performance for all data sets. Note that for the five jets and supercurrent data sets, STD and STNet achieve similar performance. This is because the overall content does not change too much over different time steps for these two data sets (i.e., the training and inference data are similar), which means data-space interpolation could lead to satisfactory results. But to achieve similar performance, STD needs around 36 million parameters while STNet requires about 16 million.</p><p>Comparison with baselines. As shown in Figures <ref type="figure" target="#fig_7">7 and 8</ref>, STNet outperforms SSR+TSR and STD in terms of visual quality and achieves better quantitative scores for most data sets compared with STD. The potential reasons are as follows. SSR+TSR directly uses two networks to perform SSR and TSR, respectively. It forces the latter network (i.e., TSR) to complete two tasks, i.e., TSR and denoising, since the results generated from the former network (i.e., SSR) are not GT, and they contain unobservable noises. These noises could be sensitive <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b25">26]</ref> when synthesizing high-quality volumes. This explains why the rendering results produced by SSR+TSR contain noises and artifacts. We try to add a denoising module into the TSR framework to clean up these noises, but the results are not satisfactory. For STD, it achieves comparable PSNR values for simple data sets (e.g., supercurrent) but cannot generate high fidelity results for complex data sets (e.g., half-cylinder). This is because extracting a global spatiotemporal representation for all intermediate and the two-ending time steps is extremely difficult for these volumes whose patterns change dynamically. We point out that the number of parameters in STD is twice of those in STNet. The reason is as follows. Ideally, to achieve a fair comparison between STD and STNet, we need to set the same number of parameters in both STD and STNet. However, under the model size of 62.5MB, STD cannot generate satisfactory STSR volumes. Therefore, we expand the width (i.e., the number of channels) of STD. Comparison with state-of-the-art compression. Figure <ref type="figure" target="#fig_8">9</ref> shows volume rendering results obtained from the upscaled volumes generated by STNet and the volumes compressed then decompressed using TTHRESH <ref type="bibr" target="#b0">[1]</ref>. We choose TTHRESH, a tensor compression solution, because it smoothly degrades the data, leads to errors smaller than other state-of-the-art algorithms, and requires a low cost for compression and decompression. We consider two scenarios: (1) keeping the same PSNR (i.e., 36.84 dB) for the half-cylinder (640) data set and (2) controlling the same compression ratio (i.e., 206.74×) for the ionization (H) data set. To achieve a fair comparison against TTHRESH, we include the model size in the computation of compression ratio. We utilize a lossless compression algorithm <ref type="bibr" target="#b34">[35]</ref> to further reduce the storage of the saved model and data. For the first scenario, as shown in Figure <ref type="figure" target="#fig_8">9</ref> (a), the image rendered by TTHRESH contains fewer cyan parts. It also produces noticeable noises in the rendering image. The top part of Table <ref type="table" target="#tab_4">4</ref> reports the compression ratios and average SSIM values for both methods. Under the same PSNR, although TTHRESH achieves a higher compression ratio, which is about 10 times compared with that of STNet, STNet achieves a higher SSIM value for the synthesized volumes than those recovered from TTHRESH. For the second scenario, as shown in Figure <ref type="figure" target="#fig_8">9</ref> (b), TTHRESH generates more red parts. The bottom part of Table <ref type="table" target="#tab_4">4</ref> gives average PSNR and SSIM values for both methods. Under the same compression ratio, although TTHRESH produces a higher PSNR value, STNet achieves a higher SSIM value and better visual quality.</p><p>Evaluation of ensemble and multivariate data sets. In Figures 10 and 11, we compare volume and isosurface rendering results from the synthesized volumes given by BL and STNet on ensemble and multivariate data sets to evaluate the generalization ability. We use an ensemble parameter (variable) X S of a data set for training, while another ensemble parameter (variable) X T of the same data set is used for inference (i.e., X S →X T ). For the half-cylinder data set, we test two cases: 640 → 320 and 640 → 6, 400. The complexity increases as the Reynolds number gets large. For 640 → 320, compared with the result generated by BL, STNet produces better visual results in the purple and cyan parts of volume rendering results and extracts finer details of isosurfaces as shown in isosurface rendering results. For 640 → 6, 400, STNet synthesizes more detailed rendering results. For example, the cyan part's lighting and the isosurfaces at the middle and right corners are closer to GT results. As for H → H+ of the ionization data set, BL produces fewer details at the bottom of the ionization for both rendering results. For instance, for volume rendering, the image generated by BL shows more purple color. For isosurface rendering, the lighting at the bottom generated by BL is inconsistent with that of GT. As for quantitative results, STNet also outperforms BL in terms of PSNR and SSIM, as shown in Table <ref type="table" target="#tab_5">5</ref>.  Evaluation of s and t. To analyze the performance of STNet with different s and t, we set s = 4 and s = 8 with different t using the five jets and supercurrent data sets, respectively. As shown in the top rows of Figures <ref type="figure" target="#fig_1">12 and 13</ref>, t = 3 achieves the best quality. However, all of them can capture the overall shape and details of the five jets, and the main difference is the size of the cyan cap. For the supercurrent data set, the rendering results are shown in the bottom rows of Figures <ref type="figure" target="#fig_1">12 and 13</ref>. All produce similar results compared to GT for Based on the above results, our suggestions for choosing s and t for different data sets are as follows.</p><p>• With s = 4, the appropriate value for t could be large for simple data sets (e.g., five jets and supercurrent), where the patterns change slowly over time. The suitable value is determined by the total sample time steps. With sufficient samples, t could be 9 for the supercurrent data set (200 time steps), while with limited samples, t could be 5 for the five jets data set (100 time steps). • With s = 4, for complex data sets (e.g., half-cylinder and vortex)</p><p>where the patterns could evolve rapidly in neighborhood time steps, 3 is a proper value for t. • With s = 8, it is almost infeasible with the current architecture for upscaling these data sets (e.g., half-cylinder and vortex), where the spatial structures are complex. • For data sets (e.g., five jets and supercurrent) where the shapes are simple and less complicated, s = 8 could still work.</p><p>Refer to Section 2 in the Appendix for additional results. Temporal coherence. To compare how well temporal coherence is preserved using BL and STNet, we show five consecutive time steps using the half-cylinder (320) data set. As shown in Figure <ref type="figure" target="#fig_13">15</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Analysis</head><p>To analyze STNet, we study the impact of pre-training and loss function. A detailed discussion is as follows.</p><p>Evaluation of pre-training. To investigate the effectiveness of adding pre-training, we train STNet with and without pre-training. Table <ref type="table" target="#tab_6">6</ref> gives the average PSNR and SSIM under these two training schemes. As we can see, the pre-training can improve about 1dB and 0.01 for the average PSNR and SSIM values, respectively. Moreover, we plot the PSNR curves over the whole sequence, as shown in   Using cycle loss only captures the overall shape of the five jets but could not preserve fine details. This is because different data samples in the high-dimensional space can be downsized to the same data in the low-dimensional space with the same downsizing function (e.g., bicubic) <ref type="bibr" target="#b36">[37]</ref>. Constrained only in the low-dimensional space, the network could jump into an undesired local minimum in the highdimensional space.</p><p>Evaluation of adversarial loss. To study the impact of adversarial loss, we optimize STNet with and without adversarial loss. Table <ref type="table" target="#tab_6">6</ref> reports the average PSNR and SSIM under these two optimizations. Although we can achieve a higher PSNR value without adversarial loss, adding adversarial loss improves SSIM values (i.e., the image-level metric). Moreover, the rendering images also confirm that adversarial loss can boost perceptual quality, as shown in Figure <ref type="figure" target="#fig_16">17</ref>. For example, without adversarial loss, the rendering image produces more red parts at the ionization's head. Therefore, these results demonstrate the usefulness of adversarial loss in improving visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>We have presented STNet, a novel generative solution for producing STSR volumes for time-varying data analysis and visualization. Leveraging post-upsampling and feature interpolation, STNet can synthesize high fidelity super-resolution sequence given two lowresolution volumes at both ends as input. Compared to BL, SSR+TSR, and STD, STNet produces time-varying sequences of better visual quality, both qualitatively and quantitatively. We also compare STNet with TTHRESH to verify its effectiveness.</p><p>STNet can be applied to the in-situ scenario: at simulation time, scientists can store the early time steps for STNet training while saving the later time steps sparsely for storage saving. Our future work aims to improve STNet in three aspects. (1) Adaptive sampling. The current temporal sampling strategy is based on uniform sampling, which may not capture the dynamic pattern well. Ideally, we need to densely store time steps when the pattern evolves rapidly and sparsely save time steps when the pattern changes slowly. Based on adaptive sampling, STNet can better learn the data structures in both spatial and temporal dimensions, and the performance can be improved further. (2) Unsupervised super-resolution. So far, STNet still relies on the high-resolution data as a reference to learn the mapping from low-resolution to super-resolution. However, this requirement impedes its implementation in the in-situ scenario. We want to explore the possibility of generating super-resolution in an unsupervised manner by incorporating knowledge distillation <ref type="bibr" target="#b13">[14]</ref> with cycle loss. (3) Larger spatial upscaling factor. For now, given complex data sets, STNet can only upscale the volumes 4 times along each spatial dimension (leading to 64 times reduction) due to the network's limited capability. We will design a more powerful framework that can handle a larger spatial upscaling factor, such as 8 or 16 <ref type="bibr" target="#b3">[4]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: STSR using two solutions: SSR+TSR and TSR+SSR.</figDesc><graphic coords="1,59.03,401.69,79.19,78.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Illustration of STNet's training and inference data at (a) pretraining and (b) fine-tuning stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is a sequence used for finetuning STNet (Figure 2 (b)), where k is the total training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Illustration of two different temporal interpolation options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Architectures of (a) dense block, (b) FU module, and (c) spatiotemporal discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Illustration of how learnable parameters change (a) without and (b) with pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>7 :</head><label>7</label><figDesc>Comparison of volume rendering results. Top to bottom: five jets, half-cylinder (640), and vortex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>8 :</head><label>8</label><figDesc>Comparison of isosurface rendering results. Top to bottom: five jets, ionization (H), Tangaroa, and supercurrent. The chosen isovalues are 0, 0.5, 0, and −0.2 , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>9 :</head><label>9</label><figDesc>Volume rendering results. Top and bottom: half-cylinder (640) and ionization (H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>10 :</head><label>10</label><figDesc>Variable and ensemble volume rendering results. Top to bottom: half-cylinder (320), half-cylinder (6,400), and ionization (H+).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>11 :Fig. 12 :</head><label>1112</label><figDesc>Variable and ensemble isosurface rendering results. Top to bottom: half-cylinder (320), half-cylinder (6,400), and ionization (H+). The chosen isovalues are −0.2, 0, and −0.2, respectively. volume and isosurface rendering. But taking a close comparison, under t = 10, the isosurface is broken into two parts at the top-left corner (refer to the red arrow). Besides, average PSNR and SSIM values are shown in Figure 14. STNet significantly outperforms BL for the five jets and supercurrent data sets under different settings of s and t. s = 4, t = 7 s = 4, t = 5 s = 4, t = 3 G T s = 8, t = 10 s = 8, t = 6 s = 8, t = 2 Volume rendering results under different s and t. Top and bottom: five jets and supercurrent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>8 ,Fig. 13 :</head><label>813</label><figDesc>, STNet can better capture temporal coherence compared with BL as BL does s = 4, t = 7 s = 4, t = 5 s = 4, t = 3 G T s = 8, t = 10 s = 8, t = 6 s = Isosurface rendering results under different s and t. Top and bottom: five jets and supercurrent. The chosen isovalues are 0.4 and −0.2, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>14 :</head><label>14</label><figDesc>Average PSNR and SSIM under different s and t. Top and bottom: five jets with s = 4 and supercurrent with s = 8.not produce meaningful intermediate time steps. This is because BL only assumes that features evolve linearly, which is not the case for most data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 :</head><label>15</label><figDesc>Volume rendering results of the half-cylinder (320) data set with five time steps (94 to 96). Top to bottom: BL, STNet, and GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Figure 16. The curves indicate that pre-training can boost the PSNR value at almost every time step. As for visual quality, volume rendering results are shown in the top row of Figure 17. Clearly, using pre-training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Fig. 16: PSNR curves with and without pre-training. can generate results closer to GT (refer to the yellow ellipse). These quantitative and qualitative analysis results confirm the usefulness of the pre-training algorithm. Evaluation of volumetric loss. Cycle and volumetric losses serve a similar role in optimization while constraining the volumes in the lowdimensional and high-dimensional spaces, respectively. So, would it still work if we only leverage one loss to train the network? To answer this question, we optimize the network with and without considering volumetric loss. Note that training without cycle loss means removing pre-training, which has been discussed. As shown in Table 6, without volumetric loss, average PSNR and SSIM values drop significantly. As for rendering quality, we display volume rendering results in Figure 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>17 :</head><label>17</label><figDesc>For example, they can keep one time step for every ten time steps simulated and downsize these time steps by 4 at each spatial dimension. During postprocess-Volume rendering results under different loss settings. From top to bottom: Tangaroa, five jets, and ionization (H). ing, the network is trained with the early time steps only. Once trained, they can recover the super-resolution intermediate time steps with high fidelity, given the sparsely output low-resolution time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Variables and dimension of each data set.</figDesc><table><row><cell>data set</cell><cell>variables</cell><cell>dimension (x × y × z × t)</cell></row><row><cell>five jets</cell><cell>intensity</cell><cell>128 × 128 × 128 × 100</cell></row><row><cell>ionization</cell><cell>H, H+, He, He+</cell><cell>600 × 248 × 248 × 100</cell></row><row><cell cols="2">half-cylinder [48] velocity magnitude</cell><cell>640 × 240 × 80 × 100</cell></row><row><cell>supercurrent</cell><cell>rho</cell><cell>256 × 128 × 32 × 200</cell></row><row><cell>Tangaroa [46]</cell><cell>velocity magnitude</cell><cell>300 × 180 × 120 × 150</cell></row><row><cell>vortex</cell><cell cols="2">vorticity magnitude 128 × 128 × 128 × 90</cell></row><row><cell cols="2">4 RESULTS AND DISCUSSION</cell><cell></cell></row><row><cell cols="3">4.1 Data Sets and Network Training</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>: initial parameters θ G and θ D for G and D, number of training epochs in pre-training and fine-tuning stages: T P , T F 1 , and T F 2 , learning rates α G and α D for G and D, respectively. /* pre-training stage */ for j = 1 •••T P do sample low-resolution data from V L ; compute L cyc according to Equation 1; update θ G ; end for /* fine-tuning stage 1: only using volumetric loss to optimize STNet */ for j = 1 •••T F 1 do sample low-resolution and high-resolution data pairs from V T ;</figDesc><table><row><cell>compute L G vol according to Equation 2; update θ G ;</cell></row><row><cell>end for</cell></row><row><cell>/* fine-tuning stage 2: taking temporal coherence into consideration */</cell></row><row><cell>for j = 1 •••T F 2 do sample low-resolution and high-resolution data pairs from V T ;</cell></row><row><cell>compute L D adv according to Equation 4; update θ D ; compute L G according to Equation 5;</cell></row><row><cell>update θ G ;</cell></row><row><cell>end for</cell></row></table><note>1 , and T F 2 are set to 200, 400, and 50 epochs, respectively, for all data sets. All these hyperparameter settings are determined based on experiments. Algorithm 1 STNet training algorithm. Require</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average PSNR (dB), SSIM, and training time (in seconds). The best ones are highlighted in bold (same for other tables in the paper). , except for the average SSIM for the vortex data set and the average PSNR for the five jets and supercurrent data sets.</figDesc><table><row><cell>data set</cell><cell>method</cell><cell cols="4">PSNR SSIM training time data set</cell><cell>method</cell><cell cols="2">PSNR SSIM training time</cell></row><row><cell></cell><cell>BL</cell><cell>27.96</cell><cell>0.751</cell><cell>-</cell><cell></cell><cell>BL</cell><cell>26.92</cell><cell>0.812</cell><cell>-</cell></row><row><cell>five jets</cell><cell cols="2">SSR+TSR 27.30 STD 40.00</cell><cell>0.685 0.892</cell><cell>64.024 24.662</cell><cell>supercurrent</cell><cell cols="2">SSR+TSR 44.54 STD 44.74</cell><cell>0.995 0.995</cell><cell>87.258 31.443</cell></row><row><cell></cell><cell>STNet</cell><cell>39.63</cell><cell>0.901</cell><cell>43.702</cell><cell></cell><cell>STNet</cell><cell>44.71</cell><cell>0.995</cell><cell>66.285</cell></row><row><cell></cell><cell>BL</cell><cell>28.12</cell><cell>0.864</cell><cell>-</cell><cell></cell><cell>BL</cell><cell>21.85</cell><cell>0.853</cell><cell>-</cell></row><row><cell>half-cylinder (640)</cell><cell cols="2">SSR+TSR 24.15 STD 35.60</cell><cell>0.792 0.907</cell><cell>66.187 27.884</cell><cell>Tangaroa</cell><cell cols="2">SSR+TSR 26.96 STD 30.07</cell><cell>0.858 0.879</cell><cell>96.037 37.964</cell></row><row><cell></cell><cell>STNet</cell><cell>36.84</cell><cell>0.944</cell><cell>45.580</cell><cell></cell><cell>STNet</cell><cell>33.26</cell><cell>0.892</cell><cell>65.568</cell></row><row><cell></cell><cell>BL</cell><cell>33.52</cell><cell>0.862</cell><cell>-</cell><cell></cell><cell>BL</cell><cell>29.78</cell><cell>0.749</cell><cell>-</cell></row><row><cell>ionization (H)</cell><cell cols="2">SSR+TSR 43.05 STD 40.67</cell><cell>0.908 0.892</cell><cell>68.123 28.556</cell><cell>vortex</cell><cell cols="2">SSR+TSR 23.89 STD 31.12</cell><cell>0.576 0.693</cell><cell>57.622 24.573</cell></row><row><cell></cell><cell>STNet</cell><cell>43.19</cell><cell>0.913</cell><cell>44.781</cell><cell></cell><cell>STNet</cell><cell>32.73</cell><cell>0.720</cell><cell>39.329</cell></row><row><cell cols="5">left) and the legs (refer to the zoom-ins on the right). For the half-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">cylinder (640) data set, both BL and SSR+TSR do not produce high-</cell><cell></cell><cell></cell><cell></cell></row></table><note>quality rendering results. STD generates the result with tiny noises and artifacts at the front (refer to the zoom-ins on the left) and more purple parts (refer to the zoom-ins on the right). As for the vortex data set, BL, SSR+TSR, and STD produce more blue and red parts over the whole volume. STNet generates closer results compared with GT. For the quantitative results, we report average PSNR and SSIM values in Table2. In general, STNet achieves the best performance among these four solutionsThe model sizes of SSR+TSR, STD, and STNet are 74.0MB, 138MB, and 62.5MB, respectively. As for the training time, STD requires the shortest time for optimization, and SSR+TSR needs the longest time. This is because SSR has one generator and two discriminators, and TSR has a recurrent generator and one discriminator. SSR+TSR needs to optimize two generators and three discriminators to go through one training data sample, incurring an expensive computational cost. Since</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average IS values at chosen isovalues.</figDesc><table><row><cell>data set (isovalue)</cell><cell>BL</cell><cell cols="3">SSR+TSR STD STNet</cell></row><row><cell>five jets (v = 0)</cell><cell cols="2">0.78 0.80</cell><cell>0.88</cell><cell>0.88</cell></row><row><cell cols="3">half-cylinder (640) (v = 0.2) 0.62 0.60</cell><cell>0.74</cell><cell>0.79</cell></row><row><cell>ionization (H) (v =0.5)</cell><cell cols="2">0.71 0.78</cell><cell>0.79</cell><cell>0.81</cell></row><row><cell>supercurrent (v = −0.2)</cell><cell cols="2">0.23 0.96</cell><cell>0.96</cell><cell>0.96</cell></row><row><cell>Tangaroa (v = 0)</cell><cell cols="2">0.57 0.59</cell><cell>0.73</cell><cell>0.75</cell></row><row><cell>vortex (v = −0.2)</cell><cell cols="2">0.82 0.79</cell><cell>0.84</cell><cell>0.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of TTHRESH and STNet. Top: average SSIM under the same PSNR of 36.84 dB. Bottom: average PSNR (dB) and SSIM under the same compression ratio of 206.74×.</figDesc><table><row><cell>data set</cell><cell>method</cell><cell cols="2">compression ratio SSIM</cell></row><row><cell>half-cylinder (640)</cell><cell cols="2">TTHRESH 1745.33 × STNet 162.62 ×</cell><cell>0.902 0.944</cell></row><row><cell>data set</cell><cell>method</cell><cell cols="2">PSNR SSIM</cell></row><row><cell>ionization (H)</cell><cell cols="2">TTHRESH 49.37 STNet 43.19</cell><cell>0.906 0.913</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Average PSNR (dB) and SSIM for ensemble and multivariate data sets.</figDesc><table><row><cell>data set (X S →X T )</cell><cell cols="3">method PSNR SSIM</cell></row><row><cell>half-cylinder (640 → 320)</cell><cell>BL STNet</cell><cell>30.01 35.26</cell><cell>0.886 0.951</cell></row><row><cell>half-cylinder (640 → 6, 400)</cell><cell>BL STNet</cell><cell>26.47 33.86</cell><cell>0.857 0.926</cell></row><row><cell>ionization (H→H+)</cell><cell>BL STNet</cell><cell>33.53 42.99</cell><cell>0.867 0.910</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Average PSNR (dB) and SSIM under different settings.</figDesc><table><row><cell>data set</cell><cell>method</cell><cell cols="2">PSNR SSIM</cell></row><row><cell>Tangaroa</cell><cell>w/o pre-training w pre-training</cell><cell>32.26 33.26</cell><cell>0.883 0.892</cell></row><row><cell>vortex</cell><cell>w/o pre-training w pre-training</cell><cell>31.75 32.73</cell><cell>0.709 0.720</cell></row><row><cell>five jets</cell><cell>w/o volumetric loss w volumetric loss</cell><cell>22.11 39.63</cell><cell>0.621 0.892</cell></row><row><cell>half-cylinder (640)</cell><cell>w/o volumetric loss w volumetric loss</cell><cell>20.62 36.84</cell><cell>0.888 0.944</cell></row><row><cell>ionization (H)</cell><cell cols="2">w/o adversarial loss 43.80 w adversarial loss 43.19</cell><cell>0.904 0.913</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by the U.S. National Science Foundation through grants IIS-1455886, CCF-1617735, CNS-1629914, DUE-1833129, IIS-1955395, IIS-2101696, and OAC-2104158. The authors would like to thank the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TTHRESH: Tensor compression for multidimensional visual data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ballester-Ripoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lindstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pajarola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7324" to="7334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generative model for volume rendering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1636" to="1650" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Isosurface similarity maps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">GLEAN: Generative latent bank for large-factor image super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
				<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Las-soNet: Deep lasso-selection of 3D point clouds</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="204" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
				<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10542" to="10552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep volumetric ambient occlusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1268" to="1278" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05525</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reconstructing unsteady flow data from representative streamlines via diffusion and deep learning based denoising</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2021</biblScope>
			<publisher>In Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSR-VFD: Spatial super-resolution for vector field data analysis and visualization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
				<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FlowNet: A deep learning framework for clustering and selection of streamlines and stream surfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1732" to="1744" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flow field reduction via reconstructing vector data from 3D streamlines using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SSR-TVD: Spatial super-resolution for time-varying data analysis and visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Accepted</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TSR-TVD: Temporal super-resolution for timevarying data analysis and visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="205" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">V2V: A deep learning approach to variable-to-variable selection and translation for multivariate time-varying data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1290" to="1300" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
				<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">InSituNet: Deep image synthesis for parameter space exploration of ensemble simulations</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S G</forename><surname>Nashed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for Learning Representations</title>
				<meeting>International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DNN-VolVis: Interactive volume visualization supported by deep neural network</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
				<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="282" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Super SloMo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for Learning Representations</title>
				<meeting>International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference for Learning Representations</title>
				<meeting>International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast and efficient compression of floatingpoint data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lindstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1245" to="1250" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
				<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PULSE: Selfsupervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2437" to="2445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Space-time superresolution using graph-cut optimization</title>
		<author>
			<persName><forename type="first">U</forename><surname>Mudenagudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="995" to="1008" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
				<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Video frame interpolation by plug-and-play deep locally linear embedding</title>
		<author>
			<persName><forename type="first">A.-D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01462</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
				<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Single image super-resolution via a holistic attention network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Experimental and numerical study of the turbulence characteristics of airflow around a research vessel</title>
		<author>
			<persName><forename type="first">S</forename><surname>Popinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Atmospheric and Oceanic Technology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1575" to="1589" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A deep learning approach to selecting representative time steps for time-varying multivariate data</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Von Ohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE VIS Conference</title>
				<meeting>IEEE VIS Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vector field topology of time-dependent flows in a steady reference frame</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Rojo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Günther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="280" to="290" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Space-time super-resolution from a single video</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3353" to="3360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Increasing space-time resolution in video</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
				<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="753" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">CNNs based viewpoint estimation for volume visualization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<idno>27:1-27:22</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Zero-shot super-resolution using deep internal learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3118" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Natural and realistic single image super-resolution with explicit natural manifold discrimination</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8122" to="8131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatiotemporal video upscaling using motion-assisted steering kernel (mask) regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Beek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Quality Visual Experience</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Mrak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Grgic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kunt</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="245" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ESGAN: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops</title>
				<meeting>the European Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Deep learning for image superresolution: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06068</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Volumetric isosurface rendering with deep learning-based super-resolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3064" to="3078" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep learning-based upscaling for in situ volume visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Situ Visualization for Computational Science</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Childs</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Bennett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Garth</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Accepted</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Zooming Slow-Mo: Fast and accurate one-stage space-time video super-resolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3370" to="3379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">tempoGAN: A temporally coherent, volumetric GAN for super-resolution fluid flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<idno>95:1-95:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Machine Learning</title>
				<meeting>IEEE International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7324" to="7334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Image super-resolution by neural texture transfer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7982" to="7991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
				<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
