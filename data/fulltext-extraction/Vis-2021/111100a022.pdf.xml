<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Superpowers as Inspiration for Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Bon</roleName><forename type="first">Wesley</forename><surname>Willett</surname></persName>
							<email>wesley.willett@ucalgary.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">Pierre Dragicevic and Petra Isenberg are with Université Paris-Saclay</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CNRS</orgName>
								<address>
									<region>Inria, LISN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adriel</forename><surname>Aseniero</surname></persName>
							<email>bon.aseniero@autodesk.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Pierre Dragicevic and Petra Isenberg are with Université Paris-Saclay</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CNRS</orgName>
								<address>
									<region>Inria, LISN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheelagh</forename><surname>Carpendale</surname></persName>
							<email>sheelagh@sfu.ca</email>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Dragicevic</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yvonne</forename><surname>Jansen</surname></persName>
							<email>jansen@isir.upmc.fr</email>
						</author>
						<author>
							<persName><forename type="first">Lora</forename><surname>Oehlberg</surname></persName>
							<email>lora.oehlberg@ucalgary.ca</email>
						</author>
						<author>
							<persName><forename type="first">Petra</forename><surname>Isenberg</surname></persName>
							<email>petra.isenberg@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Pierre Dragicevic and Petra Isenberg are with Université Paris-Saclay</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CNRS</orgName>
								<address>
									<region>Inria, LISN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Bon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Calgary</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Sorbonne Université</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>ISIR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Superpowers as Inspiration for Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9CCB88756A72C8561AD3A9CD006551C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visualization</term>
					<term>superpowers</term>
					<term>empowerment</term>
					<term>vision</term>
					<term>perception</term>
					<term>cognition</term>
					<term>fiction</term>
					<term>situated visualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations' ability to "make the invisible visible" and to "enhance cognitive abilities." Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanisms that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of "visualization superpowers" and highlight opportunities for the visualization community to create new systems and interactions that empower new experiences with data.</p><p>Material and illustrations are available under CC-BY 4.0 at osf.io/8yhfz.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>When highlighting the benefits of visualization, many researchers refer to Card and colleagues' argument <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> that visualizations "amplify cognition" in various ways, including expanding human working memory and enhancing recognition of patterns or outliers. Similarly, work on the "value of visualization" <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b94">94]</ref> has focused on the knowledgegenerating capabilities of visualizations that make people both more effective and efficient at understanding and analyzing data. Stasko <ref type="bibr" target="#b88">[88]</ref> states that visualizations enhance humans' ability to gain insights from data and to understand the essence (or gist) of what the data contains, and to do so more quickly and with greater confidence. More rarely, visualizations are discussed as enhancing people's ability to engage with data intellectually, socially, physically, or emotionally <ref type="bibr" target="#b97">[97]</ref>.</p><p>The desire to enhance human perception, cognition, or experience is not unique to visualization. Fictional narratives in superhero comics, science fiction, and fantasy have long featured characters with extraor-dinary epistemic (knowledge-related) abilities that allow them to see, reason about, and understand phenomena that are otherwise invisible or incomprehensible. For example, Neo from the film The Matrix uses accelerated perception to track the movement of fast-moving objects like bullets. Destiny, an adversary of the X-Men, possesses psionic precognition (destiny perception), which allows her to see and predict future events. Characters such as these offer a rich and expansive sample of enhanced human abilities that can inspire new, impactful visualization techniques and systems-particularly as emerging technologies like extended reality, gestural interfaces, and ubiquitous sensing bring new opportunities for embodied interactions and situated visualizations.</p><p>We contribute two conceptual frameworks inspired by fictional superpowers that (1) characterize the common underlying visual, perceptual, and cognitive mechanisms that form the basis for many fictional abilities, and (2) describe ways in which visualization and other epistemic tools can give the people who use them a sense of objective or subjective empowerment-the increased ability to achieve goals. These frameworks are the result of three years of iterative discussions driven by our experience collecting, organizing, and scrutinizing depictions of enhanced epistemic abilities from a wide range of media including comics, film, television, and video games. During this process, we gathered a broad set of superpower examples, drawing on fan-curated superhero databases and our own exposure to superpowers in fiction. We then examined these abilities in the context of existing visualization and interaction technologies, as well as notions of empowerment from the human-computer interaction (HCI) and visualization literature.</p><p>The resulting conceptual frameworks showcase a variety of visual and cognitive mechanisms (including visual synesthesia, enhanced numeracy, and enhanced comparison), which can be used to critically examine existing visualization systems and to envision new ones. They also suggest concrete ways in which new systems can elicit a sense of empowerment by adapting how visualizations are displayed and controlled, as well as where and when they are shown. Inspired by these frameworks, we envision seven possible systems that highlight the potential for visualizations to augment human vision and cognition in ways that resemble fictional superpowers. Finally, we consider how a superpower-centric framing can impact discussions of the value of visualization, its usefulness as a source for visual design inspiration, dangers of this framing, and the importance of fairness and accessibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The goal of much of human-centered computer science research is to help people extend their abilities-improving people's memory, motivation, organization, perception, etc. The work most closely related to ours investigates characterizations of human abilities and how one can enhance human abilities through system design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Epistemic Activities vs. Other Human Activities</head><p>The HCI and cognitive science research literature has introduced terminology and constructs that distinguish between types of human activities, and by extension, human abilities. Several conceptual frameworks differentiate between activities related to knowledge acquisition and those that change the physical world. Kirsch <ref type="bibr" target="#b56">[57]</ref>, for example, distinguishes epistemic actions, which help people think and learn from the world, from pragmatic actions, which change the state of the world. Cadoz <ref type="bibr" target="#b15">[16]</ref> draws a similar distinction between epistemic gestures and ergotic gestures, to which he adds the category of semiotic gesturesgestures used to communicate with other humans. Verplank <ref type="bibr" target="#b65">[66]</ref> describes interaction with the physical world as three questions: How do you feel (perceive) the world? How do you know (cognitively understand) the world? How do you do (perform action) in the world? In our work we focus on epistemic actions and ways of knowing the world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Enhancing Epistemic Abilities in Visualization</head><p>The quest to enhance or augment human capabilities is central to the visualization community. Munzner <ref type="bibr" target="#b68">[69]</ref>, for example, states that "Vis systems are appropriate for use when your goal is to augment human capabilities [...]". <ref type="bibr">Ware [98]</ref> even argues that "people are not very intelligent without external cognitive tools". Visualization research focuses primarily on enhancing humans' epistemic abilities, specifically their ability to acquire knowledge through perception, cognition, and action. Card et al.'s list of enhanced human capabilities <ref type="bibr" target="#b18">[19]</ref> highlights memory and pattern recognition. Ware <ref type="bibr" target="#b98">[98]</ref> lists pattern recognition, perception of emergent properties, problem detection, multi-scale feature detection, and hypothesis generation as abilities that can be augmented by visualization. Spence <ref type="bibr" target="#b87">[87]</ref> also discusses examples that demonstrate the huge time savings that visualization tools can bring to human data processing. Meanwhile, essentially all visualization (text)books emphasize visualizations' ability to trigger insights.</p><p>Discussions of the value of visualizations range from broad formulations that encompass both perception and cognition (such as Card <ref type="bibr" target="#b18">[19]</ref> and Ware <ref type="bibr" target="#b98">[98]</ref>) to narrower discussions of results from specific perceptual experiments. Individual studies often work from the bottom up, and measure the efficiency of specific data encodings on the perception of certain patterns (such as the length of lines) or for the enhancement of cognitive abilities (such as memory). Currently missing from the research discussions on the value of visualizations is a concrete characterization that summarizes benefits. In our work, we take a small step towards such a characterization to help researchers explore, discuss, and expand on the ways visualizations can impact people's ability to acquire knowledge and experience from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fiction as a Critical Lens for Technology</head><p>To begin characterizing human abilities that are (or could be) augmented by visualization technologies, we look to fiction for inspiration. We use fiction as a means to better understand and convey the value of visualization, mirroring the growing use of design fiction and futuring approaches in related fields. User-experience designers and hardware developers regularly draw both implicitly and explicitly from science fiction <ref type="bibr" target="#b84">[84]</ref> or use design futuring techniques to consider the social impact of new technologies <ref type="bibr" target="#b103">[103]</ref>. Design fictions also offer an accessible entry point for complex topics, such as ethics in data science <ref type="bibr" target="#b67">[68]</ref>, sometimes in the guise of a mundane object like an IKEA catalog <ref type="bibr" target="#b14">[15]</ref>. Multiple workshops have used fiction to inspire creative thinking in their participants <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">60]</ref>. HCI researchers have also begun to more explicitly engage with fiction-specifically science fiction-as inspiration for new research areas like shape-changing displays <ref type="bibr" target="#b93">[93]</ref>.</p><p>Engaging with fictional futures, technologies, or human abilities, can provoke designers and researchers and inspire them to reconsider the impacts of their work. Our aim is to help visualization designers reconsider how their contributions-visualization systems and techniquesmight transform everyday people into heroes with enhanced epistemic abilities. We next discuss specific examples of interactive systems from research that are either inspired by fictional superpowers or are experienced as superpowers by the people using them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Superpower-Inspired Interactive Systems</head><p>Many examples of sci-fi inspired interactive systems specifically seek to emulate the experience of superpowers with technology-especially in virtual reality (VR) and augmented reality (AR) research.</p><p>In some interactive systems, superpower metaphors are either explicit in the user interface or directly inspire its design. Ishibashi et al. <ref type="bibr" target="#b49">[50]</ref>, for example, proposed a VR system combined with a pulling force-feedback device that allowed players to "become" Spider-Man within their game, and haptically experience swinging through the air on elastic "webs." Rietzler et al. <ref type="bibr" target="#b81">[81]</ref> proposed an airflow feedback system for VR and note that "airflow simulation can go far beyond the simulation of wind, reaching from realistic effects to unrealistic superpowers for gaming." Closer to our interest in epistemic-enhancing technology, Voss et al. <ref type="bibr" target="#b96">[96]</ref> developed the superpower glass system based on Google Glass, which detects and labels others' emotions to help people on the autism spectrum develop emotional awareness. In an essay, Perlin <ref type="bibr" target="#b74">[74]</ref> likens AR and wearable technology including hearing devices or Bluetooth hands-free cellphones to superpowers. Some user studies have also been inspired by superpowers. For example, Imura et al. <ref type="bibr" target="#b69">[70]</ref> explored how "hyper-natural" components of interaction techniques in VR may affect locomotion performance, while Rosenberg et al. <ref type="bibr" target="#b82">[82]</ref> explored how the experience of superpowers like flight in VR may affect pro-social behavior.</p><p>Already, people using interactive systems often spontaneously liken them to superhuman abilities. For example, when Kajastila et al. <ref type="bibr" target="#b54">[55]</ref> evaluated a game that taught motor skills using exaggerated visual feedback on a screen next to a trampoline, participants reported that the exaggerated feedback made them feel like they had superpowers. Bozgeyikli et al. <ref type="bibr" target="#b13">[14]</ref>, who compared several VR locomotion techniques, noted participants' "very positive comments about the Point &amp; Teleport technique resembling a superpower, being fun and being like a magical experience." Similarly, in Raj and Ha-Brookshire's <ref type="bibr" target="#b78">[78]</ref> interviews of professionals in the IT industry, many participants discussed wearable technologies as granting superpowers.</p><p>The idea that new user interface technologies can give people enhanced abilities is far from new. However, many of the previous systems and most prior discussions on the relationship between technologies and superpowers focus on pragmatic empowerment. Due to our focus on visualization, we address the challenge of facilitating knowledge-based, or epistemic empowerment through data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We first describe the process by which we arrived at our conceptual frameworks, then clarify and motivate their scope. A timeline and appendix documenting our process are included in supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Process</head><p>We developed and refined our conceptualization of the relationship between visualization and enhanced abilities in fiction over the course of three years of discussions-including several in-person workshops and regular virtual meetings between members of the author team.</p><p>The initial concept emerged from a 2018 workshop on situated and embedded visualization. Inspired by the idea of "making the invisible visible" we discussed various reasons why data might be 'invisible', then generated a list of superheros whose abilities might render it visible. This triggered an ongoing series of post-workshop conversations about enhanced abilities and visualization, primarily grounded in well-known cinematic depictions of superheros, like Superman's x-ray vision, Spider-Man's spider-sense, and Iron Man's support from the artificial intelligence J.A.R.V.I.S. We expanded the list of abilities by immersing ourselves in online pop-culture encyclopedias, including the Fandom Superpower Wiki 1 (a community-curated database of superpowers that as of early 2021 included over 16,000 entries) as well as community-generated superpower taxonomies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b75">75]</ref>. We also revisited an extensive cross-section of original source material from movies and comics, focusing on specific characters whose abilities emphasize perception and knowledge generation. From this process, we distilled a list of 26 distinct perceptual and cognitive superhuman abilities from fiction, many of which are exhibited by a variety of characters. As we gathered each example, we looked at (a) which aspects of human abilities it amplifies, (b) in what contexts and situations characters use the ability, (c) how characters control the ability, and (d) how the ability is visually portrayed in print and other media.</p><p>After spending over a year exploring fictional superpowers and discussing 'empowerment' in visualization and HCI, we iteratively refined our list of epistemic superpowers, separating more complex superpowers into multiple distinct sub-powers. We also began to consider the technical and conceptual implications of each superpower in the context of visualization. This effort was led by one co-author, who presented drafts of the framework for "enhanced epistemic abilities in fiction" (described in Sect. 4) to fellow co-authors during weekly videoconferences, incorporating critique until the group reached consensus. The final framework highlights seven fundamental mechanisms that underpin the majority of the abilities found in our sample.</p><p>Based on this list of mechanisms, we then collected examples of real-world analogues to these abilities, including tools that operate on real environments (such as radiography) and in virtual environments (including both immersive and non-immersive visualization approaches). We performed this search using all seven mechanisms for enhanced abilities from Sect. 4 as well as five specific superpowers from our initial sample (see-through vision, magnifying vision, night vision, shared vision, and mind reading). Two co-authors led this process, presenting drafts and visual explorations in our weekly videoconferences to solicit additional examples, gather critique, and ultimately reach consensus.</p><p>Throughout our conversations, we increasingly reflected on the experience of using sensing and visualization tools and on how people's subjective and objective sense of empowerment from using these tools aligns with notions of superhuman empowerment. These discussions, grounded in our examples from the prior steps, ultimately motivated our seven dimensions of empowerment (Sect. 5), as well as our exploration of new possible abilities (Sect. 6), and higher-level reflections (Sect. 7). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scope and Terminology</head><p>Based on our analysis of superpowers in fiction as well as prior work from HCI and cognitive science (see Sect. 2), we identified two broad classes of superpowers: pragmatic superpowers that enable characters to actively manipulate things, people, or phenomena in the world, and epistemic superpowers that allow characters to gain knowledge about the world without necessarily changing it (Fig. <ref type="figure">2</ref>). In fiction, the vast majority of superpowers are pragmatic-including enhanced physical attributes (super-strength, super-agility, etc.) or manipulation abilities (telekinesis, matter manipulation, etc.). In contrast, epistemic superpowers allow characters to gain knowledge of the world without necessarily altering it. These powers can take a variety of forms including literal extensions of traditional human vision (x-ray vision) and human cognition (enhanced memory), as well as extensions of other senses (enhanced smell). They also include more esoteric and indirect abilities (such as precognition of future events, perception of parallel dimensions, or the power to instantaneously compute probabilities) which allow characters to access and reason about information in ways not available to baseline humans. In this paper, we draw inspiration from the portrayal of epistemic empowerment in fiction to characterize the ways in which data and data visualizations can empower individuals. We are interested in how enhanced abilities for epistemic empowerment can offer people unique, site-specific, and context-specific assistance in the world.</p><p>When considering existing systems, we use the broad term epistemic tool to refer to any technological system that augments humans' ability to learn about the world. Examples include scientific instruments, optical devices, static information displays (such as maps, diagrams, or books), and computer-supported information systems, of which data visualization systems and visualization dashboards constitute a subset. Epistemic tools can extend human abilities to different degrees-ranging from trivial extensions to close-to-superhuman ones. In addition, we use the term object(s) to refer to the entities about which an individual learns when using an epistemic tool. In a data visualization context, objects are usually the physical referents (such as the physical spaces, objects, or entities) to which the data refers <ref type="bibr" target="#b102">[102]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ENHANCED EPISTEMIC ABILITIES IN FICTION</head><p>Our first framework (see Fig. <ref type="figure" target="#fig_0">3</ref>) attempts to capture low-level mechanisms that underpin a wide range of epistemic superpowers in fiction. We draw the connection to visualization by breaking down complex epistemic superpowers into a set of more atomic and composable abilities, often with analogues in perception and psychology research. These mechanisms include abilities that enhance vision (increasing humans' ability to use their visual system to observe phenomena in the surrounding world) as well as examples that enhance cognition (amplifying humans' capacity to process and reason about observations). We highlight seven specific classes of enhancements including enhanced vision V and visual synesthesia Y , as well as enhanced attention A , numeracy N , recall R , comparison C , and prediction P . These classes are not intended to be an exhaustive list but instead to provide a structure for examining enhanced epistemic abilities in fiction that suggest concrete opportunities for new visualization systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Enhancing Human Vision</head><p>Humans already rely on a variety of complex and interrelated mechanisms in our visual system (including color vision, depth perception, and stereoscopic vision) to collect information about our environments. The coordination and acuity of these underlying mechanisms differ considerably across individuals. Characteristics like color perception <ref type="bibr" target="#b50">[51]</ref>, visual search speed <ref type="bibr" target="#b95">[95]</ref>, and the size of the useful field of view <ref type="bibr" target="#b34">[35]</ref> also vary with age or the presence of various genetic conditions. We use the blanket term standard human vision SV to refer to the range of typical human perceptual experiences. perceive very small objects (the microscopic vision of Marvel's Hyperion), or gain a wider field of view (the 360-degree vision of Mad Eye Moody's magical eye from the Harry Potter series)-while still adhering to traditional human models of sight. Other characters can perceive electromagnetic wavelengths outside the standard human range. This can manifest in a variety of ways, including abilities like night vision (possessed by a wide range of characters including Marvel's Wolverine and Rebellion's Judge Dredd), which lets characters see in conditions with low or even no light. Other forms of enhanced vision include the ability to see through occlusions and objects, often selectively (Fig. <ref type="figure">4</ref>). Some characters also possess the ability to see the world from viewpoints other than their own. Common ways of depicting enhanced vision include call-outs and close-ups, color-modified scenes, or establishing shots that provide larger fields of view. Meanwhile, back in the real world, humans have long experimented with telescopes and related optical tools and, since the first x-ray image in the late 1800s, have developed a range of technologies for seeing through matter <ref type="bibr" target="#b66">[67]</ref> using millimeter waves, terahertz waves, positrons, or ultrasonic waves <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b86">86,</ref><ref type="bibr" target="#b92">92]</ref>. Some systems also allow people to effectively see through matter without directly sensing what is behind it. These approaches use object instrumentation or computer modeling plus AR to display occluded objects directly in the person's physical surroundings or on relevant objects <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b76">76]</ref> (see Fig. <ref type="figure">5</ref>). Superpowers relying on visual synesthesia Y , by contrast, allow characters to "see" non-visual properties of their surrounding environment. Typically, these abilities visually transform invisible phenomena like sound, affect, or chemical composition, allowing the wielder to perceive them in-place in the surrounding environment. Examples of visual synesthesia powers include the emotion vision used by DC's Black Lantern Corps (Fig. <ref type="figure">4</ref>), the chemical vision of Marvel's Eye-Boy, or Wolf Link's scent sight in the Legend of Zelda. Because of the highly visual nature of superhero comics and related media, artists often illustrate variations of visual synesthesia (such as the "sonar vision" used by the blind Marvel character Daredevil) even in cases where the text or story specifically indicates some other perceptual mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhanced vision</head><p>Throughout human history, people have devised a range of epistemic tools to grant this ability-from thermoscopes and seismoscopes, which convert temperature and ground motion into visual form, to Chladni plates <ref type="bibr" target="#b32">[33]</ref>, which make the modes of vibration of rigid surfaces visible. More modern systems with sensory substitution <ref type="bibr" target="#b4">[5]</ref> translate stimuli in the environment from one sensory modality to another-in fact vir-tually all information visualization systems translate data into a visual form <ref type="bibr" target="#b68">[69]</ref>. Some systems (Fig. <ref type="figure">6</ref>  <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b100">100]</ref>) even overlay visualizations next to objects in physical environments <ref type="bibr" target="#b102">[102]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Enhancing Human Cognition</head><p>In addition to augmented vision, numerous fictional characters possess enhanced cognitive abilities that extend their visual perception. These abilities allow them to process and reason about the world in ways that transcend standard human cognition SC .</p><p>Abilities that provide enhanced attention A allow characters to rapidly attend to important information or visual details in their environment that typical humans would otherwise miss. These powers are common for detective characters like Sherlock Holmes who seems unaffected by phenomena such as inattentional blindness, or Marvel's Spider-Man, whose spider-sense allows him to quickly direct attention to immediate threats. In media, attention-related abilities are often illustrated via visual highlighting (including outlines, glow, or explicit call-outs) and by using zooms and other re-framings of the scene to visually draw viewers' attention to specific details.</p><p>Humans already have the ability to quickly count small numbers of items <ref type="bibr" target="#b22">[23]</ref> and make quick (though relatively imprecise) judgments about large numbers of items <ref type="bibr" target="#b91">[91]</ref>. However, numerous fictional characters display enhanced numeracy N skills, which allow them to make rapid, confident, and accurate assessments about extremely large numbers of items. Often, these abilities extend to related types of quantitative perception, allowing characters to quickly and accurately judge distances, angles, weights, and other common physical measures. Daredevil, for example, can quickly count bullets even while distracted. Jason Bourne can estimate the exact weight of other people. Technologically-augmented, android, or (half) alien characters such   Real-world epistemic tools that enhance numeracy, meanwhile, include counting devices (tally marks, mechanical counters, computervision-based counters <ref type="bibr" target="#b36">[37]</ref>), calculating devices (abacuses, slide rules, computer spreadsheets), and technical measuring instruments (rulers, weighing scales, mass spectrometers). Visualization prototypes like Re-alitySketch (Fig. <ref type="figure">7</ref>-left), also enhance numeracy by overlaying angular and distance measurements on real-world objects <ref type="bibr" target="#b90">[90]</ref>.</p><p>In other cases, characters may possess enhanced recall R , which allows them to quickly and accurately recollect past observations from memory. These abilities help characters rapidly identify phenomena or connect new observations and information to prior experiences. Photographic (or eidetic) memory is particularly common in superhero fiction, shared by omnipotent characters like DC's Superman, technologicallyaugmented ones like RoboCop, and "peak human" characters like Ozymandias from Watchmen, Terry Sloane (DC's Golden Age Mister Terrific), or Jason Bourne. Characters' ability to access their memory or other related information is often illustrated using flashbacks and other visual devices that reveal past information to readers or viewers. In some cases, characters like Carrie Wells from the CBS crime drama Unforgettable are even shown examining and exploring their memory visually, walking into and examining past scenes. However, like numeracy, recall abilities are often simply described in with characters referring to past information without illustration.</p><p>Similarly, characters can possess enhanced comparison C skills, which allow them to quickly and accurately identify differences or similarities between phenomena. Comparison abilities are often related to and build upon characters' other skills like enhanced attention, numeracy, or recall. However, the ability to detect differences need not necessarily depend on quantitative judgments and may rely entirely on characters' other finely-tuned perception skills. For example the Marvel Universe Handbook <ref type="bibr" target="#b21">[22]</ref> discusses how Daredevil "can distinguish between identical twins at twenty feet by minute differences in smell."</p><p>A variety of characters also exhibit the ability to reason about future events. In some cases these enhanced prediction P abilities are described as true precognition, in which characters see glimpses of a single inevitable future. However, more often these abilities are shown as glimpses of possible futures-sometimes framed in probabilistic terms. In other cases (as with classic DC villain the Clock King or Marvel's Mad Thinker) these abilities are portrayed as purely logical or probabilistic, with characters making precise extrapolations based entirely on existing observations. Predictive powers are visualized in a variety of different ways across media, often by showing multiple possible outcomes either sequentially or in parallel or by accentuating the statistical presentation of outcomes (as in Fig. <ref type="figure">4</ref>).</p><p>Real epistemic tools that support enhanced prediction include mechanical modeling and prediction systems (such as orreries, analog barometers, and analog ballistic computers <ref type="bibr" target="#b8">[9]</ref>) as well as the multitude of predictive computer models in use today, from weather forecast models to route and trip planners. Recent AR prototypes like Itoh et al.'s Laplacian Vision (Fig. <ref type="figure">7</ref>-right) have also demonstrated live predictions of object trajectories in real-world environments <ref type="bibr" target="#b51">[52]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Composing and Combining Mechanisms</head><p>In practice, many empowered characters in fiction exhibit abilities that combine several of these underlying mechanisms. For instance, the detective Sherlock Holmes routinely displays attention, recall, and comparison skills that far surpass those of other individuals. In visual media, these abilities are often illustrated and integrated using variations on a visual trope (sometimes termed a "Sherlock Scan" 2 ) that uses sequential zooms to highlight important or unseen details A , flashbacks to provide context R , and juxtapositions to reveal subtle differences C that together facilitate the character's feats of deduction.</p><p>Similarly, Marvel superhero Amadeus Cho (in his various incarnations, including as the Totally Awesome Hulk) combines enhanced numerical precision, prediction, and comparison skills to make precise quantitative decisions (Fig. <ref type="figure">4</ref>). In one common application, the character rapidly quantifies angles and trajectories in the space N , then predicts P and compares C outcomes for each. Cho's ability is particularly notable from a visualization perspective, as artists have often chosen to illustrate it from the character's point of view-including numbers, trajectories, and other details that integrate into the surrounding scenes. As with Holmes, these abilities are usually explained as cognitive, with the characters still relying on typical human vision SV .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DIMENSIONS OF EMPOWERMENT IN EPISTEMIC TOOLS</head><p>As highlighted in the previous section, technological analogues to many superpowers already exist-allowing real people to see through walls, make precise numerical judgments, or predict future events. However, depending on how they are implemented, some existing systems create a much stronger sense of empowerment than others. Here, we examine real-world technologies (most from research areas other than visualization) that can provide some of the enhanced abilities that appear in fiction. We combine observations from these existing systems and their fictional counterparts, examining the varying degrees to which the people who use them feel empowered and more able to achieve their immediate goals. We highlight seven dimensions of empowerment: scope, access, spatial relevance, temporal relevance, information richness, degree of control, and environmental reality, exploring the manner by which each can alter people's sense of empowerment or agency. These dimensions go beyond the simple choice of visual mappings and spotlight visualization design considerations that are underexamined in current visualization research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scope works on a narrow range of objects/settings</head><p>works on all objects/settings works on a narrow range of objects/settings w o r ks on all objects/settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TYPICALLY LESS EMPOWERING TYPICALLY MORE EMPOWERING</head><p>The scope of an epistemic tool defines the set of objects or settings on which the tool can operate. Often, the broader the scope of an epistemic tool, the more empowering it is. For example, tourist binoculars mounted in a fixed location let people see visual detail (or, through AR, additional information <ref type="bibr" target="#b38">[39]</ref>) from only one vantage point. In contrast, portable binoculars can be used in many settings, leading to a broader range of uses and greater agency. In the context of visualization, scope is often limited by the technical challenges of capturing and displaying data. For example, current electromagnetic sensing technologies that enable see-through vision (like x-ray and fMRI imaging) can typically only penetrate and detect specific types of materials and often involve complex immovable hardware. However, some exceptions offer greater portability, such as thermal imaging goggles, which can see through obstructions like smoke or fog <ref type="bibr" target="#b37">[38]</ref>. One way to broaden scope is to instrument environments with markers, sensors, transceivers, and other equipment that explicitly communicates information to a receiver. For example, Raskar et al.'s RFIG Lamps <ref type="bibr" target="#b79">[79]</ref> allow people to see information about the content of warehouse boxes by having each box communicate its contents using active RFID tags. Another approach is to render a pre-existing computer model of the occluded objects. In one of the earliest AR concepts, a military aircraft pilot could see a 3D model of the landscape around them through the airplane hull and through clouds <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41]</ref>. Similarly, many mobile sky map apps include AR features that allow viewers to see the night sky during the day, through clouds, and even through the Earth itself (when pointing the phone down) <ref type="bibr" target="#b57">[58]</ref>. Such tools do not require objects and settings to be instrumented but do require stable up-to-date models of them. As a result, the scope of current see-through vision systems based on object instrumentation and modeling is not larger (and is in fact often narrower) than sensing-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Access</head><p>usable only by individuals with specific hardware/access usable by everyone usable only by individuals with specific hardware/access usable by everyone</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TYPICALLY LESS EMPOWERING TYPICALLY MORE EMPOWERING</head><p>The access of an epistemic tool refers to who can use the tool and how easily. There are several ways that access can be facilitated or limited. Generally, tools that only exist at some specific locations (like a large particle accelerator in the Swiss Alps) have lower access than tools that can be easily copied, distributed, or used remotely. Access also relates to whether an epistemic tool can only benefit a single person or can be concurrently used by multiple people, and to how complex it is to equip or set up. For example, visual information provided by a head-mounted AR display is only accessible to the person wearing it, whereas anyone in the vicinity of a large screen or physical installation can access the displayed information. The relationship between access and empowerment is complicated but in general, the broader the access to an epistemic tool, the more empowering it is.</p><p>In the context of visualization, the increasing ubiquity of web-based systems and powerful mobile devices already highlights the potential for broad access. However, the creation of new and more empowering tools is still limited by the complexity and cost of sensing technologies, access to relevant data, and broader challenges related to visualization literacy <ref type="bibr" target="#b12">[13]</ref>. Immersive and extended-reality technologies, which rely on hardware like head-mounted displays, also introduce new costs that can limit adoption, and are likely to create new asymmetries and coordination issues between people with access and those without <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Spatial Relevance no relationship between info and environment info shown in relevant locations no relationship between info and environment info shown in relevant locations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TYPICALLY LESS EMPOWERING TYPICALLY MORE EMPOWERING</head><p>The spatial relevance of an epistemic tool reflects the distance (real or perceived) between the location where information is most useful to the person, and the location where that information is actually displayed. Since information about an object is often best interpreted in the context of the object itself, the most useful display location is frequently near or on top of the objects <ref type="bibr" target="#b102">[102]</ref>. For example, a paper star map has lower spatial relevance than an AR app that overlays information directly onto the celestial dome. However, in some cases showing information next to the physical referent does not increase spatial relevance. For example, when someone seeks information about a distant referent (like a hotel to book), it is often preferable to show that information in front of the person (for example on a computer) rather than on the referent itself, where it might be invisible. In general, tools with greater spatial relevance are more empowering, allowing individuals to perceive and interpret information where it is more actionable. Most contemporary sensor-based see-through displays, including tools like thermal cameras, already tend to have high spatial relevance. However, more complex scanning apparatuses like ultrasound imaging equipment typically still show information on a display separate from the object under inspection. AR research, meanwhile, has explored ways to overlay hidden objects directly on top of occluding ones <ref type="bibr" target="#b62">[63]</ref>,</p><p>for example during laparoscopic surgery <ref type="bibr" target="#b39">[40]</ref> or pregnancy ultrasound testing <ref type="bibr" target="#b6">[7]</ref>. These research prototypes illustrate how high spatial relevance can facilitate decision making and action, and enhance the subjective impression of possessing super-powered see-through vision.</p><p>Similarly, most traditional measuring instruments are short-range and thus have moderately high spatial relevance, even when their displays are separate from their sensors. For example, most blood pressure monitors do not display blood pressure on top of someone's body, and a voltmeter does not display voltage readings on the electric circuit itself. However, by overlaying numerical displays on top of the physical environment, AR systems like RealitySketch <ref type="bibr" target="#b51">[52]</ref> can increase spatial relevance dramatically. Using these kinds of AR approaches to overlay visualizations in real environments represents a clear opportunity for increasing spatial relevance for many other kinds of visualization applications, empowering people to translate observations into more immediate and relevant actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Temporal Relevance</head><p>no relationship between info and when it is shown info shown at relevant times no relationship between info and when it is shown info shown at relevant times</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TYPICALLY LESS EMPOWERING TYPICALLY MORE EMPOWERING</head><p>The temporal relevance of an epistemic tool reflects the temporal offset between the moment the information is delivered to the person, and the moment it would be the most useful for them to have the information.</p><p>When it is important to react to current events, temporal relevance can often be achieved by showing real-time information <ref type="bibr" target="#b102">[102]</ref>. For example, a live stock market display has a higher temporal relevance than the same information published in a daily newspaper. However, whenever information about the (possibly distant) past or future becomes useful to the person, high temporal relevance can be achieved by delivering this information at the present moment, even if the data is not directly connected to it. Often, higher temporal relevance increases empowerment, allowing individuals to more easily evaluate data and use that information to make immediate decisions. The range of temporal relevance in current systems varies widely. For example, fast sensing-based see-through vision systems like airport luggage scanners have very high temporal relevance, showing imagery that is effectively live. In contrast, it currently takes up to an hour to produce fMRI images. Measuring instruments like thermometers can have low or high temporal relevance depending on how long it takes to calculate and display the numerical results. Most information visualization systems, meanwhile, tend to focus on visualizing data collected in the past. However, business dashboards and other decisionsupport tools are increasingly focused on visualizing real-time data <ref type="bibr" target="#b25">[26]</ref>.</p><p>Temporal relevance is particularly interesting for visualizations designed to enhance prediction, with advances in predictive modeling enabling predictions with increasingly finer temporal and spatial granularities. For example, while traditional television or newspaper weather forecasts focus on daily and regional predictions, new approaches can predict extremely local precipitation amounts on a minute-by-minute basis. Likewise, mobile traffic apps now provide near-real-time traffic predictions and visualizations, allowing drivers to dynamically adjust their routes based on predicted congestion and travel times. Prototype systems like Itoh et al.'s Laplacian Vision <ref type="bibr" target="#b51">[52]</ref> or Alves et al.'s Pool-LiveAid <ref type="bibr" target="#b1">[2]</ref> go further, using AR and projection to visualize predicted object trajectories with particularly high spatial and temporal relevance. These approaches highlight the potential for many other visualization applications (including visualizing real-time predictions of auto and pedestrian behavior from autonomous vehicle models <ref type="bibr" target="#b101">[101]</ref>) that could supercharge people's decision-making abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Information Richness limited information</head><p>rich, detailed information limited information rich, detailed information</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TYPICALLY LESS EMPOWERING TYPICALLY MORE EMPOWERING</head><p>The information richness of an epistemic tool describes the quantity, variety, and accuracy of the information it is able to convey. In general, the higher the information richness of an epistemic tool, the more empowering it is.</p><p>Richness depends upon a variety of factors, including the quality and accuracy of the data as well as the fidelity of the output medium. For example, current real-world see-through vision approaches tend to be quite low-resolution and vary considerably based on the geometry and composition of the occluding objects. Tools for measuring and visualizing abstract and non-visible data, by contrast, already have the ability to convey much richer information. However, each instrument typically captures only one type of phenomenon: thermoscopes show temperature, seismoscopes show ground motion, etc. Although computer technology now enables data-agnostic methods for storing, transmitting, and visualizing data, data collection and sensing still require dedicated tools. Accuracy is also an important consideration, as a precise prediction can be thought of as carrying richer information than a vague prediction. While some epistemic tools make perfectly accurate predictions (such as predicting solar eclipses), others are very approximate (like long-term weather predictions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Degree of Control</head><p>no control direct, interactive manipulation no control direct, interactive manipulation</p><formula xml:id="formula_0">TYPICALLY LESS EMPOWERING TYPICALLY MORE EMPOWERING</formula><p>The degree of control of an epistemic tool refers to how much freedom the person has in activating and controlling the enhanced abilities that it facilitates. An always-on tool that cannot be tuned has very low control, while a configurable tool with many degrees of freedom that can be tailored to the situation offers more control. Degree of control also relates to agency (whether the person is controlling the tool <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b52">53]</ref>) and directness (whether there is a lag or gap between the person's control actions and the outcomes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b52">53]</ref>). In general, the higher the degree of control of an epistemic tool, the more empowering it is.</p><p>In practice, many real-world sensors and visualization tools provide extremely limited control and expressivity. For example, medical or security-oriented scanners and scientific measurement tools like oscilloscopes typically have many settings, but these are rarely expressive or controlled via direct manipulation. Meanwhile, physical measurement implements like measuring tapes and protractors support extremely direct interaction, but have limited functionality. In contrast, prototype see-through vision systems like RFIG Lamps <ref type="bibr" target="#b79">[79]</ref> can be directly aimed at objects. Other tools let people freely cut through virtual anatomical models by manipulating virtual <ref type="bibr" target="#b63">[64]</ref> or physical <ref type="bibr" target="#b48">[49]</ref> cutting planes. Similarly, prototype measurement and prediction tools (Fig. <ref type="figure">7</ref>-left) like RealitySketch <ref type="bibr" target="#b90">[90]</ref> and Laplacian Vision <ref type="bibr" target="#b51">[52]</ref> have begun to explore opportunities to pair direct manipulation with automation in ways that empower people with a rich, direct, and powerful sense of control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Environment Reality</head><p>entirely artificial or virtual context works in the real world entirely artificial or virtual context works in the real world</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TYPICALLY LESS EMPOWERING TYPICALLY MORE EMPOWERING</head><p>The environment reality of an epistemic tool reflects the extent to which its context is real rather than artificial. Tools that operate within the person's physical surroundings (as in Figs. <ref type="figure">5-7</ref>) are higher along this dimension than tools that only work in specific virtual environments. Another important aspect of environment reality is whether the target objects are real or fictional. For example, a VR application that lets people explore real locations (say, a virtual visit to Paris) is higher on the reality spectrum than an application for exploring fictional worlds (like a virtual visit to Atlantis), irrespective of how realistic they are. Generally, tools that operate in real environments are more empowering, since decisions and actions there tend to be more consequential and lasting than those in virtual spaces. However, creating complex and interactive epistemic tools is often much easier in virtual environments than in real ones. For example, seethrough vision is easy to enable in virtual worlds, since the computer has a model of all occluded content. Many 3D video games allow players to see enemies through walls <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref> and-if the game is realistic and the immersion high-it is possible to evoke a subjective experience close to that of actually possessing the superpower. Related occlusion management techniques exist in many visualization systems, especially for 3D visualization <ref type="bibr" target="#b35">[36]</ref>. These techniques are extremely powerful and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MY BUILDING VISION LETS ME SEE THE OCCUPANCY AND TEMPERATURE INSIDE IN REAL TIME! Fig 8.</head><p>V allow people using computers to be much more effective at their tasks. However, they are rarely experienced as granting a superpower, in part because of their low environment reality.</p><p>Broadly, environment reality remains low for the vast majority of 2D and fully immersive <ref type="bibr" target="#b64">[65]</ref> data visualization tools. However, data physicalization <ref type="bibr" target="#b53">[54]</ref> approaches, particularly interactive and dynamic ones using technologies like drones <ref type="bibr" target="#b42">[43]</ref> and robot swarms <ref type="bibr" target="#b58">[59]</ref>, highlight the potential for future visualization systems to empower people via increasingly real-world interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">NEW DIRECTIONS FOR "EMPOWERING" VISUALIZATIONS</head><p>Inspired by the understanding of epistemic superpowers and empowerment surfaced in our two frameworks, we propose a set of evocative new visualization designs and applications. While our designs are purposefully platform-agnostic and eschew details about their implementation, they highlight potential applications of the frameworks and underscore the breadth of opportunities for new, unconventional, and empowering visualizations that use situated and mixed-reality approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b85">85]</ref> to integrate into new settings and use cases.</p><p>Enhanced Vision. V New visualization systems could build upon the metaphor of enhanced vision both literally and figuratively. For example, superimposing imagery from infrared cameras onto a viewer's field of view could enable thermal vision, making it possible to use heat signatures to detect thermal leaks, electrical faults, or fevers. In comparison to current on-screen thermal imaging approaches, techniques that directly augment the viewer's existing vision could widen the tools' scope while also increasing spatial and temporal relevance. By integrating thermal imagery into viewers' existing natural light vision, such approaches could also increase information richness, supporting new applications.</p><p>Other techniques could more dramatically alter a viewer's perspective, creating panoptic vision that allows them to see from new viewpoints. For example, a system might use cameras in an environment or on a viewer's body to build a 3D model of the surrounding spaces, then visualize it as a third-person world-in-miniature <ref type="bibr" target="#b24">[25]</ref> showing the entire space, including their own body as well as areas outside of their view. Visualization designs can also borrow metaphors such as see-through vision for integrating sensed or simulated spatial data into real environments where occlusion is a problem-creating abilities like the building vision seen in Fig. <ref type="figure">8</ref>.</p><p>Visual Synesthesia. Y Synesthetic visualizations that integrate data into viewers' real-world experiences can change how individuals approach tasks or appreciate the world around them. For example, using computer vision approaches to classify the emotional states of people in the immediate environment <ref type="bibr" target="#b83">[83]</ref>, then revealing them to viewers in-context could enable new kinds of emotion vision (Fig. <ref type="figure">9</ref>). Like the  emoji cloud visualization used in MeetCues <ref type="bibr" target="#b2">[3]</ref>, these tools could help a presenter track the changing emotional state of audiences during a meeting, and adapt presentations based on the audience's response, particularly benefiting individuals with neurodevelopmental conditions <ref type="bibr" target="#b27">[28]</ref>. Such tools are likely most empowering when they operate in real-time, during an actual meeting, thereby offering high temporal relevance and high environment reality. Such approaches may also lend themselves to non-visual phenomena like sound, using audio processing tools to create dynamic music vision that visualizes musical tones, harmonies, rhythm, etc. around an audio source or a set of performers.</p><formula xml:id="formula_1">MY</formula><p>Enhanced Attention. A Future visualization tools could also direct viewers' focus towards important objects or phenomena, helping filter out noise in both their data and environment. For example, an acceleration vision system integrated into a car's windscreen (Fig. <ref type="figure">10</ref>) could visualize changes in acceleration of other vehicles, objects, or animals on a road, adding emphasis to draw attention to erratic or unexpected behavior. Similarly, expression vision might analyze facial micro-expressions for hundreds of faces at once, allowing an observer in a public space to evaluate public sentiment and highlighting faces meeting some predefined criteria, such as extreme joy or fear. Such attention-oriented approaches could extend highlighting techniques in existing visualization systems <ref type="bibr" target="#b80">[80]</ref> or video games <ref type="bibr" target="#b29">[30]</ref>. Alternatively, tools could also act implicitly to address viewers' attention biases by identifying when viewers disproportionately fixate on a subset of objects or values (one brand but not others, short/tall people but not average ones, etc.) then counteracting with debiased vision that highlights counterexamples or de-emphasizes overly-fixated ones. Such strategies could build on the growing appreciation of cognitive biases in visualization <ref type="bibr" target="#b30">[31]</ref> as well as object subtraction <ref type="bibr" target="#b47">[48]</ref>, re-scaling <ref type="bibr" target="#b70">[71]</ref>, and distortion approaches for mixed reality use cases. Existing tools already empower people by enhancing their attention, but there are many opportunities to increase their subjective empowerment by seeking tools with higher levels on any of the dimensions of our empowerment framework.</p><p>Enhanced Numeracy. N New tools for enhancing numeracy, meanwhile, could leverage computer vision to enhance people's ability to count, quantify, and analyze objects in their environment. Counting vision approaches could help individuals make precise and rapid judgements about large or abstract quantities like crowd sizes, numbers of specimens in a collection, or the volume of air in a building. These tools could also support common but high-consequence counting tasks like counting the members of a school class (Fig. <ref type="figure" target="#fig_6">11</ref>). Meanwhile, more advanced estimation vision tools might help viewers more accurately calibrate estimations and judgements-providing in-context visual references that help compensate for judgements that people are characteristically bad at, including estimating volumes, comparing or-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counting Vision</head><p>lets me count all the students quickly and keep track of each individual!  ders of magnitude, imagining exponential growth, or appreciating the uncertainty or variability in a set of measurements. In these scenarios, high temporal and spatial relevance is key to empowering people.</p><p>Enhanced Prediction. P New and empowering predictive visualizations could extend enhanced numeracy approaches, allowing viewers to extrapolate from counts, trajectories, or other information. For example, a queue vision tool might allow individuals to instantaneously count the number of people in each queue at a supermarket checkout, then provide real-time predictions about which line will move fastest. Live predictive visualizations could also dramatically transform player behavior in team sports, taking the kinds of analytic visualizations that are now widely used by managers, coaches, and players off-court <ref type="bibr" target="#b73">[73]</ref> and increasing the spatial and temporal relevance between these and the act of play. Visualization techniques like the shot vision imagined in Fig. <ref type="figure">1</ref> could display opponents' past shooting percentages, predictions of likely moves, and other information live-in an apotheosis of the increasing use of analytics in many leagues and sports.</p><p>Enhanced Comparison. C Like prediction, new enhanced comparison approaches could build upon numeracy-oriented abilities, helping viewers more efficiently compare counts and quantities in a variety of settings. These might include powers like measurement vision, which extend the basic AR measurement tools already present on many consumer smartphones and letting viewers overlay scales, measurements, or previous observations onto new settings to support visual comparison. More advanced comparison techniques might allow viewers to reorganize physical space to support comparison using "remixed reality" methods like those explored by Lindlbauer and Wilson <ref type="bibr" target="#b60">[61]</ref>. Visualization tools like these (Fig. <ref type="figure">12</ref>) could empower people by allowing them to align, filter, cluster, and rearrange virtual copies of real-world objects to make their numbers, sizes, volumes, and other characteristics easier to compare-emulating the kinds of physical arrangements created by biologist Charles Davenport <ref type="bibr" target="#b26">[27]</ref> or artists like Ursus Wehrli <ref type="bibr" target="#b99">[99]</ref>.</p><p>Enhanced Recall. R Finally, techniques that enhance people's ability for recall have considerable potential in traditional visualization systems as well as for situated and immersive ones. For example, the visual language of flashbacks and "Sherlock Scan" revisitation of past observations could inspire new approaches to persistent challenges posed by visualizing interaction histories <ref type="bibr" target="#b46">[47]</ref>, analytic provenance <ref type="bibr" target="#b77">[77]</ref>, and collaborative analysis coverage <ref type="bibr" target="#b5">[6]</ref>. In more immersive environments, new visualization tools could borrow even more literally from memory metaphors in fiction, creating systems that more explicitly emulate photographic memory. For example, immersive visualizations of extreme life-logging data-including the kinds of image, video, and metadata collections from systems like MyLifeBits <ref type="bibr" target="#b41">[42]</ref> and challenges like ImageCLEF LifeLog <ref type="bibr" target="#b71">[72]</ref>-could grant viewers new kinds of personal history vision, letting them visualize, summarize, and relive past experiences. AR information overlays also present opportunities for new visualizations that reveal complementary data to support in-place awareness and decision-making. For example, a germ vision system (Fig. <ref type="figure" target="#fig_0">13</ref>) might use historical occupancy data to help cleaning staff prioritize key areas, while a patient vision system could provide doctors with AR summaries of past patient visits during consultations. Shared spatial history visualizations could also be extremely valuable and empowering for applications like search-and-rescue, allowing multiple searchers to see past search areas as well as gaps between them.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>In addition to these frameworks and new visualization concepts, our deep discussions over the last three years also probed several additional issues at the intersection of empowerment and visualization. In this section we highlight our most important threads of discussion.</p><p>Superpower Depictions as Visual Design Inspiration. While we highlight the potential for fictional superpowers to serve as inspiration for new kinds of visualization systems, our survey of visual superpowers in comics and other media also revealed a lack of consistency in how superpowers are represented and described. We anticipated that the visual representation of superpowers in these media, as well as the ways in which characters interact with and control them, might serve as direct sources of inspiration for new visualization designs. However, in practice, epistemic abilities in comics are often described rather than shown-typically as narration by the character using them. When illustrated, artists tend to treat visualizations as visual motifs or technobabble, signaling the complex nature of the abilities but providing little specific detail. As a result, we expect that most specific depictions of epistemic superpowers in fiction are unlikely to directly specify the design of visualizations. Nevertheless, the broad set of epistemic abilities imagined in these media suggest a diversity of new applications for visualization techniques (see Sect. 4).</p><p>Visualization and Pragmatic Superpowers. Our early discussions about these ideas narrowed the scope of superpowers to those most closely related to visualization-specifically epistemic powers that enhance how people "see" or which can be illustrated (Fig. <ref type="figure">2</ref>). However, many visualizations can also be seen as giving their designers pragmatic powers over viewers. For example, narrative visualizations such as "The Fallen of WWII" <ref type="bibr" target="#b45">[46]</ref> can evoke strong emotional responses in audiences. Other visualizations may be specifically designed for persuasion or to encourage behavior change (and, as a result, changes in the physical world). Considering these as pragmatic powers switches roles-empowering the designer rather than the viewer of the visualization-and reflects an opportunity for future exploration.</p><p>Superpowers and the Value of Visualization. The visualization literature already refers to a variety of human capabilities that visualizations can enhance <ref type="bibr" target="#b18">[19]</ref>. However, when evaluating visualizations, researchers have tended to prioritize metrics of effectiveness and efficiency <ref type="bibr" target="#b94">[94]</ref>, despite the need to consider a wider range of ways in which new systems might provide value <ref type="bibr" target="#b97">[97]</ref>. The lens of fictional superpowers could help designers and researchers broaden their view. Although superpowers like accelerated perception emphasize effectiveness and speed, fiction typically places a much greater emphasis on the value of the information itself. For example, superpowers that allow people to see the future, see behind surfaces, or see in the dark provide crucial and actionable information, regardless of how rapidly or easily this information is processed by the protagonist. Moreover, powers like the ecological empathy wielded by characters like DC's Poison Ivy or Marvel's Meggan Puceanu (who can sense and communicate with their environment) imply subtle affective and emotional benefits that are not well-captured by efficiency-oriented metrics.</p><p>Ultimately, the visualization community still lacks a concrete theoretical framework for characterizing the numerous ways in which visualizations can provide value. By highlighting the value of empowerment and its correlates, our dimensions of empowerment could serve as a starting point for a broader and more holistic framing, and also suggest new approaches for evaluating the benefits of visualizations.</p><p>A more encompassing framework could help communicate and justify the importance of new kinds of visualizations and application areas, encouraging new avenues for visualization research.</p><p>Visualization Supervillains? We (like most of visualization research) have chosen to focus on positive framings. However, the narrative dynamics of superhero comics-which typically pit super-powered heroes against villains who use similar abilities for more malicious ends-also lend themselves well to critical, adversarial, and "black hat" approaches to visualization <ref type="bibr" target="#b23">[24]</ref>. In fact, even "positive" examples like our emotion vision or building vision could be easily reinterpreted as tools for surveillance and control. Meanwhile, approaches like the shot vision shown in Fig. <ref type="figure">1</ref> reflect a trend towards the increased use of analytics in that is already viewed with disdain by many sports fans, players, and journalists who assert that a focus on metrics and prediction undercuts the spontaneity of play. As such, the narrative structures of superhero fiction may serve as a useful tool for considering the negative and nuanced social implications of visualization systems, in addition to their potential benefits.</p><p>Fairness and Accessibility. In most fiction, superpowers are possessed by a very small fraction of people. When a superpower becomes widespread, it may cease to be considered a superpower (as Syndrome, the villain from The Incredibles puts it, "When everyone is super, no one will be"). Yet unlike superhero fiction, which largely focuses on confrontation and conflict, visualization research tends to envision futures in which increasing numbers of people have access to epistemic tools and society as a whole becomes more empowered.</p><p>However, considering societal empowerment also requires a deeper consideration of equity, fairness, and accessibility. Currently access to epistemic tools varies widely due to social and economic factors, as well as the immense variability of human visual and cognitive abilities <ref type="bibr" target="#b61">[62]</ref>. Visualization research has already made strides in addressing specific disabilities like color vision deficiency and a few projects have focused on empowering individuals with more profound vision impairments <ref type="bibr">[34, pp.11-13]</ref>. Looking forward, new epistemic tools might even help individuals with visual or cognitive differences gain abilities (à la Daredevil) that exceed those of typical humans. More broadly, understanding the potential social impact of epistemic tools calls for models (perhaps improving upon van Wijk's <ref type="bibr" target="#b94">[94]</ref>) that consider the value of increasing overall human empowerment and equity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper we have explored the potential for superpowers to serve as a source of inspiration for visualization, examining visualizationrelated powers in fiction, considering their real-world analogues, and imagining new visualization superpowers of our own. We intend this work as a provocation and an encouragement to the visualization community to actively examine more diverse sources of inspiration for visualization designs, applications, and theory. While our exploration draws predominantly on western superhero comics, many other media also explore possible augmentations of human abilities in ways that may prove inspiring to visualization designers and researchers. Adjacent domains like manga and anime, as well as science fiction and fantasy more broadly, are brimming with visual references, metaphors, and other ideas relevant to visualization. Exploring parallel genres like these presents opportunities for visualization research to capitalize on the kinds of design futuring and speculative design that are increasingly being explored in HCI <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b84">84,</ref><ref type="bibr" target="#b89">89]</ref>. We expect that these approaches can encourage creative and divergent thinking about the future of the field, especially as new platforms and use cases make visualizations an increasingly ubiquitous part of our lives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 .</head><label>3</label><figDesc>fig 3. WE CHARACTERIZE A SET OF UNDERLYING MECHANISMS BEHIND FICTIONAL EPISTEMIC SUPERPOWERS THAT EXTEND CHARACTERS' VISION (LEFT) AND COGNITION (RIGHT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 4. ULTRA BOY's PENETRA-VISION V</figDesc><graphic coords="4,150.83,654.77,139.06,79.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>...WHILE SITELENS SHOWS AIR QUALITY MEASUREMENTS. Guarese et al vis ualize magnetic fields... as Star Trek's Data or Mr. Spock have shown the ability to quickly compute estimates (such as the number of tribbles). Most often artists have characters demonstrate their numeric abilities via dialogue and we found very few visual representations of the ability. Occasionally, special mathematical abilities are illustrated using floating numbers and equations in the scene or by highlighting the characters or objects that are being judged before the result is revealed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig</figDesc><graphic coords="5,317.51,57.65,148.31,71.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig 9.</figDesc><graphic coords="7,318.23,635.81,106.51,94.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig 10.</figDesc><graphic coords="8,46.43,36.53,106.36,99.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 11 .N</head><label>11</label><figDesc>Fig 11.</figDesc><graphic coords="8,309.59,36.17,106.36,95.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig 13.</figDesc><graphic coords="9,55.67,50.57,141.64,84.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>R</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>epistemic SUPERpowers that Are inherently visual and/or are illustrated visually by artists.</head><label></label><figDesc></figDesc><table><row><cell cols="2">1 https://powerlisting.fandom.com/</cell></row><row><cell></cell><cell>'</cell></row><row><cell></cell><cell cols="2">EPISTEMIC SUPERPOWERS</cell></row><row><cell></cell><cell cols="2">characters gain knowledge about things, people, or phenomena</cell></row><row><cell></cell><cell>EPISTEMIC POWERS</cell><cell>EPISTEMIC POWERS</cell></row><row><cell>ALL SUPERPOWERS</cell><cell cols="2">PRAGMATIC SUPERPOWERS THAT ARE VISUAL characters "see" in enhanced ways THAT ARE ILLUSTRATED artists use visuals to "show" characters' enhanced abilities MENTAL PHYSICAL</cell></row><row><cell></cell><cell>characters actively manipulate</cell><cell>'</cell></row><row><cell></cell><cell>things, people, or phenomena</cell><cell>thoughts, ideas, and emotions</cell></row></table><note>FIG 2. we focus specifically on the subset of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ThE room looks empty now, but germ VISION</figDesc><table><row><cell></cell><cell>allows</cell></row><row><cell>me to scan the place and detect</cell><cell>high risk</cell></row><row><cell>pathogens based on historic</cell><cell>Occupancy data!</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://tvtropes.org/pmwiki/pmwiki.php/Main/SherlockScan</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank all of the SEVEN workshop participants and colleagues who contributed to the genesis of this work. This research was funded in part by Inria-Calgary associated team SEVEN, the Canada Research Chairs Program, the Natural Sciences and Engineering Research Council of Canada (NSERC), Alberta Innovates Technology Futures (AITF), SMART Technologies, ULC, and the ANR grant ANR-19-CE33-0012.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pervasive play</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bonsignore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neustaedter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the Conference on Human Factors in Computing Systems (CHI)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3317" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">PoolLiveAid: augmented reality pool table to assist inexperienced players</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodrigues</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meetcues: Supporting online meetings experience</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Aseniero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Constantinides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quercia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Visualization Conference (VIS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="236" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Assassin&apos;s Creed Wiki Community. Eagle vision</title>
		<ptr target="https://assassinscreed.fandom.com/wiki/Eagle_Vision,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sensory substitution and the humanmachine interface</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bach-Y Rita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Kercel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="541" to="546" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Supporting team-first visual analytics through group activity representations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Graphics Interface (GI)</title>
				<meeting>of Graphics Interface (GI)</meeting>
		<imprint>
			<publisher>CHCCS</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="208" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Merging virtual objects with the real world: Seeing ultrasound imagery within the patient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bajura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instrumental interaction: an interaction model for designing post-WIMP user interfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beaudouin-Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="446" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">1930-mathematical functions embodied in ballistic cams</title>
		<author>
			<persName><forename type="first">E</forename><surname>Beauxis-Aussalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<ptr target="http://dataphys.org/list/mathematical-functions-embodied-in-ballistic-cams/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Advantage of terahertz radiation versus x-ray to detect hidden organic materials in sealed vessels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bessou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Caumes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chassagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dautant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ziéglé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Communications</title>
		<imprint>
			<biblScope unit="volume">285</biblScope>
			<biblScope unit="issue">21-22</biblScope>
			<biblScope unit="page" from="4175" to="4179" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of augmented reality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in HCI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="73" to="272" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Research through design fiction: Narrative in real and imaginary abstracts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blythe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="703" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data visualization literacy: Definitions, conceptual frameworks, exercises, and assessments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Börner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bueckle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ginda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Academy of Sciences</title>
				<meeting>of the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="1857" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Point &amp; teleport locomotion technique for virtual reality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bozgeyikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katkoori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer-Human Interaction in Play (CHI PLAY)</title>
				<meeting>of Computer-Human Interaction in Play (CHI PLAY)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The IKEA catalogue: Design fiction in academic and industrial collaborations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Supporting Group Work (GROUP)</title>
				<meeting>of Supporting Group Work (GROUP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Le geste canal de communication homme/machine: la communication&quot; instrumentale</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cadoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technique et science informatiques</title>
				<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="31" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Form follows sound: designing interactions from sonic memories</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caramiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Altavilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Pobiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3943" to="3952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Information visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HCI handbook: Fundamentals, evolving technologies, and emerging applications</title>
				<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Readings in Information Visualization: Using Vision to Think</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MARVisT: Authoring glyph-based visualization in mobile augmented reality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2645" to="2658" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The elements of a super-hero</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Sims</surname></persName>
		</author>
		<ptr target="https://comicsalliance.com/superhero-periodic-table-superpowers/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Official Handbook of the Marvel Universe: Daredevil</title>
		<author>
			<persName><forename type="first">J</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marvel</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Subitizing: The neglected quanitfier</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sarama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Constructing Number: Merging Perspectives from Psychology and Mathematics Education</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Norton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Alibali</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Black hat visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE VIS Workshop on Dealing with Cognitive Biases in Visualisations (DECISIVe</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A design space exploration of worlds in miniature</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Danyluk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human factors in streaming data analysis: Challenges and opportunities for information visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Arendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="254" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The statistical study of evolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Davenport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Popular Science Monthly</title>
		<imprint>
			<biblScope unit="page" from="447" to="460" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Computer vision in autism spectrum disorder research: a systematic review of published studies from</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A J</forename><surname>De Belen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bednarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Del</forename><surname>Favero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Translational Psychiatry</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2009">2009 to 2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deus Ex Wiki-Smart Vision (DXMD)</title>
		<author>
			<persName><forename type="first">Deux Ex Wiki</forename><surname>Authors</surname></persName>
		</author>
		<ptr target="https://deusex.fandom.com/wiki/Smart_Vision_(DXMD" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3" to="2021" />
		</imprint>
	</monogr>
	<note>Wiki page</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A visual interaction cue framework from video game environments for augmented reality</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Dillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T H</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oehlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A task-based taxonomy of cognitive biases for information visualization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dimara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bezerianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1413" to="1432" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What is interaction for data visualization?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dimara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="129" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">1787-chladni plates. list of physical visualizations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<ptr target="http://dataphys.org/list/chladni-plates/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data physicalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Moere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Handbook of HCI</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The useful field of view test: normative data for older adults</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Wadley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Clay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Roenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. of Clinical Neuropsychology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="286" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A taxonomy of 3d occlusion management for visualization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tsigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1095" to="1109" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Computer vision classifier and platform for automatic counting: more than cars</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Espinoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Ecuador Technical Chapters Meeting (ETCM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Can thermal imaging see through walls? Website</title>
		<author>
			<persName><surname>Flir</surname></persName>
		</author>
		<author>
			<persName><surname>Systems</surname></persName>
		</author>
		<ptr target="https://www.flir.com/discover/cores-components/can-thermal-imaging-see-through-walls" />
		<imprint>
			<biblScope unit="page" from="3" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Enhancing cultural tourism experiences with augmented reality technologies</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Susperregui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Linaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Symposium on Virtual Reality, Archaeology and Cultural Heritage (VAST)</title>
				<meeting>of the Symposium on Virtual Reality, Archaeology and Cultural Heritage (VAST)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Augmented reality visualization for laparoscopic surgery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Livingston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rademacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Drake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
				<meeting>of Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="934" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The super cockpit and its human factors challenges</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Furness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Human Factors Society Annual Meeting</title>
				<meeting>of the Human Factors Society Annual Meeting</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="48" to="52" />
		</imprint>
		<respStmt>
			<orgName>SAGE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MyLifeBits: fulfilling the Memex vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lueder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Multimedia</title>
				<meeting>of Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="235" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bitdrones: Towards using 3d nanocopter displays as interactive self-levitating programmable matter</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rubens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Braley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Characterizing asymmetric collaborative interactions in virtual and augmented realities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Grandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Debarba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maciel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Augmented situated visualization methods towards electromagnetic compatibility testing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Andreasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maciel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The fallen of World War II</title>
		<author>
			<persName><forename type="first">N</forename><surname>Halloran</surname></persName>
		</author>
		<ptr target="http://www.fallen.io/ww2" />
		<imprint>
			<biblScope unit="page" from="3" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graphical histories for visualization: Supporting analysis, communication, and evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1189" to="1196" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pixmix: A real-time approach to high-quality diminished reality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Herling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Broll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Mixed and Augmented Reality (ISMAR)</title>
				<meeting>of Mixed and Augmented Reality (ISMAR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Passive real-world interface props for neurosurgical visualization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Kassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="452" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spider hero: a VR application using pulling force feedback system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ishibashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Da Luz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Segi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Terada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miyata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Virtual Reality Continuum and its Applications in Industry (VRCAI)</title>
				<meeting>of Virtual Reality Continuum and its Applications in Industry (VRCAI)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="197" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Agerelated decline in color perception and difficulties with daily activitiesmeasurement, questionnaire, optical and computer-graphics simulation studies</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nagamachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hiramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Osaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Industrial Ergonomics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Laplacian vision: Augmenting motion prediction via optical see-through head-mounted displays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orlosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kiyokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Augmented Human International Conference (AH)</title>
				<meeting>of the Augmented Human International Conference (AH)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An interaction model for visualizations beyond the desktop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2396" to="2405" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Opportunities and challenges for data physicalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kildal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3227" to="3236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Empowering the exercise: A body-controlled trampoline training game</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kajastila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Holsti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hämäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science in Sport</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="23" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Millimetre wave and terahertz technology for detection of concealed threats-a review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Kemp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Infrared and Millimeter Waves and the Conference on Terahertz Electronics</title>
				<meeting>Infrared and Millimeter Waves and the Conference on Terahertz Electronics</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="647" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On distinguishing epistemic from pragmatic action</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="549" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">How augmented reality can help your stargazing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kohles</surname></persName>
		</author>
		<ptr target="https://www.wikitude.com/blog-augmented-reality-star-gazing/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Zooids: Building blocks for swarm user interfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Le</forename><surname>Goc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parsaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Follmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UIST</title>
				<meeting>of UIST</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="97" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">MeMod: A Modular Hacking and Programming Toolkit For Everyday Objects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Tangible, Embedded, and Embodied Interaction (TEI)</title>
				<meeting>of Tangible, Embedded, and Embodied Interaction (TEI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="757" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Remixed reality: manipulating space and time in augmented reality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lindlbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Survey on individual differences in visualization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Crouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ottley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="693" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pursuit of &quot;x-ray vision&quot; for augmented reality</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Livingston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sandor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Augmented Reality Environments</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="67" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Full body virtual autopsies using a state-of-the-art volume rendering pipeline</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winskog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lundstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="869" to="876" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Henry Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuerzlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Immersive Analytics</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Designing Interactions. MIT</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moggridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The early history of x-ray diagnosis with emphasis on the contributions of physics 1895-1915</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Mould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in Medicine &amp; Biology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1741</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">In the data kitchen: A review (a design fiction on data science)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ext. Abstracts of Proc. of CHI</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">110</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Visualization Analysis &amp; Design</title>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An evaluation of the effects of hypernatural components of interaction fidelity on locomotion performance in virtual reality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nabiyouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Imura</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Figueroa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Mohler</surname></persName>
		</editor>
		<meeting>of Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments</meeting>
		<imprint>
			<publisher>ICAT-EGVE</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Eurographics</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Augmented perception of satiety: controlling food consumption by changing apparent size of food with augmented reality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Narumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kajinami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tanikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Overview of imageclef lifelog 2020: lifelog moment retrieval and sport performance lifelog</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-T</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName><surname>Dang-Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">State of the art of sports data visualization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Perin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Stolper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="663" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Eccescopy: to look, is to see</title>
		<author>
			<persName><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">XRDS: Crossroads, The ACM Magazine for Students</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="39" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The giant-size omnibus of superpowers</title>
		<ptr target="https://popchart.co/products/the-giant-size-omnibus-of-superpowers" />
	</analytic>
	<monogr>
		<title level="m">Pop Chart</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Through the hololens looking glass: augmented reality for extremity reconstruction surgery using 3d vascular models with perforating vessels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lawton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spyropoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European radiology experimental</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Characterizing provenance in visualization and data analysis: an organizational framework of provenance types and purposes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Ragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">How do they create Superpower? An exploration of knowledge-creation processes and work environments in the wearable technology industry</title>
		<author>
			<persName><forename type="first">D</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ha-Brookshire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Fashion Design</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="93" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Technology and Education</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">RFIG Lamps: Interacting with a Self-Describing World via Photosensing Wireless Tags and Projectors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Willwacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">406415</biblScope>
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Chartaccent: Annotation for data-driven storytelling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Pacific Visualization Symposium (PacificVis)</title>
				<meeting>of the Pacific Visualization Symposium (PacificVis)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="230" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">VaiR: Simulating 3D Airflows in virtual reality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rietzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Plaumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krnzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rukzio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Human Factors in Computing Systems (CHI)</title>
				<meeting>of Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5669" to="5677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Virtual superheroes: Using superpowers in virtual reality to encourage prosocial behavior</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Baughman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Bailenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Unsupervised deep representations for learning audience facial behaviors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Navarathna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Helminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
				<meeting>CVPR Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1132" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Make It So: Interaction Design Lessons from Science Fiction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shedroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Noessel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Rosenfeld Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">DXR: A toolkit for building immersive data visualizations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sicat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cordeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="715" to="725" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">The history of real time ultrasound</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Somer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Congress Series</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1274</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Information Visualization: Design for Interaction. Pearson Education Limited, 2 nd edition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Spence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Value-driven evaluation of visualizations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Beyond Time and Errors: Novel Evaluation Methods for Visualization (BELIV)</title>
				<meeting>of Beyond Time and Errors: Novel Evaluation Methods for Visualization (BELIV)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Design fiction. Interactions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sterling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="20" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Realitysketch: Embedding responsive graphics and visualizations in AR through dynamic sketching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diverdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leithinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. User Interface Software &amp; Technology (UIST)</title>
				<meeting>User Interface Software &amp; Technology (UIST)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="166" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Four types of ensemble coding in data visualizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franconeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Vision</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="11" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Physical principles and technology of clinical pet imaging</title>
		<author>
			<persName><forename type="first">D</forename><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the Academy of Medicine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="145" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">SCI-FI: shape-changing interfaces, future interactions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Troiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tiab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Nordic Conference on Human-Computer Interaction (NordiCHI)</title>
				<meeting>of the Nordic Conference on Human-Computer Interaction (NordiCHI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The value of visualization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Visualization</title>
				<meeting>of Visualization</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005-10">Oct 2005</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Visual search for change in older adults</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Veiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Storandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Abrams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology and aging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">754</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Superpower glass: delivering unobtrusive real-time social cues in wearable systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adjunct Proc. of the Joint Conference on Pervasive and Ubiquitous Computing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1218" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">An emotional response to the value of visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klatzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hurtienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hornecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barrass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="8" to="17" />
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Information Visualization: Perception for Design</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The Art of Clean Up: Life Made Neat and Tidy</title>
		<author>
			<persName><forename type="first">U</forename><surname>Wehrli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chronicle Books</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Sitelens: Situated visualization techniques for urban site visits</title>
		<author>
			<persName><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHI</title>
				<meeting>of CHI</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">I drive-you trust: Explaining driving behavior of autonomous cars</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ext. Abstracts of Proc. of CHI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Embedded data representations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="461" to="470" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Real-fictional entanglements: Using science fiction and design fiction to interrogate sensing technologies</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Wyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DIS</title>
				<meeting>of DIS</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="567" to="579" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
