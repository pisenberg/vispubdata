<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Efficient Dual-Hierarchy t-SNE Minimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mark</forename><surname>Van De Ruit</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Billeter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elmar</forename><surname>Eisemann</surname></persName>
						</author>
						<title level="a" type="main">An Efficient Dual-Hierarchy t-SNE Minimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEBF87765CB1D5982603B54A86E5604F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High dimensional data</term>
					<term>dimensionality reduction</term>
					<term>parallel data structures</term>
					<term>dual-hierarchy</term>
					<term>GPGPU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1: Our method leverages a pair of spatial hierarchies over the embedding (center right) and a field (far right) over the embedding space to accelerate t-SNE minimization. Progression of minimizations (left) using these hierarchies is shown for a 60K point MNIST dataset (top) and a 1.2M point ImageNet dataset (bottom). The hierarchies are visualized for the last iteration of minimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The exploration of high-dimensional data has received significant interest. Non-linear dimensionality reduction techniques have made it possible to visualize structures in large-scale high-dimensional datasets, leading to discoveries in many different domains, such as immunology <ref type="bibr" target="#b22">[23]</ref> and forensic analysis <ref type="bibr" target="#b12">[13]</ref>. The ability to successfully preserve local structures in the data is especially important. The t-Distributed Stotachstic Neighbour Embedding (t-SNE) algorithm <ref type="bibr" target="#b24">[25]</ref> achieves this goal by matching pairwise similarity distributions, representing the original data in the high-dimensional space and a possible embedding in a low-dimensional space. The algorithm consists of two phases. First, a similarity distribution is constructed over the high-dimensional data. Second, a minimization is performed using the Kullback-Leibler (KL) divergence <ref type="bibr" target="#b8">[9]</ref> between this distribution and a low-dimensional distribution, which is initially constructed over a random embedding.</p><p>Both phases of the t-SNE computation are costly operations, becoming impractical for very large datasets. While significant effort has been invested into lowering the computational cost of the similarity  <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, the minimization remains costly. Here, efforts have focused on efficiently mapping the minimization to GPU hardware <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> or on reducing the O(N 2 ) runtime complexity; the commonly used Barnes-Hut t-SNE (BH-SNE) <ref type="bibr" target="#b23">[24]</ref> initially obtained a O(N log N) runtime complexity, and O(N) complexities were achieved afterwards by both Linderman et al. <ref type="bibr" target="#b10">[11]</ref> and Pezotti et al. <ref type="bibr" target="#b19">[20]</ref>. While effective for smaller 2D embeddings, millions of points remain costly and there is a significant overhead in 3D.</p><p>Our work introduces a pair of sparsely constructed spatial hierarchies to accelerate the t-SNE minimization. The first hierarchy is constructed over the embedding, and the second over a discretization of the embedding's space. We approximate N-body computations, a costly part of the t-SNE minimization, by computing interactions between the two hierarchies using a dual-hierarchy traversal. During traversal, we eliminate the majority of these interactions using an improved formulation of the BH-SNE approximation <ref type="bibr" target="#b23">[24]</ref>. While our minimization retains a O(N) runtime complexity, the number of considered interactions is significantly reduced. As N-body computations previously dominated the runtime of t-SNE for two-and especially three-dimensional embeddings, our method provides a strong improvement, significantly outperforming the state-of-the-art while generating high-quality embeddings. Further, our method is designed with GPGPU programming in mind, leveraging the compute capabilities of modern GPUs.</p><p>We first formally introduce t-SNE (Sect. 2) and related work (Sect. 3). We then cover our method (Sect. 4), its implementation details (Sect. 5), and evaluation (Sect. 6), before concluding (Sect. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">T-SNE</head><p>t-SNE models a dataset of points X = x 1 ,...,x N in a high-dimensional space through pairwise similarities, represented as a symmetric joint probability distribution P. Likewise, a randomly initialized embedding of low-dimensional points Y = y 1 ,...,y N is represented in a similarity distribution Q. The goal of t-SNE is to minimize the difference between P and Q according to a cost function.</p><p>The distribution P, defined over the high-dimensional data points, represents the joint similarity p i j between all pairs x i and x j . This similarity can be interpreted as the probability of these data points being near to each other in high-dimensional space. In a similar manner, the similarity between representative low-dimensional embedding points y i and y j is represented as q i j . To minimize the difference between P and Q, the cost function C is used</p><formula xml:id="formula_0">C(P, Q) = KL(P Q) = N ∑ i=1 N ∑ j =i p i j ln( p i j q i j ),<label>(1)</label></formula><p>which is the KL-Divergence between P and Q. During minimization the positions of embedding points are updated to minimize this cost. The joint similarity p i j is modeled through centering of a pair of Gaussian kernels on either high-dimensional data point as</p><formula xml:id="formula_1">p i j = p i| j + p j|i 2N ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">p j|i = exp(−( x i − x j 2 )/(2σ 2 i )) ∑ N k =i exp(−( x i − x k 2 )/(2σ 2 i ))<label>(3)</label></formula><p>and variance σ i is defined according to the local density in the highdimensional space around x i . As p j|i acts on a local neighbourhood outside of which influence diminishes rapidly, the effective number of considered points is typically much lower than N. It is instead based on a user-controlled perplexity value μ, and σ i is then chosen such that μ = 2 − ∑ N j p j|i log p j|i <ref type="bibr" target="#b3">(4)</ref> holds for each i. For the low-dimensional similarity q i j , a Student's t-Distribution with one degree of freedom is used instead of a Gaussian distribution. q i j is defined as</p><formula xml:id="formula_3">q i j = (1 + y i − y j 2 )Z −1 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_4">Z = N ∑ k=1 N ∑ l =k 1 + y k − y l 2 −1 . (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>Intuitively, to ensure that distribution Q closely represents P, their local neighbourhoods should match each other. Hence, the algorithm iteratively moves randomly-initialized embedding points around to match this criterion. This movement stems from a gradient descent applied to C. In each iteration, the gradient is computed and subsequently used to update the positions of the embedding points relying on its analytical formulation over y i :</p><formula xml:id="formula_6">δC δ y i = 4(Z N ∑ j =i p i j q i j (y i − y j ) − N ∑ j =i q 2 i j Z(y i − y j )) (7) = 4(F attr i − F rep i ).<label>(8)</label></formula><p>As shown, the gradient is decomposed into F attr and F rep , which allows for a potential reformulation as an N-body problem, where each of the N embedding points exerts attractive and repulsive forces on surrounding points. As is typical for N-body problems, the computational complexity is O(N 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>After the introduction of t-SNE <ref type="bibr" target="#b24">[25]</ref>, Barnes Hut SNE (BH-SNE) <ref type="bibr" target="#b23">[24]</ref>, reduced the runtime complexity to O(N log N), and memory complexity to O(N). It models the similarity computation in Equation 3 as a k-nearest-neighbour (KNN) graph problem, computed using Vantage Point trees <ref type="bibr" target="#b27">[28]</ref>. In addition, a Barnes-Hut approximation <ref type="bibr" target="#b0">[1]</ref>, previously used in physics calculations, significantly reduces the number of force computations in the N-body problem. More recent developments can be divided into two areas: improving similarity computations and improvements/replacements of the minimization algorithm. Early on, Approximated tSNE (A-SNE) <ref type="bibr" target="#b18">[19]</ref>, relied on principles of Progressive Visual Analytics <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref> to selectively refine parts of approximate embeddings during the optimization, while replacing a precise KNN-graph with an approximate graph relying on a forest of randomized KD-trees. A similar approach was demonstrated with LargeViz <ref type="bibr" target="#b21">[22]</ref>, which instead leverages randomized projection trees to obtain similarities. In addition, it links the minimization's objective function to a probabilistic graph-visualization model, which is optimized through an asynchronous stochastic gradient descent. A rather different approach is Uniform Manifold Approximation and Projection (UMAP) <ref type="bibr" target="#b11">[12]</ref>, which instead performs a minimization between topological representations of the high-dimensional and low-dimensional spaces. While it provides superior performance to all t-SNE variants described so far, it has been shown to suffer from many of the same downsides <ref type="bibr" target="#b7">[8]</ref>. Despite the improvements, current available implementations are orders of magnitude slower than more recent GPGPU solutions.</p><p>A fast GPU-based approach is CUDA-SNE <ref type="bibr" target="#b1">[2]</ref>. The method approximates KNN in a manner similar to A-SNE <ref type="bibr" target="#b18">[19]</ref> with the GPUbased FAISS library <ref type="bibr" target="#b5">[6]</ref>, and maps the BH-SNE <ref type="bibr" target="#b23">[24]</ref> optimization to a GPGPU programming environment. Although this approach achieves good performance on large datasets, it largely relates to engineering optimizations and remains bound by O(N log N) runtime complexity.</p><p>More recently, linear runtime complexity was reached by Fast Fourier transform accelerated interpolation-based t-SNE (FIt-SNE) <ref type="bibr" target="#b10">[11]</ref>, demonstrated with a CPU-based implementation. It uses an alternative approximation for computing repulsive forces by redefining them in terms of a convolution over an equispaced grid, which is subsequently interpolated to recover repulsive forces.</p><p>A similar GPU-based approach was developed by Pezotti et al. <ref type="bibr" target="#b19">[20]</ref>, named GPGPU linear complexity t-SNE (L-SNE). The authors rewrite Equation 8 as a function of scalar and vector fieldscontinuous functions assigning scalar or vector values to positions in space -which are then approximated in a discrete format using a GPU texture in O(FN) time (where F is the size of the discrete texture). Afterwards, force components are recovered through texture interpolation, which is highly efficient on GPUs. The method's runtime is dominated by the computation of this field texture, which suffers from scaling in either F or N and is particularly inefficient for 3D embeddings. In the following, we briefly cover this field-based formulation before presenting our approach, which avoids these shortcomings.</p><p>Given are the scalar and vector fields S : R d → R and V : R d → R d , d being the dimensionality of the embedding, typically 2 or 3. At an arbitrary position p the fields are defined as</p><formula xml:id="formula_7">S (p) = N ∑ i 1 + y i − p 2 −1 , (<label>9</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">V (p) = N ∑ i y i − p 1 + y i − p 2 2 (10)</formula><p>Based on the Student's t-distribution, S represents the effective density of the embedding space, while V represents the gradient of the repulsive forces applied. Assuming for now that these fields are available, attractive forces can be approximated in a restricted neighbourhood as</p><formula xml:id="formula_10">Fattr i = Ẑ ∑ ∈kNN(i) p i q i (y i − y ),<label>(11)</label></formula><p>as seen in BH-SNE <ref type="bibr" target="#b23">[24]</ref>. The normalization factor Ẑ is now approximated in linear time by consulting the scalar field:</p><formula xml:id="formula_11">Ẑ = N ∑ =1 (S (y ) − 1). (<label>12</label></formula><formula xml:id="formula_12">)</formula><p>The repulsive force for a single point is approximated as</p><formula xml:id="formula_13">Frep i = V (y i )/ Ẑ. (<label>13</label></formula><formula xml:id="formula_14">)</formula><p>Computing an approximate gradient now requires linear runtime, as the fields are queried in constant time, approximated in a discrete texture format, and separately computed through a summation of the contributions of all embedding positions. Formally, positions in the fields sum kernels S and V as follows:</p><formula xml:id="formula_15">S (p) = N ∑ i S(y i − p), S(t) = 1 + t 2 −1 , (<label>14</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">V (p) = N ∑ i V (y i − p), V (t) = t 1 + t 2 −2 . (<label>15</label></formula><formula xml:id="formula_18">)</formula><p>While this leads to a linear runtime, there are two observations. The kernels S and V are again based on a Student's t-distribution and have limited effects on far-away positions, but are applied to all positions with full accuracy. In addition, as the kernels have a fixed support in the embedding space, the field's discrete representation must grow with the embedding as the minimization progresses, gradually becoming larger. Pezotti et al. <ref type="bibr" target="#b19">[20]</ref> propose that F N generally holds. However, while the texture grows slowly in two dimensions, the addition of a third dimension (which implies a cubic scaling of F) strongly reduces potential effectiveness. While theoretically of linear runtime, the solution is not optimal when F becomes large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DUAL-HIERARCHY T-SNE MINIMIZATION</head><p>Here, we present our approach to an efficient t-SNE minimization using the field-based formulation <ref type="bibr" target="#b19">[20]</ref>. Our approach reduces the field texture's construction time, which dominates the original runtime and renders the solution impractical for higher embedding dimensionalities. We observe that this discrete representation in form of a texture requires evaluating many small regions with varying local interactions, but similar global interactions. We propose to represent both the embedding and the discrete field as spatial hierarchies, henceforth referred to as the embedding hierarchy and the field hierarchy respectively. We perform a dual traversal over these hierarchies, during which we employ an improvement of the approximation criterion used in BH-SNE <ref type="bibr" target="#b23">[24]</ref> to selectively compute interactions between hierarchy nodes, which represent large regions in the embedding and the field (Fig. <ref type="figure">2</ref>). These interactions between the regions are not directly transferred to data points but are first stored in the hierarchy itself; specifically, for a region, the interaction is added to its corresponding node of the hierarchy. Hereby, we benefit from both hierarchies. After dual traversal, we accumulate these interactions that are stored throughout the hierarchy to form a complete, yet sparsely-computed approximated field. In this way, we improve upon the original O(FN) complexity of the field computation, as our cost approaches O(N). We provide a proof in the supplementary material, but suggest to first follow the algorithm in this section to ease understanding. Fig. <ref type="figure">3</ref> shows an overview of our method, divided into three steps: hierarchy construction, dual traversal, and field accumulation. We detail each step in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hierarchy Construction</head><p>We construct hierarchies over the embedding and field (Fig. <ref type="figure">3</ref>, first part). Meyer et al. <ref type="bibr" target="#b13">[14]</ref> showed that a careful choice of the spatial hierarchy provides performance improvements to BH-SNE <ref type="bibr" target="#b23">[24]</ref>. We choose our structures with efficient execution on the GPU in mind.</p><p>As embedding hierarchy, we select an implicit linear bounding volume hierarchy (BVH), constructed in linear time on the GPU (Sect. 5). In a BVH, each node stores an axis-aligned bounding box (AABB) Fig. <ref type="figure">2</ref>: We use embedding and field hierarchies (top), comparing their nodes to compute interactions between many points and large portions of the field in a single step (bottom left). Where refinement is necessary, we descend one or both hierarchies (bottom mid), continuing until points interact with a full-resolution field (bottom right). Fig. <ref type="figure">3</ref>: We generate embedding and field hierarchies, and dual traverse these using a work queue. When a numeric approximation of the interactions suffices, we cull node pairs. Further, we evaluate and store interactions at different levels of detail in the hierarchies. A final traversal constructs the field used in the t-SNE minimization.</p><p>encompassing the child-node bounding boxes, while leaf nodes directly contain one or more objects (i.e., embedding points). BVHs provide a close fit around contained data, and allow for refitting of AABBs without fully rebuilding the hierarchy. The latter is an important costsaving measure, made possible because embedding positions move slowly during the minimization. As with BH-SNE <ref type="bibr" target="#b23">[24]</ref>, nodes in the embedding hierarchy track their center of mass, defined as the average of the contained embedding points. The center of mass c i of a node e i with mass m i is simply</p><formula xml:id="formula_19">c i = 1 m i ∑ j∈emb(e i ) y j , (<label>16</label></formula><formula xml:id="formula_20">)</formula><p>where emb(e i ) defines the indices of the points in the node.</p><p>Observing the discrete grid nature of the field texture, we select a sparse implicit quad-/octree for the field hierarchy. We mark cells of interest in the grid that we build our hierarchy upon. This is done in O(F log N) time, but marking costs are negligible in practice (&lt; 1% of total compute time). For each grid cell, we descend the previously generated embedding hierarchy to determine if the cell overlaps or borders embedding points. We then construct the sparse field hierarchy with the marked cells as leaf nodes. Computing the field for these locations suffices, as it will only be accessed here during the minimization. Each node f j in the hierarchy has scalar and vector field entries Ŝ j and V j , which are initialized as 0 at the start of every iteration of the minimization and used as intermediate storage during traversal. Contrary to the embedding hierarchy, nodes in the field hierarchy represent regions and their center of mass c j is simply their region's geometric center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dual Traversal</head><p>With both hierarchies available, we perform a dual traversal (Fig. <ref type="figure">3</ref>, second part), formulated as a top-down breadth-first traversal of a single, larger tree. This tree consists of nodes representing node pairs (e i , f j ), where e i and f j are respectively nodes in the embedding and field hierarchies. Each node pair represents a potential interaction between the embedding points and field regions described by the two contained nodes. We model traversal using a work queue, in which we store node pairs in the dual hierarchy that still have to be traversed. At the start of traversal, a root node pair, i.e. (e 0 , f 0 ), is pushed on the queue. During traversal, a node pair is popped from the queue, and is subsequently subdivided. We descend one level in both hierarchies under each node if possible. If both nodes are leaves, we compute the underlying interactions directly. Otherwise, the different possible pairs of child nodes from both hierarchies are tested via an approximation criterion (Sect. 4.3), to determine if they represent interactions with a sufficient accuracy. If this criterion fails for a child node pair, it is pushed on the work queue for further subdivision. If it holds, we will not further descend into the dual hierarchy underneath this child node pair but process them directly.</p><p>To process a node pair, we compute the interactions by using an approximation of the kernels in Equation <ref type="formula" target="#formula_15">14</ref>and Equation <ref type="formula" target="#formula_17">15</ref>:</p><formula xml:id="formula_21">Ŝ (e i , f j ) = m i S(c i − c j ),<label>(17)</label></formula><p>V</p><formula xml:id="formula_22">(e i , f j ) = m i V (c i − c j ).<label>(18)</label></formula><p>Both values are computed once and will be used for all m i points in the embedding node and all regions under the field node instead of evaluating m i values for potentially many field cells. These values are atomically added to Ŝ j and V j in the field node f j .</p><p>Traversal is finished once the work queue is empty. We purposefully subdivide node pairs before testing an approximation criterion -as opposed to the inverse -for implementation reasons (Sect. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Barnes-Hut Approximation</head><p>We modify the dual-hierarchy approximation criterion of BH-SNE <ref type="bibr" target="#b23">[24]</ref> to determine whether a node pair can be processed. Originally, given lengths r i , r j of the diagonals of two nodes' AABBs, and node centers c i , c j , the following term is evaluated:</p><formula xml:id="formula_23">max(r i , r j ) c i − c j &lt; θ . (<label>19</label></formula><formula xml:id="formula_24">)</formula><p>The parameter θ defines a maximum allowed ratio, interpreted as the tangent of an angle in a triangle whose opposite and adjacent  edges have lengths max(r i , r j ) and c i − c j respectively. A larger θ means larger bounding boxes closer to each other pass the test, leading to earlier processing in the hierarchy (faster traversal) but a coarser approximation. Similarly, if θ = 0, the hierarchies are traversed fully, leading to an inefficient but accurate computation. For single-hierarchy traversals, θ typically lies between 0.1 and 0.5 <ref type="bibr" target="#b23">[24]</ref>. The condition is simple as it is evaluated many times, assuming that all bounding boxes in both hierarchies have regular sides (as is the case for quad-/octrees).</p><p>Hierarchies such as a linear BVH tend to produce irregular bounding boxes that closely fit the contained data. Here, the Barnes-Hut criterion is suboptimal, as it considers a bounding box based on its diagonal, which is not a good representative of all sides when having a highly irregular bounding box. Hence, we modify Equation 19 to project the diagonals d i , d j of the nodes' bounding boxes so the approximation criterion accurately matches this irregularity, leading to projected diagonal lengths r i , r j . We visualize our approach in Fig. <ref type="figure" target="#fig_0">4</ref>.</p><p>To obtain projected diagonal lengths, we first compute a unit vector t along the difference c i − c j , but reflected across axes so it is nearorthogonal to the diagonals. In two dimensions, this is simply:</p><formula xml:id="formula_25">t = c i − c j ||c i − c j || −1 1 . (<label>20</label></formula><formula xml:id="formula_26">)</formula><p>A suitable length is then obtained through vector rejection as:</p><formula xml:id="formula_27">r i = d i − t (d i • t) . (<label>21</label></formula><formula xml:id="formula_28">)</formula><p>We compute r j in the same manner and use max(r i , r j ) for the comparison in Equation <ref type="formula" target="#formula_23">19</ref>. We evaluate this criterion in Sect. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Field accumulation</head><p>After dual traversal, we collect the approximate interactions stored in the hierarchies (Fig. <ref type="figure">3</ref>, third part). In particular, field hierarchy nodes now store intermediate parts of the actual fields in Ŝ j and V j . As in <ref type="bibr" target="#b19">[20]</ref>, we want to interpolate the discrete field to obtain approximate field values at embedding positions, which is difficult in a hierarchy.</p><p>Hence, we flatten it to recover a coarsely approximated texture, i.e., we ascend the field hierarchy upwards once for each non-empty leaf node, accumulating encountered field scalar and vector values and storing their sum in the respective texture position of said leaf node.</p><p>This requires O(F log F) time when gathering upwards from a leaf to the root. Performing the operation in reverse leads to O(F) time, but becomes less practical on GPU hardware. Afterwards, the field can be queried for interpolated scalar and vector field values per point. Fig. <ref type="figure" target="#fig_1">5</ref> displays an accumulation of different levels of the field hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>Our technique is implemented using a GPGPU approach. We develop our implementation in a combination of the OpenGL 4.6 API and CUDA 11, although the concepts we described can be applied on other APIs. Our implementation is available online. 1  Mirroring the algorithmic description, our t-SNE implementation consists of two parts. We first generate the joint similarity distribution P in the same manner as Chan et al. <ref type="bibr" target="#b1">[2]</ref>, using approximate KNN information with k = 3μ obtained through the GPU-based FAISS library <ref type="bibr" target="#b5">[6]</ref>. The formulation of the distributions remains the same as BH-SNE <ref type="bibr" target="#b23">[24]</ref>. Second, we mirror the matrix-based minimization used by Pezotti et al. <ref type="bibr" target="#b19">[20]</ref>. During the minimization, we invest time at the start of each iteration to rebuild or refit our spatial hierarchies, and then perform a dual-hierarchy traversal, replacing the expensive field computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hierarchy Construction</head><p>As mentioned, we implement the embedding hierarchy as an implicit linear BVH, constructed on the GPU in O(N) time. We outline the general method, but refer the reader to Lauterbach et al. <ref type="bibr" target="#b9">[10]</ref> for a full description. In short, the linear BVH method reduces BVH construction to a single sorting operation (Fig. <ref type="figure">6</ref>). Each of the N embedding points is assigned a Morton code based on their discretized position in 2D/3D space. Based on these codes, the points are bucketed in leaf nodes, which are subsequently arranged along a space-filling z-order or Morton curve in a O(N) parallel radix sort, using the Morton codes as keys. After sorting, levels of the hierarchy are constructed iteratively by grouping nodes which share the same high order bits in their respective Morton codes. Our implementation adopts the work-queue based approach of Garanzha et al. <ref type="bibr" target="#b4">[5]</ref>. Faster and more recent construction algorithms can be used at the cost of increased code complexity. For a parallel radix sort, we leverage the implementation available in the CUDA-based CUB library <ref type="bibr" target="#b16">[17]</ref>, which can access specific buffer objects in OpenGL through the included interoperability library.</p><p>We implement the field hierarchy as a sparse implicit quad-/octree due to the discrete nature of the field texture. As nodes in this hierarchy are regular, we do not store bounding box information, instead deriving these from node indices when necessary. The only information we store in a node is its type and the mentioned intermediate scalar and vector values used during traversal.</p><p>Although the embedding changes rapidly during early iterations, changes are less pronounced later on. Early iterations of t-SNE, typically the first 250, use early exaggeration, multiplying p i j by some scalar to aggressively separate clusters. We use this to our advantage to reduce hierarchy-construction costs significantly. While we rebuild hierarchies on every iteration during early exaggeration, we only do so at regular intervals after. We can often simply refit bounding boxes around the newly updated positions, avoiding costly sorting. As the number of leaves in the field hierarchy can change at each iterationleading to a substantially different spatial hierarchy -we include the cells bordering embedded points as leaves, which enables a reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dual Traversal</head><p>As described in Sect. 4, dual-hierarchy traversal is formulated as a breadth-first traversal, in which node pairs are read, subdivided, and tested for further traversal. We leverage a pair of work-queues to track traversal. At the start of traversal, an initial set of node pairs (matching to root nodes) is written to the first, or primary work-queue. During a single step of traversal, all node pairs on the primary work-queue are subdivided and tested for Equation <ref type="formula" target="#formula_23">19</ref>. Node pairs which fail the approximation criterion are pushed on the secondary work-queue, which is subsequently swapped with the primary work-queue for the 1 https://www.github.com/markvanderuit/dual_hierarchy_tsne Fig. <ref type="figure">6</ref>: Linear BVH <ref type="bibr" target="#b9">[10]</ref> construction: points are discretized and assigned Morton codes, after which they are sorted along a spatial curve. Sorted points are then subdivided into a hierarchy based on their codes. next traversal step. We repeat this process until the primary work-queue is empty or the leaf levels are reached, at which point traversal has completed.</p><p>As root nodes encompass the entire embedding, they will always be subdivided. As an optimization, we start traversal at a lower level in both hierarchies by pushing all pairs corresponding to the selected levels on the work-queue (we use levels 3/2 for a 2D/3D embedding, leading to 4096 node pairs for hierarchies with fan-outs 4/8).</p><p>To optimize subdivision, we leverage local cross-communication capabilities of modern GPUs (subgroups in OpenGL/GLSL, warps in CUDA) to test multiple combinations of node pairs per GPU thread (invocation) while minimizing memory operations. To subdivide a single node pair on both sides, we use two threads (four for quadtrees, eight for octrees), having each thread load a single child node from both sides of the hierarchies. The total number of node pairs that must be tested (four for binary trees, 16 for quadtrees, 64 for octrees) can be obtained by rotating the child nodes on one side of the hierarchy along the 2/4/8 threads, using the subgroup capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Single-Hierarchy Fallback</head><p>As described in Sect. 3, the discrete field grows in size as the minimization progresses. At its start, the small discrete field implies few regions of interest require computation, leading to a sparse hierarchy. In this scenario, a dual traversal is inefficient as the field hierarchy's levels have too few nodes to fully occupy the GPU. We establish a maximum positive difference in depths d e , d f between the embedding and field hierarchies (i.e., d e − d f ≤ d max ) to determine when dual hierarchy traversal is used. We empirically established d max = 4 as a suitable threshold. Whenever we forego a dual-hierarchy traversal, we only construct the embedding hierarchy, and depth-first traverse it for the entire field in O(F log N) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We first evaluate specific choices of our method, and afterwards compare against state-of-the-art solutions in terms of computational cost and embedding quality. All experiments run on a particular dataset use the same configuration and parameters. Further, all experiments are conducted on a single machine with an Intel Core i7-9900 (16 logical threads @3.1 GHz), 16 GB of DDR4 RAM and a GeForce RTX 2080 Ti GPU with access to 11 GB VRAM. For each experiment, we record the minimization runtime and resulting KL-divergence as a direct measure of how far a specific minimization has progressed. As KL-divergence is directly coupled to minimization, we additionally consider an unrelated metric, selecting Nearest-Neighbourhood-Preservation (NNP) as described by Venna et al. <ref type="bibr" target="#b25">[26]</ref>. It measures how well local neighbourhoods in the low-dimensional embedding preserve characteristics of their high-dimensional counterparts. In order to obtain correct results for NNP, the gradient descent must be (mostly) converged. Hence, we use a larger number of iterations for larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We select four datasets that are frequently applied in the evaluation of dimensionality reduction algorithms such as t-SNE. As different datasets typically differ in size, dimensionality, and structure, we select Table <ref type="table">1</ref>: Time and space complexities for the different stages of our method compared to other methods. N is input size, K is restricted neighbourhood size, and F is field size used by our method and L-SNE <ref type="bibr" target="#b19">[20]</ref>. N int and p are parameters of FIt-SNE <ref type="bibr" target="#b10">[11]</ref>: N int represents a discrete grid size, and p represents the number of equispaced points over parts of said grid. Note that F, N int and p are independent of N.    Rel. 3D runtime (%) Rel. 3D KL-div. (%) Fig. <ref type="figure">7</ref>: Comparison of a BVH with/without refitting and a quad-/octree as a spatial hierarchy over 2D/3D embeddings of the MNIST and ImageNet datasets. We minimize for increasing numbers of iterations and plot the resulting relative runtime and KL-divergence. Baseline results are established with GPGPU linear complexity t-SNE <ref type="bibr" target="#b19">[20]</ref>.</p><formula xml:id="formula_29">F rep lookup * O(N) O(F + N) F rep lookup * O(N) O(F + N) F rep computation O(pN) O(pN int + N) F rep computation O(N log N) O(N)</formula><p>separate iterations and perplexity μ for each dataset. Specific sizes and parameters selected for each dataset are displayed in Table <ref type="table" target="#tab_2">2</ref>.</p><p>The commonly-used MNIST dataset consists of labeled 28 × 28px grayscale images of handwritten digits, each represented as a vector storing an image's pixel values. MNIST is often used for this kind of evaluation as it contains 10 clearly-defined classes corresponding to 10 different digits. The similar Fashion-MNIST <ref type="bibr" target="#b26">[27]</ref> contains images of 10 different types of clothing, instead of digits, which are sometimes closely related but harder to separate into clusters with an algorithm such as t-SNE. For this reason, we included it in our evaluation.</p><p>The ImageNet dataset <ref type="bibr" target="#b2">[3]</ref> stores approximately 1000 categories of random images of objects at varying resolutions. We use a reduced and formatted version previously published by Fu et al. <ref type="bibr" target="#b3">[4]</ref>, processed such that each vector in the dataset has a dimensionality of 128.</p><p>The GoogleNews dataset stores a collection of three million words, each represented as a vector generated by Word2Vec <ref type="bibr" target="#b14">[15]</ref>. This tool consumes a text corpus -in this case originating from Google News -and assigns words in the corpus a representative vector in such a way that words are closely related if they share a similar context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Hierarchy Evaluation</head><p>We first evaluate our choice of spatial hierarchy. Although our method works with different hierarchies, we focus on the implicit linear BVH <ref type="bibr" target="#b9">[10]</ref>. Use of alternatives such as a quad-/octree is possible. How- Fig. <ref type="figure">8</ref>: Comparison of our modified Barnes-Hut approximation and the regular form for generation of 2D/3D embeddings over the MNIST and Imagenet datasets. We minimize for differing θ ∈ [0.2, 0.6] and plot the resulting relative runtime and KL-divergence. Baseline results are established with GPGPU linear complexity t-SNE <ref type="bibr" target="#b19">[20]</ref>.</p><p>ever, BVHs have several benefits: they fit the contained data closely, and their bounding volumes can be refitted when data changes. Refitting instead of rebuilding provides a significant reduction in runtime over consecutive iterations. We compare minimizations of MNIST and ImageNet in four cases: using quad-/octrees, using a BVH rebuilt every iteration, and using BVHs that are rebuilt after four or eight iterations of refitting. No refitting is performed in the first 250 iterations as early exaggeration takes place. Results are displayed in Fig. <ref type="figure">7</ref>. The quad-/octree has to be rebuilt every iteration. It only matches the BVH performance when the latter is always rebuilt. The benefit of the BVH becomes apparent when refitting is used, e.g., during four iterations. However, this degrades BVH quality, and refitting for too many iterations results in unpredictable runtimes. This is seen in the ImageNet minimization for 8 iterations of refitting, where the embedding still undergoes significant changes after the early exaggeration phase. Here, refitting degrades the hierarchy quality. We show iteration runtimes in Fig. <ref type="figure" target="#fig_5">12</ref>, where these effects are visible. In practice, we employ four consecutive iterations in all other examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Barnes-Hut Evaluation</head><p>Next, we evaluate our modified Barnes-Hut approximation criterion in conjunction with a BVH. This criterion handles irregular bounding volumes, which occur in a BVH, better than the original. We compare minimizations of MNIST and ImageNet with our criterion and the  <ref type="bibr" target="#b19">[20]</ref>, a CUDA-SNE <ref type="bibr" target="#b1">[2]</ref> implementation of FIt-SNE <ref type="bibr" target="#b10">[11]</ref>, and our method (DH-SNE). The first row shows minimization runtimes for 2D embeddings over increasingly large subsets of data. The second row repeats this experiment for 3D. The third row shows evolution of KL-divergence over the same subsets. Horizontal axes for the first three rows are logarithmic. The fourth row shows NNP in the form of precision/recall curves. Our method outperforms the state-of-the-art on large datasets in terms of runtime for 2D and 3D. In addition, it retains a similar quality to linear tSNE <ref type="bibr" target="#b19">[20]</ref> in terms of KL-divergence and NNP. 3D embeddings are consistently of higher quality.  Larger θ leads to a coarser approximation and faster traversal as nodes are culled earlier. While this parameter was evaluated in BH-SNE <ref type="bibr" target="#b23">[24]</ref> in the context of single-hierarchy traversal, the established θ ≤ 0.5 does not hold for our method. In addition, the parameter's impact on traversal may vary across 2D/3D embeddings. We investigate its effect in both scenarios in Fig. <ref type="figure">9</ref>. We consider θ = 0.25 a good tradeoff for 2D, and θ = 0.4 for 3D. There is a noticeable increase in KL-divergence for larger θ across all datasets, which becomes visible as grid-like patterns. The supplemental material shows generated 2D/3D embeddings for different values of θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparative Evaluation</head><p>Finally, we compare with state-of-the-art techniques with linear runtime complexity. First, we select the field-based L-SNE developed by Pezotti et al <ref type="bibr" target="#b19">[20]</ref>, with which we generate both 2D/3D comparisons. This technique shows excellent performance on smaller datasets and provides high quality embeddings in comparison with earlier techniques. We use field scalings of 2.0 (2D) and 1.2 (3D) for measurement with both our method and L-SNE. We also select a current version of CUDA-SNE <ref type="bibr" target="#b1">[2]</ref> which, instead of a Barnes-Hut approximation, recently adapted the O(N) FIt-SNE <ref type="bibr" target="#b10">[11]</ref> to the GPU. Although their approach incurs an overhead for smaller datasets, it outperforms the original implementation due to a linear runtime. This implementation only generates 2D embeddings, so we only compare it in this regard. Older BH-SNE <ref type="bibr" target="#b23">[24]</ref> or baseline O(N 2 ) t-SNE algorithms <ref type="bibr" target="#b24">[25]</ref> have been omitted, as their practical performance is typically orders of magnitude slower.</p><p>As our technique uses a field similar to Pezotti et al. <ref type="bibr" target="#b19">[20]</ref>, we expect to produce similar embeddings at improved runtime performance. With regards to FIt-SNE <ref type="bibr" target="#b10">[11]</ref>, we expect to reach similar or improved performance on large datasets, while producing substantially different embeddings, as our minimization differs from theirs by definition. To correctly compare the different minimization methods, we ensure they use identical KNN information and an identical joint similarity distribution P. We further ensure all methods use an identical initial embedding, and use identical parameters for their gradient descent. Differences between methods then correspond solely to the differences in their respective complexities. We provide an overview of the different time/space complexities of each method in Table <ref type="table">1</ref>.</p><p>The first two rows of Fig. <ref type="figure">10</ref> show minimization runtimes for 2D/3D embeddings separately. We run on increasingly large subsets of the datasets to show how minimization progresses. A logarithmic scale is used on horizontal axes to account for large dataset sizes. We list exact runtimes of minimizations on the full dataset in Table <ref type="table" target="#tab_3">3</ref>.  <ref type="table" target="#tab_2">2</ref>), generated by our method.</p><p>Our technique performs exceedingly well for sufficiently large datasets; starting at approximately 27K points (indicated by a dotted vertical line) it outperforms compared methods in both the relatively small MNIST and Fashion datasets. On the full datasets, a 1000 iteration minimization requires 1.36s, compared to 1.70s for L-SNE <ref type="bibr" target="#b19">[20]</ref>. This gap widens significantly in the 1.2M point ImageNet dataset, where our method completes a 4000 iteration minimization in 51.10s, down from 346.94s. On the 3M point Word2Vec dataset, FIt-SNE <ref type="bibr" target="#b10">[11]</ref> performs a 4000 iteration minimization in 94.90s, while our method requires 86.90s. Convergence between the methods on the Word2Vec dataset is explained by attractive-force computations (Equation <ref type="formula" target="#formula_10">11</ref>), which become exceedingly expensive for denser local neighbourhoods. For comparison: both existing methods perform relatively poorly on the smaller ImageNet dataset, where a higher perplexity value leads to larger neighbourhoods. To confirm this, we display runtimes of separate components in our method in Fig. <ref type="figure">11</ref>. Evidently, attractive-force computation becomes a dominating factor in the minimization.</p><p>Observed scaling for 3D embeddings remains linear in all experiments, though there is a runtime overhead compared to 2D embeddings. This is expected, given the computational overhead involved in a third dimension. Linear complexity tSNE <ref type="bibr" target="#b19">[20]</ref> is impractical for large datasets, as runtime spikes around 100K points, while our technique is orders of magnitude faster and completes a full 4000 iteration minimization on the 3M point Word2Vec dataset in 238.67s.</p><p>While our technique improves runtime, embedding quality is another important metric. In the last two rows of Fig. <ref type="figure">10</ref>, we examine KLdivergence of generated embeddings for increasingly large subsets of the datasets, in addition to computed NNP. The NNP metric is displayed in the form of precision/recall plots. For this, we repeat an experiment performed by Pezotti et al. <ref type="bibr" target="#b19">[20]</ref>. For each point in a dataset, we observe points in an increasingly large neighbourhood of a size k based on perplexity. For every value from k = 1 to k = 3μ, we compute T , defined as the accurate number of points belonging to both points' neighbourhoods. Precision is computed as T /k and recall is computed as T /(3μ). By averaging generated curves for each point, a representative curve is obtained for the entire dataset.</p><p>As demonstrated, our approximation has a minor impact on embedding quality compared to linear complexity tSNE <ref type="bibr" target="#b19">[20]</ref>. The CUDA-SNE <ref type="bibr" target="#b1">[2]</ref> implementation of FIt-SNE <ref type="bibr" target="#b10">[11]</ref> interestingly delivers a lower quality of embeddings for the specified metrics. As explored by Linderman et al. <ref type="bibr" target="#b10">[11]</ref>, FIt-SNE reaches comparable levels of quality to BH-SNE <ref type="bibr" target="#b23">[24]</ref>, so these results are expected. The field-based approximation used by Pezotti et al. <ref type="bibr" target="#b19">[20]</ref> is established to be more accurate, which is a quality our method mostly retains. We display embeddings generated with our method in Fig. <ref type="figure" target="#fig_6">13</ref>. Further, we show example minimizations in a supplemental video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have presented a novel and improved minimization algorithm for t-SNE, providing significant performance improvements above the state-of-the-art, especially for large datasets and higher dimensional embeddings. The latter point is a crucial step forward, as it can improve embedding quality and could be of high relevance in many applications relying on a 3D visualization. For this reason, we have made an implementation of our method available (Sect. 5).</p><p>Our method illustrates that a field-based formulation of t-SNE, previously shown to have linear runtime, can still be significantly accelerated via a dual-hierarchy traversal. This allows us to compute N-body interactions efficiently, as is demonstrated in a GPGPU-based environment on modern graphics hardware. Our experiments reveal significant run-time improvements with regards to linear complexity t-SNE <ref type="bibr" target="#b19">[20]</ref> and FIt-SNE <ref type="bibr" target="#b1">[2]</ref> for two-and three-dimensional embeddings, while achieving comparable quality.</p><p>With these improvements, the t-SNE algorithm is, at this stage, dominated by the required KNN and attractive force computations, which are interesting challenges for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Simple modification to Barnes-Hut approximation [1]. Instead of constant radii based on bounding box diagonals, we reproject diagonals, handling irregular bounding boxes regardless of their respective positions to each other.</figDesc><graphic coords="4,344.39,185.81,89.42,89.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: The field hierarchy is ascended from leaf nodes (L0) and intermediate values in consecutive smaller levels (L1, L2, . . . ) are added, resulting in an approximate field. A sparse vector field is visualized, red and green colors marking x-and y-directions of the vectors.</figDesc><graphic coords="4,307.55,185.81,126.26,89.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F*</head><label></label><figDesc>attr computation O(KN) O(KN) F attr computation O(KN) O(KN) F attr computation O(KN) O(KN) F attr computation O(KN) O(KN) Component has negligible computational runtime (Fig. 11). ** Can be performed in O(F) time (Sect. 4.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Fig.9: Evaluation of θ for generation of 2D/3D embeddings. We show the resulting relative runtime and KL-divergence of three datasets. Baseline results (θ = 0) are 100% and are established with GPGPU linear complexity t-SNE<ref type="bibr" target="#b19">[20]</ref>. In larger datasets and small θ &lt; 0.2, 3D minimizations may exceed the memory capacity of our GPU and are not shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Influence of refitting a BVH during minimization of the Im-ageNet dataset. We show runtime per iteration for all 4000 iterations (top) and the last 250 iterations (bottom). Note the spike in runtime after early exaggeration for 8 iterations of refitting.</figDesc><graphic coords="8,307.55,49.37,142.46,135.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Embeddings of datasets (Table2), generated by our method.</figDesc><graphic coords="8,307.55,49.37,249.74,262.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The numbers of points and dimensions, number of minimization iterations, and selected perplexity μ for each dataset.</figDesc><table><row><cell>Dataset</cell><cell>Points</cell><cell>Dims.</cell><cell>Iters.</cell><cell>μ</cell></row><row><cell>MNIST</cell><cell>60000</cell><cell>784</cell><cell>1000</cell><cell>50</cell></row><row><cell>Fashion</cell><cell>60000</cell><cell>784</cell><cell>1000</cell><cell>50</cell></row><row><cell>ImageNet</cell><cell>1250000</cell><cell>128</cell><cell>4000</cell><cell>10</cell></row><row><cell>Word2Vec</cell><cell>3000000</cell><cell>300</cell><cell>4000</cell><cell>5</cell></row><row><cell></cell><cell>Quad-/Octree</cell><cell>BVH</cell><cell>BVH, 4 iters</cell><cell>BVH, 8 iters</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Compared runtimes of different methods for a 2D minimization using the full datasets listed in Table2.</figDesc><table><row><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell>CUDA-SNE</cell><cell></cell><cell>L-SNE</cell><cell>Ours</cell></row><row><cell></cell><cell cols="2">MNIST</cell><cell></cell><cell>5.86s</cell><cell></cell><cell>1.70s</cell><cell>1.36s</cell></row><row><cell></cell><cell cols="2">Fashion</cell><cell></cell><cell>5.74s</cell><cell></cell><cell>1.83s</cell><cell>1.40s</cell></row><row><cell></cell><cell cols="2">ImageNet</cell><cell></cell><cell>94.12s</cell><cell></cell><cell>346.94s</cell><cell>51.10s</cell></row><row><cell></cell><cell cols="2">Word2Vec</cell><cell></cell><cell>94.90s</cell><cell></cell><cell>212.11s</cell><cell>86.90s</cell></row><row><cell></cell><cell></cell><cell cols="2">MNIST</cell><cell></cell><cell></cell><cell cols="2">ImageNet</cell></row><row><cell></cell><cell>65</cell><cell cols="2">70 2D Relative runtime (%) 75</cell><cell>80</cell><cell>10</cell><cell cols="2">11 2D Relative runtime (%)</cell><cell>12</cell></row><row><cell>2D Relative KL-div. (%)</cell><cell>100.2 100.4 100.6 100.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100 100.2 100.4 100.6 100.8</cell><cell>2D Relative KL-div. (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Regular</cell><cell></cell><cell>Modified</cell></row><row><cell>3D Relative KL-div. (%)</cell><cell>100.2 100.4 100.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100.1 100.2 100.3</cell><cell>3D Relative KL-div. (%)</cell></row><row><cell></cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell></cell><cell>5</cell><cell>10</cell><cell>15</cell></row><row><cell></cell><cell></cell><cell cols="2">3D Relative runtime (%)</cell><cell></cell><cell></cell><cell cols="2">3D Relative runtime (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Comparison of the runtimes of the most expensive components of our method on three datasets of varying sizes, for 2D/3D embeddings. The different perplexity values listed in Table2imply different local neighbourhood sizes, leading to varying attractive force computation costs (Equation11). As demonstrated, runtime is dominated by KNN and attractive force computations on larger datasets.</figDesc><table><row><cell>0</cell><cell>1,000</cell><cell>2,000</cell><cell></cell><cell>3,000</cell><cell>4,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">2D 3D Fig. 11: BVH Attractive forces Apply forces Hierarchy traversal Hierarchy constr. Similarities KNN BVH, 4 iters 0 2 • 10 −2 3 • 10 −2 0.38 0.38 0.23 0.38 0.4 0.31 0.31</cell><cell>1</cell><cell>2 Runtime (s) 1.5 MNIST BVH, 8 iters</cell><cell>3 2.84 2.86</cell><cell>0</cell><cell>0.48 0.97 1.66 3.36 3.16 2.09 2.19</cell><cell>2 0</cell><cell>4 0 37.1 Runtime (s) 21.91 22.53 22.41 ImageNet</cell><cell>60 56.44</cell><cell>0</cell><cell>1.31 2.58 36.25 44.24 1.43 32.86 7.16 23.11 0.33 0.33</cell><cell>200</cell><cell>400 Runtime (s) Word2Vec</cell><cell>600</cell><cell>800 666.8 663.21</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A hierarchical O(N log N) force-calculation algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hut</surname></persName>
		</author>
		<idno type="DOI">10.1038/324446a0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="page" from="446" to="449" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GPU-Accelerated T-SNE and its applications to modern data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><surname>T-Sne-Cuda</surname></persName>
		</author>
		<idno type="DOI">10.1109/CAHPC.2018.8645912</idno>
	</analytic>
	<monogr>
		<title level="m">30th International Symposium Computer Architecture and High Performance Computing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="330" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AtSNE: Efficient and robust visualization on GPU through hierarchical optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330834</idno>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simpler and faster HLBVH with work queues</title>
		<author>
			<persName><forename type="first">K</forename><surname>Garanzha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pantaleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallister</surname></persName>
		</author>
		<idno type="DOI">10.1145/2018323.2018333</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics</title>
				<meeting>the ACM SIGGRAPH Symposium on High Performance Graphics</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2019.2921572</idno>
		<idno type="arXiv">arXiv:1702.08734</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Big Data</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PixelSNE: Pixel-aligned stochastic neighbor embedding for efficient 2D visualization with screen-resolution precision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13418</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">UMAP does not preserve global structure any better than t-SNE when using the same initialization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kobak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Linderman</surname></persName>
		</author>
		<idno type="DOI">10.1101/2019.12.19.877522</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Information Theory and Statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Lauterbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2009.01377.x</idno>
	</analytic>
	<monogr>
		<title level="j">Fast BVH construction on GPUs. Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="375" to="384" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient algorithms for t-distributed stochastic neighborhood embedding</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rachh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Hoskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
		<idno>abs/1712.09005</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">UMAP: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality reduction and visualisation of hyperspectral ink data using t-SNE</title>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Melit</forename><surname>Devassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>George</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.forsciint.2020.110194</idno>
	</analytic>
	<monogr>
		<title level="j">Forensic Science International</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving Barnes-Hut t-SNE scalability in GPU with efficient memory access strategies</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T R</forename><surname>Pozo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M N</forename><surname>Zola</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN48605.2020.9206962</idno>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.5555/2999792.2999959</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
				<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Opening the black box: Strategies for increased user involvement in existing algorithm implementations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mühlbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gratzl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Streit</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346578</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1643" to="1652" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CUB: Cooperative primitives for CUDA C++</title>
		<ptr target="https://github.com/NVIDIA/cub" />
	</analytic>
	<monogr>
		<title level="j">NVIDIA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Accessed: 2021-06-24</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical stochastic neighbor embedding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12878</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Approximated and user steerable tSNE for progressive visual analytics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2570755</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1739" to="1752" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Linear tSNE optimization for the web</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P F</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno>abs/1805.10817</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive visual analytics: Userdriven visual exploration of in-progress analytics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Stolper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346574</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1653" to="1662" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visualization of large-scale and high-dimensional data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<idno>abs/1602.00370</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual analysis of mass cytometry data by hierarchical stochastic neighbour embedding reveals rare cell types</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Unen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reinders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Koning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Lelieveldt</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-017-01689-9</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2017</biblScope>
			<date type="published" when="1740">1740</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using tree-based algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">93</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Viualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information retrieval perspective to nonlinear dimensionality reduction for data visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Venna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nybo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aidos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="451" to="490" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>abs/1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data structures and algorithms for nearest neighbor search in general metric spaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
				<meeting>the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
