<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ehsan</forename><forename type="middle">Jahangirzadeh</forename><surname>Soure</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Kuang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mingming</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zhao</surname></persName>
						</author>
						<title level="a" type="main">CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">621B03021EC6F7263A715A3E7371AE60</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>User experience</term>
					<term>usability problems</term>
					<term>think-aloud</term>
					<term>video analysis</term>
					<term>machine learning</term>
					<term>visual analytics</term>
					<term>collaboration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. CoUX is a collaborative visual analytics tool to support multiple UX evaluators with analyzing think-aloud usability test recordings.</p><p>From an input video, a video analysis engine extracts various types of features, which are stored on a back-end and presented on a front-end visual interface to facilitate the identification of usability problems among UX evaluators. Moreover, the front-end, consisting of three interactively coordinated panels, communicates with the back-end to support individual problem logging and annotation as well as collaboration amongst a team of UX evaluators.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Digital products have become increasingly feature-rich and often require users to navigate through an ever-growing number of onscreen elements, such as pressing a sequence of buttons to place an order on a smartphone. The increasing complexity of digital interfaces makes it challenging to achieve compelling user experience (UX). UX professionals often need to work collaboratively to identify and resolve UX problems via in-depth user evaluations. Of many evaluation approaches, usability testing with think-aloud protocol is widely used <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref> and considered as the single most useful method <ref type="bibr" target="#b41">[42]</ref>. When using thinkaloud protocols, participants verbalize their thoughts while performing actions. This allows UX evaluators to gain insights into their thought processes that is inaccessible to mere observations <ref type="bibr" target="#b33">[34]</ref>.</p><p>• Ehsan Jahangirzadeh Soure and Jian Zhao are with the University of Waterloo. E-mails: {ejahangi,jianzhao}@uwaterloo.ca. Despite being useful, analyzing recorded think-aloud videos is tedious, challenging, and time-consuming <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b43">44]</ref>. First, UX evaluators need to make decisions by attending to multiple behavioral signals in both the visual and audio channels and conducting multiple tasks simultaneously in a fast pace <ref type="bibr" target="#b5">[6]</ref>, such as observing participants' actions, listening to their verbalized thoughts, inferring usability problems, and taking notes. Moreover, to increase the reliability and completeness of the analysis, UX evaluators are recommended to work collaboratively <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> to overcome the evaluator effect <ref type="bibr" target="#b23">[24]</ref>-the fact that different UX evaluators may uncover or interpret usability problems differently. Unfortunately, fewer than 30% of UX evaluators have a chance to collaboratively analyze the same usability test session due to practical constraints (e.g., limited company resources <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>).</p><p>To mitigate these challenges, we propose a collaborative visual analytics tool, CoUX, to assist a team of UX evaluators with identifying, discussing and consolidating usability problems in think-aloud usability test videos for digital products. Our approach is partially inspired by recent studies extracting acoustic and textual features (e.g., loudness, pitches, and sentiment) from a video to help identify usability problems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. We further leverage various machine learning techniques to detect acoustic and textual features directly from the audio (without manual transcripts), as well as user interactions (e.g., scrolling speed and scene breaks) from the video frames. To better support UX evaluators' decision making, CoUX segments a video into meaningful chunks based on the semantics exhibited in the think-aloud audio, extracts various visual, acoustic, and textual features, and visual-izes the information collectively on multiple synchronized timelines. This design allows UX evaluators to easily attend to multiple streams of information likely indicating problems, to discover problems that might be otherwise overlooked, and make informed decisions about the occurrence and severity of the problems.</p><p>More importantly, CoUX is empowered with a collaborative decision support for discussing and consolidating analysis results among multiple UX evaluators. We draw on insights from studies of collaboration amongst UX evaluators and collaborative visualization (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b62">[64]</ref><ref type="bibr" target="#b63">[65]</ref><ref type="bibr" target="#b64">[66]</ref><ref type="bibr" target="#b67">69]</ref>). CoUX allows UX evaluators to analyze a video independently, and then enter a collaborative mode to discuss and summarize their analyses, minimizing the evaluator effect <ref type="bibr" target="#b23">[24]</ref>. In independent analysis, detected usability problems and their severity levels, as well as UX evaluators' reasoning, are automatically organized in a chat box like interface. During collaboration, UX evaluators are enabled with interactive and visual support from CoUX to make decisions collaboratively by discussing their findings in structured conversational threads and consolidating the results, synchronously or asynchronously.</p><p>Our design of CoUX is grounded in design considerations derived from the literature and our interviews with two UX professionals. For evaluation, we conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX helped improve the completeness and reliability of their analyses with an effective support for discovering, discussing, and consolidating UX problems. CoUX allowed them to spot problems that they might otherwise have neglected, and encouraged focused conversations to seek clarification from and respond to their partners.</p><p>In summary, we make the following contributions: (1) a video analysis pipeline that extracts multiple acoustic, textual, and visual features from a think-aloud recording to facilitate UX problem identification;</p><p>(2) a visual analytics tool, CoUX, that supports problem identification, annotation, and collaboration for UX evaluators in an integrated environment; (3) Insights into the results of a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is inspired and informed by related work in three areas: usability testing analysis, machine learning for user experience research, and collaborative visual analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Usability Test Analysis Tools</head><p>Numerous commercial tools have been developed to support UX evaluators with conducting usability test and reviewing test session data. The first category is offline tools that need to be installed on local machines, such as Morae [62], Noldus Viso <ref type="bibr" target="#b42">[43]</ref>, and Silverback <ref type="bibr" target="#b57">[58]</ref>. These tools allow UX evaluators to review sessions with functionalities like note-taking and marking events on the video progress bar, on top of basic usability test support such as screen recording, survey administration, and results exporting. However, many offline applications have been retired due to the emerging trend of remote and online user testing platforms <ref type="bibr" target="#b37">[38]</ref>. These online platforms allow for more flexible collaboration, such as UserTesting.com <ref type="bibr" target="#b61">[63]</ref> and FullStory <ref type="bibr" target="#b16">[17]</ref>. While these tools support a range of user testing and analysis functions, their data analysis capabilities are limited to session playback, note-taking, tagging, and mouse point clouds. In contrast, we design CoUX to meet the increasing demand for online, remote, and collaborative tools that support usability test session review with advanced analysis support.</p><p>In the research community, several prototypes have been developed to facilitate UX problem identification. Usability Problem Inspector <ref type="bibr" target="#b0">[1]</ref> was designed for UX evaluators to inspect a test session on the fly and was shown to be effective at helping evaluators find important usability problems in an interface design. However, to better understand the user's behavior and interactions, UX evaluators often have to repeatedly review the usability test recording to pinpoint the problems. Skov and Stage conducted an empirical study of a conceptual tool to demonstrate its usefulness for problem identification with a group of usability evaluators <ref type="bibr" target="#b58">[59]</ref>. VA2 <ref type="bibr" target="#b3">[4]</ref> supports evaluation session analysis by combining multiple sources of information including interaction logs, think-aloud speech, and eye-tracking data. However, unlike CoUX, collaborative features and online remote access are not explored. Several other visual analytics tools support better understanding of users' behaviors based on large interaction logs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49]</ref>. However, none of them focus on reviewing think-aloud recordings.</p><p>In sum, the above tools primarily provide basic functions for analyzing the content of a test session, such as playback, note-taking, tagging, and some user interaction visualization (e.g., click heatmap), and offer limited collaborative features, such as sharing notes or clips. In contrast, CoUX adopts computational methods to extract rich features from the audio, transcript, and video content of the test session and visualizes these features as auxiliary information to better inform the analysis process. Additionally, CoUX considers the specific collaboration needs among UX evaluators such as discussing and resolving conflicts in detecting UX problems and rating the problem severity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AI-Assisted UX Data Analysis</head><p>Recently, researchers began to leverage artificial intelligence (AI) to assess the usability of digital interfaces <ref type="bibr" target="#b46">[47]</ref> and detect UX problems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref>. For example, user interaction events were utilized to create machine learning (ML) classifiers to detect usability issues of websites <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b47">48]</ref> and virtual reality applications <ref type="bibr" target="#b21">[22]</ref>. In addition, user interaction paths were compared to construct graph-based AI models to detect potential UX problems <ref type="bibr" target="#b28">[29]</ref>. Although these automatic methods were promising, they were primarily based on users' interaction logs, which only indirectly reflect some aspects of UX problems and lack a true understanding of the UX problems. In contrast, UX evaluators tend to use multi-modal information from both the acoustic and visual channels of a test session to pinpoint and interpret problems <ref type="bibr" target="#b5">[6]</ref>.</p><p>To address the limitations of AI, VisTA is equipped with AI as an assistant by detecting and highlighting video segments containing potential UX problems <ref type="bibr" target="#b12">[13]</ref>. It extracts features such as negative segments and abnormal pitches, which are indicators of UX problems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. We employ a similar philosophy to overcome the constraints of AI. We further extract the speech, textual, and visual features from think-aloud usability test recordings and present them to UX evaluators to assist with their analysis in CoUX. Moreover, we take a step further to extract additional features from the video such as scrolling speed.</p><p>Unlike VisTA that is designed to support a single UX evaluator, CoUX is able to support both individual analysis and collaboration among UX evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Collaborative Visual Analysis Tools</head><p>One critical challenge for UX problem detection is the vague evaluation procedures, which can lead to bias or unclear problem criteria <ref type="bibr" target="#b23">[24]</ref>. Thus, different UX evaluators could detect different sets of problems when assessing the same interface, known as the evaluator effect <ref type="bibr" target="#b23">[24]</ref>. Most evaluators perceive this effect when merging their individual findings with teams <ref type="bibr" target="#b24">[25]</ref>. Thus, collaboration and involvement amongst UX evaluators are integral to both increasing the reliability <ref type="bibr" target="#b23">[24]</ref> and improving completeness of the problems identified <ref type="bibr" target="#b55">[56]</ref>. However, few systems have been developed to adequately support collaborative analysis of usability test sessions. When designing CoUX, we strive to support UX evaluators' collaboration for detecting problems, annotating or assessing problem severity using usability heuristics <ref type="bibr" target="#b40">[41]</ref>, and initiating discussion in one integrated environment.</p><p>Moreover, the design of CoUX draws on insights from both colocated (e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>) and distributed (e.g., <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">[64]</ref><ref type="bibr" target="#b63">[65]</ref><ref type="bibr" target="#b64">[66]</ref><ref type="bibr" target="#b67">69]</ref>) collaborative visualization tools, while these tools do not focus on analyzing think-aloud sessions. In particular, we are inspired by prior work on the support of coordination and synthesis in collaborative analysis activities. Robinson explored the co-located synthesis of findings from paired participants after each had completed an asynchronous individual analysis phase <ref type="bibr" target="#b50">[51]</ref>. They found that establishing common ground and role assignment are critical aspects of collaborative synthesis. Mahyar and Tory extended this concept to link common work within a visualization tool to support collaborative sensemaking of documents <ref type="bibr" target="#b35">[36]</ref>. CoUX follows these principles by employing both an individual and a collaborative analysis modes, further with the ability to merge problem annotations and severity ratings, helping establish common ground.</p><p>Visualizing the analysis history is another strategy for coordination and synthesis, especially in asynchronous collaboration. <ref type="bibr">Sarvghad et al.</ref> found that collaborative data analysis can benefit from displaying data dimension coverage of history <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>. Similarly, KTGraph highlights of previously investigated data in a graph visualization to support collaboration <ref type="bibr" target="#b67">[69]</ref>. CoUX supports coordination by showing previously annotated UX problems on a video timeline. Also, visual cues of segments of the video timeline are changed based on the state of the problems identified, such as in the uninitiated or in-progress phases.</p><p>Furthermore, allowing analysts to use tags and links to organize their comments and identify others' contributions improves final analytic results <ref type="bibr" target="#b64">[66,</ref><ref type="bibr" target="#b66">68]</ref>. Accordingly, CoUX enables user-generated comments and tags for identified problems to explicitly communicate the intent, uncertainty, and progress of their discussion via conversational threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN OF COUX</head><p>Our main goal is to support UX evaluators in making decisions of usability problems and generating reliable annotations via collaboration. Towards this, we conducted 30-minute semi-structured interviews with two experts (E1 and E2) who are experienced in UX research. E1 is an assistant professor in information science at a university, whose research applies mainly qualitative methods. They complete the majority of their data analysis through Google Sheets <ref type="bibr" target="#b19">[20]</ref>. E2 is a UX researcher at a start-up company with over four years of experience practicing UX. As part of his daily job, he uses Zoom <ref type="bibr" target="#b68">[70]</ref> and Gong.io <ref type="bibr" target="#b18">[19]</ref> to conduct and analyze user evaluation sessions. The goal of these interviews was to understand the current practices and challenges of UX evaluators in analyzing video-recorded usability test sessions and assess their needs for a new collaborative decision making and video analysis tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Considerations</head><p>Based on our interview findings and prior work, we derived the design considerations for CoUX.</p><p>D1: Leverage various information about the video to enhance the robustness of problem identification. Research has indicated that users tend to verbalize their thoughts with abnormal speech features (e.g., abnormal loudness, pitch, and speech rate) when they encounter problems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>; Their verbalizations also contain more negative sentiments, questions, and verbal fillers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. Moreover, UX evaluators can identify more usability problems when these features are presented during analysis <ref type="bibr" target="#b12">[13]</ref>. When discussing her video analysis strategies, E1 said: "I do analyze the speech features but I don't have a good automatic tool to do so." E2 also mentioned that he observes "hesitation and pauses in users' speech" to decide whether they encounter a usability problem. Furthermore, UX evaluators also correlate these verbalizations with the visual content of the recordings. In an international survey of UX professionals, 95% of them believed that the user's actions (e.g. scrolling on the interface, pressing the wrong button) were helpful in identifying usability problems <ref type="bibr" target="#b11">[12]</ref>. CoUX supports these needs for determining UX problems by employing machine learning to automatically extract acoustic, textual, and visual features from the recording, which are then presented collectively on its interface.</p><p>D2: Provide an integrated environment for both video review and problem logging to ease the problem annotation. In addition to displaying useful information, it is critical to provide a seamless user interface for both video review and problem annotation. Previous studies have shown that UX evaluators often have to review recordings and take notes in separate applications, such as spreadsheets, text editors, and presentation tools <ref type="bibr" target="#b14">[15]</ref>. This finding was echoed by E1 who usually stores all the videos in a separate folder while all the analysis and coding is done on a spreadsheet. As a result, she finds that "organizing and sorting through the files has been tricky." E2 experiences a similar problem as he reviews the videos on Zoom cloud recordings but keeps his annotations in a separate document. Using separate applications leads to difficulty when trying to pinpoint specific problems during discussions. E1 said that "we don't have a way to solve timestamps so we just have to manually track it down and put it on a cell and then when we want to review it, we have to retreat to that specific segment in the video." E2 mentioned "sometimes the design lead wants to see exactly how the user reacted so I need an easier way to show her the snippet of the recording." To address these challenges, CoUX provides an integrated environment with both video reviewing and problem logging functions, allowing UX evaluators to become more organized and efficient during usability test video analysis.</p><p>D3: Support collaboration between UX evaluators with both individual and collaborative modes. UX evaluators may have their own biases and limitations when analyzing usability problems, which is known as the "evaluator effect" <ref type="bibr" target="#b23">[24]</ref>. Thus, it is important to have multiple evaluators collaborate with each other. Indeed, collaboration amongst evaluators has been found to enhance both the reliability <ref type="bibr" target="#b23">[24]</ref> and thoroughness <ref type="bibr" target="#b55">[56]</ref> of the problems identified. To serve this purpose, collaboration typically happens among two or more evaluators who first perform independent analysis of the same data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. E1 stated that she and at least one other coder would annotate the same video individually by hiding the columns on a spreadsheet. E2 also described reviewing the video individually at first before sharing results with colleagues, which is in line with this best practices process. We aim to design CoUX by following this workflow with two modes: an individual mode for independent problem identification and a collaborative mode for problem merging, decision making, and discussion. This mitigates the confirmation bias since evaluators rely on their own judgment for initial assessments and decisions before seeing others' results.</p><p>D4: Allow for both synchronous and asynchronous communication between UX evaluators. Maintaining effective communication between UX evaluators is critical to achieve successful collaboration during the analysis of usability problems. Research has shown that the most frequent form of collaboration is short discussions at the outset of analysis <ref type="bibr" target="#b14">[15]</ref>. This was reiterated by E1: "after we finished coding, we'll highlight the disagreements and then during our meeting time we'll discuss and resolve those highlights." E2 also mentioned that he discusses the results with the team in short meetings after the session. This type of synchronous communication should be supported by CoUX, e.g., with an instant messaging feature. Further, in the event that a synchronous meeting is not possible, which is not uncommon in practice, E1 and her collaborators would leave comments on the spreadsheet and tag the other person. Thus, asynchronous communication should also be supported to allow the messages to be viewed and discussed at a later time. Thus, we aim to adopt a similar workflow where UX evaluators can discuss and decide both synchronously and asynchronously using comments in a thread and consolidate their opinions using interactive visual support from CoUX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COUX SYSTEM 4.1 System Overview</head><p>We developed the CoUX system based on the aforementioned design considerations. As shown in Fig. <ref type="figure" target="#fig_3">1</ref>, CoUX consists of a back-end storage &amp; processing and a front-end visual interface, both of which require data extracted from a video analysis engine.</p><p>The video analysis engine contains three modules for extracting different types of features from the session recording, including the Acoustic, Textual, and Visual Analyzers (D1). The outputs of the video analysis engine are uploaded into the Session Data &amp; Features storage hosted in the back-end. The back-end also contains the Problem Annotations and Interaction &amp; Collaboration storage. The Problem Annotations storage saves all the inputs from UX evaluators regarding the usability problems, while the Interaction &amp; Collaboration storage supports all the actions that the UX evaluators perform in the front-end.</p><p>The front-end is composed of three interactively coordinated views: the Video Player, Feature Panel, and Problem Panel. The Video Player allows UX evaluators to play, pause, and rewind the session recording, as well as view a timeline of their annotations above the video progress bar. The Feature Panel presents all the extracted features and highlights the ones that correspond to the current timestamp of the video. Lastly, the Problem Panel allows UX evaluators to enter descriptions of problems that they identified, the design heuristics or principles violated (e.g., Nielsen's heuristics <ref type="bibr" target="#b38">[39]</ref>, Norman's principles <ref type="bibr" target="#b44">[45]</ref>), custom tags, and their severity ratings <ref type="bibr" target="#b40">[41]</ref>. The interface includes a toggle for UX evaluators to switch between individual and collaboration modes (D3).</p><p>In the individual mode, the Problem Panel displays the comments entered by a single UX evaluator. In the collaboration mode, the Problem Panel also displays the comments of other UX evaluators and allows for both synchronous and asynchronous communication through the chat functionality (D4). The three above views together are shown on the same CoUX interface, which provides UX evaluators an integrated environment for both video review and problem annotations (D2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Video Analysis and User Feature Extraction</head><p>To assist UX evaluators with thorough identification of usability problems, CoUX analyzes think-aloud videos by segmenting them into small meaningful chunks and extracting various features related to the user in the video (D1). The video segments are automatically detected using the Auditok library <ref type="bibr" target="#b56">[57]</ref> at periods of silence characterized by the lack of acoustic activity. By doing so, the entire long video is cut into small "bite-size" portions to facilitate UX evaluators' analysis, each of which may correspond to one or few usability problems. Each segment is then transcribed using the Google Speech Recognition API <ref type="bibr" target="#b65">[67]</ref>. The audio, transcript, and video of the segments are used to extract three main categories of user features: acoustic, textual, and visual.</p><p>• Pitch: Users tend to change their pitch when they encounter a problem while thinking aloud <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. For the corresponding audio of each segment, we computed the frequency of the speech using the "sound to speech function" in the Praat-Parselmouth library <ref type="bibr" target="#b27">[28]</ref>.</p><p>Based on the mean and the standard deviation of the pitch over the entire session, a segment is categorized as containing abnormal pitch if at least 10% of the values are over two standard deviations away from the mean. Thus, it is given one of the three values: 1 for abnormally high, 0 for normal, and -1 for abnormally low. • Loudness: Loudness has been shown as another useful speech feature for analyzing usability test sessions <ref type="bibr" target="#b6">[7]</ref>. We utilized the "sound to intensity" function in Praat-Parselmouth <ref type="bibr" target="#b27">[28]</ref> to extract the intensity of the sound (in dB). The detection of abnormalities and assigned values are the same as the pitch feature. • Speech Rate: We computed the speech rate by dividing the number of words spoken in a segment by its duration, where the number of words was counted based on the transcript. Only abnormally slow speech is detected based on prior research showing that users slow down when encountering an issue <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. Thus, each segment is labelled 1 for abnormally slow or 0 for normal. • Negations: Negations in users' think-aloud verbalizations may indicate that they encounter a usability problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. To determine if users said a negation, we applied a keyword-matching to the transcripts to detect the following words: no, not, don't, doesn't, didn't, can't and never <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. • Questions: Questions are another type of indicator for usability problems, indicating a user may be in doubt. Similar to negations, we utilized a keyword-matching algorithm containing the following words: what, which, why, how, and where <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14</ref>]. • Verbal Fillers: Verbal fillers indicate hesitations in the user's speech, which may suggest a problem. We utilized a keyword-matching algorithm containing the words: um, uh, and like <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14</ref>]. • Sentiment: The sentiment of a user's speech (e.g., positive, neutral, or negative) is another source of useful information for problem identification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b60">61]</ref>. We used the Valence Aware Dictionary and Sentiment Reasoner library <ref type="bibr" target="#b25">[26]</ref> to detect the sentiment based on the transcripts for each video segment. Based on the compound score (between −1 and 1), a segment is labelled as positive ((0.2, 1]), negative ([−1, −0.2)), or neutral ([−0.2, 0.2]). • Scrolling Speed: When using a digital product, the amount of scrolling may reflect a user's confusion. For example, frequently scrolling back and forth on a webpage could indicate that a user has difficulty in understanding the interface <ref type="bibr" target="#b2">[3]</ref>. Thus, we extracted the scrolling speed (in the amount of pixel movement per second) for each segment using the dense optical flow algorithm from OpenCV <ref type="bibr" target="#b45">[46]</ref>, resulting in a continuous time-series. • Scene Break: Frequent switching of views may also indicate that the user has difficulty locating the desired item on a digital interface <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. We used the OpenCV-based video scene detection library <ref type="bibr" target="#b4">[5]</ref>,</p><p>which performs a comparison of sequential frames in a video and detects substantial changes in content. This results in a series of timestamps of these scene breaks. These features are meant to provide extra information to help UX evaluators review think-aloud sessions of digital products and make decisions regarding usability problems. The features are selected based on our interviews and the literature as mentioned above. However, it remains an open question of whether this feature set is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CoUX User Interface</head><p>For better work organization, CoUX features a project management page showing all the videos that need to be analyzed upon logging into the system. Clicking on any video opens the main CoUX interface. This interface consists of three key components (Fig. <ref type="figure" target="#fig_0">2</ref>): (a) a Video Player for viewing the recorded think-aloud sessions, (b) a Feature Panel for displaying various extracted features based on the analysis in Sec. 4.2, and (c) a Problem Panel for logging discovered usability problems and discussing them with other UX evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Problem Identification</head><p>Effectively identifying potential UX problems is the key objective of reviewing a think-aloud video. On the left, CoUX comprises all necessary elements for problem identification based on various information extracted from the video (D1). First of all, an integrated video player (Fig. <ref type="figure" target="#fig_0">2-A</ref>) is provided to prevent any switching between different tools, which is the largest element on the screen to facilitate the video browsing. The player supports all regular functionalities like play, pause, forward, and rewind. Further, similar to the YouTube chaptered design, the player progress bar shows the automatically-generated segments (Fig. <ref type="figure" target="#fig_3">2-a1</ref>) that split the video into "bite sizes" (see Sec. 4.2). Below the player, a couple of visualizations are placed on the Feature Panel (Fig. <ref type="figure" target="#fig_0">2-B</ref>) to facilitate the use of all the extracted features while reviewing the video. CoUX distinguishes discrete and continuous features, and displays them on two sub-panels. First, discrete features (i.e., all the acoustic and textual features) are visualized in the Feature Matrix (Fig. <ref type="figure" target="#fig_3">2-b1</ref>), where rows indicate the features and columns represent the video segments. All values in the matrix are shown as icons and colors instead of text to allow UX evaluators to quickly scan and recognize the feature values that could signify a problem. For example, represent neutral, negative, and positive sentiments;</p><p>represent filler words (e.g., um, uh), negations, and questions; and represent high and low anomalies. Second, continuous features (i.e., the visual features) are shown in a Feature Chart (Fig. <ref type="figure" target="#fig_0">2-b2</ref>), where the scrolling speed is implemented as a line chart and the scene breaks are represented as vertical green lines.</p><p>These features serve as auxiliary data for the video to enhance the thoroughness of problem identification by UX evaluators. While the video is playing, CoUX dynamically highlights the corresponding segments in both the Feature Matrix and Feature Chart, with a lighter blue background. In addition, a red vertical line representing the playhead moves on the Feature Chart while the video is playing. In contrast, the column width of the Feature Matrix does not reflect the time length of each segment (instead, a fixed width). Thus, a Sankey visualization <ref type="bibr" target="#b49">[50]</ref> (Fig. <ref type="figure" target="#fig_1">2-b3</ref>) is placed between the player progress bar and the matrix to indicate the correspondence. Similarly, a red curve is shown on the Sankey to indicate the playhead. This design increases the readability and scalability; if each column width of the Feature Matrix maps exactly to the segment length, some columns could be too narrow to display any readable features whereas others could be very wide, wasting the space. Lastly, all the above visualizations are clickable, which facilitates navigation to different parts of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Problem Annotation</head><p>Once an evaluator identifies a UX problem, they can log the problem with the Problem Panel (Fig. <ref type="figure" target="#fig_0">2-C</ref>), integrated seamlessly within CoUX (D2). When an evaluator starts to type in the chatbox-like interface at the bottom of the panel (Fig. <ref type="figure" target="#fig_3">2-c1</ref>), the video automatically pauses so that they do not need to manually click the video controls. Annotations can be bound to video playtime by checking the time check  <ref type="bibr" target="#b38">[39]</ref> and Norman principles <ref type="bibr" target="#b44">[45]</ref>) and a slider for problem severity rating <ref type="bibr" target="#b40">[41]</ref>. (E) A popup panel for adding custom tags. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>box</head><p>. Evaluators can add comments or descriptions for the identified problem, and select predefined heuristic tags from a grouped dropdown list and a severity level (0-4, where 4 indicates the highest severity) <ref type="bibr" target="#b40">[41]</ref> with a slider (Fig. <ref type="figure" target="#fig_0">2-D</ref>). CoUX supports common tags including Nielsen's heuristics <ref type="bibr" target="#b38">[39]</ref> and Norman's principles <ref type="bibr" target="#b44">[45]</ref>. Moreover, evaluators can add their custom tags via a popup panel (Fig. <ref type="figure" target="#fig_0">2-E</ref>). These tags can be created within custom groups and set to either applicable to a specific video or all videos in a project.</p><p>After an annotation is submitted, CoUX adds an Annotation Card (Fig. <ref type="figure" target="#fig_0">2-c2</ref>) to the Problem Panel, which displays all the cards bound to the active video segment. Each Annotation Card shows the problem tags, severity, comments/descriptions, and corresponding evaluators. Moreover, the Annotation Timeline (Fig. <ref type="figure" target="#fig_0">2-a2</ref>) updates with a new solid Annotation Square pinned onto the video progress bar, which shows an overview of all problems with colors indicating their creators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Problem Discussion and Collaboration</head><p>CoUX supports an individual mode and a collaborative mode to mitigate the "evaluator effect" (D3). In the individual mode, an evaluator can oversee their own Annotation Cards and Squares. When switching to the collaborative mode with the mode button on the top, evaluators can navigate to each other's identified problems by simply clicking the corresponding elements and start a discussion to consolidate their annotations. Evaluators can still create new problem annotations in this collaborative mode. The discussion/consolidation is moderated via chat threads, similar to Slack <ref type="bibr" target="#b59">[60]</ref>, to support both synchronous and asynchronous collaboration (D4).</p><p>As it is possible that evaluators created different annotations about the same underlying problem during the individual mode, CoUX allows them to merge the Annotation Cards. To do so, an evaluator first clicks a card and then three buttons pop up: discuss , merge , and delete (Fig. <ref type="figure" target="#fig_1">3-A</ref>). When the "merge" button is clicked, a shaking animation highlights mergeable cards. In addition to a merged Annotation Card (Fig. <ref type="figure" target="#fig_1">3-B</ref>), a new Annotation Square is added on the Annotation Timeline while the previous squares become hollow . Currently, merging is only allowed for problems in the same video segment, but more than two cards can be merged.</p><p>Moreover, clicking the "discuss" button enters an in-situ Discussion Panel (Fig. <ref type="figure" target="#fig_1">3-D</ref>) of this Annotation Card (Fig. <ref type="figure" target="#fig_1">3-C</ref>). They can then add new comments and propose a different severity rating for the problem, or discuss the merging if applicable. New and existing comments are displayed based on their timestamps. Evaluators can also pin important comments. A thread visualization helps evaluators review all the proposed severity ratings (Fig. <ref type="figure" target="#fig_1">3-D</ref>). If an annotation has a conflict in the severity rating (i.e., more than one severity ratings are proposed), evaluators are asked to determine the final severity for the annotation; otherwise, this problem remains unresolved, with a warning icon associated with the Annotation Card. Evaluators can also add or remove heuristics by clicking on the edit button on the top of the panel. These Annotation Cards on the Problem Panel provide an informative summary about a problem. Each card shows all the tags, severity ratings, participating evaluators, and pinned comments in a carousel view (Fig. <ref type="figure" target="#fig_1">3-C</ref>). For merged cards, evaluators can also unmerge them through a button .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USER STUDY</head><p>We conducted a user study to assess the usefulness and effectiveness of CoUX in think-aloud video analysis. Specifically, our exploration was guided by: RQ1 -How does CoUX support evaluators in analyzing think-aloud sessions? RQ2 -How do teams work together and communicate during their analysis through CoUX? RQ3 -What are the general challenges in collaborative UX video analysis?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Participants and Apparatus</head><p>We recruited 12 participants (two males, nine females, and one not disclosed, aged 23-32) via social media and mailing lists. They were UX designers (N = 4), UX researchers (N = 4), and UX/HCI graduate students (N = 4). On average, they had three years of experience in UX (SD = 2.2). Eleven (91.7%) self-reported being very familiar or extremely familiar with identifying usability problems, with one participant being moderately familiar (M = 4.17, SD = 0.55). The participants were recruited in pairs. They had all collaborated with their partners before on at least one project. Seven (58.3%) were very or extremely familiar with their partner, with the rest being moderately familiar (M = 3.83, SD = 0.80).</p><p>Participants completed the study remotely with their own computers while communicating with the moderator through video-conferencing software. Participants were asked to make the application window full screen throughout the study. Participants used the largest screen available. The average display size was 20 inches (SD = 7.12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Study Videos</head><p>We collected two recorded usability test sessions in which users were instructed to use digital products with the think-aloud protocol. In the practice video (length: 3 minutes 34 seconds), a user was asked to find a photo of an instruction manual for an early telescope on a Science and Technology Museum's website. In the study video (length: 11 minutes 15 seconds), a user was asked to complete three tasks on a Food Delivery Mobile App, including: (1) find the Wegmans store on the Amherst St.;</p><p>(2) buy 10 bottles of classic Coke and 10 bottles of Sprite, and some full sheet pizzas with any topping while staying under a budget of $100; and (3) change the pick up order to delivery instead. These videos were chosen since they are representative of digital interfaces: one for a desktop website and the other for a smartphone application. There were also numerous usability issues in both videos which promoted discussions between the participants and their partners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Task and Design</head><p>Each pair of participants conducted the study together and was asked to review the study videos and identify usability problems using CoUX. There were two phases in the study session: (1) an Individual phase and (2) a Collaborative phase. In the individual phase, participants identified usability problems and submitted the annotations of these problems independently. In the CoUX interface, they could only see the problem cards that they had inputted. In the collaborative phase, the problem annotations of both partners were revealed to each other. Then, they were asked to review each other's annotations, merge cards as desired, and discuss the problems before reaching a final decision. Splitting the session into two phases was based on the recommendation  that to serve the purpose of improving reliability, collaboration should happen among two or more usability practitioners who first perform independent analysis of the same dataset <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Procedure</head><p>To begin, each pair was given a short video tutorial about CoUX.</p><p>Participants were able to ask any questions about the study and the system. They were then introduced to the usability test video review task, and instructed to assume that developers of the products will have limited time to address the problems identified in the session. This assumption resembled the fact that UX practitioners often have limited time to analyze test sessions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44]</ref> and allowed for a more realistic evaluation of the extracted features and collaboration support in CoUX.</p><p>After the tutorial, the participants completed a practice trial by first analyzing the museum video individually for five minutes, then collaborating with their partner for another five minutes. This allowed them to become familiar with the system and the full procedure of the two-phase task. In the study session, participants were first asked to identify usability problems with the food delivery app individually for 25 minutes and then filled out a short survey based on the 5-point Likert Scale, which sought to understand the usefulness of each feature and the ease of use of the annotation functionality in CoUX. After a short break, they had 15 minutes for the Collaboration phase where they discussed each other's problems and tried to consolidate them into a final set. At the end, each pair of participants independently completed another short survey about their collaboration experiences. These survey questions were based on previous findings about collaborative analysis <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b55">56]</ref>. When performing both the individual and collaborative tasks, participants were asked to communicate only within CoUX. This would allow them to fully explore and use CoUX during the study. We then conducted a semi-structured group interview to collect their feedback about the system. All the interview sessions were video-recorded, and participants' interactions with the system (e.g., clicks, video-playing behaviors) were logged. The study lasted about 90-100 minutes and participants received $25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>We first present participants' general user experience of CoUX (Sec. 6.1) and then how they used the features during their individual analysis (Sec. 6.2) and collaboration (Sec. 6.3) respectively, based on our RQs. Participant x in the study pair n is labeled as Pn-x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">General User Experience</head><p>Overall, participants felt that CoUX was a useful tool to support their analysis of a usability test video recording. Fig. <ref type="figure" target="#fig_4">4</ref> presents participants' ratings on different aspects of CoUX. They agreed that they could find all the functionalities easily in the interface to perform their tasks (Md = 4, IQR = 1). Table <ref type="table" target="#tab_1">1</ref> shows the usage statistics of the main functions of CoUX. This suggests that all functions were used by participants, in particular the extracted features, problem annotations, and chat threads, which will be explained in the following sections.</p><p>Moreover, participants appreciated that CoUX integrated analytics, collaboration, and communication features together in one integrated environment. "Usually we were using Google sheets <ref type="bibr" target="#b19">[20]</ref> to coordinate and it was getting quite difficult, because we had to follow up with another person... it was messy and difficult but right now, it seems quite easy [with CoUX]."-P2-1 Eleven (91.7%) participants agreed or strongly agreed to recommend CoUX to others (Md = 4, IQR = 0).  Participants also compared it to previous tools that they had used. "Actually I use a similar tool just like this <ref type="bibr">[Lookback [35]</ref>], but it doesn't offer any analytic information, like loudness or pitch."-P3-1 Sentiment was rated as the most useful feature (Md = 4, IQR = 1.5). One reason was that this feature was perceived to be accurate by the participants. "I feel like this is providing useful insight and... the sentiment easily got me to the areas in the video where the user was confused."-P5-1 Another reason was that "the icons and sentiments are very easy to understand."-P4-1 One (P3-2) mentioned that even though the negative sentiments often point them to areas with a usability problem ("I think 90% of them are accurate"), they were still cautious about using the neutral sentiments and relied on their own judgment for those segments. Thus, human judgment was still exercised as P6-2 pointed out: "It was showing a neutral sentiment but I thought the user was actually very frustrated... so I still relied on my own intuition."</p><p>UX keywords were generally viewed positively and rated as the second-most useful features (Md = 3, IQR = 2), and were perceived to be accurate. For example, "The UX keywords matched up with what my impression was while watching the video."-P5-1 Also, P6-1 appreciated the accuracy of the question marks, e.g., clicking on it navigated P6-1 back to the segment where the user of the app was confused.</p><p>Speech features, i.e., loudness (Md = 2.5, IQR = 1), pitch (Md = 2.5, IQR = 1), and speech rate (Md = 3, IQR = 1.5), were perceived as less reliable than sentiment and UX keywords. Participants thought that these features could be augmented with the context and other information. "The pitch was interesting, but I feel like I still have to listen to a combination of their tone and the context."-P6-2 Moreover, participants felt these features were new and needed more explanations about how they were determined. "I'm not sure I can trust this stuff because I'm not sure <ref type="bibr">[they]</ref> were based on what logic."-P1-2</p><p>Scrolling speed (Md = 3, IQR = 1.5) and scene breaks (Md = 2.5, IQR = 2.5) were appreciated by some participants who used the peaks as possible indicators of users' confusion. "I actually almost paid all of my attention on the scrolling speed. Compared to the icons, I definitely prefer to look at the visualizations."-P1-2 "I looked at the peak of the waves of the scrolling speed just to double-check what was happening."-P6-1 Also, P3-1 thought the scrolling speed was useful, but would like to see numbers instead of a relative scale from "slow" to "fast." The scene breaks were used as "indicators of task changes to skip through or go back to review further."-P6-1</p><p>However, these two features were relatively less used. One reason was that "they are too far away from the video so while I'm watching the video I couldn't see that information."-P2-2 In addition, P4-2 mentioned "I didn't really look at the scrolling speed, because I think this is not super relevant to this task."</p><p>Interestingly, participants also used a combination of different features to better locate segments that contained usability problems. "In the same column, if there were two or more icons that show abnormalities, I paid more attention to it because it's more obvious."-P1-2 The above observations confirmed the success of D1, which was to leverage various information about the video to support evaluators' analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Problem Annotation: Problem Panel</head><p>Participants used the problem annotation function of CoUX (Fig. <ref type="figure" target="#fig_1">3</ref>) extensively as they recorded the usability problems in this area. On average, participants entered 18.3 problems in this recording (SD = 7.0). Overall, participants felt that it was the most important component of the interface and entering annotations was "pretty clear and straightforward" (Md = 4, IQR = 1). "The chat box..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. is really useful because it's very clear and very easy to use."-P1-1</head><p>In addition, they liked the functionality to attach heuristics (Md = 3.5, IQR = 2.5) and severity rating (Md = 5, IQR = 1) to each problem description. "I like the heuristic function that I can select from the heuristics which are already there and I can add my own options as well, so that was also very helpful."-P2-1 However, we also received some mixed feedback about having to attach a heuristic to every annotation. "I think the heuristics are great, but I don't think it should be mandatory. I usually make notes of activities and those aren't things that I would tie to a heuristic."-P6-2 In this case, the system could be modified to allow heuristics to be optional.</p><p>Five out of the six pairs used the custom tagging function extensively as it allowed them to add tags that were "more relevant to the actual video, like here there is the older adult and accessibility issues that are more specific than Nielsen's."-P2-1 Below are some custom tags participants added: "Information Architecture", "Navigation", "Test Condition", "Older adults preferences." These custom tags were both concrete and diverse, reflecting their unique experiences and expertise.</p><p>The above feedback also demonstrates the benefits of CoUX by providing an integrated platform to assist problem annotation, as guided by D2. "We can finish the analysis and make the comments in one screen instead of using lots of applications."-P2-2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Collaborative Analysis (RQ2) 6.3.1 Effects of Collaboration</head><p>Overall, participants felt that this collaborative session was an important component of their analysis process. They felt that it allowed them to reach decisions more easily and quickly (Md = 3.5, IQR = 1), as well as more fairly and comfortably (Md = 4, IQR = 1).</p><p>Participants pointed out two main benefits from the collaboration support. First, collaboration helped them improve the completeness of their results (Md = 5, IQR = 1). As shown in Table <ref type="table" target="#tab_1">1</ref>, participants identified on average 4.2 more problems after the collaboration phase. This is 39.3% more compared to the number of problems they identified in the individual analysis. "Some of the problems I didn't recognize but she did, so her annotation reminded me that here is a problem, and I can write the feedback on it."-P1-1 Second, collaboration allowed them to improve the reliability of their results (Md = 5, IQR = 1). On average, participants commented on 6 (57.1%) problems that they had not previously identified (Table <ref type="table" target="#tab_1">1</ref>), demonstrating that they conducted a more robust analysis. "To obtain the most unbiased feedback and the most unique ideas without biasing each other, this was really helpful."-P5-1 Similarly, having more people analyze the problem allowed for different perspectives of the issues to be explored. "We actually noticed that the same area in the video has problems, but we focused on different aspects of the problem."-P3-2 One participant was unsure about the severity rating, but looking at her partner's annotation of the same problem gave her "confidence that three is a good rating for this issue and it's not too high."-P2-1 Such feedback demonstrates the successful implementation of D3, which was to support collaboration between UX evaluators with both individual and collaboration modes for the purposes of improving completeness and reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Usage of Collaboration Support</head><p>Annotation Timeline. In the collaborative phase, participants viewed and discussed each other's annotations of usability problems. They liked seeing the problem annotations of their partner (Md = 5, IQR = 1) and used the annotation timeline extensively to navigate to each annotation. "For the collaborative session, I pretty much clicked on all the boxes (annotation squares) to get what I needed."-P5-1 Participants also used the blinking annotation squares (Fig. <ref type="figure" target="#fig_0">2-a2</ref>) as indicators: "In terms of the flashing squares, I would go through and check them for new changes."-P6-2</p><p>Problem Merging. Participants used the merging function when they identified the same problem in the same segment. This allowed them to have a focused conversation regarding a certain problem in one place. On average, each pair merged 1.9 problems (SD = 1.7) in the test video (Table <ref type="table" target="#tab_1">1</ref>). "At the three-minute timestamp, there were two cards where we were exactly talking about the same issue of the <ref type="bibr">[user]</ref> tapping on 'My Cart' and it was not responsive, so I merged them together."-P6-1 Some participants mentioned that they might need to merge multiple problem annotations of the same underlying problem in different segments as one overarching usability issue.</p><p>Chat Threads. For each usability problem, participants left on average 2.8 comments (SD = 0.7) in each thread (Table <ref type="table" target="#tab_1">1</ref>). Participants utilized the chat threads (Fig. <ref type="figure" target="#fig_1">3-D</ref>) in four ways. First, they used it as a record or documentation of their discussion. Having the discussion on the same interface as the video allowed the participants to review the particular segments for the usability problems that their partners identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"We can see the process of what we discussed and what we talked about so we won't forget what we say and review the video at the same time."-P1-1 "The threads are like a place where we can document the final decision."-P3-1 "This is really useful for when we want to have a clear trail of the analysis."-P6-1</head><p>Second, participants used it as a means to seeking clarification from their partners on their usability problem descriptions. For example, P2-2 left comments like "Can you explain more about this problem?" In another case, P6-1 mentioned a usability problem about "the scrolling area is limited without clear color indication." p6-2 then utilized the thread to reply "What do you mean by this? Do you mean on screen?" In this way, they were able to sync up on the specific element of the interface that caused the problem. Third, it was utilized for consolidating the heuristics and severity rating. For example, P3-1 commented "The user was looking for pop but missed the 'drink' section, maybe the word 'drink' is not associated with the word 'pop' in her point of view" and tagged it with the "consistency and standards" heuristic. P3-2 found the same issue but offered an alternative heuristic, "This could be because there are no drink pictures on this page" and tagged it with the "visibility" heuristic. P3-2 then utilized the chat to communicate with P3-1: "I agree the severity is 2, but it is more of a visibility issue..." After P3-1 agreed, they removed "consistency" and selected the final severity rating.</p><p>Lastly, these threads were viewed as a precursor to video-call/inperson meeting. "This is good for me to quickly see what my partner and I agreed on and then we can skip those in the meeting."-P4-2</p><p>Collaboration Modes. Although participants collaborated synchronously in the study, they pointed out that the tool would also allow them to collaborate asynchronously by both leaving notes and following up with their partners' annotations in the corresponding thread whenever they had time. "A beneficial scenario would be if we're working in different countries so we can't analyze and talk in real-time."-P6-2 Further, participants mentioned additional functions to support asynchronous collaboration, such as revision history (P1-1, 2-1, 3-1) and e-mail notifications of new comments (P1-2, 2-2, 4-2).</p><p>These results confirm the support of D3 and D4 by CoUX, via enabling effective and seamless collaboration on UX problem analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Challenges (RQ3)</head><p>From this study, we also learned about the potential challenges in collaborative think-aloud video analysis, which could shed light on future research avenues.</p><p>Managing Disagreements. Although participants appreciated the support of CoUX for merging problem annotations, they pointed out that managing disagreements is a difficult task in nature because each evaluator has their own interpretations. Evaluators could disagree on whether there is a usability problem. For example, Pair 4 had a disagreement on whether a problem was actually an issue with the app interface. "[P4-1] just put she thought that this app is too much trouble, but from my understanding, it's because maybe she's not familiar with the iOS keyboard."-P4-2 They could also disagree on the severity of a usability problem. "I think collaboratively agreeing on what the final severity rating is more difficult, after this session, I still think there's problems that are still open-ended and unagreed upon."-P5-1 Similarly, Pair 1 disagreed on the interpretation of the actions of the user in the video while she was adjusting the quantity of the drinks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"So we disagreed on what the user was trying to do but agreed it is the efficiency of use problem."-P1-1</head><p>To manage these disagreements, participants usually "left comments with explanations of... why we think that that is a problem."-P2-1 Although managing disagreements could be time-consuming and difficult, participants believed that discussions, such as those in the chat threads of CoUX, could lead to more robust analysis. "Having the debates during collaboration are actually where the meat of the analysis is."-P6-1</p><p>Workspace Awareness. Although participants collaborated synchronously in the study, they sometimes worked on different portions of the video and thus missed the "real-time" element of a synchronous collaboration. "It feels like we're not on the same page, because when I work on the first card, she is probably working on the second card, so we cannot get the real-time feedback [on the same card]."-P1-1 Thus, future work should explore ways to increase workspace awareness.</p><p>Chat vs. Conference Calls. Many participants considered chat threads as a "light-weight" communication and enjoyed its flexibility that allowed them to respond whenever they had time. "I think communicating via the chat is totally fine, I would rather have a new thread for every issue."-P5-2 "I would prefer this because... I can come back to it and comment whenever I have time instead of getting on a call and having long discussions."-P2-1 However, some participants felt that using a video or voice call could be more efficient for resolving disagreements. "You'd save more time by just hopping on a quick call rather than keep typing explanations [in chat threads] over and over again."-P5-2 Additionally, some participants also suggested a combination of both depending on the depth of their conversations. "I think it would only be useful to have a call if we actually really disagreed about something and we couldn't come to a consensus in the chat."-P5-2 Thus, in collaborative interfaces, the trade-offs between light-weight chat threads and more heavy-weight video-calls need to be further explored in the future.</p><p>In this section, we discuss the key lessons and observations from our study and the limitations of our work. We also point out some design implications obtained from the study and potential future directions.</p><p>Problem Identification. CoUX visualized behavioral signals indicating usability problems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref> from the acoustic, textual, and visual information of the videos on its Feature Panel. Participants creatively used these features to become alerted about potential upcoming problems when playing the video, to skip less important portions of the video if pressed for time, and to facilitate their revisitation of the video in their second-pass analyses. These usages of the Feature Panel demonstrated the flexibility of CoUX for analyzing usability test videos with different time budgets, which is a common challenge <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>In general, participants trusted the sentiment and UX keywords more than other features, because they could intuitively draw connections to usability problems. For relatively new features (e.g., speech, scrolling speed, and scene breaks), participants did not fully trust them as they did not clearly see the underlying logic. Also, their existing experience affected their perception of the features; for example, they felt speech features could be affected by an individual's speaking behavior and thus were unreliable. These usage patterns indicate that participants actively scrutinized and interpreted the features instead of passively accepting them. This is encouraging as it suggests that CoUX supports, rather than replaces, UX practitioners' independent analysis.</p><p>Problem Annotation. Participants rated highly about being able to annotate the problems, attach UX heuristics violated, and provide severity ratings all in one integrated system. While existing commercial tools allow UX practitioners to attach problem descriptions while watching a video (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b61">63]</ref>), no such tools provide the ability to attach UX heuristics and severity ratings at the same time, which are two important pieces of information to have when analyzing usability problems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54]</ref>. Furthermore, participants enjoyed being able to create their custom tags for UX problems when they felt the standard heuristics were too generic to describe the problems accurately, which echos previous findings <ref type="bibr" target="#b29">[30]</ref>.</p><p>Collaboration. In the collaborative mode, CoUX visualizes the problems identified by their partners, which helped participants catch the missing ones and become more confident about those identified by both. Moreover, CoUX allowed them to have focused conversations regarding whether to merge different interpretations of the same problem. The chat threads supported both synchronous and asynchronous collaboration, and provided a track record of their analysis history, allowing for revisiting how they arrived at a decision. As a result, participants felt their analysis was more complete and reliable, suggesting that CoUX helps UX practitioners reduce their limitations and biases.</p><p>Our study also revealed challenges for further investigation. First, while participants felt chat threads would be sufficient in many cases to resolve their disagreements, a video/audio call could avoid backand-forth textual chats. As the video/audio call is more disruptive, it remains an open question of whether and how to integrate it into CoUX.</p><p>Second, displaying the provenance and history of analysis and data is critical to increase the team awareness and collaboration efficiency <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b67">69]</ref>. While CoUX supports this via various aspects such as the Annotation Squares, chart threads, etc., future work should investigate ways to enhance the capability of CoUX with advanced visualizations, such as graphs <ref type="bibr" target="#b66">[68,</ref><ref type="bibr" target="#b67">69]</ref> and analysis coverage <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>Third, managing redundant information is important and can be timeconsuming. To address this issue, CoUX supports problem annotation merging. Future work can employ machine learning to recommend duplicates of annotations and suggest auto-merging.</p><p>Limitations. Our research took a first step to designing an integrated analytics tool to support both individual and collaborative analysis of think-aloud videos. Although our exploratory study with six pairs of UX practitioners revealed its potential as well as the analysis patterns, a more comprehensive controlled experiment would allow us to quantitatively compare against existing tools. Potentially, we could conduct think-aloud tests of CoUX and use CoUX to analyze the test videos.</p><p>Second, while CoUX visualizes various features from the video, other behavioral signals, such as facial expressions and body language, could suggest usability problems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31]</ref>. Meanwhile, certain features, such as the scrolling speed, might only be relevant to certain products, such as mobile apps and websites. Thus, future work should consider extracting other types of behavioral signals and display only relevant features to UX practitioners based on the context. The features could be made configurable between continuous and discrete forms, which would allow UX practitioners to switch as needed.</p><p>Third, we used one think-aloud video for our exploratory study (besides one video for training), which was of a specific length and only included one type of task. For longer videos, UX evaluators may use the Feature Panel more often to navigate to segments with indicators of usability problems. They may also need to conduct their analysis in multiple rounds, resulting in more asynchronous communication. We also found potential misconceptions between the perceived usefulness of features and their performance suggested by the literature (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>). Future work should collect more usability test videos of different products and tasks to better understand the potential misconceptions. As the accuracy of the features could affect users' impressions and usage of an AI <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>, we could encode and visualize the uncertainty of a feature if its accuracy is low (e.g., using the color transparency).</p><p>Lastly, although our study only evaluated the collaboration between pairs of UX practitioners, CoUX allows three or more UX practitioners to collaborate simultaneously. It is, however, still an open question of what challenges UX practitioners might encounter when collaborating with three or more colleagues.</p><p>Design Implications. Our study generates several design implications for developing future collaborative analysis tools for UX evaluators. First, as discussed earlier, our participants had concerns about speech features and other features new to them, even though our algorithm already accounted for individual's speaking behaviors, etc. Thus, when employing machine learning to assist decision making, it is necessary to best convey the meanings of extracted features, in particular when such meanings are counter-intuitive, to UX practitioners.</p><p>Second, as UX practitioners continue to use CoUX, it will be able to accumulate their custom problem tags over time and even suggest relevant tags for them to consider. These custom tags could reflect on UX practitioners' experience and expertise, and future systems should leverage such customization and even support sharing custom tags with their colleagues to complement each other's analysis.</p><p>Third, in some cases, participants felt that it was hard to reach an agreement. As the ultimate goal of the collaboration is to expose users to different perspectives and to increase the completeness and reliability of their analysis, reaching an agreement is not always necessary <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b54">55]</ref>. The future design of such systems should explicitly signal to UX practitioners that disagreements with discussions are acceptable. Moreover, the tool must distinguish the disagreements with discussions from the disagreements without discussions, as the latter should be highlighted to encourage evaluators to engage in discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Informed by the literature and a formative study with UX experts, we designed a visual analytics tool, CoUX, to support UX practitioners in both independently analyzing a usability test video and collaborating with each other. CoUX extracts acoustic, textual, and visual features from think-aloud videos using machine learning and includes a chatboxlike interface for problem annotation and discussion among others. We conducted an exploratory user study with six pairs of UX practitioners in collaborative video analysis tasks. The results show that CoUX helped them improve the completeness and reliability of their analyses. The results also show different features allowed them to spot problems that they might otherwise have neglected and to have focused conversations to seek clarification from and respond to their partners. In sum, our work has taken a first step to creating an integrated environment to support the analysis and collaboration of usability test videos among UX practitioners and highlighted further research directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The CoUX user interface, showing a realistic study session of two UX evaluators (see Sec. 5) analyzing a think-aloud video recording of a food delivery mobile app: (A) a Video Player for viewing the video; (B) a Feature Panel for displaying various extracted features to assist the analysis; and (C) a Problem Panel for logging discovered usability problems and discussion. (D) Problem annotation via a dropdown for common heuristics tags (e.g., Nielsen heuristics<ref type="bibr" target="#b38">[39]</ref> and Norman principles<ref type="bibr" target="#b44">[45]</ref>) and a slider for problem severity rating<ref type="bibr" target="#b40">[41]</ref>. (E) A popup panel for adding custom tags.</figDesc><graphic coords="5,53.63,49.13,349.70,230.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. CoUX supports the collaboration among UX evaluators via a chat thread design: (A) merging two Annotation Cards, (B) merged results, (C) showing the conversation between a pair of UX evaluators, and (B) the Discussion Panel of the selected card.</figDesc><graphic coords="5,440.39,327.29,126.38,177.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 -</head><label>1</label><figDesc>Strongly Disagree 5 -Strongly Agree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Participants' questionnaire ratings (Likert 1-5) after the individual phase (Q1-14) and after the collaboration phase (Q15-Q20).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• Emily Kuang is with the Rochester Institute of Technology. E-mail: emily.kuang@mail.rit.edu. • Mingming Fan is with the Hong Kong University of Science and Technology</figDesc><table /><note>and the Rochester Institute of Technology. E-mail: mingmingfan@ust.hk.• † These authors contributed equally. * Corresponding authors. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Usage statistics of various functions in CoUX.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by the NSERC Discovery Grant and Mingming Fan's startup grant at HKUST.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Determining the Effectiveness of the Usability Problem Inspector: A Theory-Based Model and Tool for Finding Usability Problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hartson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williges</surname></persName>
		</author>
		<idno type="DOI">10.1518/hfes.45.3.455.27255</idno>
	</analytic>
	<monogr>
		<title level="j">Human factors</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="455" to="482" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Judging the Severity of Usability Issues on Web Sites: This Doesn&apos;t Work</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bailey</surname></persName>
		</author>
		<ptr target="https://www.usability.gov/get-involved/blog/2005/10/judging-the-severity-of-usability-issues.html" />
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using protocol analysis to evaluate the usability of a commercial web site</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benbunan-Fich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information &amp; management</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="163" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Blascheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurzhals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467871</idno>
		<title level="m">VA2: A Visual Analytics Approach for Evaluating Visual Analytics Applications. IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2016-01">Jan. 2016</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scenedetect: A cross-platform, OpenCV-based video scene detection program and Python library</title>
		<author>
			<persName><forename type="first">B</forename><surname>Castellano</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding usability practices in complex domains</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chilana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
		<idno type="DOI">10.1145/1753326.1753678</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Human Factors in Computing Systems -CHI &apos;10</title>
				<meeting>the 28th International Conference on Human Factors in Computing Systems -CHI &apos;10<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Assessing Concurrent Think-Aloud Protocol as a Usability Test Method: A Technical Communication Approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cooke</surname></persName>
		</author>
		<idno>doi: 10. 1109/TPC.2010.2052859</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Professional Communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="202" to="215" />
			<date type="published" when="2010-09">Sept. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recovering Reasoning Processes from User Interactions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stukes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ribarsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Richter Lipford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno>doi: 10. 1109/MCG.2009.49</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Usability evaluation for mobile device: a comparison of laboratory and field tests</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th conference on Human-computer interaction with mobile devices and services</title>
				<meeting>the 8th conference on Human-computer interaction with mobile devices and services</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic Detection of Usability Problem Encounters in Think-aloud Sessions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
		<idno>doi: 10.1145/ 3385732</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Concurrent Think-Aloud Verbalizations and Usability Problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3325281</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Practices and Challenges of Using Think-Aloud Protocols in Industry: An International Survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Usability Studies</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="102" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Truong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934797</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="343" to="352" />
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Older adults&apos; think-aloud verbalizations and speech features for identifying user experience problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tibdewal</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445680</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI &apos;21</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems, CHI &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis in practical usability evaluation: A survey study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Følstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-C</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornb</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208365</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th SIGCHI Conference on Human Factors in Computing Systems -CHI &apos;12</title>
				<meeting>the 30th SIGCHI Conference on Human Factors in Computing Systems -CHI &apos;12<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analysis in usability evaluations: An exploratory study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Følstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">.-C</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<idno type="DOI">10.1145/1868914.1868995</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries, NordiCHI &apos;10</title>
				<meeting>the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries, NordiCHI &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010-10">Oct. 2010</date>
			<biblScope unit="page" from="647" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">FullStory -Robust Analytics, Session Replay, Heatmaps, Dev Tools, and more</title>
		<author>
			<persName><surname>Fullstory</surname></persName>
		</author>
		<ptr target="https://www.fullstory.com/platform" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Psycholinguistics: Experiments in spontaneous speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Goldman-Eisler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><surname>Io</surname></persName>
		</author>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
		<ptr target="https://www.gong.io/" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Google Sheets: Free Online Spreadsheets for Personal Use</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://www.google.ca/sheets/about/,2021" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic detection of usability smells in web applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grigera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rivero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="129" to="148" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated usability evaluation of virtual reality applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Harms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction (TOCHI)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Voyagers and voyeurs: Supporting asynchronous collaborative visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/1435417.1435439</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="97" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Evaluator Effect: A Chilling Fact About Usability Evaluation Methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hertzum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno>doi: 10.1207/ S15327590IJHC1501 14</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="204" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What you get is what you see: Revisiting the evaluator effect in usability tests</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hertzum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno type="DOI">10.1080/0144929X.2013.783114</idno>
	</analytic>
	<monogr>
		<title level="j">Behaviour &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="162" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International AAAI Conference on Weblogs and Social Media (ICWSM-14)</title>
				<meeting>the Eighth International AAAI Conference on Weblogs and Social Media (ICWSM-14)<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collaborative brushing and linking for colocated visual analytics of document collections</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2009.01444.x</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Eurographics / IEEE -VGTC Conference on Visualization, Euro-Vis&apos;09</title>
				<meeting>the 11th Eurographics / IEEE -VGTC Conference on Visualization, Euro-Vis&apos;09<address><addrLine>Chichester, GBR</addrLine></address></meeting>
		<imprint>
			<publisher>The Eurographs Association &amp; John Wiley &amp; Sons, Ltd</publisher>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="1031" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Praat-parselmouth: Praat in Python, the Pythonic way</title>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<ptr target="https://parselmouth.readthedocs.io/en/stable/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting usability problems in mobile applications on the basis of dissimilarity in user behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>In</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">102364</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluating a Methodology to Establish Usability Heuristics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roncagliolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Inostroza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rusu</surname></persName>
		</author>
		<idno type="DOI">10.1109/SCCC.2012.14</idno>
	</analytic>
	<monogr>
		<title level="m">31st International Conference of the Chilean Computer Science Society</title>
				<imprint>
			<date type="published" when="2012-11">2012. Nov. 2012</date>
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward usability problem identification based on user emotions derived from facial expressions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Johanssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bernius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bruegge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM 4th International Workshop on Emotion Awareness in Software Engineering (SEmotion)</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Will You Accept an Imperfect AI? Exploring Designs for Adjusting End-user Expectations of AI Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kocielnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300641</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Analysis of strategies for improving and estimating the effectiveness of heuristic evaluation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Hvannberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/1028014.1028051</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Nordic Conference on Human-Computer Interaction, NordiCHI &apos;04</title>
				<meeting>the Third Nordic Conference on Human-Computer Interaction, NordiCHI &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2004-10">Oct. 2004</date>
			<biblScope unit="page" from="241" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Using the &quot;Thinking Aloud&quot; Method in Cognitive Interface Design</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lewis</surname></persName>
		</author>
		<editor>IBM T.J. Watson Research Center</editor>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Simple and powerful user research</title>
		<author>
			<persName><surname>Lookback</surname></persName>
		</author>
		<author>
			<persName><surname>Lookback</surname></persName>
		</author>
		<ptr target="https://lookback.io/" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supporting communication and coordination in collaborative sensemaking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mahyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346573</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1633" to="1642" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring Think-Alouds in Usability Testing: An International Survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPC.2011.2182569</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Professional Communication</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="19" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Remote Moderated Usability Tests: Why to Do Them</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pernice</surname></persName>
		</author>
		<ptr target="https://www.nngroup.com/articles/moderated-remote-usability-test-why/" />
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">10 Usability Heuristics for User Interface Design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<ptr target="https://www.nngroup.com/articles/ten-usability-heuristics/" />
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Enhancing the explanatory power of usability heuristics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="DOI">10.1145/191666.191729</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;94</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;94<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1994-04">Apr. 1994</date>
			<biblScope unit="page" from="152" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">how-to-rate-the-severity-of-usability-problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<ptr target="https://www.nngroup.com/articles/" />
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Severity Ratings for Usability Problems</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Thinking Aloud: The #1 Usability Tool</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<ptr target="https://www.nngroup.com/articles/thinking-aloud-the-1-usability-tool/" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Record &amp; annotate -Recording options and easy annotation</title>
		<author>
			<persName><surname>Noldus</surname></persName>
		</author>
		<ptr target="https://www.noldus.com/viso/record-annotate" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">What do usability evaluators do in practice? an explorative study of think-aloud testing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nørgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<idno type="DOI">10.1145/1142405.1142439</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Designing Interactive Systems, DIS &apos;06</title>
				<meeting>the 6th Conference on Designing Interactive Systems, DIS &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The design of everyday things: Revised and expanded edition. Basic books</title>
		<author>
			<persName><forename type="first">D</forename><surname>Norman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Optical Flow function</title>
		<author>
			<persName><surname>Opencv</surname></persName>
		</author>
		<ptr target="https://docs.opencv.org/3.4.13/d4/dee/tutorial_optical_flow.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A machine learningbased usability evaluation method for elearning systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Delen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turkyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zaim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="63" to="73" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Customizable automatic detection of bad usability smells in mobile accessed web applications</title>
		<author>
			<persName><forename type="first">F</forename><surname>Paternò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schiavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services</title>
				<meeting>the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Helping Users Recall Their Reasoning Process</title>
		<author>
			<persName><forename type="first">H</forename><surname>Richter Lipford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stukes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST.2010.5653598</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visual Analytics Science and Technology</title>
				<meeting>the IEEE Conference on Visual Analytics Science and Technology<address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interactive sankey diagrams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Riehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hanfler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Froehlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization, 2005. INFOVIS 2005</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collaborative synthesis of visual analytic results</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="DOI">10.1109/VAST.2008.4677358</idno>
	</analytic>
	<monogr>
		<title level="m">VAST&apos;08 -IEEE Symposium on Visual Analytics Science and Technology, Proceedings</title>
				<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Exploiting analysis history to support collaborative data analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarvghad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Graphics Interface Conference, GI &apos;15</title>
				<meeting>the 41st Graphics Interface Conference, GI &apos;15</meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="123" to="130" />
		</imprint>
		<respStmt>
			<orgName>CAN</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visualizing Dimension Coverage to Support Exploratory Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarvghad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mahyar</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598466</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Rating the Severity of Usability Problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sauro</surname></persName>
		</author>
		<ptr target="https://measuringu.com/rating-severity/" />
		<imprint>
			<date type="published" when="2013-07">July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">How to Assign the Severity of Usability Problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sauro</surname></persName>
		</author>
		<ptr target="https://measuringu.com/severity-ratings/" />
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Heuristic Walkthroughs: Finding the Problems Without the Noise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sears</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15327590ijhc09032</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="234" />
			<date type="published" when="1997-09">Sept. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Auditok: A module for Audio/Acoustic Activity Detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sehili</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><surname>Silverback</surname></persName>
		</author>
		<ptr target="https://silverbackapp.com/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Supporting problem identification in usability evaluations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Skov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Australia Conference on Computer-Human Interaction: Citizens Online: Considerations for Today and the Future, OZCHI &apos;05</title>
				<meeting>the 17th Australia Conference on Computer-Human Interaction: Citizens Online: Considerations for Today and the Future, OZCHI &apos;05<address><addrLine>Narrabundah, AUS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><surname>Slack</surname></persName>
		</author>
		<author>
			<persName><surname>Slack</surname></persName>
		</author>
		<ptr target="https://slack.com/" />
		<title level="m">Where work happens</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Paraverbal indicators of deception: A meta-analytic synthesis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Sporer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schwandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="421" to="446" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Usertesting: The human insight platform</title>
		<ptr target="https://www.usertesting.com/" />
		<imprint/>
	</monogr>
	<note>UserTesting</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">ManyEyes: A Site for Visualization at Internet Scale</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kriss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mckeon</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70577</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1121" to="1128" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Designing for social data analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kriss</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2006.65</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="549" to="557" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">CommentSpace: Structured support for collaborative visual analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979407</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Factors in Computing Systems (CHI)</title>
				<meeting>the Conference on Human Factors in Computing Systems (CHI)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">SpeechRecognition: Library for performing speech recognition, with support for several engines and APIs, online and offline</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Uberi)</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Annotation graphs: A graph-based visualization for meta-analysis of data based on user-authored annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Breslav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2598543</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="2017-01">Jan 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Supporting handoff in asynchronous collaborative sensemaking using knowledgetransfer graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="340" to="350" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<ptr target="https://zoom.us/" />
		<title level="m">Web Conferencing, Webinars, Screen Sharing</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
