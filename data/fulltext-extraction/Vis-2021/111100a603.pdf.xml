<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramid-based Scatterplots Sampling for Progressive and Streaming Data Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
							<email>zhangjian@sccas.cn</email>
						</author>
						<author>
							<persName><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<email>cwfu@cse.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Jean-Daniel</forename><surname>Fekete</surname></persName>
							<email>jean-daniel.fekete@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Yunhai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hudson</forename><forename type="middle">River</forename><surname>Staten Island</surname></persName>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">is with University Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LISN</orgName>
								<address>
									<region>Inria</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pyramid-based Scatterplots Sampling for Progressive and Streaming Data Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E081D29C1FD94FF1BE23FD93065013F9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Different sampling methods for presenting the "New York City TLC Trip Record" data with 2 million data points, which are partitioned into chunks, each of 100k data points. (a) The opaque scatterplot is overlaid on the New York map and rendered as (b) a transparent density map, where some major features are highlighted. (c,d,e) The top and bottom rows show the results of different sampling methods in the 9th and 10th frames, respectively, where each result has around 1K points sampled from the original data chunk. Comparing (c) reservoir sampling <ref type="bibr" target="#b28">[28]</ref>, (d) KD-tree-based sampling <ref type="bibr" target="#b10">[10]</ref>, and (e) our progressive pyramid-based sampling, we can find our method more consistent in preserving high-density areas (see the LGA and JFK airports circled in green) and low-density areas (see the Expressway interstate 678 labeled by a red rectangle), while maintaining the density difference between different regions (see the Staten Island and the west bank of the Hudson River labeled by the purple and yellow dashed boxes).</p><p>Abstract-We present a pyramid-based scatterplot sampling technique to avoid overplotting and enable progressive and streaming visualization of large data. Our technique is based on a multiresolution pyramid-based decomposition of the underlying density map and makes use of the density values in the pyramid to guide the sampling at each scale for preserving the relative data densities and outliers. We show that our technique is competitive in quality with state-of-the-art methods and runs faster by about an order of magnitude. Also, we have adapted it to deliver progressive and streaming data visualization by processing the data in chunks and updating the scatterplot areas with visible changes in the density map. A quantitative evaluation shows that our approach generates stable and faithful progressive samples that are comparable to the state-of-the-art method in preserving relative densities and superior to it in keeping outliers and stability when switching frames. We present two case studies that demonstrate the effectiveness of our approach for exploring large data. Index Terms-Scatterplots, sampling, pyramid, progressive visualization, streaming visualization, scalability, big data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visualization systems should have low latency to maintain the user's attention, and therefore to support effective data exploration. As data continues to grow in size, progressive visualization has emerged as a promising paradigm to control latency. By partitioning data into chunks, we can transfer data chunks and render a visualization step-by-step over a limited bandwidth channel, allowing analysts to explore intermediate results with a controlled latency. By doing so, computational scalability is ensured for big data visualization. This paradigm is similar to streaming data visualization <ref type="bibr" target="#b13">[13]</ref>, where the data is continuously generated and the visualization is incrementally updated.</p><p>Point-based visualizations are commonly used for showing various forms of data such as bivariate data, multidimensional projections, and points over a map; we refer to them as scatterplots for convenience. However, such scatterplots might not work effectively because, for large data, they suffer from the overplotting issue, i.e., as the data density increases, the number of visual marks overlapping increases as well, thus decreasing the readability of the visualization. Even small data with non-uniform distributions suffer from this issue, limiting the perceptual scalability of scatterplots. In contrast, continuous density fields bypass this issue and can effectively reveal patterns in highdensity regions. However, patterns in low-density regions (e.g., outliers) can become less visible <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">10]</ref>. For example, the region labeled by the purple dashed box is void in Fig. <ref type="figure">1(b)</ref>, but it has a few points in the original scatterplot (see Fig. <ref type="figure">1(a)</ref>).</p><p>Several approaches have been proposed to reduce overplotting in scatterplots, among which sampling is widely used. By carefully choosing a subset of the data for display, well-designed sampling methods <ref type="bibr" target="#b4">[5]</ref> can faithfully maintain the visual perception of relative data densities and outliers. Some recent methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">10]</ref> further attempt to preserve the relative class densities in multi-class scatterplots. All these methods, however, assume that the whole data is preloaded into memory, and thus they may not work well for progressive visualization, where the scatterplots are incrementally updated with new data coming. For example, the sampling results of successive frames generated by a KD-tree-based method <ref type="bibr" target="#b10">[10]</ref> might not be temporally coherent; see the yellow lasso regions in the two frames shown in Fig. <ref type="figure">1(d)</ref>. Though the classic reservoir sampling <ref type="bibr" target="#b30">[30]</ref> can select a fixed size of random samples from a data stream while preserving temporal coherence, it inherits the drawback of uniform random sampling that it misses the outliers in low-density regions; see the red box in Fig. <ref type="figure">1(c)</ref>.</p><p>In this article, we present a new sampling approach for visualizing scatterplots based on a density map pyramid. Our method is faster than the previous methods and is also enhanced to work in a progressive and streaming manner to remain interactive even when applied to very large datasets. In line with the design guidelines of visual sampling <ref type="bibr" target="#b5">[6]</ref>, progressive visualization <ref type="bibr" target="#b41">[41]</ref>, and streaming visualization <ref type="bibr" target="#b22">[22]</ref>, our approach is formulated based on the following three design requirements:</p><p>(i) DR1: maintaining the relative data densities and outliers on par with the state-of-the-art approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">10]</ref>; (ii) DR2: being sufficiently efficient to generate successive visualization frames under human latency limits <ref type="bibr" target="#b29">[29]</ref>; and (iii) DR3: preserving temporal coherence between successive visualization frames while capturing the data characteristics. We first describe a static version of our approach and then a progressive and streaming version. Our approach is based on a pyramid representation <ref type="bibr" target="#b39">[39]</ref> of the density map obtained from an input scatterplot, where each scale is sub-sampled to half the width and height of the previous scale. With this representation, the sampling at each level is first performed individually on each local region and then refined between adjacent regions of different parents in the pyramid to further meet DR1. Compared to the state-of-the-art KD-tree-based sampling <ref type="bibr" target="#b10">[10]</ref>, our pyramid-based sampling not only better preserves local characteristics in low-density regions (the red box in Figs. <ref type="figure">1(d,e</ref>)) but also runs faster by about an order of magnitude.</p><p>For progressive and streaming visualization, in which the data is processed incrementally in chunks, we gradually update the sampled results with each updated density map while preserving the temporal coherence. Like the static pyramid-based sampling, the incremental update has two steps: it works first on each local region, then on adjacent regions to fix them. Both steps are performed level by level in the pyramid. In doing so, the temporal coherence between successive visualizations can be better preserved; see an example in Fig. <ref type="figure">1(e)</ref>.</p><p>We evaluate our static approach using 40 large datasets and quantitatively compare the quality and computation time with the existing methods. The results show that our method is comparable to the stateof-the-art sampling methods in preserving relative data density and outliers while running about an order of magnitude faster than them. Further, we evaluate our progressive/streaming sampling by comparing its results to the ones of the reservoir and our static sampling. Our code is available on GitHub 1 . The main contributions of this work are:</p><p>• We develop a pyramid-based scatterplot sampling approach that 1 https://github.com/ChenXin360104/ProgressiveWaveletSampling preserves the relative data densities and outliers; it runs an order of magnitude faster and has good quality, as the state-of-the-arts. • Beyond the existing methods, we can adapt our method for progressiveness and streaming, making it even faster while maintaining the temporal coherence between successive frames; and • We quantitatively evaluate our sampling results and present two case studies that highlight the usefulness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our method relates to scatterplot sampling, as well as streaming and progressive visualizations. Below, we discuss these areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scatterplot Sampling</head><p>Various approaches <ref type="bibr" target="#b19">[19]</ref> have been proposed to address overplot in scatterplots, among which sampling is widely used. Sampling aims to faithfully represent the original data by carefully selecting a subset of the data for display. The simplest scheme is random sampling <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b18">18]</ref>, which treats all data points equally. However, it loses important data patterns (e.g., outliers) in low-density regions. To address this issue, a few perception-driven methods have been proposed for preserving various data characteristics such as density <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b32">32]</ref>, and outliers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">10]</ref>. Recently, Yuan et al. <ref type="bibr" target="#b47">[47]</ref> conducted an empirical evaluation of seven representative methods and provided guidance for selecting methods to use in different applications. Among the existing perception-driven sampling methods, recursivesubdivision-based sampling <ref type="bibr" target="#b10">[10]</ref> is the most relevant to our work. It proposes a customized KD-tree method for guiding multi-class scatterplot sampling, which explicitly characterizes the relative data densities, relative class densities, and major outliers. This method is as efficient as the above single-class sampling methods but performs best in balancing between the relative data densities and outliers. However, its customized KD-tree structure has to be built from the root to determine the split axes; doing so is not only very costly but also likely unstable to changes in the data. Thus, this method may not be applicable to streaming or progressive scenarios. In contrast, our proposed pyramid-based sampling method is much faster and allows local updates to preserve temporal coherence between successive frames.</p><p>So far, almost all existing sampling methods in visualization <ref type="bibr" target="#b47">[47]</ref> assume that the whole data can fit into the memory, which might not be true for the analysis of massive amounts of data. In a similar vein, Provost et al. <ref type="bibr" target="#b35">[35]</ref> propose progressive sampling for training a model that uses progressively larger samples until the model accuracy no longer improves. Because of its efficiency, progressive sampling has been used lately for association rules discovery <ref type="bibr" target="#b33">[33]</ref>, deduplication indexing <ref type="bibr" target="#b17">[17]</ref>, and mining of frequent items <ref type="bibr" target="#b36">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Streaming and Progressive Visualization</head><p>Streaming data is a continuous flow of data generated from various sources <ref type="bibr" target="#b0">[1]</ref>. Visualizing it usually consists of collecting values in a sliding window of the data (e.g., the last second or minute), visualizing it, and iteratively collecting the data in the next time window and visualizing it again, frame by frame as an animation (the sliding windows of two adjacent frames can overlap). So a streaming scatterplot shows all the points in the last time window and forgets all the old ones.</p><p>Progressive data is generated when e.g., loading a file progressively through a slow network <ref type="bibr" target="#b45">[45]</ref>. The points arrive chunk by chunk depending on the loading speed until the whole file has arrived. Each time a new chunk is loaded, the visualization can be updated to reflect the portion of the whole data that has been loaded. Contrary to streaming data, the past is not forgotten and the future is not infinite. Still, the visualization appears and improves frame by frame.</p><p>Streaming data visualization has recently gained a lot of attention. Gansner et al. <ref type="bibr" target="#b23">[23]</ref> proposed combining the node-link diagram with a map metaphor for incrementally visualizing text streams in real-time. Tanahashi et al. <ref type="bibr" target="#b42">[42]</ref> presented an efficient storyline generation algorithm from streaming data that uses the layouts of previous steps to position the incoming data points for preserving temporal coherence. Crnovrsanin et al. <ref type="bibr" target="#b12">[12]</ref> developed an incremental layout algorithm for visualizing online dynamic graphs. Fujiwara et al. <ref type="bibr" target="#b22">[22]</ref> extended an incremental PCA <ref type="bibr" target="#b38">[38]</ref> for visualizing streaming multidimensional data. All these techniques share the characteristic <ref type="bibr" target="#b13">[13]</ref> that the streaming visualization needs to be updated with the incoming data while maintaining the temporal coherence.</p><p>Like streaming visualization, progressive visualizations also need to deliver intermediate results with low latency to allow analysts to better control the exploration process <ref type="bibr" target="#b20">[20]</ref>. Although the intermediate results might not be accurate, several user studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b48">48]</ref> demonstrated that progressive visualizations show comparable performance to instantaneous ones in insight generation and perform better than the blocking ones. Stolper et al. <ref type="bibr" target="#b41">[41]</ref> further show that they can be efficiently combined with visual analytics and propose progressive visual analytics (PVA). To provide design guidelines for developers, Schulz et al. <ref type="bibr" target="#b40">[40]</ref> introduce an incremental visualization model with partitioned data and visualization operators for facilitating intermediate visualization updates. Angelini et al. <ref type="bibr" target="#b1">[2]</ref> systematically characterize the requirements, benefits, and challenges of PVA systems, and Micallef et al. <ref type="bibr" target="#b31">[31]</ref> further characterize PVA users in terms of their roles, tasks, and focus of analysis.</p><p>Several progressive systems and algorithms have been designed recently using high-dimensional projections visualized with density maps <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b44">44]</ref>. However, to the best of our knowledge, there are no progressive sampling approaches designed for scatterplots that satisfy the requirements of progressive and streaming visualization, and our approach fills this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>The goal of scatterplot sampling is to simulate the density distribution of the input data in a display with a limited number of visual pixels. In this section, we define the basic elements used in the design of our approach to achieving this goal. These include three maps (density map, visibility map, and assignment map) and three ratios (density ratio, visibility ratio, and assignment ratio). Before describing them, we denote data samples as the input samples from the original data in the scatterplot and display samples as the ones assigned to different regions for showing in the final scatterplot.</p><p>Maps. For a scatterplot to be shown on a given display, we define: Density map D: R 2 → R, where D(x) is the number of data samples located at pixel x.</p><p>Visibility map V : R 2 → {0, 1}, where V (x) is 0 if D(x) is zero, and 1 otherwise. It records whether a density map pixel is empty.</p><p>Assignment map A: R 2 → N. It records the number of samples that have been assigned to each region to eventually generate the display samples of our scatterplot sampling.</p><p>Our method first constructs D and V from the input data and then determines the assignment counts in A based on the algorithm in Sect. 4 to produce the final sampled scatterplot.</p><p>Ratios. Given two regions Ω A and Ω B of the same area on the display, the data density ratio between them is defined as:</p><formula xml:id="formula_0">δ (Ω A , Ω B ) = ∑ x∈Ω A D(x) ∑ x∈Ω B D(x) .</formula><p>Also, we can define the visibility ratio ν with visibility map V and the assignment ratio α with assignment map A using a similar formulation (δ stands for density, ν for visibility, and α for assignment). So, to preserve relative densities, α(Ω A , Ω B ) of the assignment map should be close to δ (Ω A , Ω B ) of the density map. However, simply enforcing this constraint would make most outliers disappear in the final scatterplot because, for outliers located in low-density regions, the assignment ratios of low-to high-density regions would become nearly zero according to the constraint. Since there is no clear definition of an outlier in scatterplots, we preserve outliers by requiring the assignment ratio α(Ω A , Ω B ) in low-density regions to maintain a good balance between the density ratio δ (Ω A , Ω B ) and the visibility ratio ν(Ω A , Ω B ) in the final scatterplot. The reason is that the visibility map emphasizes the outliers by treating all the visible pixels equally.</p><p>Map Pyramid. We use a pyramid representation for maps D, V , and A. For a 2D map M, its pyramid representation is a sequence of maps</p><formula xml:id="formula_1">{M i } n−1 i=0</formula><p>, where M i−1 is a lower-resolution version of M i <ref type="bibr" target="#b37">[37]</ref>. So, the finest level of the assignment map is similar to a visibility map, in which each pixel value is either zero or one. Yet, the assignment map is for display samples, whereas the visibility map is for data samples. There are multiple types of pyramids. We choose to use a simple one based on non-overlapping 2 × 2 blocks of pixels <ref type="bibr" target="#b39">[39]</ref>.</p><p>Given a 2 n by 2 n density map M, the pyramid G is built by recursively subdividing the map into four quadrants, similar to a complete quad-tree construction where each non-leaf node is the sum of its four children nodes. Doing so, we can construct a set of maps G M = {M 0 ,...,M n−1 }, where M n−1 corresponds to input map M and M 0 is the sum of all values in M n−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROGRESSIVE SCATTERPLOT SAMPLING</head><p>After loading a data chunk from the source (e.g., files in a local hard disk or server), our progressive sampling first builds two pyramids from the input density and visibility maps, followed by a static pyramid-based sampling to produce an assignment map that records if a pixel becomes a display sample. For the first data chunk, we directly select the display samples using the assignment map. For subsequent data chunks, we need to first detect the changed regions by comparing the pyramid coefficients of the latest density map with the previous one. For regions with enough changes, we need to update and re-select their display samples. Once the display samples are chosen, the sample selection of the current chunk is complete; we can perform the rendering (of the changed regions only) and then proceed to process the next data chunk.</p><p>From an input density map D, we first build the pyramids G D and G V based on D and its visibility map V , respectively. In the following, we will describe the detail of the two major stages of our approach, i.e., pyramid-based sampling and incremental update, and discuss the parameters and their influence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pyramid-based Sampling</head><p>Fig. <ref type="figure" target="#fig_2">2</ref> shows the pipeline of our pyramid-based sampling. Once pyramids G D and G V are built, a pyramid-based sampling is performed in a hierarchical way to generate an assignment map A i at each level. To fulfill DR1, we need to identify outliers in the input data, yet the definition of an outlier can be ambiguous. Since low-density regions are often regarded as outliers <ref type="bibr" target="#b7">[8]</ref>, we sidestep this issue by classifying regions into high-and low-density and placing samples into regions of different densities using different strategies.</p><p>We initialize A 0 as the total number of pixels in V ; this number is essentially the sample budget, the upper bound on the number of display samples filled in the assignment map. Hence, the goal of the top-down sampling process is to distribute this number over the four subregions recursively, while preserving the relative data densities and outliers. Starting from the top-level, i.e., A 0 , we classify the four subregions of A 1 into high-and low-density regions, assign samples to each of these regions with different objectives, and then refine the assignment map to further maintain the density ratios between each pair of adjacent regions. This top-down process iteratively refines the assignment maps, such that the number of assigned display samples gradually decreases for preserving the relative densities. The user can specify a certain level (called stopLevel) to stop this procedure and directly assign the present sample budget to subregions until the last level based on the visibility ratios. As outlined in Algorithm 1, the sampling at each level involves four major components: region classification, bilateral assignment, direct assignment, and sampling refinement. With the final assignment map, we obtain the final sampling result by randomly choosing one data sample for each grid cell.</p><p>Region Classification. For each region at level i in pyramid G D , we compute the density ratios among its four subregion nodes as follows. Suppose the density values in these subregions are   </p><formula xml:id="formula_2">{d 1 , d 2 , d 3 , d 4 }, the</formula><formula xml:id="formula_3">Pyramid G Pyramid G 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Density map D Visibility map V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Pyramid-based sampling</head><p>Input: density map D, visibility map V , and map size 2 l × 2 l Output:</p><formula xml:id="formula_4">assignment map A function PYRAMID-BASED SAMPLING(D, V , l) [G D , G V ] = construct one pyramid for D and another for V A 0 = V 0 i = 0 while i &lt;l − 1 do for each node j in V i do if i &lt;stopLevel then [R high , R low ] = ClassifyRegions(G D , j) AssignToHighDensityRegions(A i , G D , G V , R high ) if R low exists then AssignToLowDensityRegions(A i , G D , G V , R high , R low ) end if else AssignDirectly(A i , G D , G V ) end if end for if i &gt;0 then RefineBoundary(A i+1 , G D , G V ) end if i++ end while return A l−1 end function</formula><p>density ratio δ k for the kth subregion is defined as</p><formula xml:id="formula_5">δ k = d k / max j d j ∈ [0, 1]. (<label>1</label></formula><formula xml:id="formula_6">)</formula><p>If φ k is smaller than a threshold (say λ ), the corresponding region is regarded as a low-density region; otherwise, it is a high-density region.</p><p>In our experiment, we set λ to 0.1, indicating that the density value of dense regions is 10 times larger than that of sparse regions.</p><p>Bilateral Assignment. To address the discrepancy between the number of data samples and the number of visible pixels, we need to choose a proper number of display samples for each region to preserve the relative densities and outliers. Based on the region classification, we fulfill DR1 by assigning the sample budget to high-and low-density regions using two different strategies. For a high-density region Ω h , we aim to preserve its data density ratio with the adjacent regions as much as possible (i.e., the assignment ratio should be close to the associated data density ratio), while for low-density regions Ω l , we aim to preserve the outliers in the final scatterplot (i.e., the associated assignment counts should not be always zero to keep some of the outliers). Suppose we work with the jth region at the ith level, we assign the sample budget first to its high-density children nodes in four steps:</p><p>1. Find the kth subregion with the maximal density among its four children subregions (nodes); 2. Compute the number of display samples a k for the kth subregion; 3. Compute the sampling ratio η = a k /d k ; and 4. Compute the number of display samples for the other high-density regions by multiplying their data densities d h (h = k) by η. In step 2, a k is obtained by allocating A i j in terms of visible pixels:</p><formula xml:id="formula_7">a k = ceil(A i j * V i+1 k /V i j ).<label>(2)</label></formula><p>where V i j is the number of visible pixels in the corresponding region of the jth subregion at level i. Namely, the proportion of assignment for the largest density is based on the visibility map and the rest is proportionally sized by the density map. In doing so, the relative data density is preserved while respecting the sample budget as much as possible. Fig. <ref type="figure" target="#fig_3">3</ref>(b) shows a running example.</p><p>After the sample assignment is done for the high-density regions Ω h , we count the number of display samples assigned to Ω h , say A i+1 h , and then deal with low-density regions Ω l . To balance the density ratio and outliers in low-density regions, we first compute the density ratio δ (Ω l , Ω h ) between Ω l and Ω h . Likewise, we compute the visibility ratio ν(Ω l , Ω h ). Then, we find a proper number of samples to assign to Ω l , say A i+1 l , by minimizing the following objective: argmin</p><formula xml:id="formula_8">A i+1 l (1 − ω) α − δ (Ω l , Ω h ) 2 + ω α − ν(Ω l , Ω h ) 2 , (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>where α = A i+1 l /A i+1 h and ω is the weight between these two terms. Setting the derivative of Equation 3 with respect to A i+1 l to zero yields the following solution:</p><formula xml:id="formula_10">A i+1 l = A i+1 h × (1 − ω)δ (Ω l , Ω h ) + ων(Ω l , Ω h ) ,<label>(4)</label></formula><p>where ω controls how much we want to preserve the data density ratio. Putting ω to 0 solely keeps the high-density regions, so all outliers will disappear. Setting it to 1 keeps all the outliers but will heavily destroy the relative density, and overplotting will persist since too many data samples will be kept. In our experiment, ω is empirically set to 0.2 by default to put more emphasis on preserving the data density ratio. We then allocate the samples into each region in proportion to the number of non-empty pixels in V i+1 , like 8 and 13 in Fig. <ref type="figure" target="#fig_3">3(a)</ref>.</p><p>Direct Assignment. If the current level is higher than the given stop-Level, we perform a direct assignment for each subregion in this level, where the sample budget is kept during the assignment. Thus, we can adjust the details of high levels and control the number of points in the final result. For the jth subregion at level i, we conduct the assignment in two steps. We first sort its four subregions at level i + 1 in descending order of the data density ratio δ i+1 k . Based on the ordering, we assign the number of display samples to the kth subregion as</p><formula xml:id="formula_11">A i+1 k = min(ceil(A i j * V i+1 k /V i j ), r) (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>where r is the number of unassigned display samples. At last, r is zero and the total sum of display samples assigned to the four subregions equals A i j , ensuring no loss in display samples after the assignment. Sampling Refinement. Applying different sample assignment strategies to different kinds of regions might destroy the relative densities among the boundary regions and introduces blocking artifacts in the final sampling results; see Figs. <ref type="figure" target="#fig_4">4(b,c</ref>). After carefully examining the results, we found that these artifacts are caused by the discontinuity between adjacent regions with different parents in the pyramid. To alleviate this issue while avoiding large computational overhead, we propose to refine the assignment map A i by requiring every pair of adjacent regions of different parents at each level in the tree to preserve the relative data densities as much as possible.</p><p>Suppose two adjacent subregions of different parents at level i are l and h, where the density value D i l of l is smaller than the density value D i h of h. There are two possible cases that violate the data density ratios: (i)</p><formula xml:id="formula_13">D i l D i h &gt; A i l A i h and (ii) (D i h − D i l )(A i h − A i l ) &lt; 0.</formula><p>Case (i) is caused by the direct assignment, which respects the visibility ratio of each local region, whereas case (ii) is caused by the bilateral assignment for the two different kinds of regions. In both cases, A i l and A i h need to be fixed. Figs. <ref type="figure" target="#fig_4">4(b,c</ref>) show an example for each case, where the scatterplot is generated with different stopLevel values. The maps of the highlighted regions in Fig. <ref type="figure" target="#fig_4">4(b</ref>) indicate that the density ratios between two adjacent regions become larger in the assignment map, for example, the ratio 1072/1714 ≈ 0.625 is changed to 15/64 ≈ 0.234. In contrast, the relative density is changed in the opposite manner as in Fig. <ref type="figure" target="#fig_4">4</ref>(c), for example, the ratio 106/1218 ≈ 0.087 is changed to 11/5 = 2.2.</p><p>For case (i), we re-allocate the total samples ns = A i h + A i l in these two regions in proportion to the data densities:</p><formula xml:id="formula_14">A i h = D i h ns D i h + D i l .<label>(6)</label></formula><p>In doing so, the number of assigned display samples in the other (lowdensity) regions is ns − A i h . For case (ii), we re-allocate the total samples in these two regions by balancing density ratio δ (Ω l , Ω h ) and visibility ratio ν(Ω l , Ω h ). By formulating this goal with Equation 3 and differentiating it, we obtain the number of samples nh assigned to the high-density region A i h : nh = ns</p><formula xml:id="formula_15">(1 − ω) D i l +D i h D i h + ω V i l +V i h V i h . (<label>7</label></formula><formula xml:id="formula_16">)</formula><p>where ns is the total number of samples A i h + A i l . Once nh is obtained, we set A i l to ns − nh. As shown on the left of Fig. <ref type="figure" target="#fig_4">4</ref>(c), the assignment map generated after the refinement better preserves the density ratios. Since the refinement in the coarse level will diffuse to subsequent levels in the pyramidbased sampling process, the artifact can almost be removed in the final sampling result (see the right of Fig. <ref type="figure" target="#fig_4">4(c)</ref>). Time Complexity. As shown in Algorithm 1, the time complexity of our method depends only on the size of the density map n = 2 l × 2 l . The level of the pyramid is therefore l and we need to classify each node (subregion), and then assign and refine the sample budget for each node at each level. Overall, the time complexity of our method is O(n) ignoring the construction time of the density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Incremental Update</head><p>When the first data chunk is loaded, we produce the first assignment map A with the static pyramid-based sampling presented in Sect. 4.1. Since A is an approximation of the density map, we need to incrementally update it for each newly arriving data chunk. Given D as the latest density map associated with all the data chunks that have arrived so far, we generate a new assignment map Ā by performing a static pyramidbased sampling on D. Then, we detect the regions from pyramid G A whose relative densities are changed in G D level by level and update these regions in A with the values in Ā. Finally, we further refine the updated A to ensure the relative data densities between the updated regions and their adjacent ones are preserved, during which the regions to be changed are also detected level by level. Fig. <ref type="figure" target="#fig_5">5</ref> illustrates these three steps with the 4 × 4 region on the top left of Fig. <ref type="figure" target="#fig_2">2(d)</ref>, where the regions to be updated in A are highlighted in red. In the following, we describe the last two steps in detail.</p><p>Local Region Update. Given A and D, we compute two pyramids for them and detect the regions with the changed relative densities from coarse level to fine level. For every region at the ith level, we check if the density ratio in A i has a large difference from the one in Di . Suppose the node index is j at the (i − 1)-th level, and A i−1 j and Di−1 j are both nonzero, the density ratio difference is defined as</p><formula xml:id="formula_17">μ i−1 j = 1 4 4 ∑ k=1 A i 4 j+k A i−1 j − Di 4 j+k Di−1 j . (<label>8</label></formula><formula xml:id="formula_18">) If μ i−1 j</formula><p>is larger than a given threshold ε, we regard node j at the (i − 1)-th level as "changed" and stop checking its descendant regions in finer levels. For the node, if A i−1 j or Di−1 j becomes zero, we also label its region as "changed." Once the finest level is done, we update all these changed regions in A with the values in Ā.</p><p>Adjacent Region Refinement. The first step individually refines each local region, so it might not preserve the relative densities between adjacent regions, resulting in visible artifacts. Like the sampling refinement step in the static sampling, we also further check whether the updated regions and their adjacent regions respect the corresponding data density ratio in D. With the updated A, we re-compute the pyramid again and then find the adjacent regions that need to be further updated from the coarse to fine levels.</p><p>Suppose the changed region j and its adjacent region k are at level i, we compute the density ratio difference μ in A i and Di as</p><formula xml:id="formula_19">μ = A i j A i k − Di j Di k . (<label>9</label></formula><formula xml:id="formula_20">)</formula><p>We mark the adjacent region as "changed" if μ is larger than threshold ε, and stop checking its descendant regions in finer levels. Like local region update, we refine all changed regions in A with the values in Ā.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">5</ref>(a), the arriving data chunk produces three changed regions in the latest density map D highlighted in purple. Then, the bottom left region in the second level of the assignment map is labeled as "changed" and then all its subregions in the finer level are updated in Fig. <ref type="figure" target="#fig_5">5(b)</ref>. Last, the subregion with the black border in Fig. <ref type="figure" target="#fig_5">5(c</ref>) is further updated by the adjacent region refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Analysis</head><p>All the parameters we describe below affect the quality and stability of the sampling, but they can be changed at any time in an interactive environment so the user can tune them according to the data at hand for producing the desired results. More discussions about the influence of parameter choices can be found in the supplemental material. Density threshold λ &amp; Outlier weight ω. These two parameters jointly affect the number of outliers to be preserved. Parameter λ determines which regions are classified as low density and weight ω determines how many data points in low-density regions we want to keep. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, a large λ leads to more sparse regions (see the orange boxes in Fig. <ref type="figure" target="#fig_6">6(a,b,c</ref>)) and a large ω preserves more data points in low-and medium-density regions (see the orange and red boxes in Fig. <ref type="figure" target="#fig_6">6(d,b,e</ref>)). When λ and ω are both large, more outliers are preserved, whereas the relative densities may not be correct in some regions. In our experiments, we set them to 0.1 and 0.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StopLevel.</head><p>Parameter stopLevel determines the level of details to preserve, thus affecting the number of samples to keep. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>(b,g,h), a small stopLevel leads to more details in low-density regions and more samples, while a large stopLevel reduces the number of samples to preserve the relative densities in fine level. We suggest setting it to the last level by default and automatically search one level as stopLevel where the corresponding total sum of display samples is close to the number of samples specified by users.</p><p>Ratio threshold ε. Parameter ε influences the temporal coherence between successive visualizations; see Fig. <ref type="figure" target="#fig_7">7</ref>. A large ε tends to keep more samples selected from the previous frames and helps to maintain stability, but might not reflect the relative data densities in the latest frame. In contrast, a low ε better preserves the relative data density but may introduce large changes on the display samples. We empirically set it to 0.25, which works well for most tested data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We implemented our method in C++ and tested it on a PC with an Intel Core i5-4590 3.3GHz CPU and 24GB memory. To confirm that our method can meet our three design requirements (DR1, DR2, and DR3), we performed two quantitative comparisons with the state-of-the-art methods in two settings of static sampling and progressive sampling. Also, we conducted two case studies to demonstrate the effectiveness of our method on real datasets. The full evaluation results, including the screenshots and the corresponding scores of different metrics for each dataset, can be found in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparative Evaluation of Static Sampling</head><p>We compare the sampling results produced from various methods in terms of sampling quality and runtime performance, in which all the scatterplots are displayed in 1600 × 900 pixels.</p><p>Methods. We compare our method with seven sampling methods: random sampling (RS), blue noise sampling (BNS) <ref type="bibr" target="#b11">[11]</ref>, density-based sampling (DBS) <ref type="bibr" target="#b32">[32]</ref>, non-uniform sampling (NUS) <ref type="bibr" target="#b4">[5]</ref>, outlier biased density-based sampling (OBDBS) <ref type="bibr" target="#b46">[46]</ref>, multi-view z-order sampling (MVZS) <ref type="bibr" target="#b24">[24]</ref>, and KD-tree-based sampling (KBS) <ref type="bibr" target="#b10">[10]</ref>. Though KBS and MVZS are designed for multi-class sampling, we use them on single-class data without multi-class constraints. For all methods, we use the Python implementations provided by Yuan et al. <ref type="bibr" target="#b47">[47]</ref> for comparing the sampling quality. We set the parameters of our method to be λ = 0.1, ω = 0.2, and stopLevel to the last level, and then adjust the  parameters of other methods to generate similar numbers of samples. The details can be found in the supplemental material.</p><p>Datasets. For a comprehensive evaluation, we collected 40 datasets with substantial variations in data distributions and data size ranged from 4K to 2M. Among them, 12 synthetic datasets were generated by mixing Gaussian distributions and a uniform distribution, and 28 real datasets were collected from the UCI data repository <ref type="bibr" target="#b16">[16]</ref> and Kaggle <ref type="bibr" target="#b26">[26]</ref>. The details can be found in the supplemental material.</p><p>Metrics. We employ two numerical measures proposed by Bertini and Santucci <ref type="bibr" target="#b4">[5]</ref>: Perceived Data Densities ratio (PDDr) and Erased Sample Regions ratio (ESRr), both based on the division of the pixel display into a set of non-overlapping, equal-sized regions. PDDr measures how much the density ratio is preserved for each pair of regions:</p><formula xml:id="formula_21">PDDr = ∑ i ∑ j&lt;i χ i j σ (sgn(D(Ω i ) − D(Ω j )), sgn(A(Ω i ) − A(Ω j ))) ∑ i ∑ j&lt;i χ i j ,<label>(10)</label></formula><p>where χ i j = D(Ω i )+D(Ω j ) (number of data samples in regions Ω i and Ω j ), sgn(v) is a sign function, and σ (v 1 , v 2 ) returns one if v 1 equals v 2 , otherwise zero. The range of PDDr is [0, 1] and a large value indicates better preservation of relative data densities.</p><p>ESRr measures the proportion of lost outliers in low-density areas by computing the ratio of void regions caused by the sampling:</p><formula xml:id="formula_22">ESRr = ∑ i σ (A(Ω i ), 0) N , (<label>11</label></formula><formula xml:id="formula_23">)</formula><p>where N is the number of non-empty regions in the original scatterplot. ESRr also ranges [0, 1] but a small value indicates better outlier preservation. We set the size of the region Ω to 40 × 40 pixels. (c) Execution Time (sec) 10,000 1,000,000 100,000 Results. Screenshots of the original scatterplots and sampled results generated by all methods on various datasets with complete scores can be found in the supplemental material. The violin plots in Figs. <ref type="figure" target="#fig_9">8(a,b</ref>) summarize the PDDr and ESRr scores of each method on all datasets.</p><p>Fig. <ref type="figure" target="#fig_9">8</ref>(a) shows that PDDr of our method is very close to those of RS, KBS, DBS, BNS, and MVZS. OBDBS is slightly worse than our method, while NUS is the worst. In contrast, NUS performs better than the other methods in ESRr, while our method is ranked as the second, as shown in Fig. <ref type="figure" target="#fig_9">8(b)</ref>. Though NUS has the lowest ESRr, our method outperforms it in PDDr clearly. This result confirms that our method balances well the preservation of relative data densities and outliers.</p><p>Runtime. We only implemented KBS and NUS in C++ for a fair runtime comparison, since the other methods involve expensive computation and are slower as shown in previous work <ref type="bibr" target="#b10">[10]</ref>. Fig. <ref type="figure" target="#fig_9">8(c</ref>) summarizes the execution time of all tested datasets for KBS, NUS, and our method. We can see from the violin plot that our method is faster than NUS and even more than ten times faster than KBS on average. Fewer outliers and lower variance indicate that the runtime of our method is rather stable, regardless of the data set size.</p><p>To further examine the time performance of our method, we generated a set of synthetic data with a gradually increasing number of data samples. Fig. <ref type="figure" target="#fig_9">8(d</ref>) plots the execution time for running different sampling methods on these datasets. We can see that our method is twice faster than NUS and around ten times faster than KBS for the data with more than 100k points. The reason for the almost constant running time of our method is that its time complexity depends only on the resolution of the density map. Note that we did not take into account the pre-processing time such as computing the density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparative Evaluation of Progressive Sampling</head><p>Competitive analysis <ref type="bibr" target="#b6">[7]</ref> is a widely-used approach for evaluating online algorithms, by comparing the performance with an equivalent offline algorithm. In our progressive setting, users will rely on intermediate results to make early decisions and thus we measure the performance of each frame, following the practice in Jo et al. <ref type="bibr" target="#b25">[25]</ref>.</p><p>Method. We conducted a competitive analysis of the progressive version (ε = 0.25) and the static version (without incremental update) of our method, and the reservoir sampling method <ref type="bibr" target="#b28">[28]</ref>. Our method uses the same parameter settings as the one in the first experiment.</p><p>Data. To learn how our method works in practice, we use two datasets activity and census, which have different characteristics as shown in Table <ref type="table" target="#tab_0">1</ref>. For activity and census, their partitioned chunk sizes are 10,000 and 100,000, respectively. Measures. We hypothesized that the progressive version is more stable than others since there should be fewer changed samples. Thus, we further compute the number of changed points (NCP) between every pair of consecutive frames, besides PDDr and ESRr. For each frame, the PDDr and ESRr scores are based on the comparison between the assignment map and the density map of the whole data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Results. Figs. 9(a,b) show the NCP values for each frame of these two datasets. In the first frame, all points are newly added and thus its NCP is the highest. Later, all methods quickly converge to stable values but the NCP values of our progressive are smaller than the others. Because of the large density variation, all the methods show more fluctuations on the activity dataset, whereas our progressive method is the most stable. We conclude that our progressive version is more stable than the static version and the reservoir sampling. Figs. <ref type="figure" target="#fig_11">9(c,d</ref>) show the PDDr scores of each frame of these two datasets, which have different performances. The PDDr scores of the three methods in the activity dataset gradually increase to a value of 0.96. Although the scores of our progressive methods are less than reservoir sampling at the beginning, they quickly converge to similar values. In contrast, the scores of the three methods on the census dataset are quite stable in all frames. After examining the data and each sampling result, we found that each data chunk contains similar spatial distribution. Since all scores of our progressive method are around 0.9, it is still good enough to preserve relative data densities.</p><p>Figs. 9(e,f) show the ESRr scores of each frame of these two datasets. We can see that our progressive method performs the best, followed by our static method, while reservoir sampling is the worst. The reason for our progressive method performing better than the static method is  <ref type="table" target="#tab_0">2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2</ref>   <ref type="figure">PDDr (c,d</ref>), and ESRr (e,f) evolve over iterations for loading the two tested datasets (activity and census) for sampling by the three methods being compared: static and progressive versions of our method vs. reservoir sampling. Our progressive method is the most stable and makes a good balance between preserving relative data densities and outliers.</p><p>that its incremental update step pays more attention to the regions with significant data density changes, thereby encouraging the retention of outliers in low-density regions. Conversely, the static method updates the whole density map and might lose some outliers for better balancing the preservation of relative density and outliers. In this configuration, by chance, retaining the outliers produced a slightly better result transiently. Moreover, the difference in ESRr between our progressive method and reservoir sampling gradually increases to 0.12 for the activity dataset, while the difference almost stays the same (0.11) for the census dataset. Thus, we conclude that our progressive method also maintains a good balance between preserving relative data densities and outliers.</p><p>Runtime. Regarding the time performance, our progressive method is similar to our static method but a bit slower than the reservoir sampling. The average runtime of our method and reservoir sampling for the two datasets are 0.024s vs. 0.013s and 0.15s vs. 0.13s; yet, our method is sufficiently fast for supporting interactive analysis.</p><p>Based on the results of the three measures and time performance, we can conclude that our method well satisfies the three design requirements (DR1-DR3) for progressive visualization of large scatterplots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case studies</head><p>We conducted case studies with two real-world datasets about astrophysics and stock prices, corresponding to the two settings of progressive and streaming visualizations, respectively. Note that the density map is accumulative in progressive visualization, and it is based on the data within the current time window in streaming visualization.</p><p>Hertzsprung Russell diagram. Astronomers often study stellar evolution by exploring the relationship between the star temperature and observed luminosity using scatterplots; such plots are commonly referred to as the Hertzsprung-Russell diagram <ref type="bibr" target="#b14">[14]</ref>. Here, we used a subset of the Gaia Data Release 2 <ref type="bibr" target="#b43">[43]</ref> collected by the European Science Agency. The subset contains 1,322,033 stars within 200 parsecs from The Sun; two typical attributes of the stars are luminosity and temperature. Fig. <ref type="figure" target="#fig_15">10(a)</ref> shows the input scatterplot (top) and the associated density map (bottom); each pair of the overlaid red and green lines corresponds to an unresolved binary system of two identical stars <ref type="bibr" target="#b2">[3]</ref>.</p><p>By partitioning the data into chunks, each of 100,000 stars, Figs. <ref type="figure" target="#fig_15">10(b,c</ref>) show four frames in progressive visualization generated by reservoir sampling and by our method with stopLevel = 6 (10 for the total), respectively. From the 8th frame, the patterns highlighted in     In contrast, all these structures cannot be clearly revealed in Fig. <ref type="figure" target="#fig_15">10(b)</ref>. Compared to the 8th and 9th frames, the number of changed points in both methods is small. Overall, our method can preserve relative data densities and outliers well, while maintaining the temporal coherence.</p><p>Since reservoir sampling is not designed for progressive visualization, we admit that the comparison is not entirely fair, given that our method supports progressive visualization. Yet, we would like to highlight that no progressive visualization methods have been designed for scatterplots sampling. Stock Prices. We used the historical stock market datasets from Kaggle <ref type="bibr" target="#b26">[26]</ref> from Jan. 1, 1996 to Aug. 7, 2020, and explored the relationship between the stock volume and stock percentage change. To simulate the setting of streaming data visualization, we set the time step as one day and the time window as 30 days. In doing so, our method loads the data chunk containing records of the next day for the incremental update while discarding the records of the day one month earlier. To reveal the major patterns, we remove the data items with a stock volume larger than 1 million or with stock percentage change larger than 30%.</p><p>To see how the Sep. 11 attacks affected the stock market, we com- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this article, we proposed a scatterplot sampling method based on a pyramid-based decomposition of the density map for progressive and streaming data visualizations. In the static setting, it can perform similarly to state-of-the-art methods in maintaining relative densities and preserving outliers but it is faster. Based on a pyramid representation of the density map, our progressive sampling is achieved by first performing a local region-based sampling, then a refinement between adjacent regions and an incremental update for progressive and streaming visualization. The quantitative evaluation and case studies demonstrate the effectiveness of our method for exploring large and streaming data.</p><p>Our approach still has some limitations. First, our bilateral assignment may still produce blocking artifacts; we will develop a smooth nonlinear function to alleviate this issue. Second, it is hard to find proper parameters (λ , ω, and stopLevel) showing meaningful patterns, while improper ones might lead to bad visualizations. For example, the PDDr scores in Figs. <ref type="figure" target="#fig_6">6(f,g</ref>) are 0.888 and 0.938, while the ESRr scores in Figs. <ref type="figure" target="#fig_6">6(a,f</ref>) are 0.376 and 0.094. In the future, we will explore automatic methods for setting such parameters for better balancing the preservation of relative data densities and outliers. Last, we plan to conduct a user study as Yuan et al. <ref type="bibr" target="#b47">[47]</ref> for investigating how our results align with human perception in both static and progressive settings. In the future, we will extend our approach to deal with multi-class datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall pipeline of our static pyramid-based sampling: (a) the input map D and its corresponding visibility map V ; (b) pyramids G D and G V constructed from D and V , respectively; (c) the pyramid-based sampling process that fuses G D (upper) and G V (lower) level by level to assign the sample budget to the next level assignment map [A 0 , ..., A n−1 ] (middle), where the last level of G D and G V equal D and V , respectively; (d) the final assignment map A = A n−1 ; and (e) the visualization with randomly selected samples based on A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustrating the bilateral assignment process for high-and lowdensity regions. (a) The density map (left) and visibility map (right) at the root whose four subregions in the density map are classified as high density (in orange) and low density (in blue); (b) two steps to assign the sample budget to high-density subregions: first the subregion with the largest density, then the other high-density subregion(s) based on the density ratio; (c) two steps to assign sample budget to low-density subregions: count the number of display samples in the whole region, then allocate it to each low-density subregion based on the visibility ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustrating the sampling refinement process that repairs the assignment map to alleviate the blocking artifacts. (a) The input scatterplot (top) made up of two Gaussians and its transparent visualization (bottom); (b,c) Results generated with different stopLevel: 5 for Case (i) shown in (b) and 9 for Case (ii) shown in (c). From left to right: the sampled scatterplots generated without refinement (left), the maps related to the highlighted regions (middle), and the scatterplots after the refinement (right), in which the blocking artifacts are almost removed. The red bold lines indicate the boundary between subregions of different parents, whereas the orange backgrounds indicate the subregions involved in the refinement.</figDesc><graphic coords="5,432.71,129.65,123.08,69.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Illustrating the incremental update process using the top-left 4 × 4 region in Fig.2 (d). Our method updates the assignment map for a new data chunk in three steps: (a) update density map D, where the values of three purple cells are changed from 44, 60, and 26 to 79, 90, and 61, and then compute a new assignment map Ā; (b) perform a local region update by identifying the changed subregions against the previous assignment map A highlighted in red and updating these regions with the corresponding values in Ā; and (c) perform an adjacent region refinement, where the cell with the dashed black border in assignment map A is changed and its value is further updated with the one in Ā.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Parameter analysis on the Person Activity dataset [16] with the associated PDDr and ESRr scores (see Section 5). The orange dashed boxes and red dashed circles represent typical low-and medium-density regions, respectively. (a,b,c) A large λ results in more regions classified as low-density regions; (d,b,e) a large ω results in more display samples in low-density regions; (f) when λ and ω are both large, almost all low-density regions in the original scatterplot are kept but the data density ratios cannot be maintained; (b,g,h) decreasing stopLevel introduces more display samples; outliers in low-density regions can be shown more clearly but overplotting will have resulted.</figDesc><graphic coords="7,55.55,275.69,120.26,67.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The scatterplots of the 7th and 8th frames under different ε. The red circles show that larger ε can help prevent unnecessary changes of display samples in low-density regions.</figDesc><graphic coords="7,55.55,351.65,120.26,67.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Results of quantitative comparison. (a,b) These violin plots summarize the values of PDDr (a) and ESRr (b) over all the tested datasets, where a larger PDDr score is better and a smaller ESRr score is better; (c) the violin plot shows the log-scale computational times of three most efficient methods with C++ implementations tested over all the datasets; and (d) the curves show the relationship between the execution time (running the sampling procedure) and the data size of different methods by using synthetic datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. These line charts show how the number of changed points (a,b),PDDr (c,d), and ESRr (e,f) evolve over iterations for loading the two tested datasets (activity and census) for sampling by the three methods being compared: static and progressive versions of our method vs. reservoir sampling. Our progressive method is the most stable and makes a good balance between preserving relative data densities and outliers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Progressive sampling on the Gaia Data Release 2. (a) the scatterplot of the input data (top) and the density map (bottom); (b,c) the results of the intermediate frames generated by reservoir sampling (b) and our method (c).</figDesc><graphic coords="9,63.11,292.61,64.70,64.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Scatterplots that show the relationship between stock volume (horizontal) and stock percentage change (vertical) for two different time ranges: before the Sep. 11 attacks (left column) and the whole Sep. 2001 (right column). (a,b) the overplotted scatterplots of the original data; and (c,d) streaming visualization results of our method from (a,b), showing that our method can produce faithful visualizations.</figDesc><graphic coords="9,63.11,368.33,64.70,64.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 (</head><label>10</label><figDesc>Fig.10(c) gradually become clear. Among them, the patterns in the purple and yellow boxes have similar structures as the ones at the bottom of Fig.10(a), while the outliers in the red circle are better preserved. In contrast, all these structures cannot be clearly revealed in Fig.10(b). Compared to the 8th and 9th frames, the number of changed points in both methods is small. Overall, our method can preserve relative data densities and outliers well, while maintaining the temporal coherence.Since reservoir sampling is not designed for progressive visualization, we admit that the comparison is not entirely fair, given that our method supports progressive visualization. Yet, we would like to highlight that no progressive visualization methods have been designed for scatterplots sampling. Stock Prices. We used the historical stock market datasets from Kaggle<ref type="bibr" target="#b26">[26]</ref> from Jan. 1, 1996 to Aug. 7, 2020, and explored the relationship between the stock volume and stock percentage change. To simulate the setting of streaming data visualization, we set the time step as one day and the time window as 30 days. In doing so, our method loads the data chunk containing records of the next day for the incremental update while discarding the records of the day one month earlier. To reveal the major patterns, we remove the data items with a stock volume larger than 1 million or with stock percentage change larger than 30%.To see how the Sep. 11 attacks affected the stock market, we com-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>pared two scatterplots (see Figs. 11(a,b)) generated from the dataset for two different time-ranges: (i) from Aug. 11, 2001, to Sep. 10, 2001 (see Fig. 11(a)) and (ii) the whole Sep. 2001 (see Fig. 11(b)). We can see that the stock percentage change has much larger variations in Fig. 11(b) than the ones shown in Fig. 11(a) and more points with negative stock changes can be observed than those with positive changes.Based on these observations, we see that the Sep. 11 attacks resulted in a negative effect and many stocks show large fluctuations. However, the streaming sampling results in Figs.11(c,d) shows that the major trend is almost preserved in both ranges. Further comparing the samples in the red boxes reveals more samples with negative stock change and large volumes in Sep. 2001. After checking the density maps shown in the supplemental material, we conclude that our streaming sampling method produces faithful visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets employed in the competitive analysis.</figDesc><table><row><cell></cell><cell># points</cell><cell cols="3"># samples chunk size variance</cell></row><row><cell>activity</cell><cell>164,860</cell><cell>∼ 5,500</cell><cell>10,000</cell><cell>high</cell></row><row><cell>census</cell><cell>2,000,000</cell><cell>∼ 2,100</cell><cell>100,000</cell><cell>low</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the grants of the NSFC (61772315, 61861136012), the High-End Foreign Experts Project (No. G20190015039), the Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University (No.VRLAB2020C08), and the CAS grant (GJHZ1862).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Akidau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chernyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lax</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Review and Characterization of Progressive Visual Analytics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Schulz</surname></persName>
		</author>
		<idno type="DOI">10.3390/informatics5030031</idno>
	</analytic>
	<monogr>
		<title level="j">Informatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaia Data Release 2 -Observational Hertzsprung-Russell diagrams</title>
		<author>
			<persName><forename type="first">C</forename><surname>Babusiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barstow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vallenari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bossini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bressan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cantat-Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1051/0004-6361/201832843</idno>
	</analytic>
	<monogr>
		<title level="j">Astronomy &amp; Astrophysics</title>
		<imprint>
			<biblScope unit="volume">616</biblScope>
			<biblScope unit="page">A10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Steering the Craft: UI Elements and Visualizations for Supporting Progressive Visual Analytics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13205</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="491" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">By Chance is Not Enough: Preserving Relative Density through Nonuniform Sampling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV.2004.1320207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Visualisation</title>
				<meeting>the International Conference on Information Visualisation</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="622" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Give Chance a Chance: Modeling Density to Enhance Scatter Plot Quality through Random Data Sampling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
		<idno type="DOI">10.1057/palgrave.ivs.9500122</idno>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Online Computation and Competitive Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LOF: Identifying Density-Based Local Outliers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<idno type="DOI">10.1145/335191.335388</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
				<meeting>the ACM SIGMOD International Conference on Management of Data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2000-05">May 2000</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">93104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual Abstraction and Exploration of Multi-class Scatterplots</title>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346594</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Recursive Subdivision Technique for Sampling Multi-class Scatterplots</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934541</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="729" to="738" />
			<date type="published" when="2020-01">Jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stochastic Sampling in Computer Graphics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Cook</surname></persName>
		</author>
		<idno type="DOI">10.1145/7529.8927</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="72" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Incremental Layout Method for Visualizing Online Dynamic Graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Crnovrsanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-27261-02</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Algorithms and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="80" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human Factors in Streaming Data Analysis: Challenges and Opportunities for Information Visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Arendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13264</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="254" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mass loss rates in the Hertzsprung-Russell diagram</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nieuwenhuijzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Van Der Hucht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy and Astrophysics Supplement Series</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">By Chance Enhancing Interaction with Large Data Sets through Statistical Sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ellis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1556262.1556289</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Visualisation</title>
				<meeting>the International Conference on Information Visualisation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="https://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive sampling for deduplication indexing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Efstathopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">964</biblScope>
			<date type="published" when="2012">Nov. 13 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Density Control Through Random Sampling: an Architectural Perspective</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dix</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV.2002.1028760</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information Visualisation</title>
				<meeting>the International Conference on Information Visualisation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Taxonomy of Clutter Reduction for Information Visualisation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dix</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70535</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1216" to="1223" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Primet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05162</idno>
		<title level="m">Progressive Analytics: A Computation Paradigm for Exploratory Data Analysis</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trust Me, Im Partially Right: Incremental Visualization Lets Analysts Explore Large Datasets Faster</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schraefel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208294</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
				<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1673" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Incremental Dimensionality Reduction Method for Visualizing Streaming Multidimensional Data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shilpika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934433</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="418" to="428" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive Visualization of Streaming Text Data with Dynamic Maps</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Gansner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>North</surname></persName>
		</author>
		<idno type="DOI">10.7155/JGAA.00302</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Graph Algorithms and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="515" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934799</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="739" to="748" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PANENE: A Progressive Algorithm for Indexing and Querying Approximate k-Nearest Neighbors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2869149</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1347" to="1360" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kaggle</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">P5: Portable Progressive Parallel Processing Pipelines for Interactive Data Analysis and Visualization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2934537</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1151" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reservoir-Sampling Algorithms of Time Complexity O(n(1 + log</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/198429.198435</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="481" to="493" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Effects of Interactive Latency on Exploratory Visual Analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346452</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2122" to="2131" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Convenient Algorithm for Drawing a Simple Random Sample</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Bellhouse</surname></persName>
		</author>
		<idno type="DOI">10.2307/2347297</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="184" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Human User in Progressive Visual Analytics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Micallef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aupetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kohlhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
		<idno type="DOI">10.2312/evs.20191164</idno>
	</analytic>
	<monogr>
		<title level="m">EuroVis (Short Papers)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Density Biased Sampling: An Improved Method for Data Mining and Clustering</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<idno type="DOI">10.1145/335191.335384</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
				<meeting>the ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient Progressive Sampling for Association Rules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2002.1183923</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining</title>
				<meeting>IEEE International Conference on Data Mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Approximated and User Steerable tSNE for Progressive Visual Analytics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lelieveldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Höllt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2570755</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1739" to="1752" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient Progressive Sampling</title>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
		<idno>doi: 10.1145/ 312129.312188</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mining Frequent Itemsets through Progressive Sampling with Rademacher Averages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riondato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783265</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1005" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<title level="m">Multiresolution Image Processing and Analysis</title>
				<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Incremental Learning for Robust Visual Tracking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-007-0075-7</idno>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Quadtree and Related Hierarchical Data Structures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<idno type="DOI">10.1145/356924.356930</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="260" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An Enhanced Visualization Process Model for Incremental Visualization</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2462356</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1830" to="1842" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Progressive Visual Analytics: User-Driven Visual Exploration of In-Progress Analytics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Stolper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2014.2346574</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1653" to="1662" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An Efficient Framework for Generating Storyline Visualizations from Streaming Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2392771</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="730" to="742" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gaia Data Release 2 -Summary of the Contents and Survey Properties</title>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<idno>doi: 10. 1051/0004-6361/201833051</idno>
	</analytic>
	<monogr>
		<title level="j">Astronomy &amp; Astrophysics</title>
		<editor>id.A1</editor>
		<imprint>
			<biblScope unit="volume">616</biblScope>
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Brushing Dimensions -A Dual Visual Analysis Model for High-Dimensional Data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Turkay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Filzmoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2011.178</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2591" to="2599" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Turkay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pezzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Binnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rusu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08032</idno>
		<title level="m">Progressive Data Science: Potential and Challenges</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interactive Correction of Mislabeled Training Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno>doi: 10. 1109/VAST47406.2019.8986943</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visual Analytics Science and Technology</title>
				<meeting>the IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Evaluation of Sampling Methods for Scatterplots</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030432</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1720" to="1730" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How Progressive Visualizations Affect Exploratory Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galakatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Crotty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2016.2607714</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1977" to="1987" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
