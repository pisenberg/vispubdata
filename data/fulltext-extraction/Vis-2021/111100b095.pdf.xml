<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pramod</forename><surname>Chundury</surname></persName>
							<email>pchundur@umd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Biswaksen</forename><surname>Patnaik</surname></persName>
							<email>bpatnaik@umd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yasmin</forename><surname>Reyazuddin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pramod Chundury</orgName>
								<orgName type="institution">Jonathan Lazar</orgName>
								<address>
									<settlement>Biswaksen Patnaik</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Niklas Elmqvist are with the University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Tang</surname></persName>
							<email>christinetang075@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Lazar</surname></persName>
							<email>jlazar@umd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Elmqvist</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Pramod Chundury</orgName>
								<orgName type="institution">Jonathan Lazar</orgName>
								<address>
									<settlement>Biswaksen Patnaik</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Niklas Elmqvist are with the University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National Federation of the Blind</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Poolesville High School</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D2BD99BD343556882C85913D1C30E3E6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Accessibility</term>
					<term>blind users</term>
					<term>sonification</term>
					<term>visualization</term>
					<term>spatial layouts</term>
					<term>sound perception</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&amp;M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&amp;M experts-all of them blind-to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible-using sonification and auralization. However, our experts recommended supporting a combination of senses-sound and touch-to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Navigating outdoor space using non-visual senses. Blind individuals build mental maps (visual thinking) by sensing environmental sounds, and use their cane for performing echolocation and perceiving haptic feedback; in other words, a combination of sound and touch towards non-visual sensemaking of spatial concepts. Our work explores how these capabilities can be used for accessible data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visualizations are largely inaccessible to individuals who are blind or have visual impairments. Screen readers, the method blind people most commonly use to transform on-screen text to speech, generally cannot parse pixel visualizations, and few web-based visualizations provide sufficient textual descriptions or the underlying datasets <ref type="bibr" target="#b48">[49]</ref>. People with visual impairments are a large population of potential data visualization users. In 2015, globally there were 253 million people with visual impairments, out of whom 26 million were blind, and this number is estimated to reach around 703 million by the year 2050 <ref type="bibr" target="#b3">[4]</ref>.</p><p>. This is also not just a sociotechnical problem, but a potentially legal one; for example, in the United States, Section 508 of the Rehabilitation Act requires that all federal government websites be accessible for people with disabilities, and the Americans with Disabilities Act similarly requires accessibility for most websites of public accommodations <ref type="bibr" target="#b46">[47]</ref>. Blind people 1 navigate a 3D world of space and objects, and are therefore equally capable of understanding spatial layouts as sighted individuals. However, despite continuous advances in visualization research, little effort is devoted to accessibility. While our focus in this paper is on blind individuals, inaccessible visualization practices not only affect blind people, but also those with other impairments such as motor or cognitive impairments <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b48">49]</ref>. We believe that the visualization community must work to lower barriers for blind individuals by focusing on accessible visualization and data analysis.</p><p>Accessibility technologies for blind users tend to employ a method called sensory substitution <ref type="bibr" target="#b18">[19]</ref> by conveying data using other senses such as hearing and touch, and even smell <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref> instead of vision. Efforts to render visualizations accessible for blind users have been explored by research communities such as human-computer interaction (HCI), accessibility, and sonification for specific contexts. Of the two most common sensory substitutes-sound and touch-sound is by far the easiest to deploy since it does not require any specialized hardware. While there exist many examples of sonification and auralization <ref type="bibr" target="#b36">[37]</ref> (the use of non-verbal and verbal sound), these efforts primarily involve the blind community only as users or testers, and not as full-fledged informants or design partners <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> in the development team.</p><p>In this paper, we seek to add to the growing body of literature on the potential of accessible visualization design by understanding how blind individuals perceive the world around them using non-visual senses. Such knowledge would enable the design of more efficient audio representations of data that leverage the mechanisms that blind people already use in their everyday life. Our goal is to broadly understand <ref type="bibr" target="#b0">(1)</ref> how blind individuals perceive and retain sound and touch-based information so that visual layouts such as charts can be effectively translated into non-visual senses, and <ref type="bibr" target="#b1">(2)</ref> what this means for accessible visualization design so that future tools are robust enough to support multiple levels of visualization-related tasks <ref type="bibr" target="#b12">[13]</ref>. To this end, we conducted semi-structured interviews with 10 Orientation and Mobility (O&amp;M) instructors (experts), all of them blind, to understand how they teach their students (blind individuals) to navigate physical space using the sense of sound and touch. We chose this particular population because these O&amp;M instructors not only have significant personal expertise in leveraging non-visual senses to navigate the physical world everyday life, but also the knowledge and experience of teaching these skills to others. As part of the interviews, we also conducted discussions with these instructors on how to use sound and touch to convey data, and to translate visualizations. We found that blind individuals who undergo O&amp;M training engage in experiential learning <ref type="bibr" target="#b41">[42]</ref>-e.g., hands-on learning followed by reflection-to calibrate their minds to mapping sounds and tactile feedback to real-world aspects such as size, distance, angles, and position. We also learned how individuals actively use echolocation and environmental sounds to make sense of physical space, and the importance of sensory integration (e.g., combining audio and tactile feedback) in understanding it. Based on these findings, we derive design implications for accessible visualization design, with a focus on audio-speech and non-speech, and tactile representations, feedback and interactions. We also discuss the importance of training and usability of tools for accessible visualization for blind individuals.</p><p>The contributions of this paper are the following: (1) results from semi-structured interviews with 10 O&amp;M experts that convey how blind individuals perceive spatial concepts using sound and touch; (2) design implications for accessible visualization design with the idea of sensory integration of sound and touch; and (3) a design space on accessible visualization for blind individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Accessibility and HCI research commonly use sensory substitution <ref type="bibr" target="#b18">[19]</ref> techniques to convey feedback to people with visual impairments. Below we discuss prior work across research disciplines in the use of sensory substitution to aid in data analysis and visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mental and Spatial Mapping</head><p>Mental maps are cognitive constructs that are used to understand and explain the environment around a person to support spatial thinking and discussions <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b76">76]</ref>. These mental maps are stored as schematic representations <ref type="bibr" target="#b72">[72]</ref>, usually based on rectangular grid structures <ref type="bibr" target="#b42">[43]</ref>, and contains information related to objects, spatial relations between objects, landmarks, intersections, and route descriptions <ref type="bibr" target="#b43">[44]</ref>.</p><p>Research shows that blind and sighted people construct spatial maps in similar ways <ref type="bibr" target="#b57">[58]</ref>, and that blind individuals use a combination of sensory cues such as auditory, tactile, movement, and proprioception to perceive, store and recall spatial concepts <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b75">75]</ref>. Research shows that the "visual" cortex of the blind is activated to process other sensory modalities <ref type="bibr" target="#b49">[50]</ref>. More recently, Hersh <ref type="bibr" target="#b38">[39]</ref> conducted interviews with 300 blind and visually impaired individuals about perceiving spatial layouts, and identified that these individuals used their hearing, touch, and a combination of both to perceive space.</p><p>Orientation and Mobility (O&amp;M) training has been studied in the literature primarily to inform technology development to support O&amp;M training <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref>. O&amp;M training teaches perceptual and conceptual abilities to tackle indoor and outdoor navigation tasks. In fact, O&amp;M skills have been shown to be transferable between virtual environments to the real-world and vice-versa <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45]</ref>. This partly motivated our focus on O&amp;M instructors in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sound Perception and Sonification</head><p>Sonification is the use of non-speech sounds to convey data or information where data are mapped to sound parameters to generate sound <ref type="bibr" target="#b36">[37]</ref>. Auralization is the process of modelling and simulating the experience of sound in a virtual space <ref type="bibr" target="#b21">[22]</ref>. Audification <ref type="bibr" target="#b37">[38]</ref>, a type of sonification is the technique of directly mapping all data values one-to-one continuously to audio-samples. The aforementioned data mapping techniques have been extensively used by the International Community of Auditory Display (ICAD). In a typical auditory display, one may have multiple auditory dimensions (e.g., frequency or loudness of a tone) with each dimension bearing a light information load (data), or relatively more number of auditory dimensions, with each dimension bearing a considerably higher information load. The information being represented using sounds range from a low-resolution equivalent of images <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b77">77]</ref>; to system functionality and actions-auditory icons and earcons <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>; and abstract data such as temperature and pressure <ref type="bibr" target="#b74">[74]</ref>. Brittel <ref type="bibr" target="#b13">[14]</ref> presents a review of literature on sonification in conveying geospatial data by mappings to non-speech sound dimensions such as frequency and timbre, and to temporal characteristics such as duration and time.</p><p>Sonification using spatial sounds-2D or 3D, where the position of the sound source is modeled, helps blind and partially sighted users explore spatial layouts such as virtual city maps <ref type="bibr" target="#b47">[48]</ref>. Nasir and Roberts <ref type="bibr" target="#b56">[57]</ref> present a comprehensive review on spatial sound and sonification techniques that are beneficial in conveying spatial as well as non-spatial data (such as pie charts). The authors state that the complete potential of spatial 3D audio is yet to be explored. Duraiswami et al. <ref type="bibr" target="#b21">[22]</ref> present techniques for creating virtual auditory spaces to aid acoustic source localization. Geronazzo et al. <ref type="bibr" target="#b31">[32]</ref> present a spatial sonification system that enables audio-haptic exploration of virtual maps, and show that a 3D spatial audio and tactile combination outperforms just tactile feedback and tactile feedback with 2D audio.</p><p>Sound-based substitution has the advantage of being easily available in professional settings to blind individuals through personal computing and audio devices such as computers, smartphones, and headphones. However, there are disadvantages such as requiring extensive training <ref type="bibr" target="#b50">[51]</ref>, and varying auditory perception of individuals. Humans find it hard to distinguish different levels of a sound dimension (e.g., multiple frequency levels) and are better able to distinguish between different dimensions (e.g., between two tones with fewer segments across frequency and loudness) <ref type="bibr" target="#b59">[60]</ref>. Walker et al. <ref type="bibr" target="#b74">[74]</ref> experimentally compared sound mapping ensembles created by sound designers to be "Intuitive", "Okay", "Bad" and "Random"; and the "Random" ensemble resulted in the highest task accuracy over "Intuitive" or "Okay" as one would expect. Hence, it is crucial to empirically test the auditory display system with the intended user to define the most efficient mappings. We seek to better understand such variability in sound perception through interviews with blind O&amp;M instructors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Accessible Visualization using Sound and Touch</head><p>Visualizations are by definition visual in nature, so making them accessible to a blind audience is a significant challenge with facets, many of them social rather than just technical in nature <ref type="bibr" target="#b48">[49]</ref>. Non-verbal sound (sonification) and speech (auralization) have been used in place of visual representations. Early solutions used musical cues to convey shapes and graphs, but required visually impaired users to have musical knowledge <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b64">64]</ref>. While not directly related to visualization, audio and haptic feedback have been used to help people with visual impairments understand the structure of web pages representing buttons, links, and search features <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>. NASA researchers developed MATHTRAX <ref type="bibr" target="#b67">[67]</ref>, an interactive graphing software that sonifies mathematical data, functions, and equations for blind and low-vision students. Visualization-specific audio translations such as sonification of 2D data tables <ref type="bibr" target="#b63">[63]</ref>, line charts <ref type="bibr" target="#b14">[15]</ref>, shapes <ref type="bibr" target="#b30">[31]</ref>, bar charts <ref type="bibr" target="#b22">[23]</ref>, pie charts <ref type="bibr" target="#b27">[28]</ref>, and network structures <ref type="bibr" target="#b35">[36]</ref> have mapped audio notes to data values as effective non-visual chart equivalents. The aforementioned solutions were experimentally evaluated with blind individuals; with sighted or blindfolded individuals oftentimes included for comparison. While these chart specific solutions are effective, issues with training and usability due to lack of familiarity with audio dimensions still persisted. Sonification of axes and labels improve point estimation by providing contextual references <ref type="bibr" target="#b69">[69]</ref>-leading to improved graphical perception.</p><p>Tactile representations are effective in representing data and in translating visual representations such as maps <ref type="bibr" target="#b39">[40]</ref> and bar charts <ref type="bibr" target="#b71">[71]</ref> into touch-perceivable equivalents. Guinness et al. <ref type="bibr" target="#b34">[35]</ref> used miniature robots to convey data, and found that target acquisition was easier using tactile feedback as compared to sound. The sense of smell could also potentially be used as a complementary modality towards making visualization accessible for blind individuals <ref type="bibr" target="#b7">[8]</ref>. Zhao et al. <ref type="bibr" target="#b83">[83]</ref> created a tool that used both sound and speech to enable visually impaired users to explore maps and several other statistical data graphics. Computational methods have been used to extract semantic information that can subsequently be sonified or read aloud <ref type="bibr" target="#b25">[26]</ref>, from charts in applications such as accessible floor plans <ref type="bibr" target="#b32">[33]</ref>, to metadata added to charts generated in R <ref type="bibr" target="#b26">[27]</ref>. However, these solutions work only for charts authored in specific tools. Multimodal solutions using a combination of sound and touch have been shown to be more effective than using single modalities, and have inherently focused on chart translation, interaction, and authoring as well <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b84">84]</ref>. Web chart accessibility focuses on screen reader integration leading to solutions that integrate naturally into blind individuals' technology ecosystem <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b84">84]</ref>.</p><p>Much work has been done across communities such as HCI, Accessibility, ICAD, and cognition to make data and charts accessible. We believe that many of these solutions could make specific chart types or data accessible. As newer and more complex innovation in the visualization community grows, there is a need within the visualization community for frameworks or models to guide researchers and technologists to make their innovations accessible. For example, Brown et al. <ref type="bibr" target="#b15">[16]</ref> propose audio representation guidelines for graphs and tables, and Zhao and colleagues <ref type="bibr" target="#b82">[82,</ref><ref type="bibr" target="#b83">83]</ref> propose audio information seeking principles (AISP) for abstract data. In our work, we focus on bridging these disciplines, confirming results from and adding to past work, and broadly focus on sensory substitution for spatial understanding by interviewing experts from the blind community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>To better understand how blind individuals learn to use sound and touch to perceive and navigate physical space, we conducted semi-structured interviews with 10 blind Orientation and Mobility (O&amp;M) instructors.</p><p>Here we first provide background on O&amp;M training, present our study rationale, and then describe our data collection and analysis process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Orientation and Mobility (O&amp;M) Training</head><p>Blind individuals enroll in Orientation &amp; Mobility (O&amp;M) training to learn to become independent travellers. As part of their O&amp;M training, individuals are taught to use environmental cues to construct mental maps of the space around them. Orientation and Mobility instructors teach blind individuals-"clients" or "students"-to travel both indoors and outdoors, and to increasingly rely less on the visual sense. It is often assumed that blind individuals are a homogeneous user group, but research has shown that the attitudes, needs, and behavior of persons who are blind vary greatly. In addition to O&amp;M or Cane Travel, blind individuals are also able to enroll in programs such as Braille learning, Technology, Job Readiness, and Wood Shop Training.</p><p>Orientation and mobility experts receive National Orientation and Mobility Certification (NOMC), a certification that is offered by the National Blindness Professional Certification Board (NBPCB). Certified trainers teach under the Structured Discovery Cane Travel (SDCT) model; one that focuses on individuals acquiring non-visual travel skills through experiential learning based on personal experiences. The instructors teach concepts such as cane grips, mental mapping, environmental cues; and problem solving <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref>. Structured Discovery Cane Travel is one among two primary O&amp;M training models; the other one-Sequential Learning (SL)-is a medical model for rehabilitation that was designed in the 1940s for World War II veterans and did not allow blind individuals to become teachers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Study Rationale</head><p>While there are potentially many user groups to interview in order to understand sensory substitution mechanism, we chose blind O&amp;M instructors because they (1) have significant lived experience of using non-visual senses in perceiving space, as well as (2) are competent at teaching these skills to others, and have thus spent a significant amount of time retrospectively thinking about the skills. O&amp;M training is particularly relevant because these skills have been shown to transfer to other contexts and settings in prior work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>More specifically, visualizations such as maps, scatterplots, bar charts, and graphs rely on visual semantics such as shapes, size, color, position, labels, and axes to convey data to sighted individuals. Speaking to O&amp;M instructors is a reliable way to explore how to translate a visualization's visual semantics into non-visual modalities based on how blind individuals perceive visual semantics of space. Compared to blind sonification or tactile graphic designers who may use their own experiences, blind instructors have a broader view from their training and certification to teach other blind individuals. Prior work also shows that the intuition of sonification designers may not lead to the best data-to-sound mappings <ref type="bibr" target="#b74">[74]</ref>. Blind individuals are also taught mathematics and graphing primarily using tactile graphics such as embossed or Braille charts by Teachers of Students with Visual impairments (TSVI) <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b81">81]</ref>. TSVI focus more on tactile representations, while O&amp;M instructors focus more on teaching spatial understanding from sound and touch. Tactile charts have several limitations <ref type="bibr" target="#b81">[81]</ref> such as cost, information overload, and longer production time. While out of the scope of this work, we do think interviews with TSVI could lead to interesting insights on aspects such as chart authoring, and collaboration in classroom settings.</p><p>Ultimately, we hope to apply these sensory substitution insights towards accessible visualization design. Additionally, we believe that comprehending fundamental chart concepts such as reference frames, estimating distances, understanding angles, and other visual variables <ref type="bibr" target="#b9">[10]</ref> are similar to understanding and visualization navigation layouts and routes in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Participants</head><p>We recruited 10 blind Orientation and Mobility instructors to learn about their personal and professional perspectives on how blind individuals use sound to create mental maps, understand their surroundings, and navigate in a physical space. We focused on SDCT instructors because the self-confidence levels of SDCT students are higher than those from SL training.</p><p>Table <ref type="table" target="#tab_0">1</ref> provides an overview of the participants. Two of our participants were also itinerant trainers who would visit their students at their preferred locations, while most of our participants conducted classes at institutions. Participants taught students as young as 5 years and as old as 70 years; and also students with other disabilities. OM4 was also a certified TSVI. Overall, the participants had a strong expertise in teaching their students to travel using non-visual skills.</p><p>Participants were recruited through mailing lists associated with the National Federation of the Blind (NFB). The study was approved by our university's Institutional Review Board, as well as the Research Advisory Council of the NFB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Collection and Analysis</head><p>We conducted semi-structured interviews via video and audio conferencing on the internet to collect our data. We then transcribed and analyzed the resulting data using thematic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Semi-structured Interviews</head><p>Each semi-structured, audio-recorded interview was scheduled for 60 minutes, and participants-as rehabilitation experts-were compensated with a $100 Amazon gift card. Broadly, the research goal was to understand the design space to help design and build tools that support This could mean making data representations such as visualizations or charts more accessible, or finding new ways of representing data in using sound-both speech, and non-speech.</p><p>To understand O&amp;M training procedures, we asked participants how they trained blind individuals to rely on sound for creating mental maps of the environment as well their use of auditory interfaces in activities of daily living. Secondly, we asked the participants how blind individuals perceive and infer different aspects and properties of sound, such as loudness, position and direction, pitch, repetition, moving vs. static sounds, and verbal sounds (speech). Finally, to brainstorm about the idea of translating a virtual and visual layout (a visualization) into an audio representation, we introduced our design idea to foster discussion and receive feedback. Participants were introduced to the idea of a web-based interface that allows users to upload a chart image; the tool will then extract text labels, data, and other semantic information from charts-for example, bar charts, scatterplots, maps, and line charts. Next, the tool will translate the visual elements and data into audio representations by simulating spatial audio <ref type="bibr" target="#b28">[29]</ref>. Users can interact with the sounds using their keyboard or touch screen. The O&amp;M instructors were not data visualization experts, but this part of the session guided participants to discuss their familiarity with audio and tactile feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Transcription</head><p>The audio recordings of the 10 interviews-10 hours and 38 minutes, were transcribed using an online service-Rev <ref type="bibr" target="#b2">[3]</ref>. On average, each interview lasted 64 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Analysis</head><p>We used thematic analysis <ref type="bibr" target="#b45">[46]</ref> to open code the transcripts. We started by randomly selecting two transcripts to be open-coded by two researchers. In other words, we each separately tagged text from the transcripts with multiple codes (see below examples) to add semantic structure to our data. After open coding, the two researchers discussed and merged the codes to create an initial codebook. The merging process included a discussion of rephrasing codes, adding missing codes, removing codes that resulted in a codebook that was agreed upon by both researchers to improve reliability of our results. Next, one researcher coded the remaining transcripts using the initial codebook, and added codes as they emerged.</p><p>Some examples of the codes that were used are: "O&amp;M concepts", "Indoor navigation", "Residual vision use", "Multiple sounds","Technology use in O&amp;M", "Embodied Cognition", "Prior chart knowledge", "Sound Isolation strategies and challenges", and "Sound Mapping and Inference." As new codes emerged, we returned to older transcripts to apply the new codes. Overall, 89 unique codes were used, and 258 excerpts were extracted from the coding process. Our codes are included in the supplementary material. Results from our analysis have also been reviewed by one of the co-authors who is blind, and has long experience in information, data, and knowledge work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FINDINGS</head><p>In this section, we present three main themes that broadly describe (1) Orientation and Mobility concepts that reveal how blind individuals perceive elements and layouts of space using non-visual senses, (2) how individuals interact with space using sound and touch, and (3) the challenges of using non-visual sense in using visualization. Throughout this report, we highlight insights derived from this process as follows:</p><p>Insight #1. Orientation and Mobility training highlights many audio and touch affordances that may be useful for creating accessible representations of data and visualizations.</p><p>The color coding for these boxes signify whether they arise from perception, interaction, effectiveness, challenges, or design guidelines. We discuss the implications of our insights in detail in section 5, but briefly explain how our insights relate to visualization in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Perceiving Space as a Blind Individual</head><p>O&amp;M training does not follow a prescriptive approach. Instead, it generally encourages Socratic questioning to help students associate their own meaning to various environmental cues. When asked specifically about different properties of sound, all participants mentioned that while certain properties can have an objective meaning, it is very hard to prescribe a particular threshold when considering the magnitude of these properties. For example, while loudness of sound might increase as someone walks closer to a sound source, estimating precise distances is still challenging. Additionally, participants noted that there is a lot of variability among students in terms of perceiving different sounds.</p><p>Here we describe specific instances of using different properties of sound and touch in the non-visual sensemaking of physical space.</p><p>Participants described the white cane as the primary tool used in the orientation and mobility process. The non-folding version is recommended by the NFB, and is the primary one used in the O&amp;M training process. Other canes, such as folding ones, are also used by blind individuals, but are not recommended because of the limited haptic feedback that they offered compared to the non-folding cane. Such haptic feedback is critical; the cane was described as an "extension of the self," and cane techniques help students actively interact with the environment to receive sensory information. For example, participants described "shorelining" as a technique used by students to understand their position on the sidewalk by walking in "parallel" using the grass or the edge of the sidewalk as a "reference." Insight #2. Many white cane interaction techniques help blind individuals receive haptic feedback while traveling.</p><p>Additionally, a sweeping motion of the cane on the ground can also convey tactile information to the students based on the continuous haptic feedback that the hollow stem of the cane transfers to the individual's hands. OM4 explained how texture changes in the aisles of a grocery store are perceived during indoor navigation: "There's a texture change when you're in front of the grocery aisle or the refrigerated section, or even the fruit and vegetable sections. Because, that's a little bit rougher, the texture is a little rough." By leveraging the familiar metaphor of actively probing to understand a surface, tactile solutions could strongly couple interaction and representation to improve accessibility.</p><p>Participants indicated that the sounds, especially distinct sounds, produced by certain objects in the environment, such as lawn mowers, HVAC (heating, ventilation, and air conditioning) systems, streetcar, and traffic sounds, provide many clues about the environment around them. These ambient sounds, when mapped with concepts that students have stored in their memory, assist in the process of navigation and understanding the shape or layout of the surrounds, especially when walking into unfamiliar routes or buildings. OM6 explains: "I think there's also another thing which is the knowledge that a person has accumulated over the years. When I walk into a high-rise building, there's really only a couple possible places that the elevators could be located. And so I'm able to use that knowledge to help me figure <ref type="figure">out</ref> where I need to go to find the elevator.". Chart literacy can be challenging for sighted individuals, and is even more for blind individuals if they are unable to perceive the concepts used in highly customized bespoke charts-i.e., non-standard charts. The importance of concept building indicates a need to effectively represent geometric units used in charts through non-visual senses.</p><p>Insight #3. Sounds known to individuals from lived experiences help build spatial awareness.</p><p>Many participants mentioned that the loudness is usually associated with the distance between the student and the sound source. For example, OM7 mentioned that they talk to their students about loudness, where a change in loudness indicates changing distance between the two reference points-the individual and the sound source: "But I'll ask them, "The sound that you hear, does it seem quiet or does it seem louder to you?" [...] And if they say,"Oh yeah, it seems quieter." Then I might say, "Why does it seem quieter?" [...] And if they honestly cannot make that association, then what I will do is have them walk toward the sound and ask them what's happening to the sound as they approach it". One participant also indicated that students perceive bigger objects, such as a truck, to be louder than smaller objects, such as a car, indicating that the loudness may also be associated with size.</p><p>Insight #4. Loudness is commonly used for perceiving spatial distance, and is also associated with size of objects producing the sound.</p><p>Not many participants mentioned the pitch of a sound being directly mapped to a particular physical property, but indicated that higher pitched sounds were easy to isolate from other environmental sounds. The pitch of the sound was associated with familiar concepts such as the sound of a car engine: "If a car is idling, I will point out to the student that when the car is shifted into the reverse, the pitch of the engine goes lower because the engine is laboring and that's something to be aware of because that car could come backing out and you don't want to be in the way." (OM6) Insight #5. Pitch is used to recognize known objects and their state; higher pitched sounds are generally easier to isolate from other environmental sounds.</p><p>OM7 mentioned that their students were cognizant of the "sound space" around them, and interestingly described certain aspects using visual space. When students are immersed in auditory environments, multiple sounds, with their different dimensions occupy the auditory capacity. OM7 describes the visual space in front of the students, indicating that sounds are mapped to visual space: "They should tell me that the sound not only gets louder, but it takes up more of the sound space. It takes up more space in front of them. Like a fountain in the distance, it will sound quieter, but it won't take up quite as much of the stereo space in front of them, in the same way that a distant fountain won't take up as much visual space." Insight #6. Individuals map visual space by sampling the various sounds of the environment and interpreting changes in sound dimensions such as loudness.</p><p>Participants also mentioned that the sequence of sounds and the duration of a particular sound were important features to interpret to understand sound. Similar sounding tones when played sequentially can also be distracting: "So as long as there's a clear enough duration in between, like if you're going to play a sound, you want there to be a gap in between.[...] So if they just get distracted for a second, they might miss that there was two clicks instead of one." (OM8) Insight #7. The sequential nature of sound dictates that sampling frequency and duration be optimized to improve accuracy of spatial awareness.</p><p>Participants perceived the absence of echoes or sounds, and associated the presence of large objects at a certain distance causing sound to be blocked and creating "sound shadows"-another visual concept. OM6 describes this phenomenon as follows: "An individual that has well-developed listening skills can hear those echoes off of telephone poles and even sometimes off of sign poles, depending on conditions. The person can also hear sound shadows from objects in the environment where they're blocking out the sound of maybe a car passing by. They hear a moment where it's blocking out the sound."</p><p>Insight #8. The absence of sound-sound shadows-could be interpreted as being caused by intersecting objects in space.</p><p>The soundscape, i.e., the sound space around the individuals, often consists of multiple sound sources at different positions; some louder than others, and at different pitches. Participants mentioned that one of the important skills that students learn is "sound isolation." Based on the problem or task at hand, individuals needed to focus their attention on certain sounds, and oftentimes these relevant and necessary sounds would be occluded by environmental sounds.</p><p>Prior work on effectiveness of data-to-sound mapping recommends empirical assessment based on end users and the data task <ref type="bibr" target="#b74">[74]</ref>. Our findings about loudness, pitch, and lack of sound being mapped to spatial or geometric properties could be a starting point for accessible sound representations since they are based on blind individuals' mental models. The idea of sound shadows has not been explored in prior work, and could be an interesting design material for audio charts in the future.</p><p>Insight #9. Individuals isolate and focus their attention on specific sounds in a soundscape based on the task at hand. Participants also mentioned that some important sounds such as the "buttons at the (road crossing) intersection" sounded very similar to environmental sounds such as the sounds of "birds chirping." The ability to isolate different sounds when needed was a skill that depended on the the person because "people have (had) different levels of hearing ability, and then being able to discriminate [...] used judgement on the cues that they're getting." (OM3)</p><p>Participants mostly agreed that familiarity of sounds make sound isolation much easier. Participants also describe how triangulation is used to establish their position with more confidence: "You usually have to have more than just one source of information to make a solid deduction as to where you are. You might need to feel the direction of the sun at that particular time of day, hear where the sound is coming from and maybe some other sound off in the distance, maybe traffic off in the distance somewhere tells you." (OM6) Insight #10. Familiarity as well as dissimilarity of sounds in a soundscape help an individual to switch their hearing focus.</p><p>Participants mentioned that "sound localization" is part of the concepts covered during training, and is a fundamental concept that "can be taught or is a learning curve". However, there are variations among students' efficiency in localizing sound. This could be because of students' hearing impairments, when present, that causes "bilateral imbalance in their hearing." (OM3) Participants indicated that static sounds can be used as landmarks or reference points. This indicates that sound localization, while being a fundamental human feature, can be "highly dependent on a person's ability to discern an angle of sound." Whether a sound is static or moving, also influences how much an individual can sample and associate meaning: "[...] when you have a static sound you can use that specific sound for orientation purposes, right? Say that sound's been occurring at that location for the last several minutes, so why don't we use that as a point of reference as opposed to a moving sound?" (OM1)</p><p>Prior work shows that sonification of chart axes adds context to audio charts <ref type="bibr" target="#b69">[69]</ref>. Since static sounds are useful as references, accessible design could consider metronomes or always-present sounds to convey chart boundaries, legends, or even scales of the axes.</p><p>Insight #11. Estimating positions of and distances between sound sources varies greatly between individuals based on hearing and spatial awareness, with static sounds being most helpful.</p><p>For sighted individuals viewing a chart, their eyes move rapidly to perform actions similar to auditory actions such as filtering, scanning, differentiating, etc. But with hearing supporting lower information bandwidth, careful accessibility design may be needed to translate charts showing a large amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interacting with Space as a Blind Individual</head><p>Overall, participants indicated that the white cane helped in interacting with the environment as an "extension of their body" and acts like an "antenna." Participants noted that canes with a metal tip encourages the use of "auditory information" (OM4), whereas folding canes tend to use a plastic tip that does not provide the same auditory feedback (OM4). The cane helps produce a crisp sound that bounces off objects around the students. The echo that is produced conveys different aspects of the environment such as distance from buildings, number of objects, and wide versus narrow spaces. The duration of the echo also conveys meaning to the students. OM8 explains echolocation as follow: "So an example is when they're in a parking lot looking for a building, I would have them tap at various parts of the ground in front of them, they might tap at nine o'clock first, then 12 o'clock and three o'clock, based on the time it takes for the echo to come back to them, then that will tell them whether there is a building in that particular direction or not. <ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>...] If the building is within 15, 20, 25 feet away from them where the echo would return, then they'll quickly hear echo back, and the length of the echo varies depending on how far the object is."</head><p>Insight #12. Blind individuals often use echolocation to understand space around them.</p><p>In general, participants mentioned that their students visualize objects and space using their own body as reference. For example, participants described the positions and direction of objects using terms such as "eye-level", "front", "back", "left side", "right side" and "above." This indicates that, considering 3D space, perspective appears to be from the point-of-view of the individual's body or as extension of the body in the form of the white cane. All participants mentioned the use of "cardinal directions" by their students during travel. Many participants also described the use of the sun's position by perceiving the direction of heat to gain an understanding of both time of the day as well as understanding the cardinal direction they were traveling.</p><p>Insight #13. Blind individuals often interact with sound from a perspective that is relative to different parts of their body.</p><p>Our finding on egocentric sound perception suggests that future work on accessibility could explore egocentric perspectives for both sound and touch-based solutions. While this has been explored in pie chart sonification with sighted individuals <ref type="bibr" target="#b27">[28]</ref>, our findings highlight the value in exploration with blind individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness, Information Access, and Usage</head><p>The findings above draw primarily from discussions about the idea of transferring sound and touch perception skills acquired from O&amp;M training towards accessible sensemaking and charts. Participants were divided on using sound alone, and many participants recommended the use of both tactile and sound-based representations. Participants noted that many students are familiar with using "tactile maps" to understand spatial layouts as compared to sound. Some of these participants opined that "touch and sight are more comparable than sound and sight" (OM2), and also believed that "touch is (was) probably a quicker avenue to the brain" (OM3).</p><p>Insight #14. Tactile representation allow haptic feedback, which is more comparable to vision than sound, and can help people to gain an overview of the canvas.</p><p>While tactile representations were preferred owing to familiarity, the idea of audio charts could succeed with enough training: "Or for those who become blind, some of them already have that experience through their visual sense. It's just a matter of getting them to calibrate it to their other senses. And that's also quite doable. We know that the brain is plastic. So over a period of time, if given the correct education, the brain can rewire itself to accept auditory, tactile, and proprioceptive input and spatial input." (OM7) Insight #15. Sound representations can be made to work with sufficient training and calibration of sound perception.</p><p>Participants provided positive feedback about the idea of audio charts, and also provided specific feedback to make sound perception easier for blind individuals. OM8 recommended that "audio descriptions" are the best way to provide spatial awareness by giving an overview of the canvas. OM5 also recommended using speech to convey specific information such as numbers because from their past experience, audio charts that use non-speech sounds are limited by the inaccuracy of mapping sounds to numbers: "It's not that they couldn't give me comparative information, in other words this sound is lower than this other sound, but that they couldn't give me the specific information, if that makes sense." Insight #16. Non-speech sounds are more suitable for comparison while speech is preferred for conveying specific quantities.</p><p>Overall, participants recommended pursuing the idea of sensory integration as "haptics could enhance that whole sound interpretation'.' A major reason for preference of tactile charts stemmed from prior experiences. OM8 explained that their institution uses an "embosser like a Braille printer" to explain scatterplots as "there is nothing in a screen reader that can read those (chart) types of data, unless there's a description of it." Insight #17. A combination of both sound and touch-sensory integration-best supports real-world analytical tasks.</p><p>Participants recommended using haptic feedback in addition to audio descriptions: "something that they (students) can feel underneath their fingertips as well, something that they can explore tactically, i.e., using touch." Haptic feedback was also described to be useful as interaction feedback; OM6 explained with an example of traversing a bell curve: 'If you have a chart displaying a bell curve, having maybe vibration or something that indicates to the person that they are following that curve with their hand so that they feel the actual curve, in addition to whatever information you might be able to provide, might be helpful in being able to use that chart effectively." Our findings on the need and effectiveness of using multimodal solutions are in line with prior work in the sonification and accessibility disciplines.</p><p>Participants mentioned that students perceive charts or visualizations, especially maps, as a 2D construct. Many individuals may have seen charts before becoming blind, or have "felt" charts in the form of tactile representations. Many participants indicated that students might know the concepts of "Axes", "Labels", and "Marks," and may also be aware of specific visualizations such as bar charts, line charts, and pie charts. However, participants also recommended a phases of understanding "how well that (blind individual) person's geometric concepts are" (OM6), and that designers and researchers make an attempt to "absolutely have to have the person understand what a chart is... they have to feel it, because otherwise it makes no sense." (OM5) Insight #18. Many blind individuals are familiar with charts and its elements as 2D constructs, but accessible visualization design should still strive to explain chart elements.</p><p>When asked to describe the information that may be important to convey, participants highlighted the need to convey "individual layers of information and the person can go through one layer at a time or they could overlay different layers of information, so that they can be in control of what they're trying to understand." (OM6). OM8 discussed how the complexity of the chart, stemming from the amount of data could lead to information overload, while also indicating limitations of tactile representations: "But the problem is, if there are five things on the scatterplot, you can easily fill all five dots, and get that data from there. But if it goes any higher than that, if there's things that start coming together, the more information that is on there, the more inaccessible it becomes." Insight #19. Sensory and information overload is a major usability challenge when using non-visual senses to understand charts.</p><p>All participants recommended that, regardless of sound or haptic representations, solutions try to reduce the learning curve for individuals, by incorporating prior chart knowledge. Finally, participants mentioned that accessible design should also focus on creating and retaining a shared understanding of charts between sighted individuals and blind individuals, as this becomes important: "especially in a work environment, but also probably in a school environment. It's the ability to communicate the understanding of the chart, <ref type="bibr">[...]</ref> , because what we've got to do is communicate that chart, not some kind of a different chart, because it's that chart that the sighted world is going to use. And in most of the companies that I've ever worked in, in my life, I was the only blind person there, so what was useful to me, if it wasn't useful to other people on my team, or in the company, then it wasn't going to be useful. In other words, we live in a sighted world, and if we live in a sighted world then we have to figure out how am I going to communicate the information I have to other sighted people in a way that makes sense to them, and if you say, ''well, that seems unfair that you should have to do that''-too bad." (OM5) Insight #20. Solutions towards accessible visualization should not introduce new barriers for collaboration between sighted and blind individuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Towards Accessible Charts: Needs and Challenges</head><p>O&amp;M students tend to have varying levels of visual and other abilities while partaking in the training. For example, students may have some amount of residual but depleting vision, whereas some have no residual vision at all. Additionally, some students may have congenital blindness (from a young age) or may have become blind at a later stage in their lives. Finally, students may also have other disabilities, such as hearing and/or cognitive impairments along with vision loss. Our participants stressed that each individual, while all have some amount of vision loss, have varying abilities. In the case of students with hearing loss, participants did encourage the use of residual vision (if available), but "tweak the training to encourage them to use their vision in a realistic way <ref type="bibr">[...]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>but as far as just aligning properly and possibly seeing the color of the light, if they can see that." (OM2)</head><p>Insight #21. Individuals have varying levels of visual acuity, familiarity with charts, and other disabilities that require personalized designs for accessible visualizations.</p><p>All of the participants mentioned the use of "sleep shades" to occlude any residual vision of their students to ensure that they do not resort to using their weaker sense to travel. The use of sleep shades is central to the framework of Structured Discovery Cane Travel, and participants indicated that relying less on vision during the training helps optimize the use of residual vision after training. OM3 explains:</p><p>"From a practical point of view, it doesn't make sense to start with the weakest sense. We know people will use their vision, and [there is] nothing wrong with that. But the idea is to develop mastery through the other senses, and then when vision is introduced after the training [...] In my opinion, by developing those alternative skills, using non-visual skills and techniques, actually helps optimize the use of vision." Insight #22. O&amp;M trainers prioritize using only non-visual sense during training to avoid students using their weakest sense (residual vision) in sensemaking.</p><p>The primary goal for a student or client partaking in O&amp;M training is to rely less on the visual sense while navigating the physical space around them. In addition to cane travel, participants mentioned that their students are taught to rely less if not eliminate their dependency on the visual sense to orient and navigate the environment. Instead, participants mentioned that students predominantly were taught to rely on their sense of sound and touch to navigate the physical space around them. While cane travel was the primary outcome of the training process, participants also indicated that the emphasis on learning to "completely use non-visual skills" (OM1) in tasks such as "decisionmaking," "route planning," and "route navigation." Insight #23. O&amp;M training familiarizes students with other analytical abilities such as planning, decision making, and navigation which indicates the importance of training and usability for accessible visualization systems.</p><p>Many participants explained about the importance of assessing different aspects about their students such as the level of vision and hearing, the amount of nervousness, and their travel routes and destinations, with the aim of building more confidence and effective decision-making and problem solving skills. This is especially important when students practice outside of the training classes without the instructors. In such circumstances, students need to be comfortable and confident in applying skills learned during training. OM5 summarizes these needs: "So, initially, it's a great deal of finding out what people do [...] Probably getting a real understanding of the student initially is really important to me, because I want to make sure that whatever I'm teaching them is something that, A) they're going to be able to understand, and B) that they're going to use it. ". Insight #24. O&amp;M trainers measure success not only by assessing navigation tasks, but also by assessing confidence levels and levels of comfort in relying on non-visual senses.</p><p>All our participants emphasized that instructors generally are aware of the differences in abilities, needs, and prior knowledge of their students; requiring them to "minimize prescriptive learning" (OM1). Instead, participants mentioned that O&amp;M training is more of an "experiential and incremental learning" experience. The process involves many "repetitions" which aids the students in "calibrating" their senses, and potentially updating their sensory memory. OM3 describes the role of "Socratic questioning" in a typical lesson: For example, in a typical lesson, I don't provide a lot of answers. If a student says, ''am I going the right way?'', we use Socratic questioning, meaning we ask them, ''well, tell me about what you are sensing to whether you think you're going the right way.'' And we'll help them through that, but help them think through the process in both multi-sensory, but probably more important developing self confidence through that problem solving strategy.". Insight #25. Individuals undergoing O&amp;M training calibrate their non-visual senses through incremental learning and practice.</p><p>Our insights from this section strongly indicate the need for accessibility solutions to define success in terms of task completion, confidence in accurately perceiving charts and data with low uncertainty. Prior sonification and accessibility work acknowledges the importance of training <ref type="bibr" target="#b50">[51]</ref>. Our findings additionally highlight other aspects such as including calibration and assessing confidence during the learning. This becomes especially important as the bandwidth of the sound and touch senses are lower compared to vision.</p><p>Participants emphasized the need for their students to perceive and gauge the physical properties of objects in both indoor and outdoor environments. For example, during outdoor navigation, objects such as "buildings," "people," "vehicles," and "parking lots" were described to be essential features of mental maps. In indoor navigation, objects such as "chairs," "walls," "doors," and "electronic appliances" were important references. Each of these objects could be "static" or "moving objects", had a set of physical properties such as "shape," "size," "texture," "height," and "position." In addition to the aforementioned properties, each of these objects could also have characteristic sounds and tactile properties that provided students with the ability to orient themselves, and construct mental maps of the space around them. Other spatial concepts include "intersections," which indicated a sense of visualizing "perpendicular" streets or hallways. Students also actively perceived "distances" using units such as "city blocks" and "steps." Insight #26. Awareness and building of concepts related to indoor and outdoor navigation are crucial to perceiving and interacting with physical space using non-visual senses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this paper, our aim was to make inroads towards supporting the millions of blind and visually impaired users for whom data exploration, especially large-scale data exploration, remains inaccessible. Data visualization and visual analysis, with its principles, techniques, and tools, continue to introduce barriers for the blind community. Here we describe design implications that we believe will guide visualization designers, analysts, data scientists, programmers, and researchers alike towards accessible visualization and visual analysis.</p><p>Overall, the findings validate previous research <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref> that sound and touch play a very important role in perceiving spatial layouts. Additionally, the findings also confirm that a combination of senses are used to perceive the environment, and familiarity of tactile charts could indicate that haptic feedback and tactile information may be necessary to effectively use sound-based representations. Many of our findings converge around the importance of interaction with non-visual environmental cues. While building accessible solutions, perhaps there are two levels of interaction using non-visual senses that need to be supportedinteraction with non-visual representations, and interaction with the data itself. Visual interactions in visualization and visual analysis have been extensively studied in the visualization community <ref type="bibr" target="#b79">[79]</ref>, and for accessible solutions the focus on interactions is more important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design Considerations for Accessible Visualization</head><p>Our most fundamental finding is that it makes sense to use visualization as an intermediary for sonification. In other words, instead of sonifying the data directly, the visualization can serve as an inspiration for how to spatialize the data in the sonified representation. Our findings indicate that blind individuals tend to already be aware of the visual structures used in common charts from lived experience, even if they were born blind. Rather than entirely discarding existing visual structure in favor of new structures based on alternate senses, we recommend focusing on better translation of visual structures such as marks and encoding. This means that the term "accessible visualization" is not misnomer. The approach can also facilitate collaboration and communication between sighted and blind individuals in data analysis. The importance of such visual semantics for blind and low-vision individuals has also been confirmed in other work <ref type="bibr" target="#b60">[61]</ref>.</p><p>D1. Retain visual structures used in common charts, such as bar charts, pie charts, scatterplots, and line charts, for other media.</p><p>Our participants could only speculate about how sound and touch could be used in data analysis and translation of visual representations. However, the findings provide a detailed overview of how sound is mapped to different spatial and geometric properties such as size, shape, position, and distance. In visualization, these spatial or geometric properties are considered "visual variables" and are used to construct and classify visualizations <ref type="bibr" target="#b16">[17]</ref>.</p><p>Another design guideline is to consider using a combination of sound-both speech and non-speech-and touch modalities to complement visualizations. There is limited research on using sound and touch together in visualization, but designers and researchers can review a rich body of research in accessibility and HCI to identify how to build interactions using a combination of sensory modalities <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">53]</ref>. Future work should explore how to effectively map visual variables to appropriate audio <ref type="bibr" target="#b37">[38]</ref> and haptic variables.</p><p>D2. Use a combination of sound-speech and non-speech-and touch modalities to translate visual and interactive representations.</p><p>To a large extent, sighted individuals can gain a complete overview of the chart canvas simply by looking at the canvas. However, the sequential nature of sound and need for sampling in audio-based sensemaking indicates that gaining an overview of data from a soundscape is harder without active interactions with sound. If charts are translated to audio, interactions may only be with a subset of the visualization at any given time, and blind users may not gain an overview instantaneously.</p><p>D3. Supporting interaction is more important for non-visual sensemaking than visual sensemaking.</p><p>D4. Clearly distinguish interface-related modal feedback from data-related modal feedback.</p><p>In O&amp;M training, "concept building" and calibration of non-visual senses is central to the process of learning to perceive the environment using sound and touch. The findings also indicate that blind individuals are not a homogeneous group, and have different levels of perceptive abilities. While sound and touch as modalities of feedback are perceivable by blind individuals, we believe that prescriptive mapping data quantities or chart elements to audio and touch modalities will not work for blind individuals. We recommend that systems translating visual elements to other modalities include a training module, and settings to adjust levels of audio or haptic dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D5. Allow customization and calibration of modal encoding to effectively support accessible graphical perception.</head><p>During O&amp;M training, blind individuals perceive audio and haptic feedback knowing that they are in a 3D space. In addition to passively absorbing modal feedback, these individuals use human echolocation techniques to actively interact with their space. When designing audio and touch interactions for accessible 2D charts, their position relative to the visualization planes and layers needs to be clearly understood. Stereo sound <ref type="bibr" target="#b82">[82]</ref> has a clear mapping to a 2D canvas, but perceiving height remains difficult owing to limitations of the human ear. Using spatial audio, in the form of binaural audio <ref type="bibr" target="#b66">[66]</ref> or ambisonics <ref type="bibr" target="#b28">[29]</ref> provides an encoding space for a 3D dimension, but the user needs to be aware of the orientation of the chart canvas. D6. When using sound, clearly convey mapping between spatial dimension (2D or 3D) to the soundscape.</p><p>Research also demonstrates that using haptic and audio feedback allows blind and visually impaired individuals to effectively understand geometric and spatial concepts such as circuit diagrams <ref type="bibr" target="#b19">[20]</ref>.</p><p>D7. When possible, use tactile representations and haptic feedback to provide chart overviews.</p><p>Improving chart decoding <ref type="bibr" target="#b17">[18]</ref>, exploring natural language generation to create audio descriptions <ref type="bibr" target="#b61">[62]</ref>, and using natural language interfaces <ref type="bibr" target="#b55">[56]</ref> could also significantly improve the accessibility of visualization systems. D8. Consider using automation and natural language to generate and convey insights.</p><p>We found that blind individuals' analytical goals are no different from sighted individuals. One of the main challenges described by the participants pertained to cognitive overload from excess information being "visualized" in charts. Some participants described a layered approach to conveying information to overcome this challenge. This is similar to the paradigm of Shneiderman's Visualization Mantra <ref type="bibr" target="#b68">[68]</ref>: Overview First, Zoom and Filter, and Details on Demand. In the sound domain, prior work discusses Audio Information Seeking Principles <ref type="bibr" target="#b82">[82]</ref>, which are again related to understanding and serving different levels of user intent. For example, just like for a visualization, overview is going to be important also in a sonification. Additionally, it is important to understand the role of sound and touch in different levels of visualization tasks <ref type="bibr" target="#b12">[13]</ref>. Finally, our experts recommended that knowing the current position on the chart being explored is very important for individuals as they "zoom" in and out of the translated charts.</p><p>D9. Avoid sensory overload by using interactions to view layers of information-both raw data and sensory.</p><p>To improve usability and perception, principles such as ability-based design <ref type="bibr" target="#b78">[78]</ref> can be adapted while building tools for chart accessibility. Accuracy of data and insights are important aspects of accessibility. Our participants also echoed concerns about the unfamiliarity of audiobased charts, and recommended that we design solutions that are "easy to learn" and support "all" data needs. D10. Clearly define and evaluate usability, and support various levels of analytical tasks.</p><p>Finally, incorporating sociotechnical considerations into the research process will help overcome issues related to technology costs, simplistic solutions, and inaccurate assumptions <ref type="bibr" target="#b48">[49]</ref>. In particular, consider involving people with disabilities in your visualization work-as we did in this study-to better position it with regards to accessibility <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b70">70]</ref>. The intention is to avoid poorly researched, poorly documented, and poorly maintained artifacts that may lead to unrealistic, unsustainable, and impractical solutions for our users.</p><p>D11. Consider socioltechnical factors to build sustainable, holistic, and cost-effective solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Example: Accessible Bar and Pie charts</head><p>Our goal in this paper is to provide a design framework to guide researchers in the visualization community to design accessible charts and visualization tools. Here we demonstrate how our framework of design guidelines and insights can be applied to bar charts and pie charts. We do so by adopting the notion of auditory sweeps <ref type="bibr" target="#b82">[82]</ref>: a spatial traversal of a visual representation indicating the order the visual data will be sonified. For example, a horizontally-aligned bar chart-one where the bars are arranged on a common horizontal axis-typically uses a horizontal auditory sweep: values for each are sonified in the order left to right. Analogously, a more complex 2D representation such as a geographic map or a scatterplot could use different sweeps, such as zig-zagging, up-to-down, left-to-right, etc. Auditory sweeps have also been adapted for web charts, such as in Highcharts <ref type="bibr" target="#b0">[1]</ref>, and more recently for touchscreens, such as iOS audio charts <ref type="bibr" target="#b1">[2]</ref>. Below, we demonstrate how one can use our design principles in conjunction with past sonification literature to pie charts and bar charts.</p><p>Bar and pie charts can be decomposed into building blocks such as the drawing plane, reference axes, and the visual marks representing quantity <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>: position for bar charts, and angle for a pie chart. To be able to create a sonified representation of these 2D charts, we thus need to be able to represent the space, axes, and marks using sound.</p><p>Overview. Overview is a central task in data visualization <ref type="bibr" target="#b68">[68]</ref>. To support this task in sonification, we use auditory sweeps that present data by "sweeping" over the visual space: from left to right for a barchart, and in a circular motion from 12 o'clock for a piechart. We thus retain the visual construct of the original visualization (D1).</p><p>Marks and Channels. Our insights can also be used to map visual channels to sound dimensions. For example, timbre can be mapped to different colors <ref type="bibr" target="#b13">[14]</ref>. While perception varies across blind individuals, pitch is often used to recognize and distinguish objects from other sounds (Insight 5). Static sound helps estimate relative position (Insight 11) between sound sources and spatial audio provides references with respect to the user's body (Insight 13). Furthermore, the dissimilarity of sounds, especially non-verbal ones, helps them switch focus across sounds (Insights 10 and 16). These insights may be used to convey individual marks by separating them in space.</p><p>The above example is not a comprehensive solution, but only a design sketch. For existing examples of sonification, please see High-Charts <ref type="bibr" target="#b0">[1]</ref> or iOS audio charts <ref type="bibr" target="#b1">[2]</ref>. However, for many existing sonification tools in the marketplace, it is unclear whether blind individuals were involved in the design and development process. We recommend that future work in this space actively involve blind individuals, ideally as design partners <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND CONCLUSION</head><p>Our findings are based on a limited sample of 10 blind O&amp;M instructors. We make an assumption that the expertise in training multiple blind individuals validates and adds weight to the opinions of the participants. Additionally, the feedback from the visualization brainstorming portion may not capture the opinions of students as participants mainly conveyed their own perspectives. Finally, blind individuals who do not have O&amp;M learning experience, or those who do not experience the SDCT model of teaching, may have different opinions and perceptive abilities when considering sound and touch.</p><p>We have reviewed prior work on sonification, HCI, and accessible visualizations, and found insights that can guide researchers interested in building accessible visualization solutions. To avoid a technologycentred approach, we engaged the blind community in our work by interviewing and discussing chart accessibility with 10 blind O&amp;M experts. We found that touch modalities are more comparable to vision, and blind individuals are more familiar with tactile charts. We also found that complementing visualizations using a combination of touch and sound can lead to more holistic solutions. Finally, we identify key insights and discuss accessible visualization design considerations to guide technologists and designers interested in developing solutions for blind users. We hope our work will lead to more research on accessibility for blind individuals in the visualization community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Demographics and O&amp;M experience of the 10 participants. Abbreviations: M-Male; F-Female; NB-Non-binary.</figDesc><table><row><cell>ID</cell><cell cols="3">Age Gender Education</cell><cell>O&amp;M Experience</cell></row><row><cell>OM1</cell><cell>30</cell><cell>M</cell><cell>High school</cell><cell>3 -5 years</cell></row><row><cell>OM2</cell><cell>32</cell><cell>M</cell><cell>Master's degree</cell><cell>5 -7 years</cell></row><row><cell>OM3</cell><cell>55</cell><cell>M</cell><cell cols="2">Associate degree More than 7 years</cell></row><row><cell>OM4</cell><cell>57</cell><cell>F</cell><cell>Ph.D.</cell><cell>More than 7 years</cell></row><row><cell>OM5</cell><cell>66</cell><cell>M</cell><cell>High school</cell><cell>More than 7 years</cell></row><row><cell>OM6</cell><cell>62</cell><cell>M</cell><cell>Master's degree</cell><cell>More than 7 years</cell></row><row><cell>OM7</cell><cell>36</cell><cell>NB</cell><cell>Master's degree</cell><cell>More than 7 years</cell></row><row><cell>OM8</cell><cell>25</cell><cell>M</cell><cell>Master's degree</cell><cell>3 -5 years</cell></row><row><cell>OM9</cell><cell>27</cell><cell>NB</cell><cell>Master's degree</cell><cell>6 months -1 year</cell></row><row><cell cols="2">OM10 30</cell><cell>M</cell><cell>Master's degree</cell><cell>3 -5 years</cell></row><row><cell cols="5">data analysis through sound and touch representations and interactions.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While people-first language is preferred by most people with disabilities, much of the blindness community prefers the use of the term "blind people." In this text, we use both approaches interchangeably.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We gratefully acknowledge the anonymous O&amp;M instructors who participated in this interview study. This PDF file has been made accessible for blind individuals with alternate-text for all images. We thank the reviewers for their careful and informative feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Accessibility module for highcharts</title>
		<ptr target="https://www.highcharts.com/accessibility/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Audio graphs for ios</title>
		<ptr target="https://developer.apple.com/documentation/accessibility/audio_graphs" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">REV: Convert audio &amp; video to text</title>
		<ptr target="https://www.rev.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">World blindness and visual impairment: despite many successes, the problem is growing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ackland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Resnikoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Community Eye Health</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">100</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SoundLines: exploration of line segments through sonification and multi-touch interaction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ahmetovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bernareggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mascetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pini</surname></persName>
		</author>
		<idno type="DOI">10.1145/3373625.3418041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Computers and Accessibility</title>
				<meeting>the ACM Conference on Computers and Accessibility<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Communicating graphical information to blind users using music: The role of context</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Alty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Rigas</surname></persName>
		</author>
		<idno type="DOI">10.1145/274644.274721</idno>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>eeding of the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="574" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tactile graphics with a voice: using QR codes to access text in tactile graphics</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scofield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Ladner</surname></persName>
		</author>
		<idno type="DOI">10.1145/2661334.2661366</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Computers &amp; Accessibility</title>
				<meeting>the ACM Conference on Computers &amp; Accessibility<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scents and sensibility: Evaluating information olfactation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Batch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akazue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376733</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The promise of empathy: Design, disability, and knowing the &quot;other</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Rosner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300528</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="1" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semiology of Graphics: Diagrams, Networks, Maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bertin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Esri Press</publisher>
			<biblScope unit="volume">ISBN</biblScope>
			<biblScope unit="page" from="978" to="979" />
			<pubPlace>Redlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Earcons and icons: Their structure and common design principles</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Blattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sumikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="44" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Transferring Virtual Reality Training to Real World Settings in Individuals with Low Vision and Dual-Sensory Impairments</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>The University of Alabama at Birmingham</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multi-level typology of abstract visualization tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2013.124</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2376" to="2385" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Seeking a reference frame for cartographic sonification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brittell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Browsing modes for exploring sonified line graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brewster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramloll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Computer Society Conference on Human-Computer Interaction</title>
				<meeting>the British Computer Society Conference on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Design guidelines for audio representation of graphs and tables</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Brewster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramloll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Readings in Information Visualization: Using Vision to Think</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing for the non-visual: Enabling the visually impaired to use visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13686</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Assistive Technologies: Principles and Practice</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Polgar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Elsevier Health Sciences</publisher>
		</imprint>
	</monogr>
	<note>4th ed.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TangibleCircuits: An interactive 3D printed circuit education tool for people with visual impairments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">U</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panotopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376513</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The role of children in the design of new technology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Druin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour and information technology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Capturing and recreating auditory virtual reality</title>
		<author>
			<persName><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Zotkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Gumerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>O'donovan</surname></persName>
		</author>
		<idno>doi: 10. 1142/9789814299312 0027</idno>
	</analytic>
	<monogr>
		<title level="m">Principles And Applications Of Spatial Hearing</title>
				<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="337" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A browser extension for providing visually impaired users access to the content of bar charts on the web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Web Information Systems and Technologies</title>
				<meeting>the Conference on Web Information Systems and Technologies</meeting>
		<imprint>
			<publisher>INSTICC Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SVGPlott: an accessible tool to generate highly adaptable, accessible audio-tactile charts for and from blind and visually impaired people</title>
		<author>
			<persName><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.1145/3316782.3316793</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on PErvasive Technologies Related to Assistive Environments</title>
				<meeting>the ACM Conference on PErvasive Technologies Related to Assistive Environments<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">O&amp;M indoor virtual environments for people who are blind: A systematic literature review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Fac ¸anha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<idno>doi: 10. 1145/3395769</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020-08">Aug. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating a tool for improving accessibility to charts and graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ferres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lindgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sumegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tsuji</surname></persName>
		</author>
		<idno type="DOI">10.1145/2533682.2533683</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Producing accessible statistics diagrams in R</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J R</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sorge</surname></persName>
		</author>
		<idno>doi: 10. 1145/3058555.3058564</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web for All Conference on The Future of Accessible Work</title>
				<meeting>the Web for All Conference on The Future of Accessible Work<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pie chart sonification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1109/IV.2003.1217949</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings on Seventh International Conference on Information Visualization</title>
				<meeting>on Seventh International Conference on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
	<note>IV 2003.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ambisonics-an overview</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Furness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Audio Engineering Society</title>
		<imprint>
			<date type="published" when="1990-05">May 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Auditory icons: Using sound in computer interfaces</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
		<idno>doi: 10.1207/ s15327051hci0202 3</idno>
	</analytic>
	<monogr>
		<title level="j">Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="177" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards large scale evaluation of novel sonification techniques for non visual shape exploration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gerino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Picinali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bernareggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alabastro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mascetti</surname></persName>
		</author>
		<idno type="DOI">10.1145/2700648.2809848</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Computers &amp; Accessibility</title>
				<meeting>the ACM Conference on Computers &amp; Accessibility<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interactive spatial sonification for non-visual exploration of virtual maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geronazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bedin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brayda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Campus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Avanzini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2015.08.004</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accessible on-line floor plans</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madugalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<idno type="DOI">10.1145/2736277.2741660</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web</title>
				<meeting>the International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="388" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Designing with and for children with special needs: An inclusionary model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Druin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fails</surname></persName>
		</author>
		<idno type="DOI">10.1145/1463689.1463719</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Interaction Design and Children</title>
				<meeting>the ACM Conference on Interaction Design and Children<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="61" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RoboGraphics: Dynamic tactile graphics powered by mobile robots</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muehlbradt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kane</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308561.3353804</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Computers and Accessibility</title>
				<meeting>the ACM Conference on Computers and Accessibility<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Exploring sonic parameter mapping for network data structures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Baltaxe-Admony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kurniawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Forbes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The Sonification Handbook</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Neuhoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Logos Verlag Berlin</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sound and meaning in auditory data display</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2004.825904</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="730" to="741" />
			<date type="published" when="2004-04">April 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mental maps and the use of sensory information by blind and partially sighted people</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hersh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3375279</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Accessible maps for the blind: Comparing 3D printed models with tactile graphics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Holloway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Butler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173772</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page" from="1" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cognitive maps in perception and thought. Image and Environment: Cognitive Mapping and Spatial Behavior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="63" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Experiential learning: Experience as the source of learning and development</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kolb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Pearson FT press</publisher>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modeling spatial knowledge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0364-0213(78)80003-2</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="153" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The cognitive map: Could it have been any other way</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4615-9325-615</idno>
	</analytic>
	<monogr>
		<title level="m">Spatial Orientation</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="345" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving orientation and mobility skills through virtual environment for people who are blind: past research and future potential</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Disability</title>
				<meeting>International Conference on Disability</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="393" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Research Methods in Human-Computer Interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hochheiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ensuring Digital Accessibility through Process and Policy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wayfinding without Visual Cues: Evaluation of an Interactive Audio Map System</title>
		<author>
			<persName><forename type="first">E</forename><surname>Loeliger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stockman</surname></persName>
		</author>
		<idno type="DOI">10.1093/iwc/iwt042</idno>
	</analytic>
	<monogr>
		<title level="j">Interacting with Computers</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sociotechnical considerations for accessible visualization design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lundgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.2019.8933762</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Visualization Conference</title>
				<meeting>the IEEE Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="16" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sensory substitution: Closing the gap between basic research and widespread practical visual rehabilitation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maidenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amedi</surname></persName>
		</author>
		<idno>doi: 10. 1016/j.neubiorev.2013.11.007</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Blind in a virtual world: Using sensory substitution for generically increasing the accessibility of graphical virtual environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maidenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Buchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amedi</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2015.7223381</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Virtual Reality Conference</title>
				<meeting>the IEEE Virtual Reality Conference<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="233" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Disability studies as a source of critical inquiry for the field of assistive technology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kasnitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/1878803.1878807</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Computers and Accessibility</title>
				<meeting>the ACM Conference on Computers and Accessibility<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spotlights and soundscapes: On the design of mixed reality auditory environments for persons with visual impairment</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3378576</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Beyond a visuocentric way of a visual web search clustering engine: The sonification of whatsonweb</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Mele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borsci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liotta</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-14097-656</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computers Helping People with Special Needs</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the International Conference on Computers Helping People with Special Needs</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6179</biblScope>
			<biblScope unit="page" from="351" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Developing sounds for a multimodal interface: conveying spatial information to visually impaired web users. Georgia Institute of Technology</title>
		<author>
			<persName><forename type="first">E</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Strain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">NL4DV: A toolkit for generating analytic specifications for data visualization from natural language queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narechania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3030378</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="379" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Sonification of spatial data. Georgia Institute of Technology</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The influence of visual experience on the ability to form spatial mental models based on route and survey descriptions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Noordzij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zuidhoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Postma</surname></persName>
		</author>
		<idno>doi: 10.1016/j .cognition.2005.05.006</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="342" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wayfinding without vision: An experiment with congenitally totally blind people</title>
		<author>
			<persName><forename type="first">R</forename><surname>Passini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Proulx</surname></persName>
		</author>
		<idno type="DOI">10.1177/0013916588202006</idno>
	</analytic>
	<monogr>
		<title level="j">Environment and Behavior</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="252" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Information of elementary multidimensional auditory displays</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ficks</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.1907300</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="158" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Examining visual semantic understanding in blind and low-vision technology users</title>
		<author>
			<persName><forename type="first">V</forename><surname>Potluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Grindeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Froehlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mankoff</surname></persName>
		</author>
		<idno>doi: 10. 1145/3411764.3445040</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A formative study on designing accurate and natural figure captioning systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the ACM Conference on Human Factors in Computing Systems</title>
				<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3334480.3382946</idno>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Using non-speech sounds to improve access to 2D tabular numerical information for visually impaired users</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramloll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Brewster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4471-0353-0</idno>
	</analytic>
	<monogr>
		<title level="m">People and Computers XV -Interaction without Frontiers</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="515" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The use of music in a graphical interface for the visually impaired</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Rigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Alty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IFIP TC13 International Conference on Human-Computer Interaction</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Howard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hammond</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Lindgaard</surname></persName>
		</editor>
		<meeting>the IFIP TC13 International Conference on Human-Computer Interaction</meeting>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Teachers of students with visual impairments share experiences and advice for supporting students in understanding graphics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of visual impairment &amp; blindness</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="475" to="487" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A system for the complementary visualization of 3D volume images using 2D and 3D binaurally processed sonification representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rossiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1109/VISUAL.1996.568129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Visualization</title>
				<meeting>the IEEE Conference on Visualization</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="351" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Shelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hodgson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName><surname>Mathtrax</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The eyes have it: A task by data type taxonomy for information visualizations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1109/VL.1996.545307</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium in Visual Languages</title>
				<meeting>the IEEE Symposium in Visual Languages<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Tick-marks, axes, and labels: The effects of adding context to auditory graphs</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Nothing about us without us: Investigating the role of critical disability studies in HCI</title>
		<author>
			<persName><forename type="first">K</forename><surname>Spiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mankoff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3334480.3375150</idno>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Exploring interactions with physically dynamic bar charts</title>
		<author>
			<persName><forename type="first">F</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<idno>doi: 10. 1145/2702123.2702604</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Human Factors in Computing Systems</title>
				<meeting>the ACM Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3237" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Structures of mental spaces: How people think about space</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
		<idno>doi: 10.1177/ 0013916502238865</idno>
	</analytic>
	<monogr>
		<title level="j">Environment and Behavior</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="80" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visuo-spatial imagery in congenitally totally blind people</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vecchi</surname></persName>
		</author>
		<idno type="DOI">10.1080/741941601</idno>
	</analytic>
	<monogr>
		<title level="j">Memory</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="102" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mappings and metaphors in auditory displays: An experimental assessment</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kramer</surname></persName>
		</author>
		<idno type="DOI">10.1145/1101530.1101534</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="407" to="412" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The role of body-based sensory information in the acquisition of enduring spatial representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Greenauer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00426-006-0087-x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Research</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="332" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Down under or centre stage? The world images of Australian students</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Walmsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Saarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Maccabe</surname></persName>
		</author>
		<idno type="DOI">10.1080/00049189008703012</idno>
	</analytic>
	<monogr>
		<title level="j">The Australian Geographer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="173" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Visual experiences in the blind induced by an auditory sensory substitution device</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Consciousness and cognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="492" to="500" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Ability-based design: Concept, principles and examples</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Froehlich</surname></persName>
		</author>
		<idno>doi: 10. 1145/1952383.1952384</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Toward a deeper understanding of the role of interaction in information visualization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jacko</surname></persName>
		</author>
		<idno>doi: 10 .1109/TVCG.2007.70515</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1224" to="1231" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Multimodal virtual reality versus printed medium in visualization for blind people</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Brewster</surname></persName>
		</author>
		<idno type="DOI">10.1145/638249.638261</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Assistive Technologies</title>
				<meeting>the ACM Conference on Assistive Technologies<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Quality, importance, and instruction: The perspectives of teachers of students with visual impairments on graphics use by students</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Zebehazy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Wilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Impairment &amp; Blindness</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="16" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Interactive Sonification of Abstract Data-Framework, Design Space, Evaluation, and User Tool</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>College Park, College Park, MD, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Data sonification for users with visual impairment: A case study with georeferenced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lazar</surname></persName>
		</author>
		<idno>doi: 10. 1145/1352782.1352786</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">ChartMaster: A tool for interacting with stock market charts using a screen reader</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Treviranus</surname></persName>
		</author>
		<idno type="DOI">10.1145/2700648.2809862</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Computers &amp; Accessibility</title>
				<meeting>the ACM Conference on Computers &amp; Accessibility<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
