<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Evaluation-Focused Framework for Visualization Recommendation Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zehua</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Phoebe</forename><surname>Moh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Hoffswell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yeon</forename><surname>Tak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sana</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eunyee</forename><surname>Malik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leilani</forename><surname>Koh</surname></persName>
						</author>
						<author>
							<persName><surname>Battle</surname></persName>
						</author>
						<title level="a" type="main">An Evaluation-Focused Framework for Visualization Recommendation Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3582A9BB9372714CE010DED1961E9449</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Visualization Tools, Visualization Recommendation Algorithms</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an evaluation perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The visualization community has developed a wide variety of systems for recommending how to visualize data <ref type="bibr" target="#b38">[38]</ref>. The algorithms behind these systems aim to help users uncover meaningful insights in their data by automatically generating visualizations for analysts to explore. For example, Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> encourages broad data exploration by recommending effective charts based on Mackinlay's <ref type="bibr" target="#b24">[24]</ref> design principles. VizDeck <ref type="bibr" target="#b17">[17]</ref> and Foresight <ref type="bibr" target="#b7">[7]</ref> recommend visualizations based on standard statistical characteristics of the dataset. SeeDB <ref type="bibr" target="#b30">[30]</ref> recommends visualizations based on a self-defined criterion of statistical "interestingness", or divergence of a sub-population from the whole.</p><p>While this panoply of recommendation algorithms provides many viable alternatives, it is unclear which algorithm should be prioritized for any given visualization scenario. In a review of existing evaluation practices, we find that many recommendation systems evaluate their recommendation algorithms in isolation <ref type="bibr" target="#b22">[22]</ref>, or construct benchmarks that their systems are already optimized for <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>. Even evaluations that do compare different algorithms do not measure user performance <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b26">26]</ref>. In other words, our community tends to generate new visualization recommendation algorithms without giving commensurate thought on how to evaluate them. As a result, the visualization community lacks rigorous theoretical and empirical guidance for how and when to apply each of these algorithms effectively.</p><p>One way to address this problem is to develop a standardized framework for comparing different visualization recommendation algorithms. Given that the purpose of these algorithms is to help analysts visually explore their data, a standardized framework should enable us to directly compare algorithms based on how they impact a user's per- formance for a variety of visual analysis tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">18]</ref>. The framework should also facilitate comparison of the algorithmic performance of the proposed approaches; for example, the framework should enable us to compare how each algorithm enumerates and traverses the design space of candidate visualizations in search of an optimal recommendation. In this paper, we propose an evaluation-focused framework to enable more effective theoretical and empirical comparisons of visualization recommendation algorithms. Our framework is based on the central process connecting most if not all of these algorithms: to generate the "best" recommendations, an algorithm must be able to enumerate the space of possible visualization designs and rank this design space, often by approximating and comparing the utility of candidate visualizations. Our evaluation framework is defined through three major components: (1) a network representing the space of all possible visualization designs for a given dataset, where nodes are visualization designs and edges connect designs that differ by a single encoding or data transformation; (2) the method a recommendation algorithm uses to traverse the design space to enumerate candidate visualization designs; and (3) an oracle used to approximate and rank the value of candidate visualizations that are enumerated.</p><p>Existing frameworks such as CompassQL <ref type="bibr" target="#b35">[35]</ref>, ZQL <ref type="bibr" target="#b29">[29]</ref>, and Draco <ref type="bibr" target="#b26">[26]</ref>, focus on generating new visualization recommendation algorithms, rather than comparing algorithms. As a result, behavioral differences are not intuitively captured through these frameworks, making it difficult to reason about the differences in algorithmic performance. For example, it is not clear how one might cluster different recommendation algorithms based on their behavioral similarity. With our framework, these behavioral differences become obvious. For example, Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> by default recommends visualizations which are one design or data transformation away from the current visualization, representing a narrow but efficient traversal of the visualization design space. In contrast, machine-learning-based algorithms enumerate and rank massive sub-spaces of visualization designs represented by the model's input features <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>We demonstrate the generality and coverage provided by our framework by comparing the behavior of five visualization recommendation systems: Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>, DeepEye <ref type="bibr" target="#b22">[22]</ref>, Foresight <ref type="bibr" target="#b7">[7]</ref>, Show Me <ref type="bibr" target="#b25">[25]</ref> and Dziban <ref type="bibr" target="#b21">[21]</ref>. We also show how our framework clarifies gaps in the literature where new algorithms can be formed, simply by varying traversal method and oracle combinations. Using two common graph traversal methods, breadth-first search (BFS) and depth-first search (DFS), and the oracles for Voyager <ref type="bibr" target="#b36">[36]</ref> and Dziban <ref type="bibr" target="#b21">[21]</ref>, we construct four recommendation algorithms: CompassQL+BFS (i.e., Voyager), CompassQL+DFS, Dziban+BFS, and Dziban+DFS. We then use our Algorithm Framework System Fig. <ref type="figure">1</ref>: Visualization recommendation algorithms are often specified using frameworks, and evaluated using implemented systems.</p><p>framework to design a user study to guide the empirical evaluation of these four visualization recommendation algorithms. Our results show that subjects did not perform significantly better with Dziban compared to CompassQL in focused-oriented tasks, however subjects did find Dziban's recommendations to be more intuitive in post-task survey ratings. These findings reinforce our argument that we need more evaluationfocused frameworks to elucidate the benefits of existing recommendation algorithms in real-world visual analysis scenarios. All of our data and code are available online on OSF: https://osf.io/txqsu/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>As illustrated in Fig. <ref type="figure">1</ref>, users can only interact with visualization recommendation algorithms when provided with an interface, and often a full system through which to interact. Furthermore, several algorithms are specified using existing visualization recommendation frameworks, which often take the form of specialized languages. In this section, we discuss the relevant literature in specifying visualization recommendation algorithms, and evaluating both algorithms and systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visualization Recommendation Algorithms</head><p>Existing recommendation algorithms can be separated into two main categories based on how the ranking engine (oracle) is implemented: rule-based or machine learning-based. Rule-based algorithms enumerate and then rank visualizations using heuristics based on theory or experimental findings in visual perception <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>. For example, theory work from Bertin <ref type="bibr" target="#b3">[4]</ref> and Mackinlay <ref type="bibr" target="#b24">[24]</ref> has been incorporated within rule-based algorithms behind a wide range of recommendation systems, including Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> and Show Me <ref type="bibr" target="#b25">[25]</ref>. Other recommendation systems, such as VizDeck <ref type="bibr" target="#b17">[17]</ref> and Foresight <ref type="bibr" target="#b7">[7]</ref> rank visualizations using manually-selected statistical rules.</p><p>Instead of ranking visualizations with manually-derived rules, other algorithms train machine learning models to generate recommendations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32]</ref>. For example, Hu et al. <ref type="bibr" target="#b13">[13]</ref> trained a deep learning model to learn the most common visualization designs from a large corpus of data sets and their associated Plotly visualizations. One of the Draco applications developed by Moritz et al. <ref type="bibr" target="#b26">[26]</ref>, Draco-Learn, was implemented by training models to learn effectiveness criteria from previous experimental findings. <ref type="bibr">Luo et al. [22]</ref> strive to balance the best of both strategies by combining deep learning with hand-written rules to generate recommendations.</p><p>Our framework provides a means of comparing these different ranking strategies, or oracles, in a systematic and repeatable manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization Recommendation Frameworks</head><p>Several frameworks have been proposed to make it easier to create new visualization recommendation algorithms. CompassQL <ref type="bibr" target="#b35">[35]</ref> is a query language created by Wongsuphasawat et al., which can produce different types of recommendation algorithms by varying phases of the recommendation process, such as enumerating, choosing, and ranking. For example, the visualization recommendation algorithm in Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> is implemented with CompassQL. ZQL <ref type="bibr" target="#b29">[29]</ref> is another query language that serves a similar purpose for visualization recommendation in the Zenvisage system. Draco <ref type="bibr" target="#b26">[26]</ref> is an alternative framework for specifying visualization recommendation algorithms based on answer set programming. Using Draco, one can specify new algorithms using a combination of encoding constraints and weights for these constraints. In this way, Draco enables the creation of new recommendation algorithms, without the creator having to worry about how to enumerate the underlying visualization design space.</p><p>To evaluate recommendation algorithms, we need to know not only the constraints imposed by the algorithms (the focus of current frameworks), but also the strategies employed to apply these constraints. Furthermore, we need to know what the differences are between strategies in order to reason about how they impact the performance of the system running the algorithm and the decisions of users who view the recommendations. Current frameworks omit these details, making it difficult to use them to evaluate and compare different algorithms. In contrast, our framework gives a clear definition of the visualization design space and considers an algorithm's traversal method through enumeration, which makes the enumeration process comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluating Visualization Recommendation Algorithms</head><p>Evaluation is crucial since it provides evidence of whether a proposed algorithm actually helps users explore their data more effectively. However, not all algorithms are evaluated in terms of how they improve user exploration performance. For example, Foresight <ref type="bibr" target="#b7">[7]</ref> only provides some usage scenarios to demonstrate its efficacy. On the other hand, although some existing systems do empirically evaluate user performance, the proposed algorithms are evaluated in isolation. For instance, Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> and SeeDB <ref type="bibr" target="#b30">[30]</ref> were compared to a baseline with no recommendations provided. VizDeck <ref type="bibr" target="#b17">[17]</ref> claimed that VizDeck users completed tasks with higher accuracy and less time compared to IBM ManyEyes <ref type="bibr" target="#b31">[31]</ref>, Google Fusion Tables <ref type="bibr" target="#b10">[10]</ref>, and Tableau <ref type="bibr" target="#b33">[33]</ref>. However, none of the compared systems provide recommendations.</p><p>Even when multiple algorithms are compared in the literature, user performance is still not the focus. Dziban <ref type="bibr" target="#b21">[21]</ref> was evaluated by calculating the ranking of its recommended visualizations in both Draco <ref type="bibr" target="#b26">[26]</ref> and GraphScape <ref type="bibr" target="#b19">[19]</ref> algorithms to check whether it provides a favorable tradeoff in terms of effectiveness and similarity. On the other hand, DeepEye <ref type="bibr" target="#b22">[22]</ref> was tested by ground-truth data, which was derived by having students to label whether a visualization is good or bad. Similarly, VizML <ref type="bibr" target="#b13">[13]</ref> was compared with CompassQL <ref type="bibr" target="#b35">[35]</ref>, DeepEye <ref type="bibr" target="#b22">[22]</ref>, ShowMe <ref type="bibr" target="#b25">[25]</ref> and Data2Vis <ref type="bibr" target="#b8">[8]</ref> using an effectiveness score which was calculated based on human-labeled data. However, human-perceptually "good" visualizations do not necessarily help the actual analysis process. Since tasks are not taken into account in the labelling process, there exists no evidence of whether these benchmark results can carry over into the actual human performance with higher level analysis tasks.</p><p>In this paper, we show how our framework can be used to theoretically compare visualization recommendation algorithms, and empirically evaluate user performance for different visual analytics tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EVALUATION FRAMEWORK</head><p>In this section, we describe our framework, which is based on the general recommendation process followed by the majority of visualization recommendation algorithms: enumerate, search, and rank. To demonstrate how our framework can be applied, we compare five existing representative visualization recommendation systems: Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>, DeepEye <ref type="bibr" target="#b22">[22]</ref>, Foresight <ref type="bibr" target="#b7">[7]</ref>, Show Me <ref type="bibr" target="#b25">[25]</ref> and Dziban <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Defining the Core Components of the Framework</head><p>Visualization recommendation algorithms are a form of search algorithm, which generally follow two basic steps: traverse candidates within the larger search space, and evaluate these candidates against specific search criteria. In the case of visualization recommendation, the traversal step involves enumerating the visualization design space, and the evaluation step requires ranking the candidate visualizations for the subsequent recommendation. However, before an algorithm can enumerate candidate visualizations, the visualization design space must first be clearly defined. In this section, we define the visualization design space, as well as the enumeration and ranking steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Defining the Visualization Design Space</head><p>To facilitate comparison, we must first establish a consistent definition of the visualization design space that can be applied to a wide range of algorithms. Prior work uses graph theory to model visualization spaces <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b23">23]</ref>, however previous definitions cover only a fraction of the full design space <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b35">35]</ref>. We contribute a generalization of these existing visualization spaces using graph theory. In our framework, we consider the full design space of all possible visualizations, which is defined as the combination of data attributes, encoding channels, and data transformations that can be applied to a given dataset. By leveraging this full design space, individual algorithms can be compared in terms of the particular subspaces they traverse.</p><p>Tracking Visualization Designs Within the Design Space Graph. Suppose we are generating recommendations for a movies dataset D, containing n attributes A = {a 1 ,...,a n }, such as movie title, creative type, gross, release date, etc. There are m possible transformations T = {t 1 ,...,t m }; each transformation has a set of parameters to determine how it can be applied to the data. For example, one possible data transformation is calculating the average of movie gross: AVG(a gross ), which is parameterized by only one attribute. On the other hand, there also exist k possible encoding channels C = {c 1 , ..., c k } which are used to visualize the combination of attribute and data transformation, such as, the 13 encoding channels proposed by Mackinlay <ref type="bibr" target="#b24">[24]</ref>. We represent the visualization design space for this dataset as a network graph G = (N, E). Each node of the graph n ∈ N contains visualizations which are defined by a set of data attributes (a i , a j , etc.), data transformations (t i , t j , etc.), and encoding channels (c i , c j , etc.):</p><formula xml:id="formula_0">n = {v | v = [c i (a i ,t i ), c j (a j ,t j ),...]}<label>(1)</label></formula><p>Here, a i , etc. are attributes from D, and t i , etc. are data transformations operated on attributes a i , etc., while c i , etc. are encoding channels used to visualize the combination of (a i ,t i ), etc. v is a visualization defined by the attribute set {a i , a j } and its corresponding data transformations {t i ,t j } and encoding channels {c i , c j }. Edges between each pair of nodes represent operations that transform one node to another, such as adding one attribute, or changing the data transformation or encoding channel of an attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Defining Sub-spaces for Different Types of Algorithms</head><p>To more efficiently navigate the visualization design space, recommendation algorithms can merge multiple visualizations into one node, or even ignore nodes, reducing the total edges that needed to be traversed. We discuss how different algorithms manipulate the visualization design space, based on the three types of recommendation algorithms proposed by Wongsuphasawat et al. <ref type="bibr" target="#b35">[35]</ref>: algorithms suggesting what attributes and/or transformations to visualize (data query recommendations), what encoding channels to apply to selected data (visual encoding recommendations), or both (hybrid recommendations).</p><p>Visual Encoding Recommendations. These algorithms focus on enumerating and ranking variations in encoding choices (e.g., Show Me <ref type="bibr" target="#b25">[25]</ref>), requiring access to attribute, transformation, and encoding information. However, to reduce the cost of enumerating the visualization design space, these algorithms often require the user to select what attributes and transformations to visualize in advance. In this way, all nodes that include non-user-selected attributes can be ignored. We can represent this user selection-based subspace in the following way:</p><formula xml:id="formula_1">n = {v = [c i (a i ,t i ),...] | selected(a i ,t i ) = 1, ∀(a i ,t i ) ∈ v}</formula><p>(2) Data Query Recommendations. These algorithms tend to focus on recommending attributes and/or transformations, and ignore encoding channels (e.g., Foresight <ref type="bibr" target="#b7">[7]</ref> and SeeDB <ref type="bibr" target="#b30">[30]</ref>). To ignore design variation, we can merge all visualizations that vary only by encodings into one node (i.e., remove all encoding channel specifiers {c i , c j ,...}):</p><formula xml:id="formula_2">n = {v | v = [(a i ,t i ), (a j ,t j ),...]}<label>(3)</label></formula><p>Hybrid Recommendations. These algorithms consider variations in attributes, transformations, and encoding channels (e.g., Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>). As a result, the full expressiveness of the visualization design space graph is required. However, enumerating all possible combinations of attributes, transformations, and encoding channels can be prohibitively expensive. In the next section, we discuss how algorithms efficiently enumerate candidate visualizations within this space of possible visualization designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Defining the Enumeration Step</head><p>Given a formal definition of the visualization search space G, search algorithms must then traverse this space to identify qualified candidate results. The result of the enumeration step is a list of candidates that match the input requirements, which are then passed to the ranking step. However, the full visualization design space is an exponential function of attributes {a 1 , ..., a n }, transformations {t 1 ,...,t m }, and encoding channels {c 1 ,...,c k }, making it prohibitively large to search in its entirety. As a result, visualization recommendation algorithms must address a trade-off between recommendation breadth and execution cost, where higher quality results can be achieved by enumerating and ranking more of the visualization design space, but performing this additional work increases the algorithm's execution time.</p><p>Input Nodes to the Enumeration Step. In response to this tradeoff, recommendation algorithms generally enumerate visualizations based on one or more reference nodes, often the nodes that contain the user's current selected attributes or visualization, or auto-generated reference nodes derived from simple heuristics. For example, Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> uses the node that contains the user's current visualization as a reference (denoted as n 0 ), otherwise Voyager generates univariate visualizations by default.</p><p>Applying Constraints to Bound the Number of Candidate Nodes. Algorithm designers tend to keep the space of traversed visualizations quite small by imposing strict manual constraints on what parts of the space can be traversed. The most common constraints limit either the maximum path length that can be explored from some reference node n 0 , or the maximum number of inputs contained within a candidate node. Using our design space notation from 3.1, we can represent all nodes with a maximum path length of 2 from n 0 as:</p><formula xml:id="formula_3">{n | dist(n, n 0 ) ≤ 2, n ∈ N},<label>(4)</label></formula><p>and nodes comprised of visualizations (v) with at most two inputs as:</p><formula xml:id="formula_4">{n | |v| ≤ 2, v ∈ n, n ∈ N}. (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>For example, Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> only considers nodes that differ from the user's current visualization by at most one attribute or data transformation, i.e., by setting the path length threshold to one for Eq. 4. This example is illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>. DeepEye <ref type="bibr" target="#b22">[22]</ref> only outputs twoattribute visualizations, i.e., by setting |v| = 2 in Eq. 5. DeepEye also limits data transformation choices to three types (aggregating, binning and sorting), and encoding choices to one of four basic visualization types (bar, pie, line, scatterplot), i.e., by setting |T | = 3 and |C| = 4.</p><p>Navigating the Bounded Design Space to Enumerate Candidates. Once the constraints of the traversal are established, then algorithms must select a method for enumerating specific designs within this bounded space. Given one or more reference nodes, there are three basic approaches to the enumeration process:</p><p>• a random traversal, such as by listing random combinations of valid attributes, transformations, and/or encoding channels; • a tree-oriented traversal, such as breadth-first or depth-first search along G, originating at n 0 ; • a cluster-oriented traversal, where nodes are clustered by predefined criteria, and clusters closest to n 0 are prioritized. We see that these traversal strategies lead to varying degrees of depth and breadth in the coverage of the design space. For example, random and cluster-oriented traversals can cover a broader range of G, but at the risk of having few nodes explored close to the user's current visualization n 0 . In contrast, tree-oriented traversals will have dense coverage near n 0 , but may have little or no coverage elsewhere in G.</p><p>Note also that this traversal process need not take place all at once. For example, in the case of algorithms that rely on machine learning (a) Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> uses tree-based enumeration with max path length of 1.</p><p>(b) Foresight <ref type="bibr" target="#b7">[7]</ref> enumerates all data attributes with a max input of 2.</p><p>(c) DeepEye <ref type="bibr" target="#b22">[22]</ref> uses a clustered enumeration with an input of 2 attributes. Fig. <ref type="figure">3</ref>: A comparison of attribute enumeration methods for three existing recommendation algorithms. Each square is a node in the visualization design space, where the current node (n 0 ) is colored black. models, enumeration may happen both in the training phase (random traversal of training inputs) as well as in the prediction phase (clusteroriented traversal within the model structure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Defining the Ranking Step</head><p>Given the candidates generated by the enumeration step, the purpose of the ranking step is to order these candidates in terms of how closely they match a set of pre-defined search criteria. In the case of visualization recommendation algorithms, the search criteria represent the quality and relevance of the candidate visualization. We use "oracle" to refer to the part of the algorithm that assesses candidate quality and relevance.</p><p>Oracle Inputs &amp; Structure. Oracles often take as input the user's recent history of visualizations created and interactions performed, as well as statistics about the current dataset. Using these inputs, oracles typically compute one or more scoring features and rank enumerated candidates using a weighted function of these features, or a model. Feature weights for the model can be represented mathematically, such as by assigning numerical weights to calculated heuristics to produce a single score <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>, as well as procedurally, such as through ordered pruning rules to eliminate low-quality candidates <ref type="bibr" target="#b22">[22]</ref>. There are three types of models that oracles often use to rank candidates: behavioral models, statistical models, and machine learning models.</p><p>Behavioral models. These models are generally represented as manual heuristics derived from user studies and/or field observations. For example, APT <ref type="bibr" target="#b24">[24]</ref>, Draco-APT <ref type="bibr" target="#b26">[26]</ref>, Show Me <ref type="bibr" target="#b25">[25]</ref>, and Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> are based in part on manually-derived best practices, particularly for enhancing visual perception. In another example, the BDVR algorithm <ref type="bibr" target="#b11">[11]</ref> compares the user's most recent interactions to the four most common interaction patterns observed with the HAR-VEST system. The BDVR algorithm then ranks visualizations based on whether they would be produced by the closest matching patterns.</p><p>Statistical models. These models often use a pre-defined set of aggregate statistics to compare candidates <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b30">30]</ref>. For example, Foresight <ref type="bibr" target="#b7">[7]</ref> analyzes the dataset to be visualized for statistical properties selected by the user, such as skew, outliers, and linear relationships, and scores candidate visualizations according to these features.</p><p>Machine Learning models. These models take large corpora of existing user data as input to an offline training phase <ref type="bibr" target="#b26">[26]</ref>. During the training phase, these models generally cluster similar visualization designs, and develop hierarchical data structures to efficiently index into these clusters. Recent approaches use deep learning models to avoid the need for feature engineering prior to training <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>Hybrid models. Hybrid oracles are also possible, where multiple models may be used. Oracles may also need to prune redundant candidates if they are too similar in quality and relevance <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparing Existing Algorithms Using the Framework</head><p>Using the three main components of our framework, we can evaluate a wide range of visualization recommendation algorithms. We demonstrate the flexibility of our framework by analyzing algorithms from five existing works: Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>, DeepEye <ref type="bibr" target="#b22">[22]</ref>, Foresight <ref type="bibr" target="#b7">[7]</ref>, Show Me <ref type="bibr" target="#b25">[25]</ref> and Dziban <ref type="bibr" target="#b21">[21]</ref>. We compare the high-level intuition behind the enumeration strategies in Fig. <ref type="figure">3</ref>, and the enumeration constraints in Table <ref type="table" target="#tab_1">1</ref>. We selected these five algorithms because they cover all three types of recommendation algorithms proposed by Wongsuphasawat et al. <ref type="bibr" target="#b35">[35]</ref>, and their results can be generalized to many other systems. For instance, Show Me <ref type="bibr" target="#b25">[25]</ref> uses a visual encoding recommendation algorithm. Foresight uses a query recommendation algorithm that is similar to other query recommendation algorithms (e.g., VizDeck <ref type="bibr" target="#b17">[17]</ref>). DeepEye uses a hybrid algorithm that is machine learning based <ref type="bibr" target="#b22">[22]</ref>.</p><p>Other machine learning approaches are similar to DeepEye, differing primarily by model type or input data used for training <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b26">26]</ref>.</p><p>Show Me <ref type="bibr" target="#b25">[25]</ref>. As discussed in Sect. Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>. Voyager applies tree-oriented enumeration with an aggressively bounded search space in terms of attributes. As mentioned in 3.1.3, Voyager uses the user's current visualization to generate relevant charts with a maximum path length constraint of one (see Fig. <ref type="figure">3a</ref>). Constraining the attribute space allows Voyager to enumerate more encoding channels than other algorithms, as shown in Table <ref type="table" target="#tab_1">1</ref>. The Voyager oracle applies Mackinlay's effectiveness rules <ref type="bibr" target="#b24">[24]</ref>.</p><p>Dziban <ref type="bibr" target="#b21">[21]</ref>. Dziban is a visualization recommendation API that uses Draco <ref type="bibr" target="#b26">[26]</ref> as the implementation base. Dziban contains a hybrid visualization recommendation algorithm that builds on the Graph-Scape <ref type="bibr" target="#b19">[19]</ref> and Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> oracles. Given a user's prior query, Dziban can recommend new transformations and encoding channels, however, it does not recommend new attributes to visualize. Dziban prioritizes perceptually similar visualizations in its ranking step.</p><p>Foresight <ref type="bibr" target="#b7">[7]</ref>. The Foresight system ranks visualizations based on "insight" scores derived from user-selected statistical features or data attributes. Foresight enumerates all possible pairings of data attributes, as well as all individual attributes, but restricts the final visualizations to either a bar chart, a box plot, or a scatterplot. Thus, Foresight performs a full attribute enumeration within a bounded search space.</p><p>DeepEye <ref type="bibr" target="#b22">[22]</ref>. Although DeepEye can be extended to support different numbers of attributes, the paper focuses on enumerating visualizations with two attributes and at most three data transformations (see Table <ref type="table" target="#tab_1">1</ref>). DeepEye supports four visualization types: bar, pie, line, and scatter. Though the DeepEye authors describe their enumeration method in terms of trees, when compared using our evaluation framework, DeepEye actually performs cluster-oriented enumeration. The oracle ranks visualization candidates using both hand-written rules from visualization experts, and a suite of binary classifiers trained using visualization preference data collected from user studies. Note that the hand-written rules are used as heuristics to prune the search space, interleaving the enumeration and ranking steps.</p><p>Comparing Algorithms in Terms of Enumeration Trade-Offs. We see wide variation in the depth and breadth of design space coverage in Fig. <ref type="figure">3</ref>, and also in the enumeration constraints in Table <ref type="table" target="#tab_1">1</ref>. For example, Voyager provides broad attribute and transformation coverage near the user's current visualization, represented in black in Fig. <ref type="figure">3a</ref>, but Voyager leaves much of the visualization design space unexplored. However, Voyager enumerates more encoding channels compared to other algorithms, as shown in Table <ref type="table" target="#tab_1">1</ref>. Dziban does not enumerate attributes, limiting its search space to transformations and encoding channel variations only; in return, Dziban can also enumerate a larger range of encoding channels. Show Me takes this restriction one step further by only enumerating and ranking variations in encoding channels, enabling broad and deep coverage of the encoding space, but virtually no coverage of the attribute and transformation space.</p><p>In comparison, we see in Fig. <ref type="figure">3b</ref> that Foresight enumerates all attribute combinations within its bounded search space, providing both broader and deeper coverage of attributes. However as a trade-off, we see in Table <ref type="table" target="#tab_1">1</ref> that Foresight severely limits the space of encoding channels that may be enumerated. We see that DeepEye makes a similar tradeoff to Foresight. In Fig. <ref type="figure">3c</ref>, we see that DeepEye's cluster-oriented enumeration approach provides greater enumeration depth than both Voyager and Foresight, but it also lacks thorough coverage of attributes (and transformations) across the bounded search space. However, the cost of this increased attribute/transformation enumeration depth is reduced encoding channel coverage, as shown in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Comparing Algorithms in Terms of Ranking Trade-Offs. Three of the four algorithms we compare utilize behavioral ranking models (Voyager, Dziban, DeepEye). These behavior-based heuristics are fast to apply to visualization candidates, but can take significant effort to derive on account of having to conduct user studies and/or field studies beforehand to collect the data <ref type="bibr" target="#b27">[27]</ref>. Even when the data is collected, significant manual effort may also be required to hand-tune the resulting models <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>. This issue of effort is also observed for machine-learning models, such as in the case of DeepEye, which required extensive data collection to train its machine-learning oracle. In the case of Voyager and Dziban, existing heuristics, algorithms, and user study data were used to develop the oracles, which can help reduce the burden of training and tuning new models. Foresight's oracle requires no training since it relies on a pre-defined set of statistics. However, Foresight must calculate these statistics for all enumerated attribute combinations, making its execution more expensive. Foresight uses statistical sketches to reduce the processing time.</p><p>Once these algorithms are finally trained and tuned, a natural question is: which algorithm provides the best recommendations for a given visualization task? Though this question could be evaluated theoretically, existing approaches often use a somewhat reductive approach of approximating users' analytic performance through low-level perceptual heuristics (e.g., <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>). Perception is only one component of a user's analytic performance and is a poor approximation of user performance in higher-level visual analysis tasks, such as prediction or exploration <ref type="bibr" target="#b2">[3]</ref>. Instead, we argue for an empirical evaluation approach that is more task-sensitive. To compare the quality of generated recommendations, we provide a demonstration of using our framework to empirically evaluate different algorithms in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BENCHMARKING RECOMMENDATION ALGORITHMS</head><p>We show that our framework could compare a wide range of existing visualization recommendation algorithms theoretically in the previous section. Whereas in this section, we show how our framework could guide the empirical comparison of various recommendation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Algorithms for Standardized Evaluation</head><p>Existing recommendation algorithms either have no interface presented <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b26">26]</ref> or the systems built on the top of them utilize different interface designs <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>, which makes it hard to conduct a standardized evaluation. Moreover, various systems allow different kinds of user input, which brings even more difficulties to the evaluation and comparison. For instance, the majority of systems allow selected data fields as input <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>, while some also allow inputting statistical features <ref type="bibr" target="#b7">[7]</ref>, or visualization types <ref type="bibr" target="#b17">[17]</ref>.</p><p>Thus, to standardize the benchmark of different recommendation algorithms, we implement an interface to wrap around algorithms that are generated by applying the enumeration approach and oracle behind existing recommendations. In this paper, by varying the traversal method and the oracle, we come up with four new visualization recommendation algorithms to evaluate. The graph traversal method would be either BFS or DFS, and the oracles are CompassQL <ref type="bibr" target="#b35">[35]</ref> and Dziban <ref type="bibr" target="#b21">[21]</ref>. Both BFS and DFS are tree-oriented traversal methods. While BFS enumerates with a maximum path length of one, DFS enumerates along the path until the current node or the space boundary is reached. The CompassQL version that we use is the same as the one behind the Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> systems, which ranks visualizations by effectiveness. On the other hand, Dziban is built on the top of Draco <ref type="bibr" target="#b26">[26]</ref> and Graph-Scape <ref type="bibr" target="#b19">[19]</ref>, which takes both effectiveness and perceptual distance into consideration while ranking visualizations.</p><p>We evaluate CompassQL <ref type="bibr" target="#b35">[35]</ref> and Dziban <ref type="bibr" target="#b21">[21]</ref> based on the availability of their source code, whereas the code for many other algorithms is not publicly accessible <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b22">22]</ref>. Moreover, by adding the ranking strategy of GraphScape <ref type="bibr" target="#b19">[19]</ref> to optimize the perceptual distance, Dziban <ref type="bibr" target="#b21">[21]</ref> claims to provide a considerable benefit over Draco-CQL <ref type="bibr" target="#b26">[26]</ref>, which is a re-implementation version of CompassQL <ref type="bibr" target="#b35">[35]</ref>. We benchmark these two ranking engines to see whether there exists a significant improvement in user performance. The visualization design space is the same for all algorithms, where each node contains visualizations with the same data attributes, and each edge represents adding or removing one data attribute. We only consider visualizations with 3 data attributes or less, thus no attribute can be removed from a univariate chart, and no attribute can be added to a three-attribute chart. Oracles would need to make other data variation decisions, like whether to add data transformations (binning or aggregating), as well as design variation decisions, like applying which visual encoding for each attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interface Design</head><p>Fig. <ref type="figure" target="#fig_1">4</ref> shows the interface for evaluating the set of new visualization recommendation algorithms, which consists of a top panel, a data panel (left), a visualization gallery (middle), and a task panel (right). Our interface design is inspired by the Voyager systems <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>. Since we focus on evaluating the recommendation quality of each algorithm, limited interactions are allowed in the interface, such as selecting attributes, bookmarking or specifying a chart, and also hovering over a chart to check the value of a particular data point. We share the source code and a demonstration video of our interface in the OSF repository.</p><p>The Top Panel (A). By clicking the button in the top panel, a bookmark gallery of visualizations saved by the user pops up. Participants are encouraged to bookmark charts that could answer the question during the user study.</p><p>The Data Panel (B). It shows the name of the current dataset and presents a list of all data fields within the dataset. The list is grouped by the data type and then ordered by the variable name alphabetically. For each variable, it shows the data type icon, the variable name and then a checkbox representing whether the variable is included in the specified view. Users can click on the checkbox to include or exclude an attribute from the specified view (C). The related views (D) will provide different recommendations based on the current specification.</p><p>The Visualization Gallery (C &amp; D). It consists of two views: the specified view (C) and the related views (D). Each chart contains a label on the top-left corner showing which data attributes are visualized in the chart, and a bookmark button ( ) on the top-right corner, which triggers whether the bookmark gallery includes or excludes the chart.</p><p>The specified view (C) is the best chart recommendation for the currently selected variables. The related views show recommended charts by the current recommendation algorithm based on the specified view. When no data field is selected, the related views show univariate visualizations. By default, the related views display the top five recommended charts based on the specified view, if users want to explore more, they can click on the Load More button ( Load More) to view additional recommendations. The list button ( ) on each chart in the related views allows users to update the specified view and display new recommendations from this starting point (n 0 ).</p><p>The Task Panel (E). It consists of (1) the current task description, (2) an input area for users to answer the question, (3) a checkbox for users to self-check if they bookmarked charts that could help answer the question, (4) a button to revisit the bookmark gallery, (5) the posttask questionnaire, and (6) a submit button to navigate to the next step. When participants click on the submit button, the answer of the task, the specifications of the bookmarked charts, the response of the post-task questionnaire, and also the interaction log will be sent to the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Study Design</head><p>The study followed a 4 (recommendation algorithms) × 2 (dataset) mixed design, thus in total there are 8 designs. We utilized a betweensubjects study design; each participant only conducted one analysis session, with a random combination of recommendation algorithm and dataset. All participants completed the study remotely. Visualization Tools. The interface (Fig. <ref type="figure" target="#fig_1">4</ref>) was the same for every participant, but the recommendation algorithm was varied to generate different visualizations in the specified and related views.</p><p>Datasets. We utilized two Voyager <ref type="bibr" target="#b36">[36]</ref> datasets for the evaluation: movies and birdstrikes. The movies 1 dataset contains 3,201 records and 15 attributes (7 nominal, 1 temporal, 8 quantitative). The birdstrikes 2 dataset is a redacted version of FAA wildlife airplane strike records with 10,000 records and 14 attributes (9 nominal, 1 temporal, 4 quantitative).</p><p>Participants. We recruited nine subjects for each condition, for a total of 72 participants (23 female, 49 male), all of whom successfully completed the study and were included in our analysis. All participants claimed to have proficient computer skills and prior experience using at least one of the following or similar tools/programming languages: Excel, Tableau, Voyager, Python/matplotlib, R/ggplot, D3. We recruited participants from both academia and industry. Of the 72 participants, 40 were students while 32 were professional participants from the industry. We compensated participants with a $10 Amazon gift certificate.</p><p>Study Protocol. Each participant completed a 60-minute online session, consisting of: (1) study overview and consent form; (2) a demographic survey; (3) 10-min tutorial and demo with a dataset distinct from those used for the actual analysis sections; (4) 40-minute analysis block with one study design; and (5) the exit-survey. During the study session, participants were asked to complete four analysis tasks, two focused and two open-ended (see Table <ref type="table" target="#tab_3">2</ref>). After each task, participants were asked to reflect on their experience using the recommendation tool to complete the task in a short post-task questionnaire with a symmetric 5-point scale, from strongly disagree (-2) to strongly agree (+2):</p><p>• Confidence in Understanding Data: I am confident in understanding the dataset. • Confidence in Answer: I am confident in my answer to the task.</p><p>• Efficiency: The related views made it easier to explore the data.</p><p>• Ease of Use: The related views were easy to understand. • Utility: The related views were useful for completing the task.</p><p>• Overall: I would use this tool for similar tasks in the future. After completing all four tasks, participants completed a survey to evaluate the recommender. The following questions were asked:</p><p>• What are the advantages and disadvantages of the tool?</p><p>• Do you have any other comments on the recommendation system? Tasks. We designed four visual analytics tasks (see Table <ref type="table" target="#tab_3">2</ref>) for each dataset based on prior studies of data analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37]</ref>. These four tasks cover all three analysis task classes discussed by Battle et al. <ref type="bibr" target="#b1">[2]</ref>: quantitative, qualitative, and exploratory. T1 and T2 are focused tasks; T1 involves two data attributes, while T2 involves three data attributes. T1 asks participants to find the extremum, which is a qualitative task, while T2 asks participants to retrieve a specific value from a subset of the data, which is a quantitative task. Both T3 and T4 are exploratory tasks. T3 provides a particular direction for the data exploration, while T4 asks participants to freely explore the dataset.</p><p>Collected Data. Since the user study was conducted remotely, for each task, we collected participants' (1) answers, (2) bookmarked charts, (3) interaction logs, and ( <ref type="formula" target="#formula_3">4</ref>) responses of post-task questionnaires. We also obtained comments from the exit-study survey.</p><p>Pre-registration. We pre-registered <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b34">34]</ref> the conditions, measurements, analysis (using Bayesian regression models to test if there is a significant difference in the stated measurements), and data collection criteria on the website AsPredicted 3 before collecting any data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BENCHMARK RESULTS</head><p>We obtained 72 valid study results which passed the exclusion criteria in our pre-registration. We also had a pilot study with five participants, where we derived the informative priors for our quantitative analysis.</p><p>We now present the analysis of study results, focusing on the accuracy and completion time of focused-tasks, user interaction activities during open-ended tasks, post-task questionnaire responses, and qualitative feedback. For quantitative analyses, we adopted Bayesian models to estimate the 95% credible interval (CI) for each parameter. Since the data type of our collected data varies, we had to apply various Bayesian regression models. We used the logistic regression for analyzing the accuracy, the linear regression for the completion time and interaction logs, and the ordinal regression for post-task questionnaire responses. We chose Bayesian models because they allow us to draw more reasonable conclusions about the true values of our parameters from small-n studies than the null hypothesis significant testing (NHST). The Bayesian 95% credible interval represents the interval that we are 95% sure contains the true value, which is different from the NHST confidence interval. On the other hand, in terms of estimating differences, like the differences between design A and B (i.e. A-B), if the Bayesian 95% credible interval is greater than 0 and not overlapping with 0, it means that we are 95% sure that design A performed better than B. We provide our experiment code, data collected for both experiments, and analysis scripts as supplemental materials in the OSF repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Focused Tasks</head><p>We use accuracy and completion time as the two metrics to evaluate and compare the empirical utility of the four recommendation algorithms in supporting focus-oriented analysis.  Fig. <ref type="figure">6</ref>: The completion time of focused tasks for all recommendation algorithms. We show posterior distributions, 50% and 95% CIs of expected titer thresholds for both the Movies and Birdstrikes dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Accuracy</head><p>To analyze task accuracy, we trained a Bayesian logistic regression model for the two focused tasks to model the probability of a correct answer given an oracle and graph traversal combination. It shows in Fig. <ref type="figure" target="#fig_2">5</ref> that CompassQL+DFS and Dziban+BFS had higher accuracy than Dziban+DFS, while CompassQL+BFS seemed to have the lowest accuracy rate. However, since all of the 95% CIs overlap, we cannot make a formal conclusion about which algorithm performed significantly better in the accuracy of focused tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Completion Time</head><p>We derived a weakly informative prior on completion times in seconds from the pilot study: N(μ = 360.48, σ = 224.40). As shown in Fig. <ref type="figure">6</ref>, all 95% CIs overlap with each other, thus we cannot conclude which recommendation algorithm had a significant effect on the completion time of focused tasks. However, it is interesting to see that while participants spent the most time with CompassQL+DFS, the accuracy with Com-passQL+DFS was the highest. This relationship could imply that the longer time that participants spent in the task led to a higher accuracy.</p><p>Although not significant, it generally takes less time for participants to complete tasks with the Movies dataset than the Birdstrikes one. On the other hand, the accuracy with the Movies dataset is also slightly higher. This finding is reasonable, since people are more familiar with Movies data than Birdstrikes data in real life.</p><p>In summary, since all 95% CIs overlap in both the accuracy and the completion time analysis, we conclude for preciseness and decisiveness that the four new recommendation algorithms have no significant impact on the performance of participants in focused tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Open-ended Tasks</head><p>To evaluate the utility of different algorithms for supporting open-ended tasks, we analyze the interaction logs from the user study. Since the user study was conducted remotely, we lack eye-tracking data to show which visualizations users were attending to. Taking inspiration from Voyager <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>, we analyze the number of unique variable sets shown on screen to assess which recommendation algorithm provides broader data exploration during the open-ended tasks. Moreover, we extend the analysis to the number of unique visual designs. Unlike the variable set which only considers the combination of data fields, the visual design takes data transformations and visual encodings into account. Since  each edge in the visualization design space only represents the attribute modification, and oracles need to make choices for data transformations and encoding channels, it would be interesting to see whether the oracle would provide different visualization designs from the same node while the reference node (n 0 ) is different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Exposed Variable Sets &amp; Visual Designs</head><p>Fig. <ref type="figure" target="#fig_3">7</ref> shows that CompassQL+BFS, Dziban+BFS, and Dziban+DFS exposed more unique variable sets and visual designs than Com-passQL+DFS in the open-ended tasks. On the other hand, we also see that Dziban exposed more numbers of visual designs than variable sets, so did BFS, which means Dziban and BFS recommended more design variants with the same variable sets, while CompassQL+DFS seemed to only recommend roughly one visual design for each variable set. We also find that participants were exposed to slightly more unique variable sets and visual designs in the exploration task than in the prediction task, which is reasonable since the exploration task encourages participants to explore the dataset freely while the prediction task restrains a direction for the data exploration.</p><p>To check the significance, we also run a Bayesian linear regression model on the exposure difference between BFS and DFS, as well as the difference between Dziban and CompassQL, as shown in Fig. <ref type="figure" target="#fig_4">8</ref>.</p><p>From Fig. <ref type="figure" target="#fig_4">8a</ref> we can see that there is a significant difference in the average number of exposed variable sets between the two traversal methods, BFS and DFS. In the prediction task, BFS exposed significantly more variable sets with both the Birdstrikes dataset (b = 11.746) and the Movies dataset (b = 14.163). We also find a similar pattern of the exposure difference in the exploration task that BFS exposed significantly more variable sets than DFS. However, since the 95% CIs overlap with the auxiliary line at 0, we cannot conclude that Dziban exposed significantly more unique variable sets than CompassQL.</p><p>On the other hand, in terms of the number of visual designs, we Dziban exposed significantly more visual designs than CompassQL with both the Movies and the Birdstrikes dataset in both tasks. It is interesting to see that although Dziban did not expose significantly more unique variable sets, it exposed significantly more unique visual designs than CompassQL, which means Dziban tends to recommend more design variants than data variants (as shown in Fig. <ref type="figure" target="#fig_3">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Interacted Variable Sets &amp; Visual Designs</head><p>We also analyze the number of unique variable sets and visual designs that participants interacted with during the open-ended tasks. We include interactions like specifying ( ), bookmarking ( ), and mousehovering for more than half a second. From Fig. <ref type="figure">9</ref> we do not see much difference in the number of interacted variable sets and visual designs among different recommendation algorithms. It seems that participants interacted with more visual designs than variable sets with BFS, which means that BFS provides more interesting design variants that participants would like to interact with. On the other hand, the number of unique variable sets and visual designs are about the same with DFS. In other words, DFS did not expose as many interesting design variants as BFS (Fig. <ref type="figure" target="#fig_3">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Post-task Questionnaires</head><p>We used the Bayesian ordinal regression model to analyze the user responses from the post-task questionnaires. Since we used a symmetric 5-point scale (-2 strongly disagree, +2 strongly agree) in the post-task questionnaire, our prior on user score of (range [−2, 2]) is expressed as a normal distribution N(0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Confidence Rating</head><p>In the post-task questionnaire, we asked participants to rate their confidence in understanding data, and also in their answers.</p><p>Confidence in Understanding data. As shown in Fig. <ref type="figure">10</ref>, BFS performed slightly better than DFS on users' confidence in understanding data. Dziban also had a higher confidence rating than CompassQL.</p><p>Confidence in Answer. On the other hand, BFS performed slightly worse than DFS on users' confidence in their answers. However, the Dziban oracle still had a slightly higher rating than CompassQL.</p><p>In summary, we don't see much difference in users' confidence ratings between the two traversal methods, BFS and DFS. On the other hand, Fig. <ref type="figure">10</ref> shows that the Dziban oracle performed better than CompassQL, however, the outperformance was not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Recommendation Algorithm Preference</head><p>We also asked participants to rate the related views in different aspects: efficiency, ease of use, utility, and overall (Fig. <ref type="figure" target="#fig_6">11</ref>). Efficiency. In terms of the efficiency rating, BFS received a significantly higher rating than DFS. However, although Dziban received a slightly better rating than CompassQL, we cannot conclude that a significant difference exists between the two oracles in the efficiency experience since the 95% CIs overlap with the line at 0.</p><p>Ease of Use. Similar to the efficiency rating, BFS performed significantly better than DFS with respect to the ease of use. However, there is no evidence supporting that Dziban performed significantly better than CompassQL in terms of the ease of use.</p><p>Utility. Both 95% CIs overlap with the auxiliary line at 0, thus we cannot conclude which traversal method or oracle is significantly better than the other one, although we can see that BFS and Dziban have slightly higher utility ratings than DFS and CompassQL respectively.</p><p>Overall. Similar to the utility, since both 95% CIs overlap with the line at 0, users did not significantly prefer one traversal method or oracle over the other one.</p><p>In summary, we see some significant differences in the user preference between the two traversal methods. In particular, participants significantly preferred BFS in the efficiency and ease of use experience. Although not significantly, we still can see that Dziban received a slightly higher rating in each experience compared to CompassQL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Participant Feedback</head><p>DFS is not preferred for focused tasks. Participants dislike DFS for focused tasks since the recommended charts could have more fields added compared to the current specified chart. One participant found that "The recommendations were useful but most of the time distractive and too many for answering specific questions." Another said that "When I checked one attribute, the recommendation charts always include three attributes. I would prefer if it was only two factors for the first two [focused] tasks." However, when it comes to open-ended tasks, participants had a different point of view in terms of the DFS traversal method; one participant mentioned that "This tool is good for exploring the data, especially it is the first time seeing (the data)."</p><p>Dziban is preferred as an oracle. Since Dziban takes the perceptual distance from the current chart into account, the behavior makes more sense to participants when they explore the related views. One participant commented on CompassQL that "(I am) unsure if there is any logic on the recommended charts, sometimes they are completely useless and just layer on another random metric or dimension". Another participant also pointed out that "The recommendations (from CompassQL) were often ineffective and created out of unrelated fields". When it came to the recommendations from Dziban, participants provided more positive feedback. One said that "The tool helps explore datasets and provides useful recommendations in terms of related measures and dimensions to enable getting useful insights." Another participant also commented that "For the most part, the tool added fields that made sense to include in addition to the original choices."</p><p>Both oracles need to be improved. Overall, we got positive feedback about the recommended charts, however, we also found some comments about the disadvantages of both oracles. The most common issue for CompassQL is it recommends scatterplots a lot since it only emphasizes effectiveness and when it comes to three attributes, it picks area or color as the third encodings, which sometimes confuses participants. One commented that "I didn't understand the shaded circle. I guess it could be there are various different values that are big and small." In terms of color encoding, one participant commented that "(It seems to be) often picking categories to represent color where there were so many colors as to make them all meaningless", and another pointed out that "Colors did not seem related to essential data." On the other hand, Dziban also considers the perceptual distance thus it tends to recommend charts that look similar to the original one but does not consider the effectiveness enough, like using text as a mark type in a scatterplot. A participant commented on Dziban that "Don't recommend views where a text value would dominate the visualization." One way to address these problems would be including more hand-tuned constraints, such as not using color to visualize more than a certain number of categories, not using area encoding when the overlapping exists, and not using text as a mark type to visualize long content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION &amp; FUTURE WORK</head><p>In this paper, we presented an evaluation-focused framework that can describe many existing visualization recommendation algorithms, and showed how our framework could guide the theoretical and empirical comparison of such algorithms. We conclude this paper with a discussion of guidelines for new recommendation algorithms, key benchmarking takeaways, limitations, and opportunities for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Framework As Guidelines</head><p>We now discuss how our framework could serve as a guideline not only for the future construction of recommendation algorithms but also for benchmarking a larger range of existing recommendation systems. 6.1.1 For Future Recommendation Algorithms Our framework consists of three major components: (1) the visualization design space, (2) the traversal method, and (3) the oracle. While constructing new recommendation algorithms, one should think about whether any of the components in the algorithm is new to the community. For example, does my visualization design space contain more (meaningful) visual designs than other existing automated systems? Is my algorithm using a new way to traverse the visualization space which could help the actual analysis? Is there a new creative ranking strategy that has not been covered by the existing literature? On the other hand, thinking about different combinations of the three components is another creative opportunity for constructing new recommendation algorithms. Among the new algorithms evaluated in this work, Com-passQL+DFS, Dziban+BFS and Dziban+DFS have not previously been proposed to the community, although the ranking engines (CompassQL and Dziban) have been researched as key contributions in this space. 6.1.2 For Benchmarking Various Automated Systems Although we did not benchmark existing automated systems since they leverage different interfaces and have limited code availability, our user study design still provides an at-a-glance overview of how our framework could be used to guide the evaluation and comparison of various automated systems. Without a standardized interface design and style of user inputs, it is difficult to compare multiple recommendation algorithms. By leveraging our framework, one could compartmentalize the three main components of the algorithm and test them within our standardized interface. For instance, one of our proposed algorithms, CompassQL+BFS utilizes the same idea of the graph traversal method and the ranking engine behind the Voyager systems. In such a way, our framework could not only evaluate the recommendation algorithm as a whole but also compare different components. As shown in the previous section (Sect. 5), our results not only show which algorithm performed better but also which traversal method or oracle was preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Takeaways from Benchmarking</head><p>From Sect. 5, we can see that there is actually no significant difference between recommendation algorithms in the participants' performance with focused tasks. On the other hand, for open-ended tasks, we find that BFS exposed significantly more unique variable sets and visual designs than DFS, while Dziban exposed significantly more unique visual designs than CompassQL, but not variable sets. However, when it comes to interacted variable sets and visual designs, we do not see any significant difference between BFS and DFS traversal methods and between Dziban and CompassQL oracles. This finding raises an important point: significantly more exposure does not necessarily lead to significantly more interactions. When designing a new visualization recommendation algorithm, exposing more data variants and design variants is a good trend. However, if more exposure does not lead to more interactions, the resulting recommendations may lack the right level of "interestingness" for a worthwhile data exploration experience.</p><p>On the other hand, in terms of participants' preferences, we do find that participants significantly prefer BFS over DFS in the utility and ease of use ratings. Participants also prefer Dziban rather than CompassQL in all metrics (efficiency, utility, ease of use, and overall), although the rating difference is not significant. Participants' post-study feedback also reveals their preference for Dziban as an oracle. Since Dziban takes the perceptual distance into account, participants could better understand why such visualizations are recommended.</p><p>As we mentioned before, Dziban is an improved version of Draco-CQL (a re-implementation of CompassQL), which takes the perceptual distance into account. However, we do not find a significant difference in the user performance between Dziban and CompassQL in focusedtasks, and the only significant improvement in open-ended tasks is that more visual designs are exposed (but not necessarily interacted with) While the Dziban paper did present a comparison with Draco-CQL and GraphScape, it did not consider the user performance. Based on their benchmark results, they claimed that Dziban provides a considerable benefit by suggesting charts that are effective, but also perceptually closed to the current one. Nevertheless, without a framework to evaluate and compare the user performance between algorithms, we do not know whether the benefit would carry over into the actual analysis process.</p><p>From another perspective, we also find that users' preferences change with different analysis tasks, which implies that it is hard for a single algorithm to perform well across all tasks. When designing a new recommendation algorithm, one should think about which type of task to prioritize based on the expected goals of the intended users. Alternatively, the recommendation system could switch to different algorithms depending on the particular task that users want to accomplish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Limitations &amp; Future Work</head><p>Given the necessary level of visualization and analysis expertise for our participants, our recruitment protocol could not leverage standard crowdsourcing platforms, which limited the number of participants that we could feasibly recruit. As a result, we limited our evaluation to two traversal methods and two existing ranking engines, CompassQL and Dziban. However, it would be exciting to involve other promising traversal methods and oracles in future evaluations.</p><p>Given the current COVID-19 restrictions, the entire study was conducted remotely, which made it difficult to fairly perform a longer study session (like the 2-hour session in Voyager's evaluation <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>). Therefore, we took a step back and chose the between-subjects study design, where each participant was exposed to only one recommendation algorithm. However, the result would be more accurate and comparable if we could have conducted a within-subjects study.</p><p>As the study session length is limited, we could only pick a small number of visual analytics tasks to evaluate the user performance, while there exists a larger group of analysis activities in real-life practice <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">16]</ref>. Moreover, our benchmark results imply that analysts prefer different algorithms for different analysis tasks. Thus, one of the promising future work directions would be to include more analysis tasks into the benchmarking to better understand how different algorithms affect the user performance in various analysis tasks.</p><p>In this work, we focused on researching how different recommendation algorithms would affect the performance, behavior, and preference of participants, thus we only included limited interactions in our interface design. However, from the post-study interviews, we find that participants would like to see the interface include more robust functionality, like filtering or supporting user-specified aggregations. It would be interesting to see how the participant performance, particularly when interacting with charts, changes with those extra features, and whether such features would significantly affect the overall study results. Since the source code for our empirical evaluation is publicly accessible, it would be easier to accomplish the aforesaid incremental evaluations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Illustration of the visualization design space and the enumeration step of a hybrid recommendation algorithm, using movies data as an example. The user's current visualization is at node n 0 .</figDesc><graphic coords="2,307.43,140.57,99.74,55.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Interface for the user study. The top panel (A) provides the button to view the bookmark gallery. The data panel (B) contains the dataset name and data fields. Users can manually select which fields to be visualized. The visualization gallery contains the specified view (C) and related views (D). The specified view displays the current specified chart while related views show recommended charts relevant to the specified chart. The task panel (E) contains the current task and also the post task questionnaire.</figDesc><graphic coords="6,82.31,49.37,436.22,184.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: The predicted accuracy of focused tasks for all recommendation algorithms. We show posterior distributions, 50% and 95% CIs of expected titer thresholds for both Movies and Birdstrikes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The number of exposed variable sets and visual designs of open-ended tasks among all recommendation algorithms. We show posterior distributions, 50% and 95% CIs of expected titer thresholds.</figDesc><graphic coords="7,371.39,164.21,71.26,58.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The differences in average numbers of exposed variable sets and visual designs. We show posterior distributions, 50% and 95% CIs of expected titer thresholds for both Birdstrikes and Movies dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Fig. 9: The number of interacted variable sets and visual designs of open-ended tasks among all recommendation algorithms. We show posterior distributions, 50% and 95% CIs of expected titer thresholds.</figDesc><graphic coords="8,187.31,58.37,67.34,58.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 :</head><label>11</label><figDesc>Fig.11:The differences in user preference rating. We show posterior distributions, 50% and 95% CIs of expected titer thresholds.</figDesc><graphic coords="8,390.47,56.93,62.61,62.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>A comparison of enumeration constraints for existing visualization recommendation algorithms.</figDesc><table><row><cell>Algorithms</cell><cell># of Attributes</cell><cell>Transformations</cell><cell>Encodings</cell></row><row><cell>Voyager, Dziban, Show Me</cell><cell>N/A</cell><cell>aggregation, binning, sorting</cell><cell>position, length, area, shape, color</cell></row><row><cell>Foresight</cell><cell>≤ 2</cell><cell>N/A</cell><cell>position, length</cell></row><row><cell>DeepEye</cell><cell>= 2</cell><cell>aggregation, binning, sorting</cell><cell>position, length, area</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>3.1.2, Show Me recommends visual encodings based on user-specified attributes and data transformations. By assuming the attributes and transformations are fixed, Show Me can enumerate and rank all relevant nodes that vary only by visual encodings (see Eq. 2).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>List of all task prompts used in the study for the Movies dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/vega/vega-datasets/blob/master/data/movies.json 2 https://github.com/vega/vega-datasets/blob/master/data/birdstrikes.csv</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://aspredicted.org/blind.php?x=2vi5tq</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank the HCIL, the BAD Lab, and our paper reviewers for their thoughtful feedback. This work was supported in part by NSF award IIS-1850115 and an Adobe Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="DOI">10.1109/INFOVIS.2005.24</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Proceedings of the 2005 IEEE Symposium on Information Visualization, INFOVIS &apos;05</title>
				<meeting>the the 2005 IEEE Symposium on Information Visualization, INFOVIS &apos;05<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Database benchmarking for supporting real-time interactive querying of large data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Catarci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Santucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Binnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3389732</idno>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Characterizing exploratory visual analysis: A literature review and evaluation of analytic provenance in Tableau</title>
		<author>
			<persName><forename type="first">L</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="145" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semiology of Graphics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bertin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>University of Wisconsin Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">User-Oriented Generation of Contextual Visualization Sequences</title>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computing Machinery</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The meaning of &quot;significance&quot; for different types of research [translated and annotated by eric-jan wagenmakers, denny borsboom, josine verfhagen, rogier kievit, marjan bakker, angelique cramer, dora matzke, don mellenbergh</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>De Groot</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and han lj van der maas</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<idno type="DOI">10.1016/j.actpsy.2014.02.001</idno>
	</analytic>
	<monogr>
		<title level="j">Acta Psychologica</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="188" to="194" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Foresight: Recommending visual insights</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pedapati</surname></persName>
		</author>
		<idno type="DOI">10.14778/3137765.3137813</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1937" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data2vis: Automatic generation of data visualizations using sequence-to-sequence recurrent neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dibia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCG.2019.2924636</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="33" to="46" />
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MuVE: Efficient multiobjective view recommendation for visual data exploration</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ehsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sharaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chrysanthis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2016.7498285</idno>
	</analytic>
	<monogr>
		<title level="m">32nd IEEE International Conference on Data Engineering</title>
				<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-05-16">2016. May 16-20, 2016. 2016</date>
			<biblScope unit="page" from="731" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Google fusion tables: Data management, integration and collaboration in the cloud</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Langen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shapley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/1807128.1807158</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Symposium on Cloud Computing, SoCC &apos;10</title>
				<meeting>the 1st ACM Symposium on Cloud Computing, SoCC &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Behavior-driven visualization recommendation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/1502650.1502695</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Intelligent User Interfaces, IUI &apos;09</title>
				<meeting>the 14th International Conference on Intelligent User Interfaces, IUI &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graphical histories for visualization: Supporting analysis, communication, and evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1189" to="1196" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">VizML: A machine learning approach to visualization recommendation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DIVE: A mixed-initiative system supporting integrated data exploration workflows</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Orghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hidalgo</surname></persName>
		</author>
		<idno>doi: 10.1145/ 3209900.3209910</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human-In-the-Loop Data Analytics, HILDA&apos;18</title>
				<meeting>the Workshop on Human-In-the-Loop Data Analytics, HILDA&apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A model and framework for visualization exploration</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Jankun-Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="357" to="369" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enterprise data analysis and visualization: An interview study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paepcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2012.219</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2917" to="2926" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">VizDeck: Self-organizing dashboards for visual analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Key</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aragon</surname></persName>
		</author>
		<idno type="DOI">10.1145/2213836.2213931</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIG-MOD International Conference on Management of Data, SIGMOD &apos;12</title>
				<meeting>the 2012 ACM SIG-MOD International Conference on Management of Data, SIGMOD &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="681" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Assessing effects of task and data distribution on the effectiveness of visual encodings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="157" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GraphScape: A model for automated reasoning about visualization similarity and sequencing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025866</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2628" to="2638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The value of preregistration for psychological science: A conceptual analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Japanese Psychological Review</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dziban: Balancing agency &amp; automation in visualization design via anchored recommendations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376880</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI &apos;20</title>
				<meeting>the 2020 CHI Conference on Human Factors in Computing Systems, CHI &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepEye: Towards automatic data visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2018.00019</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 34th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image graphs -a novel approach to visual data exploration</title>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE Visualization 1999 Conference (VIS &apos;99), VISUALIZATION &apos;99</title>
				<meeting>the 10th IEEE Visualization 1999 Conference (VIS &apos;99), VISUALIZATION &apos;99<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automating the design of graphical presentations of relational information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<idno>doi: 10. 1145/22949.22950</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="110" to="141" />
			<date type="published" when="1986-04">Apr. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show Me: Automatic presentation for visual analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70594</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1144" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Formalizing visualization design knowledge as constraints: Actionable and extensible models in Draco</title>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno>doi: 10. 1109/TVCG.2018.2865240</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="438" to="448" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Beyond heuristics: Learning visualization design</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saket</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dibia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno>abs/1807.06641</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A rank-by-feature framework for interactive exploration of multidimensional data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1057/palgrave.ivs.9500091</idno>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="96" to="113" />
			<date type="published" when="2005-05">05 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast-forwarding to desired visualizations with zenvisage</title>
		<author>
			<persName><forename type="first">T</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Parameswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SeeDB: Efficient data-driven visualization recommendations to support visual analytics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<idno type="DOI">10.14778/2831360.2831371</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
				<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2015-09">Sept. 2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2182" to="2193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ManyEyes: A site for visualization at internet scale</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kriss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mckeon</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2007.70577</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1121" to="1128" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DataShot: Automatic generation of fact sheets from tabular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="895" to="905" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An analytic data engine for visualization in tableau</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wesley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Terlecki</surname></persName>
		</author>
		<idno type="DOI">10.1145/1989323.1989449</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;11</title>
				<meeting>the 2011 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1185" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wicherts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Veldkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Augusteijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Aert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Van Assen</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2016.01832</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1832</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards a general-purpose query language for visualization recommendation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939502.2939506</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human-In-the-Loop Data Analytics, HILDA &apos;16</title>
				<meeting>the Workshop on Human-In-the-Loop Data Analytics, HILDA &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Voyager: Exploratory analysis via faceted browsing of visualization recommendations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2015.2467191</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="649" to="658" />
			<date type="published" when="2016-01">Jan 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Voyager 2: Augmenting visual analysis with partial view specifications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025768</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17</title>
				<meeting>the 2017 CHI Conference on Human Factors in Computing Systems, CHI &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2648" to="2659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A survey on automatic infographics and visualization recommendations. Visual Informatics, 4</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.visinf.2020.07.002</idno>
		<imprint>
			<biblScope unit="volume">08</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
