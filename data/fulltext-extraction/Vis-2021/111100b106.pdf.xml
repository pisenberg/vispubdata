<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Crescentia</forename><surname>Jung</surname></persName>
							<email>csjung@wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Shubham</forename><surname>Mehta</surname></persName>
							<email>smehta23@wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Atharva</forename><surname>Kulkarni</surname></persName>
							<email>akulkarni23@wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yuhang</forename><surname>Zhao</surname></persName>
							<email>yuhang.zhao@cs.wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yea-Seul</forename><surname>Kim</surname></persName>
							<email>yeaseul.kim@cs.wisc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">36BF65820B646903B04128CF1A56E65C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>accessible visualization</term>
					<term>assistive technologies</term>
					<term>alternative text for graphics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would.</p><p>The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users' cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visualizations are a powerful medium to summarize complex data effectively, as they offload the cognitive burden of a reader by leveraging visual perception <ref type="bibr" target="#b14">[17]</ref>. The visualization community has devised theories and principles to design effective visualizations so that readers can extract insights without much mental effort. As a result, visualizations are pervasive across the media, scientific communities, and government agencies to present data. In addition to efficient data communication, visualizations also provide credibility to the content presented in articles <ref type="bibr" target="#b45">[48]</ref> or can complement the content <ref type="bibr" target="#b20">[23]</ref>.</p><p>However, people with visual impairments have been largely underserved in receiving the benefit of "using vision to think" that visualizations offer. Not being able to understand visualizations may result in aggravating information inequality of people with visual impairments. A few assistive technologies have been proposed to bridge this gap over the past years, attempting to make visualizations accessible to a broader audience. For example, popular approaches include tactile visualizations (e.g., <ref type="bibr" target="#b86">[89,</ref><ref type="bibr" target="#b35">38]</ref>) that use embossed surfaces to present visualizations and data sonification (e.g., <ref type="bibr" target="#b63">[66,</ref><ref type="bibr" target="#b87">90]</ref>) that maps the data to various dimensions (e.g., pitch, volume) of sounds. However, integrating these technologies into the online environment, in which we frequently consume data, is a major challenge.</p><p>Communicating visualizations with text can be a practical way to convey the information in visualizations, especially in web environments. For example, HTML supports many ways to add invisible text fields near graphics to allow people with visual impairments to access the description of graphics through screen readers. These descriptions used to replace graphics are known as alternative text (alt text). Alt text serves as a textual substitution of a graphic that describes the visual components within it. This helps people to construct the graphic mentally when they have no access to the graphic. Several organizations provide guidelines to create alt text for visualizations. However, the guidelines do not offer rationales and empirical evidence for their recommendations. Thus, it remains unclear whether the suggested ways of generating alt text align with what people with visual impairments need or how effective the recommendations are.</p><p>In this work, we investigate how to formulate alt text for visualizations to best serve people with visual impairments. As a first step, we surveyed current guidelines on generating visualization alt text. Next, we reviewed current practices to understand to what extent people follow the guidelines and what other approaches people use when formulating alt texts. As we could not find many examples of alt texts generated for visualizations from major news outlets, we collected alt texts from academic papers. We focused on alt texts written by people who have expertise in visualizations and graphics (IEEE VIS &amp; TVCG), have knowledge of accessibility (ACM ASSETS), or are often prompted to write alt texts for visualizations (ACM CHI as the submission system encourages authors to submit alt texts for figures). We found that people use diverse strategies, ranging from providing brief explanations of the charts to listing all data points. The observed practices are only partially aligned with the surveyed guidelines.</p><p>To evaluate the surveyed guidelines and further probe information needs for visualization alt texts, we interviewed 22 participants with visual impairments. In the study, participants examined visualizations by reading the alt text formulated with several strategies through their screen readers. Then, participants were asked to share their preferences and opinions on the strategies and provide insights toward the ideal visualization alt texts. Our findings support many of the surveyed guidelines, such as the need to provide information about the chart type, axes, access to data points, and description of data trends. However, contrary to guidelines, some participants want to enrich their mental picture of the visualization by knowing visual attributes used in visualization, such as color encoding. This is in line with another finding of the study -that participants wish to "visualize" visualizations in their head while listening to alt texts. This finding implies that alt texts should describe the necessary visual components to fill the gap in their imagination and ease the construction of an image of the visualization. Additionally, our study findings provide evidence to guide how an author should generate alt texts for their visualizations, including which component to mention first, how an author should describe the trend in the visualization, and what language tone the author should maintain.</p><p>Our contribution is three-fold:</p><p>1. We survey and summarize existing guidelines for the generation of visualization alt text. We also analyze current practices to demonstrate how aligned they are with the guidelines and identify other commonly used strategies. 2. We report findings from an interview study with 22 people with visual impairments that provide insights into the properties of good alt texts and into the role of visualizations for people with visual impairments to support people with visual impairments in the context of reading online news. 3. We propose actionable recommendations for generating informative alt texts along with rationales and suggest system-level improvements (e.g., structures of HTML, multimodal integration) informed by the study to better support visualization interpretation for people with visual impairments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>People with visual impairments are a large population that includes people who are blind and have low vision. According to the World Health Organization, about 285 million people in the world have visual impairments, of whom 39 million are entirely blind <ref type="bibr" target="#b57">[60]</ref>. Due to their vision loss, this population faces difficulties in understanding graphics, which hinders their education and employment opportunities.</p><p>With the advances of the Internet and computer technology, numerous physical readings and learning materials have been digitized into web pages or other digital formats (e.g., pdf, doc). To enable people with visual impairments to access digital information, screen readers have been developed. Screen readers are software programs that scan the text on a screen and communicate the information to people with visual impairments via speech (some also have braille output via a braille display) <ref type="bibr" target="#b29">[32]</ref>. Users control which components in the page the screen reader communicates through several key combinations on the keyboard or a touch gesture on a smartphone. Most mainstream computer or smartphone systems have embedded screen readers. Some examples include VoiceOver for Mac and iOS <ref type="bibr" target="#b6">[9]</ref>, Narrator for Windows 10 <ref type="bibr" target="#b50">[53]</ref>, and TalkBack for Android <ref type="bibr" target="#b38">[41]</ref>. There are also standalone screen reader softwares, such as JAWS <ref type="bibr" target="#b67">[70]</ref> and NVDA <ref type="bibr" target="#b3">[6]</ref>.</p><p>While making text more accessible, screen readers cannot interpret images for people with visual impairments. Alternative text is the major source that screen readers rely on to communicate graphics. Alternative text, also known as "Alt text," was first introduced to the HTML 2.0 specification in 1995 <ref type="bibr" target="#b9">[12]</ref>. Alt text is an invisible text block that can be inserted as an alt attribute of the &lt;img&gt; tag. The original purpose was to inform web viewers of an image's content when the image could not show properly. Nowadays, alt text is commonly used by people with visual impairments to access image content via screen readers.</p><p>Adding alt text to images has thus become one of the most important principles for web accessibility <ref type="bibr" target="#b82">[85]</ref>. Similarly, the longdesc attribute in an &lt;img&gt; tag is used to provide a more lengthy explanation about non-text elements <ref type="bibr" target="#b2">[5]</ref>. URLs, a location in the page referred by id or class, or text can be part of longdesc. Even though longdesc is deprecated in HTML5, WCAG still recommends it to be provided for complex graphics.</p><p>For other graphic formats such as SVG elements, the &lt;title&gt; and &lt;desc&gt; tags can also be used to convey the contents of visualizations in an easily accessible textual form. However, compatibility issues with some screen readers have been reported <ref type="bibr" target="#b81">[84,</ref><ref type="bibr" target="#b69">72]</ref>. Beyond web pages, alt text features have been extended to various digital document formats (e.g., docx, pptx, pdf). Commercial content-editing and viewing software, such as Microsoft Word and Adobe Acrobat, also enables users to add alt text to images, thus making digital documents more accessible to people with visual impairments <ref type="bibr" target="#b49">[52,</ref><ref type="bibr" target="#b4">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visualization accessibility for visually impaired people</head><p>Several strategies have been explored for communicating visualizations to people with visual impairments by relying on senses other than vision. One popular approach is sonification <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b78">81,</ref><ref type="bibr" target="#b63">66]</ref>, which seeks to encode the data of a chart into various dimensions (e.g., pitch, volume, pan) of an audio signal. Several studies have shown that audio charts can communicate patterns in the data effectively <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b63">66]</ref>.</p><p>Data sonification has been used on various visualizations such as line graphs, bar graphs, and maps <ref type="bibr" target="#b87">[90,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b39">42]</ref>. Tactile visualizations realized via haptic feedback, braille display, or embossed prints are another popular approach to enhance accessibility for people with visual impairments <ref type="bibr" target="#b80">[83,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" target="#b86">89,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b40">43,</ref><ref type="bibr" target="#b23">26]</ref>. Yet, the low-resolution nature of tactile visualizations limits their use for more complex visualizations <ref type="bibr" target="#b26">[29]</ref>. Another recent exploration suggests that olfaction can be use to perceive data <ref type="bibr" target="#b59">[62]</ref>. To overcome the limitation of one modality, multiple sensory inputs can also be combined to complement each other <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b33">36,</ref><ref type="bibr" target="#b71">74,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b79">82,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b46">49]</ref>.</p><p>Summarizing visualizations in textual form is another way to convey visual information to people with visual impairments <ref type="bibr" target="#b22">[25,</ref><ref type="bibr" target="#b52">55,</ref><ref type="bibr" target="#b34">37]</ref>. EvoGraphs is a jQuery plugin that enables creating visualizations that can be read by screen readers <ref type="bibr" target="#b68">[71]</ref>. EvoGraphs automatically formulates an alt text that contains each data point and some representative values of the data (e.g., max, min, mean). However, these systems often focus on one or two basic chart types, such as bar and line charts.</p><p>While not aiming at supporting people with impairment specifically, several techniques have been proposed to automatically generate captions for visualizations <ref type="bibr" target="#b28">[31,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b51">54,</ref><ref type="bibr" target="#b41">44,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b56">59,</ref><ref type="bibr" target="#b83">86,</ref><ref type="bibr" target="#b13">16]</ref>. A more thorough survey on accessible visualizations can be found <ref type="bibr" target="#b44">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Study on alternative text for images</head><p>Unlike visualization alt texts, methods to generate alt texts for images (i.e., image description 1 ) have been explored extensively. As presented in <ref type="bibr" target="#b70">[73]</ref>, alt text generation can be categorized in three groups: human-generated (e.g., <ref type="bibr" target="#b10">[13]</ref>), computer-generated (e.g., <ref type="bibr" target="#b74">[77,</ref><ref type="bibr" target="#b85">88]</ref>), and hybrid approaches (e.g., <ref type="bibr" target="#b53">[56,</ref><ref type="bibr" target="#b64">67,</ref><ref type="bibr" target="#b65">68]</ref>). While many efforts have been put into investigating formulating and evaluating image descriptions, a large proportion of images found online lack alt text descriptions <ref type="bibr" target="#b32">[35]</ref>.</p><p>Recommendations for alt text formulation depend on the purpose of the image, presented objects, individuals, active motions, location, colors, and emotions <ref type="bibr" target="#b60">[63]</ref>. Studies in image alt text often focus on social network contexts where people share their photos actively <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b54">57,</ref><ref type="bibr" target="#b77">80,</ref><ref type="bibr" target="#b88">91]</ref>. Updated guidelines for social networking sites suggest containing elements such as the number of people and facial expressions <ref type="bibr" target="#b84">[87]</ref>. Recent research demonstrates that people with visual impairments have different needs based on the context where they encounter the images (e.g., social network, e-commerce) <ref type="bibr" target="#b70">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PHASE 1: GUIDELINES FOR VISUALIZATION ALT TEXTS</head><p>To understand the current standard, we collected the existing guidelines for generating visualization alt texts. Since the goal of alt texts is to replace the graphics, most guidelines address how to describe visual components in visualizations to support people to mentally construct visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Guideline collection</head><p>We collected guidelines via Google search using relevant keywords such as visualization accessibility, visualization alternative text, etc., resulting in 31 postings. We excluded accessibility guidelines that were not explicitly related to blind or low vision individuals (e.g., guidelines for color blind people), guidelines not focused on formulating alt texts, or guidelines created by citing other guidelines. This process resulted in four sets of guidelines, namely the WCAG guidelines [1], the Penn State's accessibility guidelines <ref type="bibr" target="#b0">[2]</ref>, the Diagram Center's guidelines [4], and CFPB's guidelines <ref type="bibr" target="#b76">[79]</ref>. The Web Content Accessibility Guidelines (WCAG) 2.1 is a widely used collection of recommendations for increasing the accessibility of Web content <ref type="bibr" target="#b1">[3]</ref> Penn tion of the image must be provided. In appropriate cases, State <ref type="bibr" target="#b0">[2]</ref> a numeric table representing the chart data will provide additional accessibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagram</head><p>Charts and graphs must be converted into accessible ta-Center <ref type="bibr">[4]</ref> bles; a brief description and a summary, if needed, should be provided. Additional information such as the title and axis labels should be included as well. However, visual attributes such as color are not necessary to include unless there is an explicit need for them. CFPB <ref type="bibr" target="#b76">[79]</ref> The alt texts should include one sentence of what the chart is and the chart type. There should also be a link to a CSV or another machine-readable data format with the raw data. Moreover, the data must have descriptive column labels. Take into consideration that screen readers do not let users skip or speed up while reading alt texts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Findings</head><p>Table <ref type="table" target="#tab_1">1</ref> summarizes the four guidelines. These guidelines offer recommendations for generating alt texts to visualizations, including the structure of alt text and which components to include. Structure of alt text. WCAG recommends two-part alt texts for complex visualizations, with the first part containing a short description of the chart and the second part a more detailed description <ref type="bibr">[1]</ref>. WCAG suggests not to include the long description in an alt text. Instead, they propose various ways to incorporate long descriptions, for example, by specifying a link to the long description with an &lt;a&gt; tag adjacent to the chart or using the longdesc attribute. According to WCAG, the short description should also indicate how/where to access the long description.</p><p>Concerning length, different guidelines prescribe varying principles. While WCAG [1] encourages longer descriptions as visualizations contain substantial amount of information, other guidelines [4, 2, 79] suggest one or two sentences. For example, CFPB asserts that alt texts must be short yet descriptive since screen readers may not allow users to skip or speed up while reading alt text <ref type="bibr" target="#b76">[79]</ref>.</p><p>Components in alt text. Alt text should provide a meaningful and informative description of a visualization that is sufficient for a reader to understand its content <ref type="bibr" target="#b76">[79]</ref>. To support this goal, CFPB and the Diagram Center's guidelines encourage alt texts to include a one-sentence summary of the chart, chart type, and axis labels <ref type="bibr" target="#b76">[79,</ref><ref type="bibr">4]</ref>. WCAG also suggests mentioning all visually presented scales and values <ref type="bibr">[1]</ref>. According to the Diagram Center's guidelines, visual attributes (e.g., the color of a bar or line types) don't need to be explained unless there is a special need <ref type="bibr">[4]</ref>. Some guidelines also encourage to include a summary of the data trends [4, 79, 1].</p><p>Data tables. For complex visualizations, the conventional alt text provided via the alt attribute in HTML may not suffice to provide the information needed. In addition to a text summary, a data table can further enhance accessibility to a visualization. Both CFPB's and Diagram Center's guidelines suggest including a link to the data represented in the visualization in a format accessible to screen readers, e.g., CSV or other machine-readable formats <ref type="bibr" target="#b76">[79,</ref><ref type="bibr">4]</ref>. Also, tables should contain descriptive column labels <ref type="bibr" target="#b76">[79]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Summary</head><p>Overall, the guidelines emphasize the need for descriptive and succinct language in alt texts, including information on the chart type, axes, and data trends. Some visual attributes (e.g., color) may be omitted unless there is an explicit reason. Many sources echoed that formatted tables are essential in understanding visualizations, even though they are not a part of alt texts. While these guidelines provide a useful starting point, they lack rationales for why each component should or should not be included. Furthermore, the guidelines do not reference empirical evidence of how they support the needs of people with visual impairments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PHASE 2: ANALYZING CURRENT PRACTICES</head><p>In addition to surveying guidelines, we inspected current practices to further understand the status quo. The goal is to examine whether authors follow the guidelines, and if not, to identify which strategies authors use when formulating alt text for visualizations.</p><p>We sought to collect alt text and visualization pairs from online media outlets as this is a common way in which the general public consumes visualizations. We sampled 30 visualizations from three major news outlets (NYT, The Washington Post, FiveThirtyEight) who frequently published visualizations alongside articles. However, we found no alt texts associated with the sampled visualizations (no alt attribute if the visualization was presented as an image element, nor desc tag if the visualization was presented as an SVG element). Hence, we shifted our approach to obtain examples from academic publications. Specifically, we collected alt text and visualization pairs from IEEE VIS &amp; TVCG, ACM ASSETS, and ACM CHI over the last two years (i.e., 2019, 2020). We chose these three publication venues to favor authors with expertise in visualizations and graphics (IEEE VIS &amp; TVCG), with knowledge of accessibility (ACM ASSETS), or authors that are often prompted to write alt texts for visualizations (ACM CHI as the submission system encourages authors to submit alt texts for figures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data collection &amp; data preliminary</head><p>We downloaded the academic publications in a PDF format from the IEEE Xplore digital library and ACM digital library. We then converted the PDFs to accessible text containing the embedded alt texts using Adobe Acrobat. Next, we used a custom Python script to identify and extract alt texts from the accessible text. We also downloaded the figures from Semantic scholar and mapped them to the extracted alt texts by matching both the publication DOI and the figure number.</p><p>We collected total of 2,278 publications (VIS&amp;TVCG:723, AS-SETS:95, CHI:1,460) with 7,493 figures (VIS&amp;TVCG:2,518, AS-SETS:281, CHI:4,694). Among those figures, 40% contained alternative text descriptions (VIS&amp;TVCG:0%, ASSETS:65%, CHI:51%). To filter only visualizations (e.g., bar, line, area chart, scatterplot, boxplot etc.) from plain images, one researcher reviewed the figures manually. After filtering, we removed the alt texts that only contain the figure number (e.g., "Figure <ref type="figure" target="#fig_0">1"</ref>) or random placeholders (e.g., "abc"), resulting in 0 alt text-visualization pairs from the IEEE VIS &amp; TVCG collection, 89 pairs from the ASSETS collection and 752 pairs from the CHI collection.</p><p>For each pair in the aggregated collection, we marked whether the alt texts mentioned summary, chart type, axes, data trends, visual attributes (color and shape), and data points. Two researchers annotated all alt text-visualization pairs independently and discussed them together to resolve disagreements (there were around 5% of disagreements initially). We also calculated the length of each alt text using NLTK's sentence tokenizer <ref type="bibr" target="#b11">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the frequency that each component appears in alt texts for the ASSETS and CHI collections. Almost all alt texts include a summary that describes the visualization's overall topic (e.g., "Input speed of three techniques"). The second most common component for the ASSETS collection was axes information (65 out of 89 alt texts, 73%). In the CHI collection, chart type was mentioned 34% of the   time (254 out of 752 alt texts). In both collections, visual attributes (e.g., color and shape) were the least common among all components.</p><p>We also analyzed which combination of components was most used when formulating alt texts (Fig. <ref type="figure" target="#fig_1">2</ref>). A summary of the visualization together with axes, ticks, and data points was the most common combination (18%) in the ASSETS collection. The second most common combination also included data trends in addition to the summary, axes, ticks, and data points (13%). Finally, alt texts that only provide a summary were the third most common (8%). In the CHI collection, a summary alone was the most common alt text type (48%), followed by a summary with the chart type (12%) and a summary with data points (4%). The average length was 3.5 sentences (Median=3, SD=3.3). Alt texts from ASSETS (M=4.4, SD=2.9) were slightly longer (t=-2.8, p¡0.01) than those in the CHI collection (M=3.5, SD=3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Summary</head><p>The analysis shows that the authors do not entirely follow current guidelines. For example, the authors only mention the actual data points, chart type, and data trends about half of the time in the AS-SETS collection and less than a third of the time in the CHI collection. While alt texts from the ASSETS collection are better aligned with the guidelines, only half of the alt texts contain the chart type (e.g., "this chart describes" instead of "this bar chart describes"). Without mentioning the chart type, visually impaired individuals may struggle to imagine the depicted visualization. The CHI collection seriously lacks visualization accessibility since 48% of the time, the only available information is the summary. Length-wise, the collected alt-texts were longer on average than the guidelines often prescribe, but the numbers were highly varied.</p><p>All 30 visualizations we sampled from major news outlets did not contain alt texts associated with the visualizations. Since visualizations are a critical part of information consumption, this observation is alarming. Similarly, the IEEE VIS &amp; TVCG collection did not include any text alternatives to the figures. To exclude the possibility that the script couldn't detect alt texts from the collection, we randomly chose 100 papers and manually checked them through Adobe Acrobat. We urge the IEEE VIS community to employ a system that encourages the use of alt texts to make science more accessible to people with visual impairments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PHASE 3: SEMI-STRUCTURED INTERVIEW</head><p>To evaluate the guidelines and further understand the needs of people with visual impairments concerning visualization alt texts, we conducted semi-structured interviews. Specifically, the goal of the study is to identify empirical evidence to motivate guidelines and further derive implementation in alt text formulation for visualization. We examined participants' behaviors in the news consumption scenario, which we believe is one of the frequent encounters of visualizations online for the general public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Participants</head><p>We solicited study participation by circulating an IRB-approved flyer on listservs hosted by organizations serving the blind and low vision population. Our recruitment criteria were designed to recruit participants who are 1) at least 18-year-old, 2) legally blind, and 3) using screen readers daily. We received 150 responses from potential participants, contacted participants for the interview on a first-comefirst-served basis, and recruited participants until we saturated the findings <ref type="bibr" target="#b5">[8]</ref>. The final pool of participants consisted of 22 participants (15 female, 7 male), whose ages ranged from 20 to 46 (M=28.6, SD=6.2, Table <ref type="table" target="#tab_3">2</ref>). Among 22 participants, 21 were blind, and 1 had low vision. All interviews were conducted via Zoom. Each interview lasted on average 60 minutes with a standard deviation of approximately 10 minutes. We compensated their participation with a $20 Visa gift card. The numbers show that the largest negative net changes, hence the decrease and cuts in jobs, came from the leisure and hospitality sector. In comparison, the largest increase in jobs came from the professional and business services. Utilities and the construction sector come at the middle of the bar graph, depicting the least amount of net change. In the map, each state is filled with the color that corresponds to the state's data. A percentage of 6% is depicted in blue, 12% depicted in grey, 21% depicted in purple on a gradient scale. Statewide averages with select metro areas are overlaid as circles.</p><p>The map depicts that states within the Northeast, Midwest, and the upper West seem to have percentages within the lower 6% to 12% range compared to the Southwest and Southeast states that are generally within the 12% to 21% range. Overall, the southern part of the U.S. have more severe. problem than the north part of the U.S.</p><p>The first row is a table header In terms of the daily dose, it has an increasing trend with a few zero days and some spikes. The seven-day average is overall increasing gradually. There are spikes of a significant number of new doses reported around January 11th with approximately 2.25 million doses, and on January 19th, nearly 3.5 million doses were reported per day.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Study stimuli</head><p>To provide the context of a real-world visualization reading scenario, we prepared four articles from online news covering different topics.</p><p>We included multiple chart types to maximize participants' exposure to different components (e.g., bar, line) of the visualization, as well as the number of variables encoded to enable participants to examine additional encodings (e.g., color), to expose them to varying complexities. Figure <ref type="figure" target="#fig_4">3</ref> shows the selected visualizations.</p><p>In creating the study stimuli, we shortened the original article text to only one or two paragraphs related to the visualization, so as to conduct the study in a reasonable amount of time (e.g., 1 hour).</p><p>Three out of the four visualizations (V2, V3, V4) were in an SVG format. For SVG elements, desc tags serve to place alt texts, equivalent to the alt attribute for an image. However, due to potential comparability issues of desc tags with some screen readers <ref type="bibr" target="#b69">[72]</ref>, we converted the three visualizations to a bitmap image and added an alt text via the alt attribute.</p><p>For each of the four visualizations, we formulated alt texts in four different styles informed by Phase 1. The four styles contain different visualization components and different specificity mentioned in the guidelines to prompt participants to think about their preference when some components and the specificity are present and absent (Fig 3 <ref type="figure">):</ref> • Brief description (Fig. <ref type="figure" target="#fig_4">3a</ref>): Since several guidelines from Sec. 3 emphasize to have a short description [1, 79], we formulated a brief description. We first described the type of visualization <ref type="bibr" target="#b76">[79]</ref> followed by a summary of the data that the visualization represents.</p><p>• Detailed description (Fig. <ref type="figure" target="#fig_4">3b</ref>): A few guidelines encourage to include more detailed aspects of the visualization <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>. In this style, we first described the type of chart followed by a summary, a depiction of visualization elements including axes, range, marks, and mapping. To evaluate the guideline discouraging description of visual attributes (e.g., color, shape) [4], we included them in this style of description, when applicable.</p><p>• Data trends (Fig. <ref type="figure" target="#fig_4">3c</ref>): Data trends are another component that many guidelines suggested to include in alt texts [4, 79, 1]. We described the overall trends that were visually apparent.</p><p>• Data points (Fig. <ref type="figure" target="#fig_4">3d</ref>): All the guidelines we surveyed highlight the importance of having access to raw data [2, 79, 4, 1]. To validate the emphasis on having a separate table element in the guidelines, we created two different ways to access the data. The first version includes data points within the alt texts, which sequentially read out the raw data. The second version includes the data in a formatted HTML table. This allows the participants to navigate the table with their screen readers. In this case, we indicated that a table could be found below within the alt text.</p><p>We formulated four different alt texts styles for each of the four chosen visualizations, resulting in 16 different stimuli. All participants saw all of the four articles paired up with one of each of the alt text styles. We counterbalanced the pairing between the articles and styles, and randomized the order of presenting the articles. Specifically for the Data points style, half of the participants saw the data points in the alt text and the other half saw the data points in an HTML format.</p><p>Fig. <ref type="figure">4</ref>. Study procedure. We asked participants their demographic information, conditions, and prior experience. After that, participants examined each visualization once at a time by reading the alt text with their screen readers. After each visualization, we gave them a full description of the visualization and asked them to improve the alt text based on their information needs. After examining all of the visualizations, we asked questions regarding their preferences and needs toward alt texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Procedure</head><p>Figure <ref type="figure">4</ref> shows the overall procedure of the interview. We first asked participants demographic questions, including their age, gender, education level, occupation, as well as their vision conditions, including their diagnosis, onset age, and remaining vision. Then, we asked about their experience with assistive technologies. Specifically, we asked what assistive technologies they have been using and how long they have been using them. We also asked participants how often they encountered visualizations and the situations they encountered them.</p><p>Next, participants were prompted to open the URLs presenting the study stimuli that were sent just before the study started through email. At that point, we asked them to share their screen to observe which part of the study stimuli they were interacting with. While the participants interacted with the stimuli, we asked them to express what they were doing every step of the way (think aloud).</p><p>We then asked participants what they learned about the visualization and how they learned that information. The example questions include: What do you think that the chart is about? How does the alttext help you learn about the chart? Besides alt-text, have you used any other strategies to understand the chart? After each visualization, we gave a full description of the visualization, consisting of the detailed description, data trends, and the summary of data points (Fig. <ref type="figure" target="#fig_4">3b, c, d</ref>).</p><p>We then asked the participants how they would regenerate or refine the alt texts and which aspect of the given alt texts was helpful to envision the visualization and the underlying data.</p><p>After examining all four visualizations, we asked their preference regarding length, contents, and usefulness of alt texts. Participants were also asked about the role of visualizations in understanding the article, how they compare the alt texts requirements between images and visualizations, and the perfect alt text system that they envision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Analysis</head><p>We first transcribed the recorded study sessions. We conducted a thematic analysis, a method to identify themes within the data <ref type="bibr" target="#b12">[15]</ref>. Three researchers coded three participants' transcripts independently and discussed them together to create a consolidated codebook. Then, we categorized the codes in the codebook based on the emerging themes using affinity diagrams and axial coding. The discussion resulted in 12 themes (e.g., Prior experience, Visual components) and 152 codes. One researcher coded the rest of the transcripts using the codebook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Findings</head><p>We present the findings from our thematic analysis of the semistructured interview conducted on 22 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Prior Experience with Visualizations</head><p>Context. Thirteen participants stated they read visualizations in academic settings. P20 stated, "I encounter visualizations a few times a week at school." The participants that read visualizations in an academic setting expressed that visualizations raise questions such as, "What is this showing? What do these numbers represent?" and lack "any descriptive text except for possibly a caption," as stated by P3.</p><p>Participants also found visualizations while seeking information on the web. As stated by P17, "There's a lot of charts and graphs when looking for information about current events. The charts and graphs show what areas have increased cases and other information." In addition, we observed that another common situation where visualizations appear is in professional settings. As noted by P6, "I encounter charts a lot when I'm observing survey data," and also by P8, "When I am at work there's some charts [that are related to my work]."</p><p>Frequency. In terms of frequency, two participants mentioned that they encounter visualizations on a daily basis, nine participants mentioned more than three times a week, and eleven participants mentioned once a week.</p><p>Assistive technology. Braille displays and tactile materials are a common medium used by participants to read visualizations, especially in academic settings. 15 out of 22 participants explicitly mentioned using them. For example, P10 stated she has been reading visualizations "through tactile braille, embossed braille, and hardcopy materials." P11 shared, P11 "for images specifically I use a screen reader. But the main way that I do this [visualization] is through tactile." Many participants also expressed their familiarity and extended use of braille displays. For example, P15 mentioned that "I read braille, and I've been using that my entire life," and P10 noted that "I have been reading braille since I was in kindergarten, so I've been reading braille for a long time."</p><p>We observed that familiarity with tactile visualizations allowed participants to better understand particular visualizations, such as bar charts. As stated by P10, "We've grown up learning those concepts and know like what a bar chart is, I know what a histogram is." However, some uncommon types of charts that they have never experienced with a braille display might lead to confusion, as noted by P15: ''I've never seen a bubble chart before."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">The Motivation to Understand Visualizations</head><p>The motivation to understand visualizations spanned several reasons, including the access to information not included within the article text, to inspect the claims made in the article, draw independent conclusions, and share the gained information with others.</p><p>To probe different aspects of data that are not mentioned in the article text. Participants discussed that the information obtained from reading the visualizations allows them to observe aspects of the data that are not mentioned in the news article. For example, P14 expressed their frustration that "Even though I get a general understanding, I want to know more than what the author of the article wants you to know. What about the Southeast or the Northeast?" This sentiment was also observed from other participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P17 noted that "The article talks about a couple of different examples, whereas the visualization has far more industries. So if you wanted to look at an industry that wasn't highlighted in the article, it is provided in the visualization."</head><p>To inspect the claims and draw conclusions. The desire to inspect the claims made within an article was also widely echoed among participants. As P6 commented, "The article mentions that there were some decreases in jobs in particular areas and it was helpful that I could look at the visualization and see that number to fully understand what was being discussed." Moreover, while inspecting visualizations, participants had the opportunity to confirm the article's claims or build upon the information given within the article. For example, P17 shared that "Even though I previously read that the trend was increasing gradually overall, after looking at the table, it seems to show that while it is increasing overall, there are some points where the trend flattens out." P17 also noted that "If the claims were to be not honest, I would look at the data." These sentiments about authors' bias and misinformation were observed by other participants. P2 shares that "I am skeptical of any statistic that comes out of nowhere." When no data is provided, the participants expressed their inability to do inspect the claim and draw their own conclusions. P9 went further and asserted that their goal to read visualizations was to "obtain the data" to draw their own version of conclusions.</p><p>To share with others. Participants expressed that they would like to share the information gained from visualizations with others. P14 noted that "I would like the important information in a visualization because I may want to share it with someone sighted and explain what the visualization is conveying." Within the aspect of sharing the gained information from a visualization, P14 mentioned that "I would like to know the trends if I was to share it with someone else."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Constructing Mental Model of Visualizations</head><p>Participants expressed the desire to construct a mental model of a visualization that is similar to what sighted people would view in order to visually interpret the visualizations. Creating a mental image seemed to enhance participants' understanding of visualizations. As mentioned by P9, "I want to make sure I understand the visualization and where the data points are, by trying to visualize it." Similarly, P1 noted that "I imagine the visualization because I like to understand each piece of information." While constructing a mental picture, many participants had follow-up questions regarding the visualization and its details. For example, after having a follow-up discussion regarding the visualization, P10 shared that "The additional information helped knowing how this graph is laid out and I can visualize what is being described." Other participants also stated that having supporting details guided them to construct a better landscape of the visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As demonstrated by P18, "I was drawing with my hand to try to visualize what the chart might look like based on the given alternative text." P12 shared that "If I had more information about the visualization, it would have helped me to mentally visualize what was depicted."</head><p>Overall, participants expressed the desire to know enough visual details about the visualization to imagine them. Many participants connected this aspect to the knowledge that a sighted person would have, such as P6, who noted that "I want to get the same information that a sighted person is getting when looking at a visualization." Additionally, P21 shared, "I want to imitate the way that sighted people would skim the visualization. I would want to see it the way that a sighted person would think about it from their phenomenological experience. I am visualizing what I am missing that would be a normal thought in a sighted person's head." We observed that creating mental visualizations allowed participants to "think like a sighted person when I am reading an alternative text," as explained by P15. Notably, many participants expressed their frustration with the limitations of creating a mental picture of the visualizations. In P14's words, "It is difficult to visualize the whole graph in my head." This sentiment indicates that, although participants attempted to create mental images, constructing and keeping the mental images for long periods is difficult, likely due to the sequential (and non-interactive) nature of audio delivery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Information Needs for Alt-texts</head><p>We learned the participants' expectations and needs regarding what descriptive information to include in visualization alt texts.</p><p>Chart type. 11 out of 22 participants echoed that providing the type of chart is helpful, especially at the beginning of the alt text. Since they learned charts at school, stating the chart type can prepare them to fill in the missing information specific to each chart type. As P10 mentioned, "I think the type of chart is helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is this a pie chart or bar chart, line graph, etc. I think that's a good start to guide a person to think of and imagine [the visualization]."</head><p>Another benefit of starting by stating the chart type is to indicate the beginning of an alt text for the visualization. For example, P11 shared</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"So [I would give] priority to the type of charts. Just to give you an indication where the chart is. Especially [because] some screen readers only mention 'graphics' at the end when they read the image."</head><p>Axis, range, &amp; ticks. Information about the axis in a visualization was often highlighted as necessary to understand visualizations and participants often noticed whenever this information was missing (e.g., with brief descriptions). For example, when P17 was prompted to improve the brief description style of alt texts, they stated: "What's on the X-axis, what's on the Y-axis. That is for sure." P5 also said, "I didn't hear the X and Y-axis. So, that's a little harder for me to follow."</p><p>The range of values in each axis also communicates boundary statistics of the data set depicted by the visualization, which helps form a more concrete picture. As P7 stated, "It <ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>range] gives me the boundaries in the framework of what the chart is telling me."</head><p>We did not observe evidence that participants wish to know tick information. When we mentioned the axes' increments in the detailed description, no participants pointed out this information to be helpful. Also, while examining the visualization with brief descriptions, insights or tables, which do not contain tick information, no participants noticed their absence.</p><p>Data trends. Another important component of alt text for visualizations, as highlighted by 14 out of 22 participants, is the description of data trends. Compared to alt texts for images, communicating visualizations have a purpose, often encapsulated by the visible data trends. As P3 shared, "There's usually a main point that they're trying to emphasize, like the trends in a certain data set. So I think that, for charts, unlike images, it's important for the alternative text to describe the trends that are being demonstrated." Participants expressed that authors should describe data trends in visualization alt texts. For example, P1 wanted a summary of the data trends when that infor-mation was missing from the alt text "'Seven-day average is overall increasing [in stimulus V1].' That's kind of the summary I'd put on the map [in stimulus V2]." P9 mentioned that "[alt text should include] a visual description of what is being described and some relevant pieces of data, for example, trends or peaks." P10 suggested including a "snapshot" of the visualization in alt texts: "A snapshot you can just take a look, and say: okay, that's the trend."</p><p>In describing data trends, referencing data is essential to provide a concrete picture of the visualization instead of simply describing the pattern. For example, while examining stimulus V4, P1 mentioned, "[In addition to acknowledging the performance gap, I'd like to know] how wide the gap is between boys and girls. For example, this chart shows the difference of 20%." Specifying the data range can further help comprehend the data trend, as noted by P4: "[I would describe it as an] upward trend increasing right now, or in the past couple of weeks." When depicting the trend, referencing visual attributes such as color to describe trends would not be ideal as people may not readily make sense of them. For example, when we mentioned "There is a cluster of orange circles positioned relatively top of the chart" as a part of describing V4, P17 shared that "[I would prefer it to] just say English and math instead of being the orange and blue circles."</p><p>Beyond the trends, 4 out of 22 participants also wanted to be aware of visible outliers, spikes, or dips in the data. For example, P9 shared, "To be able to visualize it, I would definitely add more of those dates, in particular the spikes or [dates] that have zero <ref type="bibr">[cases]</ref>." Similarly, when prompted for their ideal alt text, P2 suggested, "I would say a line graph showing the trends with spikes on these certain dates." P1 wondered, "There are some trends from one to three million and it's going up, but are there also some spikes or is it going up overall?"</p><p>Colors. Participants' preferences toward information regarding colors were varied. Some participants wanted to know, while others did not. Learning about the colors seemed to be a personal preference. Among those who favored knowing the colors, they were especially curious when the color encoded data and when contextualizing the color scale was not challenging.</p><p>To justify their preference, participants often stated that color helps them picture the visualization. It also helps them understand how the visualization uses colors to represent the data. For example, P16 said that "I personally like that the different colors for the positive and negative changes are mentioned. I just think it levels the playing field a bit." P3 shared, "Those details, like colors, aren't necessarily needed, but I think it helps to beef up my understanding of what the chart looks like." On several occasions, participants wanted color information only when it encoded data. For example, when P2 examined V2, she mentioned, "I don't really care about the colors [green bar]; I just care about how many vaccines are being distributed." However, she stated while reading V1, "With the rest of the information depicted in the table, all you needed [in the alt text] was a color key."</p><p>The communication of color was especially frowned upon when it encoded a continuous value (V2) using a color gradient schema. In this case, participants seemed to prefer direct access to the values instead of contextualizing them through the color information. As stated by P18, "If the information in the alt texts were to describe the different colors, that would have just been extraneous. It would have been more information than was necessary. It wouldn't have been necessary because the percentages are there."</p><p>Regardless of their preferences, participants stated that referencing the colors to explain other aspects of data later in the alt text is overwhelming. For example, P17 shared, "I do remember that orange was English and blue was math and then later the alt texts refer to the color to explain something. So I had to scroll back up and be like, wait, which one was that?" To address the cognitive load of the color mapping, participants wished for more intuitive colors that would help them remember the underlying encoding. P5 suggested, "The positive ones were a light blue and the negative ones were a dark blue, but I would use completely different colors. For the negative ones, I would use a different color like red. That would be helpful to remember." Data points. All participants favored having access to the underlying data of the visualization. As highlighted by the participants, hav-ing access to the data can be beneficial in several ways. It enables participants to interact with the data by seeking out patterns by themselves or finding specific data entries that they might be interested in. P7 shared that "I'm in Michigan so I wanted to see the data, or what color Michigan was. I wouldn't be able to do that [with the given alt texts]." Having access to the data also enhances trust in the authors' claims as, if in doubt, the participants would be able to confirm the claims independently. For example, when asked to comment on an alt text describing the data trends, P14 mentioned, "I don't mind [the data trends], but I would want to see it for myself anyway."</p><p>When given a table, participants overwhelmingly preferred the table formatted by HTML table elements (i.e., th, td, tr tags), since they are easy to navigate using the arrow keys with a screen reader. We observed that all participants who were given the HTML formatted table were able to fluently navigate the table without having any trouble. P20 mentioned, "I think this <ref type="bibr">[HTML formatted]</ref>  Regarding the table contents, participants stated their preference for sorted tables with the sorting criteria explicitly stated, since such tables would be easier to parse. For example, P3 mentioned, "The graph is organized from the biggest gain down to zero, and then down to the biggest loss. I like that the table is organized in the same way. If it was out of order, that would be confusing." P2 added that "The table caption should indicate how the table is sorted." Clearly defined data columns, potentially with the respective units and a brief description of their meaning or how they are computed, were also deemed necessary to fully understanding the data. P3 stated, "I don't really understand some of the numbers. [The achievement gaps] are all numbers less than one. I don't know how those numbers are calculated."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Style Needs for Alt-texts</head><p>Length. The length suggested by the majority of participants ranges from 2 to 8 sentences. For example, P1 shared that "I would say from three to six or sometimes eight sentences. I don't mind waiting for information. If I want to read it, I read it." However, P5 asserted that more than six sentences is too long: "I would say, maybe between four and six sentences, because once you get beyond six sentences, it gets to be a lot, and it can be overwhelming and confusing sometimes." Some participants specifically stated that it depends on the visualization's complexity and how much information is already covered by the article. For example, P8 mentioned, "That depends on the chart and what's being described. Sometimes it's such a busy chart that you're going to have to write pages of analysis on what's in it, and it depends on what information is provided in the article as well." A few participants mentioned they don't mind the length as long as it has all the information they need. As P6 stated, "I always want things to be more specific if it can be. So I don't mind if there's long alt text."</p><p>Language. Most participants prefer plain terms, as exemplified by P2's comment, "From an accessibility standpoint [I prefer] simpler terms", and P13's preference for "more of a simplified type of language so that it's not so technical, and it makes it easier." We learned that participants did not understand some technical terms, such as gradients. As P14 shared, "I didn't exactly know what they mean by the gradient. Maybe, explain it in simpler terms, like the shade." Also, we learned that participants prefer an objective tone. P16 shared that "It almost seems like somebody different than the person writing the article should do the alt text." P17 mentioned that dec-orating the data trend with adjectives sounds less objective: "It only moved by this much or only decreased by that much, even the word 'only', to me feels like a little bit of an opinion."</p><p>Order &amp; navigation of alt text. As stated above, participants prefer the chart type to be stated first, together with summary of the visualization. Then, several participants declared their preference for additional detailed information and, more importantly, followed by data. For example, P14 specifically mentioned that "Ideally [I'd like a] brief description and summarization that covers all the vital points, followed by more data."</p><p>Participants mentioned that this type of structure could also serve different needs of people, as stated by P17, "I really liked the structure of having a brief description and then a detailed description so that if somebody didn't want to look at all that, and they just wanted the basics, they could get that." In a similar vein, P18 wished for a more structured alt text: "If it had, 'this is the description of this axis, and this is the description of that axis and here's the description of the bars,' that might make it a little easier."</p><p>Participants mentioned they wished to have some interactivity. Simple interactivity can be a capability to navigate sentence by sentence. Unlike plain text, some screen readers do not support sentence by sentence reading for alt texts. As P15 stated, "The only thing I hate about the way that alternative text runs is you can't read it sentence by sentence. Depending on what operating system I'm using, I can't. I wish you could just like read it sentence by sentence."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">Summary</head><p>Our findings demonstrate that participants were trying to visualize visualizations in their head while listening to alt texts. We also observed that participants' purposes of understanding visualizations were similar to the purposes that sighted people might have while interacting with visualizations. Therefore, alt texts should be designed to support people with visual impairments to mentally visualize the graphics as well as to complete simple visualization tasks.</p><p>Aligned with the guidelines, participants expressed the importance of chart type, axes, data trend described in alt texts. Our findings highlight the necessity of providing a range of axes to bind their imagination. The guidelines recommend that it is unnecessary to mention the visual attributes of the visualizations, such as the color, which was found to have an opposing response from the participants, as several preferred being told the colors being used. Tables were one of the most important aspects that participants desired, as the guidelines outline. Participants depended on the accessible tables to complete many tasks, such as retrieving the data and inferring the underlying patterns. However, the importance of providing details in the table caption, such as how a measure is calculated and how the table is sorted, was lacking in the guidelines. Contrary to the majority guidelines, most participants preferred more than two sentences of alt text to ensure that all of the necessary information is available in alt texts, even if they may skip. Unlike images often auxiliary to the surrounding contents in online news, visualization carries the complementary information to the presented texts, having participants wish to have access to all the information.</p><p>Given the findings, the current practice we observed from Phase 2 is not sufficient to satisfy participants' information needs. For example, especially in the CHI collection, lacking data points would prevent participants from extracting any information other than the mere fact that there is a visualization. Also, lacking chart type information would prevent people with visual impairments from imagining anything, even if the alt text contains all other information about visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Recommendations for generating alt texts</head><p>Through the study, we observed that visually impaired participants want to use visualizations to accomplish similar goals that sighted people would have. We also observed that participants actively try to construct a mental model of the visualization in their heads while listening to alt texts. To support their goals and the construction of an accurate mental picture through alt texts, we suggest formulating alt texts of visualizations by considering the following:</p><p>• Indicate the start of the alt text by mentioning the chart type. The chart type serves as a "template" that provides people a starting point for constructing a mental picture of the visualization, helping them fill in missing details later. If the type of chart is beyond a commonly used one (e.g., bubble chart), briefly explain how it looks compared to a common type (e.g., bar chart, line chart, scatterplot) if possible.</p><p>• Communicate the scope of the data by explaining the axes and their range. Describing the increments of ticks may not be necessary.</p><p>• Explain the visible data trends as if you would provide a "snapshot" of the visualization. When describing trends, reference the data instead of visual attributes to avoid the unnecessary cognitive effort to remember mappings (e.g., use "the English cluster is located below zero" instead of "the orange cluster is located below zero"). Specify the range of values in which the trend is observed to convey a concrete scene (e.g., use "it shows an increasing trend between Jan 3rd to Feb 3rd" instead of "it shows an increasing trend").</p><p>• To enrich the mental picture of visualizations, optionally provide a brief description of the mapping between the data and visual attributes (e.g., color, shape) if applicable.</p><p>• To accommodate the different amounts of information needs, place a brief description of the visualization first (i.e., chart type with a summary of the topic), then describe it in detail (e.g., axes, range, mapping, trend). Possibly, mention "Details of the visualization are as follows" after the brief description so that those who do not wish to continue listening can skip. The detailed description can be placed in the alt text, the longdesc attribute 2 of an image element, or &lt;desc&gt; tag in an SVG element. Tables laid out in the same order as in the visualization also help people envision the depicted chart with less effort. The table should contain a caption explaining its contents, the sorting criteria, and how the various data fields were calculated. The designer can use the well-crafted labels used in the visualization to name the columns. When possible, prioritize the shortest name that conveys a column's meaning since screen readers repeatedly read the column name when moving between columns. If the table contains too many rows, sample the data and indicate so in the caption. Ideally, stratify the samples based on the critical variables.</p><p>• Use plain language instead of visualization-specific language (e.g., shade vs. gradient) to enhance accessibility. Avoid decorating data (e.g., only 10%, very large gap) to maintain objectivity.</p><p>Since some screen readers do not fully support the desc tag of an svg element (equivalent to the alt attribute of an img element), we recommend designers to consider creating a dummy image element below the visualization to add an alt text in the alt attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Design implication beyond simple alt texts</head><p>Given the apparent limitation of textual descriptions, designers could combine other modalities to complement visualization alt text. We envision a scenario where the alt text describes the visualization components first and sonified data trends are played afterward. As P22 shared, "[First explain the visualization components] then you could play an audio version of the graph. Like, as the daily doses would spike, the sound would go up and down in pitch." Another scenario considers linking the screen reader system with a haptic display to provide tactile feedback when a reader encounters visualizations online. As P5 noted, "Having a way to convert these visualizations into tactile options such as braille along with the audio of the alternative text would help enhance our form of understanding and learning."</p><p>Visual retrieval operations, which sighted people can perform with visualizations, are not fully enabled for people with visual impairments. For example, readers may want to obtain information from their own state while looking at a choropleth representing the hunger rate or may wish to compare the doses administrated between two dates. We observed that people with visual impairments achieved this goal by examining the data table. However, locating the values of interest by navigating a table element takes a long time and imposes a cognitive burden. Dynamically personalizing the alt text or the table contents (e.g., stating data for the readers state, city, or zip code based on their IP address) can partially lower their burden of locating a specific value. Accurate Q&amp;A modules (e.g., <ref type="bibr" target="#b43">[46]</ref>) with natural language query capabilities (e.g., <ref type="bibr" target="#b55">[58]</ref>) would drastically reduce the burden of retrieving information from visualizations by allowing people with visual impairments to pose the question they want to answer.</p><p>While the practice to provide alt text for images has been promoted, the majority of images on the web lack alt texts <ref type="bibr" target="#b70">[73]</ref>. In addition to encouraging designers to provide alt texts, the technologies proposed in the visualization community and other fields can also enhance accessibility by automating the alt text generation process. We envision an alt text generation system that automatically detects the type of chart and the underlying data from a rendered image using the models proposed by Savva et al. <ref type="bibr" target="#b66">[69]</ref> or Poco and Heer <ref type="bibr" target="#b61">[64]</ref> depending on the chart type. For D3 visualizations, the visual components and the data can be extracted by the model proposed by Harper and Agrawala <ref type="bibr" target="#b37">[40]</ref>. Data trends can also be extracted by automated techniques <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b47">50]</ref>. Then, the system can use a natural language generation pipeline (e.g., <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b75">78]</ref>) to formulate alt text from the extracted components, following the proposed guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Limitation &amp; future work</head><p>Due to technical issues of supporting accessible SVG for some screen readers <ref type="bibr" target="#b69">[72]</ref>, we rendered all of the visualizations in our study into images. As a result, all readable elements, such as labels, ticks, and textual annotations, were lost. As a next step, we wish to investigate how we could leverage readable elements in an SVG, in addition to alt text, to enhance the understanding of visualizations for people with visual impairments.</p><p>In this work, we focus on alt texts for several static charts with varying numbers of variables and encodings, specifically in online document reading scenarios. Future work may expand the scope of the investigation to cover more complex types of visualizations (e.g., uncertainty visualization) and interactive visualizations, as well as other contexts like academic settings, etc. Also, we primarily focused on people without remaining vision. Future work should explore strategies for communicating visualizations for people with low vision.</p><p>Finally, the insights gained from our qualitative investigations can also inform quantitative studies to observe how much the recommended alt text composition can help people with visual impairments complete visualization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We analyzed the existing guidelines and current practices for constructing text alternatives for visualizations. We also reported findings from an interview study with 22 people with visual impairments. Our investigation provides insights into how people with visual impairments wish to use visualizations and how they construct an image in their head while listening to alt texts. We identified information needs in visualization alt text to enhance the accessibility of visualizations. Visualizations are a powerful tool to communicate data and their use is pervasive in the media. Thus, ensuring visualization accessibility for people with visual impairments is essential for information equality. We hope that our findings will contribute to the overall accessibility of visualizations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The frequency of each component that appears in the alt texts from the ASSETS and CHI collections.</figDesc><graphic coords="4,44.87,272.93,72.02,62.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The frequency of the top-3 most common combinations of components used in alt texts from the ASSETS and CHI collections.</figDesc><graphic coords="4,137.99,538.37,107.72,73.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>chart describes the number of job changes in 13 sectors in January.A horizontal bar chart depicting the net change in the number of jobs in January. Job categories on the vertical axis from top to bottom sorted by the number: Professional and business services, Government, Information, Wholesale trade, Mining and logging, Financial activities, Utilities, Construction, Manufacturing, Transportation and warehousing, Retail trade, Health care and social assistance, leisure and hospitality. The length of the bar represents the number of job changes ranged from -61000 to 97000. Positive net changes are colored in a darker blue, and negative net changes are colored in a light blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. Date Doses administered by day 7-day average Dec 20 556208, Dec 21 57909, Dec 22 0, Dec 23 393908, Dec 24 0, Dec 25 0, Dec 26 936560 27779, Dec 27 0 19834 […] chart with bars and a line, the horizontal axis depicts dates in increments of 7 from December 20th to February 14th. The vertical axis depicts the number of doses administered in increments of one million. The vertical axis starts at 0 and increments by 1 to 3 million doses. One green bar in the bar graph depicts the number of new doses reported by day. A darker green line on top of the bars depicts the seven-day average number of doses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Study stimuli. Each participant saw all of the visualizations parked with a different style of alt text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Brief summary of each guideline source.</figDesc><table><row><cell>such as charts [4]. Finally, CFPB Design System is an open-source re-</cell></row><row><cell>source for teams at the Consumer Financial Protection Bureau (CFPB)</cell></row><row><cell>that helps teams produce accessible products [79].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Demographics of participants. Pid=Participant ID. G=Gender (M=Male, F=Female). Edu=Education (H.S.=High School, B.S.=Bachelors of Science, B.A.=Bachelors of Arts, M.A.=Masters of Arts, M.S.=Masters of Science, J.D.=Doctor of Jurisprudence, A.A.=Associates).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The first row is a table header. Jobs Net change Professional and business service 97000 Government 43000 Information 16000 Wholesale trade 14300 Mining and logging 9000 Financial activities 8000 Utilities 600 […]</figDesc><table><row><cell></cell><cell></cell><cell>HTML formatted table</cell></row><row><cell></cell><cell></cell><cell>This map</cell></row><row><cell></cell><cell></cell><cell>visualization</cell></row><row><cell></cell><cell></cell><cell>describes the</cell></row><row><cell>V2 [62]</cell><cell>2</cell><cell>percentages of households nationwide reporting</cell></row><row><cell></cell><cell></cell><cell>insufficient food in</cell></row><row><cell>Choropleth, Poverty</cell><cell></cell><cell>the past seven days</cell></row><row><cell></cell><cell></cell><cell>by state.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The first row is a table header. State Value WA 8.1 OR 8.5 CA 12.7 MT 12.7 ID 10.8 NV 14.1 UT 8.3 AZ 10.9 WY 11.2 CO 10.6 NM 17.4 ND 8.8 SD 8.3 NE 10.8 […]</figDesc><table><row><cell></cell><cell></cell><cell>HTML formatted table</cell></row><row><cell></cell><cell></cell><cell>This bubble chart</cell></row><row><cell></cell><cell></cell><cell>describes the math</cell></row><row><cell></cell><cell></cell><cell>and English score</cell></row><row><cell>V4 [73]</cell><cell>4</cell><cell>gap between boys and girls by parents'</cell></row><row><cell>Bubble chart</cell><cell></cell><cell>income in about</cell></row><row><cell>Gender difference in</cell><cell></cell><cell>1,800 large school</cell></row><row><cell>academic</cell><cell></cell><cell>districts.</cell></row><row><cell>performance</cell><cell></cell><cell></cell></row></table><note>In this bubble chart, the horizontal axis depicts a range from poorer parents from the left to richer parents on the right. The vertical axis depicts a range of grade levels in increments of 0.5 grade levels. A line in the middle of the graph separates the top portion depicting that girls test better and the bottom half depicting that boys did better. English tests are depicted in orange circles and math tests are depicted in blue circles. Each circle represents each school district, and there are about 1,800 circles in the chart. The size of the circles represents the number of students in the school district.The chart shows that on English tests, girls test better than boys regardless of their parents' socioeconomic status. In comparison, on math tests, boys from richer districts tend to test better than girls from richer districts.The first row is a table header. School district average number of student, math gap, ela gap, income New York City Public Schools 70964 0.015 -0.69 41887.676 Los Angeles Unified 49211 0.002 -0.676 40928.449 […] HTML formatted table</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>table was really effective. I think the table in some ways was more effective than the other things [textual information]." P6 echoed the importance of the formatted table: "Having it separate from the alternative text is important because I could more easily look at it with my screen reader." Participants who were given non-formatted tables inside the alt text stated their frustration for not being able to navigate them. P14 shared, "The problem is that I cannot use my table navigation commands because this is not an actual table element." Another disadvantage of having a non-formatted table is that participants are not able to keep track of where they are at. In the case of a formatted table, when moving from a different column, screen readers will read the column name, then the value. P14 noted, "I need a table, like a real table, because otherwise I have to constantly remember which column I'm at."</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>• Place the data table as a hidden HTML element next to the visualization, and mention at the end of the alt text how to access the data table (e.g., "The data for this chart is available in the table below"). Alternatively, place the data table on a separate page, and add a link to it in the longdesc. Often, visualization designers sort data by a meaningful variable to make the pattern in the visualization more salient. The same principle should apply to the table design. Sorted tables help people recognize the pattern while navigating the table.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Conventionally, the community concerning alternative texts for images refers the alt texts more generally as image descriptions or image captions</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although longdesc is deprecated, most screen readers can read it. If a new standard for long descriptions is introduced, we recommend people to use it.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://accessibility.psu.edu/images/charts/" />
	</analytic>
	<monogr>
		<title level="j">Charts &amp; accessibility</title>
		<imprint>
			<date type="published" when="2018-06">Jun 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Web content accessibility guidelines (wcag) 2.1</title>
		<imprint>
			<date type="published" when="2018-06">Jun 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Using longdesc</title>
		<idno>W. 2.0</idno>
		<ptr target="https://www.w3.org/TR/WCAG20-TECHS/H45.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Access</surname></persName>
		</author>
		<author>
			<persName><surname>Nvda</surname></persName>
		</author>
		<ptr target="https://www.nvaccess.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Add alternate text and supplementary information to tags</title>
		<author>
			<persName><forename type="first">Adobe</forename></persName>
		</author>
		<ptr target="https://www.adobe.com/accessibility/products/acrobat/pdf-repair-add-alternative-text.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How many participants are really enough for usability studies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alroobaea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Mayhew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 Science and Information Conference</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Apple</surname></persName>
		</author>
		<ptr target="https://support.apple.com/guide/voiceover/welcome/mac" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How teens with visual impairments take, edit, and share photos on social media</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI conference on human factors in computing systems</title>
				<meeting>the 2018 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic description generation from images: A survey of models, datasets, and evaluation measures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muscat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="409" to="442" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Html 2.0 materials</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berners-Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Connolly</surname></persName>
		</author>
		<ptr target="https://www.w3.org/MarkUp/html-spec/" />
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">VizWiz: Nearly Real-time Answers to Visual Questions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tatarowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using thematic analysis in psychology</title>
		<author>
			<persName><forename type="first">V</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Qualitative research in psychology</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="77" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling a graph viewer&apos;s effort in recognizing messages conveyed by grouped bar charts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on User Modeling, Adaptation, and Personalization</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="114" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Readings in information visualization: using vision to think</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02850</idno>
		<title level="m">Figure captioning with reasoning and sequence-level training</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Digitizer auditory graph: making graphs accessible to the visually impaired</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;10 Extended Abstracts</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3445" to="3450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Here&apos;s where the jobs are -in one chart</title>
		<ptr target="https://www.cnbc.com/2021/01/08/where-the-jobs-are-december-2020-chart.html" />
		<imprint/>
		<respStmt>
			<orgName>CNBC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generation of texts for information graphics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Corio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Datasite: Proactive visual data exploration with computation of insight-based recommendations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Yalc ¸in</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="267" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">When does an infographic say more than a thousand words? audience evaluations of news visualizations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kruikemeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lecheler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Studies</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1293" to="1312" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive sight: textual access to simple bar charts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Review of Hypermedia and Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="279" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A browser extension for providing visually impaired users access to the content of bar charts on the web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WEBIST (2)</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SVGPlott: An Accessible Tool to Generate Highly Adaptable, Accessible Audio-tactile Charts for and from Blind and Visually Impaired People</title>
		<author>
			<persName><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments</title>
				<meeting>the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analysis of tactile chart design</title>
		<author>
			<persName><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Proceeding Series, Part F1285</title>
				<imprint>
			<date type="published" when="2017">197-200, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of tactile chart design</title>
		<author>
			<persName><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments</title>
				<meeting>the 10th International Conference on PErvasive Technologies Related to Assistive Environments</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="197" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improve the accessibility of tactile charts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP Conference on Human-Computer Interaction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="187" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A user study to evaluate tactile charts with blind and visually impaired people</title>
		<author>
			<persName><forename type="first">C</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers Helping People with Special Needs</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PostGraphe: A System for the Generation of Statistical Graphics and Text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fasciano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Natural Language Generation Workshop</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>For</surname></persName>
		</author>
		<author>
			<persName><surname>Blind</surname></persName>
		</author>
		<ptr target="https://www.afb.org/blindness-and-low-vision/using-technology/assistive-technology-products/screen-readers" />
		<imprint/>
	</monogr>
	<note>Screen readers</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The tactile graphics helper: providing audio clarification for tactile graphics using machine vision</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fusco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Morash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International ACM SIGACCESS Conference on Computers &amp; Accessibility</title>
				<meeting>the 17th International ACM SIGACCESS Conference on Computers &amp; Accessibility</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scientific diagrams made easy with iveo tm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bulatov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers for Handicapped Persons</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1243" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">It&apos;s almost like they&apos;re trying to hide it&quot;: How User-Provided Image Descriptions Have Failed to Make Twitter Accessible</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference on -WWW &apos;19</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gravvitas: generic multi-touch presentation of accessible graphics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP Conference on Human-Computer Interaction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="30" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Implementing disability accommodations in a widely distributed web based visualization and analysis platform-weave</title>
		<author>
			<persName><forename type="first">H</forename><surname>Granz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tuccar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purushe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Universal Access in Human-Computer Interaction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robographics: Dynamic tactile graphics powered by mobile robots</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muehlbradt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 21st International ACM SIGACCESS Conference on Computers and Accessibility</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The comprehension of stem graphics via a multisensory tablet electronic device by students with visual impairments</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gorlewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Impairment &amp; Blindness</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="404" to="418" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deconstructing and restyling d3 visualizations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual ACM symposium on User interface software and technology</title>
				<meeting>the 27th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Get started on android with talkback</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Help</surname></persName>
		</author>
		<ptr target="https://support.google.com/accessibility/android/answer/6283677?hl=en" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Interactive 3D sonification for the exploration of city maps</title>
		<author>
			<persName><forename type="first">W</forename><surname>Heuten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-01">Jan. 2006</date>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page">164</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring new paradigms for accessible 3d printed graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International ACM SIGACCESS Conference on Computers &amp; Accessibility</title>
				<meeting>the 17th International ACM SIGACCESS Conference on Computers &amp; Accessibility</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="365" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dvqa: Understanding data visualizations via question answering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kad ´ar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´a</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07300</idno>
		<title level="m">Figureqa: An annotated figure dataset for visual reasoning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Answering questions about charts and generating visual explanations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Accessible visualization: Design space, opportunities, and challenges</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riegelhuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="173" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Trust and recall of information across varying degrees of title-visualization misalignment</title>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Development of a talking tactile tablet</title>
		<author>
			<persName><forename type="first">S</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gourgey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Technology and Disabilities</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Characterizing automated data insights</title>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13060</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Development and evaluation of two prototypes for providing weather map data to blind users through sonification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sizemore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Usability Studies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="93" to="110" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Add alternative text to a shape, picture, chart, smartart graphic, or other object</title>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://support.microsoft.com/en-us/topic/add-alternative-text-to-a-shape-picture-chart-smartart-graphic-or-other-object-44989b2a-903c-4d9a-b742-6a75b451c669" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Complete guide to narrator</title>
		<author>
			<persName><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://support.microsoft.com/en-us/windows/complete-guide-to-narrator-e4397a0d-ef4f-b386-d8ae-c172f109bdb1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Generating Explanatory Captions for Information Graphics</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mattis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evaluating the accessibility of line graphs through textual summaries for visually impaired users</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carberry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASSETS14 -Proceedings of the 16th International ACM SIGAC-CESS Conference on Computers and Accessibility</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rich representations of visual content for screen reader users</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cutrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI conference on human factors in computing systems</title>
				<meeting>the 2018 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">with most of it being pictures now, i rarely use it&quot; understanding twitter&apos;s evolving accessibility to blind users</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zolyomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bahram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2016 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5506" to="5516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narechania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stasko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="379" />
			<date type="published" when="2021-02">Feb. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoque</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09142</idno>
		<idno>arXiv: 2010.09142</idno>
		<imprint>
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Global data on visual impairment</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Organization</surname></persName>
		</author>
		<ptr target="https://www.who.int/blindness/publications/globaldata/en/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Review of designs for haptic data visualization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paneels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Haptics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="137" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Information olfactation: Harnessing scent to convey data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elmqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="726" to="736" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Describing images on the web: a survey of current practice and prospects for the future</title>
		<author>
			<persName><forename type="first">H</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Computer Interaction International (HCII)</title>
				<meeting>Human Computer Interaction International (HCII)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reverse-engineering visualizations: Recovering visual encodings from chart images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="353" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A growing number of americans are going hungry</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Post</surname></persName>
		</author>
		<ptr target="https://www.washingtonpost.com/graphics/2020/business/hunger-coronavirus-economy/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Comparing user performance on parallel-tone, parallel-speech, serial-tone and serial-speech auditory graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sakhardande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jadhav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP Conference on Human-Computer Interaction</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="247" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Toward Scalable Social Alt Text: Conversational Crowdsourcing as a Tool for Refining Vision-to-Language Technology for the Blind</title>
		<author>
			<persName><forename type="first">E</forename><surname>Salisbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Evaluating and Complementing Vision-to-Language Technology for People who are Blind with Conversational Crowdsourcing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Salisbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="5349" to="5353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Re-Vision: Automated classification, analysis and redesign of chart images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chhajta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology -UIST &apos;11</title>
				<meeting>the 24th Annual ACM Symposium on User Interface Software and Technology -UIST &apos;11<address><addrLine>Santa Barbara, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">393</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Scientific</surname></persName>
		</author>
		<ptr target="https://www.freedomscientific.com/products/software/jaws/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">evographs-a jquery plugin to create web accessible graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Forouraghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE Annual Consumer Communications &amp; Networking Conference (CCNC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Why does a link not take the title of a contained svg as it&apos;s accessible name?</title>
		<author>
			<persName><surname>Stackoverflow</surname></persName>
		</author>
		<ptr target="https://stackoverflow.com/questions/63208144/why-does-a-link-not-take-the-title-of-a-contained-svg-as-its-accessible-nam" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">person, shoes, tree. is the person naked?&quot; what people with vision impairments want in image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Audiofunctions: Eyes-free exploration of mathematical functions on tablets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taibbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bernareggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gerino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ahmetovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mascetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers for Handicapped Persons</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">See how the vaccine rollout is going in your county and state</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N Y</forename><surname>Times</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/interactive/2020/us/covid-19-vaccine-doses.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Where boys outperform girls in math: Rich, white and suburban districts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Upshot</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/interactive/2018/06/13/upshot/boys-girls-math-reading-tests.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Cfpb design system</title>
		<ptr target="https://cfpb.github.io/design-system/guidelines/data-visualization-guidelines" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">How blind people interact with visual content on social networking services</title>
		<author>
			<persName><forename type="first">V</forename><surname>Voykinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azenkot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Leshed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th acm conference on computer-supported cooperative work &amp; social computing</title>
				<meeting>the 19th acm conference on computer-supported cooperative work &amp; social computing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1584" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Universal design of auditory graphs: A comparison of sonification mappings for visually impaired and sighted listeners</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Mauney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Feeling what you hear</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brewster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">1123</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Effectiveness of tactile scatter plots: comparison of non-visual data representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mizukami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers Helping People with Special Needs</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Using aria to enhance svg accessibility</title>
		<author>
			<persName><forename type="first">L</forename><surname>Watson</surname></persName>
		</author>
		<ptr target="https://www.tpgi.com/using-aria-enhance-svg-accessibility/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Alternative text</title>
		<author>
			<persName><surname>Webaim</surname></persName>
		</author>
		<ptr target="https://webaim.org/techniques/alttext/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Recognizing the intended message of line graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Theory and Application of Diagrams</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="220" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge from External Sources</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="4622" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Automatic Alt-text: Computer-generated Image Descriptions for Blind Users on a Social Network Service</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wieland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Farivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</title>
				<meeting>the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing<address><addrLine>Portland Oregon USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-02">Feb. 2017</date>
			<biblScope unit="page" from="1180" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Tactile presentation of network data: Text, matrix or diagram</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Holloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;20</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Data Sonification for Users with Visual Impairment: A Case Study with Georeferenced Data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lazar</surname></persName>
		</author>
		<idno>4:1- 4:28</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The effect of computergenerated descriptions on photo-sharing experiences of people with visual impairments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azenkot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2017">2017</date>
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
