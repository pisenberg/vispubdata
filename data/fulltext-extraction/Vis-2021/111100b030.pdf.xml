<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geo-Context Aware Study of Vision-Based Autonomous Driving Models and Spatial Video Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Suphanut</forename><surname>Jamonnak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinyi</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Md</forename><surname>Amiruzzaman</surname></persName>
						</author>
						<title level="a" type="main">Geo-Context Aware Study of Vision-Based Autonomous Driving Models and Spatial Video Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">390A71FDCEFF90394E2AFF72437469BC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visualization System</term>
					<term>Spatial Video</term>
					<term>Autonomous Driving</term>
					<term>Vision-based Deep Learning Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. Visual interface for geo-context aware analysis of prediction data from autonomous driving models. (A) Interactive map view for deep learning model predictions and video data; (B) Autonomous driving model performance charts; (C) Trip filters based on model performance metrics or spatial conditions; (D) Trip distribution charts of selected video trip data; (E) Model prediction view for comparing multiple model predictions; (F) Street view thumbnails showing training video details at critical locations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning (DL) in computer vision has achieved success in learning autonomous driving models (ADM) from large-scale crowd-sourced video datasets. The vision-based models are trained to predict instantaneous driving behaviors from spatial video data captured by on-vehicle • S. Jamonnak (Email: sjamonna@kent.edu), Y. <ref type="bibr">Zhao</ref>  cameras while moving vehicles traverse road networks in different types of built environments. The massive video datasets present diverse visual appearances, dynamic traffic situations, and meanwhile register realistic drivers' behaviors along the trajectories. The datasets are utilized for the development of DL models in autonomous driving. Domain researchers and practitioners of autonomous cars are continuously accumulating video data over multiple regions and cities. Many datasets and models are publicly shared to promote research progress. These data analytics topics are of interest and importance to users including DL model designers and autonomous driving practitioners. Due to the large volume, complexity, and heterogeneity of the data, visual exploration techniques are demanded which can leverage the emerging DL research products in autonomous driving within geograph-ical context. In this paper, we present a geo-context aware visualization system for the study of predictions made by vision-based ADMs together with large-scale video data. The scope of work is mainly for model performance comparison and analysis, and the target users are researchers of the related application fields.</p><p>The system is built up based on an open repository of ADM videos including real driver actions and predictions from three different DL neural networks. A large number of video trips are processed and matched to geographical locations in a big city by their trajectory footprint locations. In a spatial database, the heterogeneous data of videos, images, DL model predictions are integrated with important attributes including streets, regions, weather, and time periods, and realtime queries are well supported using different conditions. Therefore, users are enabled to overview, search, and explore the performance of ADMs with geographical filters, and meanwhile, geographical locations with specified ADM performances can also be easily studied. A set of visualizations are designed for interactive study in both coarse and fine levels of geographical units. Video contents of traffic scenes are also visualized for model prediction analysis. Moreover, differences and similarities of three DL networks are classified and compared at locations, so that users can investigate specific model performances with geographic cues.</p><p>In summary, the main contributions of this paper include: • We develop visual analytics (VA) techniques and a prototype that support domain users to study and compare multiple ADMs for their prediction performance in both city and street levels. • We present visual analysis functions that facilitate bidirectional data analysis, either from spatial conditions to ADM prediction performance or from model accuracy/perplexity to spatial attributes. • We seamlessly integrate large-scale spatial video data and ADM prediction data within efficient data management and interactive geo-visualization interface. A few case studies were conducted with the new VA system in a big city area. Its utility and effectiveness were evaluated by domain experts. They agreed that the system fills the gap between the emerging largescale ADM data and the analytical capability, which can be well utilized in their fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vision Based Autonomous Driving DL Model</head><p>Autonomous driving is an important developing technology that is useful for improving traffic, reducing emission, and transforming driving culture <ref type="bibr" target="#b60">[60]</ref>. Vision-based autonomous driving technology achieves fast and huge progress with DL models. Deep reinforcement learning <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43]</ref> have been applied for motion control. The earliest ALVINN <ref type="bibr" target="#b40">[40]</ref> uses a 3-layer shallow network with simulated road images for action prediction. Later, a CNN is trained with road images and steering angles like Nvidia PilotNet <ref type="bibr" target="#b10">[11]</ref>. More recent works for motion control are based on both CNN and RNN, where RNN is used for handling temporal sequences in video streams. FCN-LSTM <ref type="bibr" target="#b58">[58]</ref> uses a fully-convolutional network together with an LSTM to learn the visual and temporal information and make the decision on the action of motion. Based on a CNN-LSTM architecture, Drive360 <ref type="bibr" target="#b22">[22]</ref> further comprises a fully connected network to integrate information from multiple sensors for the prediction of the driving maneuvers. The performance of deep learning models is measured quantitatively for statistical comparison. But there is a lack of interactive visual analytics tools to analyze the model performance with geographical attributes, which is the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Analytics of DL Models</head><p>Interactive visualization tools have been used for an in-depth understanding of how deep learning models work <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b41">41]</ref>. Many VA tools (e.g., <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b55">55]</ref>) allow users to interact with the activation maps and network structure, and the prediction/classification results. These tools have the potential but are not yet applied to DL networks of ADM, where for instance, LSTM and CNN model visualizations may be integrated with street-view perception data.</p><p>For vision-based DL models, computational approaches of deep learning explanation have been addressed through a variety of algorithms <ref type="bibr" target="#b44">[44]</ref>. A general taxonomy classified them into three main categories <ref type="bibr" target="#b20">[20]</ref>: input modification methods, deconvolutional methods, and input reconstruction methods. Deconvolutional Networks (De-convNets) <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b61">61]</ref>, Guided Back Propagation <ref type="bibr" target="#b49">[49]</ref>, Class Activation Mapping (CAM) <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b63">63]</ref>, were the popular approaches. Recently, LRP has become an emerging focus from computer vision researchers (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b51">51]</ref>). Heatmaps were mostly used in these methods to visualize input pixels' relevance values to prediction results. VisLR-PDesigner <ref type="bibr" target="#b24">[24]</ref> provides a comprehensive visualization tool for LRP design and exploration. These methods and tools can be applied in the future to street-view-based ADMs to visually explain what perception features affect the driving action decisions.</p><p>Few VA systems are designed for autonomous driving models. Visu-alBackProp <ref type="bibr" target="#b9">[10]</ref> highlights network elements in Nvidia PilotNet <ref type="bibr" target="#b10">[11]</ref> that affect steering decision. VATLD <ref type="bibr" target="#b19">[19]</ref> focuses on the understanding of the accuracy and robustness of traffic light detectors by disentangled representation learning and semantic adversarial learning. It allows users to obtain valuable insights to improve the CNN model performance. These tools do not study the prediction results of autonomous driving models and the spatial video data in the context of city-wide geography. Our system presents geographical visualizations together with model prediction metrics in a VA system, which enables users to conduct interactive studies among spatial, video, and model prediction data elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Geospatial and Urban Data Visualization</head><p>A variety of VA methods and tools have been developed to visually make sense of geo-spatial data <ref type="bibr" target="#b2">[3]</ref>. They are developed for visualizing origin-destination movement data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b65">65]</ref>, vehicle trajectory data <ref type="bibr" target="#b0">[1]</ref>, space-time data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b36">36]</ref>, flow maps <ref type="bibr" target="#b39">[39]</ref>, Flowstrates <ref type="bibr" target="#b12">[13]</ref>, OD maps <ref type="bibr" target="#b56">[56]</ref>, and visual queries <ref type="bibr" target="#b17">[17]</ref>. Vehicle trajectories are visually studied with various visual metaphors and interactions, such as GeoTime <ref type="bibr" target="#b29">[29]</ref>, SemanticTraj <ref type="bibr" target="#b0">[1]</ref>, TripVista <ref type="bibr" target="#b21">[21]</ref>, FromDaDy <ref type="bibr" target="#b26">[26]</ref>, TrajGraph <ref type="bibr" target="#b25">[25]</ref>, and more <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b57">57]</ref>. They have been widely used for urban and transportation data analytics, which however, have not been extended to leveraging the autonomous driving data.</p><p>On the other hand, street-view images have been used in landscaping, urban planning, transportation, and social studies. <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b64">64]</ref>. Recent development of DL technologies (e.g., Segnet <ref type="bibr" target="#b6">[7]</ref> and PSPnet <ref type="bibr" target="#b62">[62]</ref>) make this process less expensive and faster, which can find objects and extract semantic categories from street-view images and videos. A VA system <ref type="bibr" target="#b27">[27]</ref>, GeoVisuals, is developed to interactively manage, visualize, and analyze spatial video and geo-narratives using a set of visualization widgets and interaction functions. Moreover, VA systems for model diagnosis based on multi-modal sensors (camera, lidar, radar) have been developed and adopted in the AD industry (e.g. https://avs.auto/demo/).</p><p>In this paper, we present a new VA system for the efficient study of vision-based ADMs inside a geospatial visualization platform. Our system integrates model prediction visualization with city-wide geographic environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VISION BASED AUTONOMOUS DRIVING MODELS</head><p>Automated driving in urban scenes presents big interest and challenge in researchers and practitioners, leading to the advent of deep learning models together with the development of massive autonomous driving datasets. A survey reviewed the studies regarding present challenges, system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human-machine interfaces <ref type="bibr" target="#b60">[60]</ref>. Vision-based DL models trained by spatial videos of traffic scenes have achieved impressive advances, which are studied in this paper. Next, we introduce the basics of autonomous driving models (ADMs) and video datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discrete Action Driving Model</head><p>Deep learning models used for autonomous driving action prediction usually take road view images from video cameras to predict possible </p><formula xml:id="formula_0">G(i, a) : IMG × ACT → R<label>(1)</label></formula><p>where i ∈ IMG represents a set of images, and a ∈ ACT represents a potential motion action. This function generates the probability of the action in real number. A typical set ACT includes four discrete motion actions as:</p><formula xml:id="formula_1">ACT = {go straight, slow or stop, le f t turn, right turn} . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The set can also be extended to more actions (e.g., using slightly left turn, sharp left turn, and so on). These actions are usually qualified from the angular velocity and other car driving attributes. If the continuous velocity is used, it will lead to a continuous action driving model usually used for lane following problems. In this paper, we build the system based on the typical discrete action driving model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training with Spatial Videos</head><p>In order to design DL models for motion action prediction, largescale spatial video datasets are needed for training and evaluation. Usually, crowd-sourced video datasets are collected from dashcam video cameras mounted on vehicles. The vehicles travel across roads and videotape various road conditions, while their GPS trajectories are stored. Meanwhile, real drivers' actions are recorded with sensors.</p><p>It is demanded that these videos are generated in a large variety of geographical locations and settings, weathers, and time periods in a day, so that the DL models are well trained to face various traffic situations. Therefore, the DL model performance should also be studied based on these geographical attributes, which is one of our system goals.</p><p>There exist a group of open datasets, such as KITTI <ref type="bibr" target="#b18">[18]</ref>, Cityscape <ref type="bibr" target="#b16">[16]</ref>, Comma.ai <ref type="bibr" target="#b45">[45]</ref>, Oxford <ref type="bibr" target="#b35">[35]</ref>, synthetic Princeton Torcs <ref type="bibr" target="#b15">[15]</ref> and GTA <ref type="bibr" target="#b42">[42]</ref>, which are publicly shared by ADM designers and researchers <ref type="bibr" target="#b58">[58]</ref>. In this paper, we utilize one of the largest datasets, the Berkeley DeepDrive Video (BDDV) dataset <ref type="bibr" target="#b59">[59]</ref>.</p><p>The BDDV dataset includes more than 10k hours of road videos in city, rural, and highway environments in multiple cities, multiple weather conditions (sunny, rain, snow, etc.), and both day and night times. It also comes with real driver actions and GPS trajectory data which can be mapped to locations at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DL Neural Networks</head><p>A set of DL neural network architectures have been developed to make action predictions for the function G. Fig. <ref type="figure" target="#fig_0">2</ref> illustrates two networks. The input of image frame at current time t is processed with either CNN (Convolutional Neural Network) or FCN (Fully Convolutional Network), and the output is combined in LSTM (Long short-term memory) network with previous states including the previous frames and actions in a few seconds prior to t. Then, the probabilities of motion actions in ACT at t is generated for prediction. The two models are named as CNN-LSTM and FCN-LSTM. In addition, a temporal CNN (TCNN) architecture can also be applied by adding an additional temporal convolutional module with a fixed time window <ref type="bibr" target="#b58">[58]</ref>. TCNN1 uses only a single image frame in the temporal window, so the two LSTM-fused models should have a better performance compared with TCNN1. We follow the methods in <ref type="bibr" target="#b58">[58]</ref> with CNN-LSTM, FCN-LSTM, and TCNN1 to show the visual study of multiple ADMs while other models can be further included. In particular, the three models are trained by video data from BDDV in New York City area in our prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Performance Metrics</head><p>ADM prediction performance is usually measured by accuracy and perplexity. The accuracy is defined as</p><formula xml:id="formula_3">accuracy = N c N , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where N is the total number of predictions, and N c is the number of correct predictions when argmax a G(i, a) = a real . Here a real is the labeled action from the real driver. The number N is usually counted for a whole test dataset to show the global accuracy of a model. In this paper, we realize that the accuracy can also be defined on a geographical region, so that the performance can be measured and studied within the geo-environment. Model perplexity at time t is defined over a sequence of n predictions along the same driving path prior to t as <ref type="bibr" target="#b58">[58]</ref>:</p><formula xml:id="formula_5">perplexity = e − 1 n ∑ n k=1 log G(i k ,a k ).<label>(4)</label></formula><p>Here, i k and a k are the k-th image frame and predicted action in the sequence. The perplexity is defined at each prediction and a low perplexity indicates more confidence in the prediction. An averaged perplexity of an ADM is often computed overall predictions in the whole test dataset. In this paper, a perplexity value is linked to the location where the prediction is made. Therefore, the average of perplexities at a geographical region or street can be computed interactively to indicate model performance with respect to geographical objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GEO-CONTEXT AWARE DATA PROCESSING 4.1 Video Trips and Data Extraction</head><p>We download raw spatial video datasets from the public BDDV repository. In our prototype, we use the videos covering New York City (NYC) and its suburban area. The data is stored in the format of clips in the length of 40 seconds, with a frame rate at 30 fps and a high resolution at 720p. We call each clip as a video trip. Each video trip is associated with TripID, speed, GPS locations and timestamps, altitude, and other vehicle information. A total of 12,000 video trips with 130hours of driving experience are used in our system. The total number of predictions is about 1.44 million. The video files are approximately 250GB in size.</p><p>In addition to locations on the trace, each video trip contains spatial attributes including: (1) distinct times of day (day, night, dawn/dusk), (2) different street types (city streets, highways, residential, tunnel, gas station, parking lots), and (3) different weather types (clear, overcast, rainy, snowy, cloudy, foggy). The dataset contains approximately equal amounts of video trips in day-time and night-time. It also includes a large portion of weather conditions such as rain and snow. It is of great interest to study the ADMs with respect to these spatial attributes. For convenience in this paper, we call these attributes as spatial conditions, with an extended definition of "spatial" information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Processing and Transformation</head><p>The extracted video trip data is processed in several steps: (1) generating TFRecords data; (2) making ADM predictions; and (3) computing ADM performance values. Generate TFRecords: To make efficient visual analysis, we transform raw data into TFRecords optimized for Tensorflow computing. The binary data format uses less disk space and less time to process. It is also essential to combine multiple data types for DL networks. For videos, ffmpeg library <ref type="bibr" target="#b52">[52]</ref> is used to extract frame images from video. Three images are extracted per 1 second. They are stored together with speed and timestamps in TFRecords. ADM prediction: CNN-LSTM, FCN-LSTM, and TCNN1 networks are used to generate predictions of driver actions at each frame of these video trips, compatible with the recorded actual driver's action. The pre-trained DL networks <ref type="bibr" target="#b58">[58]</ref> are employed. For each frame, the neural network output includes a vector of prediction probability values for each action in {1: go straight, 2: slow or stop, 3: turn left, 4: turn right}. The action with the largest value is considered as the predicted action. For example for a frame image with the prediction values {0.14, 0.35, 0.82, 0.46}. The most probable action is "turn left". This action links to an accurate geo-location since the frame image has a recorded GPS location.</p><p>Computing model performance values: The accuracy and perplexity values (see Sec. 3.4) are finally computed for each frame image and link to the corresponding location as well. Here for a single image, the accuracy is either 1 or 0. For perplexity, it is computed from the previous seven predictions, that is, from seven previous image frames. These measures can be further aggregated to compute accuracy and perplexity for spatial units such as a trip and a spatial region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Map Matching and Spatial Database</head><p>After data extraction and ADM computation, a set of locations along each video trip are linked to the predictions, performance values, actual actions, car speed, and spatial attributes. To enable geo-context aware visualization, these data items need to be matched with geographical objects such as streets, regions, and cities. Fast data query is needed to support interactive exploration. Therefore, we perform mapmatching and then store and manage data in a specifically designed spatial database. Map matching: We first download road network geometry in NYC area from an open GIS data repository, OpenStreetMap (OSM) <ref type="bibr" target="#b8">[9]</ref>. We further retrieve zip code regions containing geometric boundaries. Each video trip is processed while each location with prediction results is mapped to a street segment and a zipcode region it resides in. In the process, some trips may go across different zipcode regions. Their GPS trajectories are cut into these regions respectively. Data management: A spatial database is devised to support data queries for interactive visualization. A NoSQL database is used specifically for the ADM and video data, instead of using a traditional relational database. The reason of choice is that the NoSQL database provides easy programming and efficient indexing for unstructured data including videos, images, and locations as a "document". It can also easily perform spatially based read or write operations on such a single data entity. A document encapsulates the location, timestamp, predictions, accuracy and perplexity, and various spatial attributes. It also includes links to the corresponding video clips and image frames, as well as the street segment and zipcode region it belongs to. This data structure can further incorporate other spatial and ADM information if needed. A video trip is further stored as a sequence of such locations. Moreover, the database also stores geo-structures of street networks and region geometries. In implementation, MongoDB is used for its popularity and easy to operate JSON files in data transfer.</p><p>The ADM prediction measures are further aggregated by averaging them over trips, streets, and zipcode regions. Furthermore, to support semantic query, we project the averaged accuracy and perplexity to multiple categories (bins). For example, a region with 64% accuracy is categorized into the "60-70" percentage category. These categories facilitate easier visual exploration than raw numerical values. They can be used as ADM performance conditions for users to query data with different prediction performance ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Query and Indexing</head><p>Table <ref type="table" target="#tab_1">1</ref> summarizes the queries used in our system based on spatial conditions and ADM performance conditions. To enable fast data retrieval, spatial indexing is constructed in the database (MongoDB in default uses a B-tree subdivision of the space). Then geohash strings  (e.g. $geoWithin, $geoIntersects) are used to quantify the locations to a cell in the tree. This scheme can quickly retrieve locations inside any queried region or streets by different conditions. Furthermore, we create Boolean operational indexes for both accuracy and perplexity categories, and for spatial attributes. A variety of attributes thus can be combined in a data query using "AND" and "OR" operations. For example, the database can immediately respond to a query of video trips and locations where the ADM predictions happen at "day OR night" with a "snow" weather and the accuracy value ranges between "30-40" percentage. Such operations facilitate flexible data analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">System Implementations</head><p>The datasets are processed on computers with Intel i7-8700K CPU and 16GB memory. The predictions are conducted with TensorFlow, with GPU acceleration on either Nvidia GTX 1070 GPU with 8GB texture memory or Nvidia K80/T4 GPU with 12GB memory. The data processing time of each video trip including predictions of three ADMs cost on average about 5 minutes.</p><p>The visualization interface is implemented with Native JavaScript framework as client-side, bundling with several JavaScript libraries including Node.js, Mapbox-GL, and D3.js <ref type="bibr" target="#b11">[12]</ref> libraries. The serverside script is implemented using Express.js and Mongoose.js JavaScript library in order to perform different query requests from MongoDB database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESIGN OF THE VISUALIZATION SYSTEM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">VA Task Characterization</head><p>ADM researchers currently use statistical metrics such as accuracy and perplexity over benchmark datasets for model evaluation. They also compare predicted driver actions with actual driver actions by finding specific image frames in training datasets. However, their study has not been well supported by visual exploration, which can provide in-depth and interactive investigation connecting multiple models with locations and video contents. Therefore, our visual analytics system is designed for the following tasks: • T1. Visually exploring model performance metrics: Manage multiple models and a large set of training videos so that their performance metrics can be easily identified and understood. The exploration can be done with both large-scale analysis of global performance and fine-scale analysis of driver prediction behaviors. • T2. Comparing different models: Show and examine prediction results of various ADM implementations, so that users can easily compare these models and analyze their relations. • T3. Analyzing models characteristics with visual contents: Together with model performance study, distill and display related Users thus are able to link model behaviors to geo-spatial and/or environmental factors. We realized that there exist more tasks in promoting wider use of ADM and its datasets, such as linking behaviors of neural network components with geospatial and street-view disparities, which can be further addressed in future work by VA community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visualization Design</head><p>With respect to the tasks, we design a visualization system which is illustrated in Fig. <ref type="figure">1</ref>. The interface includes a set of coordinated views supporting interactive visual study: • Interactive Map View (Fig. <ref type="figure">1A</ref>): Geo-environment is visualized together with model performance and video data. Users can overview the whole city and can also select an arbitrarily sized region of interest. Here, three types of information visualizations are layered on the map: 1. Region heatmap: Zipcode regions are color-coded by one of the three ADM attributes including the density of locations where ADM predictions are made, ADM accuracy, and ADM perplexity.</p><p>As shown in the legend on the top left corner of Fig. <ref type="figure">1A</ref>, users can also select individual models for visualizing these attributes. 2. Video trips: Video trips are visualized in orange as trajectories.</p><p>A kernel density estimation (KDE) algorithm is applied since many trips overlap in locations. The visualization shows their distribution in geo-space. 3. Critical locations: ADM locations are shown as blue points on the map, while users can select to show only those locations with specified driving actions, as shown in the legend on the top right corner of Fig. <ref type="figure">1A</ref>. Users can make these layers visible or invisible on the map view. As shown in Fig. <ref type="figure">1A</ref>, the region heatmap shows the average perplexity of all ADMs at zipcode regions in the NYC area. The available video trips in the area are also visible. The visualization shows that the benchmark video dataset has traversed most parts of Manhattan and many locations in other parts of NYC.</p><p>• ADM Performance Chart (Fig. <ref type="figure">1B</ref>): Performance statistics of the three models, TCNN1, CNN-LSTM, and FCN-LSTM, are shown which are aggregated in realtime for any selected region on the map view. Users can select to check the accuracy or perplexity values of these models aggregated with respect to street type, weather, time of day. For example in Fig. <ref type="figure">1B</ref>, the accuracy and perplexity are shown for different street types (highway, city street, etc.). In addition, users can also check the aggregated performance values according to actual driver actions. • Trip Filters (Fig. <ref type="figure">1C</ref>) and Trip Distribution Charts (Fig. <ref type="figure">1D</ref>):</p><p>Users can select video trips for bidirectional analysis.  the same as the actual action, the radio button is checked. Then, each row in this view represents a combination of predictions of different ADMs. The count of all predictions (i.e., locations and images frames) of each combination is shown. Buttons in the first column are all checked since makes it easy for comparison in rows with actual actions. For example, the first row in Fig. <ref type="figure">1E</ref> indicates that there are 34,342 predictions in the selected trips which only FCN-LSTM gives a correct prediction. Users can also make critical locations of these predictions visible on the map to study them in detail. • Street View Thumbnails (Fig. <ref type="figure">1F</ref>): Visual contents of the critical locations are shown in the thumbnail view. Users can click to enlarge the image which is also linked to its position on the map. In the design of thumbnails, we realized that there are many images that are similar because predictions are made every 1/3 second from a video clip. Showing these sequential images cannot well utilize the visualization space. Therefore, we need to show different representative images at the top of the thumbnail view. Here, we implement an image comparison algorithm based on Structural Similarity Index Measure (SSIM), which automatically finds the most different images <ref type="bibr" target="#b54">[54]</ref>. As shown in Fig. <ref type="figure">1F</ref>, the thumbnail images show different locations and situations. Users can further see more images by sliding down this view. We limit the maximum number of video frames to be shown in this thumbnail view to 300. The choice is based on (1) users would not scroll down to check more than three hundred images;</p><p>(2) the selected region in the spatial study usually do not have more than hundreds of different locations with different views; and (3) the interactive performance of the interface will not be affected. Under each image, four driving action icons are designed to help users quickly identify the actual and predicted actions, which are shown in Fig. <ref type="figure" target="#fig_1">3</ref>. The icons are colored as black (actual action), red (TCNN1 prediction), blue (CNN-LSTM prediction), and green A supplemental video is presented to show interactions on the interfaces and use cases. In this paper, the image and video contents have been modified by blurring when necessary to preserve privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USE CASES</head><p>A primary goal of autonomous driving researchers is to visually explore the performance of ADMs. Our system allows them to do this in different spatial regions. Next, we show examples of users performing visual exploration with our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Case 1: Visual exploration with spatial conditions</head><p>Fig. <ref type="figure">1</ref> shows the global performance and street type information, together with video trips. First, users can observe spatial distribution of the data in the NYC area in Fig. <ref type="figure">1A</ref>. It helps users understand that most videos are recorded in Manhattan and also cover major roads in Queens and Brooklyn. The training is related to the built-in environment in these areas. Thus, the trained models may not work well on other suburban or rural areas. In addition, the performance measures in the whole area are shown in Fig. <ref type="figure">1B</ref>, where the accuracy and perplexity of the three networks are displayed with respect to street types. It can be found that they have similar values. But for parking lots, TCNN1 has relatively low accuracy. For tunnel locations, all the models have a high perplexity indicating low confidence. This knowledge can be used for model analysis and training improvement.</p><p>Users further observe the average accuracy and perplexity of zipcode regions in the area by hiding the trips on the map. For example, users draw a polygon around one region with low perplexity and zoom in to study it. It can be seen on the map that this area is around LGA airport.</p><p>As shown in Fig. <ref type="figure" target="#fig_2">4</ref>, in this area, most of the video trips are in the highways and roads around the airport, while a few of them ar in the residential area. In Fig. <ref type="figure" target="#fig_2">4B</ref>, users choose to show critical locations where all three ADMs fail in prediction, which appear as blue points on the map. By studying Fig. <ref type="figure" target="#fig_2">4C</ref> in the street view thumbnails, users can click each location on the map or on the thumbnails to study individual situations.</p><p>Users then study weather-based ADM performance, they find snowy day links to a low accuracy (Fig. <ref type="figure" target="#fig_2">4A</ref>). Then a snowy day in the spatial conditions filter is selected as shown in Fig. <ref type="figure" target="#fig_4">5A</ref>, finding two related video trips whose accuracy and perplexity distributions are displayed. The two trips are shown on the map (Fig. <ref type="figure" target="#fig_4">5B</ref>). Users study on the model prediction view where one row is selected (Fig. <ref type="figure" target="#fig_4">5C</ref>). This row reflects ten predictions where CNN-LSTM and FCN-LSTM give correct predictions and TCNN1 fails. The ten prediction locations are shown on the map and the street view thumbnails (Fig. <ref type="figure" target="#fig_4">5D</ref>). It can be seen that snow on roadsides may affect the performance of TCNN1 while the traffic is very light.</p><p>Users can also conduct a study of street types. As shown in Fig. <ref type="figure" target="#fig_5">6A</ref>, the video trips at parking lots (based on Fig. <ref type="figure" target="#fig_2">4</ref>) are selected and they have low accuracy and high perplexity values. Users observe their locations which are in the outdoor airport parking lot. In Fig. <ref type="figure" target="#fig_5">6B</ref>, a set of thumbnail images are shown. These images are part of the locations where only FCN-LSTM gives correct predictions and other ADMs fail. In these scenes, the actual driver's actions are all stop while TCNN1 and CNN-LSTM give predictions of go forward. It may be due to the lack of training for stop situations, and also shows the advantage of FCN-LSTM, which the original designer claimed as the best model <ref type="bibr" target="#b58">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case 2: Visual study of locations with model accuracy condition</head><p>In addition to study ADMs with spatial conditions, it is also of interest to perform visual analytics based on performance metrics. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, users can study locations related to low performance (accuracy &lt; 50%) of ADM prediction. In Fig. <ref type="figure" target="#fig_6">7A</ref>, users select accuracy percentages lower than 50. In the overview of Fig. <ref type="figure" target="#fig_6">7B</ref>, it is interesting to find that most of such trips are distributed in the lower Manhattan area. This observation matches the common opinion that it may be hard for autonomous vehicles to operate in complex built-in environments such as lower Manhattan. This area has a complex transportation network, situations and rules, high-rise buildings, and many pedestrians.</p><p>After zooming in to this area, users select the top row in the Model Performance View (Fig. <ref type="figure" target="#fig_6">7C</ref>), where all three ADMs give wrong predictions at 146 locations shown on the map (It can also be seen that the bottom row indicates only 40 locations have correct predictions). Users check the critical locations in the top row by studying their visual contents in Fig. <ref type="figure" target="#fig_6">7D</ref>. They can click to enlarge them for details (such as in Fig. <ref type="figure" target="#fig_6">7E</ref>). It can be realized that these wrong predictions happen in busy city streets. In Fig. <ref type="figure" target="#fig_6">7E</ref>, one situation happens when stopped traffic just starts to move at an intersection. Here the actual driver's action is go ahead, and the ADMs predict slow and stop. This indicates the anticipation by the real driver that there is no need to slow based on prior experience. On the contrary, the ADMs rely on vision information only which may not easily give anticipated predictions in this situation.</p><p>Moreover, in Fig. <ref type="figure" target="#fig_6">7F</ref>, users find these error predictions mostly happen at "daytime" in comparison to "night", which is possibly due to the rush hours in this busy city. And they happen mostly in "undefined"(data missed) and "rainy" days and on "city streets". Such information may provide guidance for collecting future training data to improve model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Case 3: Drill-down visual study of video trips</head><p>Fig. <ref type="figure" target="#fig_7">8</ref> shows the drill-down visual exploration of video trips following the study in Fig. <ref type="figure" target="#fig_6">7</ref>. Here, users select one trip on the map view (Fig. <ref type="figure" target="#fig_7">8A</ref>) which goes from south to north along a major city street. The weather is clear and the time is night, which are shown at the bottom of the map. The video of this trip played in Fig. <ref type="figure" target="#fig_7">8D</ref>. Users also observe the details in Fig. <ref type="figure" target="#fig_7">8F</ref> to overview the ADM predictions of this trip. In the timeline view, users realize that there exists a sudden increase of perplexity values for all three ADMs. They drag the slider to these locations. The video is also played to the corresponding locations where a car ahead is braking. The three predictions give conflicting results to the actual action. While the real driver's action is go straight, the ADMs predict slow and stop. Here, the vehicle speed is low (5.51 mph) and so the real driver feels it is OK to go ahead. But the ADMs suggest preventive driving to slow/stop the car for safety. Users further click to see the details of prediction vectors, as shown in Fig. <ref type="figure" target="#fig_7">8G</ref>. They find the prediction scores for "stop/slow"(1.44 or 1.45) are just slightly higher than "go straight" (1.14 -1.27), which partly explains the high perplexity. In such cases, maybe the models can be refined to include preventive driving actions through better training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERT EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Domain Experts and Procedure</head><p>We interviewed several domain experts to evaluate the utility of our new approach to satisfy their needs in domain research. The experts were from two major research areas. One group (GA) had two experts (professors with Ph.D. degrees) working on using machine/deep learning in engineering fields including the study of unmanned vehicles, robots, and their swarms. One professor had developed driving recognition models based on DL and human behavior study. The models were used to build trust between humans and autonomous vehicles. Another group (GU) had three experts (all with Ph.D. degrees and two professors) working in urban study, remote sensing, geography, and land use. They have an interest to understand ADMs and their relations to geographical factors. We conducted individual virtual meetings with each expert for about 1 to 2 hours including system demonstration, test use, and interview for system evaluation.</p><p>First, we introduced the ADMs and video datasets used in the system. We then showed our Web-based VA interface. Next, the experts explored the interface within the NYC area through an internet browser. Finally, they were asked to evaluate the system in several facets by answering questions including:</p><p>• System utility:</p><p>For GA: How do you think this system can contribute to the study of autonomous driving models? For GU: How do you think this system can contribute to the study of geographical infrastructure and transportation? • Comparison:</p><p>How do you compare this system with existing visualization approaches in your field? • Usefulness of visualizations: How do you evaluate the visualization functions in support of data analysis? • Limitation: What are the limitations of the system and visualization functions? • Suggestions: How do you suggest us to further improve the system?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Evaluation Results</head><p>We collected and summarized experts' answers to the questions.</p><p>• System utility:</p><p>For GA: The experts agreed that the system could contribute to the research in ADM. They identified that the system could help researchers analyze performances and identify the causes of unqualified data, or wrong labels, or threshold settings for ADMs. They claimed the system "... be really helpful to analyze model bad performance, and potential risk ...". With this system the models "will be continuously improved for performance and reliability". They pointed "One of the biggest challenges in an ADM model is to identify where a model is not performing well. In my opinion, the system will be highly appreciated by the ADM-based researchers" to analyze if there is a need to collect training datasets at different locations.</p><p>For GU: The experts believed that this work has the potential to be used in infrastructure and transportation study. They had not realized that such large-scale open-source datasets (e.g., BDD) were utilized by geographical and urban researchers. They said this system may help domain users utilize such spatial and video datasets in a new way. Furthermore, they liked the system since it could "downscale spatial analytic to fine-level and space-time resolution within a highly visual context". "This is more understandable to domain researchers on urban infrastructure and transportation." • Comparison:</p><p>For GA: The experts said there were no existing visualization systems that did the same work. One professor claimed that "I think this is a cutting edge ADM system".</p><p>For GU: The experts did not realize any visualization system using ADMs and spatial video datasets for geography-based study. All groups liked the new approach of this system and thought it could contribute to their fields with the new datasets and models. One expert noticed that "the system interactively visualizes the algorithms in the urban scene ...".</p><p>• Usefulness of visualizations: All the experts in both groups agreed that the system was very useful in ADM data analysis with the visualization functions. They preferred to use spatial condition filter and model metric filters. The mostly liked functions were the ability that they could study ADMs and locations with respect to environmental attributes such as day, night, rainy, or snowy.</p><p>For GA: The experts found it was very useful to observe the errors of different models in the timeline view, which provides a very detailed analysis. GA experts also liked region-based study: "I like the feature as it allows us to explore the performance of an ADM for any specific region of a city." For GU: The experts agreed that the system allowed them to work on ADM and spatial video data, with familiar functions as in many GIS tools. They mentioned the algorithm performance analysis could help urban scientists understand the impacts of urban environment settings on driving behaviors. "The system is also useful to identify location feature with clustered incidents in very few locations." • Limitation:</p><p>For GA: The experts would like the system to integrate object detection or other computer vision-based techniques (e.g. semantic segmentation) for further analysis of video contents. For example, moving humans (e.g. pedestrians) in the scene played critical roles in driving decision making, which could be used in the system. They also realized domain users might need some efforts to learn and understand the visualization system. They also suggested extending the system to more open datasets and cities.</p><p>For GU: The experts would like the visualization system to include available road conditions, traffic congestion, the dilapidation of houses in the neighborhood, etc. One expert also pointed that the system did not allow users to upload their own data. • Suggestions:</p><p>The experts suggested us to address the above limitations. Moreover, some of them suggested that "interpretation of unsatisfied performance such as cause or consequence generation could be improved." One expert mentioned that it would be helpful if researchers were allowed to make changes in an existing model and see performance changes. An GU expert also suggested adding a data fusion function to integrate various urban datasets to enrich the urban background information for the ADM comparison. Some also suggested a set of visualization details such as adding more labels and explanations to the terms and functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND CONCLUSION</head><p>In this paper, we develop and evaluate new visualization techniques for analyzing ADM prediction data within a geographical environment. It presents a new VA system which can be used by domain experts to enhance their data analytic capability of large-scale ADM data. The system can be further improved in several directions. System Scalability: The system is designed for the BDD dataset with some limitations of scalability. First, we currently study three ADMs.</p><p>Although we expect a few other models can be added directly, if more than 10 models are to be studied together, the system interface should be re-designed to accommodate them. Second, the driving actions are limited to four. If more actions are interested such as "slightly turn left or right", the system should easily handle it. However, if continuous action predictions of wheel angles and speed ranges need to be analyzed, the system will need to be revised. Third, the system cannot integrate more AD attributes such as multiple camera inputs and IMU sensor data, which would be our future study topics.</p><p>When the size of models, actions, and data increase, it may become overwhelming for users to conduct effective studies. Excessive exploration efforts may impair their capability to form insights. Therefore, automatic recommendation algorithms and visualizations may need to be developed for the system. ADM refinement: The system mostly focuses on data analytics of ADM predictions with geo-visualization tools. However, it does not solve the problem of how the spatial context information can be used to improve the perception models with actionable insights. Moreover, the system is not designed for on-the-fly model debugging and refinement. In the future, VA tools can be developed to integrate spatial information with the design of such neural networks by AD experts. Video content: Video contents are used in this work as image frames coupled with predictions and locations. The spatial videos can be further processed by computer vision tools, such as DL-based segmentation and object detection. This information can be further integrated into the VA system. For example, pedestrians may be extracted and add a new aspect in visual analysis to better explain driving predictions. Data integration: There exist many other types of urban datasets which may be incorporated into the ADM analytical system. In the current system, the spatial conditions include street types, weather, and video recording time. We could further include traffic data, neighborhood data (e.g., highrise buildings), and other influential information in the analysis. Domain research: The emerging ADM datasets including large-scale spatial video data may become a useful and interesting resource for research of a variety of urban scientists. As we can foresee the prevalence of autonomous driving vehicles in a near future, urban researchers either from transportation studies, urban planning, or social sciences want to know more about ADMs and their influences. Our system may give them a visualization tool to start the work, by integrating their domain-specific problems.</p><p>The real world AD system can be much more complex with different perception models with multiple sensor inputs. In future work, we first plan to address the limitations and suggestions pointed out by the domain experts in evaluation. We will also extend the use of this tool to more ADM scenarios. We will further make new technical development as discussed in this section.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of CNN-LSTM and FCN-LSTM networks. future motion. Realtime image frames in a sequence of time steps are utilized to provide current and previous state signals, and the ADMs implement a driving action function as:</figDesc><graphic coords="3,53.75,49.25,250.94,141.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Driving action icons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual exploration of model performance with spatial conditions in a selected region. (A) Observe ADMs performance on snowy days; (B) Select wrong predictions of all three ADMs. Their locations are shown on the map as blue points; (C) Visualize corresponding video frames. video contents and street-view environments, to provide important cues of critical situations that affect model prediction results. • T4. Studying models with spatial condition: Integrate geo-context information into the above model analytical tasks by facilitating users with spatial conditions to perform ADM analysis, in the levels of streets and regions, and in the aspects of weather and time. Users can conduct efficient browsing, filtering, and queries, as well as a drill-down study of detailed information. • T5. Studying locations with ADM prediction behaviors: Discover geo-locations that have specified prediction results of interest.Users thus are able to link model behaviors to geo-spatial and/or environmental factors. We realized that there exist more tasks in promoting wider use of ADM and its datasets, such as linking behaviors of neural network components with geospatial and street-view disparities, which can be further addressed in future work by VA community.</figDesc><graphic coords="5,59.03,49.49,502.98,282.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>First, users can choose video trips based on spatial conditions in the Spatial Conditions Filter as shown in Fig.1C. Then, the distribution charts of ADM accuracy and perplexity show the histograms of those trips satisfying the conditions (Fig.1D). The trips are also shown on the map. Users can click on bars of the charts to further filter the trips. Second, users can also choose to use Model Metrics Filter to select trips based on the accuracy and/or perplexity values. For example, they can extract trips with an average accuracy smaller than 60% to find questionable prediction results at spatial locations. With this filter, the Trip Distribution Charts show the histograms of the selected trips according to street type, weather, time of day. Users can interactively select specified bars on the charts as well. The filtered data will be updated on the map view.• Model Prediction Filter (Fig.1E): After trips of interest are selected, users can study and compare different model predictions results. In this view, four columns represent actual driver action, and predicted actions from the three models. If a model's prediction is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visual study of ADMs at snowy days in a local area. (A) View model performance distributions; (B) Visualize locations on map; (C) Study prediction features of ADMs; (D) Observe key video frames.</figDesc><graphic coords="6,44.99,49.49,250.46,334.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visual study of ADMs at parking lots. (A) Filter data with paking lots; (B) Study related video details.(FCN-LSTM prediction), which are used throughout the system. In addition, users can conduct a drill-down study of videos and critical locations on video trips. A visualization interface of Trip Study Interface is shown in Fig.8. It coordinates several views including: • Regional Map View (Fig.8A): Users can click to select one trip or one location on the main interface, and then the regional map view automatically zooms into fine details around the selected object. A map inlet (Fig.8B) shows this area's location in the global view. All the video trips are visualized as trajectories with a start point (green) and end point (yellow). The dataset does not have an excessive amount of trips in small areas, so it does not affect system performance by showing all trips. Several trips may travel the same road. With this view, they can be clearly identified. An active trip can be selected which is highlighted and its driving direction is shown in black arrows. Users can also drag a marker to see details of the video and ADM prediction contents. • Trip List View (Fig.8C): This view lists all trips inside the zoomin area. Each video trip's ID, prediction accuracy, and perplexity are shown for users to quickly check ADM performance. Users can click to select one trip in the list (or directly click on the map). Then this trip's video content is shown in Fig. 8D. Below the video, the actual speed and predictions of ADMs at the current frame are shown. Here in addition to the driving action icons, we also show the actual prediction probabilities. Users can further click them to show a popup view of the full prediction vectors (as shown in Fig. 8G). • Trip Timeline View (Fig. 8E): Studying the variation of model performance over a trip from the trip start to end time is an important method. We design the timeline view where four rows indicate actual and three ADMs, respectively. At each row, the line chart shows the perplexity values at each corresponding location along this trip. And the dotted line separated by different action icons is used to indicate the prediction actions along the trip. For example, the bottom three rows have the predictions of stop/slow, then go straight, and then stop/slow again. But the predictions are not made at the same locations. A slider above allows users to drag along the trip to show accurate values of speed and prediction perplexities. It also changes the video contents in Fig. 8D and the marker location in Fig. 8A.A supplemental video is presented to show interactions on the interfaces and use cases. In this paper, the image and video contents have been modified by blurring when necessary to preserve privacy.</figDesc><graphic coords="6,307.55,49.49,250.46,195.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visual study of locations with model performance condition. (A) Select trips whose accuracy of three ADMs below 50%; (B) Visualize spatial distribution of trips in the city; (C) Filter locations with wrong predictions; (D) Observe visual contents in thumbnails; (E) Enlarge view of selected thumbnail images; (F) Study data distributions with spatial attributes.</figDesc><graphic coords="8,44.99,49.49,513.26,292.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Drill-down visual study of video trips. (A) Regional map view; (b) Global map inlet; (C) Video trip list view: (D) Corresponding video of a selected trip; (E) Trip timelines view of model prediction details; (F) Slider on the timelines to show detail performance values. (G) Pop-up view of prediction vectors.</figDesc><graphic coords="9,53.99,49.37,513.21,286.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,77.63,131.21,465.77,260.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Querying by spatial and ADM performance conditions.</figDesc><table><row><cell>Query by</cell><cell>Spatial Conditions</cell></row><row><cell>Region</cell><cell>Any polygon shape</cell></row><row><cell>Street</cell><cell>Street Segment ID</cell></row><row><cell>Time of Day</cell><cell>day, night, dawn/dusk</cell></row><row><cell>Street Type</cell><cell>city street, highway, residential,</cell></row><row><cell></cell><cell>tunnel, parking, gas station</cell></row><row><cell>Weather</cell><cell>clear, overcast, snowy, rainy, cloudy, foggy</cell></row><row><cell>Query by</cell><cell>ADM Performance Conditions</cell></row><row><cell>Accuracy</cell><cell>1-10, 10-20, ... , 90-100 percentage</cell></row><row><cell>Perplexity</cell><cell>1-10, 10-20, ... , 90-100 percentage</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank the anonymous reviewers. This work was partly supported by NSF-1739491 and Kent State University graduate assistantship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantictraj: A new approach to interacting with massive taxi trajectories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Dohuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Space, time and visual analytics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Demsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dransch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dykes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Fabrikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Kraak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tominski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1577" to="1600" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revealing patterns and trends of mass mobility through spatial and temporal abstraction of origin-destination movement data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2120" to="2136" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial generalization and aggregation of massive movement data. Visualization and Computer Graphics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="219" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual analytics of movement: An overview of methods, tools and procedures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="24" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Explaining predictions of non-linear classifiers in NLP</title>
		<author>
			<persName><forename type="first">L</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
				<meeting>the 1st Workshop on Representation Learning for NLP<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">Aug. 2016</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Interpreting and Explaining Deep Neural Networks for Classification of Audio Signals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><surname>Openstreetmap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visualbackprop: Efficient visualization of cnns for autonomous driving</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Ackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zieba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4701" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Explaining how a deep neural network trained with end-to-end learning steers a car</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<idno>abs/1704.07911</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">D³ data-driven documents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bostock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ogievetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2301" to="2309" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flowstrates: An approach for visual exploration of temporal origin-destination data</title>
		<author>
			<persName><forename type="first">I</forename><surname>Boyandin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lalanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Eurographics / IEEE -VGTC Conference on Visualization, EuroVis&apos;11</title>
				<meeting>the 13th Eurographics / IEEE -VGTC Conference on Visualization, EuroVis&apos;11<address><addrLine>Chichester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
	<note>The Eurographs Association &amp;#38</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename></persName>
		</author>
		<title level="m">#38; Sons, Ltd</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rnnbow: Visualizing learning via backpropagation gradients in rnns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="39" to="50" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cityscapes dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scharwächter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on The Future of Datasets in Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual exploration of big spatio-temporal urban data: A study of new york city taxi trips</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2149" to="2158" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IJRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VATLD: A visual analytics system to assess, understand and improve traffic light detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Shekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Grün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<title level="m">A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural Networks. Int. Conf. Mach. Learn. Workshop Vis Deep Learn</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tripvista: Triple perspective visual trajectory analytics and its application on microscopic traffic data at a road intersection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Pacific Visualization Symposium</title>
				<meeting>the 2011 IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">End-to-end learning of driving models with surround-view cameras and route planners</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2674" to="2693" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A visual designer of layer-wise relevance propagation models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jamonnak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trajgraph: A graph-based visual analytics approach to studying urban network centralities using taxi trajectory data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="169" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FromDaDy: Spreading Aircraft Trajectories Across Views to Support Iterative Queries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tissoires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Conversy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSAC-TIONS ON VISUALIZATION AND COMPUTER GRAPHICS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1017" to="1024" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geovisuals: a visual analytics approach to leverage the potential of spatial videos and associated geonarratives</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jamonnak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Dohuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2115" to="2135" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end race driving with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geo time information visualization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kapler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="146" />
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<title level="m">ConvNetJS: Deep Learning in your browser</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Retainvis: Visual analytics with interpretable and interactive recurrent neural networks on electronic medical records</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="309" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semanticsspace-time cube: A conceptual framework for systematic analysis of texts in space and time</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1789" to="1806" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual analysis of route diversity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Visual Analytics Science and Technology</title>
				<meeting>IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2011-10">October 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Topkube: A rankaware data cube for real-time exploration of spatiotemporal data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Klosowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1394" to="1407" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sidewalk extraction using aerial and street view images. Environment and Planning B: Urban Analytics and City Science</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">2399808321995817</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end driving in a realistic racing game with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toromanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="474" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flow map layout</title>
		<author>
			<persName><forename type="first">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization, 2005. INFOVIS 2005</title>
				<imprint>
			<date type="published" when="2005-10">Oct 2005</date>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Alvinn: An autonomous land vehicle in a neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing the Hidden Activity of Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning framework for autonomous driving</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="70" to="76" />
			<date type="published" when="2017-01">2017. Jan 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning a driving simulator</title>
		<author>
			<persName><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hotz</surname></persName>
		</author>
		<idno>abs/1608.01230</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Streetvizor: Visual exploration of human-scale urban forms based on street views</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Arisona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schubiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burkhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1004" to="1013" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6034</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (workshop track</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interpretable deep neural networks for single-trial EEG classification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Converting video formats with ffmpeg</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux Journal</title>
		<imprint>
			<biblScope unit="issue">146</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual traffic jam analysis based on trajectory data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van De Wetering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2159" to="2168" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals</title>
				<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wongsuphasawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Visualisation of origins, destinations and flows with od maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dykes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slingsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cartographic Journal</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interactive visual exploration of a large spatio-temporal dataset: Reflections on a geovisualization mashup</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dykes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slingsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1176" to="1183" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A survey of autonomous driving: Common practices and emerging technologies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yurtsever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carballo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="58443" to="58469" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="818" to="833" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016-Decem:2921-2929, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Social inequalities in neighborhood visual walkability: Using street view imagery and deep learning technologies to facilitate healthy city planning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sustainable Cities and Society</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">101605</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visual abstraction of large scale geospatial origin-destination movement data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
