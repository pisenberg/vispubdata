<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Where Can We Help? A V isual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenbin</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lincan</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Arvind</forename><forename type="middle">Kumar</forename><surname>Shekar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Gou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liu</forename><surname>Ren</surname></persName>
						</author>
						<title level="a" type="main">Where Can We Help? A V isual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C5AC354B92A36D359B2B7B37B59AB312</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Model diagnosis</term>
					<term>semantic segmentation</term>
					<term>spatial representation learning</term>
					<term>adversarial learning</term>
					<term>autonomous driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. 1. The VASS system: (a) data summarization including data configurations (a1) and statistics of objects' attributes (a2); (b) MatrixScape summarizes objects' performance at different levels of detail, including a block view (b1) providing an overview of objects' performance over different semantic classes, data sources, and model versions, and a zoomed-in view (b2) visualizing various performance information of individual objects, such as context-aware spatial information, performance scores, ground truth masks (b3), and visual appearance (b4); (c) a live test view of a selected object including the driving scene (c1), ground truth mask (c2), prediction mask (c3), and model's sensitivity with respect to the selected object (c4).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semantic segmentation plays a crucial role in scene understanding for autonomous driving. It partitions an input image into semantically meaningful regions at the pixel level and assigns each region with a semantic label such as pedestrian, car, and road. With recent advances in deep learning, semantic segmentation models (such as fully convolutional networks <ref type="bibr" target="#b28">[29]</ref>, U-Net <ref type="bibr" target="#b38">[39]</ref>, and DeepLab <ref type="bibr" target="#b4">[5]</ref>) built upon deep convolutional neural networks (CNNs) have shown promising results in various segmentation tasks such as segmentation of lost cargos <ref type="bibr" target="#b33">[34]</ref> and semantic understanding of urban street scenes <ref type="bibr" target="#b7">[8]</ref>.</p><p>Despite the superior performance of deep semantic segmentation models, a thorough evaluation of models' accuracy and robustness is required before deploying them to autonomous vehicles due to safety concerns. On one hand, we need to analyze models' accuracy over objects with numerous semantic classes and data sources to fully understand when and why models tend to fail. On the other hand, identifying and understanding models' potential vulnerabilities are critical to improve models' robustness against unseen driving scenes. This is particularly important for some critical objects, such as lost cargos <ref type="bibr" target="#b33">[34]</ref> and pedestrians in urban street scenes <ref type="bibr" target="#b7">[8]</ref>, as these objects may appear in many possible locations over which models have not been trained.</p><p>Various visual analytics techniques have been developed to interpret and diagnose general CNNs, such as visualizing and analyzing neuron activation and feature maps <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref>, exploring and examining model predictions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>, and investigating training dynamics <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>. However, most of the existing approaches focus on classifiers, the method of visualizing and analyzing the performance of deep semantic segmentation models still lacks investigation in autonomous driving.</p><p>Moreover, existing approaches pay little attention to the potential vulnerabilities of DNN-based models, especially for the safety-critical objects in unseen scenes for autonomous driving. Several pioneering work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref> has been proposed to identify and understand the potential vulnerabilities of CNNs with the recent advances in adversarial learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44]</ref>. However, those approaches focus more on assessing models' robustness with respect to objects' appearance by adding either imperceptible noises <ref type="bibr" target="#b3">[4]</ref> or perturbing learned visual representations (e.g. lighting, color, and blurriness) <ref type="bibr" target="#b13">[14]</ref>. The influence of objects' spatial and context information, such as position, size, and interaction with their context is less studied. The context-aware spatial information of objects plays an important role in autonomous driving because the position of many safety-critical objects (e.g., lost cargos, pedestrians, and cars) varies strongly in driving scenes and often affects models' performance significantly <ref type="bibr" target="#b20">[21]</ref>.</p><p>To address these challenges, we propose VASS, a Visual Analytics approach to diagnosing and improving the accuracy and robustness of Semantic Segmentation models, especially for movable objects such as lost cargos and pedestrians that are critical in autonomous driving.</p><p>Specifically, we first learn a context-aware spatial representation of objects, such as position, size, and aspect ratio, from given driving scenes. With this spatial representation, we can: 1) estimate the distribution of objects' spatial information (e.g., possible positions, sizes, and aspect ratios) in different driving scenes; 2) summarize and interpret models' performance with respect to objects' spatial information; 3) generate new test cases by properly inserting new objects into driving scenes by considering scene contexts. Then, we use adversarial learning to efficiently generate unseen test examples by perturbing objects' position and size within the learned spatial representation. At last, we develop a visual analytics system that visualizes and analyzes models' performance over both natural and adversarial data and derives actionable insights to improve models' accuracy and spatial robustness.</p><p>We demonstrate the effectiveness of our approach through two case studies including lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative improvements of models' performance with obtained actionable insights from VASS. In summary, the contributions of this paper are fourfold:</p><p>• The first visual analytics framework for diagnosing and improving deep semantic segmentation models over crucial movable objects in autonomous driving; • A context-aware spatial representation learning method to extract representation of objects' spatial information (e.g., position, size, and aspect ratio), which serves as the foundation of adversarial data generation, visual summarization and interpretation; • A spatial adversarial learning method to generate movable objects in given driving scenes and identify models' potential vulnerabilities with respect to objects' spatial information; • A proven human-in-the-loop method for performance improvement with two real-world use cases in a deployed system, where domain experts can inject their insights to models via visual analytics guided data generation and augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Semantic Segmentation</head><p>Semantic segmentation is an essential component in many perception systems such as autonomous driving. In this work, we focus on the semantic segmentation of image data, which partitions images into multiple segments and classifies each segment into a class. Numerous techniques have been developed for semantic segmentation, such as watersheds <ref type="bibr" target="#b31">[32]</ref>, conditional random fields <ref type="bibr" target="#b16">[17]</ref>, and active contours <ref type="bibr" target="#b18">[19]</ref>.</p><p>Recently, deep learning has been playing an increasingly important role in semantic segmentation with remarkable performance improvements. Deep semantic segmentation models are mostly built upon fully convolutional networks <ref type="bibr" target="#b28">[29]</ref> with various improvements to capture multiscale contextual information, including feature fusion <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b51">52]</ref>, encoder-decoder architecture <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>, multiscale and pyramid networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, and atrous convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Similar to most other deep learning techniques, deep semantic segmentation models are difficult to interpret and diagnose. Moreover, they are vulnerable to adversarial examples or edge cases. Although a few methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46]</ref> have been proposed to visualize and analyze the segmentation of medical images, which mainly deal with static or deformable objects. Techniques addressing the challenges of the segmentation of crucial movable objects in autonomous driving are less studied. In this work, we focus on diagnosing semantic segmentation models for autonomous driving, especially the model robustness with respect to objects' spatial context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interpreting and Diagnosing CNNs</head><p>Model Interpretation Visual analytics has been widely used to understand how CNN works. For example, Liu et al. <ref type="bibr" target="#b26">[27]</ref> proposed CNNVis to understand the behavior of CNN classifiers by grouping neurons with similar roles. Rauber et al. <ref type="bibr" target="#b36">[37]</ref> use dimensionality reduction to reveal the relations between neurons and feature maps. Wang et al. <ref type="bibr" target="#b47">[48]</ref> proposed GANViz to understand the mechanism behind generative adversarial networks. Recently, explainable surrogate models (e.g., linear models) are used to explain the behavior of CNNs. For instance, Wang et al. <ref type="bibr" target="#b48">[49]</ref> proposed DeepVID that uses a linear regression model to explain the behavior of CNN classifiers. Ming et al. <ref type="bibr" target="#b30">[31]</ref> use decision rules to summarize and interpret the behavior of classification models. Compared with these approaches, our work tackles the semantic segmentation problem in autonomous driving for the first time and presents an approach to assessing and improving the performance of semantic segmentation models in addition to explaining the behavior of them.</p><p>Model Diagnosis To assess and understand models' performance, numerous visual analytics approaches have been developed. A group of methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38]</ref> focuses on visualizing and analyzing the accuracy of classifiers by summarizing and comparing models' prediction results, namely true/false positives/negatives. Recent work has been paying more attention to analyzing potential vulnerabilities of CNN models. For example, Cao et al. <ref type="bibr" target="#b3">[4]</ref> developed AEVis to understand how adversarial examples fool a target model by comparing the model's behavior between adversarial and normal data instances. Ma et al. <ref type="bibr" target="#b29">[30]</ref> explain and explore model vulnerabilities through visual analytics equipped with data poisoning attacks. On one hand, these approaches can facilitate the understanding of models' potential vulnerabilities, on the other hand, they do not generate adversarial examples with semantic meanings, which limits their capability of generating actionable insights to guide the improvement of models' robustness.</p><p>To address this limitation, Gou et al. <ref type="bibr" target="#b13">[14]</ref> use disentangled representation learning to extract semantic latent representations of data instances (e.g., color and brightness of traffic lights) and semantic adversarial learning to generate adversarial examples that are semantically meaningful. With visual analytics, they developed several performance improvement strategies based on actionable insights derived from the adversarial examples. However, their method is limited to static objects such as traffic lights and only studies the robustness of objects' appearance without considering the surrounding context. Compared with their approach, our work focuses more on the influence of objects' spatial information, such as position, size, and interaction with the context. Moreover, our approach is developed for semantic segmentation problems that often involve the recognition of numerous semantic classes. In summary, both methods are complementary to each other to access and improve the robustness of perception systems for autonomous driving.  The goal of semantic segmentation is to assign semantic labels (e.g., pedestrian, car, and road) to every pixel in an input image (Fig. <ref type="figure" target="#fig_0">2</ref>), which is widely used in autonomous driving for scene understanding. Recently, various deep learning approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref> have been proposed to improve the performance of semantic segmentation. As semantic segmentation models keep evolving in both academia and industry, we make the proposed approach applicable to any model without accessing model parameters, namely model-agnostic. Hence, in this work, we focus on the inputs and outputs of models without studying models' detailed architectures.  Three metrics are used to evaluate a semantic segmentation model, including Intersection over Union (IoU), mean Intersection over Union (mIoU), and instance-level Intersection over Union (iIoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND 3.1 Semantic Segmentation Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metrics for Model Accuracy</head><p>IoU Given a ground truth segmentation mask G i and a predicted segmentation mask P i for a semantic class c i , IoU quantifies the agreement between two masks as the ratio between the intersection and the union of G i and P i , namely IoU = G i ∩P i G i ∪P i . This metric quantifies model performance at the class level.</p><p>mIoU For a semantic segmentation problem with multiple classes c 0,1,...,k−1 , mIoU measures the overall accuracy of a prediction as the average of the IoUs for all classes, namely mIoU = ∑ k−1 i=0 G i ∩P i kG i ∪P i . This metric quantifies model performance across all classes.</p><p>iIoU iIoU measures model performance at the instance level and is calculated as: iIoU = G j ∩P j G j ∪P j , where G j and P j are ground truth and prediction segmentation masks for an object instance I j . We introduce iIoU to measure models' accuracy for two reasons: 1) IoU is biased towards objects with large size, but many critical objects have small sizes, 2) model accuracy at instance level is used for the generation of adversarial examples (Sect. 6.2).</p><p>Here is how we assign each pixel in a prediction segmentation mask to an object instance for iIoU calculation (Fig. <ref type="figure" target="#fig_2">3</ref>). We first obtain the ground truth masks of object instances (Fig. <ref type="figure" target="#fig_2">3a</ref>) either from instance segmentation labels (if available) or by treating each connected component as an object instance. Then, for a prediction segmentation mask (Fig. <ref type="figure" target="#fig_2">3b</ref>), we assign each pixel to either a ground truth object instance or a false positive instance based on its distance to the ground truth object instances (Fig. <ref type="figure" target="#fig_2">3c</ref>). A distance threshold of 20 pixels is consistently used in this work (the dashed region in Fig. <ref type="figure" target="#fig_2">3c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN REQUIREMENTS</head><p>The general goal of VASS is to diagnose, understand, and improve the accuracy and robustness of semantic segmentation models for movable objects by considering their spatial and contextual information. To this end, we worked with four domain experts and distilled two main themes of design requirements during several design iterations.</p><p>R1: data summarization and generation. This theme is to provide a summarization of objects' spatial information and to generate unseen test cases that cover objects at edge positions.</p><p>• R1.1: Summarization of objects' spatial information. The summarization should capture objects' spatial information with semantic meanings (e.g., on the left side of the road and close to the vehicle) for complex driving scenes. Hence, we need a model that can understand the semantic meaning of various driving scenes and map objects' spatial information to a representation that conforms to the scene semantics. • R1.2: Generation of unseen test cases. To identify a model's potential vulnerabilities regarding objects' spatial and contextual information, we need a method to efficiently generate new objects at proper positions in given scene contexts that can fail the model. R2: model performance evaluation and improvement. Summarizing, interpreting, and improving models' performance is a common requirement for model diagnosis <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>. In this work, we focus on visualizing and interpreting a semantic segmentation model's accuracy and robustness with respect to objects' spatial and contextual information to derive actionable insights for performance improvement.</p><p>• With the actionable insights derived through visual analytics, a method is needed to injecting human knowledge into model development to improve models' accuracy and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OVERVIEW</head><p>The overview of the proposed approach is shown in Fig. <ref type="figure">4</ref>, which aims to diagnose and improve the accuracy and robustness of semantic segmentation models for movable objects such as lost cargos and pedestrians. Our approach consists of three major components, including context-aware spatial representation learning (Sect. 6.1), spatial adversarial learning (Sect. 6.2), and performance analysis and improvement through visual analytics (Sect. 7) as detailed below.</p><p>We start with analyzing the original data (e.g., training, validation, and test data if available) (Fig. <ref type="figure">4a</ref>) and generating unseen test cases (Fig. <ref type="figure">4b</ref>) that can fail a target model. Specifically, we focus on identifying the model's potential weakness with respect to objects' spatial information (e.g., position and size) for each driving scene. To this end, we first introduce context-aware spatial representation learning (Fig. <ref type="figure">4c</ref>) to extract a latent representation of objects' spatial distribution conditioned on given driving scenes. The learned representation captures objects' possible spatial properties, such as position, size, and aspect ratio, in given driving scenes, and is also used to visually organize the objects to summarize model performance. Then, we propose spatial adversarial learning (Fig. <ref type="figure">4d</ref>) that efficiently generates movable objects in a scene to fail a model by searching plausible spatial transformation of a movable object in the learned latent space. Afterwards the original and adversarial data are fed into the target model to obtain segmentation results for follow-up visual analysis.</p><p>We design and develop interactive visualization components (Fig. <ref type="figure">4e</ref>) to explore and analyze the segmentation results of the original and adversarial data. Through visual analytics, we identify and understand the model's weakness and derive actionable insights to improve the model's accuracy and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">METHOD 6.1 Context-aware Spatial Representation Learning</head><p>A representation learning approach is used to extract a latent representation of movable objects' spatial information (e.g., position, size, and aspect ratio) conditioned on given driving scenes. The extracted spatial representation is later used to create interpretable data summarization (R1.1) and to guide the generation of unseen test cases (R1.2).</p><p>Conditional variational autoencoder (CVAE) <ref type="bibr" target="#b41">[42]</ref> is adapted to perform context-aware spatial representation learning, which includes two components: an encoder e θ and a decoder d φ (i.e., deep neural networks with weights θ and φ , respectively), as shown in Fig. <ref type="figure">5</ref>. Given an object within a driving scene, its bounding box b i = [x min i , y min i , x max i , y max i ] is encoded into a latent vector z i of dimension size D via the encoder e θ , with the driving scene's ground truth segmentation (i.e., a mask with a semantic class label at each pixel position), m i , as the condition. The latent vector z i is then mapped into a reconstructed bounding box bi using the decoder d φ , which is also conditioned on the semantic segmentation mask m i . Here, the condition input, m i , is crucial to enable the model to learn context-aware spatial representation. We use β -VAE <ref type="bibr" target="#b2">[3]</ref> to disentangle the latent representations, which combines the reconstruction loss and the latent loss with a weight β , namely = r + β l . Through experiments, we set β to 2e-3 to balance the reconstruction accuracy and the disentanglement of the latent representation. After training, the encoder and the decoder are used for data summarization and generation as detailed below.</p><p>• With the encoder, each object's bounding box can be mapped into a latent vector that captures its spatial information, such as position and size relative to the driving scene. The dimensions of the latent vectors also have semantic meanings, such as left to right, near to far, and small to large (Fig. <ref type="figure">5a</ref>). The latent vectors are then used to summarize the performance of semantic segmentation models with respect to objects' spatial information. • Given samples drawn from the latent space, the decoder can generate objects' possible positions and sizes (i.e., bounding boxes) in given driving scenes (blue boxes in Fig. <ref type="figure" target="#fig_4">6</ref>), which are used to guide the generation of adversarial examples for robustness test. Note that the possible positions and sizes of the generated boxes are dependent on the given scene contexts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Spatial Adversarial Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Object Creation and Insertion</head><p>Given a driving scene, we first properly insert a new object into it for adversarial search. We do not change or move existing objects in the scene to avoid introducing unnecessary artifacts. To make the inserted object conform to the scene semantics (e.g., pedestrians should not be placed in the sky), we leverage the learned spatial representation to sample a possible position with the following procedure.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">7a</ref>, we first draw a sample z i from the latent space and map it into a bounding box b i using the decoder d φ and the semantic segmentation mask m i of the target driving scene x i . Then we search all training data to find an object that has the most similar bounding box with the generated box b i , and the retrieved object is scaled and translated to fit into b i . The reason for selecting an object with a similar bounding box is to keep the fidelity of the object after scaling and translation. To blend the new object into the driving scene seamlessly, we use Poisson blending <ref type="bibr" target="#b32">[33]</ref> to match the color and illumination of the object with the surrounding context. Meanwhile, Gaussian blurring is applied on the boundary of the object to mitigate the boundary artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Spatial Adversarial Learning</head><p>Spatial adversarial learning is conducted to properly and efficiently move the inserted object in the scene so that a target model fails to detect it. The main idea is to perturb the inserted object's spatial latent presentation to find the fastest way to move the object to fool the target model, as shown in Fig. <ref type="figure" target="#fig_5">7b</ref> and algorithm 1.</p><p>Specifically, given a driving scene x i with an object o i placed in a bounding box b i , the adversarial example is generated by searching for a new bounding box b i to place the object such that the model f fails to predict the transformed object's segmentation correctly. To determine whether the model fails, it is evaluated on the new scene x i with the transformed object o i and compared with the new semantic segmentation mask m i . The iIoU (larger is better as defined in Sect. 3.2) of the transformed object o i is then computed and compared with an iIoU threshold τ, and the model fails if the iIoU of the object is smaller than τ, namely, iIoU(o i , f (x i ), m i ) &lt; τ.</p><p>To make sure the bounding box b i is semantically meaningful with respect to the driving scene, we perform the adversarial search in the latent space instead of manipulating the bounding box directly. To find a latent vector z i with a minimal change that produces an adversarial example, we adopt the black-box attack method proposed in <ref type="bibr" target="#b13">[14]</ref> such that we don't have to know the architecture of the semantic segmentation model explicitly. First, we use a gradient estimation approach with natural evolution strategies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b49">50]</ref> (line 3-10 in algorithm 1) to find the gradient direction in the latent space that makes the iIoU drop at the fastest pace. Then we move the latent vector z i along the gradient direction iteratively with a predefined step size η <ref type="bibr" target="#b12">[13]</ref> (line 11-17 in algorithm 1) until the iIoU is smaller than a threshold τ or the magnitude of z i − z i is greater than a threshold λ . Note that while moving the object, we only apply Gaussian blurring to blend it with the driving scene, which is because we want to focus on the model's performance change caused by the change of object's spatial information rather than the color shift introduced by Poisson blending.</p><p>Spatial Robustness With the adversarial examples, we can interpret the robustness of a target model. To this end, we define a spatial robustness score sr i for each object o i as the mean absolute error between the latent vectors z i and z i normalized by the standard deviation of each latent dimension, namely, sr i = |z i − z i |/|z std |. This score captures how much change in the latent space we need to fail the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Data Collection for Visual Analytics</head><p>After the data preprocessing step (i.e., representation and adversarial learning), we collect the original (namely, training, validation, and test) and adversarial data along with the model's prediction to drive the visual analytics system. Specifically, for each object, we extract its spatial information (i.e., bounding box, size, and latent representation), appearance (i.e., image and semantic segmentation patches), and performance metrics (i.e., iIoU, ground truth class, and prediction class). Note that the pixels of an object could be predicted as different classes, for which we define the object's prediction class as the class with the maximum number of pixels. For the adversarial object, we also extract the robustness and the gradient direction to analyze the attack patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">VISUAL ANALYTICS SYSTEM</head><p>With the original and adversarial data, we design and implement a visual analytics system to diagnose, understand, and improve the performance of semantic segmentation models, as shown in Fig. <ref type="figure">1</ref>. The visual analytics system consists of three views: the summary view, the MatrixScape view, and the driving scene view as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Summary View</head><p>This view includes a summarization of data configurations (Fig. <ref type="figure">1-a1</ref>) and statistics of objects' key properties (Fig. <ref type="figure">1-b1</ref>). The data configurations view provides the basic configurations of the data including the data splits, the instance classes, and the models of interest. In addition, bar charts are used to show the histogram of objects' key properties including object size, iIoU, and robustness.</p><p>The summary view provides an overview of models' performance (R2.1) and enables users to filter data for detailed analysis in other views. For example, users can select the data splits and the instance classes they want to focused on in the data configuration view. Also, users can brush on the bar charts to further filter the data by limiting the range of object size, iIoU, and/or robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">MatrixScape View</head><p>The main view of our system is the MatrixScape view, which shows the performance landscape of numerous objects from different aspects of data attributes (Fig. <ref type="figure" target="#fig_6">8a</ref>) and at different levels of detail (Fig. <ref type="figure" target="#fig_6">8b and  c</ref>). This view is designed to help users: 1) identify interesting subsets of data by comparing models' performance across different semantic classes, data sources, and model versions; 2) understand models' performance over objects' spatial information within the context (R2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Spatial adversarial learning</head><p>Input: a driving scene x i and its semantic segmentation mask m i , an object o i and its bounding box b i decoded from the latent vector z i , a decoder d φ , a model f , sample size K and scale δ for gradient estimation, iIoU threshold τ, attack step size η, maximal attack distance λ Output: an adversarial example contains a latent vector z i , a bounding box b i , a transformed object o i , a scene x i , and a semantic segmentation mask m i // Return because the model is already failed </p><formula xml:id="formula_0">1 if iIoU(o i , f (x i ), m i ) &lt; τ then 2 return z i , b i , o i , x i , m i //</formula><formula xml:id="formula_1">z i = z i 12 while z i − z i &lt; λ do 13 z i = z i − η∇ 14 Update bounding box: b i = d φ (z i ) 15 Transform object: o i , x i , m i = o i→ b i , x i , m i 16 if iIoU(o i , f (x i ), m i ) &lt; τ then // Return an adversarial example 17 return z i , b i , o i , x i , m i 18 return "Attack failed within budegt λ ."</formula><p>At the first level, namely the block view, the objects are partitioned into groups and provide an overview of the objects' performance with respect to user selected categorical attributes such as the ground truth/prediction class, data source, model version (Fig. <ref type="figure" target="#fig_6">8b</ref>). Users can analyze the data from different aspects by using different categorical attributes to group the objects. For example, while grouping the objects based on their ground truth and prediction classes (Fig. <ref type="figure" target="#fig_6">8-b1</ref>), users can have a confusion matrix view of the model performance, where the size of each block encodes the number of objects within it and the color shows the average iIoU or robustness scores of those objects. By changing one categorical attribute to data source or model version, users can compare models' performance across different data sources or model versions (Fig. <ref type="figure" target="#fig_6">8-b2</ref>). Users can also group the objects based on only one categorical attribute to visualize the data distribution. For example, the distribution of objects classes can be obtained by grouping the objects based on the ground truth class, as shown in Fig. <ref type="figure" target="#fig_6">8-b3</ref>.</p><p>After identifying interesting data blocks, users can zoom and pan to visualize and explore individual objects in the detailed view, as shown in Fig. <ref type="figure" target="#fig_6">8c</ref>. The objects within each block are organized and visualized based on the method presented in <ref type="bibr" target="#b13">[14]</ref>. Specifically, the objects are aggregated into bins based on the user selected numerical attributes, such as the learned latent representation, size and iIoU. Similar to the block view, users can change the numerical attributes to aggregate the objects. For example, users can select two of the latent dimensions and use the objects' latent representation on these dimensions to aggregate the objects. After the aggregation, the spatial pattern of models' performance can be visualized by selecting a representative object for each bin and visualizing the object using different visual encodings, such as iIoU/robustness (Fig. <ref type="figure" target="#fig_6">8-c2</ref>), image patch (Fig. <ref type="figure" target="#fig_6">8-c3</ref>), and semantic segmentation patch (Fig. <ref type="figure" target="#fig_6">8-c4</ref>). Users can define how to select the representative object of each bin. In the remainder of this paper, we use the object with the median iIoU as the representative. Also, when using only one numerical attribute, the data distribution of the select attribute can be visualized for each block (Fig. <ref type="figure" target="#fig_6">8-c5</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Interactions for Model Diagnosis</head><p>To minimize users' effort in exploring and analyzing the data for model diagnosis, additional tools are developed, including the block ranking, spatial marker, and gradient glyphs. Block Ranking To aid users compare the data groups in the block view, the block size can be normalized along each row/column based on the total number of objects the row/column contains. Meanwhile, the rows and columns can be ranked based on the total number of objects they contain or the variance of the number of objects within the blocks.</p><p>Spatial Marker To help users compare the performance of objects at different spatial regions, line markers are added in the detailed view (blue dashed line in Fig. <ref type="figure" target="#fig_0">1-b2</ref>) and the average iIoU of the objects from each region separated by the marker line is computed and shown.</p><p>Gradient Glyph Gradient directions of the adversarial examples are aggregated along each row and column and visualized as arrow glyphs ( Fig. <ref type="figure" target="#fig_0">1-b2</ref>) to support users to quickly identify attack patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Driving Scene View</head><p>The driving scene view (Fig. <ref type="figure">1c</ref>) provides detailed visualization and analysis for a selected object. This view first shows the driving scene with the bounding box of the object (Fig. <ref type="figure">1-c1</ref>) to help users understand the context of the drive scene. Note that for an adversarial object, its bounding box before (blue box) and after attacking (red box) are visualized simultaneously in the driving scene to facilitate users' understanding of the attack pattern. Additionally, a zoom in view of the object is presented on the side of the driving scene, with which users can visualize and compare the ground truth (Fig. <ref type="figure" target="#fig_0">1-c2</ref>) and the predicted (Fig. <ref type="figure" target="#fig_2">1-c3</ref>) semantic segmentation of the object to understand the model's behavior on the object.</p><p>To further investigate and understand a model's behavior on an object, we use sensitivity analysis <ref type="bibr" target="#b40">[41]</ref> to analyze which region of a driving scene is more sensitivity to the segmentation result of the object. Specifically, we compute the derivative of the model's prediction with respect to each pixel of the input driving scene using back-propagation and treat the magnitude of the derivative as the sensitivity of each pixel. The sensitivity map is then overlaid on the driving scene to highlight sensitive regions as shown in Fig. <ref type="figure">1-c4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CASE STUDIES AND EVALUATION</head><p>We worked with four domain experts from a function testing team for autonomous driving systems who conduct model diagnoses in their daily work. Two of the experts are co-authors of this paper, and the other two have 5+ years of experience with autonomous driving models, especially object detection and semantic segmentation models. We refined VASS several iterations to meet the experts' requirements in diagnosing and improving semantic segmentation models and deployed it in the experts' testing platform. The following two use cases demonstrate how the experts use VASS for model diagnosis and improvement as the system is deployed in their work environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Detecting Lost Cargos on the Road</head><p>In this use case, the experts studied the Lost and Found dataset <ref type="bibr" target="#b33">[34]</ref>, which aims to detect lost cargos on the road. Three classes are used in this dataset to segment the driving scenes, including lost cargo, road, and background. The dataset includes 1036 training images with 1554 lost cargo instances and 1203 test images with 2184 lost cargo instances. We used DeepLabv3+ <ref type="bibr" target="#b5">[6]</ref>, which is a state-of-art deep learning model for semantic segmentation, to evaluate the proposed approach. Specifically, a TensorFlow implementation 1 with the xception65 backbone <ref type="bibr" target="#b6">[7]</ref> was used to train a base model. The model was trained for 200k iterations on the training images, and 304 images were taken from the test dataset to validate the training results and select the best model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Representation and Adversarial Learning</head><p>Representation Learning The bounding boxes of the 1554 training lost cargo instances and the semantic segmentation masks of the driving scenes were used to perform the context-aware spatial representation learning presented in Sect. 6.1. The spatial representation learning model, namely the CVAE, was trained for 50k iterations with a learning rate of 5e-4 and a latent dimension of 4.</p><p>Adversarial Learning With the guide of the learned spatial representation, we generated adversarial objects that fool the base model based on spatial adversarial learning (Sect. 6.2.2). The hyperparameters for spatial adversarial learning are as follows: K = 64, δ = 0.2, τ = 0.2, η = 3e − 3, and λ = 0.8. We conducted spatial adversarial learning two times for each training image and obtained 1261 adversarial objects out from the 2072 inserted objects with an attack success rate of 61%. The experts were first interested in the overall model performance. With the confusion matrix view of the model's performance on the validation dataset (in Fig. <ref type="figure" target="#fig_7">9a</ref>), they found there were many false positives and false negatives, and zoomed into the blocks to examine these cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Exploring and Analyzing Model Performance</head><p>Exploring the False Positives The visual summarization enabled the experts to identify critical objects such as false positives efficiently, as shown in Fig. <ref type="figure" target="#fig_8">10</ref> (highlighted by red contours). For example, the experts found that the false positives are often coming from specific cases, such as persons (Fig. <ref type="figure" target="#fig_8">10a-b</ref>) and road markings (Fig. <ref type="figure" target="#fig_8">10c-d</ref>). They also found that some of the false positives are actually mislabeled data (Fig. <ref type="figure" target="#fig_8">10e-g</ref>), which could be used to improve the data quality.</p><p>Examining the False Negatives Besides the false positives, the experts also would like to understand in which case the model fails to detect a lost cargo and to derive actionable insights for model improvement. To this end, the experts started by examining the performance landscape for the lost cargos in the validation dataset, as shown in Fig. <ref type="figure" target="#fig_9">11a</ref>. They had two initial observations.</p><p>Understanding the Spatial Representation To understand the model's performance with respect to objects' spatial information, the experts first analyzed the learned latent representation. They found that the objects' spatial information is mainly encoded in two latent dimensions. As shown in Fig. <ref type="figure" target="#fig_7">9b</ref>, dimension 0 mainly encodes objects' horizontal position with regard to the road, and dimension 3 mainly captures objects' distance to the vehicle. Note that the size of the objects is also captured in those dimensions. For example, if we move an object far away from the vehicle by changing its latent representation in dimension 3, the object's size will also decrease. Because the rest two dimensions do not encode much information, the experts focused on dimensions 0 and 3 for analysis. Distance to the vehicle matters. They first found that the model's performance is highly related to the object's distance to the vehicle. Specifically, the model is not performed well in the region above the blue line in Fig. <ref type="figure" target="#fig_9">11a</ref>, which corresponds to lost cargos that are far away from the vehicle. They then compared the training (Fig. <ref type="figure" target="#fig_9">11b</ref>) and the validation dataset (Fig. <ref type="figure" target="#fig_9">11a</ref>) side by side to check whether the lost cargos in the training dataset cover that region. They observed that the model performed badly even for the training dataset in that region. Therefore, the experts concluded that the lost cargos that are far away from the vehicle are difficult to train and generally have worse performance compared with lost cargos close to the vehicle.</p><p>Objects' interaction with context also makes a difference. The experts found that there are a few lost cargos that are close to the vehicle but have a low iIoU. By exploring those cases, the experts observed that most of them are on the boundary of the road (Fig. <ref type="figure" target="#fig_9">11-a1</ref>). Moreover, the experts found that the prediction result of the lost cargo in Fig. <ref type="figure" target="#fig_9">11-a1</ref> is more sensitive to the background than the lost cargo itself. Nevertheless, the model focused more on the lost cargo for correctly predicted lost cargos, as shown in Fig. <ref type="figure" target="#fig_9">11-a2</ref>. Note that, the sensitive regions are scattered because the semantic segmentation model uses atrous convolution as described in Sect. 3.1. Based on the above observations, the experts came up with a hypothesis that the interaction between the lost cargo and background could impact the model's performance.</p><p>Studying Potential Failure Cases The experts also explored and analyzed the generated adversarial objects (Fig. <ref type="figure" target="#fig_9">11c</ref>) to identify the base model's potential vulnerability. Interestingly, the failure patterns here also confirmed two initial observations they had.</p><p>First, most of the adversarial objects are within the region above the blue line in Fig. <ref type="figure" target="#fig_9">11c</ref>. Also, by visualizing the adversarial gradient directions on latent dimension 3, the experts found that most of the adversarial objects were generated by pushing away from the vehicle.</p><p>Secondly, the adversarial gradient directions on latent dimension 0 reveal that the objects were pushed towards the side of the road to fail the base model. To understand the reason behind this attack pattern, the experts examined the adversarial objects that were pushed to the side of the road. For example, for the objects in Fig. <ref type="figure" target="#fig_9">11</ref>-c1 and c2, the experts compared the position of those objects before (blue boxes) and after attacking (orange boxes). They found that those adversarial objects tend to overlap with the background class after attacking.</p><p>Base on these observations, the experts concluded that the model tends to fail on 1) objects that are far away from the vehicle and 2) objects that overlap with the background class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.3">Visual Analytics Assisted Performance Improvement</head><p>To improve the model's performance, adversarial data was mixed with the original training data and used to retrain the model. Based on the insights obtained from the above analysis, we developed three data augmentation strategies to retrain the model, as detailed below.</p><p>Global attack is based on the adversarial data generation method presented in Sect. 6.2.2 without injecting any human knowledge into the generation process. To retrain the model, we mixed the original training data with 2072 adversarial objects generated using global attack.</p><p>Position guided attack is based on the visual analytics insight that the model is particularly weak in specific regions. In this case, it is the region that is far away from the vehicle. Hence, we generated more adversarial data within that region. Specifically, we inserted a new object whose latent representation on dimension 3 is greater than -1 into each training image and generated adversarial data based on those objects. To retrain the model, we replaced half of the global adversarial objects with the position guided adversarial objects while keeping the total number of adversarial objects consistent for comparison.</p><p>Interaction guided attack is based on the visual analytics insight that objects overlapping with the background are hard to be segmented correctly. Hence, we generated more adversarial objects on the boundary between the road and the background. Specifically, we inserted a new object into each training image and make sure the inserted object is overlapping with the background with rejection sampling. Again, we generated adversarial objects from the inserted objects and replaced half of the global adversarial objects to retrain the model.</p><p>The performance of the data augmentation strategies are compared with the base model (Table <ref type="table" target="#tab_4">1</ref>), which is measured by mIoU, IoU, and iIoU for five trials. Models' performance was evaluated on the test dataset and the adversarial objects generated on the test dataset using global attack. The experts found that all three methods improved the performance of the model significantly, especially for the iIoU of the lost cargos, which is improved from 0.333 to 0.359 on the test data and 0.145 to 0.362 on the adversarial data. Moreover, the visual analytics guided methods especially the interaction guided attack can further improve model's performance on the adversarial data significantly compared with the global attack as demonstrated in Table <ref type="table" target="#tab_4">1</ref>. Lost Cargo Road Background Base 0.145±0.004 0.482±0.035 0.810±0.003 0.963±0.001 0.752±0.012 Adv (global) 0.342±0.020 0.567±0.008 0.806±0.003 0.963±0.001 0.778±0.003 Adv (spatial) 0.357±0.018 0.567±0.020 0.800±0.009 0.962±0.001 0.776±0.006 Adv (interact) 0.362±0.021 0.572±0.027 0.803±0.004 0.963±0.001 0.779±0.009</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Semantic Understanding of Urban Street Scenes</head><p>In this use case, the experts evaluated the proposed approach on the Cityscapes dataset <ref type="bibr" target="#b7">[8]</ref>, which focuses on semantic understanding of urban street scenes. The dataset segments driving scenes with 19 semantic classes, and the experts were interested in the pedestrian class the most as it is safety-critical in autonomous driving. 2975 training images were used to train the DeepLabv3+ <ref type="bibr" target="#b5">[6]</ref> model for 200k iterations, and 500 validation images were used to validate and test the trained model because the test images are not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Representation and Adversarial Learning</head><p>As the experts were interested in the model's robustness on the segmentation of pedestrians, unseen test cases were generated by placing pedestrians at different locations within a context. To this end, the bounding boxes of the 7729 training pedestrians and the semantic segmentation masks were used to learn a 4-dimensional spatial representation of the pedestrians. Then the spatial representation was used to guide the generation of adversarial objects. Note that the spatial  Performance Overview The experts started with visualizing the confusion matrix to understand the overall performance of the model on different semantic classes as shown in Fig. <ref type="figure">1-b1</ref>. With the confusion matrix, the experts were able to analyze the class distribution and identify interesting classes efficiently. For example, they found that the misclassification of trains is mostly related to the cars or buildings, but for cars or pedestrians, they could be misclassified as various classes.</p><p>Comparing the Original and Adversarial Objects To investigate the model's performance on the segmentation of pedestrians, the experts compared the model's performance on the training and adversarial objects as shown in Fig. <ref type="figure" target="#fig_10">12a</ref>, where each row represents a data split and each column represents a class that the pedestrians are classified as. The size of each block encodes the number of objects proportional to the total number of objects in each data split. Here, the experts ranked the columns based on the difference between the training and the adversarial data in terms of the size of the blocks to identify the classes that the pedestrians are easier to be misclassified as in the adversarial learning. Fig. <ref type="figure" target="#fig_10">12a</ref> shows the first 10 columns except the pedestrian class, from which the experts found that the adversarial data has more pedestrians being misclassified as specific classes compared with the training data, such as rider, vegetation, building, pole, and fence. By zooming into those blocks in the adversarial data and visualizing the ground truth segmentations as shown in Fig. <ref type="figure" target="#fig_10">12b</ref>, the experts found that most of the misclassification was caused by interaction between the pedestrians and the surrounding context. For example, the pedestrians were placed in front of buildings, poles, and fences to fail the model. One exception is the rider class, which is not caused by placing pedestrians near riders, motorcycles, or bicycles.</p><p>Studying Failure Cases The experts further conducted the instance-based analysis to verify the above findings. Fig. <ref type="figure" target="#fig_11">13</ref> shows two examples that the pedestrians are misclassified as the above mentioned classes. In Fig. <ref type="figure" target="#fig_11">13a</ref>, the experts observed that the pedestrian was initially placed on the side of a pole and pushed in front of the pole by adversarial learning to fail the model. After being pushed in front of the pole, the pedestrian's leg and the pole were in a straight line, which confused the model such that it segmented the pedestrian's leg as a pole. Also, the sensitivity analysis result shows that the leg and the surrounding context are more sensitive compared with the body of the pedestrian. In Fig. <ref type="figure" target="#fig_11">13b</ref>, the pedestrian was misclassified as a fence after being placed in front of it, and the sensitivity analysis reveals that the model is also sensitive to the fence when it predicts the segmentation of the pedestrian. Hence, the experts concluded that the interaction between pedestrians and several specific classes plays an important role in the model's performance on the segmentation of pedestrians.</p><p>Analyzing the Influence of Objects' Position The experts also examined the influence of pedestrians' position on the model's performance by studying the learned spatial representation and the gradients of the adversarial attacks as shown in Fig. <ref type="figure" target="#fig_0">1-b2</ref>. Similar to the Lost and Found dataset, the experts found that pedestrians' spatial information is mainly encoded in two out of the four dimensions. One dimension (latent dimension 1) encodes pedestrians' horizontal position, and the other one (latent dimension 3) encodes pedestrians' distance to the vehicle as shown in Fig. <ref type="figure" target="#fig_0">1-b2</ref>. The major difference is that the pedestrians are mainly on the sidewalks on either side of the road and are rarely located in the middle of the road. With the guide of the learned spatial representation, the experts visualized the gradients of the adversarial attacks. Unlike the Lost and Found dataset, there were no clear patterns for the gradients, with which the experts concluded that the interaction between the pedestrians and the context places a more important role than the position and size of the pedestrians for this use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.3">Visual Analytics Assisted Performance Improvement</head><p>Based on the insights obtained from the above analysis, we focused on the interaction guided attack to improve the model's performance, which is compared with the base model and the global attack strategy.</p><p>For the interaction guided attack, we generated more pedestrians that overlap with the identified classes (i.e., vegetation, pole, fence, wall, or traffic sign) that are easier to fail the model based on Fig. <ref type="figure" target="#fig_10">12</ref>. Here, we did not use the building, road, and car classes because the model has already performed very well on those classes and there is The evaluation results are shown in Table <ref type="table" target="#tab_5">2</ref>. Overall, the adversarial augmentation can improve the model's performance on the adversarial data significantly (i.e., iIoU increased from 0.17 to 0.727 and 0.726 with the two strategies) while preserving the model's performance on the natrual test data. Particularly, as for interaction guided attack, the experts found that the accuracy of the pedestrians on the natural test data were also improved for both iIoU and IoU on top of the global attack. The experts also found that the interaction guided attack could improve the overall performance of the model on the adversarial data (i.e., performs best in 11 out of the 19 classes) while preserving the iIoU of the pedestrians compared with the global attack. The experts were quite satisfied with the improvements considering the strong baseline and the complexity of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Domain Experts Feedback</head><p>We collected and summarized the feedback provided by the domain experts while they were completing the two case studies.</p><p>Flexible Data Exploration Overall, the domain experts liked the "flexibility" of the tool. Comparing with aggregated metrics, this tool enabled them to "target the failed cases from a very high-level summary to a pixel-level misclassification". In particular, they found that the ability to compare different classes, data sources, and models is "the most insightful" part of the tool. They also found that the tool is "specifically useful" in alleviating their pain points for model diagnosis, such as "identifying outliers" and "finding mislabeled data". For example, they were surprised that there are quite a few mislabeled lost cargos in the first use case, which could be corrected to improve the data quality.</p><p>Model Weakness Identification The experts found that the identified "corner case scenarios" are "very insightful", especially "the interaction between different objects" such as pedestrians and poles. In addition, they appreciated the visualization of the object instances in the latent space, which is "useful to map the spatial failures back to the image space". They were also happy with the performance improvements of data augmentation, which enhance their confidence to "take actions in data collection based on the explainable insights".</p><p>Improvement The experts also suggested several directions to improve VASS. First, they mentioned that the options used to explore the MatrixScape are a bit "overwhelming" at first, which could be alleviated by providing "predefined configurations". Secondly, they would like to "reduce the manual effort" when extending the tool to other datasets and tasks. 9 DISCUSSION, LIMITATIONS, AND FUTURE WORK How realistic the generated data should be? It is an intriguing and open question how photo-realistic the generated driving scenes should be to test and improve DNNs <ref type="bibr" target="#b39">[40]</ref>. Our approach demonstrates that moderate fidelity (e.g., placing movable objects into scenes with Gaussian and Poisson blending) might be enough. This also echos some recent observations in the data augmentation research field <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref>. For example, some data augmentation strategies, such as mix-up <ref type="bibr" target="#b50">[51]</ref> and cut-and-paste <ref type="bibr" target="#b10">[11]</ref>, may not generate photo-realistic training data but increase model performance via regularization mechanisms. However, it is invaluable to investigate how much extra mileage we can gain for model testing and improvement, if we adopt more advanced generation techniques, such as GAN-based compositing approaches <ref type="bibr" target="#b44">[45]</ref>, and leverage extra scene information, such as using 3D information to help 2D generation <ref type="bibr" target="#b21">[22]</ref>. This is one future direction we aim at.</p><p>How large or diverse the generated data should be to inject human knowledge into models? One useful method of injecting human knowledge to mitigate some model weakness is data generation and augmentation with domain insights <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>, which is also observed in this work. The key here is to find the weak points of current data coverage and then augment a model with generated data covering these gaps. We find these weak points (e.g., spatial distribution and interaction patterns of low performance areas) by context-aware spatial adversarial search and visual analytics guided identification. However, another open question is to understand how diverse or large we need to generate these data to sufficiently cover these data gaps. One promising direction is to explore and learn rich generation policies via reinforcement learning <ref type="bibr" target="#b8">[9]</ref>, and then recommend policies for domain experts to augment the models. We also plan to extend this work in this direction.</p><p>Generalizability of spatial representation learning. It is challenging to generalize a learned spatial representation to different classes of objects, because the distribution of objects' position and size varies significantly for different classes. For example, the lost cargos are mainly located on the road and the pedestrians are generally walking on sidewalks. Hence, the spatial representation learning has to be conducted if experts want to study a new type of object, which takes less than 4 hours on an NVIDIA V100 GPU. In the future, we would like to extend the spatial representation learning for different classes of objects by introducing categorical latent dimensions to represent object classes.</p><p>A unified framework towards human-in-the-loop model validation for autonomous driving. We observe two themes of visual analytics approaches to validate perception models in autonomous driving: a) studying the impacts of objects' visual appearances (e.g., variations of object appearances <ref type="bibr" target="#b13">[14]</ref>, object artifacts of stickers and drawings <ref type="bibr" target="#b19">[20]</ref>) over model performance; and b) researching the influence of context-dependent spatial information of objects over performance, like this work. The two threads of research are complementary with each other, and we plan to unify these two themes in the future.</p><p>Other future work. We will also improve system usability based on the feedback from domain experts, including a dashboard to summarize the distilled findings (e.g., spatial and interaction model weakness), generating and sharing evaluation reports, and including other data sources (e.g., radar and lidar data). 10 CONCLUSION This work brings the visual analytics for autonomous driving one step further in diagnosing and improving semantic segmentation of critical objects. Our approach, VASS, focuses on analyzing models' performance with respect to objects' spatial and contextual information, such as position, size, and interaction with their context, for which we develop context-aware spatial representation learning to extract representation of objects' position and size that conforms to the scene semantics and use spatial adversarial learning to generate objects at edge positions to identify models' potential vulnerabilities. Additionally, a visual analytics system is developed to visualize and analyze models' accuracy and robustness and derive actionable insights for performance improvement. We demonstrate the effectiveness of VASS through two case studies including lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative evaluation on the improvement of models' performance with actionable insights obtained from VASS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The goal of semantic segmentation is to partition an input image into multiple segments and classify each segment into a class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Assign each pixel in a prediction segmentation mask to an object instance for iIoU calculation. (a) Ground truth masks of object instances; (b) segmentation masks of prediction; (c) instance masks of prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Approach overview. Given original data (a), we first learn a context-aware spatial representation (c) based on the objects' bounding boxes (green boxes) and given scene masks, This representation captures interpretable spatial information of the possible position, size, and aspect ratio of the objects (blue boxes). Starting from a sampled box in this representation, we can insert an object and search for a position (orange box) to fail the model using spatial adversarial learning (d). The adversarial and original data are then fed into the target model to obtain segmentation results, which are visualized and analyzed with (e) a visual analytical system to obtain actionable insights to improve the model's accuracy and robustness.</figDesc><graphic coords="4,450.83,62.33,93.83,52.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Possible positions, sizes, and aspect ratios of pedestrians (blue bounding boxes) in different driving scenes, which are generated by decoding samples drawn from the context-aware latent space and hence capture the scenes' contextual information.</figDesc><graphic coords="4,57.23,449.93,105.98,53.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Spatial adversarial learning to generate data and test a semantic segmentation model: (a) obtaining a context-aware possible position by sampling the learned spatial latent space to insert a new object; (b) perturbing the object's position and size to fail the model by searching the spatial latent space with adversarial learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The design of MatrixScape, objects with different types of attributes (a) are first grouped based on different categorical attributes and visualized as a matrix of blocks (b). By changing the categorical attributes used to group the objects, the matrix has different meanings, such as confusion matrix (b1), data/model comparison (b2), and categorical data distribution (b3). While zooming into a block (c), the objects are aggregated into bins based on the numerical attributes (c1) and the bins can be visualized with different visual encodings, such as color encoding of iIoU/robustness (c2), image patch (c3), and semantic segmentation (c4) patch. Moreover, the bins can form a histogram to represent the data distribution in the block (c5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. (a) Confusion matrix of the base model's performance on the validation dataset; (b) Zoomed-in view of a block of the confusion matrix, which visualizes the performance landscape of lost cargos with respect to the learned spatial representation.</figDesc><graphic coords="6,319.19,427.49,76.34,57.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Two observations for the regions that are labeled as road or background but are predicted as lost cargos (highlighted by red contours). 1) False positives: (a-b) persons and (c-d) road markings; 2) (e-g) miss labeled regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The performance landscape of lost cargos in the (a) validation, (b) training, and (c) adversarial dataset, where the model performs badly in the region above the blue line. Also, the model is more sensitive to the surrounding context for lost cargos on the boundary of the road (a1) compared with lost cargos in the middle of the road (a2), which makes lost cargos close to the boundary of the road (c1-c2) vulnerable to adversarial attacks.representation and adversarial learning were conducted with the same hyperparameter setting as what was used in the Lost and Found dataset, except the iIoU threshold τ, which was increased to 0.3 because the base model is strong in segmenting pedestrians. In the end, we conducted the spatial adversarial learning five times for each training image except the training images with no space to insert a new object. After attacking, we obtained 7373 adversarial objects out from the 13805 inserted objects with an attack success rate of 53%.8.2.2 Exploring and Analyzing Model Performance</figDesc><graphic coords="8,313.79,244.97,125.90,63.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. (a) Comparing the number of pedestrians being misclassified as different classes cross the training and the adversarial data. (b) Visualizing the ground truth segmentation of the pedestrians being misclassified as buildings (b1), poles (b2), and fences (b3).</figDesc><graphic coords="8,213.95,300.89,69.38,78.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Adversarial examples generated for the Cityscapes dataset. After attacking, pedestrians are misclassified as (a) poles and (b) fences by the base semantic segmentation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>• {Wenbin He, Lincan Zou, Liang Gou, Liu Ren} are with Robert Bosch Research and Technology Center, USA.</figDesc><table /><note>E-mails: {wenbin.he2, lincan.zou, liang.gou, liu.ren}@us.bosch.com. • Arvind Kumar Shekar is with Robert Bosch GmbH, Germany. E-mail: arvindkumar.shekar@de.bosch.com. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Estimate the gradient of z i with respect to iIoU 3 Sample σ 0 ,...,σ k−1 ∼ N (0, I) for K times 4 Get perturbed latent vectors:z ik = z i + δ σ k , ∀k 5 Get perturbed bounding boxes: bik = d φ (z ik ), ∀k 6 Get transformed objects: õik , xik , mik = o i→ bik , x i , m i 7 Get iIoUs: u k = iIoU( õik , f ( xik ), mik ), ∀k 8 Normalize iIoUs: ûk = (u k − mean(u))/std(u), ∀k 9 Estimate gradient: ∇ = 1 Kδ Σ K−1 k=0 σ k ûk 10 Normalize gradient: ∇ = ∇</figDesc><table /><note>∇// Move z i along the gradient direction 11 Initiate latent vector:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Performance improvement with adversarial data augmentation on the Lost and Found dataset, measured by mIoU, IoU, and iIoU (5 trials, larger is better).</figDesc><table><row><cell></cell><cell cols="4">Accuracy on Original Test Dataset</cell></row><row><cell></cell><cell>iIoU Lost Cargo</cell><cell>Lost Cargo</cell><cell>IoU Road</cell><cell>Background</cell><cell>mIoU</cell></row><row><cell>Base</cell><cell cols="5">0.333±0.004 0.574±0.021 0.810±0.004 0.964±0.001 0.783±0.007</cell></row><row><cell cols="6">Adv (global) 0.357±0.009 0.599±0.007 0.806±0.007 0.963±0.001 0.789±0.003</cell></row><row><cell cols="6">Adv (spatial) 0.359±0.009 0.594±0.008 0.799±0.006 0.963±0.001 0.785±0.004</cell></row><row><cell cols="6">Adv (interact) 0.358±0.007 0.594±0.014 0.802±0.003 0.963±0.001 0.786±0.006</cell></row><row><cell></cell><cell cols="4">Accuracy on Adversarial Test Dataset</cell></row><row><cell></cell><cell>iIoU Lost Cargo</cell><cell></cell><cell>IoU</cell><cell></cell><cell>mIoU</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Performance improvement with data augmentation on the Cityscapes dataset, measured by mIoU, IoU, and iIoU (5 trials, larger is better).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Accuracy on Original Validation Dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">iIoU Pedestrian Road Sidewalk Building Wall Fence Pole TLight TSign Vegetation Terrain Sky Pedestrian Rider Car Truck Bus Train Motorcycle Bicycle IoU</cell><cell>mIoU</cell></row><row><cell>Base</cell><cell>0.617 0.981 0.849</cell><cell>0.930 0.603 0.628 0.665 0.708 0.801 0.927</cell><cell>0.637 0.952 0.821 0.638 0.956 0.849 0.893 0.797</cell><cell>0.665</cell><cell cols="2">0.782 0.794</cell></row><row><cell>Adv (global)</cell><cell>0.616 0.982 0.854</cell><cell>0.930 0.593 0.628 0.662 0.708 0.800 0.926</cell><cell>0.634 0.952 0.830 0.638 0.955 0.850 0.888 0.773</cell><cell>0.661</cell><cell cols="2">0.781 0.792</cell></row><row><cell cols="2">Adv (interact) 0.618 0.982 0.853</cell><cell>0.930 0.602 0.628 0.665 0.708 0.800 0.927</cell><cell>0.642 0.953 0.831 0.645 0.955 0.832 0.885 0.772</cell><cell>0.663</cell><cell cols="2">0.782 0.792</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy on Adversarial Test Dataset</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">iIoU Pedestrian Road Sidewalk Building Wall Fence Pole TLight TSign Vegetation Terrain Sky Pedestrian Rider Car Truck Bus Train Motorcycle Bicycle IoU</cell><cell>mIoU</cell></row><row><cell>Base</cell><cell>0.173 0.982 0.845</cell><cell>0.921 0.563 0.595 0.664 0.700 0.796 0.929</cell><cell>0.640 0.950 0.681 0.545 0.950 0.834 0.898 0.758</cell><cell>0.600</cell><cell cols="2">0.763 0.769</cell></row><row><cell>Adv (global)</cell><cell>0.727 0.983 0.850</cell><cell>0.923 0.544 0.605 0.664 0.701 0.802 0.930</cell><cell>0.645 0.951 0.798 0.610 0.951 0.841 0.867 0.681</cell><cell>0.614</cell><cell cols="2">0.767 0.775</cell></row><row><cell cols="2">Adv (interact) 0.726 0.982 0.845</cell><cell>0.923 0.552 0.607 0.668 0.703 0.800 0.930</cell><cell>0.648 0.952 0.800 0.620 0.951 0.821 0.867 0.704</cell><cell>0.610</cell><cell cols="2">0.770 0.777</cell></row><row><cell cols="3">little space to improve. Again, the generated adversarial objects were</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">mixed with the original training data to retrain the model. In total,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">we generated 13805 global adversarial objects, and 8283 interaction</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">guided adversarial objects with half of the global adversarial objects.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/models/tree/master/research/deeplab</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual methods for analyzing probabilistic classification data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1703" to="1712" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<title level="m">Understanding disentangling in β -VAE</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analyzing the noise robustness of deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Dreossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06962</idno>
		<title level="m">Counterexample-guided data augmentation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07177</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boxer: Interactive comparison of classifier results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heimerl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="181" to="193" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">VATLD: A visual analytics system to assess, understand and improve traffic light detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Shekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic-sExplorer: Visual analytics for robot control tasks involving dynamics and LSTM-based control policies</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wittenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
				<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiscale conditional random fields for image labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpiñán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="695" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2137" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-aware synthesis and placement of object instances</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10393" to="10403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AADS: Augmented autonomous driving simulation using data-driven algorithms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">28</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NATTACK: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
				<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3866" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analyzing the training processes of deep generative models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards better analysis of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ParseNet: Looking wider to see better</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations Workshop</title>
				<meeting>International Conference on Learning Representations Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Explaining vulnerabilities to adversarial machine learning through visual analytics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1075" to="1085" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RuleMatrix: Visualizing and understanding classifiers with rules</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Watershed of a continuous function</title>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="112" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
				<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lost and found: Detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Intelligent Robots and Systems</title>
				<meeting>IEEE International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GraphNet: Learning image pseudo annotations for weakly-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
				<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual analytics for the exploration and assessment of segmentation errors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Raidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J J</forename><surname>Marcelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breeuwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M M</forename><surname>Van De Wetering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics Workshop on Visual Computing for Biology and Medicine</title>
				<meeting>Eurographics Workshop on Visual Computing for Biology and Medicine</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing the hidden activity of artificial neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Squares: Supporting interactive performance analysis for multiclass classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Medical Image Computing and Computer-Assisted Intervention</title>
				<meeting>Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
				<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to generate synthetic data via compositing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Opening up the &quot;black box&quot; of medical image segmentation with statistical shape models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Landesberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andrienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bremm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wesarg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Computer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="893" to="905" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DQNViz: A visual analytics approach to understand deep Q-networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="288" to="298" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">GANViz: A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepVID: Deep visual interpretation and diagnosis for image classifiers via knowledge distillation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2168" to="2180" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Natural evolution strategies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Glasmachers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="949" to="980" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">Mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
