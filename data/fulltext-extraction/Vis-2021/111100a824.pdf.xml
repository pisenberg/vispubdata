<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haekyu</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nilaksh</forename><surname>Das</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Duggal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Austin</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Omar</forename><surname>Shaikh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fred</forename><surname>Hohman</surname></persName>
						</author>
						<author>
							<persName><roleName>Polo</roleName><forename type="first">Duen</forename><surname>Horng</surname></persName>
						</author>
						<author>
							<persName><surname>Chau</surname></persName>
						</author>
						<title level="a" type="main">NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1E67A17D7896C0D31BF73DE8FAD2665C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.1" ident="GROBID" when="2022-06-13T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clicking a neuron adds it to the Neuron Neighbor View.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neuron Projection View</head><p>1. Sarah starts exploring graph view. She selects neuron cluster for "dog face'' (pink).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph View</head><p>Cluster Popup 2. Sarah explores neuron embedding, and wonders why one "dog face" neuron is farther away from the rest.</p><p>3. Sarah discovers the "dog face" neuron is surrounded by other dog-related concepts like "furry body" and "furry head".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neuron Neighbor View 4e-734</head><p>Fig. <ref type="figure">1</ref>. NEUROCARTOGRAPHY scalably summarizes concepts learned by deep neural networks, by automatically discovering and visualizing groups of neurons that detect the same concepts. 1. Our user Sarah starts exploring which neuron clusters (shown as circles) play an important role for InceptionV1 to predict the Maltese dog class, through the Graph View (at C), which clusters neurons based on the semantic similarity of the concepts detected by the neurons, and shows how those concepts interact to form higher-level concepts. Selecting the neuron cluster for "dog face" (in pink) visualizes its member neurons' features with example patches in the Cluster Popup (at D). 2. Member neurons are highlighted in the global Neuron Projection View (at A), which summarizes all neurons' concepts from all layers by projecting them on a 2D space, placing related concepts closer together; Sarah wonders why one "dog face" neuron (#734) is farther away from the rest. 3. Adding that neuron to the Neuron Neighbor View (at B) enables discovery of the most related neurons (nearby blue neurons in projection), such as "furry body" (neuron 4e 3x3-146) and "furry head" (4d 3x3-45), suggesting that the proximate projection region is in fact capturing dog-related concepts. Concepts of neurons and neuron groups are manually labeled. Abstract-Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present NEUROCARTOGRAPHY, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. NEUROCARTOGRAPHY introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting "dog faces" of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting "dog face" and "dog tail" are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. NEUROCARTOGRAPHY scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The NEUROCARTOGRAPHY visualization runs in modern browsers and is open-sourced.</p><p>Index Terms-Deep learning interpretability, visual analytics, scalable summarization, neuron clustering, neuron embedding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The authors are at Georgia Institute of Technology. E-mail: {haekyu, nilakshdas, rahulduggal, apwright, oshaikh, fredhohman, polo} @gatech.edu. <ref type="bibr">Fred</ref>  Deep Neural Networks (DNNs) have demonstrated remarkable success in many applications, such as object detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b59">60]</ref>, speech recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>, and data-driven health care <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. However, they are often considered opaque due to their complex structure and large number of parameters. To help practitioners and researchers more confidently and responsibly deploy machine learning models, there have been major efforts and calls from both government and industry to enable greater model interpretability <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b56">57]</ref>. There is research that aims to explain models' predictions based on the inputs, such as regions of input images that contribute the most to the models' predictions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref>. However, such techniques often do not describe how and where the input features are used within the model. Recent research posits a key step towards answering these questions is to interpret neurons (also called channels), since they are highly activated for specific features from the input data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42]</ref>. While neuron-level interpretation may be a promising approach to discover insights, inspecting individual neurons can be time-consuming; furthermore, individual inspection does not easily reveal how multiple neurons may detect the same features, which means users could easily miss the bigger picture of the DNNs' decision-making processes. For example, Fig. <ref type="figure" target="#fig_0">2</ref> shows that the "dog face" concept is detected by multiple neurons in InceptionV1 model. Although it is a well-documented phenomenon that multiple neurons detect similar features <ref type="bibr" target="#b39">[40]</ref> (especially in model pruning research <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref>), there is a lack of research in <ref type="bibr" target="#b0">(1)</ref> developing scalable summarization techniques to discover concepts collectively learned by multiple neurons, and (2) enabling users to interactively interpret such concepts and their similarities. NEURO-CARTOGRAPHY aims to fill this critical research gap. Contributions. In this work, we contribute: • NEUROCARTOGRAPHY, an interactive system that scalably summarizes and visualizes fundamental concepts that contribute to the behaviors of large-scale image classifier models (Fig. <ref type="figure">1</ref>), such as InceptionV1 <ref type="bibr" target="#b52">[53]</ref>. NEUROCARTOGRAPHY automatically discovers groups of neurons that detect the same concepts and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions (Sect. 7). • Two scalable concept summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts that neurons detect (e.g., neurons detecting "dog faces" of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they cooccur (e.g., neurons detecting "dog face" and "furry body" are placed closer in the embedding space, as seen in Fig. <ref type="figure">1A, B</ref>). Both efficient techniques avoid naively comparing all neuron pairs, resulting in a time complexity that is linear to the number of neurons, rather than quadratic time. Our techniques scale to large data, such as ImageNet ILSVRC 2012 with 1.2M images <ref type="bibr" target="#b46">[47]</ref> (Sect. 6). • Interactive exploration of Concept Cascade enables users to selectively initialize and examine how a concept detected by a neuron group would trigger higher-level concepts across subsequent layers in a neural network. NEUROCARTOGRAPHY visualizes the userselected concept's "cascading effect," helping users interpret the successive concept initiations and relationships (Sect. 7.2.2). • Empirical findings through large-scale human evaluation and discovery scenarios. Through a large-scale human evaluation, we demonstrate that NEUROCARTOGRAPHY detects neuron groups representing coherent concepts with consistent meaningful human interpretations (Sect. 8). We describe how NEUROCARTOGRAPHY can help discover several interesting and surprising findings through usage scenarios, like identifying concept cascades for related classes (e.g., dogs of different breeds) and identifying isolated concepts that are unrelated to all other concepts in a neural network (Sect. 9). • An open-sourced, web-based implementation that helps broaden people's access to neural network interpretability research without the need for advanced computational resources. Our code and data are open-sourced 1 , and the system is available at the following public demo link: https://poloclub.github.io/ neuro-cartography/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK: NEURAL NETWORK INTERPRETABILITY</head><p>A Deep Neural Network (DNN) takes as input a data instance and outputs its class label. Transforming the input to the output may potentially involve billions of numerical computations, the scale of which  is incomprehensible to humans. Recent research aims to summarize these calculations into abstract concepts, helping humans interpret how DNNs work. In this section, we provide an overview of existing research for neural network interpretability that motivate our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Interpretation at Input-, Layer-, and Neuron-Level</head><p>A common approach to interpret how DNNs operate internally is to study features detected by the models. Saliency Maps <ref type="bibr" target="#b49">[50]</ref> and Grad-CAM <ref type="bibr" target="#b47">[48]</ref> explain a DNN's prediction at the input-level, by finding contributing features from the input images. TCAV <ref type="bibr" target="#b27">[28]</ref> explains a feature's importance for a class at the layer-level, by vectorizing activations in a layer and measuring the vector's sensitivity to the classes. A growing number of techniques <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref> aim to interpret such features at the neuron-level. Given a predefined segmented image that highlights regions for human-interpretable concepts, Net2vec <ref type="bibr" target="#b15">[16]</ref> represents these concepts as the linear combination of thresholded activation maps from activated neurons. These techniques quantitatively determine how each neuron contributes to the concept image. Featurevisualization techniques <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>, on the other hand, algorithmically generate synthetic images that maximally activate a particular neuron, thus visualizing what features are preferred by each neuron. Other methods <ref type="bibr" target="#b41">[42]</ref> also generate examples cropped from real images in the dataset that highly activate a specific neuron. In NEUROCARTOGRA-PHY, we use dataset examples to represent features detected by each neuron.</p><p>Some research focuses on interpreting the relationships among features in the form of embedding vectors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38</ref>]. Karpathy's t-SNE feature representation <ref type="bibr" target="#b25">[26]</ref> visualizes similarity among features at the input-level, by projecting each image onto 2D space using the image's DNN features (from a full-connected layer). Nguyen et al. <ref type="bibr" target="#b37">[38]</ref> represent multifaceted features of a neuron at the input-level. For example, it embeds onto 2D space all bell pepper images that activate a neuron detecting bell peppers, revealing the neuron's facets (e.g., red, green, yellow peppers). Activation Atlas <ref type="bibr" target="#b4">[5]</ref> computes embeddings at the layer-level, where each embedded point represents a linear combination of all neurons' activation strengths in a layer, based on spatial patches extracted from those neurons' activation maps. As the above techniques generate embeddings at abstraction levels different from ours, it is unclear how they may directly map each point in the embedding space back to specific neurons. In other words, existing approaches cannot pinpoint which neurons are responsible for detecting specific features, which NEUROCARTOGRAPHY supports (Sect. 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic Similarity of Neurons</head><p>Recent research <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56]</ref> suggests that neurons tend to detect similar features. Fig. <ref type="figure" target="#fig_0">2</ref> illustrates this claim by highlighting how different neurons detect the same feature of a dog's nose. Olah et al. <ref type="bibr" target="#b39">[40]</ref> highlights many examples of similar neurons in InceptionV1 and visualizes which concepts are detected by such neurons; however, the examples are manually curated by the authors. Identifying neurons that discover similar concepts also has practical benefits: in the neural network compression community, several methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref> leverage potential neuron redundancies to generate compressed models while maintaining prediction accuracy. Even though these methods can measure neurons' similarity, there is limited work in interpreting their semantic similarity. CNNPruner <ref type="bibr" target="#b28">[29]</ref>, an interface for pruning neurons, helps users understand the role of neurons through the filter visualization technique <ref type="bibr" target="#b51">[52]</ref>. However, it describes its pruning approach mainly based on metrics that measure instability and sensitivity of neurons, ignoring groupings of semantically similar neurons. We draw inspiration from the above important prior research to automatically find groups of similar neurons and interpret semantic similarities among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Connection among Neurons</head><p>A key role in neural networks is played by neurons. Neurons are responsible for receiving and transmitting activation signals. However, they do not work in isolation. For a neuron to detect a feature, it requires an orchestrated interaction among many neurons across different layers. Recent research <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref> visually explains how higher level concepts can be constructed by neural connections. In the context of adversarial attack, some methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref> identify where in a network the activation pathways of a benign and attacked input instance diverge, and how those diverging activations arrive at an incorrect prediction through connections among neurons. Inspired by these techniques, we summarize and visualize how neuron groups interact through connections among them, providing a new way to interpret concept cascading across layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN CHALLENGES</head><p>Our goal is to build an interactive visual summarization of concepts learned by neural networks. Concretely, we aim to help users better understand what concepts are represented internally by groups of neurons, and how these concepts are transformed into the final prediction through interactions among neuron groups. We identify the following five design challenges (C1-C4) associated with developing our summarization techniques and designing NEUROCARTOGRAPHY.</p><p>C1 Discovering neurons that detect similar concepts. Existing research on DNN interpretability tends to focus on inspecting individual neurons <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42]</ref>. While helpful, neuron-level inspection cannot easily reveal how clusters of neurons may detect the same concept, even though it is common for multiple neurons to detect similar features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref>. As a result, users can easily miss higher-order interactions that explain how DNNs operate. C2 Understanding the associations between related concepts. Interpreting individual features represented by a neural network can be useful to understand what the model sees from input data <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>. However, a model doesn't base its prediction on a single feature from the input. Instead, the final prediction is often an amalgamation of multiple concepts detected by the model. This raises fundamental questions about the associations between related concepts. For example, when a concept is detected by a neural network (e.g., "dog face"), what other concepts are likely to be detected at the same time, and how are they related (e.g., would "dog tail" and "dog leg" be strongly related to "dog face")? C3 Scaling up concept summarization to all classes, neurons, and large datasets. Recent research in our visualization community has started to prioritize scalability to support large datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>. However, understanding how those approaches may extend to enable the discovery of groups of similar neurons-and encode semantic relatedness of concepts detected by the neurons-remains unclear. In the context of our work, computing neuron relationships can be computationally expensive. A naive algorithm to measure the neuron similarity would require comparing all neuron pairs, which is computationally expensive (i.e., time complexity is quadratic in the number of neurons). This naturally leads us to the question: how can we more efficiently support grouping neurons and encoding concept relatedness for complex DNNs? C4 Understanding concept influence in a network. A promising approach to interpret a model's internal behaviour involves understanding how the model detects and combines features during inference <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41]</ref>. Recent research has proposed approaches to help users interpret how features may be connected <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>, but these approaches are performed at the neuron level, and are limited to only analyzing the relationships of neurons across two adjacent layers, instead of across the whole network. To this end, our work aims to answer a broader set of questions: How can a group of neurons detecting a concept trigger other concepts across the connections and layers of a DNN? Furthermore, how can we design an interactive visualization to support flexible exploration of such a "concept cascade"?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN GOALS</head><p>We distill the design challenges identified in Sect. 3 to the following design goals (G1-G4) that guide NEUROCARTOGRAPHY's development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G1 Clustering similar neurons based on activation overlap.</head><p>We aim to address a major research gap in existing work by developing techniques to discover neurons that detect the same features (C1). Specifically, we build on prior research findings that neurons tend to selectively respond to certain input features; in the context of DNNs, this means such neurons' activation maps have larger values at locations where the feature is present in the input image <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b58">59]</ref>. Our idea is to group neurons based on how similar their activation maps are by (1) comparing the locations of the highly-activated values in the maps (Sect. 6.1) and ( <ref type="formula" target="#formula_1">2</ref>) visualizing the concepts that are detected by such neuron groups (Sect. 7.2). G2 Encoding concept associations between related concepts. We aim to analyze and visualize how concepts are related based on how often they co-occur (C2). Our intuition is that neurons detecting highly related concepts (e.g., "dog face", "dog tail") are frequently co-activated. We aim to preserve these concept associations by learning vector representations for neurons that detect concepts associations on large image datasets (Sect. 6.2). Furthermore, we visualize the concept embedding to enable users to interactively explore and understand related concepts across different level of abstraction such as "dog face" (higher level) and "dog eyes" (lower level) (Sect. 7.1). G3 Scalable summarization of concepts learned by a neural network. We aim to scale up neuron clustering (G1) and neuron embedding (G2) techniques to all neurons and all images by avoiding an explicit comparison of all neuron pairs (Sect. 6). For scalable neuron clustering, we aim to project neurons' activation patterns into a reduced dimension and hash neurons into buckets by using these reduced projections as the key. Using this technique, we can hash similar neurons in the same bucket with high probability; more importantly, we can do this in time linear to the number of neurons instead of quadratic (Sect. 6.1). For scalable neuron embeddings, we aim to subsample neuron pairs and use the sampled pairs to learn neuron vectors that will preserve the general properties of concept relatedness. From these vectors, we can infer concept relatedness of any neuron pair without comparing them directly (Sect. 6.2). G4 Interactive interface to explore Concept Cascade We aim to design and develop an interactive interface that enables users to selectively initialize and examine how a concept detected by a neuron group would trigger higher-level concepts across subsequent layers in a neural network C2. NEUROCARTOGRAPHY visualizes the user-selected concept's cascade effect, helping users interpret the successive concept initiations and relationships (Sect. 7.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MODEL CHOICE AND BACKGROUND</head><p>In this work, we demonstrate our approach using InceptionV1 <ref type="bibr" target="#b34">[35]</ref>, a prevalent, large-scale image classifier that achieves top-5 accuracy of 89.5% on the ImageNet dataset that contains over 1.2 millions images across 1000 classes. InceptionV1 consists of multiple inception modules of parallel convolutional layers. In each module, there are four layers: an input layer, an intermediate layer where kernels' size are 3x3, another intermediate layer where kernels' size are 5x5, and an output layer. Each inception module is given a name of the form "mixed{number}{letter}," where the {number} and {letter} denote the location of a layer in the network. In InceptionV1, there are 9 such modules: mixed3{a,b}, mixed4{a,b,c,d,e}, and mixed5{a,b}.</p><p>The input and output layer are given by the module name. For the intermediate layers, an suffix of either 3x3 or 5x5 is appended. For example, mixed3b is an earlier input layer and mixed3b 3x3 is an intermediate layer. While there are more technical complexities in each inception module, we follow existing interpretability literature and consider the 9 mixed layers as the primary layers of the network <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Although this work uses specific architectural choice, the proposed summarization and visualization techniques are general and can be applied to other neural network architectures in other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SCALABLE NEURAL NETWORK SUMMARIZATION</head><p>NEUROCARTOGRAPHY introduces two new scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by those neurons, and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur. NEUROCARTOGRAPHY leverages these techniques to summarize concepts learned by neural networks. We formulate neuron clusterings in Sect. 6.1, describe neuron embeddings in Sect. 6.2, and detail how we filter concepts that are important for the prediction of each class in Sect. 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Neuron Clustering</head><p>We aim to discover groups of neurons within the same layer that detect the same concepts. Our main idea is to group neurons that have similar activation maps. A neuron's activation map is a 2D image representing the neuron's response to an input instance, computed by the convolution of a trained kernel applied to the previous layer. A neuron's activation map reflects features detected by a neuron, showing increased values in regions of the map where detected features exist. Thus, if two neurons have similar activation maps, where highly activated areas of two activation maps largely overlap, we group the neurons. For example, in Fig. <ref type="figure" target="#fig_0">2</ref>, two neurons 460 and 483 in layer mixed4c of InceptionV1 are grouped by our approach, since their activation maps have high values on similar areas. Our neuron clustering approach has two phases. First, in the preprocessing stage, we cluster neurons quickly and efficiently without looking at neurons' activation maps in detail. Next, in the main clustering stage, we further divide the preprocessed neuron groups based on the degree of overlap in the neurons' activation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Preprocessing: Group Neurons Based on Common Preferred Images</head><p>The preprocessing stage aims to efficiently and quickly cluster neurons before comparing neurons' activation maps in detail. Our main idea is to group neurons if they are highly activated by many common images.</p><p>For each neuron i, we first find a set of k images that maximally activate i. We sort the images by the maximum value of activation maps of i for given those images, and take the first k images. We denote the set of top k images for i as X i . For two neurons i and j, we define their similarity as the Jaccard similarity of X i and X j as follows.</p><p>Definition 1 Concept Similarity Based on Top Images. Given two neurons i and j, and the neurons' top image sets X i and X j , we define the similarity of i and j as the Jaccard similarity between X i and X j .</p><p>This value is 0 when the two image sets are disjoint, and 1 when they are equal. Neurons i and j are more similar when the Jaccard similarity is closer to 1. We formally define the similarity of i and j in Eq. ( <ref type="formula" target="#formula_0">1</ref>)</p><formula xml:id="formula_0">SimTopImgs(i, j) = |X i ∩ X j | |X i ∪ X j |<label>(1)</label></formula><p>To scalably group neurons based on the common image sets, NEU-ROCARTOGRAPHY uses two techniques: (1) Min-Hashing efficiently approximates the Jaccard similarity between two neurons' top image sets; (2) Locality-Sensitive Hashing (LSH) efficiently hashes similar neurons in terms of the Jaccard similarity into the same buckets with high probability. It is a popular technique to use Min-Hashing and LSH to efficiently estimate Jaccard similarity between two sets and find sets of similar items, due to their scalability and theoretical guarantees on the accuracy of finding nearest neighbors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Min-Hashing. It is computationally costly to measure the Jaccard similarity between large sets due to the expensive set intersection and union operations that Eq. ( <ref type="formula" target="#formula_0">1</ref>) involves. Min-Hashing <ref type="bibr" target="#b2">[3]</ref> efficiently estimates the Jaccard similarity. Let h be a hash function that randomly maps the entire items {1, ..., N} to {1, ..., N} in one-to-one correspondence. Let h min be a min-hash function that outputs the minimum value retrieved from the function h: for a set S, h min (S) = min s∈S (h(s)). The key property of Min-Hashing is that the probability of the h min values of two sets being equal is equal to the Jaccard similarity between the sets. Formally, for two sets S 1 and S 2 , Pr[h min (S 1 ) = h min (S 2 )] = Jaccard Similarity between S 1 and S 2 . For each neuron i, we define I i as the index of images in X i . By using the theoretical property of Min-Hashing, Pr[h min (I i ) = h min (I j )] = SimTopImgs(i, j).</p><p>Locality-Sensitive Hashing (LSH). Min-Hashing efficiently estimates the similarity of two neurons' top common images. However, it is still computationally expensive to measure the similarity of all neuron pairs. LSH is a scalable technique that finds reasonable approximations for grouping similar items without comparing all item pairs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref>. For each neuron i and its top images' index set I i , we produce n Minhash values h 1 (I i ), ..., h n (I i ) with n hash functions h 1 , ..., h n . Then we partition the n values into b bands, each consisting of r values, such that n = b × r. For each band, we hash neurons into the same buckets where r hash values of such neurons are identical. Then we finally cluster i and j in the same group, if they appear in the same bucket in at least one band. Theoretically, the probability that neuron i and j will hash to the same bucket in at least one of the b bands is 1 − (1 − s r ) b , where s is the true Jaccard Similarity between I i and I j <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Main Clustering: Group Neurons Based on Overlap of Activation Maps</head><p>While the preprocessing stage offers an efficient approach for preliminary neuron grouping, the main clustering stage performs finer clustering based on overlap of activation maps. In the preprocessing stage, for example, neurons for "cars" and neurons for "roads" might be grouped together, as those concepts may frequently co-occur in the same images. The main clustering stage further divides these neurons into different groups based on the concepts encoded in the activation map of the neurons. Within a preprocessed group, we finally cluster neurons in the same group, if highly activated part of the neurons' activation maps overlap significantly. We formally define the similarity of neurons i and j used in the main clustering stage as follows.</p><p>Definition 2 Concept Similarity Based on Activation Map. Given an input image x and two neurons i, j in the same layer, we denote their activation map as Z i (x) and Z j (x). To take only highly activated parts in each activation map, we quantize the activation maps as Q i (x) = Z i (x) &gt; 0 and Q j (x) = Z j (x) &gt; 0, where the quantized activation maps are a boolean matrix (i.e., true means high activation). We define the concept similarity of i and j in Eq. ( <ref type="formula" target="#formula_1">2</ref>), where ∧ and ∨ are element-wise and and or operation respectively, and numTrue(•) returns the number of true values in the input matrix. If numTrue(Q i (x) ∨ Q j (x)) = 0, the similarity between i and j is defined as 0.</p><formula xml:id="formula_1">SimActMap(i, j) = numTrue(Q i (x) ∧ Q j (x)) numTrue(Q i (x) ∨ Q j (x))<label>(2)</label></formula><p>The similarity SimActMap(i, j) in Eq. ( <ref type="formula" target="#formula_1">2</ref>) can be interpreted as the Jaccard similarity of highly activated parts in activation maps of i and j. We leverage Min-Hashing and LSH in the main clustering phase again for improved scalability. For each neuron group G created in the preprocessing stage, we sample t images from the union of every belonging neuron's top k images. We denote the set of such sampled images as X G . Formally, X G = sample(∪ i∈G X i ,t), where sample(S,t) randomly samples t items in set S. The main reason we use the sampled images (instead of all images) is that using all images is not very useful; because neurons selectively respond to only some images, the neurons are not activated at all by many images. To compare the similarity of two neurons, we only consider cases where both neurons are highly activated. Thus, we sample images from the union of top k images produced in the preprocessing stage, which includes many images that are likely to activate many neurons in the group G. For each group G produced in the preprocessing phase and for each image x ∈ I G , we run LSH to further group neurons based on the activation map. Then we finally group two neurons in the same bucket if the two neurons are hashed in the same bucket for least one image in I G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Hyperparameter Selections for Neuron Clustering</head><p>Our neuron clustering approach uses a few hyperparameters: t is the maximum number of sampled images for each preprocessed neuron group, k is the number of top images (among 1.2M images) for each neuron, b is the number of bands in LSH, and r is the size of the bands. t helps reduce runtime through sampling; a larger value means using more samples (thus longer runtime). We experimented with values in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">200]</ref> and observed little change in the results, thus we decided on t = 100. A larger k increases the chances of discovering more neuron pairs that are similarly activated. However, a value that is too large (e.g., 1M) means most neurons would have highly similar or identical sets of top images. We decided on k = 200, the highest value that provided good clustering while keeping total runtime reasonable. A larger b provides more opportunities to group neurons that have high Jaccard similarities. For preprocessing, we experimented with values in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">2500]</ref> and the clustering results stabilized after b reached 1500, thus we used b = 2000. For main clustering, we experimented with values in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>, and used b = 20 as clustering results did not change beyond that. A larger r allows us to prune neuron pairs with low Jaccard similarities. However, a value that is too large could prune neuron pairs even if they have high Jaccard similarities. Thus, we aimed to pick a value that is not too large, or too small. We experimented with values in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> for preprocessing, and <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref> for main clustering, and found the "middle" values of 3 and 15, respectively, provided good coherence among examples image patches in the cluster results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Neuron Embedding</head><p>To encode associations between concepts detected by neurons, we learn neuron embeddings that preserve the relatedness of such concepts, where neurons that detect more related concepts are located closer in the embedding space. Our embedding approach consists of two steps.</p><p>• Step 1. Learn vector representations of all neurons to encode relatedness among neurons' concepts. (Sect. 6.2.1) • Step 2. Reduce the dimensions of the learned vector representation to 2D for visualization (Sect. 6.2.2), which we will describe in Sect. 7.1.</p><p>The decision to adopt a two-step approach to first generate higherdimensional vector representations for neurons (Step 1) was motivated by prior work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b57">58]</ref>, where abstract concepts are better captured by higher-dimensional representations, which opens up the possibilities for supporting interpretation tasks at higher fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Step 1: Encode Relatedness of Neurons' Concepts via Vector Representations</head><p>The objective function J to minimize of our embedding approach is defined in Eq. ( <ref type="formula" target="#formula_2">3</ref>), where D is a set of sampled neuron pairs detecting highly related concepts, V i is the embedding vector of neuron i to learn, and σ (x) is the sigmoid function (i.e., σ</p><formula xml:id="formula_2">(x) = 1/(1 + e −x )) . J = ∑ (i, j)∈D − log(σ (V i •V j ))<label>(3)</label></formula><p>The objective function induces the embedding vectors V i and V j of neurons i and j detecting highly related concepts to yield high σ (V i •V j ).</p><p>A large value of the dot product of two vectors indicates that the vectors are far from the origin in the same direction. The sigmoid function controls the magnitude of dot product, so that those vectors do not move too far away from the origin. Thus, the objective function induces vectors of highly related neurons to be located closely and moderately far away from the origin. Our use of cross-entropy loss was motivated by prior work <ref type="bibr" target="#b33">[34]</ref> where no predefined classes or labels are available, which is the case here (i.e., no concept labels for each neuron).</p><p>To sample neurons of highly related concepts, we find neurons that are frequently co-activated. We reuse the top k images for each neuron which are obtained at the preprocessing stage of neuron clustering (Sect. 6.1.1). For each image, we first generate a list of neurons that have such image in the neuron's top k images. Then, we sample neuron pairs from the top neuron list. We first randomly shuffle the neuron list, apply a sliding window of size 2 on the shuffled list, and sample pairs of neurons that are co-occurred in the sliding window. A good property of using sampled neuron pairs instead of all pairs is that sampled pairs indirectly can imply the relation of all neuron pairs. If two pairs (a, b) and (b, c) of highly related items are sampled, we can infer that (a, c) is also highly related. Our embedding approach efficiently learns and preserves such indirect concept relatedness, as well as direct relatedness straightly from the sampled pairs. Note that the number of sampled pairs is linear to the number of neurons, not quadratic. This sampling approach results in the training data of size linear to the number of neurons, and the time complexity of our neuron embedding method is linear to the number of neurons.</p><p>To further speed up the optimization process, we use negative sampling approach: concretely, we find pairs of non-related neurons and induce their embedding vectors far apart. For a given neuron, we find another non-related neuron by randomly sampling one among all neurons in the same layer. The new objective is defined in Eq. ( <ref type="formula" target="#formula_3">4</ref>), where M is the size of negative sampling for a pair of related neurons (i, j).</p><formula xml:id="formula_3">J = ∑ (i, j)∈D − log(σ (V i •V j )) + M ∑ m=1 − log(1 − σ (V i •V m )) − log(1 − σ (V j •V m ))<label>(4)</label></formula><p>We use gradient descent to learn neuron vector representations that optimize J. The derivatives of objective function J with respect to the neuron vector V i and V j are as in Eq. ( <ref type="formula">5</ref>) and <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_4">∂ J ∂V i = − 1 − σ (V i •V j ) V j + M ∑ m=1 σ (V i •V m )V m (5) ∂ J ∂V j = − 1 − σ (V i •V j ) V i + M ∑ m=1 σ (V j •V m )V m<label>(6)</label></formula><p>We update the embedding by gradient descent as in Eq. ( <ref type="formula">7</ref>), where γ is the learning rate.</p><formula xml:id="formula_5">V i ← V i − γ ∂ J ∂V i , V j ← V j − γ ∂ J ∂V j (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Step 2: Dimensionality Reduction</head><p>To project neurons' vector representations learned in the previous step onto a 2D space, we use UMAP, a non-linear dimensionality reduction technique that preserves global data structures and local neighbor relations <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Hyperparameter Selection for Neuron Embedding</head><p>Our neuron embedding uses a few hyperparameters: size of negative sampling M, number of epochs, and learning rate γ for training. Tuning M helps prevent overfitting; a value that is too large could introduce significant noise during training. Epochs and learning rate affect the training runtime and quality. More epochs usually causes more distinct clusters to form. We experimented with different combinations of hyperparameter values and found these chosen ones provide a good visual results: M=10, epoch=30, γ=0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Filtering Each Class's Important Model Substructures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Important Neurons and Neuron Groups</head><p>Our goal is to summarize important model substructures (i.e., neurons or neuron groups) that contribute to a model's class prediction. To do so, we follow an approach similar to <ref type="bibr" target="#b22">[23]</ref>, and adapt our implementation to include neuron groups. The importance of each neuron i in layer l for the prediction for a class c is computed as the number of images of c by which i is maximally activated. Whether a neuron i in layer l is highly activated for an image x is decided by the maximum value of activation map of i for x (i.e., max(Z l i (x))). For an image x for a class c, we find 5 most activated neurons for each layer as suggested in <ref type="bibr" target="#b22">[23]</ref>.</p><p>After obtaining the importance of each neuron for a class c, we then compute importance of each neuron group for c. For a neuron group G, we take 10 neurons at most that have the highest importance for c, to avoid overcrowding the visualization, while we also observe that 10 neurons are enough to explain what concepts the group is detecting. We then compute the importance of G for c as the average of importance score of at most the top 10 neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Important Connections among Neurons and Neuron Groups</head><p>Important neurons and neuron groups summarize concepts that are important for a class prediction. We further want to describe how those concepts interact to form higher level abstractions, by representing the connections between the model substructures. We follow similar steps in <ref type="bibr" target="#b22">[23]</ref> to compute the influence from each neuron in a given layer to each other neuron in a successive layer. At a high-level, for a given class c, the influence of each neuron-neuron connection is the number of images of c that use such connection as a major path to transmit high activation signal. Thus, if two neurons have strong influence values, it means that the neuron in an earlier layer activated the other neuron in a subsequent layer for many images of the selected class. Finally, to detect if an image uses a connection as a major path, we compute the maximum convolution value of activation map corresponding to the source neuron multiplied with a slice of learned kernel tensor between the two neurons. We refer the reader to <ref type="bibr" target="#b22">[23]</ref> for a more in-depth treatment of this approach. Our implementation deviates from <ref type="bibr" target="#b22">[23]</ref> in the last step, when aggregating influence values for each neuron group. For two neuron groups G1 and G2, we compute average of the influence values between any neuron in G1 and any neuron in G2, and use such average value as the connection weight between G1 and G2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">USER INTERFACE</head><p>Based on our design goals in Sect. 4 and our neural network summarization techniques in Sect. 6, we present NEUROCARTOGRAPHY, an interactive system that summarizes concepts detected by neuron clusters (Fig. <ref type="figure">1</ref>). The NEUROCARTOGRAPHY interface consists of three components: (1) the header shows metadata and contains a few controls for the Graph View, (2) Neuron Projection View and Neuron Neighbor View provides a global overview of all neurons (Sect. 7.1), and (3) Graph View visualizes concept relations (Sect. 7.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Neuron Projection View and Neuron Neighbor View: Global Overview of Neurons' Concept</head><p>The Neuron Projection View (Fig. <ref type="figure">1A</ref>) aims to show a global overview of all neurons in a model, such that neurons detecting more related concepts are positioned closer. We project embedding of all neurons computed in Sect. 6.2 onto a 2D space.</p><p>Clicking a neuron highlights other neurons that detect the most related concepts in the Neuron Projection View and the Neuron Neighbor View.</p><p>Hovering over a neuron shows the neuron's example patches.</p><p>Fig. <ref type="figure">3</ref>. Neuron Projection View visualizes associations between related concepts based on co-occurrence, by projecting neurons on a 2D space where each rectangle is a neuron. Hovering over a neuron shows its example data patches in a popup. Clicking a neuron selects it, marking it with a white dot at its center. Related neurons are in blue, and related example patches are displayed in Neuron Neighbor View (at bottom left).</p><p>In the Neuron Projection View, each neuron is represented as a rectangle. Hovering over a neuron shows representative example patches to explain such neuron's concepts (Fig. <ref type="figure">3</ref>). Users can focus on a neuron by clicking the corresponding rectangle. As a result, the selected neuron is marked with inner white rectangle, and the selected neuron's neighbors are highlighted with blue in the embedding space. Also, at the bottom left, Neuron Neighbor View displays the neighbor neurons with their example patches (Fig. <ref type="figure">3</ref>). At the top, Neuron Projection View provides multiple filtering options, such as showing all neurons, neurons for a selected class, and neurons for selected clusters. Users can freely zoom and pan in the view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Graph View: Neuron Clusters and their Interaction</head><p>The goal of Graph View is to visualize what concepts are detected by which clusters of neurons, and how those clusters collaborate to form higher-level concepts and the final prediction (Fig. <ref type="figure">1C</ref>). Graph View provides two modes through toggle button in the header: (1) class exploration mode (Sect. 7.2.1) to visualize concepts important for a user-selected class and (2) concept cascade mode (Sect. 7.2.2) to selectively activate a concept and examine its cascade effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Class exploration</head><p>When a user selects a class in the header, Graph View shows a subgraph of the entire neural network that is relevant for the class prediction (Sect. 6.3). The nodes (circles) are neuron clusters or individual neurons within the same layers, displayed in the order of their important scores computed in Sect. 6.3.1. The edges represent influence among the nodes, where edge weights are computed in Sect. 6.3.2. Thicker edges indicate more important connections. Users can filter the graph based on the importance score, using the a slider in the header. The graph visualization is shown in a zoomable and panable canvas.</p><p>When a user hovers over a node, NEUROCARTOGRAPHY first highlights the node and the edges connected to the hovered node with pink, then displays the Cluster Popup (Fig. <ref type="figure">1D</ref>) view, which contains example patches. User can pin interesting nodes by clicking them. The pinned nodes are highlighted in the Neuron Projection View and the Graph View with pink. Neuron Projection View and Graph View are tightly integrated: hovering over a neuron in Neuron Projection View highlights its belonging cluster in the Graph View, and hovering over a node in Graph View highlights its member neurons in the Neuron  Projection View. Users can filter neurons in the Neuron Projection View to focus on pinned nodes using the dropdown menu in the header.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Concept Cascade: Successive Concept Detection Initiated by a User-Selected Concept</head><p>Besides displaying how detected concepts interact within two adjacent layers for given images of a class, Graph View visualizes how one concept can influence multiple other concepts across all layers and classes through a concept cascade. Users can enter the concept cascade mode by toggling the button in the header (Fig. <ref type="figure" target="#fig_2">4</ref>, at 1). Then, users can click a concept cluster node to select it and manually activate the selected concept cluster (Fig. <ref type="figure" target="#fig_2">4</ref>, at 2), without feeding any input images. Clicking causes the neuron cluster to induce a concept cascade that triggers higher-level concepts across subsequent layers in the model. Manual concept stimulation involves first setting every value in the activation map of the selected cluster's member neurons to 1, then forward-feeding such activation to the next layers through existing connections between neurons. In the concept cascade, neuron clusters that are highly related to Maltese dog, and strongly contribute to the prediction "furry face," are included as part of the class's summary (Fig. <ref type="figure" target="#fig_2">4</ref>, left). The cascade also includes concepts that related to the Maltese dog class, but are not as important for its prediction, such as "bear face" and "black dog face." We highlight such concepts and their connections on the right side of the class's graph summary (Fig. <ref type="figure" target="#fig_2">4</ref>, right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">System Implementation</head><p>To broaden access to our work, NEUROCARTOGRAPHY is web-based and can be accessed from any modern web-browser. NEUROCARTOG-RAPHY uses the standard HTML/CSS/JavaScript stack, and D3.js for rendering SVGs. We ran all our deep learning code on a NVIDIA DGX 1, a workstation with 8 GPUs (each with 32GB of RAM), 80 CPU cores, and 504GB of RAM. With this machine we could generate everything required for all 1000 ImageNet classes under 24 hours. The most computation intensive part was computing all neurons' activation maps for all images (once for determining top-k images for each neuron; once for main clustering stage; each run was about 5 hours). Each of the other processes, thanks to the scalability of Min-Hashing, LSH, and the sampling-based neuron embedding, took less than an hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">HUMAN EXPERIMENT TO EVALUATE NEUROCARTOGRAPHY</head><p>To validate the human interpretability of the clusters discovered with NEUROCARTOGRAPHY, we conducted a large-scale human evaluation using Amazon Mechanical Turk, a standard practice for computer vision tasks <ref type="bibr" target="#b11">[12]</ref>, basing our experimental design on similar work in image <ref type="bibr" target="#b16">[17]</ref> and language <ref type="bibr" target="#b6">[7]</ref> based cluster interpretability studies. For the experiment each user was presented a series of tasks like the task shown in Fig. <ref type="figure">6</ref>. In each task we display the example patches for 6 neurons which are composed of 6 randomly sampled neurons, or 5 neurons from a cluster (determined either by NEUROCARTOGRAPHY or hand selected) as well as one 'intruder' neuron which is randomly sampled from the rest of the network. Users were told that each task contains either all random patches, or a cluster of 3-5 related neurons, and were asked to identify which, if any, of the shown patches form a cohesive cluster and could select any number of the given options. By not disclosing the number of intruders users are forced to only select clusters which are fully coherent among themselves (as opposed to simply finding the least representative example). We can then measure 'false positives' (where users mistakenly identify the intruder as part of the cluster) and 'false negatives' (where users may decide to not include patches from the cluster). This style of study measures the performance of human annotators against model output as ground truth, the inverse of standard machine learning metrics, in order to understand the difficulty of the interpretation task that the interpretability method (in our case clusters from NEUROCARTOGRAPHY) provides. We assume that higher performance by humans equates to an easier task for humans to interpret, highlighting a better methodology for providing explainability. Our evaluation specifically takes the inverse framing of other studies which ask users to positively identify the intruder rather than the cluster <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. We do this in order to independently measure the 'false negative' class of errors that are not possible to detect using pure intruder detection.</p><p>For our study, we generated 99 unique sets of neurons such as those in Fig. <ref type="figure">6</ref>, of which 43% were generated using NEUROCARTOGRAPHY, 43% were generated from hand picked clusters, and 14% were generated completely at random with no underlying cluster. These sets were used to populate 9 different questionnaires of 11 sets which Amazon Mechanical Turk workers located within the U.S. completed, receiving compensation of $1 per questionnaire and taking an average time of 7 minutes to complete. An average of 42 unique workers completed each questionnaire, for a total of 3374 unique human judgements of clusters from 244 unique workers overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Cluster Cohesion</head><p>The measure of cluster cohesion used in other intruder detection experiments is accuracy on the binary prediction task on whether a user correctly identified the intruder. In our study this is equivalent to the false positive rate for cluster inclusion predictions. For the random baseline, the false positive rate was found to be 30.6% ±3.1% (showing how easy it is for humans to find spurious patterns where none exist) while hand picked clusters had a false positive rate of 6.1% ± 1.2%, and NEUROCARTOGRAPHY generated clusters had a false positive rate of 11.8% ± 1.7%. With 95% confidence intervals, both clustering techniques significantly outperformed the baseline on the binary task.</p><p>However, in real applications, and in the context of this study, users have varying thresholds for how similar neurons must be in order to be included within the same cluster, even if they are using the same underlying similarity heuristic for their judgements. Because Fig. <ref type="figure">6</ref>. Example question from MTurk evaluation. Users were presented with six neuron patches and asked to determine if there is a coherent cluster and if so provide a short label. In this example of a NEUROCARTOGRAPHY generated cluster we can see which neuron is the out of cluster intruder, which neurons are in the cluster, which options the user selected, and the classification results of true positives for the neurons the user correctly selected, true negative for not selecting the intruder, and a false negative error for not selecting a neuron that is in the cluster.</p><p>our study design gave participants a choice for the number of options to select within a given set, each neuron's inclusion is essentially an independent judgement of whether it fits within the cluster (if the user determines a cluster exists). By getting enough of these independent measurements for each neuron we can use the proportion of users who choose to include a neuron within a cluster as a score of how likely users expect it to be included, or how cohesive the specific neuron is with the the context of the whole cluster. By using this score instead of binary accuracy, we can evaluate the full space of trade-offs between false negative and false positive errors using the receiver operative characteristic (ROC) which provides a fuller, threshold-independent picture of performance, offering a more robust variation of the specific experimental setup and balance of options and intruders <ref type="bibr" target="#b1">[2]</ref>.</p><p>The score used to define the ROC in Fig. <ref type="figure" target="#fig_3">5</ref> was calculated using the percentage of users including a given neuron within a cluster in the case that the user determines there is a cluster present in the set (that is they select 3, 4, or 5 neurons as opposed to 0). We used this method for both hand selected and NEUROCARTOGRAPHY generated clusters (and excluded the random baseline as it contains no true positive values). Taking the area under the curve (AUC) of the ROC for each clustering method (Fig. <ref type="figure" target="#fig_3">5</ref>) we find again that hand selected clusters again outperform NEUROCARTOGRAPHY, but that both perform substantially better than chance with AUC values of 0.97 ± 0.04 for hand created clusters and 0.91 ± 0.04 for NEUROCARTOGRAPHY. This result shows that NEUROCARTOGRAPHY produces clusters that are nearly as interpretable as hand crafted clusters across different inclusion thresholds, and are both much more reliably detected than chance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Label Cohesion</head><p>To further understand the consistency of the patterns agreed upon by users, we asked users to describe individual clusters they selected. Without a ground truth for cluster descriptions, we looked to statistically compare how different users labeled the same clusters in order to see the consistency of the discovered concept. To compare cluster level descriptions, we rely on sentence level embeddings from the Universal Sentence Encoder (USE) <ref type="bibr" target="#b5">[6]</ref>. USE works by projecting sentences into embeddings which can be compared (using cosine similarity) to identify the presence of similar ideas. USE similarity is preferred to word choice overlap metrics used in <ref type="bibr" target="#b16">[17]</ref>, since it captures semantic similarity of the actual concepts being discussed regardless of phrasing (for example matching labels describing the same set with "dots", "circles", and "round objects").</p><p>First, in order to build a baseline for semantic similarity values, we calculated the average pairwise similarity between all labels across different clusters throughout the dataset to be 0.301 ± .003. Then for each unique set, we found the average pairwise similarity between the labels from all of the users with labels for that specific set. These average within-group similarities for each class of cluster are 0.59 ± 0.05 for hand picked clusters and 0.51 ± 0.05 for NEUROCARTOGRAPHY generated clusters. Both of these results are significantly higher than the baseline, showing semantic consistency between how users understand the clusters that they detect. Complementary future evaluation may assess the degree to which the neuron groups are capturing redundant semantics (e.g., track accuracy changes as neurons are pruned).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">USAGE SCENARIOS 9.1 Automatically Discovering Backbone Concept Pathways for Related Classes</head><p>DNNs are known to learn general concepts: this generalizability is widely leveraged in transfer learning, model compression, and model robustness research <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>. However, it is challenging to automatically discover and interpret which key concepts are progressively combined or connected internally in a model, or how such "backbone" concept pathways may be shared across related classes. Recent research has proposed approaches to help users interpret how features may be connected <ref type="bibr" target="#b22">[23]</ref>, but such approaches are performed at the neuron level, limited to only analyzing the relationships of neurons across two adjacent layers, instead of across the whole network. The biggest drawback is the dependence on manual processes. NEUROCARTOGRAPHY's Concept Cascade can help automatically discover backbone concept pathways for related classes across all layers. Users can selectively activate a concept and examine the concept's cascade effect to interpret the successive concept initiations in layer layers, while identifying concepts' general relationships to related classes. For example, while inspecting the Maltese dog class, we found a cluster detecting "dog face" in mixed4c (Fig. <ref type="figure">7</ref>). Through Concept Cascade mode, we manually activate this concept to trigger and discover its related concepts in subsequent layers not only for the Maltese dog class, but also for other breeds of dogs such as Beagle and Appenzeller. Through these concept cascades, we visualize how concepts may evolve over the network, such as from the generic "dog face" concept to the specific "furry dog face" concept in later layers.</p><p>Backbone concept pathways can also be used to highlight learned generalize properties, like "curve detectors" (Fig. <ref type="figure">8</ref>). Existing work <ref type="bibr" target="#b3">[4]</ref> has observed that several neurons in the earlier layers detect curves of different orientations. Even though these detectors were discovered manually, their analysis yields interesting properties of the curve concepts, like how sensitive the curve detectors are to curvature and what orientations do they respond to. Using NEUROCARTOGRAPHY, we automatically discovered more curve detectors in InceptionV1. We selected a node in mixed3b 5x5 layer which detects a curve of a specific orientation, selectively activated in the Concept Cascade mode, and discovered the curve detectors across layers such as neurons shown in Fig. <ref type="figure">8</ref>. We also observed that those curve detectors are clustered in the preprocessing stage of our clustering algorithm, but not grouped by the main clustering stage. This is because the curve detectors are highly selective for orientations, causing highly activated regions of the activation maps are different by the detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Finding Isolated Concepts</head><p>While inspecting the Neuron Projection View, we noticed a small number of neurons are distinctively positioned very far away from all other neurons in the embedding space (Fig. <ref type="figure" target="#fig_7">9</ref>, right). Inspecting such isolated neurons reveals that they are detecting the "watermark" concept (see Concept Cascade discovers dogs' Backbone concept pathways Fig. <ref type="figure">7</ref>. Through Concept Cascade, users manually activate the "dog face" concept to discover its related concepts in subsequent layers, not only for the current "Maltese dog" class but also for other breeds of dogs. Concept Cascade helps users visualize how concepts may evolve over the network, such as from the more generic "dog face" concept to the more specific "furry dog face" concept in later layers. Concepts are manually labeled. Fig. <ref type="figure">8</ref>. Concept Cascade automatically discovers neurons that detects curve of specific orientations, which have been manually found in <ref type="bibr" target="#b3">[4]</ref>.</p><p>example from mixed5b-337 layer at Fig. <ref type="figure" target="#fig_7">9</ref>, top-left). Interestingly, as watermarks can appear on almost any kinds of images independent of the image content that the watermarks are placed above (e.g., copyright watermark can appear on an image of a car, a dog, or a pineapple), this means the neurons responsible for detecting watermarks would frequently co-activate with each other, but such "watermark neurons" co-activate relatively less so with the neurons detecting the concepts that describe the image content since watermarks are not associated with only some specific features. NEUROCARTOGRAPHY's neuron embedding algorithm is able to discover this interesting phenomenon about the watermark neurons, placing them close together to reflect the concept coherence for watermark, and away from other neurons to reflect the watermark's non-specificity for image content. NEUROCAR-TOGRAPHY allows us to easily verify our observations and conclusions. For example, selecting mixed5b-337, a watermark neuron (Fig. <ref type="figure" target="#fig_7">9</ref>, topleft), in the Neuron Projection View brings in its most related neurons in the Neuron Neighbor View (e.g.,mixed5b-86, mixed4c-342, mixed4e-296), which are all watermark neurons as well. These neurons are also clustered in the Graph View (e.g., in mixed5b layer, neurons #337, #113, #289, and #86 appear in the same neuron cluster).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION, LIMITATIONS AND FUTURE WORK</head><p>We have presented NEUROCARTOGRAPHY, an interactive system that scalably summarizes and visualizes concepts learned by DNNs via scalable concept summarization techniques for neuron clustering and neuron embedding. Through a large-scale human evaluation, we have demonstrated that our techniques discover neuron groups that represent coherent, human-meaningful concepts. Our system runs in modern browsers and is open-sourced. Below, we discuss limitations of our approach and future research directions for extending this investigation.</p><p>Further dissecting poly-semantic neurons. We believe our work has taken a major step in addressing the research challenging of automatically and scalably grouping neurons that detect the same concept, going beyond manual, neuron-level inspection (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref>) to provide a higher-level perspective for the knowledge learned by a network. Our work, however, is not designed for "dissecting" neurons that may become activated for multiple seemingly unrelated concepts, which has been observed in recent work, e.g, <ref type="bibr" target="#b40">[41]</ref>. For example, in InceptionV1, at least poly-semantic neuron that responds to cat faces, fronts of cars, and cat legs <ref type="bibr" target="#b40">[41]</ref>. NEUROCARTOGRAPHY cannot "split" this neuron into multiple neuron, each detecting one concept and put that newly created neuron into its logical neuron cluster with other similar neurons in the network. Tackling poly-semantic neurons is an exciting and challenging direction for future work.</p><p>Integrating NEUROCARTOGRAPHY into more applications. Currently, our work focuses on using NEUROCARTOGRAPHY to enhance interpretability of DNNs. As DNNs are increasingly used in an everincreasing variety of applications, our approaches can help practitioners and researchers assess the effectiveness of their ideas. For example, in the neural network compression community, several methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref> leverage potential neuron redundancies to generate compressed models while maintaining prediction accuracy. NEURO-CARTOGRAPHY can help researchers interpret the semantic similarity between the compressed model and the original, uncompressed models, which helps them assess if their techniques are indeed preserving the "gist" of the knowledge important for prediction, or if they are leveraging some other features of the data of the model. Currently, concepts need to be manually labeled; automatic labeling will increase the tool's usability. Also, current Neuron Projection View presents all neurons in the same plot even though some concepts' abstraction levels could be very different; our future work includes providing users with the ability to select layers that they want to investigate. We look forward to seeing the impact that NEUROCARTOGRAPHY may contribute, from assisting evaluation of existing techniques (e.g., model compression, adversarial attacks and defenses), to developing new ones.</p><p>Visualizing other neural network models. We have justified our model choice in Sect. 5; we are working to extend support to other CNN models. Our approach can easily be adapted to simpler models (e.g., VGG <ref type="bibr" target="#b50">[51]</ref>). For more complex networks (e.g., ResNets <ref type="bibr" target="#b19">[20]</ref>, small extensions would be needed to handle more types of connections present in the network (e.g., skip connections could be represented as skip-layer edges in the graph view).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. To summarize the concepts learned by a DNN, NEUROCARTOG-RAPHY groups neurons based on how similarly they are activated, e.g., by the "dog face" concept. Here, neurons 460 and 483 in layer mixed4c of InceptionV1 model are similarly activated by the "dog face" concept, and are grouped in the same cluster by our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 . 2 .</head><label>12</label><figDesc>Click the toggle button in the header to switch to the concept cascade mode Original graph of selected class (Maltese dog) Additional concepts triggered by the selectively activated node Click a node to initiate concept cascade</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Clicking a concept cluster node activates the concept, causing the neuron cluster to induce a concept cascade that triggers higher-level concepts across subsequent layers in the model. Left: in the concept cascade, some neuron clusters are strongly contributing to current class's prediction and are shown in the class's graph summary. Right: Concepts less related are shown on the right hand side.</figDesc><graphic coords="7,79.51,92.83,198.89,93.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. ROC Curve for user estimations of cluster inclusion. Both hand picked and NEUROCARTOGRAPHY generated clusters perform well overall, implying that the clusters generated are interpretable enough to be consistently recognised by different users.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>discovers "curve detectors" in a neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. NEUROCARTOGRAPHY reveals the interesting phenomenon about the isolated "watermark" concept (example image at top-left), that watermarks are not specific to any image features (i.e., can appear on almost any kinds of images), thus watermark neurons are placed far away from all other neurons due to relatively low co-activations (see Neuron Projection View on the right).</figDesc><graphic coords="9,454.18,70.73,55.30,62.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Hohman is currently at Apple. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org.</figDesc><table /><note>Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Hannah Kim, the Georgia Tech Visualization Lab, and the anonymous reviewers for their support and constructive feedback. This work was supported in part by DARPA (HR00112030001), NSF grants IIS-1563816, CNS-1704701, and gifts from Intel, NVIDIA, Google.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="30071" to="30078" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The use of the area under the roc curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0031-3203(96)00142-2</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the resemblance and containment of documents</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)</title>
				<meeting>Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curve circuits. Distill</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="e00024" to="26" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activation atlas</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e15</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guajardo-Céspedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Universal sentence encoder. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Near duplicate image detection: Min-hash and tf-idf weighting</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">810</biblScope>
			<biblScope unit="page" from="812" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Google news personalization: scalable online collaborative filtering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Massif: Interactive interpretation of adversarial attacks on deep learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Firstman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Firstman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02608</idno>
		<title level="m">Interactively deciphering adversarial attacks on deep neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">New types of deep neural network learning for speech recognition and related applications: An overview</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8599" to="8603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rest: Robust and efficient neural networks for sleep monitoring in the wild</title>
		<author>
			<persName><forename type="first">R</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
				<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1704" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08630</idno>
		<title level="m">Cup: Cluster pruning for compressing deep neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8730" to="8738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards automatic concept-based explanations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vldb</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics (TVCG)</title>
				<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">S ummit: Scaling deep learning interpretability by visualizing activation and attribution summarizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1096" to="1106" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mutantx-s: Scalable malware clustering based on static features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 {USENIX} Annual Technical Conference ({USENIX}{ATC} 13)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="187" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">t-sne visualization of cnn codes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="https://cs.stanford.edu/people/karpathy/cnnembed/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<title level="m">terpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2668" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cnnpruner: Pruning convolutional neural networks with visual analytics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing the noise robustness of deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1091</idno>
		<title level="m">Do convnets learn correspondence? arXiv preprint</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tyka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google Research Blog</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Visualization for Deep Learning, International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Audiovisual speech recognition using deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="737" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An overview of early vision in inceptionv1</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="e00024" to="26" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Zoom in: An introduction to circuits</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="e00024" to="25" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<title level="m">Feature visualization. Distill</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">e7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The building blocks of interpretability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mining of massive datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scalable and accurate deep learning with electronic health records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajkomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning for health informatics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ravì</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deligianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreu-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop at International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">ICLR Workshop</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Guilt by association: large scale malware detection by mining file-relation graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tamersoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1524" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03665</idno>
		<title level="m">Learning structured sparsity in deep neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A comparative analysis of industry human-ai interaction guidelines</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sperrle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Endert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04224</idno>
		<title level="m">On the dimensionality of word embedding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Object detection with deep learning: A review</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3212" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
