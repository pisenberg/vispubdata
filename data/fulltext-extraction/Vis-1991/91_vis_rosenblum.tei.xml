<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Acoustic Imaging: The Reconstruction of Underwater Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rosenblumt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Kamgar-Parsit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Belcher$</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Applied Physics Laboratory</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ola</forename><surname>Engelsen1</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">+Naval Research Laboratory</orgName>
								<address>
									<postCode>20375</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Acoustic Imaging: The Reconstruction of Underwater Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>While many sensors now produce a plethora of data, some still produce an insuficient number of samples. This is true for the marine scientific community, because the theory of wave propagation in water dictates the use of acoustic frequencies for underwater imaging. In this paper we discuss reconstruction of 30 scenes using data from an acoustic imaging sonar. Two methods f o r visualizing objects in an acoustic snapshot of !he ocean are discussed: mathematical morphology and a synthesis of 30 digital imaging with volume rendering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visualizing underwater scenes is a valuable tool for marine and naval research and operations. Scientific researchers use high-resolution imaging sonars and optical systems t o create "snapshots" of underwater scenes and objects. The selection of scanning frequency is a trade-off'between range and resolution. Optical systems have high resolution but have ranges of about 10 meters in good water conditions. Perhaps even more limiting, optical sensors fail at ranges of inches in highly turbid water, a common occurence in shallow water or in water disturbed by man.</p><p>Conventional sonars operate a t longer ranges. Low frequency sonars used for mapping survey wide swaths of ocean bottom, but can only resolve features larger than tens of meters. As the acoustic frequency rises, resolution improves and range decreases. One type of sonar, called side-scan because it looks out sideways, produces inches resolution on a 2-D projection but does not resolve height information. Stewart shows a particularly interesting example of a sidescan sonar image in examining a shipwreck (see figure 7 in <ref type="bibr" target="#b10">[9]</ref>). Conventional sonars also use extensive electron-ICS to perform "beamforming", that is to electronically sweep a narrow beam across a wide field of view. This is disadvantageous when the size and weight of the sensor is important.</p><p>To improve ocean acoustic imaging capabilities, a high-resolution (for the ocean), forward-looking sonar has been developed that perceives data in a manner similar to the human eye. This acoust,ic lens system eliminates the need for beamforming electronics. Digital imaging and computer graphics al-gorithms produce high-resolution, three-dimensional acoustic snapshots of the surrounding environment. The acoustic lens is also valuable as an obstacle avoidance sensor and for bottom mapping.</p><p>Compared t o T V images the lens data is sparse and its resolution is low. Neither does the lens generally sample the scene with uniform resolution. Thus, data visualization requires that we develop algorithms to effectively grid and filter the data and to render the data as best possible. This paper describes the acoustic lens, discusses the issues concerning underwater 3D scene reconstruction from the lens data, and applies mathematical morphology and volume rendering techniques to visualize objects in the the reconstructed 3D scenes. T h e examples demonstrate the quality of images that can currently be obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Acoustic Lens</head><p>Traditional sonar systems use mechanical or electrical beamforming techniques to scan a highly directional beam over a field of view. Acoustic lens technology, on the other hand, places a large number of highly directional acoustic transducers on a grid attached to a hemispherical shell to form a retina. A thin, spherical lens cover is filled with a specially chosen fluid that focuses incoming acoustic waves onto the retina (figure 1). Transmission time delays determine range, while transducer position yields bearing and elevation coordinates. Lens technology [2] is not new, but recent advances in digital electronics now permit dealing with an large volume of data in realtime.</p><p>The retina is populated with transducers in rows. The transducers in each row are separated and are staggered to produce higher resolution beams in both azimuth and elevation. A hemispherical shell holds the focusing fluid and focuses the incoming acoustic waves. A high frequency acoustic lens was fabricated for acoustic imaging experiments. This lens has a field of view of 48 degrees in azimuth and 12 degrees in elevation. The retina mounts on a track that permits shifting the field of view in elevation for performing different missions. Each beam has a conical beam pattern for transmit and receive. The range bins, defined by the received time delays, are 10 cm for ranges up to about 100 m. The sensor is designed to focus in an ambient temperature range that spans 13 degrees Celsius without having to change the lens fluid. Data is both recorded within the lens unit and sent topside to a PC-based, real-time graphical display.</p><p>A sonar beam is backscattered whenever it crosses a surface separating two media with different indices of refraction. For example if a metallic object underwater is illuminated by a sonar, the beam is backscattered both as it enters and exits the object. The backscatter from the front surface is obviously stronger, but the return signal from the rear surface ca.n also be significant and contain valuable information. The acoustic lens senses and .retains both reflections. This is unlike threshold sonars often used for mapping and robot navigation, where the information after the first strong return is discarded <ref type="bibr">[l]</ref>. The acoustic lens also uniquely localizes the backscatter signal, unlike 3D interferometric sonars which may yield ambiguous elevations <ref type="bibr">[lo]</ref> that must be resolved by other sources of information. These advantages also help make the lens a unique sonar particularly suited for viewing 3D underwater scenes and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Volumetric Scene Reconstruction</head><p>For imaging a scene the acoustic lens typically makes one or several passes over the scene, continually pinging and collecting backscatter signals. The issue of navigational error and misregistration of passes is a difficult problem in its own [4], which we will not discuss here. In these examples the exact position and orientation of the lens are (assumed) known a t all times with respect to a global coordinate system fixed to the scene.</p><p>For scene reconstruction we partition space into a 3D matrix of cubic voxels. This 3D voxel-based volumetric representation of the environment is well suited for manipulations by most graphics algorithms and specialized architectures <ref type="bibr">[6]</ref>. We wish to estimate the return signal strength from within each voxel. High voxel values indicate the presence of objects or interfaces in those locations.</p><p>The data collected by the lens is the backscatter signal strength from each conical section of a transducer field of view. To determine the location of the conical section in the global (or scene) coordinate system, first we transform this range data t o a Cartesian coordinate system attached t o the lens according to:</p><formula xml:id="formula_0">x = r s i n d c o s d y = r s i n d s i n d t = r c o s d</formula><p>where (x, y, z ) are the Cartesian coordinates and <ref type="figure">( r , d , d )</ref> are the range, elevation angle, and azimuth of the conical section in the lens frame. 0 and q5 are known from the row and column of the transducer on the lens retina. Then we transform (t, y, t) to the global Cartesian coordinates ( X , Y , Z ) ,</p><formula xml:id="formula_1">x = xo + 112 + lay + 132 Y = YO + m l x + mzy + m3z z = 2 0 + n1x + nzy + n3.z Here ( X o , Y o , Z o )</formula><p>are the global coordinates of the origin of the lens-based system, and <ref type="bibr">( l i , mi, ni)</ref> are the direction cosines between the axes of the lens frame and the global frame. Direction cosines are usually expressed in terms of pan, tilt, and swin angles which are more readily available, e.g. see [$ for the formulas.</p><p>We can thus map the centers of all conical range bins onto the cubic grid. The backscatter energy received in a range bin is the sum of all returns from everywhere in that bin. Since a conical slice typically overlaps with several cubic voxels, we must decide on how t o apportion the backscatter energy among the voxels. In the absence of other information, it is reasonable to assume that the backscatter signal coming from a cubic voxel is proportional t o its volume overlap with the conical slice. However, calculating the volume overlaps is computationally expensive, and, in practice, not much different from other commonly used gridding schemes. The results we show in the next section are obtained using the following scheme. Consider the grid made up of voxel centers; this is the dual of the original voxel grid, shifted half a voxel in each direction. We find the cubic cell in the dual grid within which the center of conical range bin LY falls. The vertices of this cubic cell are the centers of the eight voxels that surround this range bin. We distribute the backscatter energy, f a , from range bin (Y among the eight surrounding voxels according to how close they are to the center of the range bin. T h a t is, the share of this backscatter energy for voxel i, Siar is where d,, is the distance of the center of range bin cy to the center of voxel i , and the sum, in the normalization factor, is over the eight surrounding voxels.</p><p>Each voxel typically receives contributions from several range bins, and the question is how to estimate the backscatter energy gi from voxel i from the set of returns {si,}. We have experimented with several schemes, two of which will be discussed in detail in this work. The first is to treat all gridded backscatter values gia identically and take their unbiased average where ni is the number of hits received by voxel i . The advantage of this scheme is that noise is usually reduced, but a t the same time genuinely strong returns may be blurred. Strength of acoustic backscatter depends critically on the angle of incidence; if it is normal, the return is strongest and grows weaker as the angle approaches the surface tangent. Since several different beams may illuminate the same surface patch from entirely different angles their corresponding returns may be completely different. In such cases averaging may yield a low estimate which does not really indicate the presence of a hard surface patch at that location. Also a similar situation may arise when a surface patch may be shadowed by an object in one pass and directly illuminated in another pass. In the second scheme, in order to deal with these difficulties, we take as the estimate for gi the largest return, i.e. g, = Maxa {sia}.</p><p>In this scheme sources of error can obviously have a more pronounced effect. In the examples we give in the next section both schemes yield very similar results, possibly because the scene is reconstructed from a single pass and the objects in the scene are well separated from each other so that they do not cast shadows on one another. The figures presented are made with the averaging scheme.</p><p>"What size voxel?" is also an important question. The acoustic lens, because of its conical shape beams, produces data with varying spatial resolutions. In the near range it has the highest resolution, but as we go farther the volume of conical slices increase with range-or distance-squared, reducing the resolution; note that the conical slices have equal heights (about 10 cm due to the sampling rate) while the cross-sectional area increases with distance-squared.</p><p>A second factor complicating the situation is that in a typical scenario the lens is moving, hence it may sample the same part of the scene at different resolutions in different passes. Since the scene cannot be sampled with uniform resolution, one may have to choose different voxel sizes for reconstructing different regions. Common sense and experience guide us to choose voxels with volumes comparable to the volume of conical range bins, and that the number of voxels spanning a given region should be comparable to the number of data points within that region. Prior to volume rendering two filtering operations are performed to smooth space. First all voxels with intensity values above a threshold are retained. Since the received backscatter intensity values far exceed the sensor/ocean noise levels at these ranges, this operation simply assures viewing objects. Second, there are some voxels belonging to objects that have intensity values lower than the threshold; usually because either they are not directly illuminated or the angle of incidence is high. Yet these voxels contain relevant and useful information and we wish to keep them. To do this, each voxel below the threshold is tested; it is kept only if at least n of its neighbors exceed the threshold, where a neighbor is any voxel containing at least a point in common. The choice of n depends on the object size we are looking for; it is bigger for larger objects. In the examples given in the next section the results with n = 2 , 3 , 4 are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Volume Rendering</head><p>In this section we present results of controlled experiments performed with the acoustic lens that show its capabilities for underwater imaging. We are particularly interested in finding how closely the reconstructed objects resemble the originals in size and shape. For this purpose first we reconstruct the 3D scene from lens data and isolate objects of potential interest. Then the voxel data is operated on with volume visualization algorithms [5] to recreate and display the objects.</p><p>In these examples the lens is mounted on a carriage in a tow tank and moves on a straight line at constant speed. This set-up removes complications arising from navigational uncertainty. Objects are placed on or near the tank bottom and the lens passes over them. The bottom is made of concrete, and the backscatter from it is so strong that we are able to a  deduce its position accurately. Since our purpose is to find out how objects appear to the acoustic lens, we have taken out the bottom in the reconstructed scene and replaced it with a constant intensity plane. In both examples below, the scenes are reconstructed from single passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spheres</head><p>In this experiment five metallic spheres are suspended a t a height of about 1 m above the bottom of the tow tank. <ref type="figure" target="#fig_1">Figure 2</ref> shows the experiment, with the tow carriage taking the lens over the spheres at a known rate of speed. In figure 3 we show the wire-frame image of the thresholded scene. The nearbottom clutter is seen to be well below the spheres. The clutter is possibly due to secondary reflections of acoustic beams between the bottom and the spheres, which would result in appearance of highly distorted false objects at certain locations. <ref type="figure" target="#fig_3">Figure 4</ref> Panel A is the view from the top; panel B is the same scene viewed from the side. The scene, a 40 x 40 x 40 grid, is reconstructed from a single pass. For recreating the objects we have filtered the scene and then used the V-Buffer volume rendering algorithm <ref type="bibr">[ll]</ref>. We view only a portion of the scene that is in the region of the objects of interest to avoid ray-tracing through empty voxels. The five spheres stand out clearly in the side view of the scene. The plane representing the tank bottom is included in the display to give us the correct sense of orientation.</p><p>In <ref type="figure" target="#fig_4">figure 5</ref> we show a 3D perspective of the point clouds (points with backscatter strength above a threshold) of the same scene. This is a traditional way of representing 3D data, from which one can deduce the presence of an object (clustered points) and its size, but little about the shape. The wealth of visual information contained in the volume rendered scene as compared to the point cloud figure is striking.</p><p>The lens passes over the spheres; its path in <ref type="figure" target="#fig_3">figure   4</ref> panel B is from right to left, and in panel A from bottom to top slightly to the right of spheres. Note the shading on spheres, where the side facing the lens' path appears brighter. Also note that the back of spheres in panel B are clearly visible even though they are not directly illuminated by lens beams. The three smaller spheres lined up parallel to the lens' path are 25 cm in diameter; the other two spheres have a diameter of 45 cm. The recreated scene, in spite of flawed and distorted appearance of some of the spheres, is remarkably good for several reasons: the objects stand out clearly; they are a t the correct locations in the scene; the lens' track can be inferred from shading in images (it will be interesting to see how informative shading is in a scene reconstructed from multiple passes); and even the sizes of objects are recovered correctly, within voxel resolution (voxel size is 10 cm).</p><p>Figures 6a and 6b are the left and right snapshots of a stereo pair. These figures were produced at a different view angle from figure 4, and using a different reconstruction algorithm. The scene reconstruction algorithm used for preparing figure 6, is a faster algorithm where the backscatter energy is assigned only to the voxel within which the center of a range bin falls. Resulting images are not as smooth as those prepared by the algorithm described in the previous section, where backscat.ter energy is distrihut,ed among surrounding voxels. Hence, the elongat,ed appearance of some spheres in figure 6 compared to figure 4. <ref type="figure" target="#fig_6">Figure 7</ref> shows the same scene as in <ref type="figure" target="#fig_5">figure 6</ref> except that the trilinear interpolation feature of the V-Buffer algorithm was not used. This provides a visual measure of the smoothing obtained by trilinear interpolation. <ref type="figure" target="#fig_7">Figure 8</ref> shows the result of performing color lookup table manipulations on the view shown in figure 7 to highlight the strong point,s on the return. Because the angle of incidence will be similar for adjoining rays that strike a flat surface, but quite different for a small sphere, we conjecture that this type of display may be useful t,o help determine shape.</p><p>As to the question "Do these objects in the acoustic snapshot look like spheres?", the answer is "Yes!". However, t o the question "Can we say with absolute certainty that they are spheres and not cubes, or other shapes?", we must answer "No. Not at this resolution." Even if the image were perfectly noiseless and free of distortions due to secondary cchoes, the object size compared to t8he voxel size is not large enough to yield sufficient shape information for only a single pass over the object. The diameter of spheres spans only 3 to 5 voxels, resulting in a fairly coarse digitization. Indeed, cubes when imaged with a sensor with a finite size beamwidth will appear somewhat spherical with rounded vertices and edges. To enhance the image resolution one has to decrease the voxel size. This can be achieved in two ways: (i) directly by a higher resolution lens; (ii) indirectly by sensing the scene more densely, i.e. combining and registering data from several passes. Both of these possibilities for higher resolution acoustic imaging are being explored.</p><p>Even a t these resolutions one may be able to infer some surface shape information by performing color look-up table manipulations t o produce an intensity map. For example, a sphere is expected to produce a small localized high intensity peak indicating a convex curved surface, because at least one beam is likely to illuminate the sphere a t normal angle of incidence resulting in strong backscatter, while adjacent beams will have greater incidence angle and lower backscatter int,ensit,y. On the other hand a cube should produce large patches of constant intensity indicating flat surfaces, since adjacent beams illuminating the same face of the cube will have the same incidence angle and hence the same backscatter intensity. We must also remark that since voxels are fairly large here, the reconstructed scene is sensitive to where the voxel grid is placed. For example, if we shift the positioning of the grid half a voxel, the snapshots will change somewhat. Finally, there are also waveform analysis techniques to help distinguish shape <ref type="bibr" target="#b8">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Remotely Operated Vehicle</head><p>In this experiment a 1.5-m-long remotely operated vehicle (ROV), used in nnderwater explorations, is resting on the bottom of the tow tank. A picture of the ROV is shown in <ref type="figure">figure 9</ref>. In figure 10 we show an acoustic snapshot of the ROV, reproduced from a single pass over the ROV. The ROV has been placed on the bottom of the tow tank in a rotated position from figure 10. It has been rotated about 30 degrees counterclockwise as viewed from above, so that the spherical head lies on the right and 30 degrees behind the cross-track direction. The scene is processed similarly to the data of the previous section, producing in this case a 60-cubed voxel set. The acoustic recreation of the ROV has the correct size, dimensions, and orientation with respect to the tow tank. One can also recognize the head and the body of the ROV in the volume rendered image. Finer features with sizes less than or comparable t,o a voxel dimension 15 cm) are of course not discernible because of Another valuable tool for visual inspection of a recorist,ructed scene is viewing orthogonal slices, in which planar volume slices perpendicular to a fixed axis are displayed. We have found that examining the unfiltered scene with orthogonal slices is particularly helpful in understanding volume rendering results. insu f b icient resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Mathematical Morphology</head><p>Mathernatical morphology is a set-theoretical method for image analysis. I t can quantify many aspects of geometrical st,ruct,iires in a multi-dimensional signal that agrees with human intuition and perception. In essence, mathcmatical morphology is a shape-based filt,ering that extracts desired features i n images. Recause of this built-in abi1it.y to represent and extract shape, it has been a valuable tool in many computer vision applications, especially in the area of  <ref type="bibr">, pegs 414.)</ref> automated visual inspection. This approach has also been widely used in many other applications, such as biomedical and electron microscope image analysis. In this section we describe our efforts toward automatic analysis of underwater scenes reconstructed from the lens data using morphological operations. Below we give a brief introduction of the basic ideas. See [3] for a tutorial review.</p><p>Morphological filters are nonlinear operators that locally modify the geometrical features of multidimensional signals. There are only two fundamental operations from which one can construct various filters for edge smoothing, edge detection, peak finding, etc. These two operations are called dilution and erosion. First we transform the scene into a binary three dimensional grid. Since the scene voxels have gray scale values, they have to be thresholded (usually at several different values so that we have several different binary set representations of the scene). Then we define a structuring element, which is a compact set of small size and simple shape. T h e structuring element is designed by the user to detect features of interest.</p><p>The two fundamental operations can now be defined. The dilation operation is: place the center of the structuring element on each full voxel (with value l ) , then fill all voxels covered by the structuring element. The erosion operation is: place the center of the structuring element on each full voxel; if the voxel is surrounded by other full voxels such that the structuring element can be completely contained then retain the voxel, otherwise set the voxel value to zero. An edge smoothing filter, for example, is then ob- <ref type="figure">Figure 9</ref>: Photograph of the 1.5-m-long ROV.</p><p>tained by first dilating the scene and then eroding the result, with a suitably designed structuring element; or an edge detection filter is obtained by differencing the dilated and eroded versions of the image.</p><p>To find spherical objects of a given size in the scene, one can define a variety of structuring elements. The simplest structuring element is a short vector (and its rotations in several directions). In <ref type="figure">figure</ref> 11 we show the result of dilating the scene shown in figure 3 by such a structuring elementa horizontal short vector in the across track direction. The near-bottom clutter is not shown in this figure. For detecting more complex objects like the ROV one has to define other structuring elements that detect various features and parts. This work is currently in progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Underwater imaging is an important task made difficult by the often inhospitable medium. The acoustic lens represents an important advance in imaging sonar technology, which is especially suitable for imaging underwater objects. Even though the sensor data is sparse and low resolution, compared to T V , 3D scene reconstruction algorithms together with volume visualization techniques recreate the scenes and objects with hitherto unseen clarity. Current lens resolution is limited by its heamwidth, so that recognizing smaller objects or objects with fine features is difficult in some situations. One way to overcome this problem is to have more elaborate sensing strategies by making multiple passes and combining the information. Pass registration algorithms for the lens data, needed for accurate scene reconstruction, are being developed. But we believe the best way to achieve higher resolution images is a higher resolution sensor wit,li a radically new retina methodology, the so-called continuous retina, where the beamwidths are narrower by an order of magnitude.</p><p>We are also pursuing the mathematical morphology approach for automated object recognition. Preliminary results are promising. These shape-based <ref type="figure" target="#fig_0">Figure 10</ref>: A view of the volume rendered scene with t,he ROV a t the bottom of a tow tank. The ROV is seen in orange (color). (*e colorplate, page <ref type="bibr">414.)</ref> filtering operations may also be useful in cleaning the reconstructed scene and enhancing objects before volume rendering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Lens focusing diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sketch of an experiment in the tow tank</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>This wire-frame figure shows the height of voxels with values above a threshold. The spheres are readily separated from the near-bottom clutter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Two views, from the top (a) and from the side (b), of a volume rendered scene with five spheres suspended above the bottom of a test tank (color). (see color plate, page 414.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>A 3D perspective view of point clouds of the scene shown in figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Stereo pairs of spheres (color). (See colorplate, page 414.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>The scene shown in figure 6 without trilinear interpolation (color). (see colorplate, page474.J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>This is the same view shown in figure 7, after performing color lookup table manipulations (color). (see color plate</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Naval Technology and the Office of Naval Research.</p><p>This work has been sponsorrd by the Office of</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sonar-based real-world mapping and navigat,ion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elfes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automat,ion</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Underwater Acoustic and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Folds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NATO Advanced Study Institute</title>
		<meeting>NATO Advanced Study Institute<address><addrLine>Copenhagen; Boston</addrLine></address></meeting>
		<imprint>
			<publisher>D . Reidel Publishing Company</publisher>
			<date type="published" when="1980-08" />
			<biblScope unit="page" from="263" to="279" />
		</imprint>
	</monogr>
	<note>Status of ultrasonic lens development</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image analysis using mat hematical morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Iiaralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>St'ernberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Inklligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="523" to="550" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kamgar-Parsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Itosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pipitone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">T o w a r d an automated system for a correctly regiskred bathymetric chart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arid</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Oceanic Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="314" to="325" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>'oluiri E C'zsunhzafion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Computer Society Press</publisher>
			<pubPlace>Los Alaniitos, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memory and processing architccturc for 3D voxel-based imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufnian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dakalash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Result of a morphological operation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>dila-</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generation and exploitation of plate waves in submerged, air-filled shells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Numrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dragonette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Fluid-Structure Interaction</title>
		<editor>G.C. Everstine and M.K. Au-Yang</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ASME</publisher>
			<date type="published" when="1984" />
			<biblScope unit="page" from="59" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Building an environment model using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Roth-Tabak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-06" />
			<publisher>COM-PUTER</publisher>
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multisensor visualization for underwater archaeology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="13" to="18" />
			<date type="published" when="1991-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Three-dimensional modeling of seafloor backscatter from sidescan sonar for autonomous classification and navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Stewart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990-01" />
			<pubPlace>Woods Hole, MA</pubPlace>
		</imprint>
	</monogr>
	<note>Woods Hole Oceanographic Institution Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">V-buffer: Visible Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Upson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 88</title>
		<meeting>SIGGRAPH 88</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
