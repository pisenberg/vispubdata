<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Shall We Connect Our Software Tools?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Grosse</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AT&amp;T Bell Laboratories Murray Hill</orgName>
								<address>
									<postCode>07974</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How Shall We Connect Our Software Tools?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T20:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Traditionally we connect our software tools using human-readable files. This is a conscious decision t o buy flexibility and understandability at some cost in performance relative t o binary file formats. This note explores the possibility of using shared memory functions t o retain most of the existing style while leapfrogging the speed of reading binaryfiles, at least in some environments and f o r some applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>most common case is that there is a single reader that needs the entire array, but the more general case is also important. For example, a graphics program may only need coefficients on the external faces of a domain, but as the user interactively slices the domain, the graphics needs rapid access to new parts of the array. Or imagine a kanji translation program that needs fast, scattered access to a dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Shared memory as a candidate solution 1 The performance issue</head><p>Imagine you are a scientist running a threedimensional differential equation simulation and needing to look at the solution at various time steps. Either you have a huge monolithic program to build, or else you have a huge amount of data to transfer between two programs. The latter alternative is generally regarded as superior, and there will usually be so much computation going on in both phases that the cost of even a slow Fortran formatted input/output (i/o) library is not prohibitive. But as the overall task is broken into a pipeline of more and more processes, either for software engineering or parallel processing reasons, i/o costs can become significant.</p><p>Experience building and using a collection of visualization tools[2, 31 which tend to do a modest amount of arithmetic on large arrays has been the immediate cause for examining this topic, though it is clearly a general software issue. Profiling reveals that as much as half the CPU time goes to i/o. Converting to binary i/o is undesirable, because past experience has taught that binary formats tend to become indecipherable after a few years and represent an immediate hurdle when trying to collaborate with others. Is there another way?</p><p>Consider the problem of a writer process which has generated a large array and several reader processes that each need to access some parts of the array. The Many UNIX systems provide the ability to create segments of shared memory that may be passed between processes. These segments act somewhat like temporary files, in that they continue to exist even after the process that created them dies.</p><p>The particular functions that do this (called shmget and shmat) have calling sequences specified by the System V Interface Definition (SVID) <ref type="bibr">[l]</ref>. Hence programs calling shmget are portable to many machines (e.g. then we may adopt the convention that the coefficient array may be replaced by a pointer to a shared memory segment. The file npts 10 coeff shmid 402 simdat-3/23-seriesb continues to pass through pipelines and be saved to disk just like the old one; the only change is that the library routine for reading coeff must now check if the first coefficient is shmid and if so replace malloc and read calls by shm-open. This does mean the simulation program will need a flag to specify when to use shared memory, since we certainly want to retain the default of simple ASCII, i.e. printable, output files. But the complication is restricted to a small part of the program and requires little extra effort on the part of the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGI, Stardent</head><p>Filters should always be provided to expand the shmid fields, for the benefit of programs (and people!) that do not care to deal with specialized formats. The philosophy is that the master copy is ASCII, which is the only format that should be archived. Since shared memory segments are so transitory, internal data structures may safely be changed without risk of making old files unreadable.</p><p>There is one mundane administrative issue with direct use of shmget, which returns simple integer labels. If, say, a dozen segments are created in the course of a simulation run and several simulations are active, it may not be easy to keep track of all these integers and clean up properly. Hence the convention of starting each shared memory segment with 128 bytes containing the integer as part of a descriptive character string. A tiny program that prints the string, user id, creation and access times (analogous to Is) and the system-provided command ipcrm -m label (analogous to rm) can together reduce the administrative burden to something like cleaning up the /tmp disk directory.</p><p>General use of shared memory would require careful synchronization to avoid timing bugs and deadlock. For the limited objective of accelerating i/o in pipelines, no such issues arise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmarking</head><p>Straight binary i/o (using the read and write UNIX system calls) is a plausible alternative to shared memory. To investigate the relative performance, a simple experiment was performed involving two processes Five mechanisms for passing the array were tried. First, shmget was used to create memory for the array; the writer process (a) exited before the reader (b) began. The shared memory primitive mmap, which is provided by some operating systems instead of or in addition to shmget and which involves explicit mapping of memory to disk files, was also timed. In the third and fourth cases, space for the array was created by calling malloc in both processes and then using write and (multiple) reads. For comparison, an ASCII pipeline was also timed, as was a program that merely called malloc and set the array, but did not write it out. Wall clock times given in <ref type="table">Table 1</ref> are for the best of five runs; there was not much variation over the repet it ions.</p><p>As expected, binary is much faster than ASCII. The fact that intermediate files and pipelines run at essentially the same speed shows that the operating systems' disk buffer cache in main memory is doing its job. In this particular implementation, anyway, shmget beats mmap; since the operating system is making less promise of permanence, this is not unreasonable.</p><p>One conclusion stands out most clearly. The speed of the reader under shmget, which would be the interactive graphics process under the scenario in $1, is (a) writer (b) reader total stunning. Reading the data this way is a factor of three faster than just allocating space in the normal fashion! The tests were run on an SGI 4D/240 with 128MB of main memory, running IRIX 4D1-3.3.2; process wall clock times were measured by gettimeofday. Changing between SMD and SCSI disks for the temporary file made no discernible difference; this was no surprise since the system had plenty of main memory for its disk buffer cache. Whether the programs were loaded with -1malloc or not also made no difference. Similar times for binary i/o were observed on a MIPS M2000, but on our Vax 8550 running Version 10 Research Unix, the individual times for the pipeline are 3 to 4 times faster than using intermediate files. On a i3SG-based machine running SystemV, the total times for 20,000 doubles were 8.0, NA, 8. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Practical trial</head><p>Encouraged by the results of the previous section that meaningful speedups are possible on at least soiiie important classes of machines, the approach was tested on a real application, the tensor-spline toolbox <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">31</ref>. Before the current project was conceived, 35 commands had already been implemented and editing the source for each one of them would have been intolerable. Fortunately, the entire conversion only required adding a dozen lines of code to two i/o functions and relinking.</p><p>The results were satisfying. On a typical 503 array, a trivial range computation dropped from 9 seconds elapsed time to less than a second. A graphicsintensive command improved from 17 seconds elapsed time to 6 seconds. Obviously, if we had been using binary i/o previously the improvement would have been less dramatic.</p><p>An implementation of the shmxreate interface in terms of the vendor-supplied shmget was easy; the code is displayed in <ref type="figure">Figure 1</ref> and is available by sending the message send svid from research/shm by electronic mail to net1ibQresearch.att. com or uunet !research ! netlib. Implementations for other systems would be welcome; for portability, a POSIX version based on binary disk files is planned.</p><p>The code is straightforward, though cluttered a little by error checking. The newline added to the label is perhaps obscure; in systems where shared memory segments are in the ordinary file system, that simplifies displaying the label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Caveats</head><p>I wish to emphasize that I whole-heartedly endorse the long UNIX tradition of ASCII intermediate files.</p><p>Their performance penalty is amply compensated by their self-documenting nature and the ease of connecting programs not deliberately designed to work together. In discussions on file exchange formats in the visualization community, I've fought for the ASCII camp against the binary camp, noting that one user of our toolbox even converted his binary, delta-encoded files into an ASCII format, ran compress, and wound up with a smaller file than the original. Netnews contains numerous queries from users of binary systems, asking how to convert their data; for ASCII systems, the solution is usually an obvious editor script.</p><p>The RenderMan Interface Bytestream <ref type="bibr" target="#b3">[4]</ref> (RIB) adopts the useful approach of defining both an ASCII and a fully equivalent binary form, so that adherents of both approaches are satisfied. With i/o libraries that read either form and stand-alone filters that provide easy conversion, such a scheme meets diverse needs. (Ironically, in my experience a compressed ASCII RIB file is often smaller than the corresponding compressed binary RIB file.) The point being made here, however, is that if the only reason to give up ASCII is raw performance, one might wish to switch to shared memory rather than binary.</p><p>Although many commercial UNIX systems provide these shared memory functions, it should be noted that the SVID <ref type="bibr">[l]</ref> states that these are optional and "may not be present in all implementations of the</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) setting and writing and (b) reading and summing 200,000 doubles. The size of the transfer is typical of current scientific computation, though use of only one floating point operation per array element (to emphasize i/o cost) is not! To bias the case in favor of binary i/o, this test only considers the case of reading a large contiguous array, not random access to a subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>approach. On a Cray, it might be ap- propriate to use the Solid State Disk; on distributed memory machines, explicit message-passing may be needed. The following three C functions are there- fore proposed, intended to capture the few features of shared memory that we need while allowing efficient alternative implementations. The writer process calls char* shm-create(char*name,O666,int nbytes); here name is</head><label></label><figDesc>a descriptive string; 0666 can be replaced by an arbitrary protection mode as for files; nbytes is the amount of space to be written. The return value, shm, is a pointer to an area of 128+nbytes, of which the first 128 contains a string composed of name and implementation-dependent information to yield a unique label L for the shared memory segment. After writing what it wishes in the remaining nbytes, the writer process calls</figDesc><table><row><cell>, Sun) of interest to the visualization community. Some machine architectures, however, re-quire a different additional diagnostic If our simulation and graphics programs ordinarily information. communicate by passing a file of the form npts 10 coeff 0 1 2 3 4 5 6 7 8 9</cell></row></table><note>int shm-close(char* shm) or may depend on the default closing of all segments upon process exit. The reader process gets L from the writer by some other channel, then calls char* shm-open(char* L)to get a shm pointer like before. When done the reader calls shm-close or exits. The design is modeled after that for handling UNIX files. Both shm-create and shm-open return a null pointer if they encounter an error; shm-close returns a non-zero on error. The library call perror may provide</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, 11.6, 19.8, 2.9.</figDesc><table><row><cell cols="3">shmget mmap alb</cell><cell cols="3">a&gt;x;b&lt;x ascii malloc</cell></row><row><cell>0.20</cell><cell cols="3">0.43 0.73 0.43</cell><cell>10.5</cell><cell>0.20</cell></row><row><cell>0.07</cell><cell>0.12</cell><cell cols="2">0.80 0.36</cell><cell>10.7</cell></row><row><cell>.3</cell><cell>.7</cell><cell>.8</cell><cell>.9</cell><cell>10.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">CH3046-0/91/0000/0292/$01 .OO Q 1991 IEEE</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>#include &lt;stdio.h&gt; #include &lt;sys/types.h&gt; #include &lt;sys/ipc.h&gt; #include &lt;sys/shm.h&gt; #define BAD (char *) <ref type="bibr">(-I)</ref> char* shm-create(char* name, mode-t mode, int nbytes) char* shm = 0; shmid = shmget(1PC-PRIVATE, 128tnbytes, modelIPC-CREAT); if (shmid &gt;= 0) shm = (char *)shmat(shmid, 0, 0); if (shm == BAD) fprintf (stderr,"couldn't attach %d\n",shmid) ; shm = 0;</p><p>} else sprintf (shm, "%d %. Kernel Extension.'' Also, as noted above for pipeline speeds, your own mileage may vary. There is certainly a place for binary i/o in applications needing high-speed, large-volume communication across networks. But for most purposes, humanreadable formats are the way to go, with optional use of shared memory for transferring selected large chunks of data.</p><p>I thank Bill Coughran, David Gay, Cleve Moler, Dennis Ritchie, Tom Szymanski, and especially Brian Kernighan for illuminating remarks. RenderMan is a registered trademark of Pixar. UNIX is a registered trademark of UNIX System Laboratories, Inc. VAX is a trademark of Digital Equipment Corporation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>At&amp;t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>System</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A philosophy for scientific computing tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Coughran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename><forename type="middle">E</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGNUM Newsletter</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="2" to="9" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Techniques f o r scientific animation</title>
	</analytic>
	<monogr>
		<title level="m">SPIE Proceedings</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The RenderMan Interface, Version 3.1, 1989. 3240 Kerner Blvd</title>
		<imprint>
			<pubPlace>San Rafael CA 94901</pubPlace>
		</imprint>
		<respStmt>
			<orgName>PIXAR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
