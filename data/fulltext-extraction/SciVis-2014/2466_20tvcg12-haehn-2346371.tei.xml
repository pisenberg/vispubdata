<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Design and Evaluation of Interactive Proofreading Tools for Connectomics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Haehn</surname></persName>
							<email>haehn@seas.harvard.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seymour</forename><surname>Knowles-Barley</surname></persName>
							<email>seymourkb@seas.harvard.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Roberts</surname></persName>
							<email>mlrobert@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Beyer</surname></persName>
							<email>jbeyer@seas.harvard.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Kasthuri</surname></persName>
							<email>bobby.kasthuri@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
							<email>pfister@seas.harvard.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Brain Science at</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Computer Graphics Laboratory at Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Design and Evaluation of Interactive Proofreading Tools for Connectomics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2014.2346371</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Proofreading</term>
					<term>Segmentation</term>
					<term>Connectomics</term>
					<term>Quantitative Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1: Proofreading with Dojo. We present a web-based application for interactive proofreading of automatic segmentations of connectome data acquired via electron microscopy. Split, merge and adjust functionality enables multiple users to correct the labeling of neurons in a collaborative fashion. Color-coded structures can be explored in 2D and 3D.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In computer vision, image segmentation is the process of partitioning an image into several sub-regions or segments, where individual segments correspond to distinct areas or objects in the image. Automatic segmentation approaches eliminate user interaction, which is often the bottleneck of manual and semi-automatic segmentation techniques. However, automatic approaches are computationally expensive and need to be targeted towards very specific segmentation problems and data sets to achieve high-quality results. Furthermore, even optimized automatic segmentation algorithms usually exhibit higher error rates and are less accurate than manual expert segmentations. Manual pixel labeling, on the other hand, requires users to have domain-specific knowledge and is usually a tedious and time-consuming process. A powerful alternative to fully automatic or manual approaches are interactive semi-automatic techniques. They usually rely on minimal user input to achieve an initial result and allow users to further improve the segmentation by manual adjustments. Proofreading refers to the manual and semi-automatic correction of automatic segmentations as a post-processing step. In a proofreading tool, segments can be quickly joined or split to produce the correct segmentation faster than it would be possible with manual annotation. The combination of automatic segmentation and proofreading is the preferred option for large data sets where manual segmentation is not feasible.</p><p>Our work stems from a collaboration with neuroscientists in the field of connectomics. Connectomics aims to completely reconstruct the wiring diagram of the mammalian brain at nanometer resolution, comprising billions of nerve cells and their interconnections <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44]</ref>. By deciphering this vast network and analyzing its underlying properties, scientists hope to better understand mental illnesses, learning disorders and neural pathologies <ref type="bibr" target="#b32">[33]</ref>. However, to analyze neuronal connectivity at the level of individual synapses (i.e., connections between nerve cells), high-resolution electron microscopy (EM) image stacks have to be acquired and processed <ref type="figure" target="#fig_1">(Fig. 2)</ref>. These image stacks are typically on the order of hundreds of terabytes in size and often exhibit severe noise and artifacts. The huge size of these volumes makes (semi-)automatic segmentation approaches the only viable option, however, the complex structure of the data provides difficulties  for automatic segmentation. The resulting segmentations, on average, require over 120 manual corrections per cubic micron of tissue <ref type="bibr" target="#b29">[30]</ref>. Previous research has focused on image acquisition, segmentation <ref type="bibr" target="#b27">[28]</ref> and interactive visualization and analysis <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref>, but little research has focused on the proofreading stage <ref type="bibr" target="#b37">[38]</ref>.</p><p>A powerful proofreading tool is crucial for enhancing segmentations efficiently and effectively. Particularly, communicating the three dimensional property of EM stacks is essential for identifying segmentation errors in 3D and for confirming the correctness of changes in the automatic segmentation. Existing software solutions tend to be geared towards domain experts and often have a steep learning curve. One example is Raveler <ref type="bibr" target="#b22">[23]</ref>, a stand-alone proofreading tool recently developed at the Janelia Farm Research Campus. The huge scale of the data that needs to be segmented and proofread, however, requires that proofreading will have to be crowdsourced in the future. This, on the other hand, implies that proofreading will have to be performed in a distributed setting by non-domain-experts and novice users.</p><p>In collaboration with neuroscientists at the Harvard Center for Brain Science we have developed two novel tools to increase the efficiency, accuracy and usability for proofreading large, complex EM data. First, we have developed Mojo-a powerful stand-alone software application <ref type="bibr" target="#b29">[30]</ref> for experienced as well as non-expert users that offers advanced semi-automatic proofreading features. Building on the experience we gained from Mojo, we developed Dojo ("Distributed Mojo")a web-based, distributed proofreading application that includes collaborative features and is geared towards non-expert users. Dojo offers easy access through a web browser, no installation requirements, 3D volume rendering, and a clean user interface that improves usability, especially for novice users. To evaluate the effectiveness of Dojo, we perform a quantitative user study. The study targets non-experts from all fields with no previous knowledge of proofreading electron microscopy data. We compare Dojo against Mojo and Raveler on a representative sub-volume of connectomics data. The study is designed as a between-subjects experiment with very little training for all participants and a fixed time frame of thirty minutes to proofread the given dataset. As a baseline, we also asked two domain experts to label the same sub-volume from scratch using manual segmentation.</p><p>Our first contribution is a set of requirements and design guidelines for visual proofreading applications based on our interviews with domain experts and feedback from an initial deployment of Mojo to non-expert users, interns and high-school students. Our second contribution is the design and development of Mojo, a stand-alone software application for proofreading. Based on our experiences during our work on Mojo, we defined requirements for a successive software which aims at increasing the usability of a proofreading tool geared towards non-expert users. These thoughts have led to the development of web-based Dojo, the third contribution of this paper. Dojo is easier to use and adds 3D volume rendering and collaborative features. Our final contribution is our quantitative user study. We present statistically significant results showing that novice users of Dojo are able to proofread a given data set better and faster than with existing proofreading tools. Based on these results we present design guidelines that will help developers of future proofreading tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Connectomics. Neuroscience and especially connectomics with its goals and challenges <ref type="bibr" target="#b31">[32]</ref> has received a lot of attention recently. Seung <ref type="bibr" target="#b42">[43]</ref> motivates the need for dense reconstruction of neuronal structures, underpinning the requirement for segmentation methods that automatically label huge volumes of high-resolution EM images. Registration, segmentation and annotation for neuroscience. Currently, the main bottleneck for analyzing connectomics data is the need for registration and segmentation of the underlying image data <ref type="figure" target="#fig_1">(Fig. 2</ref>). EM images are acquired slice by slice by physically cutting a block of tissue. Registration has to be performed between slices and also between individual sub-tiles of a single slice. Several methods focus on registration of large EM data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Segmentation methods for connectomics can be classified into manual <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, semi-automatic <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>, and automatic <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref> approaches. While initially manual and semi-automatic approaches were very popular, in recent years the need for automatic approaches that are scalable to volumes of hundreds of terabytes has become apparent. ITK-SNAP <ref type="bibr" target="#b48">[49]</ref> supports semiautomatic segmentation methods based on active contours as well as manual labeling. Vazquez-Reina et al. <ref type="bibr" target="#b47">[48]</ref> propose an automatic 3D segmentation of EM volumes by taking the whole volume into account rather than a section-to-section approach. They formulate the segmentation as the solution to a fusion problem with a global context. Kaynig et al. <ref type="bibr" target="#b27">[28]</ref> propose a pipeline for the automatic reconstruction of neuronal processes that is based on a random forest classifier coupled with an anisotropic smoothing prior in a conditional random field framework and 3D segment fusion. This pipeline is integrated into the RhoANA open-source software available at http://www.rhoana.org. Automatic segmentation methods are more scalable than manual or semi-automatic approaches, but often require a clean-up or proofreading step where incorrect segmentations are fixed. In 2013 the IEEE ISBI challenge <ref type="bibr" target="#b1">[2]</ref> called for machine learning algorithms for the 3D segmentation of neurites in EM data. The organizers provided test and training data, and a manual expert segmentation. While the results were quite impressive, all algorithms still exhibited a rather high error rate, motivating the need for proofreading. We use the data of that challenge in our user study. Collaborative segmentation and annotation. EyeWire <ref type="bibr" target="#b2">[3]</ref> is an online segmentation tool where novice users participate in a segmentation game to get points for segmenting neuronal structures using a semi-automatic algorithm. D2P <ref type="bibr" target="#b14">[15]</ref> uses a micro-labor workforce approach (based on Amazon's Mechanical Turk) where boolean choice questions are presented to users and local decisions are combined to produce a consensus segmentation. Catmaid <ref type="bibr" target="#b40">[41]</ref> and the Viking Viewer <ref type="bibr" target="#b5">[6]</ref> are collaborative annotation frameworks for experts, that allow users to create skeleton segmentations for terabyte-sized data sets. However, they do not offer proofreading support. Visualization and visual analytics for connectomics. A good overview of visualization for connectomics and human connectomics is given by Pfister et al. <ref type="bibr" target="#b38">[39]</ref> and Margulies et al. <ref type="bibr" target="#b34">[35]</ref> respectively. Most visualization frameworks for connectome data have only basic support for 3D rendering <ref type="bibr" target="#b0">[1]</ref> and focus on displaying network maps of connected brain regions <ref type="bibr" target="#b5">[6]</ref>. The Connectome Viewer <ref type="bibr" target="#b12">[13]</ref> offers a general processing and plug-in framework for visualization. Hadwiger et al. <ref type="bibr" target="#b15">[16]</ref> propose a visualization-driven petavoxel volume rendering framework for high-resolution electron microscopy streams. For visual analysis of connectomics data, interactive query systems have been proposed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. None of these systems, however, run in a distributed multi-user environment and many need high-performance workstations and modern GPUs for rendering and data processing. Collaborative and web-based visualization of image data. Ginsburg et al. <ref type="bibr" target="#b13">[14]</ref> propose a visualization system for connectome data based on WebGL <ref type="bibr" target="#b28">[29]</ref>. They combine brain surface rendering with tractography fibers and render a 3D network of connected brain regions. The X toolkit <ref type="bibr" target="#b17">[18]</ref> offers WebGL rendering for neuroimaging data, and SliceDrop <ref type="bibr" target="#b16">[17]</ref> is a web-based viewer for medical imaging data that supports volume rendering and axis-aligned slice views. Jeong et al. <ref type="bibr" target="#b24">[25]</ref> describe an online collaborative pathology viewer. None of these frameworks, however, support volume rendering of segmented data or interactively updating segmentations. Proofreading. Until recently, visual proofreading methods for automatically generated segmentations have not received a lot of attention. Sicat et al. <ref type="bibr" target="#b45">[46]</ref> propose a graph abstraction method to simplify proofreading. They construct a skeleton of the segmentation and identify potential problematic regions in the segmentation and guide users to these areas. Raveler <ref type="bibr" target="#b22">[23]</ref>  <ref type="figure" target="#fig_2">(Fig. 3)</ref>, by Janelia Farms, is used for annotation and proofreading and uses a quadtree-based tiling system to support large data. It targets expert users and offers many parameters for tweaking the proofreading process at the cost of a higher complexity. In this paper we introduce two novel proofreading tools: Mojo (Sec. 4.1) and Dojo (Sec. 4.2); and compare them to Raveler in a quantitative user study (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VISUAL PROOFREADING</head><p>In this section we introduce the overall workflow for proofreading high-resolution EM image stacks and discuss common issues (e.g., usability and scalability) before presenting a detailed requirement analysis of the necessary tasks in a scalable proofreading tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proofreading Workflow</head><p>The visual proofreading workflow consists of three main steps:</p><p>1. Searching for structures containing segmentation errors.</p><p>2. Modifying the existing segmentation to fix the errors.</p><p>3. Confirming the correctness of the modified segmentation.  The first step -searching for segmentation errors -requires users to be very focused. This is especially true for volumetric data because tracking 3D structures in a 2D slice-based visualization involves constant switching between slices. Segmentation data is typically displayed as a colored overlay on top of the original image data. A common strategy to spot segmentation errors is to continuously toggle the visibility of this segmentation overlay to compare the labeled boundaries to the actual image data. During this search three different types of errors can be spotted: a) merge errors or under-segmentation, where two separate structures are erroneously connected; b) split errors or over-segmentation, where a single structure is erroneously split into several different segments; and c) boundary errors, where the boundaries of a labeled segment do not match the boundaries of the structure in the image data <ref type="figure" target="#fig_3">(Fig. 4)</ref>.</p><p>The second step -modifying the existing segmentation -is mentally less demanding than the search phase but needs good tool support, to allow users to quickly and correctly modify the existing segmentation. The most common semi-automatic tools for correcting the segmentation correspond to the three error types: merge, split, and adjust. Once a segmentation error is spotted, the user chooses the appropriate tool to fix the error. Merge errors can be corrected by using the split tool to divide the segments. Split errors can be corrected by joining the segments using the merge tool. Boundary errors can be fixed by adjusting either the erroneous label or the neighboring labels. In connectomics data, these errors always look similar (e.g., a merge error fails to detect a dark boundary between two lighter structures). Therefore, after understanding these three error types it should be possible, even for non-expert users, to recognize and subsequently fix these errors.</p><p>The last step -confirming the correctness of the modified segmentation -requires the user to check if the modifications did fix the segmentation error, or if another correction step is necessary. Most proofreading tools only offer a 2D slice view for that task, however, we propose that an additional 3D view can significantly improve the speed and accuracy in which users can confirm their modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proofreading Issues</head><p>We identified several issues that have to be considered when designing proofreading tools for connectomics data. The main problems in proofreading tools usually correspond to usability issues, scalability issues, and failing to target the system towards the expected users, their needs and background knowledge. Developers of systems for experts can expect users to have a thorough domain background as well as a higher motivation to use the system, hence higher frustration threshold. Most proofreading tools are geared towards expert users and offer a full-featured segmentation framework and often a very complex user interface <ref type="figure" target="#fig_2">(Fig. 3)</ref>. When designing a system for non-experts, however, one has to pay particular attention to usability issues and guidance that will help novice users to perform their proofreading tasks. Data set sizes in connectomics are increasing at a rapid pace. Currently our collaborators routinely acquire datasets of several terabytes, but in the future this will continue to grow. One solution for handing large data is to work on sub-volumes of the data. However, it is common that neuronal structures extend across long distances especially when images are in nanometer pixel resolution. These sub-volumes would then have to be merged in an extra step to guarantee that structures across block boundaries receive corresponding labels. The problem of combining segmentations is further complicated in settings where multiple users work on the same dataset concurrently. Without a collaboration mode, each user has to either work on a different sub-volume, or segmentations of different users might overlap. The former is not scalable to a system with potentially thousands of users while the latter can result in conflicts where the segmentation between users differs, and which will have to be solved in a post-processing step. Collaborative approaches would allow users to discuss unclear areas or conflicts directly during the proofreading process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Requirement Analysis</head><p>We regularly met with our collaborating scientists to discuss their goals and define required user tasks for proofreading. We conducted informal as well as semi-structured interviews with them over the course of several months and identified several domain-specific proof reading tasks. After initial development, we installed Mojo at the lab of our domain experts, where it was used for a first informal user study. After this initial testing phase, Mojo was used by a larger group of non-experts (20+ high-school students and lab interns) to correct the segmentation of a small data set of a mouse cortex. The feedback acquired in this first deployment step led us to the development of Dojo, targeting a web-based, non-expert, multi-user environment.</p><p>We have identified the following general requirements for proofreading large-scale EM image stacks:</p><p>R1. Intuitive navigation inside the 3D image stack. Users have to be able to easily and intuitively navigate within the 3D data including zooming and navigating within a single slice and multiple slices. A 3D view is necessary to help non-expert users grasp the spatial relations in the data and should be able to display either the entire volume or only selected user-defined structures.</p><p>R2. Detection of segmentation errors. To quickly detect segmentation errors the user has to be able to easily switch between different rendering modes (i.e., showing the raw data, showing the segmentation data, showing the segmentation outline).</p><p>R3. Fast correction of errors. Correcting errors has to be as simple and easy as possible. Manual corrections have to be supported, but semi-automatic methods for splitting and merging should be the main interaction metaphors and have to be accurate, fast, and easy to use.</p><p>R4. Checking the correctness of modified segmentations. Nonexpert users have to be able to quickly judge the correctness of their last modification. This requires visualizing and highlighting the modified segment in 2D as well as in 3D.</p><p>R5. Collaborative proofreading environment. The system has to support multiple simultaneous users. The amount of data that needs to be processed necessitates crowdsourcing the proofreading step. This means that multiple users have to be able to work on the same data set and giving users the ability to work together collaboratively.</p><p>R6. Simple deployment. A crowdsourcing system has to support easy deployment and must not require any special hardware. Highschools, for example, do not allow installing any external software on their computers. To enable high-school students to work on proofreading, the system has to be web-based and should run in every browser.</p><p>R7. Minimalistic GUI. The final requirement targets the usability of the system. To support non-expert users, the user interface has to be simple and easy to understand. It is better to have fewer options that are well understood by its users, than to have a cluttered user interface that confuses novice users. <ref type="table" target="#tab_1">Table 1</ref> summarizes the different features supported by each of the proofreading tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TECHNOLOGY</head><p>In the following section we describe the technology behind Mojo and Dojo. Mojo was developed as a powerful standalone application and is now used by domain experts and trained researchers to perform proofreading. To overcome the limitations of Mojo regarding installation, multi-platform support and hardware requirements, we also developed a web-based proofreading tool. Dojo does not require any installation and can be accessed with a web browser such as Google Chrome and also runs on tablets and smartphones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mojo -A Standalone Proofreading Tool</head><p>Mojo supports the computer-assisted correction of automatic segmentation results by providing a simple scribble interface for correcting merge and split errors in automatic segmentations. Mojo is capable of annotating any volumetric segmentation data, but works best when a reasonable automatic segmentation can be provided as a starting point. Mojo is written in C++ and C# and available as open source software at http://www.rhoana.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">User Interface</head><p>The Mojo GUI <ref type="figure" target="#fig_4">(Fig. 5</ref>) reflects a typical desktop application, with a toolbar at the top of the window to control interface and view options, a 2D slice view window showing the current segmentation and EM data, and a segment list showing segment names, ids and sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Proofreading Features</head><p>The Mojo interface provides split, merge and adjust tools that enable edit operations to be performed with simple, wide brush strokes. This allows annotation to be performed on a standard workstation, without the use of a tablet drawing or touchscreen device. The interaction mode of each tool can be customized in the Mojo toolbar. The merge tool offers two interaction modes; draw and click. In draw mode, the user draws a wide brush stroke over the objects to be merged. Any objects touched by this brush stroke are combined into a single object. In click mode the user selects a seed object with a left click and merges additional objects to the seed object with a right click. Merging operates on 2D segments, 3D connected objects, or all 3D objects in the volume, depending on the selected merge behavior.</p><p>The split tool offers three interaction modes. In boundary line splitting, the user simply draws a wide brush stroke over the membrane that represents the boundary. From the lines drawn, non-adjacent perimeter pixels are found and used as seed points for a watershed algorithm. This method is very fast to compute, and results can be adjusted interactively by adding or removing pixels from the boundary line. The remaining two modes are point-based split, which is similar to a livewire segmentation where individual points are added to the split line; and region-based split, where seed regions are painted and a watershed boundary is found between them. Once one split has been performed, Mojo can predict how segments in adjacent slices will be split. User can navigate through the stack and quickly confirm or modify split operations while retaining 3D connectivity of the split objects.</p><p>In addition to the merge and split tools, the adjust tool allows the user to manually draw a region and add it to the selected segment. This tool is useful when a combination of split and merge operations are required to correct a segment. Segments which have been fully corrected can be locked, which changes their appearance to a striped pattern and ensures that they will not be included in further actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Visualization</head><p>Mojo displays the original data in a single slice view and allows users to toggle the colored segmentation overlays. Boundaries of segmented structures can be enhanced by showing contours. Additionally, the user can zoom in and pan within a single slice or navigate between slices. When an object is selected, or the mouse hovers over an object, all parts of that object are highlighted to help the user navigate through the volume and to quickly identify all parts belonging to that object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Data Handling</head><p>Mojo data is stored on the filesystem as a quadtree hierarchy of 2D image and segmentation tiles. Additional segment information, such as segment name and size, is stored in an sqlite database. To improve tile loading and processing times we maintain a tile index table to identify which tiles each segment id appears in. Additionally, we use a segment remap table to allow fast merge operations. Merging segment A with segment B can be achieved without having to modify the original segmentation tiles but by adding a look-up or redirection entry (e.g., A→B) into the segment remap table. The remap table is maintained in system memory on the GPU, so that stored segmentation tile ids can be loaded directly to the GPU and large volume changes can propagate quickly to the display. 2D merge, split and adjust operations modify segment tiles directly and the tile index table is updated accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dojo -A Distributed Web-Based Approach</head><p>Dojo is a web-based proofreading tool consisting of an image server component written in python and an HTML5/JavaScript based user interface on the client side. This has the advantage that users can access the software by simply pointing their web-browser to the URL of the Dojo server. The main goal of Dojo's GUI and interaction design is to reduce complexity for non-expert users. Furthermore, Dojo was designed to be compatible with Mojo. Both tools use the same data structures, ensuring that annotated data from Mojo can be loaded into Dojo and vice-versa. The source code of Dojo and a demo installation are available at http://rhoana.org/dojo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">User Interface</head><p>The graphical user interface of Dojo <ref type="figure" target="#fig_5">(Fig. 6</ref>) was designed with nonexpert users in mind and aims to be minimalistic and clean. The 2D slice viewer uses the full window size while controls, information and help are moved to the corners to not disturb the data visualization and to provide a distraction-free environment. All textual information is kept small but still readable. The elements of the toolbox (i.e., split, merge, adjust) are presented as simple icons that show help tooltips upon hovering. Furthermore, to reduce interaction complexity, Dojo's proofreading tools are parameter-free and only require simple pointand-click mouse interaction. Additionally, all tools can be activated with keyboard shortcuts which are documented in the lower left corner for quick reference. The 3D volume rendering view is located in the upper right corner of the main window and can be resized as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Proofreading Features</head><p>Dojo provides three proofreading tools, inspired by Mojo's toolbox described in Section 4.1.2. The key difference is that Dojo offers a single interaction mode for each tool, thus simplifying the interface. With the merge tool, the user clicks on the propagating structure and then on all segments which should be merged with it <ref type="figure">(Fig. 7)</ref>. This action connects the segments across all slices (3D merge) and can also be used to merge segments which are located on different slices.</p><p>The split tool allows users to split a single segment into two or more segments by drawing a line across the segment that is to be split <ref type="figure" target="#fig_8">(Fig. 8)</ref>. A split is confirmed by clicking in that part of the original   object that should keep the original color. Under the hood, the splitting algorithm works differently to Mojo: Dojo uses all points of the segment to be split that are not part of the drawn line as seeds for a watershed algorithm, instead of only two perimeter pixels. Additionally, before computing the watershed algorithm, the original image data is blurred by a Gaussian and then contrast enhanced, which has experimentally been proven to generate better and more stable results. The adjust tool works as in Mojo and lets users paint on the image data to extend a segmentation label <ref type="figure" target="#fig_9">(Fig. 9)</ref>.</p><p>Merging uses the remap table data structure of Mojo and is computed solely on the client. Once a client adds or removes a merge, the merge table is sent to the server to keep all clients synchronized. No pixel data needs to be modified when merging segments. Splitting is performed on the server and does require pixel modifications. A split triggers a reload event for all clients to fetch new segmentation data for the specific slice. Users identify segmentation errors by constantly comparing the original image with the segmentation results. This tedious procedure results in a high mental workload for users but Dojo offers hotkeys for switching or adjusting opacity of the layers. Also, the volume rendering component of Dojo is useful for this task <ref type="figure" target="#fig_11">(Fig. 11)</ref>. In the future, we want to integrate methods that provide user guidance by automatically identifying potential errors of such segmentations <ref type="bibr" target="#b36">[37]</ref> and showing them to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Visualization and Volume Rendering</head><p>In addition to the 2D slice view, Dojo provides full 3D volume rendering of the image stack based on WebGL. We leverage the X toolkit (XTK) for this purpose <ref type="bibr" target="#b17">[18]</ref>. Since XTK is primarily used for medical imaging data sets which are smaller in size than EM data, we extended XTK to support 32 bit label overlays and raw image data of larger sizes. WebGL does not support 3D textures, therefore, volume rendering is based on 2D textures <ref type="bibr" target="#b8">[9]</ref>. To circumvent memory and texture size limitations, the volume renderer limits the resolution of the loaded image slices to 512 × 512 for the xy-plane. This resolution is sufficient to display the 3D context of a structure and to gain a better spatial understanding of the data. The volume rendering can be activated by selecting an icon in the toolbox. Once active, the renderer displays the full image stack and segmentation volume or multiple selected segmentations. To enhance the users' orientation, the current slice position in the image stack is displayed as a red outline. Additionally, we have integrated collaborative features into the 3D view which are explained in more detail in Section 4.2.5 <ref type="figure" target="#fig_10">(Fig. 10c)</ref>. Our volume renderer is built on top of WebGL and therefore requires a web browser with WebGL support. Nevertheless, recent advances in technology have brought WebGL to all major web browsers including most smartphones and tablets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Data Handling -Large image data on the web</head><p>One key feature of Dojo is the support of large image and segmentation data. Transferring large amounts of image data between server and web client has previously been explored as part of several research projects <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41]</ref>. When designing Dojo we therefore evaluated different frameworks for interactively displaying large-scale images on the web. Most existing solutions are based on quadtree multi-resolution hierarchies that always load the currently requested resolution of the image data-low resolution levels for far-away and zoomed-out views and high resolution levels when the user zooms in. This approach is well known and similar to Mojo's quadtree data structure. Unfortunately, most existing web-based image servers exhibit poor performance when loading different zoom levels so the user experience was not comparable to Mojo or other standalone software applications. The only framework with a comparable performance was OpenSeaDragon <ref type="bibr" target="#b3">[4]</ref>. OpenSeaDragon is based on the DeepZoom protocol and was initially developed by Microsoft but later open sourced as a community project. In our initial feasibility study we used an OpenSeaDragon viewer to connect to an IIP (Internet Imaging Protocol) image server to transfer images in a very performant way. The initial development stage of Dojo focused on using these technologies and it worked well for raw images. Unfortunately, there was no easy way of transferring segmentation data nor to display it as overlays. We initially added this functionality to OpenSeaDragon in close collaboration with their core developers, however, eventually had to abandon this path. While the basic integration of segmentation data worked, the viewer exhibited severe flickering artifacts and the image server did not support correct downsampling of segmentation masks. Another problem was the bit depth: our segmentation data can have up to 64 bit per pixel, which is not supported by the OpenSeaDragon framework. Based on these experiences we decided to a) develop our own client side image viewer and b) develop our own image server.</p><p>Using a custom web-client and image server let us optimize the transfer of images to our specific requirements. We now transfer high bit-per-pixel segmentation data directly as a zlib-compressed byte stream. This results in data sizes comparable to PNG encoding without any interpolation errors or loss of precision. Furthermore, the development of a custom image server and web client led to significant performance improvements. Zooming and scrolling through the image stack can now be done in real-time, even for large volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Collaborative Features</head><p>Web pages inherently support connections from multiple clients (i.e., web browsers) and thus multiple users at the same time. By default, clients fetch information from and post information to the server. Usually, servers do not push information to all clients. Nevertheless, the latter can be achieved by using web sockets. We decided to use a web socket server in addition to standard HTTP to enable synchronization between all connected clients. Modifications of image data through merging, splitting and adjusting are sent to the server which then distributes them to each client. This can be a heavy operation depending on how many clients are connected. Thus, we limit the transfer to coordinates and meta information but never send binary data via web sockets. Additionally, each image operation gets stored as an entry in the activity log which is displayed on all connected clients, and located at the bottom right corner of the view.</p><p>Proofreading of larger volumes can be sped up considerably when multiple clients are working on the same volume. Hence, in addition to the synchronization of the current state of segmentation, we decided to add several collaborative features. Mouse cursors of all connected users are shared <ref type="figure" target="#fig_10">(Fig. 10)</ref>. If two or more users work on the same tile in the image volume, the other users' cursors are shown as little colored squares. The colors are randomly assigned and each users' action is also colored in the activity log. If the 3D visualization is active, the cursors are also displayed as colored pins or needles pointing to a position of the volume. It is possible to click on a needle to jump to the position the other user is currently working on. In addition to cursor sharing, users can actively mark an area of the data to seek help from other users. When a user sets such a mark, all other users see a large exclamation mark in the 3D view and a small exclamation mark if they view the same slice. After navigating to that position and resolving the issue, it is possible for users to mark this problem as solved. Asking for help and marking an area as fixed is also reported in the activity log. Since these collaborative features can be distracting, they are optional and can be toggled by each individual client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6">Limitations</head><p>Dojo has several limitations due to its minimalistic user interface and web-based nature (see <ref type="table" target="#tab_1">Table 1</ref>): parameter-free operations do not allow complex operations as in Mojo and Raveler, the 3D view is less versatile than stand-alone GPU approaches due to memory restrictions resulting in downsampled textures, and the undo stack is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USER STUDY</head><p>We conducted a quantitative user study to evaluate the performance and usability of three proofreading tools. In particular, we evaluate how nearly untrained non-experts can correct an automatic segmentation using Mojo, Dojo and Raveler. We designed a between-subjects experiment and asked the participants to proofread a small dataset in a fixed time frame of 30 minutes. We used the most representative subvolume of a larger freely available real-world data set where expert ground truth is available. We recruited participants from all fields with no experience in electron microscopy data or proofreading of such. As baseline, we asked two domain experts to label the same sub-volume from scratch using manual segmentation in the same fixed time frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hypotheses</head><p>We proposed three hypotheses entering the study:</p><p>• H.1 Proofreading will be measurably better with Dojo compared to other tools. When presented with identical data, users of Dojo with no experience in proofreading and very short training, will perform significantly better (i.e., more accurately) than users of Mojo or Raveler.</p><p>• H.2 Dojo's usability is higher than other tools. Users of Dojo will report increased usability and that they like the system.</p><p>• H.3 Given a fixed short time frame, proofreading by nonexperts gives more correct results than completely manual annotations by experts. In a fixed short time frame, inexperienced participants will generate a corrected data set which is more accurate and similar to the ground truth than a manual labeling from domain experts that was done in the same time frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Participants</head><p>Because we wanted to study how completely inexperienced users perform with the three proofreading tools, we recruited people of all occupations through flyers, mailing lists and personal interaction. Based on sample size calculation theory, we estimated the study sample size as ten users per proofreading tool including six potential dropouts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>. Nevertheless, all thirty participants completed the study (N = 30, 17 female; 20-40 years old, M = 27). Participants had no experience with electron microscopy data or proofreading of such and had never seen or used any of the three software tools. They received monetary compensation for their time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Conditions</head><p>The three conditions in our study were the proofreading tools Mojo, Dojo and Raveler. Each participant proofread the same dataset in a time frame of 30 minutes. The requirement for participating was no experience in proofreading EM data. Since the three tools run on different platforms (except web-based Dojo which runs on all), we used three different machines with similar and standard, off-the-shelf hardware. Therefore, the only variable was the used software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Dataset</head><p>We used a publicly available dataset of a mouse cortex (1024x1024x100 pixels) that was published for the ISBI 2013 challenge "SNEMI3D: 3D Segmentation of neurites in EM images". It was acquired with a serial section scanning electron microscope (ssSEM) with a resolution of 6x6x30 nm/pixel. We trained our automatic segmentation pipeline (Rhoana pipeline) on a similar dataset and used it to segment the data. Details of the segmentation pipeline are published in <ref type="bibr" target="#b27">[28]</ref>. Manually labeled expert segmentation was available as a ground truth for the complete dataset. Since it is not feasible to let users proofread such a large volume, we cropped a sub-volume to 400x400x10 pixels. To find the most representative sub-volume (i.e., the sub-volume with the distribution of object sizes that is closest to the empirical mean distribution of object sizes in the volume) we calculated object size histograms, used them as features in a multi-dimensional vector space and chose the sub-volume with the minimal Euclidean distance to the centroid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Procedure</head><p>Each study session started off with the participants signing the consent form and a basic demographic survey (age, sex, occupation, neuroscience background, scientific visualization background, familiarity with proofreading of segmentations and familiarity with EM data). Next, the participants were introduced to the Connectome project and its typical workflow <ref type="figure" target="#fig_1">(Fig. 2)</ref>. Then, participants sat down at their randomly assigned study station which ran one of the three proofreading tools and the experimenter explained the tool. To demonstrate merging and splitting functionalities of each tool, a training dataset was loaded which was the second most representative sub-volume of the larger dataset with the same size of the test dataset. There was no common region between the training data and the test data even though both were highly representative of the larger dataset. After explaining two merge and two split operations (average <ref type="figure" target="#fig_1">Fig. 12</ref>: Variation of information. The VI similarity measure for each tool after proofreading for 30 minutes (lower is better). Participants using Dojo achieved a lower VI than subjects with the other tools. These results are statistically significant. The red line shows the VI of the automatic segmentation before proofreading. The blue lines show the VI of the experts' manual segmentation after 30 minutes.</p><p>time about 5 minutes), the participants were asked to try the proofreading tool themselves for another 5 minutes. The experimenter then loaded the test dataset which was the same for each participant. For the next 30 minutes participants were asked to correct as many segmentation errors as possible but were warned that it was highly unlikely that they could finish proofreading the complete dataset in the given time frame. During the assessment, usage questions regarding the proofreading software were answered in a short manner.</p><p>After 30 minutes or if the participant decided to stop the experiment, the participants completed a qualitative questionnaire with ten questions regarding the software. Then, they answered the raw NASA-TLX standard questions for task evaluation <ref type="bibr" target="#b18">[19]</ref>. At the end of the session, participants were presented their similarity scores in respect to the ground truth segmentation and could give general feedback and comments. The entire study session took approximately 60 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Expert Segmentations</head><p>To generate a baseline measure regarding hypothesis 3, we asked two experts to manually annotate the representative sub-volume from scratch. The experts used their software of choice, ITK-SNAP <ref type="bibr" target="#b48">[49]</ref>. We set a time limit of 30 minutes and computed the similarity measures variation of information and Rand index as well as Edit Distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Design and Analysis</head><p>We used a single factor between-subject design with the factor proofreading tool (Mojo, Dojo, Raveler). The participants were randomly assigned to one of the three tools. From our group of participants (N = 30) we excluded two subjects (using Raveler and Mojo). One of the two subjects stated to be familiar with proofreading of EM data and for the other participant Mojo crashed after 20 minutes.</p><p>Our dependent measures were two measures of similarity between participants' results and the ground truth segmentation (manually labeled by experts, publicly available), the number of not performed merges and splits to fully correct the segmentation, as well as the participants' subjective responses recorded on a 7-point Likert scale.</p><p>Similarity was measured as variation of information (VI) ( <ref type="figure" target="#fig_1">Fig. 12</ref>) and Rand index (RI) <ref type="figure" target="#fig_2">(Fig. 13</ref>) which are common benchmarks for clustering comparison in computer vision. VI is a measure of the distance between two clusterings, closely related to mutual information, but lower being better. Rand index is a measure of similarity, related to the accuracy, meaning higher scores are better. The number of not performed merges and splits to correct the segmentation is the Edit Distance, another common metric in computer vision. We calculated these measures and treated them as continuous variables. We analyzed <ref type="figure" target="#fig_2">Fig. 13</ref>: Rand index. The RI similarity measure for each tool after proofreading for 30 minutes (higher is better). Participants using Dojo achieved a higher RI than subjects with the other tools. These results are statistically significant. The red line shows the RI of the automatic segmentation before proofreading. The blue lines show the RI of the two experts' manual segmentation after 30 minutes.</p><p>these dependent variables using analysis of variance followed by parametric tests. For the subjective responses on Likert scales, we created sub-groups and performed ANOVA according to Holm's sequentiallyrejective Bonferroni method <ref type="bibr" target="#b44">[45]</ref> and parametric tests, if relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS AND DISCUSSION</head><p>The results of our user study demonstrate a modest advantage of Dojo regarding the quality of corrections. The difference in similarity was minor but statistically significant (Sec. 6.1). Another interesting but expected finding was that proofreading yielded better performance than manual segmentation of experts starting from scratch, when given the same short time frame (Sec. 6.3). Overall, participants reported better usability of Dojo compared to the other proofreading tools (Sec. 6.4), thus confirming our initial hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Similarity</head><p>The initial segmentation for all participants was created using the Rhoana pipeline and had a VI of 0.98 and a RI of 0.53. The ten participants using Dojo had a mean VI of 0.90 (SD = 0.10). The nine participants using Mojo had a mean VI of 1.11 (SD = 0.12). The nine participants using Raveler had a mean VI of 1.12 (SD = 0.25). The ef- fect of the proofreading tool, therefore, was significant, F 2,25 = 5.04, p = .015. Post hoc comparisons (after Bonferroni correction) indicate that the mean VI for results with Dojo was significantly lower than for Mojo (t 25 = 2.06, p = .0411) and also lower than the one for Raveler (t 25 = 2.06, p &lt; .01). <ref type="figure" target="#fig_1">Figure 12</ref> shows this relation. We also analyzed the mean RI of participants using Dojo which was 0.55 (SD = 0.03). For participants using Mojo, the mean RI was 0.51 (SD = 0.02) and for participants who used Raveler, the mean RI was 0.5 (SD = 0.06). This difference was statistically significant, F 2,25 = 3.59, p = .043. Further testing (after Bonferroni correction) showed, that only the difference between Dojo and Raveler was significant (t 25 = 2.06, p = .05). The results are displayed in <ref type="figure" target="#fig_2">Figure 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Edit Distance</head><p>The edit distance metric (ED) reports how many operations have to be performed to reach the state of another labeled image. We calculated the ED for the proofread EM data as the sum of required 2D splits and 3D merges to reach the ground truth for each participant. These are the operations which can be performed by all three tools. The initial segmentation which was the input for all participants, had an ED of 54 (32 splits, 22 merges). A blank segmentation of our data had a ED value of 386, requiring 386 splits and 0 merges. The mean ED of participants for Dojo was 59.1 (SD = 23.28), for Mojo 62.9 (SD = 12.03) and for Raveler 83.22 (SD = 37.03). These results were not statistically significant but they follow the trend of better performance with Dojo. In fact, about half of the participants using Dojo were able to reduce the ED. <ref type="figure" target="#fig_3">Figure 14</ref> shows this relation. Even though the improvement of the ED was not statistically relevant, the improvements of VI and RI allow us to accept H1 and to confirm that complete novice users perform slightly better on a proofreading task using Dojo than with other tools. Interestingly, many participants were not able to improve the automatic segmentation but made it worse. In fact, only participants using Dojo were able to correct the segmentation on average. We believe that this is caused by several of factors:</p><p>1. Three-dimensional thinking. It is hard for untrained users to grasp the 3D structure of EM data. Dojo provides full 3D volume rendering to help users get a three dimensional intuition of the data, for single structures as well as for the whole EM stack. 2. Difficulty identifying boundaries. EM data can be very noisy and cell boundary detection needs to be practiced. From observations, it seemed that a large amount of time was spent by participants trying to identify boundaries. 3. Time. Participants tried to correct as much as possible in the given time frame. Even though they were told to only perform corrections when confident, they felt rushed. Therefore, we want to do a follow-up study without that fixed time frame. 4. Usability of tools. The usability of many existing proofreading tools is lacking. An overwhelming number of features and parameters are available and confuse users. Only Dojo provides parameter-free, easy-to-use tools. 5. Almost zero training. The participants of this study received, on purpose, nearly no training regarding the data or the software. We believe that training in the range of hours or days can drastically improve the performance of non-experts. Especially the manual detection of errors is very difficult for novices. Algorithms that guide the user to potential errors could greatly improve user performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Proofreading versus Manual Expert Segmentation</head><p>The two experts who were asked to manually label the data set from scratch in 30 minutes, did not reach the result of our proofreading participants starting from automatic segmentation <ref type="figure" target="#fig_1">(Figures 12, 13, 14)</ref>.</p><p>The mean values were VI=1.77, RI=0.26 and ED=277. These results were significant in comparison to Dojo (t 25 = 2.06, p &lt; .0001). This is no surprise since manual segmentation is very time consuming. Hence, we accept H3. In the given time frame experts were able to label 60.3% and 57.8% of the sub-volume. It is clear that given more time, the VI, RI and ED measures would improve rather quickly for manual expert segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Subjective Responses</head><p>All subjective responses were recorded on a 7 point Likert scale with 1=fully disagree and 7=fully agree. To ensure representative results, we grouped the questions and performed Holm's sequentially-rejective Bonferroni adjustment (N reported) before reporting statistic significance. We observed statistical significance for qualitative responses regarding usability: Participants stated that the tool is easy to use for Dojo M = 4.5 (SD = 1.27), Mojo M = 4.11 (SD = 2.03) and Raveler M = 3.22 (SD = 1.09) and that the tool is usable for Dojo M = 5.1 (SD = 1.1), for Mojo M = 4.3 (SD = 1.87) and for Raveler M = 3.11 (SD = 1.27). After adjustment with N = 2, being usable is statistically significant (F 2,25 = 4.57, p = .0408) while further analysis only confirmed significance between Dojo and Raveler (t 25 = 2.06, p = .006). We asked three questions regarding the visualization components: Participants stated that the slice visualizations were pleasing for Dojo M and Raveler M = 4.0 (SD = 1.63). Unfortunately, none of these results were significant after adjustment (segment visualization was before). Nevertheless, we do believe that the 3D volume rendering of Dojo had impact on the superior quantitative performance reported in the previous section. Other interesting observations were that participants reported that the merge tool was easy to use: for Dojo M = 5.8 (SD = 1.14), Mojo M = 4.89 (SD = 1.17) and Raveler M = 3.78 (SD = 1.39). After adjustment with N = 3, this was statistically significant (F 2,25 = 6.37, p = .0174). The NASA-TLX workload reported by the users did not yield any interesting results. As the overall conclusion, participants reported that they generally liked the used software for Dojo M = 5.4 (SD = 1.17), Mojo M = 4.33 (SD = 1.66) and Raveler M = 3.67 (SD = 1.41). This result was statistically significant with N = 1 (F 2,25 = 3.62, p = .0416) but further analysis showed significance only between Dojo and Raveler (t 25 = 2.06, p = .0135).</p><p>Because of that and the usability findings, we partially accept H2. The qualitative as well as the quantitative evaluation was in favor of Dojo which also matches our observations during the user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have presented an analysis and evaluation of proofreading tools for automatic segmentations of connectomics data. Based on this analysis and on our experience with Mojo, we developed Dojo, a web-based proofreading tool. Dojo provides several proofreading aids such as a clean and minimalistic user interface as well as 3D volume rendering. We performed a between-subjects user study regarding Dojo, Mojo and another proofreading tool called Raveler. The results of our quantitative evaluation confirm the need for easy-to-use and well-designed visualization features for proofreading tools but also show the need of user training regarding the proofreading task. The individual differences between the evaluated tools were not large due to study design limitations (see Section 6.2).</p><p>In the near future we will deploy Dojo to hundreds of high school students to proofread EM data in a collaborative fashion. Furthermore, we want to investigate in novel methods for simplifying cell boundary detection. Using interactive edge-detection to highlight boundaries could significantly improve the performance of non-expert users. We would also like to perform an in-depth user study without the time limitation to see how far proofreading can optimize faulty segmentations. Furthermore, we hope that offering an open source proofreading tool will promote the adoption of web-based scientific visualization and encourage more research in novel proofreading applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc>information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Manuscript received 31 Mar. 2014; accepted 1 Aug. 2014 ate of publication 2014; date of current version 2014. 11 Aug. 9 Nov. D . Digital Object Identifier 10.1109/TVCG.2014.2346371</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Proofreading as part of the Connectome workflow. Electron microscopy data of the mammalian brain gets acquired, registered and segmented. Since the output of the automatic segmentation algorithm is not perfect, proofreading is a mandatory stage before any analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>User interface of Raveler by Janelia Farm. The interface consists of the 2D slice view (center), the toolbox (right), additional textual information (bottom) and a simple 3D renderer showing bounding boxes of segments (bottom left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Segmentation errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>User interface of Mojo. The interface consists of the 2D slice view (center), the toolbox (top) and additional textual information as well as the label list (bottom, right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>User interface of Dojo. The interface consists of the 2D slice view (center), the 3D volume renderer (top right corner), the toolbox (top left corner), additional textual information (bottom left corner) and an activity log including all connected users (bottom right corner).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>click 3 Fig. 7 :</head><label>37</label><figDesc>Merge workflow. Dojo merges segments via mouse clicks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) select (b) brush (c) confirm (d) result</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Split workflow. Users can split connected segments in Dojo by brushing over cell boundaries. The software then calculates the best split which can be accepted or discarded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Adjust workflow. Users can perform fine-grained pixel-wise adjustments by painting on the image data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Collaborative features of Dojo. When active, the collaborative mode of Dojo facilitates proofreading the same data among multiple users. (a) 2D: cursors of other users are visible as colored squares if working on the same slice, (b) 2D: users can mark difficult regions with an exclamation mark and (c) 3D: cursors of all connected users and exclamation marks are visible as colored pins pointing to specific locations in the volume. Users can directly jump to that location by clicking on the pins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Volume rendering of proofreading results in Dojo. Prior to proofreading, incorrect segments are present (different colors). Right: After proofreading, neurons are smooth and uniformly colored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>Edit Distance. The ED similarity measure for each tool after proofreading, in respect of the ground truth segmentation after 30 minutes (lower is better). On average, participants using Dojo reached a lower ED than subjects with the other tools. The red line shows the ED of the automatic segmentation before proofreading. The blue lines show the ED of the experts' manual segmentation after 30 minutes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>= 5 . 8 (</head><label>58</label><figDesc>SD = 0.92), Mojo M = 4.7 (SD = 1.73) and Raveler M = 4.33 (SD = 1.58), the segment visualizations were pleasing for Dojo M = 5.5 (SD = 1.08), Mojo M = 5.11 (SD = 0.93) and Raveler M = 4.0 (SD = 1.5) and additional information beside 2D was useful for Dojo M = 5.0 (SD = 1.63), Mojo M = 4.0 (SD = 2.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of proofreading tools. Features offered by Mojo, Dojo and Raveler are summarized in respect to the requirements as identified in Section 3.3.</figDesc><table><row><cell>Requirement</cell><cell>Mojo</cell><cell>Dojo</cell><cell>Raveler</cell></row><row><cell>R1. Navigation</cell><cell>2D</cell><cell>2D+3D</cell><cell>2D+simple 3D</cell></row><row><cell>R2. E. Detection</cell><cell>manual</cell><cell>manual</cell><cell>manual</cell></row><row><cell>R3. E. Correction</cell><cell cols="3">advanced parameter-free very advanced</cell></row><row><cell>R4. Validation</cell><cell>2D</cell><cell>2D+3D</cell><cell>2D+simple 3D</cell></row><row><cell>R5. Collaboration</cell><cell>n/a</cell><cell>yes</cell><cell>n/a</cell></row><row><cell>R6. Deployment</cell><cell>download</cell><cell>web access</cell><cell>compilation</cell></row><row><cell>R7. GUI</cell><cell>complex</cell><cell>minimalistic</cell><cell>very complex</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Steffen Kirchhoff for his input. This work was partially supported by NSF grant OIA-1125087, the NIMH Silvio Conte Center (P50MH094271) and NIH grant 5R01NS076467-04.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno>31/03/2014</idno>
		<ptr target="http://openconnectome.github.io/Rambo3D" />
		<title level="m">Rambo3d</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<idno>31/03/2014</idno>
		<ptr target="http://brainiac2.mit.edu/SNEMI3D" />
		<title level="m">IEEE ISBI challenge: SNEMI3D -3D segmentation of neurites in EM images</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<idno>31/03/2014</idno>
		<ptr target="http://eyewire.org/" />
		<title level="m">Eyewire</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openseadragon</surname></persName>
		</author>
		<idno>31/03/2014</idno>
		<ptr target="http://openseadragon.github.io/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved registration for large electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akselrod-Ballin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. on Biomedical Imaging (ISBI &apos;09)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="434" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Viking Viewer for connectomics: Scalable multi-user annotation and summarization of large volume data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koshevoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Microscopy</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="28" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ConnectomeExplorer: Query-guided visual analysis of large volumetric neuroscience data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Awami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasthuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE SciVis 2013)</title>
		<meeting>IEEE SciVis 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2868" to="2877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring the connectome: Petascale volume visualization of microscopy data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Awami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasthuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="50" to="61" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerated volume rendering and tomographic reconstruction using texture mapping hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Volume Visualization</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TrakEM2 software for neural circuit reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schindelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Preibisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Longair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tomancak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hartenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">38011</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond the five-user assumption: Benefits of increased sample sizes in usability testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Faulkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="383" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reconstruct: A free editor for serial section microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Fiala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Microscopy</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The connectome viewer toolkit: An open source framework to manage, analyze, and visualize connectomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daducci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lemkaddem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hagmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Realtime visualization of the connectome in the browser using WebGL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E C</forename><surname>Calle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pienaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="issue">95</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DP2: Distributed 3D image segmentation using micro-labor workforce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Giuly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y.</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ellisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1359" to="1360" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive volume exploration of petascale microscopy data streams using a visualizationdriven virtual memory approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE SciVis&apos;12)</title>
		<meeting>IEEE SciVis&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Slice:drop: Collaborative medical imaging in the browser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2013 Comp. Anim. Festival</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neuroimaging in the browser using the X Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ahtam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pienaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Staveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Mental Workload</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="139" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-accuracy neurite reconstruction for high-throughput neuroanatomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helmstädter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Briggman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1081" to="1088" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Number of people required for usability evaluation: The 10+-2 rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salvendy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="130" to="133" />
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boundary learning by optimization with topological constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helmstädter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Briggman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mendenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasthuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hayworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR 2010</title>
		<meeting>IEEE CVPR 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2488" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janelia</forename><surname>Farm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raveler</surname></persName>
		</author>
		<idno>31/03/2014</idno>
		<ptr target="https://openwiki.janelia.org/wiki/display/flyem/Raveler" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tissuestack: A new way to view your imaging data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Waxenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ullmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Galloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A collaborative digital pathology system for multi-touch mobile and desktop computing platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Faulkner-Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Najarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="227" to="242" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SSECRETT and NeuroTrace: Interactive visualization and analysis tools for large-scale neuroscience datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vázquez-Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="70" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-automated neuron boundary detection and nonbranching process segmentation in electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jurrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Giuly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Paiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ellisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="29" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Large-scale automatic reconstruction of neuronal processes from electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kaynig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vázquez-Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Knowles-Barley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasthuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1303.7186</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>q-bio.NC</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">WebGL specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khronos</forename><surname>Group</surname></persName>
		</author>
		<idno>31/03/2014</idno>
		<ptr target="http://www.khronos.org/registry/webgl/specs" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mojo 2.0: Connectome annotation tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Knowles-Barley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasthuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="issue">60</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ontologybased visualization of hierarchical neuroanatomical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prohaska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rybak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Visual Computing for Biomedicine</title>
		<meeting>Visual Computing for Biomedicine</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The big and the small: Challenges of imaging the brain&apos;s circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="issue">6056</biblScope>
			<biblScope unit="page" from="618" to="623" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A technicolour approach to the connectome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Livet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Sanes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews. Neuroscience</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A modular hierarchical approach to 3D electron microscopy image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="88" to="102" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing the human connectome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Margulies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Böttger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Gorgolewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="445" to="461" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Machine learning of hierarchical clustering to segment 2D and 3D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Chklovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">71715</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph-based active learning of agglomeration (GALA): A python library to segment 2D and 3D neuroimages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">34</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Proof-editing is the bottleneck of 3D neuron reconstruction: The problem and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="103" to="105" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kaynig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Dercksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Roerdink</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.1428v2</idno>
		<title level="m">Visualization in connectomics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural process reconstruction from sparse user scribbles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vázquez-Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI 2011</title>
		<meeting>MICCAI 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="621" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CATMAID: collaborative annotation toolkit for massive amounts of image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hartenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tomančák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1984" to="1986" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">As-rigidas-possible mosaicking and serial section registration of large ssTEM datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hartenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tomančák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="57" to="63" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reading the book of memory: Sparse sampling versus dense mapping of connectomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="29" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Connectome: How the Brain&apos;s Wiring Makes Us Who We Are</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-02" />
			<pubPlace>Houghton Mifflin Harcourt</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiple hypothesis testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Shaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="561" to="584" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph abstraction for simplified proofreading of slice-based volume segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUROGRAPHICS Short Paper</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ilastik: Interactive learning and segmentation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Straehle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="230" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Segmentation fusion for connectomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vázquez-Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lichtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Cody</forename><surname>Hazlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
