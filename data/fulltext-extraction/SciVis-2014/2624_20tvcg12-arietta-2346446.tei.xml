<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">City Forensics: Using Visual Elements to Predict Non-Visual City Attributes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">M</forename><surname>Arietta</surname></persName>
							<email>sarietta@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
							<email>efros@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
							<email>ravir@cs.ucsd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
							<email>maneesh@eecs.berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">EECS Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<region>Berkeley</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CSE Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">City Forensics: Using Visual Elements to Predict Non-Visual City Attributes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2014.2346446</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Fig. 1: The violent crime rate in San Francisco is an example of a non-visual city attribute that is likely to have a strong relationship to visual appearance. Our method automatically computes a predictor that models this relationship, allowing us to predict violent crime rates from streetlevel images of the city. Across the city our predictor achieves 73% accuracy compared to ground truth. (columns 1 and 2, heatmaps run from red indicating a high violent crime rate to blue indicating a low violent crime rate). Specifically, our predictor models the relationship between visual elements (column 3), including fire escapes on fronts of buildings, high-density apartment windows, dilapidated convenience store signs, and unique roof style, relate to increased violent crime rates. Our predictor also identifies street-level images from San Francisco that have an unsafe visual appearance (column 4). Detections of visual elements are outlined in color.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A modern city is a massive and ceaseless information producer, constantly generating thousands of disparate pieces of localized information (e.g. housing prices, restaurant health inspection scores, crime statistics, precipitation levels, building code violations, water usage, etc.). Since each such city attribute is associated with a location (latitude, longitude) we typically visualize them as attribute maps. While such maps have long been used to analyze and understand cities, urban planners have recently started applying big data analysis and mining techniques to identify meaningful correlations between these mapped attributes (so called "correlation mining") <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b45">46]</ref>. These correlations can then be used to predict the value of one attribute given another when that attribute is not readily available. For instance, higher housing prices might predict higher health inspection scores and thereby reduce the number of restaurants inspected in those areas.</p><p>However, there is one type of city data that has received far less attention from the urban planning and data mining communities -visual appearance. That is, how does the visual appearance of a city relate to its other, often non-visual, attributes? We might, for example, speculate that more trees and greenery in a neighborhood imply higher housing prices. Indeed, sociologists have proposed the Broken Windows Theory <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48]</ref>, which suggests that visual evidence of neighborhood decay (e.g broken glass, graffiti, trash, etc.) correlates with increased levels of crime. With the widespread availability of street-level imagery (Google StreetView, Bing Streetside, etc.) we now have the data necessary to identify and validate such predictive relationships. But manually analyzing all street-level images at the scale of an entire city is impractical. Even today there has not been any large, city-scale verification of the Broken Windows Theory.</p><p>In this paper, we take a first step towards automatically identifying and validating predictive relationships between the visual appearance   <ref type="figure">(Figure 1</ref>) generalizes to predict the violent crime rate in Oakland with 64% accuracy. Our predictors can leverage this generalizability to predict city attributes in places where they may not be readily available. Street-level images (bottom row) are from Oakland, but show detections of visual elements from San Francisco <ref type="figure">(Figure 1</ref>). of a city and its location-specific attributes. Our approach builds on the work of Doersch et al. <ref type="bibr" target="#b12">[13]</ref> who recently introduced a method for identifying the visual elements of a city that differentiate it from other cities. However their work focuses on binary classification based only on geographic location (e.g. Paris vs. not Paris). Moreover, their method cannot directly compute the relationship between visual elements and real-valued non-visual city attributes.</p><p>Our insight is that given a non-visual city attribute and a training set of street-level images, we can extract a set of visual elements that are discriminative of the attribute. We then train a predictor that learns a set of weights over the discriminative visual elements using non-linear Support Vector Regression <ref type="bibr" target="#b42">[43]</ref>, a robust regression technique. Our predictor can then take any street-level image as input and produce an estimate of the attribute value as output. Applying the predictor across the street-level images of an entire city allows us to analyze the relationship between visual elements and non-visual city attributes, in a process we call city forensics.</p><p>Although certain city attributes like the presence of trees and graffiti have a natural connection to visual elements, more abstract, nonvisual attributes such as crime rates and housing prices relate to visual appearance in a much more complicated, non-linear way. We show our method is indeed able to discover relationships between visual elements and some of these attributes and that this relationship is general enough to predict these attributes in new cities. For example, our system finds that for San Francisco, visual elements such as fire escapes on fronts of buildings, high-density apartment windows, dilapidated convenience store signs, and a unique roof style predict violent crime rates with 73% accuracy <ref type="figure">(Figure 1</ref>). Moreover, the predictor trained for San Francisco can also predict violent crime rates for Oakland with 64% accuracy ( <ref type="figure" target="#fig_1">Figure 2</ref>). We obtain similar prediction results for a variety of other attributes including: theft rates, housing prices, population density, tree presence, graffiti presence, and the perception of danger.</p><p>To build predictors that can model the relationship between visual appearance and city attributes at the scale of a city we have developed a scalable distributed processing framework that can efficiently handle several terabytes of street-level image data. Our framework speeds up the main computational bottleneck (extracting visual elements) by an order of magnitude and requires about 3.75 hours to build a predictor for one attribute. The efficiency of our framework allows us to quickly investigate a diverse set of city attributes.</p><p>In some cases our predictions do not agree exactly with ground truth data. These discrepancies are often where we gain the most interesting insights about the connection between non-visual city attributes and visual appearance. We analyze several of these cases in Section 5. Despite these differences we show that our method enables several novel applications:</p><p>Neighborhood visual boundaries. We identify the visual boundary of neighborhoods from a set of sparse user-provided labels of areas inside and outside the neighborhood.</p><p>Attribute-sensitive wayfinding. We generate walking paths through the city that either maximize or minimize a user's exposure to a particular attribute. For example, a user might generate a path that passes through an area of high housing values.</p><p>Validating user-specified visual elements for prediction. We let users specify meaningful visual elements and then check how well those elements predict city attributes. For instance, users might test whether a visual element representing bars on windows is predictive of high crime areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Relationships between visual appearance and non-visual city attributes have been studied in a number of fields including urban planning, sociology and epidemiology. While Broken Windows Theory <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48]</ref>, which suggests that neighborhood decay correlates with higher crime rates, is a well-known example, others have examined how city appearance relates to attributes like obesity rates <ref type="bibr" target="#b13">[14]</ref>, cases of depression <ref type="bibr" target="#b25">[26]</ref>, and sexually transmitted disease rates <ref type="bibr" target="#b9">[10]</ref>. Although researchers have shown that relationships between visual appearance and city attributes exist, they have had to rely on manual inspection either by the authors themselves or more recently via crowdsourcing <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36]</ref>. In contrast, our method relies only on a set of measurements of a city attribute and an associated set of street-level images.</p><p>Researchers have also analyzed the relationship between discriminative visual elements and the unique visual appearance of cities. For instance, Doersch et al. <ref type="bibr" target="#b12">[13]</ref> find the distinctive visual aspects of Paris compared to London. Fouhey et al. <ref type="bibr" target="#b16">[17]</ref> use a similar approach for computing visual elements for indoor scenes. Our approach also builds on Doersch et al. <ref type="bibr" target="#b12">[13]</ref>, but relates the visual elements to nonvisual city attributes.</p><p>Several other works this year, all developed concurrently, consider the relationship between the visual appearance of cities and city attributes such as wealth, safety, beauty, quiet and proximity to specific types of landmarks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23]</ref>, as well as the relationship between city appearance and city identity <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b2">3]</ref>. All these methods operate on the scale of an entire street-level image (or panorama), whereas we aim to model the visual appearance of a city at a finer, image patch scale. This gives us the advantage of being able to visualize the specific parts of the image that are most related to a given attribute.</p><p>Several techniques use image-based data sources other than streetlevel panoramas to detect physical attributes of cities. Aerial and street-level LIDAR has been used to detect trees <ref type="bibr" target="#b38">[39]</ref>, water and ground coverage <ref type="bibr" target="#b6">[7]</ref>, roads <ref type="bibr" target="#b5">[6]</ref>, and buildings <ref type="bibr" target="#b36">[37]</ref>. Video is commonly used to categorize traffic in cities <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref> as well as to track crowds for the purposes of detecting flow patterns, anomalous behavior, etc <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>. Although these techniques often produce very accurate results, the availability of these sources of image data is a limiting factor in most cities. In contrast our method relies on street-level images, one of the most ubiquitous and rapidly growing sources of data available today.</p><p>Our prediction algorithm uses non-linear Support Vector Regression <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref> which has been applied in a number of different fields including graphics for 3D pose estimation <ref type="bibr" target="#b1">[2]</ref> and image restoration <ref type="bibr" target="#b27">[28]</ref>. It has also been used to learn how visual style changes over time <ref type="bibr" target="#b26">[27]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Our goal is to construct a predictor that can estimate the value of a non-visual city attribute based on visual appearance. Given a set of measured (location, attribute-value) pairs and a set of (location, streetside panorama) pairs, we build predictors in three steps:</p><p>1. We spatially interpolate the input (location, attribute-value) data to obtain attribute values over the entire city.</p><p>2. We modify the technique of Doersch et al. <ref type="bibr" target="#b12">[13]</ref> to build a bank of Support Vector Machines (SVMs) <ref type="bibr" target="#b4">[5]</ref> that detect visual elements in the panoramas that are discriminative of the attribute.</p><p>3. We build an attribute predictor from the resulting bank of SVMs using Support Vector Regression (SVR) <ref type="bibr" target="#b42">[43]</ref>.</p><p>We consider each of these steps in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interpolating Non-Visual City Attribute Values</head><p>Although our input data consists of (location, attribute-value) pairs and (location, panorama) pairs, the locations of the attribute values may not coincide with the locations of the panoramas. But, in order to train the visual element detector SVMs and then train the attribute predictor we require a set of locations for which both the attribute-values and panoramas are available. Thus, we use radial basis functions (RBF) <ref type="bibr" target="#b30">[31]</ref> to spatially interpolate the input set of (location, attributevalue) data. Specifically we use the inverse multiquadric function</p><formula xml:id="formula_0">f (r) = 1 q 1 + (er) 2<label>(1)</label></formula><p>as the radial basis function, where r is the Euclidean distance between locations and e is a parameter that controls falloff. We have found that this basis function produces a smooth interpolation and is relatively insensitive to e compared to other basis functions. We have found that setting e = 2 works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing the Visual Element Detectors</head><p>In this work we model visual appearance using visual elements. We define a visual element as a set of image patches that have a visually consistent appearance. In <ref type="figure" target="#fig_2">Figure 3</ref> for instance, the visual elements predictive of the housing prices city attribute in San Francisco includes rows of image patches containing hedges (top) gable roofs (middle) and tropical plants (bottom). Doersch et al. <ref type="bibr" target="#b12">[13]</ref> recently presented a method for constructing a bank of SVMs that can detect visual elements that are discriminative of a binary positive/negative location-based attribute (e.g. in-Paris vs. not-in-Paris) based on the method of Singh et al. <ref type="bibr" target="#b40">[41]</ref>. To train the SVMs they require a set of locations and their associated street-level panoramas that are labeled as positives and another set labeled as negatives. To apply their method on our continuous-valued city attribute data we first threshold our attributes to form a positive set and a negative set based on the mean and standard deviation of the attribute values. For instance, if our attribute is housing price we might define positive/negative sets as locations (and corresponding panoramas)</p><p>where the attribute value is greater/less than one standard deviation above/below the mean housing price. Doersch et al. have shown that a set of about 10,000 streetlevel images with 2,000 positives and 8,000 negatives is sufficient to train SVMs that can detect discriminative visual elements. The positive/negative imbalance is meant to ensure there are enough negative examples to cover the larger variation of visual appearance that may occur in a negative set. To build the positive/negative sets for our continuous-valued attributes we sample the (location, attributevalue) pairs using the RBF interpolated attributes (Section 3.1) as the sampling distribution. This sampling strategy ensures that positive samples are likely to be drawn from the locations where the attribute is most positive and the same for the negative samples. We use the inverse cumulative distribution sampling method <ref type="bibr" target="#b7">[8]</ref> to generate these samples. Finally, we associate the closest street-level panorama with each (location, attribute-value) sample. However, if the nearest panorama is more than 5 feet from the sample location we reject the sample. The result of this process is a (location, attribute-value, panorama) triplet for each sample in our training set.</p><p>To build the SVMs we must first generate image features for each of the panoramas in the training set. We extract the image features from perspective projections of Google StreetView panoramas. However, each such panorama provides a complete 360 • field of view and projecting such a wide field-of view panorama into a rectangular image will introduce huge distortions. Therefore we project each panorama at 20 • intervals across the entire azimuthal range from 0 • to 360 • and from −10 • to 30 • in the elevation angle also in steps of 20 <ref type="bibr">•</ref> . In all of our experiments we used a field of view of 30 • . This gives us an overlap of 10 • between successive projections maximizing the chance that every object appears whole in at least one projection. We then scale each projection to 400x400 and for each patch, over multiple scales, we extract a single image feature using the HOG+color <ref type="bibr" target="#b10">[11]</ref> descriptor similar to Doersch et al. This produces images features of dimension 2112.</p><p>We briefly summarize the steps we perform to build the bank of SVMs for detecting discriminative visual elements from the training data (see Doersch et al. <ref type="bibr" target="#b12">[13]</ref> for complete details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Extract the set of image features in each panorama projection in</head><p>the positive and negative sets.</p><p>2. Randomly sample image features from the positive set and compute their 100 nearest neighbors across both the positive and negative sets.</p><p>3. Compute an SVM for each nearest neighbor set that separates the nearest neighbor set from all other image features in the positive and negative sets.</p><p>4. Refine the SVMs using three iterations of the hard negative mining technique of Felzenszwalb et al. <ref type="bibr" target="#b14">[15]</ref>.</p><p>We sort the resulting SVM visual element detectors based on how well they can discriminate the positive set from the negative set and keep the top K, where K = 100 in our implementation. Given an image feature extracted from a panorama, each SVM produces a continuouslyvalued score for that feature. Positive scores indicate that the image feature is similar to the visual element that SVM is designed to detect. Note that the SVMs do not predict attribute values directly, they can only determine whether a visual element is present in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computing the Predictor</head><p>City attributes are most likely represented by a combination of many visual elements, not merely the presence or absence of a single visual element, so a direct application of the method of Doersch et al. <ref type="bibr" target="#b12">[13]</ref> is not possible. Therefore we build our predictors by learning a set of weights over the set of scores produced by our bank of visual element SVM detectors. The simplest approach for learning these weights is to build a linear regressor between the SVM scores and the corresponding attribute values. But in practice we have found that the relationship between SVM scores and attribute values is rarely linear. Instead, we use the more flexible technique of non-linear Support Vector Regression (SVR) <ref type="bibr" target="#b42">[43]</ref>. In SVR, the goal is to find a function that approximates a set of training outputs y i given a set of training input vectors x i . In our case, given a training set of (location, attribute-value, panorama projection) triplets we treat the attribute-value as the output y i and we treat the SVM detector scores as the input x i . Specifically for each triplet in the training set we build a score vector for each panorama as follows:</p><p>1. We extract a set of 7,700 image features from each panorama projection (every projection is the same size and thus produces the same number of features).</p><p>2. We compute SVM scores for each of the resulting image features using the bank of 100 SVMs we constructed in Section 3.2.</p><p>3. We retain the top 3 detection scores that each of the 100 SVMs was able to achieve across all of the image features in the panorama.</p><p>This procedure leaves us with a 300-element SVM score vector for each panorama, which we treat as the input vector x i . SVR parameterizes the functions that approximate the y i 's given the</p><formula xml:id="formula_1">x i 's as f (x i ) = w • f (x i ) + b, where f (x i )</formula><p>is a (possibly non-linear) map on the x i 's, w is a set of weights and b is bias term. The goal of SVR is to learn the parameters (w, b) that are as compact as possible while minimizing the loss of a particular parameterization (w, b) in predicting the training outputs y i . This loss is defined as:</p><formula xml:id="formula_2">L e (y i , (x i , b)) = max (|y i − f (x i ) | − e, 0)<label>(2)</label></formula><p>where e controls the magnitude of error that we can tolerate in our fitted model. Any y i that are within a distance e of the fitted model are considered equally well approximated. We set e = 0.1 in all of our experiments. A compact model is defined as having the fewest number of degrees of freedom in the feature space as possible. Hence, the objective in SVR is to minimize the L 2 norm of w subject to the constraints imposed by the loss function. An important detail in SVR is the selection of the map f , which transforms the input points into a higher dimensional space enabling non-linear regression. This map f , is usually defined with respect to a kernel function K � x i , x j � that operates on pairs of training inputs. We considered three different forms of K:</p><formula xml:id="formula_3">Linear: K � x i , x j � = x T i x j Polynomial: K � x i , x j � = ⇣ gx T i x j + r ⌘ d Gaussian: K � x i , x j � = exp ⇣ −g||x i − x j || 2 ⌘</formula><p>All of our experiments use the Gaussian kernel with g equal to 1/D, where D is the number of dimensions of the training vectors x i . In our case D = 300 -the size of the SVM score vectors. The resulting predictor is designed to take any street-level panoramic image as input, compute its SVM score vector and then estimate the corresponding attribute value. We use libsvm <ref type="bibr" target="#b8">[9]</ref> with its default settings to apply the SVR and to compute predictions for new images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VISUAL PROCESSING FRAMEWORK</head><p>Computing the support vector machines (SVM) via the method described in Section 3.2 requires performing a number of compute-and data-intensive operations. In our experiments the input to the algorithm is a set of 10,000 Google StreetView panorama projections (2,000 positives, 8,000 negatives), each of which is 640x640 pixels. Every image contributes 7,700 image features, each of which is comprised of 2112 floating-point numbers resulting in about 650GB of image data that we must process. In addition, the hard negative mining algorithm required to build the bank of SVMs performs several computation iterations before converging, each of which can require upwards of 26GB to be processed and produces around 2GB of new data.</p><p>To compute predictions of city attributes in new cities we apply a bank of 100 SVMs and compute detections for all of the associated visual elements in every street-level panorama projection in the city. The number of panoramas in a single city varies from 30,000 to 170,000, each of which has a resolution of 5324x2867, resulting in up to 2.5TB worth of data that must be processed.</p><p>In order to efficiently train the bank of SVMs that detect visual elements and compute predictions of city attributes across an entire city efficiently, we developed a distributed visual processing framework. We implemented our framework in C++ and we use the C Message Passing Interface (MPI) library <ref type="bibr" target="#b18">[19]</ref> as our communication interface. This provides a considerable efficiency improvement over the implementation of Doersch et al. <ref type="bibr" target="#b12">[13]</ref> which uses MAT-LAB and relies on a shared filesystem for communication. Our framework assumes that all inputs and outputs are expressed as matrices and that the rows/columns of an output matrix are updated only once. These assumptions enable a number of implementation features:</p><p>Scheduling. Our restriction that all variables must be expressed as matrices allows us to split inputs by columns or rows and iteratively process them on different compute nodes in parallel. When the output of a node is available, the system updates the output matrix accordingly and requeues the node with a new set of input rows/columns. By executing jobs iteratively in this manner, slower nodes receive fewer rows/columns and inefficient or malfunctioning nodes can be removed altogether. Thus, the total execution time of a job is not bound by the slowest node.</p><p>Checkpointing. Output matrices that are partially completed are periodically saved to disk and can be reloaded if a failed job is restarted.</p><p>Caching. Variables that need to be repeatedly sent to processing nodes within a single job and across jobs can be flagged to be cached on the nodes' local filesystems. For large data that remains the same across jobs or across a single job (e.g. a list of all of the images being processed with their associated metadata), the framework can instruct the nodes to load the variables from their local cache rather than re-transferring them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial variables.</head><p>The assumption that the rows/columns of an output matrix are updated only once enables us to significantly reduce the memory footprint of our jobs. We save partially completed blocks of large output matrices to disk as they finish, which avoids having to keep them in memory.</p><p>We have deployed our framework on a local heterogeneous 38 core cluster, the Texas Advanced Computing Center supercomputers Lonestar and Stampede, and on Amazon's Elastic Compute Cloud. With our efficiency features, our framework can compute the SVMs via the visual element extraction technique (Section 3.2) for a single city attribute in about 3.75 hours on the Stampede supercomputer using 48 effective cores (181 CPU hours). This is a 9.9x speedup compared Doersch et al. <ref type="bibr" target="#b12">[13]</ref>. This speedup allows us to apply our city forensics data-mining approach to analyze a diverse set of city attributes and check for correlations with visual appearance. We can compute predictions over an entire city of 170,000 panoramas (the largest set we have) in 272 CPU hours, or about 1.7 hours on the Stampede supercomputer using 160 effective cores. Our framework has been released and is available online at http://github.com/ucb-silicon/ cesium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We analyzed the performance of our city attribute predictors in estimating the value of the following city attributes in 6 American cities where available:  <ref type="figure">Fig. 4</ref>: The ROC curves and the area under those curves for the housing price attribute show an interesting phenomenon. Although Boston and Philadelphia have a similar visual appearance related to housing prices, San Francisco does not generalize well to these cities.</p><p>• Violent crime rate. We used the past year's worth of data from</p><p>CrimeMapping.com or CrimeReports.com (depending on availability) of occurrences of assault, homicide, or robbery. We also consider the violent crime rate normalized by population density.</p><p>• Theft rate. We used the past year's worth of data from CrimeMapping.com and CrimeReports.com of occurrences of theft (does not include robbery). We also consider the theft rate normalized by population density.</p><p>• Housing prices. We used prices of homes sold in the past three years according to the online real estate company Zillow.</p><p>• Population density. We used population estimates from the 2010 Census normalized by the area of the associated Census Block.</p><p>• Tree presence. We used locations where people noted trees larger than 70 centimeters via the website UrbanForestMap.org, which was only available for San Francisco.</p><p>• Graffiti presence. We used reports of graffiti submitted to the 311 websites of San Francisco, Chicago, and Boston.</p><p>• Perception of danger. We deployed a Mechanical Turk experiment asking workers to examine a series of 15 Google StreetView panoramas from San Francisco, Chicago, or Boston and "decide based on the image alone whether [they] would feel safe in the area or not at any time of day." We tested a total of about 500 panoramas per city. For each image we averaged the responses of 10 workers to determine the attribute value.</p><p>Many of these sources do not provide real-valued attribute data. Crime data, for example, is only available as individual occurrences of theft, robbery, homicide, etc. To convert these discrete measurements into continuous rates we use the interpolation scheme described in Section 3.1. This step is necessary for violent crime, theft, tree presence, and graffiti presence.</p><p>In all cases where data was available we trained predictors in San Francisco, Chicago, and Boston and tested the predictors in San Francisco, Chicago, Boston, Oakland, Philadelphia, and Seattle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of Prediction Accuracy</head><p>Figures 4-6 present quantitative accuracy results for our attribute predictors. Each entry in the tables represents the area under the receiver operating characteristic (ROC) curve for a predictor trained in one city (rows) and tested in another city (columns). Additional ROC curves and their associated tables are available in the supplemental materials. Note that when the training and test cities are the same we ensure that the set of (location, attribute-value, panorama) triplets used for testing are completely disjoint from the sets of triplets used to train the visual element detectors (Section 3.2) as well as the sets of triplets used in the SVR training (Section 3.3).  <ref type="figure">Fig. 5</ref>: For the theft rate attribute we compare the accuracy of our predictors to human predictions obtained via Mechanical Turk. We showed workers panoramas of San Francisco, Chicago, and Boston and asked whether they "would feel safe at all times of the day". On average our theft rate predictors are about 33% more accurate than humans. We also compare to population-normalized theft rates, which perform 5%-10% better in most cases. The increase in accuracy is a result of factoring out the correlation between theft rates and population density. The ROC curve is defined as the relationship between the true positive rate (TPR) and the false positive rate (FPR) of a binary classifier. A value of 0.5 indicates that the classifier is not discriminative at all i.e. its accuracy is equivalent to that of a random classifier, regardless of the prior distribution on positives and negatives. Anything between 0.5 the maximum area of 1.0 is considered discriminative <ref type="bibr" target="#b19">[20]</ref>. To compute the area under the ROC curves for our predictors we first build ground truth estimates for each of the attributes the same way we generate the positive and negative sets when we construct visual element detectors (Section 3.2). We convert our real-valued attribute predictions into binary classifications and compare to ground truth while varying a positive/negative cutoff threshold. To produce the curves we varied this threshold from 0 to 1 and computed the TPR versus FPR for each threshold value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BOS Thefts</head><note type="other">SF Thefts CHI Thefts 0</note><p>For most of the city attributes we tested, the area under the ROC curves for the intra-city predictors (diagonals in tables) was above 63% with several of the predictors achieving above 77% accuracy. The <ref type="figure">Fig. 7</ref>: The visual elements that are predictive of graffiti in Chicago do not contain actual examples of graffiti. Since we train our predictor on reports of graffiti, our predictors are modeling the relationship between visual appearance and the likelihood that graffiti will be present, not graffiti itself.</p><p>best result was for housing prices in Boston, which was 82% accurate <ref type="figure">(Figure 4)</ref>. The only intra-city predictor to perform worse than 63% was the graffiti predictor for Chicago which attained only 56% accuracy. Although there is a strong relationship between visual appearance and actual graffiti, the training data we provided to the system does not necessarily contain examples of actual graffiti -it only contains locations where graffiti has been reported. <ref type="figure">Figure 7</ref> shows the visual elements for the graffiti predictor trained in Chicago. None of these visual elements contain examples of graffiti. Rather the visual elements capture the appearance of areas where graffiti is likely to be observed (e.g. brick walls, window styles associated with specific neighborhoods, and street lights). Instead we are learning a relationship between visual appearance and locations where graffiti is likely to be found, and we find that this relationship not very predictive. That is, in Chicago graffiti occurs in many visually diverse places.</p><p>While the results of the cross-city predictions show a range of performance, more than half of the predictors are more than 60% accurate. Some notable results are the San Francisco population density predictor in Philadelphia (82%), the Chicago theft predictor in San Francisco (76%), and the Boston housing price predictor in Philadelphia (72%). The last case <ref type="figure">(Figure 4)</ref> illustrates an interesting phenomenon. Boston and Philadelphia are spatially close and share a similar architectural history, so it is not surprising that a housing prices predictor trained in Boston performs well in Philadelphia. However, the accuracy of the San Francisco housing prices predictor is relatively low in Boston and Philadelphia, having approximately the same accuracy as a random predictor. This result also makes sense considering the large differences in their respective histories and architectural styles. In fact, brick buildings, which are indicative of high housing prices in Philadelphia and Boston <ref type="figure">(Figure 8</ref>), are no longer allowed to be built in San Francisco due to earthquake concerns. Visual elements for all the attributes we analyzed are available in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalized versus Non-Normalized Crime Rate Predictors.</head><p>Normalized crime rate predictors relate visual appearance to the likelihood that an individual will be a victim of crime. In contrast, non-normalized crime rate predictors relate visual appearance to the likelihood of a crime occurring. Thus, without normalization, the resulting predictors are correlated with population density; there are more crimes where more people live. When we factor out the population density for violent crime rates and theft rates, we see a 5%-10% increase in the accuracy of our predictors in almost all cities ( <ref type="figure">Figures 5, 6</ref>).</p><p>Comparisons to Human Prediction Accuracy. We compare the accuracy of our theft rate predictors to human performance using the data we collected using Mechanical Turk on the perception of danger. As shown in <ref type="figure">Figure 5</ref> we compare the human danger ratings to the ground truth for theft rates to produce the yellow ROC curves. For San Francisco, Chicago, and Boston Mechanical Turk workers could predict theft rates with 26%-38% accuracy, whereas our predictors are 63%-80% accurate in these cities. On average our predictors outperform humans by 33%.</p><p>One potential reason for the low accuracies of some of our predictors is that ground truth attribute data may be incorrect. For example data collected about the presence of trees may be out of date. To assess such inconsistencies, we evaluated the performance of our predictions on the presence of trees in San Francisco with respect to our ground truth data and to data generated by humans. We asked Mechanical Turk workers and Facebook friends to decide whether a set of streetlevel images contained "at least 50% of a single tree". We tested 1000 images of San Francisco and an average of 10 people marked each image. Our prediction accuracy compared to our ground truth was 75%, but increased to 81% when we considered the human-derived ground truth. (See supplemental materials for a comparison of the ROC curves.) Unfortunately determining ground truth for non-visual city attributes is a difficult process as humans are often inaccurate at predicting such attributes from visual appearance as we discovered in the comparison of our theft predictor to human perception of danger ( <ref type="figure">Figure 5</ref>). <ref type="figure" target="#fig_1">Figures 1, 2 and 8</ref> show how we can use our predictors to generate predicted attribute maps for an entire city. The first column shows our interpolated predictions using a predictor trained in the city noted in parentheses. The second column shows a map of the ground truth attribute values for the test city. We show some of the visual elements that are related to the attributes in column 3. The final column shows some example panoramas from the testing cities corresponding to locations denoted in our prediction maps in column 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prediction Maps</head><p>While our predicted attribute maps do not always agree with the interpolated ground truth maps, there is significant overlap. Some of the inconsistencies between the maps are a by-product of the assumption that visual appearance is always related to non-visual attributes. For instance, in the Philadelphia housing price maps (second row in <ref type="figure">Figure 8)</ref> there is a large section near the 2 marker that we have predicted to have low housing prices, but ground truth indicates this is an area of high housing prices. Indeed the corresponding panorama does not have the visual appearance of a high housing price area. The discrepancy between our predictions and ground truth likely occurs because there is not a clear link between visual appearance and housing prices in this case.</p><p>Another interesting example we found is for theft rates in Chicago (top row). Although both of the panoramas from the marked areas exhibit a relatively non-threatening appearance and are from shopping districts in downtown Chicago, our San Francisco predictor indicates the theft rate is high. While at first these panoramas do not seem to have the visual appearance of high theft rate areas, the ground truth data indicates that our predictions are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Deep Convolutional Neural Network Features</head><p>Recent work has shown that classification and recognition algorithms that use features extracted from a deep convolutional neural network can outperform HOG features <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b17">18]</ref>. We have conducted initial experiments replacing the HOG+color features described in Section 3 with deep convolutional features generated using Caffe <ref type="bibr" target="#b21">[22]</ref>. Specifically, we used the fifth convolutional layer of the Caffe ImageNet model as our features. Each such feature contains 2304 dimensions. The rest of the algorithm -multi-scale sliding window feature extraction, SVM training, etc -was kept exactly the same. <ref type="figure">Figure 9</ref> shows that the Caffe features produce visual elements that capture higher-level aspects of visual appearance, but are less visually consistent than those generated using HOG+color features. For instance, the image patches for the first visual element (top row) for San Francisco violent crime rate (normalized by population) are all instances of signs but vary in color, orientation, and font. Likewise, the patches for the second visual element (bottom row) for San Francisco housing prices contain white buildings with tall slender windows in various orientations, but sometimes contain greenery. Finally, the first visual element (top row) for reports of graffiti in San Francisco is comprised of textured image patches some of which are examples of graffiti (patches 4, 5, 7). As shown in <ref type="figure" target="#fig_2">Figures 1, 3, 7, 8, 10, 11</ref>   <ref type="figure">Fig. 8</ref>: Our predictors are general enough to predict the value of city attributes in test cities using predictors computed in training cities. Our method closely matches the ground truth attribute values in the test cities in most cases. Our predictors detect a set of visual elements in each panorama in the test cities and combine the detection scores of these elements into a single prediction of the attribute value. Errors can occur when the value of a city attribute is not directly related to visual appearance as in the case of housing prices in Philadelphia (second row). In this case the panorama in Philadelphia at location 2 does not have the visual appearance of a high housing price area, but the ground truth indicates that it is. the visual elements generated using HOG+color features are more visually consistent, but capture less of the semantics of city appearance. Quantitatively comparing the accuracy of the Caffe features to HOG+color features we obtain mixed results. For instance, while the San Francisco housing prices predictor trained with Caffe features is 14% better (63.8% versus 49.5%) in Chicago, the graffiti predictor with Caffe features predicts reports of graffiti in Chicago with 9.6% less accuracy (47.4% versus 56.9%). In ongoing work we are exploring techniques to combine the advantages of both types of features to take advantage of the strengths of each one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICATIONS</head><p>We have implemented three applications that use our predictors to provide estimates of city attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Defining Visual Boundaries of Neighborhoods</head><p>City neighborhoods often have a unique visual appearance. Although city governments impose their own definitions of the boundaries of neighborhoods, understanding the visual boundary of a neighborhood is often useful for tourists, marketers, prospective home buyers, etc. We developed a prototype application that can determine the visual boundary of neighborhoods from a sparse set of user inputs.</p><p>We require a user to "paint" regions of a city defining areas that are definitely part of a neighborhood and areas that are definitely not. For instance, in <ref type="figure">Figure 10</ref> a user has painted an area of San Francisco inside the Mission neighborhood (red) and an area outside the Mission (blue). We use the paint strokes to define the positive and negative sets that are used to train our predictors. To determine the visual boundary of the neighborhood, we compute predictions on every panorama between the two user-defined paint strokes (first column in <ref type="figure">Figure 10</ref>). Note that the visual elements used by the predictor (third column in <ref type="figure">Figure 10</ref>) capture the unique visual appearance of the Mission including, Victorian homes with bayview-style windows, metal gates and bars on the fronts of homes, dilapidated signs, and bike lanes.</p><p>Our boundaries show better agreement with the actual appearance of the Mission neighborhood than the "ground truth" boundary, which we computed as the union of the boundaries defined by the San Francisco local government, realtor association, and from the analysis of Wahl and Wilde <ref type="bibr" target="#b46">[47]</ref>. For instance, in the region marked 1 in both maps, the corresponding panorama (top right) looks much more industrial than the classic Victorian/Edwardian look of the Mission (bottom right). Our prediction map correctly leaves the location outside the boundary of the Mission, whereas the traditional ground truth neighborhood map includes it.</p><p>Personalized Attribute Explorer. Although in this example we showed a user specifying regions that define a neighborhood, this idea can be generalized. Given a set of user-defined locations in a city, we can generate a personalized attribute predictor that would enable the user to find other locations with a similar visual appearance. We believe a system like this would be useful for exploring new areas of cities without forcing users to verbally define exactly what they find visually interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Attribute-Sensitive Wayfinding</head><p>Traditional navigation systems compute routes between two userspecified locations in a city that minimize the total distance traveled. But specific user groups such as tourists, urban hikers, and consumers often may want to compute routes between locations in a city that avoid or seek out certain city attributes, rather than finding the shortest/fastest route. For instance, a tourist who is unfamiliar with the downtown area of Chicago might want to avoid areas of high theft rates while walking between two locations in the area even if that means walking for a slightly longer period of time. Given a user-specified city attribute and a city to compute routes in, we first model the city's connectivity as a set of nodes in a graph, where each node corresponds to a single street-level panorama in the city and is connected to all other nodes that can be reached via the actual road network. We treat each prediction of the city attribute as a weight on the corresponding node and use Dijkstra's shortest path algorithm <ref type="bibr" target="#b11">[12]</ref> to compute the attribute-sensitive route in realtime as the user adjusts the importance of avoiding or seeking out the attribute.</p><p>In <ref type="figure">Figure 11</ref> we show an example of our system being used to generate a walking route that avoids high theft rates in downtown Chicago. A typical route (green) simply optimizes for the total travel distance, but passes through an area where a large number of thefts have been reported (red circles in second column). Our predictions correctly avoid this path and instead route the user through a lower theft rate area. In this case, we used a predictor of theft rates trained in San Francisco to compute the predictions, highlighting the novelty of our predictors' ability to generalize to cities not used during training.</p><p>In the last two columns of <ref type="figure">Figure 11</ref> we see that the predictor is avoiding the green path because it passes through an area with highdensity buildings and areas where people are likely to be passing through quickly (parking meters indicate no long-term visits). In contrast, our theft-avoiding route passes through less dense areas of the downtown area where there are parks and wide-open spaces; places where successfully committing a theft is less practical.</p><p>It is possible to combine this approach with the Personalized Attribute Explorer discussed at the end of Section 6.1 to create a system for finding routes that pass through visually interesting areas as defined by a user. For instance, a user might paint strokes in areas that he/she enjoys walking through, without having to specify directly what about those areas is visually interesting. If there is a consistent visual appearance in those areas, the resulting predictor could be used to find other routes in the same city or routes in new cities that expose him/her to the same visual appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Validating Visual Elements for Prediction</head><p>Despite the numerous ways researchers have manually analyzed the relationships between city attributes and visual appearance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>, there is currently no automatic way to answer simple questions like: "Does (blank) discriminate the city attribute (blank)?". For instance, the top row of <ref type="figure">Figure 8</ref> indicates that things like highdensity windows, drab windows, and parking meters are predictive of theft, but our initial intuition was that visual elements like "bars on windows" would be more predictive.</p><p>In the top left column of <ref type="figure" target="#fig_1">Figure 12</ref> we show 3 images containing examples of bars on windows and 3 images containing examples of highdensity windows that we hand-picked from Google StreetView images of San Francisco. We modified our system to use patches from these images as initial seeds for the visual elements rather than randomly sampling patches from images of high theft-rate locations. Apart from this modification, the predictor was trained with respect to the theft rate attribute in San Francisco using the same method described in Section 3.</p><p>To determine whether a user-defined visual element is discriminative of an attribute two conditions must be met: (1) the predictor must converge on visual elements that detect patches which are visually consistent with the seed set and (2) a high percentage of the detected patches must come from the positive test set. <ref type="figure" target="#fig_1">Figure 12</ref> (middle column) shows the top 3 resulting visual elements for both sets of seed images. In both cases the visual elements satisfy condition (1); the detections of the visual elements have a consistent visual appearance with the seeds images. The plot on the far right of <ref type="figure" target="#fig_1">Figure 12</ref> shows the percentage of detections of each visual element that are from the positive set (i.e. from panoramas where a theft occurred). Many of the top detections of the "bars on windows" visual elements are from the negative set, which violates condition (2). Since the plots for the "high-density windows" visual elements show a consistently higher percentage of detections from the positive set, they are more predictive of theft rates than the "bars on windows" visual elements.</p><p>This allows us to interactively verify our intuitions about what is visually predictive of city attributes in a straightforward way. It also has the potential to assist researchers in validating their work. Efforts like the PlacePulse study <ref type="bibr" target="#b37">[38]</ref> could use this system to validate whether human perception of safety is indeed visually predictive of actual safety by providing examples to our application of perceived safe locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>We present a method for automatically computing predictors of nonvisual city attributes based on visual appearance. We model visual appearance using visual elements that are discriminative of the city attributes. We show that there is indeed a predictive relationship between visual appearance and non-visual city attributes in a number of cases. We also present three applications that use our predictors to provide estimates of city attributes, demonstrating the applicability of this work.</p><p>We imagine our predictors being used to perform what we call city forensics. That is, we envision users investigating abstract non-visual city attributes in more intuitive visual ways via the predictive relationships we are able to uncover. We also believe that our predictors could be used to synthesize new content that has the visual appearance of city attributes, which would be useful in areas like procedural city generation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> and city simulation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref>.  <ref type="figure">Fig. 10</ref>: Given a user-specified set of regions that are definitely part of the Mission district of San Francisco (red strokes) and regions that are definitely not part of if (blue strokes), our system automatically determines the visual boundary of the Mission. In addition, we can describe what the visual appearance of the Mission is. For instance, we find that bayview windows, metal grates on doors and windows, dilapidated signs, and bike lanes are all visually discriminative of the Mission. Our predicted boundary agrees more closely with the actual visual boundary of the Mission compared to the "ground truth" boundary.  <ref type="figure" target="#fig_1">Fig. 12</ref>: Our system can be used to validate/invalidate whether bars on windows or high-density windows are predictive of theft. Given the user-provided set of seed images (left) that contain examples of the visual elements, we train a predictor for city attribute "theft rate" in San Francisco using the seed images to initialize the visual element extraction. We then compute the detections of the resulting visual elements for the predictor across the ground truth theft rate values. The percentage of those elements from the positive set to the negative set indicate that bars on windows are significantly less predictive of theft than high-density windows.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc>information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Manuscript received 31 Mar. 2014; accepted 1 Aug. 2014 ate of publication 2014; date of current version 2014. 11 Aug. 9 Nov. D . Digital Object Identifier 10.1109/TVCG.2014.2346446</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Our predictor relating visual elements to violent crime rates in San Francisco</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Our predictor finds that visual elements corresponding to hedges (top), gable roofs (middle), and tropical plants (bottom) are related to high housing prices in San Francisco.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Fig. 11 :</head><label>211</label><figDesc>Compared to a traditional wayfinding route (green path) our theft-avoiding route (purple path) avoids many of the occurrences of theft (red circles) present in the downtown area of Chicago. To compute our route, we predicted the theft rate in Chicago using a predictor trained in San Francisco. The visual elements being detected by the predictor are all associated with high-traffic pedestrian areas, which are likely targets for thieves. In contrast, the route that we compute passes through more open areas with parks and lower foot traffic, which presents fewer opportunities to commit thefts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>When we use deep convolutional network features instead of HOG+color features the resulting visual elements are able to capture more of the semantics of city appearance, but are less visually consistent. In addition, although the new features provide a slight increase in accuracy in some cases (e.g. housing prices in Boston), we also observe large decreases (e.g. violent crime predictions in San Francisco).</figDesc><table><row><cell>SF Violent Crime</cell><cell>(Normalized by Population)</cell><cell></cell><cell></cell></row><row><cell cols="2">SF Housing Prices</cell><cell></cell><cell></cell></row><row><cell cols="2">SF Graffiti</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>San Francisco</cell><cell>Chicago</cell><cell>Boston</cell></row><row><cell cols="2">SF Violent Crime</cell><cell>0.891</cell><cell>0.685</cell><cell>0.690</cell></row><row><cell cols="2">SF Housing Prices</cell><cell>0.695</cell><cell>0.638</cell><cell>0.573</cell></row><row><cell cols="2">SF Graffiti</cell><cell>0.608</cell><cell>0.474</cell><cell>0.622</cell></row><row><cell cols="2">Fig. 9:</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the Intel Science and Technology Center for Visual Computing, NavTeQ, a Google Research Award, and computing resources from XSEDE (via NSF grant 1011832), supported by NSF grant number ASC-130027. Additional equipment was provided by Nokia and Samsung.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aesthetic capital: What makes london look beautiful, quiet, and happy?</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CSCW 2014</title>
		<meeting>CSCW 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering 3d human pose from monocular images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What makes london work like london</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alhalawani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SGP 2014</title>
		<meeting>SGP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modelling dynamic spatial processes: simulation of urban future scenarios through cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Barredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kasanko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lavalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Landscape and urban planning</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="160" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth annual workshop on Computational learning theory, COLT &apos;92</title>
		<meeting>the fifth annual workshop on Computational learning theory, COLT &apos;92<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting roads from dense point clouds in large scale urban environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2" to="12" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classifying urban landscape in aerial lidar using 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th IEEE international conference on Image processing, ICIP&apos;09</title>
		<meeting>the 16th IEEE international conference on Image processing, ICIP&apos;09<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1681" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Statistical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Duxbury Press Belmont</publisher>
			<biblScope unit="volume">70</biblScope>
			<pubPlace>CA</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno>27:1-27:27</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">broken windows&quot; and the risk of gonorrhea</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Spear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scribner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kissinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wildgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Public Health</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">230</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A note on two problems in connexion with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="271" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graffiti, greenery, and obesity in adults: secondary analysis of european cross sectional survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ellaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bonnefoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMJ: British Medical Journal</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">7517</biblScope>
			<biblScope unit="page">611</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Mayors Geek Squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feuer</surname></persName>
		</author>
		<ptr target="http://www.nytimes.com/2013/03/24/nyregion/mayor-bloombergs-geek-squad.html" />
		<imprint>
			<date type="published" when="2013-03-23" />
			<pubPlace>New York Times</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-driven 3D primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A high-performance, portable implementation of the mpi message passing interface standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Doss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Skjellum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="789" to="828" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The meaning and use of the area under a receiver operating characteristic (roc) curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on visual surveillance of object motion and behaviors. Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="334" to="352" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Looking beyond the visible scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust multiple car tracking with occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV &apos;94</title>
		<editor>J.-O. Eklundh</editor>
		<imprint>
			<biblScope unit="volume">800</biblScope>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stressful neighborhoods and depression: a prospective study of the impact of neighborhood disorder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Latkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Curry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Health and Social Behavior</title>
		<imprint>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Style-aware mid-level representation for discovering visual connections in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Big Data: A Revolution that Will Transform how We Live, Work, and Think. Eamon Dolan/Houghton Mifflin Harcourt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mayer-Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cukier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tracking groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="56" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Interpolation of scattered data: distance matrices and conditionally positive definite functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Procedural modeling of buildings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ulmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="623" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image-based procedural modeling of facades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Streetscore-predicting the perceived safety of one million streetscapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philipoom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="779" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning high-level judgments of urban perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th European Conference on Computer Vision</title>
		<meeting>13th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aesthetic capital: What makes london look beautiful, quiet, and happy?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quercia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O'hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 17th ACM Conference on Computer Supported Cooperative Work and Social Computing</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A new method for building extraction in urban areas from high-resolution lidar data. International Archives of Photogrammetry Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Briese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="295" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The collaborative image of the city: Mapping the inequality of urban perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salesses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schechtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">68400</biblScope>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tree detection in urban regions using aerial lidar and image data. Geoscience and Remote Sensing Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Secord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zakhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="200" />
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Disorder and decline: Crime and the spiral of decay in American neighbourhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Skogan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>University of California Pr</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Airborne traffic surveillance systems: video surveillance of highway traffic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Latchman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcnair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM 2nd international workshop on Video surveillance &amp; sensor networks</title>
		<meeting>the ACM 2nd international workshop on Video surveillance &amp; sensor networks</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Modeling gentrification dynamics: A hybrid approach. Computers, Environment and Urban Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Torrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="337" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Smart cities: Big data, civic hackers, and the quest for a new utopia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Townsend</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>WW Norton &amp; Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Mapping the world... one neighborhood at a time. Directions Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wilde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Kelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Broken windows. Atlantic monthly</title>
		<imprint>
			<biblScope unit="volume">249</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recognizing city identity via attribute analysis of geo-tagged images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th European Conference on Computer Vision</title>
		<meeting>13th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
