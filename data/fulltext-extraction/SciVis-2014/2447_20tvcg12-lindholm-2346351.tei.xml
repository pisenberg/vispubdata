<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boundary Aware Reconstruction of Scalar Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lindholm</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Science and Technology</orgName>
								<orgName type="institution">Linköping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">S</forename><surname>Lindholm</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jönsson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boundary Aware Reconstruction of Scalar Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2014.2346351</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Reconstruction</term>
					<term>signal processing</term>
					<term>kernel regression</term>
					<term>volume rendering</term>
				</keywords>
			</textClass>
			<abstract>
				<p>(a) Reference image. (b) Standard post-classified Volume Rendering. (c) Our method. (d) Reconstruction artifacts are removed. (e) Features are invariant to their surroundings. (f) Gaps between adjoining features are prevented. Fig. 1: Continuity between data points is commonly assumed in visualization-yet our understanding of many objects is that they consist of disjunct features with distinct boundaries. The data set used in this figure was created specifically to highlight and categorize three types of misrepresentations that often arise in relation to boundary areas. In this paper, we present an approach that takes feature boundaries into account during reconstruction and improves the visual representation in transition regions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Classification is one of the core steps in the visualization pipeline and is commonly applied either explicitly through dedicated off-line classifiers or implicitly through the visual mapping provided by, for example, a transfer function (TF). Yet, classification based on reconstructed data is never perfect and designing cost-efficient solutions to minimize misclassification is an active area of research.</p><p>It is common practice in visualization applications to reconstruct a fully continuous signal, i.e. that given two neighboring values the signal is assumed to pass through all other values that exist between. This practice comes with a significant risk that the reconstructed data values, after the visual mapping, end up representing materials at locations where they were not present in the studied object. As an example, consider a medical Computed Tomography (CT) data set in which the attenuation values in the data represent different materials. Bone and air are two materials that are far apart in the attenuation spectrum. Then let a volume rendering ray pass from air into bone. Under the assumption of continuity, the reconstruction will create values that correspond to all intermediate attenuation levels and thus take the appearance of skin, fat, muscles, contrast agent, and other tissues. <ref type="figure">Fig. 2</ref> highlights this problem where the visual appearance of contrastenhanced vessels, observed at 400-600 Hounsfield units (HU), is not confined to the vessels, but appear in every ray that passes through both air (less than -800 HU) and dense bone (greater than 850 HU).</p><p>Rendering of transitions between differentiable materials has been addressed through methods based on derivative expressions such as gradient magnitudes. We will show that those methods are less ideal in classifying individual materials and also inevitably lead to an increase of the dimensionality of the TF with one or two dimensions. This adds interaction complexity, prevents the use of inherently multivariate TFs, and does not resolve the misclassification problem as well as the herein proposed method. We also show that the use of methods based on segmentation masks and labeled volume rendering leaves several issues unresolved.</p><p>Our approach to address the problem above is centered around the reconstruction step in the visualization pipeline. Given an object of study that consists of separate features our goal is to achieve continuous visualizations within each feature while representing discontinuous boundary regions between features. We thus change the objective of the reconstruction to no longer be restricted to the reconstruction of a single "optimal" signal, but rather to provide the overall best representation of the object of study based on prior knowledge of its features and their visualization.</p><p>The first step in our feature-constrained reconstruction method is to obtain a probabilistic classification (see Section 4) which is used to assign importance weights to the original grid points. We then reconstruct one sample value for each feature that is within the reconstruction kernel range using the importance values as reconstruction weights. We use a kernel regression framework that provides reconstruction validity along with reconstructed values (see <ref type="bibr">Section 5)</ref>. The reconstruction validity is used during the intermixing stage to limit the visual contributions from all features to their respective signals' spatial support as they are combined to a final visual output (see <ref type="bibr">Section 6)</ref>. As can be seen in <ref type="figure">Fig. 1</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref>, the output of our method resembles post-classified rendering within features and pre-classified rendering between features.</p><p>It should be noted that, in contrast to previous work, we represent features in discontinuous boundary regions through weighted extrapolation, not by explicitly modeling or classifying data behavior across these regions. This has several benefits. First, the classification of a single feature is less dependent on its surroundings, less dependent on well defined gradients, and also less sensitive to noise. Second, misclassification can be addressed without increasing the dimensionality of the TF, which allows us to maintain an easy-to-use user interface and also increases the scalability of our method to multivariate data such as Dual-Energy CT (DECT) data.</p><p>The main contributions of this paper can be summarized as:</p><p>Presentation of a boundary aware approach to reconstruction of data that handles continuity within features as well as discontinuities in boundary regions.</p><p>A novel extension of the kernel regression framework to handle reconstruction of multiple features using anisotropic kernels.</p><p>Reduction of the impact of partial volume effects while maintaining high frequency details within features through adaptive kernel sizes.</p><p>A simple-to-use approach that seamlessly combines probabilistic classification, adaptive reconstruction and visual mapping without increasing interaction complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>When the TF was introduced by Drebin et al. <ref type="bibr" target="#b4">[5]</ref> it was done so with an explicitly probabilistic classification of samples. Bayesian estimates were used to yield material mixture percentages for each voxel, which subsequently were used to combine visual contributions from the participating material using 'accumulation level intermixing' as described by Cai and Sakas <ref type="bibr" target="#b2">[3]</ref>. Our work uses the same level of intermixing to combine the materials, i.e. at each step along the ray, and a similar form of classification. However, we separate the signal reconstruction to each material and use a validity-based weighting to combine the material estimates, which solves many of the misclassification issues of the original approach.</p><p>Since the work of Drebin misclassification and partial volume problems have mainly been addressed in three ways. One category relies on the TF as a visual classifier which produces good visualizations but does not classify voxels. A second category aims to computationally <ref type="figure">Fig. 2</ref>: This example highlights one of the problems that arise when data is assumed to be continuous and no prior knowledge about the scene is used in the rendering. Left: The result of standard DVR with continuous reconstruction. Right: The proposed method, which achieves continuous reconstruction within each feature while preventing interpolation between features. The red "sheet" visible in the left image is an artifact caused by the interpolation. The problem is made more difficult by the fact that the data is acquired through Dual-Energy CT and therefore is inherently multi-variate. solve partial volume effects through statistical methods using models for all materials, or combinations of materials, that may exist in the data. A final category of works rely on explicit segmentation masks to limit the area-of-effect of the TF.</p><p>Visual classification: Gradients of the scalar field are used by Levoy <ref type="bibr" target="#b16">[17]</ref> to better discriminate the material boundaries. Kindlmann and Durkin <ref type="bibr" target="#b10">[11]</ref> introduce the boundary emphasis function that explicitly targets material boundaries. It is based on an abstraction using the scalar value, the first and the second derivatives to better visualize isosurfaces, i.e. material boundaries, through opacity mapping. This method is similar to ours in that a simplified user interface allowed ease of visual classification of the underlying volume. Kniss et al. <ref type="bibr" target="#b11">[12]</ref> extend this idea to widget-based TF design. Their user interface is, however, more complex than our technique, and we also seek to classify individual voxels, not only creating visual representations of material boundaries. A TF method capable of identifying sheet like structures, e.g. material transitions, is proposed by Sato et al. <ref type="bibr" target="#b21">[22]</ref>. The method improves classification for recognizable shapes but carries the same weaknesses as the derivative based methods due to the increase of the dimensionality of the classification space.</p><p>Statistical models: A comprehensive overview of decision theory and probabilistic models is given in the work by Bishop <ref type="bibr" target="#b0">[1]</ref> and an overview on image processing methods by Gonzalez and Woods <ref type="bibr" target="#b5">[6]</ref>. Probabilistic classification for visualization is summarized by Kaufman and Mueller <ref type="bibr" target="#b8">[9]</ref> and its advantages are demonstrated in several publications, e.g. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. Variations of the approach have mostly been applied to pre-segmented data or incorporated in off-line classification schemes with complete data models, i.e. all materials need to be known a priori. Laidlaw et al. <ref type="bibr" target="#b14">[15]</ref> use a variation of this approach in an analysis of feature space designed to estimate percentages of material occupation inside voxels in Magnetic Resonance (MR) data. The method excels in situations where a complete segmentation of all materials is desired. A similar mixture model, specific for DECT, is proposed by Heinzl et al. <ref type="bibr" target="#b7">[8]</ref>, which potentially could be used in place of our interaction based classification. Prilepov et al. <ref type="bibr" target="#b18">[19]</ref> propose an order dependent method for multi-material interface reconstruction based on such percentage data. Their work focus mainly on surface representations of binary decision boundaries where our work retains the probabilistic nature of the data throughout the rendering process. Kniss et al. <ref type="bibr" target="#b12">[13]</ref> take a more visual approach by delaying the final classification decision until render time to facilitate uncertainty exploration and risk analysis. Their method rely on  initial probabilistic expressions for each voxel and for all classes of materials in the data. Visual mapping is then performed over these classes instead of the raw data. An important difference between this method and ours is that theirs ultimately result in a piece-wise constant expression that does not allow for variations inside materials. Explicit Segmentation: Explicit binary segmentation, or labeledvolume rendering, has been used to spatially constrain the TF response, thereby successfully reducing misclassification artifacts. Label volumes by Tiede et al. <ref type="bibr" target="#b24">[25]</ref> and two-level volume rendering by Hadwiger et al. <ref type="bibr" target="#b6">[7]</ref> are two examples that provide smooth, but binary, decision boundaries. The methods do not, however, account for signal degeneration in the surrounding regions and can therefore only be used to solve some of the classification artifacts described earlier. This is studied in more detail in Section 3. Also, despite extensive research progress, accurate segmentation masks are often not available unless time consuming manual procedures are used.</p><p>Kernel regression: A key part of our work is based on the kernel regression framework <ref type="bibr" target="#b23">[24]</ref> (a generalization of, among other methods, bilateral filtering <ref type="bibr" target="#b25">[26]</ref> and moving least squares <ref type="bibr" target="#b1">[2]</ref>). A number of techniques for reconstruction based on such methods have been presented within the context of volume rendering, e.g. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. These works exclusively assume continuity in the signals.</p><p>We chose to use the kernel regression framework since it offers an intuitive connection between the probabilistic classification, weighted reconstruction, and reconstruction certainty within a unified framework. Although kernel regression has traditionally not been used for discontinuous signals we will show how this can be done and how it can easily be integrated into existing frameworks. It should also be noted that the kernel regression framework supports unstructured grids even if this aspect is not utilized within this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DVR APPROACHES AND DISCONTINUITIES</head><p>To illustrate our approach in relation to the most commonly used Direct Volume Rendering (DVR) methods we provide a small synthetic point-sampled data set in <ref type="figure" target="#fig_2">Fig. 3</ref>. The reference is a piecewise continuous object of study consisting of three separate features, f 1,2,3 (red,</p><formula xml:id="formula_0">P(f 1 |y i ) P(f 2 |y i ) P(f 3 |y i ) xẑ f 1 (x) Classification Visual Contribution</formula><p>Per Feature Reconstruction <ref type="figure">Fig. 4</ref>: Overview of the proposed method. In order to avoid artifacts stemming from incorrect interpolation across features, the processing is divided into feature-specific reconstruction operations (here</p><formula xml:id="formula_1">z f 2 (x) z f 3 (x)</formula><formula xml:id="formula_2">f 1 , f 2 , f 3 )</formula><p>. Finally, the visual contributions from each reconstructed value are recombined in order to produce a visual representation of the data.</p><p>green and blue). The objective is to create a representation of the reference from sub-sampled data. Data and TF mapped images for both the reference and the sub-sampled data (4 × 4 sample points) used to create the representations are shown on the top row (a-d). The TF, which also acts as a classifier, is shown in (e). On the bottom row (f-i), four different results are shown: pre-classified DVR, post-classified DVR, two-level DVR, and our method.</p><p>As evidenced by the images each method produces different representations of both the boundaries and internal regions. Pre-classified DVR effectively treats intermediate space between samples as uncertain. This provides a representation of lost information in the boundary area, which expresses uncertainty by combining contributions from each of the neighboring samples. Unfortunately, this also prevents the variations within the materials from being represented (dark streaks). Post-classified DVR reconstructs the signal as fully continuous. This captures the variations within the materials well, but also creates both gaps (white areas) and sheet artifacts (e.g. the green line) in the boundary regions. Two-level DVR utilizes segmentation masks to spatially localize the TF widgets to their respective regions (post-classified DVR is used within each region). This successfully removes the sheet artifacts, but interpolation of the signal near boundary regions is still an issue, causing gaps in the rendering.</p><p>We have chosen to design our method such that variations within features are preserved, and a representation of uncertainty in boundary regions is obtained, avoiding the sharp decisions made by many existing approaches. The approach builds on systematic modifications of the classification, reconstruction and visual mapping stages of the DVR pipeline as shown in <ref type="figure">Fig. 4</ref>. In the following sections we present the details of these stages and how we can achieve the desired result without further input beyond the existing TF.</p><p>For the remainder of the paper, x and y denote spatial position and data value(s) respectively while index i indicates a grid point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROBABILISTIC CLASSIFICATION</head><p>This section explains two new ways to acquire feature classifiers P(f j | y i ), i.e. the probability that feature f j occurs given the data value at grid point i. A key aspect of the two new methods is that they use a widget-based graphical interface similar to a standard TF editor, which is highly desirable thanks to the low latency between editing and visual result. In the first method each widget provides both classification and visual properties (illustrated in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>) and in the second the two qualities are separated into individual widgets (illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>(b)). When acquiring both visual properties and classification from the same widget we derive the classification for feature f j from the widget's alpha channel. To further decouple the classification from the visual mapping we normalize the response such that the classifier returns 1 across the maximum of the widget:  where α max is the maximum opacity of the widget associated with feature f j . In our experience, this approach often provides sufficient feature delineation ( <ref type="figure" target="#fig_9">Fig. 9 and 10</ref> are good examples), is especially easy to implement and does not require any additions to the user interface. Unfortunately, the above approach imparts a set of restrictions on widget design. The most obvious such restriction occurs when a classifier is meant to capture an entire feature while the visual mapping is only meant to display a smaller part of the full feature. An example of such a case is illustrated in <ref type="figure" target="#fig_4">Fig. 5(b)</ref> where the green widget represents a thin range of values and thus would be too narrow to classify the full feature.</p><formula xml:id="formula_3">P(f j | y i ) = α(y i ) α max ,</formula><p>Using dedicated classifier widgets increases the flexibility of the system at the cost of interaction complexity due to the overall increased number of widgets. Classifier widgets and visual property widgets also need to be explicitly linked in an additional step. As the interpretation for a classifier widget is strictly probabilistic it holds only a scalar value. <ref type="figure" target="#fig_4">Fig. 5</ref>(b) illustrates a setup with two stacked editors. Specificity, interaction complexity and preset capabilities follows that of a standard TF.</p><p>Relying on the user to provide classification knowledge to the system through interaction often leaves the probabilistic model incomplete. To address this issue we use an approach outlined by Lundström et al. <ref type="bibr" target="#b17">[18]</ref> based on a null class to make the final model representative for all data. In short, the user provided probabilities are first normalized before the null class is created</p><formula xml:id="formula_4">P(f j | y i ) = P(f j | y i ) max i ∑ j P(f j | y i ) (1) P(f 0 | y i ) = 1 − ∑ j P(f j | y i ).<label>(2)</label></formula><p>While the methods presented above are the only forms of classification considered in this work other sources can also be applicable. For example, off-line classifiers such as those employed by Kniss et al. <ref type="bibr" target="#b12">[13]</ref> and Saad et al. <ref type="bibr" target="#b20">[21]</ref> or modality specific methods such as those employed by Laidlaw et al. <ref type="bibr" target="#b14">[15]</ref> or Heinzl et al. <ref type="bibr" target="#b7">[8]</ref> can also be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A FRAMEWORK FOR RECONSTRUCTION OF PIECEWISE CONTINUOUS DATA</head><p>Our reconstruction framework is based on kernel regression but extends it to handle multiple features. We will therefore briefly explain kernel regression before detailing how it is combined with our feature based approach. Furthermore, to handle partial volume effects, we will describe how to adaptively choose the filter kernel size. We simplify the notation of a single feature f j by omitting the index j to improve the presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Extending Kernel Regression to Feature-Constrained Reconstruction</head><p>This section details how the kernel regression framework is used to achieve continuous signal reconstruction within individual features.</p><p>The original kernel regression framework was presented for image processing by Takeda et al. <ref type="bibr" target="#b23">[24]</ref> to provide optimal estimations of an unknown signal between sampled grid points. The framework describes how a series of constraints can be used to influence a reconstruction. The framework also supports non-regular grids as well as variations in the local expansions. We use their notation, where y i denote the values of the grid points and the reconstructed signal estimate is denotedẑ(x).</p><p>To preserve the details within a feature but discern between different features, the kernel regression framework in our approach relies on local approximations by Taylor expansion and is thus not defined for reconstructions across discontinuities. Our assumption here is that the Taylor expansion is valid within each feature and that the reconstruction will therefore be valid as long as it is performed within the continuity of a single feature. Furthermore, weight functions are often enforced to be strictly greater than zero over their entire domain to avoid the risk of undefined reconstructions. We can relax this requirement such that weight functions may be zero since the utilization of the kernel regression framework will ensure that the visual impact goes to zero when the reconstruction becomes undefined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted Reconstruction</head><p>For reference, the reconstruction with a single normalized spatial kernel takes the following form</p><formula xml:id="formula_5">z(x) = ∑ y i ∈Ω K w i y i ,<label>(3)</label></formula><p>where w i are the spatial weights of the neighboring samples and the neighborhood Ω K is defined by the kernel size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature-Constrained Reconstruction</head><p>In this work, the spatial kernel is given by a generalization of trilinear interpolation with separable weights computed in each dimension for all spatial weights:</p><formula xml:id="formula_6">w i = w X w Y w Z , with w X|Y|Z = 1 − |x − x i | X|Y|Z 2σ .<label>(4)</label></formula><p>Here, w X , w Y and w Z are the respective weights in each spatial dimension and w i denotes the final spatial weight for grid point i. The function is scaled such that the tent shape approximates a Gaussian which is truncated at ±2σ . Constructing a feature-specific weight function for kernel regression is straightforward once the conditional probabilities P(f | y i ) are known. Using the framework notation, a feature weight is defined as</p><formula xml:id="formula_7">w f = P(f | y i ).<label>(5)</label></formula><p>This is combined with the spatial weight function from Equation 4 to form the feature specific reconstruction for feature f</p><formula xml:id="formula_8">z f (x) = ∑ y i ∈Ω K w i w f y i ∑ y i ∈Ω K w i w f .<label>(6)</label></formula><p>Assuming a regular grid, normalized spatial kernels will always have support and integrate to one. The feature kernels on the other hand cannot be assumed to do so. To address this we will introduce the concept of reconstruction validity during the visual mapping stage described in Section 6. Extending the framework to multiple features is done by computing one weighted reconstruction per feature of interest defined by the user. That is, Equation 6 is evaluated for each f j . The result is a set of estimated valuesẑ f j (one for each feature of interest), which are processed in the visual mapping stage described in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Partial Volume and Kernel Size Selection</head><p>As our data model we assume an object of study to consist of a series of continuous features (independent signals) divided by discontinuities (step-functions). In case the data has been convolved with some form of point spread function (PSF) causing partial volume effects the PSF is modeled as a Gaussian with an area of influence of σ PSF in accordance with Kindlmann and Durkin <ref type="bibr" target="#b10">[11]</ref>. By using the fact that the influenced regions are restricted in the spatial domain and that their extent is often a global property that is independent of the participating materials <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> we can calculate a kernel size that captures the effect of partial volume. We observe that, for a kernel with a size equal to the PSF, the combined weight of all voxels that have been misclassified due to partial volume is less than the combined weight of a solid feature. However, using a large kernel across the whole domain can suppress details in high frequency areas. Therefore, to enable our reconstruction to handle partial volume effects and at the same time preserve details in high frequency data, we utilize a variable kernel size. The maximum kernel size is based on the PSF, σ max = 2σ PSF , which also serves as an initial estimate. In order to reduce the kernel size in regions dominated by solid features we first introduce a measure of kernel support:</p><formula xml:id="formula_9">Φ(x, f, Ω) = ∑ y i ∈Ω w i w f .<label>(7)</label></formula><p>Note that Ω is directly related to the σ used in Equation 4. In particular, we use the term feature support for an estimation within a neighborhood whose size is related to the observed PSF:</p><formula xml:id="formula_10">F f = Φ(x, f, Ω max ).<label>(8)</label></formula><p>This feature support is used to differentiate between real features and partial volume misclassifications. The variable kernel size is computed as a linear combination of the minimum (σ min ) and maximum (σ max ) kernels based on the feature support from Equation 8:</p><formula xml:id="formula_11">σ f = (F f )σ min + (1 − F f )σ max ,<label>(9)</label></formula><p>where we use σ min = 0.25 for a smallest possible kernel size of 2 <ref type="bibr" target="#b2">3</ref> voxels. If a feature f has no support, i.e. F f = 0, the reconstruction of the feature is terminated since no visual output will be generated due to a lack of valid samples. Note that the size of the reconstruction kernel does not need to be adjusted for point-sampled data without partial volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VISUAL MAPPING</head><p>A widget-based TF is used to assign color and opacity properties to primitives. Section 4 presented how each TF widget is explicitly or implicitly linked to a feature. The TF here expresses the user's interest in a particular part of a feature and more than one widget can be assigned to the same feature. We will now go through how the multiple visual contributions are combined into a joint visual response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Reconstruction Validity</head><p>Having reconstructed an estimated value for a specific feature we want to know how valid this value is to use in the latter parts of the pipeline. As a measure of validity we use the support of active reconstruction kernel. Including the kernel validity is necessary in order to smoothly avoid ill-posed reconstructions. For example, consider a region where P(f | y i ) ≈ 0 for a majority of samples in Ω K , i.e. almost no values were valid to use. This may be caused by situations where only a few far-away voxels have an impact on the reconstructed value. The kernel regression framework still provides the best possible estimate from a least squares' perspective, but it will do so using a strong normalization as the denominator in Equation 6 will be very small. To address this issue, we interpret the strength of this normalization as a measure of kernel validity Note that the kernel validity differs from the feature support in that it is computed on the potentially smaller neighborhood Ω K using σ f for spatial weights (cf. Equation 8). This is similar to the interpretation as a reconstructed conditional probability P(f | x) at the point of reconstruction as proposed by Kniss et al. <ref type="bibr" target="#b12">[13]</ref>. Due to its use as a normalization factor in the reconstruction, we prefer the interpretation of the expression as reconstruction validity within the context of this work.</p><formula xml:id="formula_12">V f = Φ(x, f, Ω K ). (10) ε a ε b ε ε ε ε b ε a ε a ≈ ε b ν ν ν 1 0 1 0 1 0 (a) Fuzzy mapping (linear γ) ε a ε b ε b ε ε a ε ε a ≈ ε b ν ν ν 1 0 1 0 1 0 (b) Medium mapping (slope γ) ε a ε b ε b ε a ε a ≈ ε</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Combining Multiple Visual Contributions</head><p>The maximum number of visual contributions that need to be combined is equal to the number of widgets in the TF. In practice, however, only one or two of the contributions will typically have non-zero opacity for a given data value (i.e, there are few places where more than two visible features overlap).</p><p>As an example, consider the visual editor in <ref type="figure" target="#fig_4">Fig. 5</ref>(b) in which two features (f 1 and f 2 ) and three visual primitives, TF p (red, green and white) have been defined. The red and green primitives are associated with f 1 , the white primitive with f 2 . Computations at any point x along the ray would then include two reconstructions and result in three visual contributions, which all need to be taken into consideration for the final visual response at point x.</p><p>Previous works have targeted the problem of combining multiple, uncertain or partial visual contributions. Cai and Sakas <ref type="bibr" target="#b2">[3]</ref> provide a categorization of intermixing schemes. Other approaches include solutions based on decision-and risk-boundaries, fuzzy representation and probabilistic animation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. It is, however, outside of the scope of this paper to provide a complete overview of these approaches.</p><p>In our work, we employ an accumulation level intermixing <ref type="bibr" target="#b2">[3]</ref> where color contributions from a single sample point are computed individually. Each contribution is additionally weighted by its reconstruction validity, V f , to separate between desired and undesired contributions. For our purposes we have chosen a fuzzy representation of uncertainty based on reconstruction validity and user interest as expressed in the alpha channel of the TF. We first add a linear mapping function to the computed validity</p><formula xml:id="formula_13">γ(V) = V − ε a ε b − ε a , with γ clamped to [0, 1].<label>(11)</label></formula><p>The visual contributions for each feature are subsequently blended into the accumulated color, C dst , and opacity, α dst , using front-to-back compositing:</p><formula xml:id="formula_14">C dst = C dst + (1 − α dst )C src α dst = α dst + (1 − α dst )α src γ(V f ),<label>(12)</label></formula><p>where C src and α src are the color and opacity from the mapping of the reconstructed valueẑ f through each associated primitive, TF p (ẑ f ). The compositing in Equation 12 is thus performed once for each widget. The blending order corresponds to the order of the widgets in the TF. Appropriate values for ε a and ε b naturally depend on the characteristics of the data. For point sampled data without partial volume the mapping is reduced to a stylistic expression of certainty, as exemplified in <ref type="figure" target="#fig_5">Fig. 6</ref>, ranging from full uncertainty visualization to a bestguess representation. All renderings based on point sampled data in this paper use ε a = 0.25 and ε b = 0.75. For imaging data with partial volume, the less precise classification often lowers the expected reconstruction validity. For this type of data, we use ε a = 0.25 and ε b = 0.5 for all renderings, which suppress most partial volume effects while also providing full opacity within features.</p><p>For non-expert users, ε a and ε b can remain hidden. The only parameter that we then choose to expose to the user is the option to manually fine-tune σ max in case the initial estimate proves incorrect. The interpretation of the parameter is straightforward as a 'high frequencies' vs. 'partial volume artifacts' tradeoff discussed earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATION TO EXISTING METHODS</head><p>The problem of correctly classifying features or transitions between features is not new. One of the dominant ways to address the problem has traditionally been to rely on derivative models of the data, i.e. local descriptions for the gradient and curvature. This section provides an analysis of how such methods can be used to achieve the piecewise continuous representations of an object of study that are the target of our work. Gradient models offer greater specificity in delineating different transitional regions, e.g. the blue sheet observable in <ref type="figure" target="#fig_6">Fig. 7(a)</ref>. Traditionally, this information has been used to visualize specific transitions in an isosurface like manner. This information can, however, also be used to classify a single solid feature.</p><p>As a first example, one can create a minimal intensity/gradient TF that only classifies and visualizes those parts of the feature that have low gradient magnitudes. <ref type="bibr" target="#b0">1</ref> Unfortunately, this results in severe gaps in the visualization between the two visualized materials, as illustrated in <ref type="figure" target="#fig_6">Fig. 7(b)</ref> top magnification. To fill the gaps and to create a more reasonable representation of the feature one may extend the classifier widgets in the intensity/gradient domain to include the nearest parts of all transition arcs associated with the targeted feature. However, as illustrated in the bottom magnification of <ref type="figure" target="#fig_6">Fig. 7(c)</ref>, this can introduce a different form of undesirable artifacts as the specificity between different transitions is worse near an arc base than at its apex. Crossing arcs also contribute to similar misclassification problems. While some of the misclassification introduced by the arcs can be removed by manually adjusting the widgets, misclassification still appears, as illustrated in the bottom magnification in <ref type="figure" target="#fig_6">Fig. 7(d)</ref>. This adjustment is also both ad-hoc and time consuming. Furthermore, the number of arcs that need to be adjusted is dependent on the number of other features  the current feature-of-interest is neighboring. This means that we may end up having to manually adjust a potentially large number of arcs even if our visualization only concerns a single feature. Whereby the visualization becomes implicitly dependent on a complete data model that includes all features that may exist in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">IMPLEMENTATION</head><p>The presented feature-constrained reconstruction is summarized in Algorithm 1. The prototype implementation that has been used to present the images in this paper is based on GLSL code and required only minor changes to an existing software implementation of postclassification DVR. TF widgets are rendered individual layers in lookup tables (1D-and 2D-array textures, respectively) to allow each widget to be sampled independently. To demonstrate the ease of implementation, and further promote the reproducibility of our approach, we provide annotated GLSL code in the supplementary material. The file contains two functions: globally continuous reconstruction and feature-constrained reconstruction.  <ref type="figure">Fig. 9</ref>: These renderings exemplify a small region of interest (ROI) of the CT angiography scene introduced earlier in <ref type="figure">Fig. 2. (a)</ref> shows the raw data of the ROI which contains a cross section of a main artery neighbored by a smaller dense material. The following two sub-images show the data as mapped through the TF with; (b) linear interpolation and (c) our feature constrained reconstruction. The visual difference (CIE76 Lab distance) between linear interpolation and our method is highlighted in (d). For reference, we provide the TF mapping with no data interpolation (e) and 3D renderings of and an extended region with linear data interpolation (f) and and our method (g). As seen in both the 2D and 3D renderings, our method prevent the smaller pieces of dense material from appearing as if they were covered in a layer of contrast agent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">RESULTS</head><p>We have applied the framework to several synthetic and real world data sets. Specifications of the data sizes and origin as well as parameters for the visualizations can be found in <ref type="table" target="#tab_0">Table 1</ref>. Dedicated classifier widgets in a separate editor were used to generate <ref type="figure" target="#fig_7">Fig. 1 and 8</ref>. For all other figures, classifiers derived from the TF widgets were sufficient.</p><p>Initially we focus on the highlighted regions in <ref type="figure">Fig. 1</ref> representing erroneous visual effects that commonly arise in a continuous model. The first highlight ( <ref type="figure">Fig. 1(d)</ref>) shows the well-known sheet artifact that appears in transitional regions between features where a visual mapping of an unrelated feature incorrectly is shown. The same problem has been discussed in relation to <ref type="figure">Fig. 2</ref>.</p><p>The second highlight ( <ref type="figure">Fig. 1(e)</ref>) demonstrates a similar sheet artifact but this time manifesting itself within, as opposed to outside, the visualized feature. In this region of the data set, the values within the 'O' are higher than what the user has selected to show. Outside the 'O' the values are all zero, which the TF also defines as transparent. However, due to the continuous interpolation, a blue sheet appears at the boundary even though the user expects a transparent output in this region. For point sampled data, our method removes this artifact through the weighted reconstruction. For data with partial volume, the effect is hard to separate from what may be natural value shifts within the feature. The effect is dependent on the number of voxels that have been erroneously labeled in the initial classification and can thereby be addressed through improved classifier specificity.</p><p>A different type of artifact is shown in the third highlight ( <ref type="figure">Fig. 1(f)</ref>). Gaps are formed between adjoining features due to a shrinking effect of both features. This effect always occurs when their respective visual primitives in the TF are disjunct, since reconstructed intermediate values are mapped to transparency. The artifact is even more noticeable in the presence of partial volume as illustrated in <ref type="figure" target="#fig_6">Fig. 7</ref> and discussed in Section 7. As highlighted in the inset images of <ref type="figure" target="#fig_6">Fig. 7</ref>, our method improves the representation of both visualized features without introducing the misclassification that appears in other methods.</p><p>The effects of adapting the kernel size to preserve high frequencies within features can be seen in <ref type="figure" target="#fig_6">Fig. 7</ref>. Here, the kernel size is increased for both features near the transition, yet the solid materials still show the high frequency variations that exist when a global small kernel is used (compare sub-figures (a) and (e)). Another example of how increased kernel size is used to reduce small scale effects of partial volume is evident in <ref type="figure">Fig. 9</ref> where the high frequencies near transitional regions are misleading. In <ref type="figure" target="#fig_7">Fig. 8</ref>, a synthetic data set with two features are given overlapping value ranges by adding noise. The sheet-type artifact discussed above causes single noise-disturbed samples to also affect their respective local neighborhoods. Our method avoids the sheet effect by restraining the spatial spread and therefore reduces the visual impact of the noise.  Finally we also provide examples to demonstrate that the approach scales well and supports data of higher dimensions. This is illustrated in <ref type="figure">Fig. 2, 9</ref> and 10 which are rendered from data acquired through DECT, i.e. inherently dual-variate data. <ref type="figure">Fig. 2 and 9</ref> originate from the same data set and depict a CT angiography study of human blood vessels which has been infused with a contrast agent. <ref type="figure" target="#fig_9">Fig. 10</ref> similarly depicts a study on vessel anatomy focusing on vessels inside a human head. Dual-energy CT is used in both cases to increase the differentiability between the contrast agent and bone. The differentiation is sufficient to derive acceptable material classifiers directly from the TF (as described in Section 4), and our method is thus applied with next to zero overhead in interaction complexity. For the small focus region in <ref type="figure">Fig. 9</ref>, our method prevents red sheets from obscuring important features. The difference image, showing CIE76 Lab color distances (pixel values in the [0, 1] range), highlights that the vessel is also less affected by thinning due to partial volume. In <ref type="figure" target="#fig_9">Fig. 10</ref> the vascular tree is made more visible by our method as the artifacts near the skull are filtered away. Reconstruction of the two energy levels was performed separately while the classification was performed in the full 2D space.</p><p>When the kernel size is fixed the proposed framework only requires a small number of extra instructions per primitive compared to an equivalent continuous reconstruction kernel (software implementation). More specifically, feature weights need to be fetched or procedurally generated, validity computed, and the final result normalized (steps 3-7 in Algorithm 1). For variable kernel sizes, our prototype implementation currently uses a brute force approach, which computes the kernel size for each sample position by sampling the neighborhood (step 1-2 in Algorithm 1), and is therefore only interactive for 2D scalar fields or for smaller focus/context lenses (subset of full viewport) or regions-of-interest of 3D scalar fields. We performed performance measurements using variable sized kernels, no shading, an early ray termination threshold of 0.98, and two TF primitives on a system with an Intel Core i7 3.6 GHz CPU with 16 GB RAM and a Nvidia GTX 770 graphics card with a single sample per voxel. A viewport size of 512 2 was used for all cases except the focus/context lens which used a 256 2 viewport with a reduced field-of-view. Performance measurements are available in <ref type="table" target="#tab_2">Table 2</ref>. Measurements on 3D data were performed on the Tooth data from <ref type="figure" target="#fig_6">Fig. 7</ref> while the 2D measures were performed on a 16-bit CT image. The results show that the method in its current form supports interactive exploration for 2D slices at high frame rates, and that it also can be applicable to 3D scalar fields if the rendering is focused on a subsection of the volume. Rendering a full volume at interactive frame rates would require further performance optimization. A performance analysis shows that the overhead is primarily introduced by the evaluation of Equation 8, which represents the sampling needed to determine the kernel size. Determining the kernel size through methods with lower computational overhead, such as gradient analysis, provides opportunities for performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSIONS AND FUTURE WORK</head><p>Separation between the object of study (what the user wants to see), the measurable signal (what can be measured) and the measured signal (the sampled data) in the context of signal reconstruction for visualization is the underpinning concept in this work. We made two crucial points; that the user's conceptual understanding of the object of study in many cases consists of multiple disjoint features, and that applying a continuous model in such cases results in severe artifacts in the rendered image.</p><p>To address the identified issues we presented a novel boundary aware visualization model that improves upon previous work by utilizing knowledge about features to be accounted for in the reconstruction stage of the visualization pipeline. The approach is general for all visualization work requiring a color mapping and we have exemplified it using slice based and volumetric rendering.</p><p>The new framework enabled the use of a piecewise continuous data model by performing a per-feature reconstruction. The theoretical foundation for this was realized by extending kernel regression to constrain reconstruction within single features. Knowledge about features was introduced using a probabilistic interpretation of classification. As this probabilistic interpretation is common in the field of visualization our work is widely applicable. Since our method does not rely on any single implementation to derive the probabilistic classifications it offers great flexibility in the trade-off between classification specificity versus computational and interaction overhead. Notably, one of the proposed probabilistic classification implementations was entirely based on the TF and thus does not introduce any extra user defined parameters.</p><p>Although performance is currently a weakness of our approach, it is applicable for slice based rendering as well as for smaller focus regions of volumetric data. Addressing performance by improving the selection of kernel sizes remains an interesting possibility for future work. Other avenues for future work include gradient reconstruction, semi-automatic creation of support functions, and an extension of representation of transitions between regions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc>information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Manuscript received 31 Mar. 2014; accepted 1 Aug. 2014 ate of publication 2014; date of current version 2014. 11 Aug. 9 Nov. D . Digital Object Identifier 10.1109/TVCG.2014.2346351</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Top row: A piecewise continuous object of study (a-b) composed of three features (f 1 , f 2 , f 3 ) is point-sampled to a 4 × 4 grid of raw data (c). Three widgets define the visual mappings and feature classifications (e). The mapping is also exemplified (d). Bottom row: Reconstruction of the object performed under the assumption of piecewise constant data (f), globally continuous data (g), segmented data (h), and piecewise continuous data (i).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Classifier Editor. TF and classifier widgets are labeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Feature classifiers can be designed as separate primitives (for full flexibility) or extracted from the visual primitives (for minimal interaction complexity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>The mapping function determines how sharp the transition boundary should be between different features. The presented images in this work use a mapping similar to (b) unless otherwise specified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Comparison of reconstruction for intensity TF, 2D intensity/gradient TF, and our method using the Tooth data set. (a) 1D TF introduces misclassified boundaries (blue sheet) and large empty space near features boundaries (top magnification). (b-d) 2D intensity/gradient TF with minimal to high user interaction, illustrating the difficulty of achieving a desired classification through complex interfaces. None of them manages to perform the desired classification. (e) Our method applied to the 1D TF correctly classifies the materials and handles empty space near feature boundaries, as shown in the top and bottom magnification, with minimal additional user input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Displaying renderings of original synthetic data (left) and with added noise (right) for post-classified DVR in (a) and our method in (b). With post-classified DVR, noise-disturbed samples will affect also their surroundings (see outlined regions). The same artifacts are much less prominent using our reconstruction method. Thus, our method can help reduce the visual impact of noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>This example is rendered from a contrast enhanced DECT data set of a human head. Using a second energy level in the CT acquisition provides improved differentiability between the contrast enhanced vessels and the skull. Even so, classification is still problematic, resulting in severe misclassifications, primarily along the inside of the skull (a). Our method (b) is capable of eliminating much of the thin artifacts, resulting in a more easily interpretable image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Specifications of data sets and visualizations employed. "Feature overlap" refers to overlap in value ranges.</figDesc><table><row><cell>Fig.</cell><cell>Source</cell><cell>Partial</cell><cell>Feature</cell><cell>Size, reference size</cell><cell>σ max</cell></row><row><cell></cell><cell></cell><cell>volume</cell><cell>overlap</cell><cell></cell><cell></cell></row><row><cell>1 2,9 3,6 8 7 10</cell><cell>Synthetic DECT scan Synthetic Synthetic CT scan DECT scan</cell><cell>No Yes No No Yes Yes</cell><cell>No Partial No Yes Yes Yes</cell><cell>128 × 64 × 1, 1024 × 512 × 1 512 × 512 × 512, N/A 4 × 4 × 1, (procedural) 16 × 16 × 16, N/A 128 × 128 × 128, N/A 256 × 256 × 256, N/A</cell><cell>0.5 1.25 0.5 0.55 1.75 1.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 Feature-Constrained Reconstruction.</figDesc><table><row><cell>Require:</cell></row><row><cell>A set of visualized features f j and a set of TF primitives TF p</cell></row><row><cell>Access to P(f j | y i ), either precomputed or as a set of parametric functions (see Section 4)</cell></row><row><cell>Algorithm:</cell></row><row><cell>for all points x along view ray do</cell></row><row><cell>for all features f j do</cell></row><row><cell>1) Compute feature support F f j (Eq. 8) 2) Select reconstruction kernel size based on F f j (Eq. 9) 3) Reconstructẑ f j (x) w.r.t. P(f j | y i ) (Eq. 6) 4) Compute reconstruction validity V f j (Eq. 10) if Validity V f j &gt; 0 then 5) Extract visual properties through TF p ẑ f j (x) 6) Compute visual contributions (Eq. 12)</cell></row><row><cell>7) Blend to buffer</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance measurements for 2D and 3D scalar fields. Interactive performance can be achieved for 2D scalar fields and small 3D scalar fields.</figDesc><table><row><cell>Data set size</cell><cell>Post (HW)</cell><cell>Post (SW)</cell><cell>σ max = 0.5</cell><cell>σ max = 1.0</cell><cell>σ max = 1.5</cell></row><row><cell></cell><cell>Ω K = 2 3</cell><cell>Ω K = 2 3</cell><cell>Ω max = 2 3</cell><cell>Ω max = 4 3</cell><cell>Ω max = 6 3</cell></row><row><cell>512 2 (2D slice)</cell><cell>0.002 s</cell><cell>0.002 s</cell><cell>0.003 s</cell><cell>0.004 s</cell><cell>0.010 s</cell></row><row><cell>16 3 (3D ROI)</cell><cell>0.002 s</cell><cell>0.005 s</cell><cell>0.008 s</cell><cell>0.050 s</cell><cell>0.184 s</cell></row><row><cell>128 3 (3D focus)</cell><cell>0.002 s</cell><cell>0.006 s</cell><cell>0.008 s</cell><cell>0.031 s</cell><cell>0.094 s</cell></row><row><cell>128 3 (3D)</cell><cell>0.004 s</cell><cell>0.022 s</cell><cell>0.036 s</cell><cell>0.171 s</cell><cell>0.516 s</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Low here refers to gradient magnitudes caused by natural variations within features which are assumed to be smaller than the magnitudes observed in transitional regions, as assumed by Kindlmann and Durkin<ref type="bibr" target="#b10">[11]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank the Center for Medical Image Science and Visualization (CMIV), Linköping University Hospital, Sweden. This work was supported in part by the Swedish Research Council, VR grant 2011-5816 and the Linnaeus Environment CADICS, and the Swedish e-Science Research Centre (SeRC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Superresolution and noise filtering using moving least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2239" to="2248" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data intermixing and multi-volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="368" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visualizing scalar volumetric data with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Djurcilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lermusiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="248" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Drebin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics (Proceedings of SIGGRAPH 1988)</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Prentice-Hall, Inc</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-quality two-level volume rendering of segmented data sets on consumer graphics hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization, VIS &apos;03</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical analysis of multi-material components using dual energy CT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heinzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision, Modeling and Visualization Workshop (VMV)</title>
		<meeting><address><addrLine>Konstanz, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Visualization Handbook, chapter Overview of Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="127" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate determination of CT point-spread-function with high precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kayugawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ohkubo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Clinical Medical Physics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-automatic generation of transfer functions for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Durkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Volume Visualization, VVS &apos;98</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-dimensional transfer functions for interactive volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="285" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistically quantitative volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Uitert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Geometric Model Extraction from Magnetic Resonance Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Partial-volume bayesian classification of material mixtures in mr volume data using voxel histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Fleischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Barr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="86" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledergerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<title level="m">MLS ray casting. IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1539" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Display of surfaces from volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uncertainty visualization in medical volume rendering using probabilistic animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lundström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1648" to="1655" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cubic gradient-based material interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Prilepov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Obermaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Deines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Joy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1687" to="1699" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image restoration in computed tomography: Estimation of the spatially variant point spread function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rathee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Koles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Overton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="539" to="545" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probexplorer: Uncertaintyguided exploration and editing of probabilistic medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1113" to="1122" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tissue classification based on 3D local intensity structure for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Westin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shiraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="160" to="180" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive segmentation of 3D medical images with subvoxel accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stalling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zöckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Hege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Assisted Radiology and Surgery</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel regression for image processing and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="349" to="366" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High quality rendering of attributed volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schiemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hohne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Visualization &apos;98</title>
		<meeting>Visualization &apos;98</meeting>
		<imprint>
			<date type="published" when="1998-10" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hardware-based nonlinear filtering and segmentation using high-level shading languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanitsar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gröller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Visualization</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
