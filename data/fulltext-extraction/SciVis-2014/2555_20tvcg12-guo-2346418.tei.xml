<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Advection-Based Sparse Data Management for Visualizing Unsteady Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Hanqi</forename><surname>Guo</surname></persName>
							<email>hanqi.guo@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhang</surname></persName>
							<email>jiang.zhang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<address>
									<settlement>Peking University</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richen</forename><surname>Liu</surname></persName>
							<email>richen.liu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<address>
									<settlement>Peking University</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
							<email>lu.liu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xiaoru</forename><surname>Yuan</surname></persName>
							<email>xiaoru.yuan@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Jian</forename><surname>Huang</surname></persName>
							<email>huangj@eecs.utk.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">National Supercomputer Center in Tianjin</orgName>
								<address>
									<settlement>Binhai, Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Meng</surname></persName>
							<email>mengxf@nscc-tj.gov.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingshan</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">Lu</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of EECS, and Center for Computational Science and Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">Xiaoru</forename><surname>Yuan</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Tennessee</orgName>
								<address>
									<settlement>Knoxville</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">Xiangfei</forename><surname>Meng</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Jingshan Pan is with National Supercomputer Center in Jinan, Shandong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Advection-Based Sparse Data Management for Visualizing Unsteady Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2014.23464</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Flow visualization</term>
					<term>Data management</term>
					<term>High performance visualization</term>
					<term>Key-value store</term>
				</keywords>
			</textClass>
			<abstract>
				<p>When computing integral curves and integral surfaces for large-scale unsteady flow fields, a major bottleneck is the widening gap between data access demands and the available bandwidth (both I/O and in-memory). In this work, we explore a novel advection-based scheme to manage flow field data for both efficiency and scalability. The key is to first partition flow field into blocklets (e.g. cells or very fine-grained blocks of cells), and then (pre)fetch and manage blocklets on-demand using a parallel key-value store. The benefits are (1) greatly increasing the scale of local-range analysis (e.g. source-destination queries, streak surface generation) that can fit within any given limit of hardware resources; (2) improving memory and I/O bandwidth-efficiencies as well as the scalability of naive task-parallel particle advection. We demonstrate our method using a prototype system that works on workstation and also in supercomputing environments. Results show significantly reduced I/O overhead compared to accessing raw flow data, and also high scalability on a supercomputer for a variety of applications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Simulation of unsteady flow is key to many domains of computational research. As computing power at large scale has become very affordable, large and more complex unsteady flow data are nowadays prevalent. From the most practical point of view, however, the computing resources available for scientific visualization are typically much smaller than those used for the original simulations <ref type="bibr" target="#b2">[3]</ref>, thereby creating a challenge. That is, how to effectively carry out state-of-the-art flow visualization, for example to visualize a tera-byte level unsteady flow, without requiring a full-scale supercomputer.</p><p>In this work we focus on particle advection, which is a fundamental part of many flow visualization and analysis methods. Typical examples include texture-based methods <ref type="bibr" target="#b22">[23]</ref>, geometry-based methods <ref type="bibr" target="#b24">[25]</ref>, Finite-Time Lyapunov Exponent (FTLE) analysis <ref type="bibr" target="#b17">[18]</ref> and source-destination queries <ref type="bibr" target="#b21">[22]</ref>. Among these methods, data access is the known bottleneck of performance, scalability and space-efficiency. Many existing implementations of advection-based flow visualization store data based on the same grid as used by the original simulation. We address the multi-faceted bottleneck by changing the grid-centered data model to a key-value store based sparse data model.</p><p>We aim at developing a general mechanism by which fast, scalable visualization of large unsteady flow can be carried out on a relatively commonly available type of computing resource. For instance, as demonstrated in Section 5, we performed streak surface computation on a 860GB turbulent flow in 92 seconds (including I/O time) using 64 processors on a system that provided only ∼100MB/sec sustained I/O bandwidth. During the entire process, only 1% of the memory is used on each processing core. We also report similar results on several different kinds of platforms as well as for other use cases such as FTLE computation. This unprecedented result demonstrates that large scale unsteady flow analysis can be effectively and generally applied, requiring only a very frugal budget of computing resource. Potential use cases include extreme scale in-situ visualization and when the flow analysis application has to be carried out on-demand on a heavily loaded computing system.</p><p>The rationale of our research is based on a few observations. First, although the entire flow dataset is large, the working set is very small during particle advection. This is true even in large scale parallel settings. Second, while data access patterns appear random in unsteady flow visualization, it is practical and reliable to discover the working set on-demand by predicting data access needs based on current directions of advection. Third, with data partition as a standard practice in parallel flow visualization, changing from coarse-grained partition to fine-grained partition is feasible, especially when modern particle tracers increasingly employ local task queues to manage the start, stop, and communication of advection tasks. To this end, key-value store based sparse data management can greatly reduce the memory footprint of a parallel particle advection run, adapt well to the on-demand data access needs, and can be efficiently managed without impeding performance or scalability.</p><p>Data reorganization has been proven successful in mesh data and unstructured data management <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1]</ref>. In our method, the working set discovered at runtime is managed on the granularity of blocklets, which contain single cells or fine-grained blocks of cells in unsteady flow. The functional architecture of our methodology has two components: (1) a parallel key-value store that collectively and predictively manages blocklet I/O to maximally hide the I/O latency from advection computation, and (2) a group of completely independent task-parallel tracers. Every advection computing task is assigned to one and only one tracer, while each tracer owns and manages a large number of advection tasks in a task queue. Tracers request data by querying the parallel key-value store, and keep the blocklets received in a local LRU cache. In the past, the field has made great progress on improving the speed and scalability of unsteady flow visualization. For example, with I/O potentially takes up 90% time in a typical visualization task <ref type="bibr" target="#b20">[21]</ref>, researchers improved efficiency by using irregular data partition <ref type="bibr" target="#b11">[12]</ref> and by optimizing file layout according to flow features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Loadbalancing is difficult because optimal task distribution is hard to predict in unsteady flow visualization. Researchers improved scalability by using a multi-tier task distribution scheme that minimized global barriers and optimized the efficiency of task redistribution <ref type="bibr" target="#b21">[22]</ref>.</p><p>Recent implementations of large-scale unsteady flow analyses commonly employ static data distribution, and intermix data parallelism with task parallelism. In part, the key consideration is that moving large amounts of data on the fly is overwhelmingly expensive. This trend is true both on large supercomputer tier platforms and also on small to medium sized institutional HPC platforms. Regardless of scenario, great efforts are needed to fine tune both the algorithms and the configuration of the runs for best performance.</p><p>In contrast, our method uses static task distribution and employs task parallelism, by accurately and efficiently discovering the working set and passing data from parallel key-value store to tracers ondemand. Our implementation of parallel key-value store effectively manages in-core data and reduces the latency of data access in general. On top of that, our method constructs a graph based predictive hint model to accurately prefetch data from high a performance system (i.e. parallel file system or SSD enhanced file system) with built-in flow control, thereby effectively hides latency due to on-demand disk I/O.</p><p>In the remainder of the paper, we describe the background in Section 2. The sparse data management and the implementation details are described in Sections 3 and 4, respectively. We demonstrate the application cases in Section 5, and then conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 Advection-based Flow Visualization</head><p>Particle advection traces integral curves from user specified seed points. It is widely used in flow visualization methods, such as texturebased methods <ref type="bibr" target="#b22">[23]</ref>, geometry-based <ref type="bibr" target="#b24">[25]</ref> methods, and flow feature extraction and tracking <ref type="bibr" target="#b31">[32]</ref>. We divide these advection-based methods into two categorizes, namely local-range and full-range analysis.</p><p>Local-range analysis seeds locally or sparsely in a given spatiotemporal domain. For example, source-destination analysis is a localrange analysis useful for visualizing contaminant transport between two regions. For another example, flow surfaces <ref type="bibr" target="#b13">[14]</ref> are ideal to illustrate unsteady flows as they directly show the behaviors of coherent moving particles.</p><p>Full-range analysis densely seeds over the entire simulation domain. In texture-based flow visualization, such as Line Integral Convolution (LIC) <ref type="bibr" target="#b6">[7]</ref> and Unsteady Flow LIC (UFLIC) <ref type="bibr" target="#b33">[34]</ref>, integral curves are computed over all spatiotemporal samples in the domain. Densely seeded integral curves are also used for extracting Lagrangian Coherent Structures (LCS) with Finite-Time Lyapnov Exponents (FTLE) <ref type="bibr" target="#b14">[15]</ref>, for identifying differences between ensemble flow simulation results <ref type="bibr" target="#b16">[17]</ref>, and for selecting flow features according to pathline attributes <ref type="bibr" target="#b15">[16]</ref>. Full-range analysis is computationally expensive, even with acceleration methods such as seeding with adaptive refinement <ref type="bibr" target="#b1">[2]</ref> and reusing the already calculated parts <ref type="bibr" target="#b18">[19]</ref>.</p><p>This work improves the performance, scalability as well as space efficiency for both the local-range and the full-range analysis using blocklet-based sparse data management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parallel Particle Tracing</head><p>In general, parallel particle tracing methods can be categorized as taskparallelism, data-parallelism and hybrid methods that combine taskand data-parallelism.</p><p>Data-parallel particle tracing relies on data partitioning and distribution for load-balancing. For example, blocks can be statically assigned by round-robin <ref type="bibr" target="#b30">[31]</ref>, hierarchical clustered <ref type="bibr" target="#b36">[37]</ref>, partitioned ac-cording to boundaries of flow features <ref type="bibr" target="#b11">[12]</ref>, or partitioned over time steps to reduce memory cost in unsteady flow FTLE computation <ref type="bibr" target="#b27">[28]</ref>. Task-parallel particle tracing revolves load-balancing by scheduling, such as workload estimation <ref type="bibr" target="#b28">[29]</ref>, dynamic load balancing <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b26">27]</ref>, etc. On-demand strategies are also used to reduce I/O costs and communication <ref type="bibr" target="#b7">[8]</ref>. More recently, hybrid methods are considered to be more scalable. DStep <ref type="bibr" target="#b21">[22]</ref> minimizes global barriers with a multi-tier task distribution and static data distribution. More recently in <ref type="bibr" target="#b16">[17]</ref>, Guo et al. demonstrated unprecedented ensemble flow analysis in Lagrangian scheme by extending DStep <ref type="bibr" target="#b21">[22]</ref>.</p><p>Common among parallel particle tracing methods, I/O is the typical bottleneck for scalability and performance. A few approaches have been proposed to improve data partitioning, including the block-based methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29]</ref>, fine-grained partitioning <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b11">12]</ref>. Optimized flow file layout can also be used to hide the latency of data retrieval <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>In this work, by transforming the in-core data management of particle advection to one using large-scale parallel key-value store, we have been able to further reduce the granularity of data retrieval to blocklets, and achieve better utilization of I/O and communication bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Key-Value Store and Data Prefetching</head><p>Key-value store is a simple, general and powerful data model for data retrieval. It is widely used in NoSQL databases for big data applications in industry settings. Key-value store techniques can be roughly categorized into RAM cache vs. persistent store.</p><p>RAM cache key-value store could be as simple as a hash table, and there are also a series of production systems such as Memcached 1 and Redis 2 . For real-time and high-throughput applications, distributed key-value store is routinely used. Distributed hash table implementation in high-end HPC environments has been reported in the literature <ref type="bibr" target="#b23">[24]</ref>. Persistent key-value store is used when data cannot fit into the memory, or it is necessary to persistently store the data. Popular solutions include BigTable <ref type="bibr" target="#b8">[9]</ref>, LevelDB <ref type="bibr" target="#b2">3</ref> , and Cassandra 4 , etc.</p><p>A key efficiency technique among key-value store systems is Sorted String Tables (SST), which is based on log-structured storage <ref type="bibr" target="#b29">[30]</ref>. In our work, we have implemented our own SST and extended it to work with global and parallel file systems. In addition to the traditional keyvalue store data models, we further developed a graph-based method to capture advection-based predictions of on-demand data access needs.</p><p>Our work is related to data caching and prefetching techniques. In parallel applications, I/O signatures are automatically traced and used to guide prefetching in MPI-IO <ref type="bibr" target="#b5">[6]</ref>. Pre-execution techniques are also leveraged to hide parallel I/O latencies without perceivable patterns <ref type="bibr" target="#b12">[13]</ref>. In our work, the data access patterns which are based on flow advection, are extracted and reused during the runtime. Although prefetching is usually beneficial, over-prefetch that saturations the system could impact performance. As demonstrated for remote visualization in <ref type="bibr" target="#b34">[35]</ref>, our system also needs to adaptively optimize prefetching parameters at runtime. <ref type="figure">Figure 1</ref> illustrates the pipeline of our approach. The central part is sparse data management, which provides high performance data retrieval for the particle tracers in visualization and analysis applications. Our pipeline is generally applicable to post-processing (e.g. the input is files containing raw flow field data), and in-situ visualization as well as co-processing, where the input to our pipeline is unsteady flow data from live simulations. The data conversion process partitions the raw data into blocklets, which are indexed by their spatiotemporal locations. Tracer processes focus on advection computation, request blocklets by spatiotemporal indices, but are otherwise not involved in concurrently managing I/O, transfer and in-core storage aspects of the unsteady flow data.  <ref type="figure">Fig. 1</ref>. The pipeline of the advection-based sparse data management. Raw data from storage or simulation is converted by fine-grained data partitioning, and then handled by the sparse data management system. The system provides high performance data access with prefetching, which brings both efficiency and scalability for field line tracing in various visualization tasks. The access patterns can be updated in runtime and effectively reused in later runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ADVECTION-BASED SPARSE DATA MANAGEMENT FOR PAR-TICLE ADVECTION</head><p>There are two mechanisms to manage data flow in our system, namely the parallel key-value store and the prefetching. The parallel key-value store is efficient and scalable. Better performance could be achieved by adding more computing resources, e.g. cores, memory, and I/O bandwidth, etc. The prefetching in our system uses a graph based model to capture and represent access patterns. We refer to this graph as prefetching hints. The prefetching hints are reusable and constructed on-the-fly. When prefetching hints have been gradually added as advection computation progresses, the reusability of prefetching hints considerably expedites data access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Flow and Process Models</head><p>As shown in <ref type="figure">Figure 2</ref>, data primarily flows from the parallel key-value store to tracers. The required data entries (e.g. cells, blocklets) are fetched from the parallel key-value store, which performs actual data I/O from the file system. Tracers update the parallel key-value store by defining new prefetching hints, so that the prefetch hints graph are continuously constructed and expanded. Each tracer also employs a simplistic LRU-cache (implemented as a simple RAM key-value store) for faster data access. In this regard, data access by a tracer is quite similar to data access on a CPU with a multi-layer cache, where if the required entries are not cached by the tracer, then it takes a longer time to fetch from the parallel key-value store. If the parallel keyvalue store does not have the data either, then the data will be fetched from the file system and take an even longer period of time. Of course the parallel key-value store leverages the hints graph to aggressively prefetch to avoid cache-miss, while also implementing flow-control to avoid congesting the I/O system.</p><p>Parallelism is achieved with distributed processes and message passing. There are two types of processes: tracer processes and keyvalue store processes (k-v processes). Tracer processes and k-v processes use the same runtime API (Section 3.3) to request/retrieve and send data entries.</p><p>For load-balance, k-v processes partition the key space by hashing and round-robin assignment. A key k is assigned to a k-v process according to: i = hash(k) mod n, where n is the number of k-v processes. Data requests from tracers are directly sent to the corresponding k-v processes, via point-to-point communication.</p><p>In addition to data requests, a tracer can also issue a prefetch hint as a chain of predicted data requests, where each request is identified by a hash key in the same way as in real data requests. Every prefetch hint is first sent to the k-v process in charge of the first key in the chain. That k-v process processes the first prefetch request, and then recursively forwards the remaining part of the chain to the k-v processes in charge of the first remaining prefetch request. This forwarding process iterates until the chain has been exhausted. After a blocklet has been prefetched, the k-v process keeps that data and also forwards that blocklet to the original tracer process that has requested that entry.</p><p>This process model is flexible for different environments. There are three scenarios. The first is to run tracer and k-v processes in the The software architecture of the system. There are two kinds of roles in the process model, namely the tracer processes and key-value store processes (i.e. k-v processes). Logically, data retrieval from tracer processes follows the hierarchy from the local and parallel key-value stores, and finally the file system.</p><formula xml:id="formula_0">I/O Requests Non-blocking Queries Local K-V Store~1μs Parallel K-V Store~1</formula><p>same MPI communicator (MPI COMM WORLD). The second way is to run the k-v processes as a service, where at the start of the analysis, the parallel key-value store has already been populated and can lead to even better analysis performance. This can be effectively implemented through dynamic process management as provided by MPI-2 (MPI Publish name), provided that the job scheduler of the computing facility supports that feature of MPI-2. The third is where an analysis process manages tracer and k-v threads, as opposed to tracer and k-v processes. To conservatively measure performance, our tests in this paper are run in the first way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prefetching and Access Pattern Reusability</head><p>Prefetching is the key to hide the latency of data access. The parallel key-value store not only loads and returns the requested data entries to the tracers, but also prefetches the highly possible entries to retrieve according to the access patterns. <ref type="figure" target="#fig_2">Figure 3</ref>(a) illustrates the prefetching hint graph for a small dataset. The data domain is uniformly partitioned into 8×8 blocklets, and each of them is indexed by their X-and Y-coordinates marked on the boundaries. In the key-value store, the indices (X,Y ) are used as the keys, and the blocklets data are stored as values. At the very beginning, there is no edges (prefetching hints) in this graph. During the particle tracing, the tracer issues a prefetching hint, if a particle passes from one blocklet (X,Y ) to another (X ′ ,Y ′ ). Thus, a prefetching hint is de- fined as (X,Y ) → (X ′ ,Y ′ ). As tracers run several times with some of different problem sets, more and more prefetching hints will join the graph. In a subsequent run, the prefetching hints will be reused. In <ref type="figure" target="#fig_2">Figure 3</ref>(a) left, the tracers compute two pathlines, which are seeded from the blocklets (1, 1) and <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>. The two blocks are requested by the tracer processes and fetched by the k-v processes. The k-v processes not only return the requested blocklets, but also prefetch the corresponding blocklets according to the prefetching hints. For example, when (1, 1) is requested, (1, 2), (1, 3). . . are also prefetched and sent to the tracers.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b), the access patterns are stored as prefetching hints in the data structure. Essentially, we extended the traditional key-value store model by adding a pre-feching hints field. Each data entry is stored as:</p><formula xml:id="formula_1">&lt;key, value, pf hints[]&gt;,</formula><p>where pf hints[] are the keys of the data to prefetch. Essentially, a directed graph was constructed by the data entries and prefetching hints. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates an example of the prefetching hint graph and the data structures.</p><p>It is noteworthy that the sparse data management itself does not generate prefetching hints. Instead, the prefetching hints are issued by the tracer processes, and reused by the k-v processes as exemplified by the above example. By iteratively running tracers, more and more prefetching hints will be added into the store, and then accelerate the data access in further runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Runtime APIs</head><p>The runtime API of the sparse data management promotes a similar design to state-of-the-art key-value store system for data retrieval. In the tracers, the key is the spatiotemporal index of the cell/blocklet, and the value stores the actual data. Users can get or modify the values (actual data) by get() or put() functions. In addition several additional APIs are designed for tracers to define prefetching hints, as the runtime itself does not analyze the access patterns of the flow data. The tracers need to explicitly define the prefetching hints to accelerate further analysis. During the data retrieval, the prefetching depths could also be optionally assigned. The runtime will recursively load the data in the memory for fast access. The most important runtime APIs provided to the tracers are as follows:</p><p>get(key, &amp;value, pf depth) If the entry key locally exists, return the corresponding data as value. pf depth is an integer which limits the maximum depth of prefetching. The runtime will prefetch the values corresponding to the keys in pf hints by Breath-First Search (BFS) in a recursive manner.</p><p>Prefetching is not performed if the depth equals to zero.</p><p>put(key, value) This function is equivalent to put in other key-value store systems. It sets the entry key to value. The entry is removed if value is empty.</p><p>add hint(key, key') Add a prefetching hint for the entry associated with key. Essentially, it adds an edge key-&gt;key' to the prefetching hint graph.</p><p>reset hints(key) Remove all prefetching hints of the entry that corresponds to key.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Task-Parallel Particle Advection with Sparse Data Management</head><p>Task-parallel particle advection is considered to be the most straightforward approach in visualizing large-scale unsteady flow, despite of complex I/O and data management. However, it is very difficult to scale due to the data access problems. Our method improves both I/O bandwidth-efficiency and scalability by managing the complex data store and access. It also simplifies the data management on the tracer side, thus users do not need to manually manage the memory use and synchronization. The only additional effort for users is to define the prefetching hints when the particle travels from one cell/blocklet to another, because the runtime cannot differentiate the traversal sequence when the runtime is concurrently accessed. The pseudo code of tracing the pathline from a given seed is shown in Algorithm 1. Massive particles can be traced with more processors in "share-nothing" and naive task-parallelism way. In our applications, especially for local-range analysis, the sparse data management only loads the data blocklets that are requested or potentially used for the analysis, without accessing the whole data. The sparse data management scheme not only simplifies the data access, but also enhances the memory and I/O bandwidth efficiency of task-parallel particle tracing. GetReq key='b' depth=1 src=1 <ref type="figure">Fig. 4</ref>. An example of inter-process communication: (a) data entry 'a' is requested by an tracer process; (b) the corresponding k-v process returns entry 'a', and forward a request to prefetch 'b'; (c) the prefetched entry 'b' is also returned to the tracer process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION AND PERFORMANCE EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inter-Process Communication</head><p>We have developed our prototype system using C++ with hybrid MPI/thread parallelism. All data communication are through message passing. In typical scenarios, the communication pattern follows the data access hierarchies shown in <ref type="figure">Figure 2</ref>. Based on the software architecture, there are three types of inter-process communications in our framework, namely tracer-to-store, store-to-tracer, and store-tostore communications. Notice that there is no communication between tracer processes. In general, the tracer processes send requests to the corresponding k-v processes, and the data entries are then sent back to the tracers. Data prefetching is done via store-to-store communications. Through the runtime API, different types of messages are sent and processed, including GetRequests (requesting an entry), SetRequests (updating or inserting an entry), HintRequests (modifying the prefetching hint of an entry), FlushRequests (dumping all updated entries into the file system) and ExitRequests (exiting after all requests are processed), etc. There are other messages for delivering data entries, process states and statistics. GetRequests are the most important requests for internal data exchange among processes. Taking (pre)fetching for example <ref type="figure">(Figure 4</ref>), GetRequests can be used for both data requesting (tracer-to-store) and for forwarding prefetching requests (store-to-store).</p><p>Asynchronous communication is leveraged to reduce the latencies. All messages are queued, and then serialized and sent to the destination processes. After receiving, the messages are deserialized and then processed. We use Google protobuf library <ref type="bibr" target="#b4">5</ref> to handle message serialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local and Parallel Key-Value Store</head><p>Regardless of the roles of processes, each runtime instance keeps a local key-value store, The local key-value store, which acts like cache (e.g. Memcached), keeps the recently used data entries and greatly improves the data locality. For tracer processes, during the particle advection, the same entries are often accessed several times from the local key-value store, without fetching from the k-v processes.</p><p>In the prototype system, in order to limit the memory footprint, Least-Recently Used (LRU) policy is employed in the local key-value store. The implementation is based on a linked list and hash table, which keep the actual key-value pairs and the pointers to the linked list nodes, respectively. During the data access, the addresses of the entries are looked up in the hash table, and the orders of linked list are updated. When the cache is reaching the capacity, the least recently used entries in the tail of the linked list are removed. Thread-safety is provided for concurrent data access. The performance of the key-value store is comparable to decent production libraries. In our experiment, the set and get throughput of the local key-value store can reach up to 1.4M and 2.6M entries per second, respectively.</p><p>The parallel key-value store is built upon the local store. Data is partitioned and distributed by keys to achieve data-parallelism. During the lookup, the runtime first checks if the data entry is locally available, other wise sends a GetRequest to the destination process according to the data partitioning. Prefetching can further enhance the performance of the parallel key-value store, if applicable. Depending on the interconnection latency and runtime payload, the latency of getting an entry from the parallel key-value store is 10 to 100 microseconds in our experiments. The overall throughput is scalable if accessed concurrently. The parallel data store also provides larger space for data caching. For example, in a typical configuration with 16 k-v processes, if the size limit of each local key-value store is 1GB, then the overall size of the parallel store space will be 16GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Persistent Key-Value Store with Global SST</head><p>Our persistent key-value store is based on Sorted String <ref type="table">Tables (SST)</ref>, which is a simple log-structured storage solution <ref type="bibr" target="#b29">[30]</ref> widely used in many popular production systems like BigTable <ref type="bibr" target="#b8">[9]</ref>. In our prototype system, we extend SST to Global SST, where data is stored in a high performance file system, such as a parallel file system or one enhanced by SSD. We omitted data replication features for better performance.  <ref type="table">Table File</ref> SSTn ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5. Reading and writing data entries with Global SSTs.</head><p>The flow chart of reading and writing entries to SSTs is illustrated in <ref type="figure">Figure 5</ref>. Generally speaking, an SST is as simple as a data file with an index file (SST Index). The data file stores the actual values, while the SST Index records the key:offset pairs for fast access, where offset is the position of the value in the data file. The system may generate a series of SSTs during execution, and an SST is immutable after it is generated. When a key exists in multiple SSTs, the one in the latest SST takes precedence.</p><p>To make SSTs into Global SSTs, we introduce the partition filter for membership test. In our method, it is imperative to be able to change N, the number of k-v processes that access the persistent key-value store. Traditionally, SSTs may be written with different data partitioning scheme defined by number of k-v processes N and the process id i. However, with such partitioning methods, given K entries and N slots, it is very hard to change N due to the expensive data redistribution. Consistent hashing <ref type="bibr" target="#b19">[20]</ref> is proposed to solve this problem, yet data moving is still required. As we need all SSTs to be globally accessible to all k-v processes, we address this problem by adding a partition filter for SSTs. It checks whether a queried key belongs to the key space partition of an SST. The query could be accelerated by the membership test with partitioning schemes defined by N and i. For example, an SST was written by the i-th k-v process (N processes in total), then partition filter checks if hash(key) mod N equals to i.</p><p>Our global SST also uses other standard SST filters to accelerate membership test. Statistic filter tests whether the queried key is within the range of the key (the maximum and minimum keys in an SST). Bloom filter <ref type="bibr" target="#b3">[4]</ref> is used to check whether a key belongs to an SST in constant time. Keys that have passed all of the filters will be actually queried in the SST indices using binary search.</p><p>During initialization, the stored Global SST meta data are loaded into memory across all k-v processes. SST indices are cached in memory, not all contents in an SST index need to be loaded.</p><p>Key-value pair query is straightforward. A k-v process first checks whether a queried key is available in local cache. If not, it then check whether the key passes the filter of SST k , SST k−1 , . . . , SST 1 , where SST k and SST 1 are the latest and earliest ones, respectively. If the key does not pass the SST filters, then the k-v process returns the query with a "not found". If the key passes the SST filters, the k-v process then checks whether the key is in the indices of the SSTs. Finally, the k-v process reads the actual data (i.e. value) from file and return the value. The actual read operation in prefetching works similarly.</p><p>For insertion into key-value store, the data record goes directly to a standard MemTable (i.e. an in-core lookup table). We implemented our MemTable as a red-black tree. When the size of MemTable exceeds the limit or receives a flush request, it will be dumped into an SST and written to the file system asynchronously. We omitted fault tolerance features (e.g. commit logs) that are important to other applications for simplicity and for reducing system overheads. For example, when the tracer abnormally exits, losing an newly inserted prefetching hint does not affect the completeness of data for future uses.</p><p>In our experiments, the peak query performance of our global SST implementation can reach up to 900 key-value pair reads per second on a 7200rpm hard drive, and about 9000 pair reads per second on a Solid-State Drive (SSD) with SATA 3.0 connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPLICATION CASES AND PERFORMANCE EVALUATION</head><p>We demonstrate three driven application cases in flow visualization and analysis with our methods, including streak surface computation, source-destination queries, and FTLE computation. The first two are typical local-range analysis, as only a small portion of data needs to be actually accessed. The third is usually full-range analysis, yet sometimes be local if regions of interest and limited time scopes are focused. We also have different environments for the performance benchmark, ranging from single workstation to supercomputing environments. The datasets and test platforms are enumerated in <ref type="table">Table 1</ref> The Beowulf cluster consists of eight computing nodes and one I/O node. Each computing node is equipped with two quad-core Intel Xeon E5520 CPUs, which work at 2.26GHz and with 48GB RAM. The I/O node shares a Lustre parallel file system to computing nodes, which is composed by only one OST (Object Storage Target). The storage device is a RAID6 disk array which contains 16 2TB HDDs. The interconnection between all nodes is InfiniBand QDR with 40Gbps theoretical bandwidth.</p><p>The single workstation platform has a quad-core Intel i5-4670 CPU, which operates at 3.40GHz. The main memory in this workstation is 16GB. The workstation is also equipped with two different consumable storage devices, including a Seagate 1TB HDD and an Intel 320 Series 120GB SSD. The two hard drives are connected to the motherboard with SATA 3 interface (6Gbps theoretical bandwidth). Random I/O performances on the both devices are reported to be 75∼100 IOPs and ∼20K IOPs, respectively.</p><p>We also use the x86-based supercomputer in National Supercomputing Center in Jinan for the tests. It consists of 700 computing nodes, and each of them has two Intel Xeon E5675 processors (hexa-core, 3.06GHz) and 36GB main memory. Our allocation can use about 10% of the resources. The interconnection is InfiniBand QDR with a theoretical bandwidth of 40Gbps. SunWay Global File System (SWGFS) is provided for high performance parallel I/O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case I: Streak Surface Computation</head><p>Streak surfaces <ref type="bibr" target="#b13">[14]</ref>, which visualize unsteady flow by advecting continuously released particles from given seed curves, are capable of depecting the flow field over the entire lifetime. A streak surface is a mesh which consists of the locus of a set of particles that are advected by a time-varying flow field. As the integration goes on, new particles seeded at different time steps are added into the surface. The meshes are usually constructed and then refined to obtain better visual effects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>. The main bottleneck in streak surface computation is the massive particle tracing, whihc requires extensive access of the whole dataset.</p><p>In the experiment, highly densely-seeded particles are generated from the seed line. In current implementation, the surface generation is done during the post-processing by triangulation, instead of using commonly-used quadrangular techniques <ref type="bibr" target="#b25">[26]</ref>, as the particles are dense enough to generate fine visualization results in this case. Further implementation could be added to adptively refine the surface on-thefly. In the particle tracing stage, seeds are partitioned by round-robin and traced in different tracer processes in parallel. With the sparse data management scheme, the naive task-parallelism implementation can still keep scalability. As streak surface computation only needs to access a small portion of data, the on-demand and high performance data access greatly reduces the I/O cost for the computation. <ref type="figure" target="#fig_4">Figure 7</ref> shows the visualization results of a large scale turbulence simulation data with our method. A seed line is selected to show the features in the unsteady flow. The dataset is defined on a curvilinear grid, with the spatial resolution of 1024 × 1024 × 720. In our experiments, we use 100 time steps for streak surface generation. As 3 velocity components are used, the effective data size is 0.82TB. The dataset is further partitioned and stored in the sparse data management system, and each blocklet contains 8 × 8 × 8 × 1 cells (9 × 9 × 9 × 2 grid size). The overall key-value store size is about 2.2TB.</p><p>The benchmark results with different memory limits and number of processes are shown in <ref type="figure">Figure.</ref> 6. The most noteworthy point in this result is the data handling of TB-scale data with limited hardware resources. First, the sparse data management is extremely memoryefficient. For 64 processes, the total amount of the distributed memory cannot fit a terabyte, but the system is even workable with very small memory limit. In this memory-bounded case, it shows super-linear acceleration as the number of processes increases. Second, the system is I/O bandwidth efficient. On average, only 10GB out of 2.2TB data is accessed to compute a streak surface. With traditional methods which require in-core flow data management, given a decent fusion parallel I/O bandwidth (say 10GB/s), merely I/O time will take about 100 seconds ideally. Yet it is not possible to keep the TB-scale data in memory on relatively a small number of computing nodes. In our results, it only takes 92 seconds to finish the computation on 8 computing nodes (including the data access time), while keeping very low memory footprint.</p><p>Although the data scale of the key-value store is usually larger than traditional data formats, our method enables large-scale visualization and analysis with resource-bounded computing facilities. The main trade-off is between the storage space and the resources (memory footprint, I/O bandwidth, etc.), i.e. larger storage space for lower memory footprint, larger storage space for lower I/O access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case II: Source-Destination Queries</head><p>Source-destination queries are important to investigate teleconnection relationships of arbitrary two regions in the flow field. In real  <ref type="figure">Fig. 6</ref>. The performance of the streak surface computation of the terabyte-scale turbulence simulation with different number of processes and configurations. The number of tracer processes and k-v processes are 1:1. Read bandwidth is the amount of bytes read divided by total running time, and total bytes communicated is the summation of sent messages sizes from all processes. Super-linear acceleration is observed when the memory limit is small. As the number of processes increases, the timings, under different memory limit, converge because the total amount of distributed memory is enough to fit the requested data.     applications, end users can make hypotheses on source-destinations by defining regions, and validate them by visualizing the field lines between the two regions. For example, the CO 2 exchange from Northern and Southern Hemispheres, pollution dispersion between the source to sensitive regions, etc. The queries are completed by tracing dense seeded pathlines from the source region. In our experiment, both memory-and I/O-bandwidth efficiency can be achieved with ondemand data access and fine-grained data partitioning.</p><p>In this case, we use simulation result from GEOS-5 global climate model which is developed by NASA Goddard Space Flight Center. The spatial resolution of this model is 1 • × 1.25 • , with 72 vertical pressure levels. The dataset consists 24 monthly averaged results of the simulation. The output of this model is stored in hybrid-sigma grid, which requires a customized interpolation scheme for particle tracing with Runge-Kutta 4th order numerical integral method. 4 out of 32 variables are used for the analysis, which are necessary for calculating the three components of the wind speed vector.</p><p>In the conversion stage, the raw data is partitioned into small blocklets of cells. In the optimal experiment, each blocklet contains 8 × 8 × 71 × 1 cells (9 × 9 × 72 × 1 grids). Instead of using individual cells, we have to keep a whole vertical column of data, in order to precisely interpolate the attribute values in the hybrid-sigma grid.</p><p>We select two regions to see how massless particles trace with the wind field from North America to East Asia within a month. 200 pathlines are uniformly seeded every month (wall clock) from the source region, and then advected in the entire domain. In <ref type="figure">Figure 8</ref>, there</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>September, 2012</head><p>September, 2011 <ref type="figure">Fig. 8</ref>. Source-destination query on GEOS-5 dataset. Every month there are 200 pathlines initially seeded in red and gradually changed to be green as they advect in the domain. Results that meet the query from North America to East Asia within two months are visualized, respectively.</p><p>are 309 pathlines in total that meet the source-destination query in September 2011 and September 2012.</p><p>We test the performance on GEOS-5 dataset on a single workstation to see the I/O bandwidth efficiency of our method <ref type="figure" target="#fig_6">(Figure 9</ref>). Three tracer processes and one k-v process run on the quad-core CPU. For each test run, we dump Linux page caches to obtain fair timings. The sparse data management greatly decreases the amount of the data access. With the optimal configuration, only 2.77% blocklets are actually accessed for this specific query. Although the size of key-value store is usually larger than the raw data, our method can still save 84.5% amount of bytes to read from the file system. In the results, SSD usually outperforms HDD, because the key-value store implementation heavily relies on random I/O operations. Prefetching also brings benefits. We ran different queries multiple times, thus the access patterns are written and effectively reused in each run. With prefetching, the overall computation time is decreased by 18.75%. In addition, we have also tested the performance on the Beowulf cluster ( <ref type="figure" target="#fig_7">Figure 10</ref>). Prefetching also improves the scalability in parallel environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case III: FTLE Computation</head><p>FTLE is widely used to measure the separation and to extract Lagrangian Coherent Structures (LCS) in flow field. The FTLE value at a certain point indicates the possibility to diverge from particles around this point within a finite time scope T . However, FTLE computation is expensive due to the massive particle tracing from dense sample points.</p><p>In this case, we demonstrate scalable FTLE computation with taskparallel particle tracing based on the sparse data management. Previously, both sample reduction <ref type="bibr" target="#b1">[2]</ref> and massively data-parallelism <ref type="bibr" target="#b27">[28]</ref> techniques are presented to accelerate the FTLE computation. Although task-parallel particle tracing (partitioning over seeds) is the most straightforward way to achieve parallelism in a "share-nothing" manner, the efficiency and scalability are limited. By improving the memory and I/O bandwidth-efficiency with the sparse data management, we are capable of observing good scalability of this method.</p><p>We trace the uniformly-seeded pathlines and compute the FTLE field of the Hurricane Isabel dataset, which is from an atmospheric simulation. The spatial resolution of the data grid is 500 × 500 × 100. 48 hourly averaged data is stored in separated files. <ref type="figure" target="#fig_8">Figure 11</ref> are the visualization results of the FTLE field. We choose the optimal blocklet size 4 × 4 × 4 × 1 for data conversion. Pathline seeds are uniformly placed in the data domain. The scalability is studied on the supercomputing environment. The performance with different number of processes are shown in <ref type="figure">Figure 12</ref>. Without complicated task distribution strategies, our method still shows good scalability as number of processes increases. Prefetching greatly improves both performance and scalability in this case. With all number of processes, the run with prefetching outperforms the run with no prefetching. On 256 processes, the run with no fetching does not accelerate anymore, but the timings with prefetching are keeping descending as the number of processes increases.</p><p>FTLE is usually regarded as full-range analysis, as users often tend to visualize the global distributions. Similar to FTLE, the denseseeded pathline computation can also be extended to other analysis, including ensemble flow simulation <ref type="bibr" target="#b16">[17]</ref> and pathline attributes <ref type="bibr" target="#b15">[16]</ref>. Yet, such tools are also used locally when a region of interest is focused, or a limited time scope is chosen. Benefits in local-range analysis with our method are still effective when the particles are traced in a local region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Observations and Discussion</head><p>All cases demonstrated in this section are from typical applications from flow visualization and analysis. The advection-based sparse data management greatly benefits the data access in various platforms, ranging from single workstation to supercomputing environments. In summary, the key observations in the three cases are as follows:</p><p>• Case I: Our method is memory-efficient, and it enables scalable visualization of large-scale unsteady flow (e.g. terabyte scale turbulence flow) while requiring a very limited amount of hardware resources.</p><p>• Case II: Our method is I/O bandwidth efficient. It greatly reduces the amount of I/O with fine-grained and on-demand data access.</p><p>• Case III: Our method greatly improves both performance and scalability of the naive task-parallel particle tracing.</p><p>There are primarily two parameters for performance tuning, including the prefetching depth and the blocklet size. Both of them are usually empirically determined. As studied by Sisneros et al. <ref type="bibr" target="#b34">[35]</ref>, in multi-leveled cache architecture, there is no theoretical ways to choose optimal prefetching parameters. For example, in processor caching, the prefetching policies and word sizes are often determined by experiments. In flow visualization and analysis, as the data features access patterns are even more complex, it requires even more efforts to achieve optimal performance. The prefetching depth cannot be too large, because over-prefetching may saturate the bandwidth <ref type="bibr" target="#b34">[35]</ref>. In  <ref type="figure">Fig. 12</ref>. The performance of the FTLE computation of the Isabel dataset with different number of processes and configurations. (a), (b) and (c) are tested on the Beowulf cluster, while (d), (e) and (f) are tested on the supercomputer. The number of tracer processes and k-v processes are 1:1. Performance with and without prefetching is shown in (a) and (d). (b) and (e), (c) and (f) display the read bandwidth (with total bytes read) and total bytes communicated with prefetching, respectively. Both performance and scalability are improved in the task-parallel particle tracing using prefetching.</p><p>general, it is usually more memory-efficient to use smaller blocklet size. However, due to the additional ghost grids storage in blocklets, it may require additional data access if the blocklets are too small. Additional space and time costs are also incurred by the key-value store.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this work, we have explored a novel advection-based sparse data management scheme for visualizing large unsteady flow data. We have made extensive use of key-value store to manage blocklets in memory, and used hint graphs for predictively guide data I/O. When visualizing terabyte-scale unsteady flow data, our approach achieved significantly better space efficiency in memory, as well as high performance and scalability. Our results demonstrate that such sparse data management greatly increases the scale of local-range analyses that are feasible for resource-constrained systems, while it also improves the scalability of full-range task-parallel particle tracing.</p><p>For future work, we want to extend our method for use by in-situ visualization. Our data management can be extended to handle irregular and unstructured grid data. We would like to also improve the finegrained data partitioning by considering adaptive mesh refinement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc>information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. Manuscript received 31 Mar. 2014; accepted 1 Aug. 2014 ate of publication 2014; date of current version 2014. 11 Aug. 9 Nov. D . Digital Object Identifier 10.1109/TVCG.2014.23464 8 8 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. The software architecture of the system. There are two kinds of roles in the process model, namely the tracer processes and key-value store processes (i.e. k-v processes). Logically, data retrieval from tracer processes follows the hierarchy from the local and parallel key-value stores, and finally the file system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The data structures in the sparse data management: (a) the prefetching hint graph of a small dataset; (b) key-value store with prefetching hints. The prefeching hints are issued by the tracers, and reused by the parallel key-value store.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Streak surface visualization of a terabyte-scale turbulence simulation: (a) the traced particles for surface generation; (b) the streak surface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Data access amount and performance on single workstation with different partitioning granularities (GEOS-5 dataset): (a) the data access amount with different blocklet size; (b) the performance on SSD hard disk; (c) the performance on HDD hard disk. Optimal prefetching depths are highlighted in (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>The performance of the source-destination queries of the GEOS-5 dataset with different number of processes and configurations. The number of tracer processes and k-v processes are 1:1. Read bandwidth is the amount of bytes read divided by total running time, and total bytes communicated is the summation of sent messages sizes from all processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>FTLE field of Isabel dataset at (a) time step 0 and (b) time step 20.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://memcached.org 2 http://redis.io 3 http://code.google.com/p/leveldb 4 http://cassandra.apache.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://code.google.com/p/protobuf</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Prof. Qingdong Cai for providing the TB-scale turbulence simulation data. This work is supported by NSFC No. 61170204. This work is also partially supported by NSFC Key Project No. 61232012 and the "Strategic Priority Research Program -Climate Change: Carbon Budget and Relevant Issues" of the Chinese Academy of Sciences Grant No. XDA05040205.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Iteration aware prefetching for unstructured grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">O</forename><surname>Akande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rhodes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Big Data</title>
		<meeting>the 2013 IEEE International Conference on Big Data</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="219" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive refinement of the flow map using sparse samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Barakat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tricoche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2753" to="2762" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visualization at supercomputing centers: The tale of little big iron and the three skinny guys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bethel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Rosendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Southard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gaither</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interactive streak surface visualization on the GPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bürger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Theisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1259" to="1266" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel I/O prefetching using MPI file caching and I/O signatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Byna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<idno>1- 44:12</idno>
	</analytic>
	<monogr>
		<title level="m">SC08: Proceedings of the ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imaging vector fields using line integral convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Leedom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 1993</title>
		<meeting>SIGGRAPH 1993</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Streamline integration using MPI-hybrid parallelism on a large multicore architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pugmire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Joy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1702" to="1713" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BigTable: A distributed storage system for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;06: Proceedings of Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flow-guided file layout for out-of-core pathline computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nouanesengsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Symposium on Large Data Analysis and Visualization</title>
		<meeting>IEEE Symposium on Large Data Analysis and Visualization</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A flow-guided file layout for out-of-core streamline computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium</title>
		<meeting>IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimizing parallel performance of streamline visualization for large distributed flow datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujishiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Pacific Visualization Symposium</title>
		<meeting>Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hiding I/O latency with pre-execution prefetching for parallel applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Byna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC08: Proceedings of the ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Surface-based flow visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edmunds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Laramee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="974" to="990" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient computation and visualization of coherent structures in fluid flow applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gerhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tricoche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1464" to="1471" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable Lagrangian-based attribute space projection for multivariate unsteady flow data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization</title>
		<meeting>IEEE Pacific Visualization</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled ensemble flow line advection and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2733" to="2742" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinguished material surfaces and coherent structures in three-dimensional fluid flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="248" to="277" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical line integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hlawatsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1148" to="1163" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the world wide web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lewin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;97: Proceedings of the ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="654" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward a general I/O layer for parallel-visualization applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="6" to="10" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simplified parallel domain traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erickson</surname></persName>
		</author>
		<idno>1-10:11</idno>
	</analytic>
	<monogr>
		<title level="m">SC11: Proceedings of the ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The state of the art in flow visualization: dense and texture-based techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laramee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doleisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vrolijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="222" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ZHT: A light-weight reliable persistent dynamic scalable zero-hop distributed hash table</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brandstatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Raicu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS&apos;13: Proceedings of IEEE International Symposium on Parallel and Distributed Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="775" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Over two decades of integration-based, geometric flow visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laramee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Peikert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1807" to="1829" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constructing streak surfaces for 3D unsteady vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Laramee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCSG&apos;10: Proceedings on Spring Conference on Computer Graphis</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed parallel particle advection using work requesting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Symposium on Large Data Analysis and Visualization</title>
		<meeting>IEEE Symposium on Large Data Analysis and Visualization</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parallel particle advection and FTLE computation for time-varying flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nouanesengsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC12: Proceedings of the ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Load-balanced parallel streamline generation on large scale vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nouanesengsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1785" to="1794" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The log-structured merge-tree (LSM-tree)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>O'neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Inf</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="385" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A study of parallel particle tracing for steadystate and time-varying flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nouanesengsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS11: Proceedings of IEEE International Symposium on Parallel and Distributed Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="580" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The state of the art in flow visualisation: Feature extraction and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vrolijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laramee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doleisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable computation of streamlines on very large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pugmire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC09: Proceedings of the ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UFLIC: a line integral convolution algorithm for visualizing unsteady flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Visualization 1997</title>
		<meeting>IEEE Visualization 1997</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="317" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A multi-level cache model for run-time optimization of remote visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sisneros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Samatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="991" to="1003" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cache-oblivious mesh layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lindstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parallel hierarchical visualization of large time-varying 3D vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC07: Proceedings of the ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
