<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Intelligent System Approach for Probabilistic Volume Rendering using Hierarchical 3D Convolutional Sparse Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><surname>Minh Quan</surname></persName>
						</author>
						<title level="a" type="main">An Intelligent System Approach for Probabilistic Volume Rendering using Hierarchical 3D Convolutional Sparse Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2017.2744078</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Volume Rendering</term>
					<term>Machine Learning</term>
					<term>Hierarchically Convolutional Sparse Coding</term>
				</keywords>
			</textClass>
			<abstract>
				<p>(a) Grayscale (b) 1D (c) 3D (d) 11D (e) 75D-CSC Fig. 1: Comparison of different volume visualization results. (a) Grayscale volume rendering, (b) 1D transfer function, (c) 3D transfer function Kniss et al. [17], (d) 11D intensity-driven feature with the random forest classifier Soundararajan and Schultz [29], and (e) ours. Among all, our method results in the most accurate classification (vase, leaves, trunks, and soils are clearly separated), showing the discriminative power of high-dimensional feature vectors generated by using hierarchical 3D convolutional sparse coding. The same user scribble input is used for (d) and (e).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Direct volume rendering (DVR) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19</ref>] is a powerful visual tool for interpreting three-dimensional (3D) volumetric data used in numerous fields, such as medicine, natural and applied sciences, and engineering. Significant research efforts have been made to improve the speed and quality of DVR over the past few decades, including designing transfer functions <ref type="bibr" target="#b26">[26]</ref>. The transfer function involves mapping data values to optical properties, i.e., color and opacity, and classifies which region in the data is visible. However, designing good transfer functions poses many challenges, especially to non-expert users, for a number of reasons. Most transfer function design approaches require significant manual user interactions, as the process inherently involves trial-and-error and is based on human decisions using data statistics, such as a histogram of the feature values. In addition, although multidimensional transfer functions have become increasingly popular due to their superior capacity for the classification of complex data, the feature spaces that can be explored in multi-dimensional transfer functions are E-mails: {quantm, juny0603, goodhen2, wkjeong}@unist.ac.kr. The authors are with Ulsan Nat'l Inst. of Science and Technology (UNIST). †Both authors contributed equally to this work, * Corresponding author hardly higher than three dimensions because of difficulties arising from the large perceptual gap between the transfer function domain and the spatial domain.</p><p>The other alternative to the conventional transfer function is explicit selection in the spatial domain to classify visible regions. The seminal work by Tzeng et al. <ref type="bibr" target="#b33">[33]</ref> introduced high-dimensional probabilistic classification based on direct user selection of the input volume. The proposed method is an intelligent system approach, meaning that users specify the regions of interest using a paintbrush tool, whereupon the selected regions (i.e., voxels) are used to build high-dimensional features to train a machine learning classifier. In this work, intensity-based features, such as voxel intensity values, the magnitude of the gradient, and the current voxel location, are used to define 11 dimensional feature vectors. A recent work by Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref> compared various supervised learning algorithms for classification within the same intelligent system approach to suggest the best classifier. This alternative approach provides easy-to-use, high-level user interaction tools for leveraging high-dimensional features for interactive classification in DVR.</p><p>The main motivation for the proposed work stems from the following observations: The intelligent system based on high-dimensional features has opened the door to a new research direction for leveraging machine learning in DVR. However, the intensity-based feature model adopted in the conventional intelligent system may not work well under certain harsh conditions (e.g., data that contain noise and anisotropic shapes). For example, as shown in <ref type="figure">Figure 1</ref>, conventional methods fail to clearly separate different structures or noise from the objects in the volume. This failure occurs because the intensity-based features cannot distinguish noise and voxels from the object of interest if they fall into a similar intensity and gradient range. In addition, there exist various image modalities, e.g., low-dose computer tomography, ultrasound, and optical coherence tomography, etc, that are either noisy or have image structures that cannot be easily interpreted using conventional intensitybased algorithms. Another observation is that the recent advances in machine learning and deep neural networks have significantly improved the image classification accuracy <ref type="bibr" target="#b17">[18]</ref>, but their direct adaptation in volume rendering has not been actively explored yet.</p><p>To address these problems, we introduce a novel high-dimensional feature model based on the 3D convolutional sparse coding (CSC) <ref type="bibr" target="#b14">[15]</ref>, a state-of-the-art unsupervised learning-based sparse representation algorithm, for accurate voxel classification in DVR. Inspired by deep convolutional neural networks, we propose a novel hierarchical CSC that successively builds up dictionaries in different scales. The proposed hierarchical CSC automatically extracts multi-scale features from the input volume, which are then used to build up high-dimensional feature vectors that encode local shapes more robustly and discriminatively compared with the conventional intensity-based features or a singlescale dictionary. This method also benefits from the characteristic of sparse representation that makes the final rendering less sensitive to noise. We also propose a robust 3D CSC method that uses a blackout approach, which is inspired by the dropout technique <ref type="bibr" target="#b30">[30]</ref> in deep neural networks and autoencoders to establish a more general and compressive collection of 3D filter banks. We then employ an intelligent system built on a random forest classifier for accurate and interactive voxel classification using high-dimensional feature vectors, up to 75 dimensions, from only a handful of selection scribbles made directly on the input volume by the user. Last, we apply the probabilistic transfer function to further customize and refine the rendered result. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Transfer Function Design Conventional DVR <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref> focuses mainly on designing transfer functions that map volume data to color and opacity values. Early transfer functions are simply one-dimensional because the mapping depends only on input intensity, but more complicated classification methods are necessary for noisy and complex 3D volume data. An immediate extension of the one-dimensional transfer function is employing more dimensions for the feature space. Kindlmann and Durkin <ref type="bibr" target="#b15">[16]</ref> suggested using first-and second-order derivatives with histogram visualization for multidimensional transfer functions. Kniss et al. <ref type="bibr" target="#b16">[17]</ref> introduced a novel user interface (widget) for easy handling of multidimensional transfer functions. Park and Bajaj <ref type="bibr" target="#b24">[24]</ref> proposed various data features, such as the gradient magnitude and second derivatives, with the voxels' spatial distances to the boundaries to determine the opacity characteristics. Correa and Ma <ref type="bibr" target="#b4">[5]</ref> advocated using the scale feature to make classifications, which was constructed based on the relative size of each voxel at different volume resolutions. Although the multi-dimensional transfer function is useful, user interactions become more challenging as the number of dimensions increases. Other methods provided either a segmentationdriven analysis of the 2D histogram <ref type="bibr" target="#b12">[13]</ref> or an abstracted attribute space between the density value versus the gradient magnitude <ref type="bibr" target="#b20">[21]</ref>. Nevertheless, manual interaction with a multi-dimensional transfer function is generally time-consuming and difficult for non-expert users. The recent work by Guo et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> introduced interesting sketch-based volume rendering systems that nicely address the challenges of conventional transfer functions shown above. However, advanced voxel classification based on machine learning and data-driven techniques has not been actively adopted to advanced sketch-based approaches yet, which leaves room for improvement in the transfer function design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning-based Volume Rendering</head><p>The transfer function in volume rendering is, in fact, a voxel classification method. Therefore, machine learning can serve as an accurate voxel classifier. Volume <ref type="bibr" target="#b0">1</ref> Source code is available at http://hvcl.unist.ac.kr/pvr/ rendering that employed machine-learning techniques was first introduced in the seminal work of Tzeng et al. <ref type="bibr" target="#b33">[33]</ref>. Their approach involves directly selecting visible voxels from the data space (i.e., volume) using a paintbrush tool, rather than manipulating the transfer function to indirectly classify the visible voxels. Feature vectors, input to a learning-based classifier, are then built from the selected voxels using the current intensities and the gradient magnitude as well as those of its six-neighbors, and location information x, y, and z, which builds up to an 11-dimensional representation for each selected voxel. A neural network with a single hidden layer is leveraged to train the feature vectors, and is eventually used to classify each voxel in the volume as foreground or background material (i.e., binary classification). Most recently, Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref> systematically compared the effect of different classification approaches on multinomial classification tasks and concluded that no single technique is significantly better than the others but, random forests are generally useful in most scenarios. On the other hand, Johnson and Huang <ref type="bibr" target="#b13">[14]</ref> explicitly considered the local frequency distribution of the 3D stencil in which a particular voxel is located at the center. This training query helps estimate the relational pattern between 3D stencils and is then deployed to each voxel for final classification. Cai et al. <ref type="bibr" target="#b3">[4]</ref> recently proposed abstract volume attributes, i.e., rules of frequency distribution, that were trained using a selection-based genetic algorithm. As a result, they were able to effectively visualize the overlapping data values in 3D volumes. A more comprehensive survey of the state-of-the-art in transfer functions for DVR can be found in <ref type="bibr" target="#b19">[20]</ref>. Although previous research efforts attempted to exploit machine learning for classification in DVR, unsupervised learning for data-driven feature vector generation, as in our proposed method, has never been examined.</p><p>Dictionary Learning and Sparse Coding Dictionary learning is a technique that helps decompose many patches of signal into a linear combination of overcomplete basis sets and that forces the associated codes to be sparse. This technique has been shown to be useful in many image-processing applications, such as image denoising <ref type="bibr" target="#b7">[8]</ref>. A similar approach is convolutional sparse coding, in which patch extraction is replaced by a convolution operator. A solution of the energy function can be found efficiently in the frequency domain, where the convolution operator can be transformed into an element-wise multiplication. The entire solver was derived within the Alternating Direction Method of Multiplier (ADMM) framework proposed by Bristow et al. <ref type="bibr" target="#b2">[3]</ref>; Wohlberg <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref> later introduced the direct inverse solution for more efficient convergence. Convolutional sparse coding can generate many compact dictionaries because of the shift-invariant nature of the filters, and the pixel-wise computation in the Fourier domain maps well to parallel architecture, such as GPUs. Hence, it has been applied to various machine vision problems, such as image inpainting <ref type="bibr" target="#b11">[12]</ref>, classification <ref type="bibr" target="#b39">[39]</ref>, segmentation <ref type="bibr" target="#b38">[38]</ref>, super-resolution <ref type="bibr" target="#b23">[23]</ref> and reconstruction <ref type="bibr" target="#b28">[28]</ref>. However, these advanced machine learning approaches have not yet been fully exploited in the volume rendering literature. In this paper, we propose a novel feature-based volume rendering method that leverages the compactness and efficiency of hierarchical 3D convolutional sparse coding to enrich the feature vectors. <ref type="figure">Fig. 2</ref>: A 64-atom dictionary generated from natural images. CSC can be regarded as an alternative to dictionary learning <ref type="bibr" target="#b0">[1]</ref>, which builds the dictionary with convolution filters instead of local  patches. In this section, we present a brief background review of CSC for 2D images. For a given image s, we would like to find its best approximation from the summation of response maps ∑ r k . This reverse problem can be solved if we impose a constraint that each response map r k is the result of convolution between a filter (or atom) d k and its associated sparse map x k . The non-linear p norm (where 0 p 1), which is applied to x k (i.e., x k is sparse), affords a feasible solution of finding such a collection of d k and x k . Once d k and x k are obtained properly, we can compute back the response map r k by convolving the corresponding filter and sparse map. Mathematically, the CSC problem is equivalent to minimizing this energy function:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW OF CONVOLUTIONAL SPARSE CODING</head><formula xml:id="formula_0">min d,x α 2 s − ∑ k d k * x k 2 2 + λ ∑ k x k 1 s.t. : d k 2 2 1 (1)</formula><p>In Equation 1, the first term measures the difference between s and its sparse approximation</p><formula xml:id="formula_1">∑ k d k * x k , weighted by α.</formula><p>The second term is the sparsity regularization of x k using an 1 norm with a weight λ instead of an 0 norm as used in <ref type="bibr" target="#b0">[1]</ref>. The remaining constraint restricts the Frobenius norm of each atom d k within a unit length. If we wish to train d k to generalize for the dictionary and to represent the features of many images obtained from a database, all the training instances can be fetched and participate in contributing to the feature extraction. For example, <ref type="figure">Figure 2</ref> shows a 64-atom dictionary that has been trained using a collection of natural and standard images (lena, barbara, etc.). Their components in Gabor-like shapes capture directional edges that match the fundamental features in our human visual perception.</p><p>Solving Equation 1 is introduced in the seminal work of Zeiler et al. <ref type="bibr" target="#b37">[37]</ref>. They proposed an alternating strategy in which a series of convex subproblems between d k and x k is solved until convergence. Because their solver is completely in the image domain, the linear complexity of convolution affects the performance of their algorithm. More efficient approaches based on the Fourier Convolution theorem are also proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>. The filter is padded with zeros to make it the same size as the image, and the Fourier transform is applied to both the padded filter and the image so that the convolution can be computed as a pixel-wise multiplication in the Fourier domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED METHOD</head><p>In this section, we introduce our high-dimensional feature-based intelligent volume rendering system in detail. <ref type="figure" target="#fig_0">Figure 3</ref> depicts the overview of the proposed intelligent volume rendering system. The proposed system consists of two componentsone component is the pre-processing stage to construct the hierarchical 3D dictionary from the input volume and to create a per-voxel multilevel high-dimensional feature vector (see the upper row in <ref type="figure" target="#fig_0">Figure 3</ref>). This process is required only once for each input volume. The other component is the interactive stage to accept user selections to train the random forest classifier using high-dimensional feature vectors to classify voxels and to generate a rendered image based on the user-given probabilistic transfer function for each class (see the bottom row in <ref type="figure" target="#fig_0">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of the Proposed System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generating 3D Dictionary using 3D CSC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Training 3D Atoms using 3D CSC</head><p>In this section, we extend 2D CSC formulation to 3D to apply for volume rendering. The main formulation is identical to Equation (1) except now the 2D image s is replaced with the 3D volume v. If we use a simplified notation without the index k and replace the result for the Fourier transform of a given variable by using the subscript f (for example, d f is the simplified notation for F d in the 3D domain to derive the solution to Equation (1)), then the 3D CSC problem can be expressed using the auxiliary variables y and g for x and d as follows:</p><formula xml:id="formula_2">min d,x,g,y α 2 v − ∑ d * x 2 2 + λ y 1 s.t. : g 2 2 1, x − y = 0, g = Pro j(d)<label>(2)</label></formula><p>where g and d are related by a projection operator as a combination of a truncated matrix followed by a zero-padding matrix in order to make the dimension of g same as that of x. The above constraint problem can be solved using an augmented Lagrangian method <ref type="bibr" target="#b28">[28]</ref> (more details can be found in the supplementary text).  The proposed method can be considered as a learning approach in which the features are learned directly from 3D volume data. One problem commonly occurs during the learning process is that the solution may converge to a local minimum. The converged result we expect is that each atom in the dictionary is used as evenly as possible and the sparse maps are as sparse as possible. However, there might be a case when only a few atoms are used to approximate the input data. In that case, only a few sparse maps have largely non-zero values and the rest are all zero maps. To avoid such cases, we propose blackout regularization. The main idea is that we choose portion q of K atoms, in which 0 &lt; q &lt; 1, and delete their corresponding sparse maps during the dictionary training process. This is similar to dropout regularizer <ref type="bibr" target="#b30">[30]</ref> in machine learning to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Improving the Dictionary using Blackout Regularizer</head><p>For this, we introduce two strategies to give major improvements over conventional dictionary training -maximal blackout and random blackout. Maximal blackout is selecting the q highest of the K atoms in terms of the energy in the corresponding sparse map, and discard them to avoid the case that only a few atoms dominate to approximate the target data. The other strategy, random blackout, is selecting q sparse maps randomly and discard, which relies on the probabilistic regularization. We used q = 12.5% in all cases of our experiments. However, neither strategy emerged as significantly better than the other. Therefore, we combined both strategies in the dictionary training process. <ref type="figure" target="#fig_1">Figure 4</ref> depicts two renderings of synthetic alphabet dataset: one is of the feature enhanced volume derived from regular dictionary, the other is from the dictionary that has been applied blackout regularizers. As shown in this figure, the blackout result tends to visualize the letters better than the result without blackout because the characters "c", "g" and "h" have misclassified their boundaries and the corresponding rendered volume has contained more noise artifacts. For quantitative comparison of <ref type="figure" target="#fig_1">Figure 4</ref>, we used two error metrics: the Dice coefficient and the SSIM of rendered results. In both metrics, the uses of blackout(s) gave higher accuracy as shown in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Constructing High-dimensional Feature Vectors using Hierarchical CSC</head><p>Once we have the method to solve the 3D CSC problem as introduced in Section 4.2, we extend the method to handle multi-scale dictionary, called hierarchical CSC. The main idea of this method is that the features learned by the CSC is defined by the size of the filter (atom). <ref type="figure">Figure 5</ref> shows an example of multi-scale dictionaries learned from the synthetic alphabet volume data.  : Volume rendering of the alphabet dataset using (a) single-scale dictionary, and (b) three-level multi-scale dictionary. In (a), some edges of the letters "c" and "g" are misclassified as "a" because their round shape looks similar, whereas in (b) only "a" letter is selected due to the higher accuracy of the multi-scale dictionary.</p><p>the dictionaries with the filters in different sizes, then we can encode much richer feature information in the dictionary, which will eventually affect the classification accuracy (see <ref type="figure">Figure 6</ref> and <ref type="table" target="#tab_4">Table 2</ref>). Another important idea is to make the dependency across filter levels. This idea is similar to the structure of the convolutional neural network (CNN) where the neuron activation from the previous level is used as the input to the convolutional layer in the following level. <ref type="table" target="#tab_4">Table 2</ref> shows that hierarchical dictionary is more accurate than multi-scale dictionary without hierarchical dependency. Based on these ideas, we propose the hierarchical CSC ( <ref type="figure">Figure 7</ref>) in which the dictionary and the sparse maps are successively constructed for every level using the result of the previous level. For example, the input volume to the current level m + 1 is the approximation a m = ∑ k d m k * x m k from the previous level m. Whenever the level increases, we enlarge the filter size but keep the volume size the same so that the filters can extract features in different scales. After collecting the multi-scale dictionaries and their corresponding sparse maps, we construct a set of the response maps {r m k } by convolving the filters {d m k } with the sparse maps {s m k }. Thus, the number of generated response maps depends on the dictionary size (i.e., the number of atoms) and the number of levels. These response maps show the spatial extent of each filter contributing to the input approximation a m . In our experiment, we used three levels (m = {1, 2, 3}, the filter size for each level is 7×7×7, 15×15×15 and 31×31×31, respectively), and each level generates 24 filters and sparse maps (k = {1, ..., 24}), as follows:  <ref type="figure">Fig. 7</ref>: Constructing high-dimensional feature vectors using hierarchical 3D convolutional sparse coding Note that the choice of the number of atoms and levels is problemspecific and empirically chosen, meaning that the number of levels is chosen based on the input volume size and the number of atoms is chosen to allocate the largest number of atoms fit to the available memory. Once we generate the multi-scale hierarchical dictionary, we finally generate the high-dimensional feature vector per voxel which will be used as the input for classification. The high-dimensional feature vector is the collection of the response map value at a given voxel location. Since we have three levels in which each level consists of 24 response maps, we have total 72 response values per voxel. The final per-voxel feature vector becomes 75 dimensions by appending the spatial location (x, y, z indices of each voxel location). Note that, unlike the CNN that reduces the image size by half using max-pooling between levels, the volume size in the hierarchical CSC must be the same across all levels because we collect the response values (i.e., d m k * x m k for all k and m) for every voxel location to construct the feature vector.</p><formula xml:id="formula_3">v 1 ≈ 24 ∑ k=1 d 1 k * x 1 k = 24 ∑ k=1 r 1 k = a 1 , v 2 = a 1 (3) v 2 ≈ 24 ∑ k=1 d 2 k * x 2 k = 24 ∑ k=1 r 2 k = a 2 , v 3 = a 2 (4) v 3 ≈ 24 ∑ k=1 d 3 k * x 3 k = 24 ∑ k=1 r 3 k = a 3 (5) … * … * … * Level</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Voxel Classification using High-dimensional Feature Vectors</head><p>Regression: Our volume renderer allows users to draw strokes and scribbles directly in the volume in order to select the structures or data classes to visualize. <ref type="figure" target="#fig_3">Figure 8</ref> depicts the interface of our painting application. Pixels can be selected using a region-growing method, similar to magic wand tools in current commercial design software. Once the painting procedure is completed on N classes of the desired materials, the application collects the feature vectors that correspond to the manually labeled voxels from the user's scribbles and feeds them through a classifier for the on-the-fly regression training.</p><p>Many types of classifiers can be considered, as discussed in <ref type="bibr" target="#b29">[29]</ref>. We empirically observed that the random forest algorithm <ref type="bibr" target="#b1">[2]</ref>, an ensemble machine learning approach that leverages multiple decision trees, outperformed the multi-layer perceptron neural network, which is in agreement with Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref>. In addition, its training time was also short compared to other learning-based classifiers. Therefore, we chose the random forest for our volume rendering system.</p><p>Classification: Once the classifier, that is, the random forest, has been trained appropriately, it can be used to predict the multinomial classification of the entire volume. The score of this one-time predictive procedure is used to assign labels that have the highest probability for all the volume data. The input in the random forest model are the full 75dimensional feature vectors that have been generated before. The results will be the expected class of the individual voxels with the highest probability among the materials of interest. This enables us to compose the volume rendering by building up multiple per-class one-dimensional transfer functions and applying the ray casting algorithm thereafter. The running time of random forest is acceptable for reasonably sized volumes, as shown in <ref type="table" target="#tab_5">Table 3</ref>. We expect further speed up by leveraging the GPU for training the random forest classifier, which is left for the future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Rendering Multi-labeled Volume using Probabilistic Transfer Function</head><p>To obtain the final rendering, the ray casting algorithm is employed to project the entire view frustum on to the two-dimensional screen.</p><p>Along each ray, color and opacity values are sampled and combined using front-to-back composition as follows:</p><formula xml:id="formula_4">Color = Color + v.Color × v.Al pha × (1 − Al pha) (6) Al pha = Al pha + v.Al pha × (1 − Al pha)<label>(7)</label></formula><p>where v.Color and v.Al pha are the color (possibly shaded using Phong shading <ref type="bibr" target="#b27">[27]</ref>) and opacity at the sampling location v. This composition continues until the ray exits the volume or the accumulated alpha value becomes close to zero. Each v.Color and v.Al pha are defined by the probabilistic transfer function as follows:</p><formula xml:id="formula_5">v.Color = ∑ n TableColor(n, v.Prob n ) × v.Prob n (8) v.Al pha = ∑ n TableAl pha(n, v.Prob n ) × v.Prob n (9)</formula><p>where n is the label index, v.Prob n is the probability of the label n at the location v, and ∑ n v.Prob n = 1. TableColor(n, x) or TableAl pha(n, x) is the user-defined 1D transfer function for the label n that maps the probability value x (between 0 to 1) to a color or an alpha value, respectively. Note that conventional methods <ref type="bibr" target="#b29">[29]</ref> use single color and alpha value per label for simplicity, e.g., TableColor(n, x) = LabelColor(n) for all x, and we can employ a similar approach by using a pre-defined linear transfer function by default. However, our probabilistic 1D transfer function allows more flexible, fine-tune control of intra-label coloralpha value assignment. In addition, since we assign a curve in the 1D transfer function that smoothly interpolates the probability values, partial volume effects (or blocky artifacts) can be effectively reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>The most time-consuming process in the proposed system is solving the 3D CSC problem. To accelerate this process, we used a multi-GPU computing server equipped with NVIDIA Titan X GPUs. The interactive visualization component is implemented and executed on a PC equipped with an Intel i7 CPU with a 12 GB main memory and an NVIDIA GTX Geforce 980 Ti GPU, offering a paintbrush user interface, on-the-fly random forest training and deployment for classification, and ray-casting volume rendering. For implementation of the random forest, we used the open-source Python library <ref type="bibr" target="#b25">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visualization of 3D Dictionaries</head><p>We extracted features from several public domain 3D volume datasets: MRI-Kiwi, MRT-Aneurysm, CT-Bonsai, low-dose CT-Chest (LdCT-Chest), CT-Tooth, and EM-Mouse. As shown in <ref type="figure" target="#fig_4">Figure 9</ref>, our method can generate 3D dictionaries from the input volume where each atom in the dictionary represents local feature in the data. For example, the atoms in the dictionary of the LdCT-Chest data represent fine-level details related to bones and surrounding muscle tissues. Similar results can be found in other datasets as well. They generate the data-dependent learned basis, which differs significantly from the universal dictionary in the Fourier, cosine and wavelet transforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Other Methods</head><p>In this section, we compare our method with Kniss et al. <ref type="bibr" target="#b16">[17]</ref> and Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref>. We implemented and extended their method to multi-material classification. Moreover, we used the same setup for random forest parameters and user scribble selection to make fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic volume data:</head><p>We assess the robustness of our algorithm by comparing the rendering on the noisy synthetic dataset <ref type="figure" target="#fig_5">(Figure 10</ref>). We created a phantom volume of a spiral structure with random noise. As shown in this figure, our method is robust to noise and renders the entire spiral shape faithfully while the other methods incompletely render the structure of spiral shape or pick up the noise artifacts in the background. This is because our method uses feature vectors from learned dictionary which is less susceptible to noise.</p><p>Real volume data: The top row of <ref type="figure">Figure 11</ref> presents the rendering results of the MRI-Kiwi dataset in the comparison of Kniss et al. <ref type="bibr" target="#b16">[17]</ref> and Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref>, and ours, respectively. We drew scribbles on the x-y plane to collect the training data for multiple classes: the background, the carpel parts (locules filled with juice), the central placenta vascular, the lateral branch, the placental vascular bundles, and the seeds. Kniss et al. <ref type="bibr" target="#b16">[17]</ref> captured many false positives surrounding the seed regions and improperly recognized the septum vascular bundles. Although Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref> showed generally good separation of the carpel region, the peel classification was not satisfactory. Our rendering, on the other hand, offers the best result, as it can clearly separate each structure with minimal errors. Similar advantages of our method are also achieved with the other datasets (see the rest of <ref type="figure">Figure 11</ref>) with which the multi-class material separation was performed. In the MRI-Aneurysm dataset, we intentionally applied two labels, one is on the thicker trunk and the other is on the thinner tubes. Kniss et al. <ref type="bibr" target="#b16">[17]</ref> failed to separate the structure, while Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref> had some errors in thinner tubes near the main trunk (see the zoom-in circle). On the other hand, our method was able to separate two regions cleanly, which is due to the multi-scale dictionary that learns features in different sizes. The bottom of <ref type="figure">Figure 11</ref> presents the results of the EM-mouse dataset with many artifacts and complicated structures. As can be seen, our method is highly robust to noise, whereas the others incorrectly picked up noise outside. In addition, thanks to the learned features, the myelinated axons are well-separated from the regular axons because different membrane thickness can be recognized and classified differently. In summary, our method is more robust to noise and is superior in classifying structures based on their shapes as well as voxel intensity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USER EVALUATION</head><p>We conducted a user study to evaluate the usability and the rendering quality of the proposed volume rendering method. Specifically, we compared our method with a multi-dimensional transfer function approach <ref type="bibr" target="#b16">[17]</ref> and a painting-based intelligent system <ref type="bibr" target="#b29">[29]</ref> to demonstrate how the proposed method performs better than conventional volume rendering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Setup</head><p>Through a preliminary questionnaire, we recruited 30 participants who had never used volume rendering software but were familiar with computers. Participants were between the ages of 19 and 25, and the number of male and female participants was the same.</p><p>In this study, we assessed the usability of the volume rendering methods and the accuracy of the rendered results. For the usability evaluation, we implemented a a 3D transfer function-based user interface <ref type="bibr" target="#b16">[17]</ref> and a painting-based user interface employed in both <ref type="bibr" target="#b29">[29]</ref> and our method. To evaluate both methods under the same conditions, we used the same ray casting (alpha compositing) and shading algorithms to produce identical rendering effects, and also used the same   <ref type="bibr" target="#b16">[17]</ref>, Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref>, and ours. Our method is robust to noise due to the nature of learned dictionary. window size for the volume renderer and the transfer function editor for measuring the number of mouse interactions.</p><p>We used a Windows PC equipped with an Intel i7-6700 CPU, 64GB of main memory, and an NVIDIA GTX Geforce 1080 GPU for the experiment. Two 27-inch monitors were used to display the volume rendering user interface, the user study manual, and the model answer (the rendered image that the participants were asked to generate). The resolution of each monitor was 1920×1080 running at a 60Hz refresh rate, and the model of the monitor was AOC 2769. The participants used a Microsoft wired desktop 400 keyboard and mouse for the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiment Procedure</head><p>The experiment consisted of three steps: 1) the training step to learn how to use the software, 2) the main experiment step to perform the given tasks, and 3) the post-survey step to evaluate the software subjectively.</p><p>Training step. The participants were first asked to watch the instruction video to learn how to use the two volume rendering systems (i.e., the 3D transfer function-based and painting-based systems). The instruction video provided basic knowledge about the software system required to perform the tasks. We chose video-guided training over instructor-guided training because we wanted to provide identical training to the participants without bias. After the training step, the participants were aware that mouse interactions, experiment time, and accuracy are automatically recorded.</p><p>Main experiment step. The participants were asked to perform three tasks. Tasks 1 and 2 were designed to assess the usability of the two user interfaces used in 3D transfer function-based volume rendering and painting-based intelligent volume rendering. Task 3 was designed to assess the accuracy of the voxel classification results from three methods: Kniss et al. <ref type="bibr" target="#b16">[17]</ref>, Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref>, and ours. The input data and their model answer for each task are shown in <ref type="figure">Figure 12</ref>. More details about the tasks are as follows:</p><p>• Task 1. In the given synthetic data with two separated 3D alphabet letters <ref type="figure">(Figure 12 (a)</ref>), the participants were asked to visualize the letter "a" only ( <ref type="figure">Figure 12 (d)</ref>). The total user time spent to make the rendered result close to the model answer of 99% or higher (similarity measured with a Dice coefficient between the ground-truth and classification results).</p><p>• Task 2. This is similar to Task 1 but more challenging because two letters overlapped <ref type="figure">(Figure 12 (b)</ref> and (e)). We expected that additional user interactions would be required for this task.</p><p>• Task 3. The participants were asked to separate three structures in the CT-Tooth dataset <ref type="figure">(Figure 12</ref> (c) and (f)) as close to the model answer as possible. This task had a time limit (10 minutes), and the similarity between the user result and the ground-truth was measured using a Dice coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">User Evaluation Result</head><p>We analyzed the collected experimental data (i.e., time, mouse interactions, and accuracy) using statistical models, such as Fitts' law <ref type="bibr" target="#b22">[22]</ref> and the one-way ANOVA <ref type="bibr" target="#b31">[31]</ref>. <ref type="figure" target="#fig_0">Figure 13</ref> (a) shows the running time of the 3D transfer function(3D TF)-based method and our paint-based method for Tasks 1 and 2 using a boxplot. We designed Task 2 more difficult by overlapping two objects, and the 3D TF method shows significant difference between Task 1 and 2 as expected. However, in our method, the running time of Task 2 is within the range of that of Task 1. This is because the users can easily generate fairly accurate results using only a few user-scribbles in our method, so after Task 1 the users become more familiar with the user interface of our method and the user time is rather reduced. We further analyzed this result using Fitts' law, which shows the relationship between the user time and the usability of the system <ref type="bibr" target="#b22">[22]</ref>. Fitts' law is a linear regression model for the user movement time (MT) and the index of difficulty (ID) as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Experiment Time Analysis</head><formula xml:id="formula_6">MT = a + b * ID<label>(10)</label></formula><p>where a and b are device-dependent coefficients. Since b affects the user time when the difficulty of the task changes, our strategy derives b  by fixing the method and performing two tasks with different difficulty levels, and then analyzing whether there is a statistically meaningful difference between the methods. For this, let us assume the ID and the MT of Tasks 1 and 2 are ID1, ID2, MT1, and MT2, respectively. Then, we can derive b from Equation 10 as follows:</p><formula xml:id="formula_7">b = (MT 2 − MT 1)/(ID2 − ID1)<label>(11)</label></formula><p>Figure 13 (b) shows the b values of 3D TF and our method. The average b value for the 3D TF is 6.53041, and that of our method is -0.75062. Our method shows negative b value because the level of difficulties of Task 1 and 2 are not significantly different for our method. Therefore, contrary to our assumption that Task 2 is more difficult than Task 1 (ID2 &gt; ID1), the learning effect during the study actually reduced the experiment time of Task 2 (i.e., MT 2 &lt; MT 1).</p><p>To show statistically significant differences between two average b values, we further applied one-way ANOVA <ref type="bibr" target="#b31">[31]</ref>. The p-value was 0.00102, which was significantly smaller than the significance level of 0.05, and the average b value for our method was significantly smaller than that for the 3D TF. Therefore, we conclude that the usability of our painting-based user interface is much higher compared to that of the conventional transfer function-based user interface. <ref type="figure" target="#fig_0">Figure 13</ref> (c) shows the accuracy result for Task 3 in which the average accuracy of Kniss et al. <ref type="bibr" target="#b16">[17]</ref>, Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref>, and our method was 0.45044, 0.73717, and 0.84109, respectively. We used a Dice coefficient <ref type="bibr" target="#b5">[6]</ref> to compare the similarity between the ground truth and the user-generated result. The p-value from one-way ANOVA was closed to zero (p &lt; 0.00001), which is smaller than the significance level of 0.05; therefore, we confirmed that at least two methods have statistically significant differences. To confirm the difference between our method and the others, we further conducted post-hoc analysis using Fisher's least significant difference (LSD) <ref type="bibr" target="#b34">[34]</ref>. Using Fisher's LSD, the significance value of our method over <ref type="bibr">Kniss et al. [17]</ref> is closed to zero (p &lt; 0.00001) and over Soundararajan and Schultz <ref type="bibr" target="#b29">[29]</ref> is 0.04, both are less than 0.05; thus, our method has statistically significant differences. In addition, our approach has the highest average accuracy; thus, we confirmed that our method is the most accurate method among the three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Accuracy Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Connection to Deep Learning. The proposed hierarchical CSC is designed to mimic multi-scale feature learning in a deep convolutional neural network without the burden of the extensive training usually required in deep supervised learning. In addition, 3D convolutional networks are even more difficult to implement and to train than the proposed 3D CSC due to the increasing size of the network. We believe the proposed hierarchical 3D CSC, alongside with other unsupervised learning techniques, such as convolutional autoencoder, can be an appropriate and practical solution for data-driven interactive volume rendering problems. The proposed hierarchical CSC is also in line with the interesting recent research direction about no-gradient learning <ref type="bibr" target="#b32">[32]</ref>, which replaces the gradient descent in deep learning with the global energy minimization problem.</p><p>Limitations. One limitation of the proposed method is the running time for the dictionary learning and high-dimensional feature construction. Although this task needs to be performed only once during pre-processing (i.e., there is no need to recalculate the dictionary and the response maps when the user changes scribble selections), this step requires approximately 30 minutes to complete on a single CPU core and 10 minutes with GPU acceleration. In addition, increasing the number of filters increases the memory consumption, as the filters and sparse maps are all in 3D. The most memory-consuming process is CSC computation, where all of the intermediate variables and their corresponding forms in Fourier domain as complex numbers (see Supplements) should be in-memory. This consumes roughly 16×(# of atoms)× of the input volume size, for example, the preprocessing of a 256 3 with a 24-atom dictionary would need 16×24×256 3 ×8(bytes), or equivalently, 48GB of memory. Once the preprocessing is done, we need (# of atoms × # of scales)× of the input volume size to store feature volumes (in unsigned integer data type), which is around 3.375GB for a 256 3 volume. Training and deploying a random forest classifier (with 20000 selected voxels) on a 256 3 (bytes) volume requires about 1.5GB of memory. Due to such memory requirement, the dimension of the largest volume data we used in our experiment is 256 <ref type="bibr" target="#b2">3</ref> . Although the time and memory problems are not the limitation of the method but the implementation issues (ADMM-based numerical solvers <ref type="bibr" target="#b28">[28]</ref> for Equation 2 are highly parallelizable, see <ref type="bibr" target="#b32">[32]</ref>), we plan to address those in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we introduced a novel interactive volume rendering framework that leverages machine learning to accurately classify voxels. Inherited from the intelligent system approach in which users simply need to specify materials of interest using rough scribbles, we improved the classification quality by employing a novel high-dimensional feature generated by using a hierarchical 3D CSC. We presented for the first time 3D volume rendering of learning-based multi-scale features that do not rely only on voxel intensity; thus, the proposed method is more robust to noise and is highly discriminative for fuzzy boundaries that results in superior rendering quality.</p><p>In the future, we plan to extend the proposed method to large-scale parallel computing systems to handle a much larger 3D volume data and dictionary. We also intend to explore various computer graphics applications that can benefit from the proposed hierarchical CSC, such as mesh segmentation and surface reconstruction. In-depth study of the connections between hierarchical CSC and a deep convolutional neural network would be another interesting future research direction. We believe that the proposed intelligent volume rendering will open up new research opportunities related to deep learning in scientific visualization and computer graphics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of the proposed intelligent volume rendering system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Comparison of voxel classification accuracy without (a) and with (b) blackout regularizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :Fig. 6</head><label>56</label><figDesc>Multi-scale dictionaries learned from the alphabet dataset. Note that the small filter (a) learned edge-like local features, the medium size filter (b) learned the fraction of alphabet shapes, and the large filter (c) learned the entire alphabet shapes.(a) Single-scale rendering (b) Three-level rendering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 :</head><label>8</label><figDesc>Volume rendering and user interface window. The user can use a paint-brush tool to select voxels on the x-y-z plane view (bottom center, three black windows with selections). Each label is assigned with a one-dimensional transfer function (top center).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>(a) Dictionaries of LdCT-Chest dataset (b) Dictionaries of CT-Bonsai dataset Volume rendering of hierarchical multi-scale 3D dictionaries. From left to right: three resolutions of 24 atoms (7 3 , 15 3 and 31 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Rendering of the noisy spiral dataset. From left to right: Kniss et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Comparison on various datasets. From left to right: Grayscale input volumes, Kniss et al.<ref type="bibr" target="#b16">[17]</ref>, Soundararajan and Schultz<ref type="bibr" target="#b29">[29]</ref> and ours.(a) Task 1 input (b) Task 2 input (c) Task 3 input (d) Task 1 model answer (e) Task 2 model answer (f) Task 3 model answer Input data (a -c) and their model answers (d -f) for each task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 :</head><label>13</label><figDesc>User evaluation results. (a) : Experiment time of Kniss et al.(P1) and our method (P2) on Task 1 (T1) and Task 2 (T2), (b): Comparison of b values from Fitts' law analysis on Task 1 and 2. b1: Kniss et al., b2: ours, and (c): Accuracy result of Task 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Hierarchical 3D Convolutional Sparse Coding Multi-level High-dimensional Feature Construction Rendering using probabilistic transfer function User Selection Voxel Classification Pre-processing Interactive processing</head><label></label><figDesc></figDesc><table><row><cell>3D Volume (v)</cell><cell>Sparse Maps {x k }</cell><cell></cell><cell>Level 1</cell><cell>Response Maps {r k } Level 2</cell><cell>Level 3</cell></row><row><cell>Trained Dictionaries {d k }</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>xyz</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">75-dimensional feature vector</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Random Forest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Training</cell></row><row><cell></cell><cell>Class 1</cell><cell>Class 2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Class 3</cell><cell>Class 4</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Class 5</cell><cell>Class 0</cell><cell cols="2">Classification</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Effect of using blackout regularizer.</figDesc><table><row><cell cols="3">Metrics No regularizer With blackouts</cell></row><row><cell>Dice</cell><cell>0.876</cell><cell>0.880</cell></row><row><cell>SSIM</cell><cell>0.934</cell><cell>0.946</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Accuracy comparison of dictionaries of different types.</figDesc><table><row><cell cols="4">Metrics Single-scale Multi-scale Hierarchical</cell></row><row><cell>Dice</cell><cell>0.900</cell><cell>0.945</cell><cell>0.997</cell></row><row><cell>SSIM</cell><cell>0.961</cell><cell>0.976</cell><cell>0.977</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Running time of the voxel classification method.</figDesc><table><row><cell>Data</cell><cell>Number of voxel selection</cell><cell>Number of labels</cell><cell>Total time (s) (Train+Deploy)</cell></row><row><cell>Spiral</cell><cell>9378 (1 slice)</cell><cell>2</cell><cell>0.420</cell></row><row><cell cols="2">128×128×128 100775 (6 slices)</cell><cell>2</cell><cell>4.690</cell></row><row><cell>CT-Tooth</cell><cell>19623 (2 slices)</cell><cell>4</cell><cell>0.698</cell></row><row><cell>100×90×168</cell><cell>57900 (6 slices)</cell><cell>4</cell><cell>1.739</cell></row><row><cell>MRI-Kiwi</cell><cell>63340 (1 slice)</cell><cell>6</cell><cell>16.400</cell></row><row><cell cols="2">256×256×256 167973 (3 slices)</cell><cell>6</cell><cell>19.174</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the National Research Foundation of Korea (NRF) funded by the Korea government (MOE) (NRF-2017R1D1A1A09000841, Basic Science Research Program) and by the Korea government (MSIT) (NRF-2017M3C7A1047904, Brain Research Program).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K-Svd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast Convolutional Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bristow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rule-Enhanced Transfer Function Generation for Medical Volume Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Size-based Transfer Functions: A New Volume Exploration Technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1380" to="1387" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Drebin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH</title>
		<meeting>the Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image Denoising Via Learned Dictionaries and Sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="895" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WYSIWYG (What You See is What You Get) Volume Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2106" to="2114" />
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local WYSIWYG volume visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Visualization Symposium (PacificVis)</title>
		<meeting>IEEE Pacific Visualization Symposium (PacificVis)</meeting>
		<imprint>
			<date type="published" when="2013-02" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Real-time Volume Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rezk-Salama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and flexible convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5135" to="5143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical Exploration of Volumes Using Multilevel Segmentation of the Intensity-Gradient Histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Ip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jaja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2355" to="2363" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distribution-Driven Visualization of Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="734" to="746" />
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierarchies for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1090" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-automatic Generation of Transfer Functions for Direct Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Durkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Volume Visualization, VVS &apos;98</title>
		<meeting>the IEEE Symposium on Volume Visualization, VVS &apos;98</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multidimensional transfer functions for interactive volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="270" to="285" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Display of Surfaces from Volume Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="1988-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ynnerman. State of the Art in Transfer Functions for Direct Volume Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Groller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadwiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="669" to="691" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jnicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Gaither</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abstracting Attribute Space for Transfer Function Exploration and Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="107" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fitts&apos; law as a research and design tool in humancomputer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-03" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="91" to="139" />
		</imprint>
	</monogr>
	<note type="report_type">Hum.-Comput. Interact.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image Super-Resolution with Fast Approximate Convolutional Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V D</forename><surname>Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<imprint>
			<date type="published" when="2014-11" />
			<biblScope unit="volume">8836</biblScope>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-dimensional transfer function design for scientific visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Bajaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Indian Conference on Computer Vision, Graphics &amp; Image Processing (ICVGIP)</title>
		<meeting>the Fourth Indian Conference on Computer Vision, Graphics &amp; Image Processing (ICVGIP)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="290" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The transfer function bake-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Kindlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="16" to="22" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Phong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1975-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Compressed sensing dynamic MRI reconstruction using GPU-accelerated 3D convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Medical image computing and computer-assisted intervention (MICCAI)</title>
		<meeting>Medical image computing and computer-assisted intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Probabilistic Transfer Functions: A Comparative Study of Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="111" to="120" />
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Using Multivariate Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Tabachnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Fidell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>5th Edition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training neural networks without gradients: A scalable ADMM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Burmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2722" to="2731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An intelligent system approach to higher-dimensional classification of volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2005-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fishers least significant difference (LSD) test. Encyclopedia of research design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient convolutional sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="7173" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient algorithms for convolutional sparse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="301" to="315" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nuclei segmentation via sparsity constrained convolutional regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>IEEE International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="1284" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Classification of Histology Sections via Multispectral Convolutional Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Spellman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="3081" to="3088" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
