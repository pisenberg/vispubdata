<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Exploration of Surveillance Video through Action Shot Summarization and Trajectory Visualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Amir</forename><forename type="middle">H</forename><surname>Meghdadi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pourang</forename><surname>Irani</surname></persName>
						</author>
						<title level="a" type="main">Interactive Exploration of Surveillance Video through Action Shot Summarization and Trajectory Visualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video visual analytics</term>
					<term>surveillance video</term>
					<term>video visualization</term>
					<term>video summarization</term>
					<term>video browsing and exploration</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We propose a novel video visual analytics system for interactive exploration of surveillance video data. Our approach consists of providing analysts with various views of information related to moving objects in a video. To do this we first extract each object&apos;s movement path. We visualize each movement by (a) creating a single action shot image (a still image that coalesces multiple frames), (b) plotting its trajectory in a space-time cube and (c) displaying an overall timeline view of all the movements. The action shots provide a still view of the moving object while the path view presents movement properties such as speed and location. We also provide tools for spatial and temporal filtering based on regions of interest. This allows analysts to filter out large amounts of movement activities while the action shot representation summarizes the content of each movement. We incorporated this multi-part visual representation of moving objects in sViSIT, a tool to facilitate browsing through the video content by interactive querying and retrieval of data. Based on our interaction with security personnel who routinely interact with surveillance video data, we identified some of the most common tasks performed. This resulted in designing a user study to measure time-to-completion of the various tasks. These generally required searching for specific events of interest (targets) in videos. Fourteen different tasks were designed and a total of 120 min of surveillance video were recorded (indoor and outdoor locations recording movements of people and vehicles). The time-to-completion of these tasks were compared against a manual fast forward video browsing guided with movement detection. We demonstrate how our system can facilitate lengthy video exploration and significantly reduce browsing time to find events of interest. Reports from expert users identify positive aspects of our approach which we summarize in our recommendations for future video visual analytics systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Surveillance cameras are ubiquitous and appear in environments such as hospitals, schools or banks. These cameras record an entire day's length of activities resulting in very long video footages that makes the process of browsing video content a laborious and time consuming task for a human observer. Fully automated video analysis methods eliminate the need for a human observer and rely on computer vision and machine learning to detect events of interest. However, these methods are not fully reliable particularly when the search criteria are subjective or vaguely defined.  Researchers have proposed various solutions to this problem by summarizing videos or visualizing the content in various forms that can potentially lead to more efficient manual browsing and exploration. Such human-in-the-loop systems rely on computer vision algorithms that are best suited for processing the video data at lower semantic levels with interaction and visualization features for facilitating robust human judgment at a higher semantic level. For example, detecting a moving vehicle and speculating whether the driver is impaired is a task with both lower and higher semantic elements, respectively. This process, also referred to as video visual analytics <ref type="bibr" target="#b12">[13]</ref> heavily relies on the interaction between the human operator and the computer system. Video visual analytics systems include a variety of dimensions consisting of video summarization <ref type="bibr" target="#b19">[20]</ref>   <ref type="bibr" target="#b31">[32]</ref>.</p><p>While a few systems have combined various video visual analytics features <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b15">[16]</ref>, researchers have suggested that further evaluations are necessary to explore how such tools facilitate a variety of analytic tasks <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b15">[16]</ref>. Ideally, novel video analytic systems with one or more dimensions can be effective in reducing users' browsing time in a wide array of search and exploration tasks with video data.</p><p>In this paper, we present sViSIT (selective Video Summarization and Interaction Tool), a video analytic system that summarizes and visualizes both the video content and the trajectories corresponding to individual moving objects. sViSIT facilitates user interaction through visual inspection of the video snippets and spatio-temporal filtering of the trajectories. We generate action shot images (stroboscopic representations of a moving object in a single image, e.g. see <ref type="figure" target="#fig_1">Fig. 1</ref>, top) to summarize and visualize the content of each and every movement in a video. We also augment the actionshot images with a space-time cube to visualize the spatial and temporal characteristics of movement trajectories (e.g. see <ref type="figure" target="#fig_2">Fig. 2</ref>, where the 2D trajectory would be self-intersecting and the 3D space time cube provides a better visualization). The system allows users to select regions of interest and filter the events based on the spatiotemporal characteristics of the movements. sVisit enables queries that (a) have subjective search criteria and are therefore challenging for an automatic vision algorithm, and (b) need acute inspection from an operator, such as: "finding a young man who has entered from a given door and has been loitering around the window", or "finding if and when a large van stopped or suspiciously slowed down on the bridge". Such complex tasks are possible primarily using sViSIT's spatio-temporal querying abilities and action-shot object movement visualization.</p><p>Our proposed contributions consist of the following: 1) a novel video visual analytics system (sViSIT) for interactive and analytic exploration of video data. The system links visualizations that contain information about both the specific content (action-shot image) and properties of the movement trajectory (space-time cube) of moving objects. 2) an evaluation consisting of a large variety of analytic tasks derived from our interaction with professional security personnel who work with such systems on a daily basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Most closely related to our contribution is work in video visual analytics <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b15">[16]</ref>. We also briefly review work specific to (a) video summarization, (b) video visualization and (c) interaction and navigation techniques for video analytics. <ref type="table" target="#tab_1">Table 1</ref> summarize some of this prior work while a thorough survey of all these elements are available in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>Video Visual Analytics In <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, the authors present a framework for video visual analytics based on the VideoPerpetuGram (VPG) <ref type="bibr" target="#b3">[4]</ref> to visualize objects' movement tracks. VPG is a visualization method that plots abstract illustrations of the movement trajectories along with some key frames at sparse locations. Users can interact with the system by applying filters on the trajectories. In another work <ref type="bibr" target="#b18">[19]</ref>, movement trajectories have been extracted and displayed on top of key frames from the video, however with limited user interaction. In <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b16">[17]</ref>, a method is presented for clustering, schematic visualization and scatter/gather browsing of the trajectories. Trajectories of moving objects provide useful information about the events in a surveillance video <ref type="bibr" target="#b24">[25]</ref>. In all of the above approaches, visualization is centred on the trajectory of the moving objects and the object itself is not visualized. Therefore, these methods are suitable for queries based on the movement characteristics of objects and not on the object itself. However, in many search and exploration tasks the visual appearance of the objects during their movement is as important as their trajectories.</p><p>Furthermore, the above systems have provided limited knowledge on browsing and exploration efficiency with multiple tasks. The most closely related user study is conducted in <ref type="bibr" target="#b14">[15]</ref>, to evaluate the efficiency of various methods for visualizing video frames in fast forward video playback. However, the evaluation is limited and elements of user interaction and movement object path extraction were not tested. To the best of our knowledge, our method of extracting movement paths and showing both movement content and properties is the first to evaluate varied tasks and analytic performance in terms of browsing time efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>Summarization Video summarization methods <ref type="bibr" target="#b28">[29]</ref> [20] <ref type="bibr" target="#b17">[18]</ref> aim at removing redundancies and providing a shorter, more concise representation of the video data for faster exploration by the human analyst. Fast forwarding and adaptive fast forwarding <ref type="bibr" target="#b14">[15]</ref> remove frames and shorten the length of the video. Detecting and displaying key frames only keeps key frames that contain important data. Mosaic images <ref type="bibr" target="#b17">[18]</ref> display a larger panoramic view of the scene created by alignment and integration of the frames for a moving camera that shows only the static background and not the moving objects. However, more recent methods in panoramic summary image <ref type="bibr" target="#b1">[2]</ref> generate a summary of both the moving object and the background in one single image while the camera is moving. These methods however are better suited for applications in arts, sports and entertainment and not for video surveillance applications where cameras record very long videos.</p><p>Methods in summarizing long video footage suitable for video surveillance include Non-chronological Video Synopsis that collapses the spatio-temporal information on the time domain <ref type="bibr" target="#b24">[25]</ref> to allow for the simultaneous display of events that happen at different moments. Similarly, Video Montage <ref type="bibr" target="#b18">[19]</ref>, collapses the data on both the time and spatial domain by pulling segments of the video from different spatial and temporal regions and stitching them together. These methods remove redundancies to display as much visual information as possible in a synopsis video which makes the summarization process very effective. However, visualization in the form of synopsis videos is condensed with many moving objects that make the browsing task difficult. In addition, the important information about the timing of events would be lost because they have been shifted in the time domain. These methods do not necessarily provide advanced filtering or content interaction <ref type="bibr" target="#b18">[19]</ref>  <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head><p>Visualization Traditional visualization of video data is limited to displaying the video frames with original or fast forward playback speed. Alternative visualizations have been proposed in recent years. Some of these methods visualize the video cube in 3D in its original <ref type="bibr" target="#b9">[10]</ref> or modified shape <ref type="bibr" target="#b6">[7]</ref>. Video Summagator <ref type="bibr" target="#b25">[26]</ref> provides a 3D visualization of the video cube and highlights the moving objects by blurring the background. Other methods extract and visualize only important information such as movement trajectories <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b16">[17]</ref>. Advanced visualization methods often entail summarization as well (described above). Dynamic Stills <ref type="bibr" target="#b4">[5]</ref>, and Video Narratives <ref type="bibr" target="#b5">[6]</ref> for example, both visualize a summary of the video by generating still images or short video clips that show the moving object at various positions in a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Interaction and Navigation</head><p>Interaction with the video content is an important element in video visual analytics as it ensures an interactive exploration of the content. The traditional interaction modalities with videos include Play; in normal or modified (e.g. Fast-Forward) playback speed, Pause, Jumping back and forward and Seeking using markers on a timeline view to navigate through video content <ref type="bibr" target="#b23">[24]</ref>. However, novel navigation techniques have been proposed for content dependent manipulation of the video playback <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b21">[22]</ref>, or content querying <ref type="bibr" target="#b31">[32]</ref>. Slit-tears <ref type="bibr" target="#b31">[32]</ref> allows for spatial querying of the video content on 1D regions of interest (line segments) and provides a time line visualization of the pixel variations across frames on the given line segments. However, it does not summarize the video in the time dimension and regions of interests are limited to the 1D area queried.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM ARCHITECTURE</head><p>The architecture of the user interaction is shown in a flowchart in <ref type="figure">Fig. 3</ref>. Automatic video processing and tracking does not require user intervention. The user interaction starts with selecting a region of interest. We describe the algorithmic steps in our visual analytics system as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motion Detection and Tracking</head><p>As the first step, our motion detection and tracking algorithm detects the moving objects in each video frame and extracts the movement trajectories. We first detect the background and update it every 5 seconds using a temporal median filter applied to the last minute of the video data before each frame (~375 frames at 6.25 frames per seconds), practically enough to converge to the background. We then generate the foreground by taking the absolute difference between each frame and the background at each pixel location. <ref type="figure">Fig. 4</ref>(a) and (b) show a sample frame and the result of background subtraction (original foreground), respectively. We convert the foreground into a binary (black and white image) mask by thresholding the pixel intensities after noise removal <ref type="figure">Fig. 4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c).</head><p>We then use a closing morphological operation to close the gaps and generate final blobs (segments) in the binary image. A connected component detection algorithm detects and labels each segment to be used as a mask to extract the moving objects <ref type="figure">Fig. 4(d)</ref>.</p><p>We track the objects by linking each moving object (segment) in each frame to the most similar (having the least distance) segment in the next frame. We define a distance measure between each pair of image segments by considering each segment as a set of points in the feature space of color components. Mahalanobis distance is a wellknown classical distance function between two data points with respect to distribution of the data points in a feature space <ref type="bibr" target="#b8">[9]</ref> and has been used as a reliable distance function between pixel values in RGB color space. The original Mahalanobis distance is defined between a pair of points. However, we propose to use the generalized Mahalanobis distance as defined in <ref type="bibr" target="#b0">[1]</ref> as the distance function between sets of points. Let X and Y be sets of feature vectors of pixels corresponding to image segments X and Y in frames F and F+1, respectively. The generalized Mahalanobis distance ( , ) between X and Y is defined <ref type="bibr" target="#b0">[1]</ref> as:</p><formula xml:id="formula_0">( , ) = �(��⃗ − ��⃗ ) −1 (��⃗ − ��⃗ )</formula><p>Where m ���⃗ X and m ���⃗ Y are average feature vectors for all the pixels in X and Y and Σ W is within-class covariance matrix defined in</p><formula xml:id="formula_1">Σ = 1 2 �∑ ( − ���⃗ )( − ���⃗ ) + ∑ ( − ���⃗ )( − ���⃗ ) �,</formula><p>where and are cardinalities of sets X and Y. In order to track all objects and extract the movement trajectories in a systematic way, we use a layered graph to represent the moving objects in the video. A layered graph is a graph in which nodes are partitioned into layers 1 , … , and all edges are between adjacent layers <ref type="bibr" target="#b27">[28]</ref>. Each layer in the graph represents one video frame and contains several nodes that represent image segments (moving objects) in that frame <ref type="figure">(Fig. 5)</ref>.</p><p>We connect (link) each node in each layer (video frame) to the node in the previous frame that has a minimum distance as explained above and also below a set threshold. We introduce a depth-first algorithm to search and detect every possible path (corresponding to a movement trajectory) in the graph. We then use this information to extract movement trajectories and generate action shot summaries (next section). We implemented the system using the above method. More sophisticated tracking algorithms such as <ref type="bibr" target="#b26">[27]</ref> can also be used if one requires more robust tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head><p>Visualization and User Interaction By extracting trajectories, our system provides interactive filtering and retrieval of specific events based on spatio-temporal characteristics of the movements. Users start by selecting region(s) of interest and the system automatically applies the filter and keeps only those movement events where the movement track (trajectory) intersects with the region of interest. Logical combinations (AND/OR) are also possible when dealing with more than one region of interest. A timeline overview of the movement events and the number of retrieved movements is displayed. The user can select any movement for further examination either by a linear search through all of them or by clicking on the corresponding timeline view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Action Shot Image For each movement event, we generate an action shot image by extracting the moving object (a segment) in each frame and adding them to the background image to create the stroboscopic effect. To minimize self-occlusion (overlapping the image segments in two consecutive frames), a new extracted segment in each frame is not added to the background unless the overlap between the new segment and the previously added one is smaller than a threshold (e.g. 0 for complete separation). This makes the method less susceptible to changes in the speed of moving objects and independent of frame rate. The user can then browse through action shot images and also play the video segment for each movement to examine the course of events in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Space-Time Cube</head><p>A space-time cube, (e.g see <ref type="figure" target="#fig_2">Fig. 2</ref> and <ref type="figure">Fig. 8</ref>) is a 3D geometrical graph where the base of the cube represents the x-y spatial dimension (image domain) and the cube's height represents the temporal dimension (time domain) <ref type="bibr" target="#b20">[21]</ref>. The space-time cube complements the action shot view as it can visualize spatio-temporal characteristics of the movements (such as speed; a steeper line indicates a slower movement) that are otherwise not apparent in the action shot image. <ref type="figure">Fig. 8</ref> for example shows an action shot that only displays a person approaching a chair while its trajectory in the space time cube clearly shows that the person sat on the chair for more than 30 seconds. Multiple trajectories can be visualized in our system simultaneously and the trajectories can be explored by viewpoint rotation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM IMPLEMENTATION</head><p>A snapshot of the sViSIT user interface is shown in <ref type="figure" target="#fig_5">Fig. 9 (a)</ref>. We implemented the system in MATLAB® and used the Image Processing Toolbox for basic image processing operations such as filtering, morphological closing or connected component detection.</p><p>Our system is able to work on video segments (we refer to them as episodes) extracted from the unlimited stream of the surveillance video data. The user can also move a sliding window to select the time interval of analysis as a period of interest (POI) with respect to each query. However, in our user study, we used episodes of half an hour length and used the whole episode as the period of interest to analyse our 30 min long videos at once. The period of interest (POI) and a bar chart of the number of moving objects in each frame during an episode are plotted <ref type="figure" target="#fig_5">(Fig. 9a,  part D</ref>). An event timeline of all movement events in the period of interest is also generated and displayed <ref type="figure" target="#fig_5">(Fig. 9a, part C)</ref>. The event timeline represents time and duration of each movement event using horizontal bars. The selected action shot and its movement trajectory in the space-time cube are shown in part A and B <ref type="figure" target="#fig_5">(Fig. 9a)</ref>, respectively. Users either browse through all the detected movements using the right and left arrow keys or jump to any part of the video by clicking on the corresponding horizontal bar on the events timelines. The original video segment corresponding to each action shot image can be played in the same window as the action shot image.</p><p>In the main user study, we compared our system against manual browsing with fast forward video playing. However, in order to properly evaluate our system and demonstrate the effect of our visualization methods, we implemented a new video player with motion detection capability in regions of interest but without the advanced visualizations in sViSIT. We refer to this video player as FFMD: Fast Forward Motion Detection. <ref type="figure">Fig. 8</ref> shows the controls in the FFMD user interface. FFMD has a slider as well as a timeline view that displays temporal regions in a video where moving objects are detected in selected region(s) of interest. Users can navigate the video through Play, Pause and Fast-Forward playback (with varying speeds). Also, they can Seek movement activities by clicking on the motion detection timeline to jump into part of the video (highlighted with green bars) when moving objects are detected inside the regions of interest. This interface is akin to those currently available on commercial systems (e.g. Panasonic® <ref type="bibr" target="#b29">[30]</ref>), in terms of functionality (i.e. motion detection in regions of interest and fast forward playback).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYSTEM EVALUATION AND USER STUDY</head><p>We conducted a two part evaluation of our system. One of the primary concerns of our domain experts, campus security personnel, consisted of improving and making the analysis more efficient. An initial controlled experiment allowed us to identify the value of sViSIT for efficient video browsing. We also met with domain experts to receive feedback on how well our design meets their analytic requirements. We hypothesized that performing common video analytic tasks will be more efficient in sViSIT in comparison to FFMD, a user friendly implementation of the common method for browsing videos. We also hypothesized that performance with sViSIT is an improvement over the general fast-forward approach. We chose a 5x playback speed as a baseline as in <ref type="bibr" target="#b21">[22]</ref> it was shown that the ability to understand the content of the video would start degrading if the playback speed increased more than about 5 times the original speed. Our domain experts also indicated that this was the typical fast-forward speed they would use.</p><p>We expect the time-to-completion of a task (TTC) to depend on when an event (or target) appeared in the video relative to its start (we refer to it as ). We also hypothesize that there are various other factors that could affect the performance of the search and exploration such as the type of search criteria, how crowded the frames are in the video and the size of the user selected region of interest (ROI). We investigate the effect of these factors on analytic performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Materials</head><p>We recorded 120 minutes of video data (4 different videos, 30 min each) from various indoor and outdoor public places. The content included movements of people and vehicles <ref type="figure" target="#fig_1">(Fig. 9, b.1 to b.4)</ref>. We down-sampled the video frame size to 608×336 pixels and video frame rate to 6.25 frames per second to make it appear similar to common video surveillance data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Tasks</head><p>In consultation with our selected domain experts (campus security) we identified 14 different tasks that require browsing the videos in search of specific events (targets) of interest ( <ref type="table" target="#tab_2">Table 2)</ref>. We evaluated our system by measuring the time-to-completion of the above tasks. In our tasks, targets appear at different times in the videos as shown in <ref type="table" target="#tab_2">Table 2</ref>. This will prevent users from making any assumption about the temporal locations of the targets and to prevent biasing the results. To minimize the selection bias, we randomly selected our tasks and videos before the user study and included all the results. Our tasks can be categorized based on the task type, video crowdedness and size of the region of interest. The various task type categories include: Type A) Presence in ROI: Detecting the presence/duration of a specific type of object (e.g. cars driving in a street, pedestrians crossing, objects left behind …). Experts suggested that this was one of the most common tasks they performed to detect loitering, vandalism and other similar events based on a specific location. Type B) Activity detection: Detection of a specific activity (e.g.</p><p>handshaking, talking, vandalism …). This was a frequent task reported by our domain experts and one which involved identifying specific actions even after localizing the time period in which the event occurred. Type C) Visual search for a given target: Detection of a specific given target that is shown to the user in an image (e.g. tasks 5,9,12). This task category was performed less frequently by our experts but occurs when witnesses identify a potential suspect. We also defined three levels for the size of the region of interest. These levels are: VL (Very Limited: a small part of the video is selected as the region of interest), L (Limited: part of the video frame is selected as the region of interest) and NL (Not limited: the region of interest is very large or non-existent, i.e. the entire video viewing angle). The region of interest limits the search of movement paths to those that intersect with the region.</p><p>We also categorized our videos based on the level of movement activities. We call these categories Uncrowded (UC), Crowded <ref type="bibr">(CR)</ref> and Very Crowded (VC), wherein the first signifies very little movement and the latter represents a high amount of movement. Most often, according to our domain experts, events of a critical nature take place in the former or uncrowded activity areas.</p><p>We first conducted a pilot study with 8 participants who performed all the above tasks using sViSIT. The purpose of this pilot study was to test the system performance by itself, the feasibility of the user study for the designed tasks and user feedback about the tool. In our feedback forms, 7 participants described the system as Very Useful and one participant as Useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Subjects</head><p>For our controlled experiment, we recruited 18 volunteer participants between the ages of 21 and 32 (university students with no prior knowledge about this project, not including the participants in the pilot study). 17 were male. We asked them to perform the 14 tasks in the same order listed in <ref type="table" target="#tab_2">Table 2</ref>. None were experts in video analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4</head><p>Procedure All sessions were supervised to ensure that users were exploiting all the features of the tool. Each session started with 15 min of introduction and training and lasted about one hour in total for each participant. Subjects sat directly in front of the monitor at a distance of about 60-70 cm and the size of the videos/images displayed was 21×12 cm on both sViSIT and FFMD user interfaces. The participants were randomly divided into two groups to counter balance the assignment of the tools to task numbers. The first (/second) group performed odd (/even) numbered tasks with sViSIT and the remaining tasks with FFMD. Each task was delivered to the users through a set of instructional dialog boxes which asked the participants to read and understand the task description before they proceeded with the task. The task descriptions and the target images to be shown to the participants (type C) were displayed on a separate monitor to their left side. Participants were instructed to click on a STOP button as soon as they found the target at which point the time-to-completion was recorded and they were able to write down the answers in a dialog box. In one trial, the user gave up after searching in the video for 12 min using the FFMD player and this trial data was discarded. We presented the tasks one by one in the given order and we supervised the study to make sure users did not gain prior knowledge about the tasks.  <ref type="table" target="#tab_2">Table 2</ref> shows average time-to-completion (TTC) of the tasks for sViSIT and FFMD video player (last two columns). TTC values are considerably smaller than the target appearance times in the video, which demonstrates the advantage of using our tools compared to manual browsing of the video with original speed. As expected, TTC is highly correlated with the relative target appearance time ( ) in the video, i.e. a target toward the end of the video will take much longer to find than one toward the start. Therefore, we define and use a normalized time-to-completion = TTC/ to be able to compare the time-to-completion between different tasks independent of . <ref type="figure" target="#fig_1">Fig. 12(a)</ref> shows the average and standard error of the normalized time-to-completion (nTTC) plotted for sViSIT and FFMD for all tasks sorted based on .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>We also compare nTTC with a baseline value of 0.2 which corresponds to the target appearance time in 5x fast forward video playback. According to <ref type="figure" target="#fig_1">Fig. 12(a)</ref>, targets that appear in the video too early (less than about 5 min, &lt;5:00, 1 min in a 5x fastforward), are more likely to be detected faster by watching the 5x fast forward video. This is expected because it always takes some time to select regions of interest and interact with the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5.1</head><p>The Effect of the Browsing Tool, Task and Video Type One way ANOVA shows the significance of the effect of the following factors on average normalized time-to-completion (nTTC) as follows. The tool (sViSIT versus FFMD): overall, average performance (nTTC) of sViSIT and FFMD are significantly different <ref type="figure" target="#fig_1">(Fig. 12b)</ref>. sViSIT is more efficient than FFMD <ref type="figure" target="#fig_1">(F(1,17)</ref>=5.584, p=0.03). Search type (A, B or C): there was a significant difference between tasks of type C (visual search for a given target) and other types in favour of type C <ref type="figure" target="#fig_1">(Fig. 13b</ref>) Crowdedness: We did not find any effect on nTTC due to crowdedness factor in our 4 videos. Limitedness of the region of interest: There is a significant difference between Limited (L)/Very limited (VL) and Not limited (NL) regions of interest. More limited regions of interest result in less nTTC as expected. <ref type="figure" target="#fig_1">(Fig. 13a</ref>)  We did not find significant differences when considering the interactions between the above factors in a multi-way ANOVA. However, we found the following <ref type="figure" target="#fig_1">(Fig. 14)</ref> for one way analysis of variance of nTTC for sViIST and FFMD (sViSIT performs better on average in all cases). sViSIT is more efficient than FFMD, for task type A (F(2,34)=5.967, p=0.006), when the video is Uncrowded (F(2,34)=4.55, p=0.017) or if the ROI is Very Limited (F(2,34)=4.46, p=0.019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Subjective Evaluation and Discussions</head><p>Post-experiment, participants completed a multiple choice questionnaire to provide subjective feedback about sViSIT and FFMD. 88% and 12% indicated that sViSIT is "much better" and "better" respectively. The other choices (which were not selected by any participant) were "FFMD much better", "FFMD better", "both are equally good" and "none of them are good". This shows the overall impression of the users was strongly in favour of sViSIT although on average trends show that they performed three out of 14 tasks faster with FFMD.</p><p>We watched and examined the user behaviour while participants performed the tasks. Our qualitative observations indicate that the performance also depends on the user strategy (especially for selecting the region of interest) and the level of experience. For example, some users got frustrated with the high number of detected movements that they needed to browse and thus kept changing the ROI. This increased their search times, but also refined their ROIs. Other users kept browsing through the results and found the target after the first set of results were obtained based on ROI selection. Some users showed innovative ways of selecting the region of interest. <ref type="figure" target="#fig_1">Fig. 15</ref>, for example shows an ROI selected by one of the users to capture movements in all directions including the target image shown. We expect that with more experience, users will be able to include varying levels of selection to improve their performance. Our qualitative observation of the user study indicates that the main advantage of sViSIT over video playback is the ability of filtering (through selection of ROI) and summarizing (through action shot images and space time cube). We also noticed that for the given tasks in our user study, users relied more on action shot images rather than space time cubes. This is consistent with a feedback from our domain experts (section 5.7 below) who believe there is more training needed for the space time cube. However, if the tasks require detection of the speed of moving objects (e.g. finding a car that slows down in a highway), then we expect use of the space time cube to be inevitable as other visualizations do not provide information about the speed of moving objects. The ability to visualize trajectories of more than one moving object in the space time cube is another useful feature for our space time cube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Expert Evaluation</head><p>We also asked our domain experts to evaluate and provide feedback on the components of our system, the action shots, that showed content of movement trajectories, the profiles of trajectories and the ability to select regions and time intervals. One security officer performed the tasks in <ref type="table" target="#tab_2">Table 2</ref> using our tool. He then indicated the benefits offered by our tool and three other members shared their opinions upon observing the user study. They indicated the significant benefits offered by our tool. We include below a few of their comments.</p><p>• The events timeline <ref type="figure" target="#fig_5">(Fig. 9</ref>.C) was seen as being very useful as it can quickly show the events with the longest duration. This could be an indicator that an individual is spending significant time in a given zone. They used the events timeline before browsing and using any other feature in sViSIT. • They indicated that the space-time cube was slightly difficult initially but was then deemed useful after 15 minutes of experience with the system. For example, they indicated that an emerging pattern during vehicle break-in involves a suspect visiting multiple cars in the parking lot. This pattern would be clearly visible as a series of arcs in the space-time cube. • They found the action shot images to be useful for both analytic purpose and documenting specific events. An action shot depicting a suspect in various positions could provide a visual summary of the activity and deemed this to be highly beneficial in incident reports. • In many of the search and exploration tasks, an officer needs to locate an incident (e.g. vandalism) within a long time-frame (up to 48 hours). The operator follows a binary search algorithm to locate the time of incident which occurs when a noticeable change occurs in the scene (e.g. a broken window). This strategy could be very time consuming and even impossible if the incident does not leave a noticeable change (for example, task 11, <ref type="figure" target="#fig_1">Fig. 11</ref>(k) in our user study). sViSIT was shown to be highly useful in locating such incidents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.8</head><p>Limitations If a video is too crowded with dense activity and without any specific criteria on the movement trajectory, there is no gain in summarization and filtering. However our system is powerful in video surveillance applications where the task often involves searching for a person/activity in a limited region of interest during a long time-frame (period of interest) with sparse activities. We have implemented a general-purpose detection and tracking algorithm in our video processing stage with no training required and with a small number of limitations (e.g. occlusion or merge/split of different moving objects). Therefore, the tracking system is prone to limitations of the current state of the art in the field of tracking. However, such limitations only affect the performance and not reliability of our system because the visualizations are still meaningful for a human observer who makes the judgements and has the ability to examine the raw video and confirm his/her findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.9</head><p>Computational Cost Although our current implementation of sViSIT in MATLAB® is not optimized for speed, it is fast enough for real time motion detection up to 3 frames/sec (608×336 pixels) of the video stream. We recorded the average computational time per video frame for one of the videos (video 4, average crowdedness) as an example. The average time needed for detection, tracking and creating the visualizations are 137, 93 and 75 milliseconds, respectively, for a total of 305ms equivalent to about 3 frames per second. This means the processing phase can be done in parallel with video recording, storing data for user interaction since the system is designed for offline browsing of the recorded video. The next phase (which is user interaction, querying and spatio-temporal filtering), is executed within a second using the existing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SUMMARY AND CONCLUSIONS</head><p>We presented and evaluated a video visual analytics system (sViSIT) that can help users search a video and find targets in a fast and efficient way. We showed that except for very short videos (less than 5min in our examples), using sViSIT is considerably faster than waiting for the target event to happen in a 5x fast-forward video ( <ref type="table" target="#tab_2">Table 2</ref> and the baseline in <ref type="figure" target="#fig_1">Fig. 12</ref>). Moreover, statistical analysis of the time-to-completion in section 5.5 <ref type="figure" target="#fig_1">(Fig. 14)</ref> confirmed our hypothesis that sViSIT performs significantly better than FFMD. This superiority is shown to be more substantial in less crowded videos with a long time-frame of interest and limited region of interest. This is expected as the power of our system is due to its ability to summarize movements individually and apply spatiotemporal filters to limit the search results. For our future work, we intend on expanding the querying capabilities of our system based on various characteristics of the movement trajectories such as speed, direction and start/end points. More advanced methods of querying are potentially possible through user-drawn target trajectories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Amir H. Meghdadi is with the Department of Computer Science, University of Manitoba, Canada. E-mail: amir@cs.umanitoba.ca. • Pourang Irani is with the Department of Computer Science, University of Manitoba, Canada. E-mail:Irani@cs.umanitoba.ca. Manuscript received 31 March 2013; accepted 1 August 2013; posted online 13 October 2013; mailed on 4 October 2013. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Our proposed system (sViSIT) extracts motion paths of moving objects in surveillance video. The content of each movement path is made available through action shot images (top images) and properties of the motion itself are visualized using a space-time cube (bottom-images). Users can quickly navigate through such content and its associated trajectories to locate items of interest. Notice that in the middle action shot image, the user can locate the suspect person who left a box on the floor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Space time cube allows spatio-temporal visualization of a movement trajectory</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Fig. 5 .Fig. 4 .</head><label>354</label><figDesc>Flowchart of the proposed system: sViSIT A layered graph representation of the moving objects in video frames. Each node represents a detected blob in one frame. The red trajectory represents a split in the tracking.(a) a sample frame (b) initial foreground (c) foreground after thresholding (d) masks applied to the frame Foreground detection for a sample frame</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .Fig. 8 .</head><label>678</label><figDesc>An example of the movement trajectory in space-time cube (centre) and the action shot image (top right) An episode of video data Vertical bars on a timeline in FFMD player represent the presence of an object in the region of interest. Users can click on the bars and it will jump to that location in the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>(a) sViSIT: selected action shot image (A), the space time cube (B), the events timeline in the adjustable period of interest marked between red and blue lines (C), timeline of the full episode displaying the number of moving objects in each and every frame (D). Events during the period of interest are visualized. (a) a snapshot of our tool (sViSIT), (b1)-(b4) one sample video frame and the region of interest from each video in the user study 5.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 . 14 Fig. 11 .</head><label>101411</label><figDesc>Target images presented to the users in type C tasks (a) to (d) Task 10 (k)Task 11 (l) Task 13 (m) task Target events in type A and B, not presented to the user, (e) to (m)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .Fig. 13 .Fig. 14 .Fig. 15 .</head><label>12131415</label><figDesc>(a) average and standard error nTTC for tasks sorted based on Tt (b) means and 95% confidence intervals for sViSIT and FFMD Normalized time-to-completion (nTTC) differences between sViSIT and FFMD, (a) average nTTC plotted for each task compared to baseline value of 0.2 ( 5x fast forward), (b) comparison between estimated means of trials in sViSIT and FFMD group Overall nTTC differences due to limitedness of ROI and task type (Crowdedness has no visible effect on nTTC) Significant nTTC differences between sViSIT and FFMD (%95 confidence intervals) for each condition (a) Task type = A, (b) ROI = Very Limited and (c) video is Uncrowded An example of an interesting ROI selected by a user</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Selected Previous Work in Video Summarization, Visualization and Interaction</figDesc><table><row><cell>First Author. Year</cell><cell>Method</cell><cell>Contribution</cell><cell>Surveillance Application</cell><cell>Summarizatio n in time domain</cell><cell>Interaction content querying</cell><cell>Object Visualization</cell><cell>Trajectory Visualization</cell></row><row><cell>Irani.1996</cell><cell>Mosaic images [18]</cell><cell>Panoramic image representing background contents, moving camera</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Bartoli.2004</cell><cell>Motion panorama [2]</cell><cell>Panoramic image representing a moving object, moving camera</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell cols="2">✓ -</cell></row><row><cell>Kapler.2004</cell><cell>Video Summagator [26]</cell><cell>Visualizes the video cube in 3D and highlights the movements</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">✓ -</cell></row><row><cell>Kang.2006</cell><cell>Video Montage [20]</cell><cell>Collapsing spatio-temporal data in both time and spatial domain.</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Caspi.2006</cell><cell>Dynamic Stills [5]</cell><cell>Action shots (a.k.a. dynamic stills) visual summaries of a single movement.</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell cols="2">✓ -</cell></row><row><cell>Pritch.2008</cell><cell>Video synopsis [29]</cell><cell>Non-chronological synopsis shifts the movements only in the time domain</cell><cell cols="2">✓ ✓</cell><cell>-</cell><cell cols="2">-✓</cell></row><row><cell>Tang.2009</cell><cell>Slit-tears [32]</cell><cell>Visualizing 1D regions of interests (line segments) in a video</cell><cell>-</cell><cell>-</cell><cell cols="3">✓ ✓ -</cell></row><row><cell>Höferlin.2009</cell><cell>Visual analytics [13]</cell><cell>VideoPerpetuGram (VPG) visualization and filtering of trajectories.</cell><cell>✓</cell><cell>-</cell><cell>✓</cell><cell cols="2">-✓</cell></row><row><cell>Correa.2010</cell><cell>Video Narratives [6]</cell><cell>An interactive authoring system to create compact representation of the movements using mosaic images</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell cols="2">✓ -</cell></row><row><cell>Höferlin.2011</cell><cell cols="2">Schematic Summaries [14] Interactive clustering and schematic visualization of the movement tracks.</cell><cell cols="2">✓ ✓</cell><cell>✓</cell><cell cols="2">-✓</cell></row><row><cell cols="2">Sunkavalli.2012 Video Snapshots [31]</cell><cell>High quality still images (snapshots or action shots) from a short video clip</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell cols="2">✓ -</cell></row><row><cell>Present paper</cell><cell>sViSIT</cell><cell>Action shot and trajectory visualization in an interactive system</cell><cell cols="2">✓ ✓</cell><cell cols="3">✓ ✓ ✓</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Task List, Target Time (Tt) and Average Time-to-Completion TTC for sViSIT and FFMD</figDesc><table><row><cell cols="2">Task Vid</cell><cell>Task description</cell><cell cols="2">Type Crowd edness ROI</cell><cell>Tt mm:ss</cell><cell>Avg. TTC sViSIT</cell><cell>Avg. TTC FFMD</cell></row><row><cell>1</cell><cell cols="2">2 Find a cyclist riding along the street.</cell><cell>A</cell><cell>UC L</cell><cell>8:42</cell><cell>00:31 00:49</cell></row><row><cell>2</cell><cell cols="2">2 Find a pedestrian crossing the street.</cell><cell>A</cell><cell>UC L</cell><cell>3:19</cell><cell>00:39 01:16</cell></row><row><cell>3</cell><cell cols="2">2 Find a pedestrian walking in the parking lot.</cell><cell>A</cell><cell>UC L</cell><cell>5:37</cell><cell>00:23 00:31</cell></row><row><cell>4</cell><cell cols="2">2 Count the number of vehicles driving along the street.</cell><cell>A</cell><cell>UC L</cell><cell>12:24</cell><cell>00:30 01:22</cell></row><row><cell>5</cell><cell cols="2">4 Find the person shown in Fig. 10(a) (carrying a green bag).</cell><cell>C</cell><cell>CR L</cell><cell>18:50</cell><cell>01:41 02:31</cell></row><row><cell>6</cell><cell cols="2">4 Find the person in red jacket entering the scene from a given corner.</cell><cell>A</cell><cell cols="2">CR VL 19:54</cell><cell>01:14 01:52</cell></row><row><cell>7</cell><cell cols="2">4 Find two people who meet and handshake.</cell><cell>B</cell><cell cols="2">CR NL 10:46</cell><cell>02:23 01:13</cell></row><row><cell>8</cell><cell cols="2">4 Find the suspicious person who talks to someone on the roof</cell><cell>B</cell><cell>CR NL</cell><cell>2:13</cell><cell>00:38 00:52</cell></row><row><cell>9</cell><cell cols="2">3 Find the person shown in Fig. 10(b) (carrying plastic boxes)</cell><cell>C</cell><cell cols="2">VC NL 18:16</cell><cell>03:50 02:34</cell></row><row><cell cols="3">10 3 Find a box being dropped off on the ground in a given area.</cell><cell>A</cell><cell cols="2">VC VL 28:08</cell><cell>02:01 03:47</cell><cell>Legend:</cell></row><row><cell cols="3">11 3 Find the person who commits vandalism on the given poster.</cell><cell>B</cell><cell cols="2">VC VL 28:16</cell><cell>00:47 01:52</cell><cell>UC(Uncrowded), CR(Crowded),</cell></row><row><cell cols="3">12 3 Find the person shown in Fig. 10(c) (carrying boxes)</cell><cell>C</cell><cell cols="2">VC NL 29:39</cell><cell>01:50 03:58</cell><cell>VC(Very crowded), NL(Not limited),</cell></row><row><cell cols="3">13 1 Find the person who is loitering around the entrance door.</cell><cell>A</cell><cell>UC VL</cell><cell>9:20</cell><cell>00:53 00:34</cell><cell>L(Limited), VL(Very limited)</cell></row><row><cell cols="3">14 1 Find the person who leaves a recycle bin on the top floor Fig. 10(d)</cell><cell>C</cell><cell>UC L</cell><cell>11:31</cell><cell>00:52 01:21</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank Christopher Bohonis, Tyler Green and Security Services at the University of Manitoba for their input in identifying the important video analytic tasks and for providing feedback on the final deployment of our tools. We also thank anonymous reviewers and our lab members who provided feedback on this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Quantitative Method for Comparing Multi-Agent-Based Simulations in Feature Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="154" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Multi-Agent-Based Simulation IX</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Motion panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Animation and Virtual Worlds</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="501" to="517" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Survey on Video-based Graphics and Video Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Borgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Daubney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the EuroGraphics conf</title>
		<meeting>of the EuroGraphics conf</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>State of the Art Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action-Based Multifield Video Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Botchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="885" to="899" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic stills and clip trailers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gamliel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="642" to="652" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic video narratives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="88" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Visualization</title>
		<meeting>of IEEE Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video browsing by direct manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bibliowitcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pattern Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive video cubism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kenji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 1999 workshop on new paradigms in information visualization and manipulation</title>
		<meeting>of 1999 workshop on new paradigms in information visualization and manipulation</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="78" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Schematic storyboarding for video visualization and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="862" to="871" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comet and target ghost: techniques for selecting moving targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>of SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video visual analytics of tracked moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop on Behaviour Monitoring and Interpretation</title>
		<meeting>of Workshop on Behaviour Monitoring and Interpretation</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">541</biblScope>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive schematic summaries for exploration of surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia Retrieval, ACM ICMR 2011</title>
		<meeting>ACM International Conference on Multimedia Retrieval, ACM ICMR 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluation of Fast-Forward Video Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2095" to="2103" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uncertainty-aware video visual analytics of tracked moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Spatial Information Science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive Schematic Summaries for Faceted Exploration of Surveillance Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Höferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="908" to="920" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient representations of video sequences and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="351" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Surveillance video summarization based on moving object detection and trajectory extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jintao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Signal Processing Systems, ICSPS 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Space-time video montage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition, CVPR2006</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GeoTime Information Visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kapler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. Information Visualization, INFOVIS&apos;04</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trailblazing: Video playback control by direct object manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kimber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dunnigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girgensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CinemaGazer: a system for watching videos at very high speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Working Conference on Advanced Visual Interfaces</title>
		<meeting>of International Working Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="108" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Browsing digital video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGCHI conference on Human Factors in Computing Systems</title>
		<meeting>of SIGCHI conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Survey of Vision-Based Trajectory Learning and Analysis for Surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1114" to="1127" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video Summagator: an interface for video summarization and navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>of SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="647" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tracking with Occlusions via Graph Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bugeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shortest paths without a map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mihalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="150" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonchronological video synopsis and indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1971" to="1984" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Panasonic video surveillance products</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video Snapshots: Creating High-Quality Images from Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1868" to="1879" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Extended Abstracts on Human Factors in Computing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3509" to="3510" />
		</imprint>
	</monogr>
	<note>Exploring video streams using slittear visualizations</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
