<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D Lung Models from Single-View Projections by Deep Deformation Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichun</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Hua</surname></persName>
						</author>
						<title level="a" type="main">DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D Lung Models from Single-View Projections by Deep Deformation Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934369</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep deformation network</term>
					<term>organ meshes</term>
					<term>3D / 4D shapes</term>
					<term>2D projections</term>
					<term>single-view</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper introduces a deep neural network based method, i.e., DeepOrganNet, to generate and visualize fully high-fidelity 3D / 4D organ geometric models from single-view medical images with complicated background in real time. Traditional 3D / 4D medical image reconstruction requires near hundreds of projections, which cost insufferable computational time and deliver undesirable high imaging / radiation dose to human subjects. Moreover, it always needs further notorious processes to segment or extract the accurate 3D organ models subsequently. The computational time and imaging dose can be reduced by decreasing the number of projections, but the reconstructed image quality is degraded accordingly. To our knowledge, there is no method directly and explicitly reconstructing multiple 3D organ meshes from a single 2D medical grayscale image on the fly. Given single-view 2D medical images, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end DeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung models with a variety of geometric shapes by learning the smooth deformation fields from multiple templates based on a trivariate tensor-product deformation technique, leveraging an informative latent descriptor extracted from input 2D images. The proposed method can guarantee to generate high-quality and high-fidelity manifold meshes for 3D / 4D lung models; while, all current deep learning based approaches on the shape reconstruction from a single image cannot. The major contributions of this work are to accurately reconstruct the 3D organ shapes from 2D single-view projection, significantly improve the procedure time to allow on-the-fly visualization, and dramatically reduce the imaging dose for human subjects. Experimental results are evaluated and compared with the traditional reconstruction method and the state-of-the-art in deep learning, by using extensive 3D and 4D examples, including both synthetic phantom and real patient datasets. The efficiency of the proposed method shows that it only needs several milliseconds to generate organ meshes with 10K vertices, which has great potential to be used in real-time image guided radiation therapy (IGRT).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Cone beam computed tomography (CBCT) has become increasingly important in cancer radiotherapy for understanding the anatomical structure of organs and pinpointing tumors during the treatments. Integrated CBCT is an important and convenient tool for patient positioning, verification, and visualization in image guided radiation therapy (IGRT). Traditional high-quality CBCT image reconstruction requires near hundreds of projections, which consequently deliver undesired high imaging / radiation dose to patients as well. The high imaging dose to healthy organs in CBCT scans <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref> is a crucial clinical concern. Practically, the imaging dose in CBCT can be reduced by reducing the number of X-ray projections and lowering tube voltage setting. On the other hand, due to the limited number of projections, the image quality is highly degraded in 3D-CBCT reconstructed by conventional methods, such as Feldkamp-Davis-Kress (FDK) <ref type="bibr" target="#b15">[16]</ref> (plenty of artifacts and noises with lower accuracy). Several strategies have been proposed to enhance the image quality of reconstructed CBCT. One major kind of approaches is to use iterative image reconstruction algorithms, such as simultaneous algebraic reconstruction technique (SART) <ref type="bibr" target="#b0">[1]</ref>, total variation (TV) minimization <ref type="bibr" target="#b46">[47]</ref>, and prior image constraint techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>. Thus, the accuracy of subsequent 3D organ modeling does highly depend on the quality of the reconstructed images. Currently, the doctors and clinicians need to use some post-processing methods / tools to segment, reconstruct, and visualize the 3D organ models, which are quite time-consuming and cumbersome.</p><p>Recently, there is an emerging trend to generate 3D models by deep</p><formula xml:id="formula_0">{ }</formula><p>neural network in computer vision, computer graphics, and visualization communities, in which the 3D shapes can be captured and represented from the input raw data in different formats such as 3D meshes, 3D point clouds, 3D volumes, multi-view 2D images, etc. Among them, deriving the 3D shape from a single view is fundamental and very challenging. Recently, deep learning techniques have been developed to generate / reconstruct 3D shapes from a single RGB natural image (e.g., photograph) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52]</ref>. Their 3D shape outputs from the neural network can be represented in different formats, such as a volume <ref type="bibr" target="#b8">[9]</ref>, point loud <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>, or surface mesh <ref type="bibr" target="#b51">[52]</ref>. However, these methods either require complicated post-processing to generate the surface models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>, or have non-manifold and invalid surface elements <ref type="bibr" target="#b51">[52]</ref>. In order to build the bridge to directly generate the 3D shape meshes from a single 2D medical grayscale image on the fly, in this work, we present a deep neural network based method, i.e., , to generate and visualize high-fidelity fully 3D / 4D organ geometric models from single-view medical images, e.g., 3D / 4D-CBCT projections, by learning the smooth deformation fields based on a trivariate tensor-product deformation technique. Experimental results are evaluated and compared with the traditional reconstruction method and the state-of-the-art in deep learning, by using extensive 3D and 4D examples, including both synthetic phantom and real patient datasets. The key of our work are as follows: It proposes an end-to-end deep learning method with a lightweight but effective neural network to reconstruct multiple high-fidelity 3D organ meshes with a variety of geometric shapes from a singleview medical image with complicated background and noises. The proposed organ reconstruction network simultaneously learns the optimal selection and the best smooth deformation from multiple templates via a trivariate tensor-product deformation technique, i.e., free-form deformation (FFD), to match the query 2D image. To our knowledge, it is the first time using deep learning framework to generate multiple 3D organ meshes (such as left and right lungs in our application) from a single-view medical image.</p><p>The application and user study on IGRT demonstrate that the accurate on-the-fly tracking and reconstruction of 3D / 4D organ shapes facilitated by our method have the potential in improving the current IGRT procedure and practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we only review some most related work on 3D shape reconstruction from single images in computer vision / graphics, visualization, and medical imaging domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Shape from Single-View Image in Computer Vision</head><p>In computer vision, graphics, and visualization, 3D reconstruction is the process of capturing the shape and appearance of real objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Traditional Learning Based Methods</head><p>Hoiem et al. <ref type="bibr" target="#b19">[20]</ref> and Saxena et al. <ref type="bibr" target="#b40">[41]</ref> started to use statistic and learning based approaches for 3D shape reconstruction from a single image several decades ago. Recently, Kar et al. <ref type="bibr" target="#b27">[28]</ref> proposed to learn category-specific 3D shape models from object silhouettes and then capture intra-class shape variation from a single image. Jin et al. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> presented an approach to reconstruct and analyze the faces from frontal (and side) images through learning a 3D morphable face model. Carreira et al. <ref type="bibr" target="#b4">[5]</ref> proposed a method to estimate the camera viewpoint using rigid structure-from-motion and then reconstruct object shapes by optimizing over visual hull proposals. Fouhey et al. <ref type="bibr" target="#b17">[18]</ref> demonstrated to learn their proposed primitives to infer 3D surface normals given a single image. Eigen et al. <ref type="bibr" target="#b12">[13]</ref> presented a method to estimate and find 3D depth relations from a single stereo image by using a multi-scale deep network.</p><p>With the help of ShapeNet <ref type="bibr" target="#b6">[7]</ref>, a richly-annotated and large-scale repository of 3D CAD models, there are several 3D reconstruction approaches presented in the recent few years. For instances, Huang et al. <ref type="bibr" target="#b21">[22]</ref> proposed a joint analysis method for shape reconstruction by estimating the camera pose, computing dense pixel-level correspondences between image patches, and finally creating a 3D model for each image by an optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Deep Learning Based Methods</head><p>Most recently, using deep learning methods to analyze and represent 3D objects is becoming a popular trend, inspired by the successes of these techniques in 2D images and 1D texts. Choy et al. <ref type="bibr" target="#b8">[9]</ref> proposed a 3D recurrent reconstruction neural network (3D-R2N2) to output a reconstruction of the object with a 3D occupancy grid format, which cannot well preserve the surface geometry of a 3D shape. In order to predict a nicer surface space, Fan et al. <ref type="bibr" target="#b13">[14]</ref> explored the generative networks for 3D geometry based on a point cloud representation. Kuryenkov et al. <ref type="bibr" target="#b29">[30]</ref> proposed a DeformNet to achieve smooth geometric deformations on point clouds for 3D shape reconstruction. However, it is well-known that a 3D point cloud may not be as efficient and effective in representing the underlying continuous 3D geometry as a 3D surface mesh. It needs some non-trivial post-processings to generate the valid surface meshes (to guarantee the manifold property).</p><p>Wang et al. <ref type="bibr" target="#b51">[52]</ref> adopted a graph-based convolutional neural network to produce the 3D geometry by progressively deforming an ellipsoid with leveraging perceptual features extracted from an input image. However, this method can only reconstruct a single genus-0 topology shape, since the initial shapes are all deformed from an ellipsoid. Another limitation is that their deformation is defined on the surface space with a linear transformation model, which is difficult for the network to compute high-fidelity large deformation to accurately capture the shape geometry and they need several regularization terms to control the shape smoothness and local consistency. Smith et al. <ref type="bibr" target="#b45">[46]</ref> extended Wang et al.'s work <ref type="bibr" target="#b51">[52]</ref> by using an adaptive face splitting strategy in order to better capture the local surface geometry, but it still has the problems of having non-manifold elements and topological constraint (by using a sphere as the initial shape). The above methods are based on surface deformation. However, one of the major limitations of surface deformation, whose deformation field is directly defined on the shape surface, is that its computational effort and numerical robustness are highly related to the complexity and quality of the surface tessellation <ref type="bibr" target="#b2">[3]</ref>. In the presence of the degenerate or poor quality triangles, the local transformations on these triangles are not well defined and thus lead to topological or non-manifold errors <ref type="bibr" target="#b51">[52]</ref>, as shown in Sec. 5. Even with quite some efforts for adding regularization terms, such as Laplacian regularization, edge length regularization, etc. <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b51">52]</ref>, it is still difficult to fully guarantee the deformation consistency in the local vertex neighborhood.</p><p>From the mathematical aspect, this problem can be avoided by space deformation. The key idea is to deform the ambient space (i.e., 3D volume space) enclosing the shapes, and thus implicitly deform the embedded surface shape (i.e., 2-manifold) <ref type="bibr" target="#b2">[3]</ref>. Compared with the surface-based deformation methods, space deformation approaches apply a trivariate deformation function to transform all the points of the original surface. One major advantage of the space deformation is that it does not depend on any particular surface representation, so that it can be used to deform all kinds of explicit surface representations, such as vertices of meshes or samples of point clouds <ref type="bibr" target="#b2">[3]</ref>. Classical free-form deformation (FFD) <ref type="bibr" target="#b41">[42]</ref> represents the space deformation by a trivariate tensor-product spline function. Pontes et al. <ref type="bibr" target="#b36">[37]</ref> proposed a learning framework (i.e., Image2Mesh) to reconstruct a single 3D object mesh from a 2D natural image by first deforming a selected template using the symmetric FFDs and then linearly combining a few more strongly related templates. However, their method predominantly relies on a complicated and pre-computed graph embedding of templates and their framework is not end-to-end trainable. Jack et al. <ref type="bibr" target="#b23">[24]</ref> proposed a method to learn FFDs for multiple templates to infer a 3D shape reconstruction from a single natural image with a plain background, but their framework is also limited to generate a single object, without considering multi-object scenario.</p><p>Besides that, all the aforementioned deep learning based methods for 3D shape reconstruction from a single image are not designed and applied to medical image reconstruction and visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Volume (Shape) from Single-View Image in Medicine</head><p>In medical image, 3D reconstruction is the process of computing the structure and tissue of real objects (not only the shape).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Volumetric Image Reconstruction Methods</head><p>Li et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> utilized a deformable image registration method to compute deformation vector fields (DVFs) for the reference of a lung motion model on graphics processing unit (GPU). Then, a principal component analysis (PCA) based lung motion model has been applied to reconstruct a volumetric image and locate 3D tumor from a single CBCT / X-ray projection. However, there are several limitations, such as a linear relationship between the image intensity of the computed and measured projection images may not be accurate. The framework settings are not fully automatical and practical for clinical use, such as some pre-processings for DVF computation are needed. The singleview reconstruction suffers from an ill-posed problem because only one angle data is used in the reconstruction. To alleviate this issue, Liu et al. <ref type="bibr" target="#b34">[35]</ref> tried a wavelet-based reconstruction approach to the acquired singe-view measurements, but the reconstruction quality is still not satisfactory for clinical applications. Recently, Henzler et al. <ref type="bibr" target="#b18">[19]</ref> proposed a convolutional encoder-decoder network to reconstruct a 3D volume from a 2D single-view cranial X-ray image. The direct coarse output is then improved to the higher resolution by a post fusion. The resulting 3D shape structure is still embedded in a 3D volume and the 3D shape can only be shown by the volume rendering with the manual-setting isosurface threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Shape Reconstruction Methods</head><p>There are few works on directly reconstructing the 3D shapes (meshes) from medical images (grayscale pixels), since it is a cross-modality problem, which is relatively challenging. The traditional solution is to reconstruct the 3D volumetric images from multiple 2D view images at first <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b46">47]</ref>, and then use image segmentation methods to extract the region of interest (ROI), such as organs or tumors; and finally generate the 3D shape meshes (i.e., isosurface) by using Marching Cubes algorithm <ref type="bibr" target="#b35">[36]</ref>. For instance, iso2mesh <ref type="bibr" target="#b14">[15]</ref> is an open-source toolbox for generating 3D surficial and volumetric meshes from binary and grayscale images, but it needs to undergo the tedious procedure due to the complicated substeps.</p><p>Some researchers investigated to fill the gap to directly build the 3D shape from a limited / sparse number of 2D medical images. Fleute et al. <ref type="bibr" target="#b16">[17]</ref> proposed to use a few X-ray images generated from a C-Arm and to build the 3D shape of the patient bones or organs by deforming a statistical 3D model to the contours segmented on the X-ray views. Tang et al. <ref type="bibr" target="#b49">[50]</ref> used a hybrid 3D atlas shape model to reconstruct or deformably register the surface of an object from two to four 2D X-ray projections of the object. Lamecker et al. <ref type="bibr" target="#b31">[32]</ref> presented a method to reconstruct 3D shapes from few digital X-ray images on the basis of 3D-statistical shape models; however, there are some empirical pre-processings needed, such as thickness of the shape model, silhouette extraction, etc. Sadowsky et al. <ref type="bibr" target="#b39">[40]</ref> presented a method for volume rendering of unstructured grids, which was applied in visualizing "2D-3D" deformable registration of anatomical models. Ehlke et al. <ref type="bibr" target="#b11">[12]</ref> proposed a novel GPU-based approach to render virtual X-ray projections of deformable tetrahedral meshes, and applied the method to improve the geometric reconstruction of 3D anatomy (e.g., pelvic bone) from few 2D X-ray images.</p><p>To our knowledge, there is no existing method to reconstruct the 3D mesh models from a single-view 2D medical image, which is a very challenging problem by using the traditional model-driven or statistical techniques. In this paper, we propose a deep learning based data-driven approach to solve this difficult but inspiring problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEPORGANNET</head><p>In this work, we propose an end-to-end deep neural network, named as DeepOrganNet, to generate 3D / 4D surface meshes of multiple organs from single-view medical image projections by learning the optimal deformations upon the best-selected mesh templates. This strategy not only prevents the poor quality of the reconstructed result with coarse voxelization or non-manifold surface mesh (widely existing in previous methods as discussed in Sec. 2.1.2), but also preserves fine and smooth surface details for 3D shape generation and visualization. In this section, we introduce main technique components of the DeepOrganNet model, including dataset generation, free-form deformation (FFD) on mesh, and organ reconstruction network with the loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Generation</head><p>3D object reconstruction is one of the most complicated tasks in computer vision / graphics and visualization fields, compared with object classification, segmentation, retrieval, etc., not to mention 3D object reconstruction from a single-view image. Such work usually needs a large-size dataset in support of the deep learning networks to learn the correct mapping from the 2D projection image to the corresponding 3D object. The current 3D shape reconstruction tasks mainly rely on some public large-size and well-established synthetic 3D shape dataset, such as ShapeNets <ref type="bibr" target="#b6">[7]</ref>, ModelNet10, and ModelNet40 <ref type="bibr" target="#b52">[53]</ref>. Additionally, the input image in most tasks is restricted to the 2D projection of a certain object under an identical lighting condition with a uniform background in order to filter out as much unrelated information as possible.</p><p>Challenges: Unlike such natural-image-to-3D-object tasks, the proposed organ reconstruction from a single X-ray image (e.g., 3D / 4D-CBCT projection) is more challenging due to following aspects. First, the dataset of 3D organ shapes, such as human lungs (in this paper, we use lung organs as illustrative examples), is very limited, and there exists neither established synthetic dataset nor available clinical dataset with a reasonable scale, which can be adopted in our task. Second, an X-ray image is essentially different from a 2D natural image, which contains obvious object profile and appearance (with the clear background in most previous works); instead the X-ray image contains the structures and details inside the object or occluded from the viewpoint (with the complicated background and noises). Third, the proposed framework does reconstruct multiple objects simultaneously, such as left and right lungs as an example. However, the previous existing approaches could work on reconstructing only one object. Generated Organ Meshes: Firstly, we address the limited dataset challenge on both 3D lung models and the corresponding 2D medical image projections. We propose a feasible strategy to generate a large number of synthetic 3D-CBCT projection images along with their corresponding 3D two-lung (left and right lungs) surface meshes with various geometric shapes by using a small amount of 3D / 4D phantom data. Given a 3D digital phantom (i.e., a volumetric image) I, we first apply the Snakes segmentation method <ref type="bibr" target="#b28">[29]</ref> to segment the 3D lung mask images and then extract the isosurface mesh S from the segmented lung mask image by Marching Cubes algorithm <ref type="bibr" target="#b35">[36]</ref>. After that, we employ a variety of shape deformations and spatial translations on S to get a new mesh S . For the organ shape deformation, we first manipulate the coordinates by multiplying a scaling ratio to globally stretch or compress the lung shape in S. The scaling ratios are either constant or gradually change. We then semi-automatically apply local distortions (e.g., dents / concaves, convexes, abnormal parts) on each lung shape using Blender software tool. Both the above global and local deformations are manipulated under the guidance of our collaborative doctors to resemble and cover the real lung shape variations. All the procedures are performed based on 3D / 4D phantoms within different respiration phases to capture the real lung breathing motions. For the organ spatial arrangement, we randomly disturb the distance between each left / right lung's bounding box center and origin within a reasonable range based on the original lung positions in the phantom to resemble various real lung shape cases, and the final new mesh is S . Generated Volumetric Images: Once we have the deformed surface mesh S along with the original 3D digital phantom image I, the deformed 3D digital phantom image I can be computed. Then, we can generate the corresponding single-view (e.g., front-view) 3D-CBCT projections. It is noted that we can obtain the deformation vector field (DVF) ΔDV of the mesh vertices from surface S to S as follows:</p><formula xml:id="formula_1">ΔDV = V S − VS,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">V S , VS ∈ R N ×3</formula><p>are the positions of mesh vertices in S and S and N is the number of mesh vertices. As a result, for each voxel αi in I, we can estimate its deformation vector Δdα i by incorporating the DVF of its k-nearest neighboring vertices on mesh S. Such process can be written as follows:</p><formula xml:id="formula_3">Δdα i = Hα i Jα i ΔDV ,<label>(2)</label></formula><p>where Jα i ∈ R K×N is an one-hot encoding matrix for the indices of the K neighbors (in this work, K is 4). Hα i ∈ R K is a weight vector in which:</p><formula xml:id="formula_4">h α i (κ) = 1 K if ||vκ − αi|| ≤ ψ, h α i (κ) = 1 K||vκ−α i || otherwise ,<label>(3)</label></formula><p>where κ ∈ 1, ..., K and ψ is the norm of the maximum DVF of mesh vertex in ΔDV . We then can calculate the resulting I with all voxels' DVF using the reconstruction method with the deformation field map <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b53">54]</ref>. Generated 2D Projection Images: Finally, we apply the Siddon ray tracing algorithm <ref type="bibr" target="#b44">[45]</ref> to generate the desired 2D front-view 3D / 4D-CBCT images of S by tracing the path of light through voxels in the 3D volumetric image I . To better simulate the realistic raw target CBCT projections from the digital phantom data and test the sensitivity of our method to the realistic complications, after the noise-free ray line integrals are computed according to the above ray tracing method, the noisy signal at each pixel on the CBCT projections is generated based on the noise model with Poisson and normal distributions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>In this way, we can generate as many 3D lung meshes and their corresponding 3D / 4D-CBCT medical projections as possible, which is quite crucial for our proposed data-driven deep learning framework. Accordingly, we can cover various types of abnormalities caused by lesions, injuries, or singularities through applying different kinds of global and local deformations in the dataset generation and intentionally increase the ratio of such abnormalities in the dataset to provide our network with adequate prior knowledge to deal with potential unusual cases. <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrates the flowchart of the dataset generation. More detailed configuration is in the implementation section (Sec. 4). A high-quality 3D mesh object usually requires dense vertices to represent fine details and thus it is computationally unfriendly, if one intends to deform it pointwisely. Instead, FFD <ref type="bibr" target="#b41">[42]</ref> deforms the 3D mesh object through a small amount of control points. FFD introduces a 3D control point grid of size (l +1)× (m +1)× (n +1), which encloses the target 3D mesh and performs the deformation in a trivariate tensor-product spline function, where the position of each vertex on the target mesh can be calculated:</p><formula xml:id="formula_5">v(s, t, u) = l i=0 m j=0 n k=0 B i,l (s)Bj,m(t)B k,n (u)p i,j,k ,<label>(4)</label></formula><p>where v(s, t, u) is an arbitrary mesh vertex coordinate in the coordinate system defined by three orthogonal axes s, t, and</p><formula xml:id="formula_6">u. Bp,q(x) = p q (1 − x) p−q x q</formula><p>is a binomial function called Bernstein polynomial of degree q, and p i,j,k is the control point at the node (i, j, k) on the grid. From Eq. <ref type="formula" target="#formula_5">4</ref>, we notice that the vertex placement of the target mesh is essentially a weighted sum of the control points. Denote V ∈ R N ×3 as the matrix form of vertices on mesh Ω, then the mesh vertex representation can be converted:</p><formula xml:id="formula_7">V = BP,<label>(5)</label></formula><p>where B ∈ R N ×Ψ is the matrix form of the trivariate Bernstein tensor for all N vertices, P ∈ R Ψ×3 is the matrix form of control point coordinates, and Ψ is the number of control points, i.e., (l + 1) × (m + 1) × (n + 1). Suppose given the displacement of these control points P, the corresponding deformed mesh Ω = (V , F) in which V is: V = B(P + P). (6) As shown in <ref type="figure">Fig. 2</ref>, in this way, the objective of the proposed deep learning network (DeepOrganNet) is to infer a P such that the resulting mesh S best matches the shape of 3D lung organ surface according to the input 2D X-ray image. FFD <ref type="figure">Fig. 2</ref>: FFD process on a 3D lung shape: it is deformed according to the displacement of the control points on a 4 × 4 × 4 grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Organ Reconstruction Network 3.3.1 Network Architecture Design</head><p>The pipeline of DeepOrganNet framework consists of three functional components: feature encoder block, deformation block, and spatial arrangement block. The overall architecture is shown in <ref type="figure">Fig. 3</ref>.</p><p>Given a single-view 3D / 4D-CBCT projection image, our network first encodes it into a latent descriptor, which contains effective information for different purposes in the reconstruction stage. Due to the dataset availability and the specific training objective, the image encoder should be lightweight to alleviate the overfitting risk but still quite efficient for the reconstruction task. Jack et al. <ref type="bibr" target="#b23">[24]</ref> has justified the adequate ability for MobileNets in the intra-class deformation, so we apply and fine-tune the pre-trained MobileNets <ref type="bibr" target="#b20">[21]</ref> to encode the input medical image. MobileNets are compact and Inception style <ref type="bibr" target="#b48">[49]</ref> network, which factorizes a standard convolution into a depthwise convolution and a 1 × 1 pointwise convolution. In addition, MobileNets introduce a width multiplier, which can reduce the width for each layer by a constant ratio and thus give us more freedom to adjust our network to best fit a relatively small amount of data in medical image scenarios. In our work, we only adopt the convolution layers in MobileNets and add a 1 × 1 convolutional layer after that to generate the image descriptor with a reasonable dimension. The detailed network configuration is shown in <ref type="figure">Fig. 3</ref>. Through our extensive experiments in Sec. 5, we find that the lightweight MobileNets is sufficient and robust to extract the informative features from a single-view medical image with complicated background and noises. Our input image is quite different from the one in most of the current natural-image-to-3D-object tasks, since their inputs are 2D images with clear object profile and boundary, which are generated based on the light illumination and reflection; however, our X-ray images are generated based on ray tracing technique to compute the attenuation of the energy absorption. One of the advantages in the X-ray-image-to-3D-object task is that the input X-ray images can include some shape information, which is occluded by the natural 2D images. It can make the viewer to see through the front shape surface and thus alleviate the occlusions. We will show examples in the experiment section (Sec. 5).</p><p>Furthermore, another advantage of our DeepOrganNet, compared with current natural-image-to-3D-object tasks in which the prediction is only designed for a single object, is that our task is essentially designed for reconstruction of multiple separate objects, along with a spatial arrangement between each other. As a result, we split different branches from the whole image descriptor to reconstruct different organs (i.e., different disconnected components), such as the left and right lungs independently, instead of learning all the objects together. Based on our observations and explorations, this scheme is more effective for the network to learn discriminative features from different objects than the one to learn everything together (such as using one branch scheme with a set of different two-lung templates). Each branch is responsible for the corresponding single lung generation by deforming differently-geometric templates. In this work, we use left and right lung organs as the testbed for our study, but the proposed framework can be easily extended to multi-organ (more than two organs) scenarios. Suppose we select n l left lung templates and nr right lung templates for corresponding branches, left lung branch learns a set of the deformation parameters for all of the n l templates { PL i } n l i=1 according to the left lung shapes from the input 2D image simultaneously, where PL i ∈ R Ψ×3 is the deformation parameter (i.e., the control points' displacements) for a single template Li = (VL i , FL i ). The deformed template L i can be achieved by:</p><formula xml:id="formula_8">V L i = BL i (PL i + PL i ) ,<label>(7)</label></formula><formula xml:id="formula_9">F L i = FL i ,<label>(8)</label></formula><p>where BL i and PL i are the pre-computed transformation matrix and control point position matrix for the template Li. The mesh connectivity does not change during the deformation as shown in Eq. <ref type="bibr" target="#b7">(8)</ref>. In addition, the left lung branch also learns a set of selection weights {wL i } n l i=1 for each left lung template, which are determined along with the template deformations as shown in Eq. (13) and Eq. <ref type="bibr" target="#b14">(15)</ref>. The final left lung prediction is then selected by:</p><formula xml:id="formula_10">L pred = L imax ,<label>(9)</label></formula><p>where imax = arg max  <ref type="figure">Fig. 3</ref>: The architecture of our DeepOrganNet. The DeepOrganNet first encodes the input image into a descriptor using MobileNets (without fully-connected layers) followed by a 1 × 1 convolution layer (dimension reduction). DW refers to the depthwise separable convolution block (two separable convolutional layers, functionally equivalent to a standard convolutional layer) and the numbers are output channel sizes (i.e., widths) of each layer / block. Every template in either left or right lung branch learns its own selection weight w and deformation parameters P through an independent fully-connected layer with dimension 193, including 192 for P and 1 for w. The deformed templates with the highest selection weights (e.g., templates L1 and R1 are selected in this example) in both branches are arranged according to the translation vector T l and Tr learned from another fully-connected layer to generate the final combined multi-organ meshes.</p><formula xml:id="formula_11">{wL i } n l i=1 .</formula><p>lung prediction R pred . By splitting two branches to extract the corresponding effective information from the image descriptor, the learning objectives become more specific and clearer. At this stage, the network only focuses on how to deform the templates with respect to the lung geometry from the input image. The spatial information, such as the gap / distance and the relative positions between left and right lungs, are reserved for the next stage.</p><p>As long as we have L pred and R pred ready, the next step is to combine them together so as to generate final left and right lung meshes with the correct relative spatial arrangement according to the input image. In order to achieve this, we learn two translation vectors T l ,</p><p>Tr from the image descriptor. Then the entire prediction of the new organ meshes Ω = (V Ω , F Ω ) of both lungs is:</p><formula xml:id="formula_12">V Ω = V L pred + T l , V R pred + Tr ,<label>(10)</label></formula><formula xml:id="formula_13">F Ω = F L pred , F R pred .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Loss Functions</head><p>In this subsection, we define three kinds of losses in our network not only to constrain the output shape results but also to optimize the training process. Deformation Loss: To ensure the deformation accuracy, we choose Chamfer loss <ref type="bibr" target="#b13">[14]</ref> to regulate the accuracy of the vertex locations on a single lung prediction. The Chamfer loss is defined as:</p><formula xml:id="formula_14">C(P, Q) = p∈P min q∈Q p − q 2 2 + q∈Q min p∈P p − q 2 2 ,<label>(12)</label></formula><p>where p and q are points from two mesh vertex sets P and Q. Essentially, for each point in P or Q, the Chamfer loss finds the nearest vertex in the other point set and sums up all pair-wise distances. In our framework, we apply weighted Chamfer loss for both lung branches as:</p><formula xml:id="formula_15">L def orm = n l i=1 wL i C(V L pred , VL gt )+ nr i=1 wR i C(V R pred , VR gt ),<label>(13)</label></formula><p>where VL gt and VR gt are the ground truth for left and right lung meshes (aligned at the origin), respectively. In this way, the proposed network is enforced to give the highest weight to the template, which can be deformed best to match the ground truth. Now, we can select the best template among all potential candidates in the datasets for predicting each organ individually and automatically. Translation Loss: The second loss term Ltrans is intended to learn the translation vectors T l and Tr. It is defined as:</p><formula xml:id="formula_16">Ttrans = T l − ctr l 2 2 + Tr − ctrr 2 2 ,<label>(14)</label></formula><p>where ctr l and ctrr are the ground truth translation vectors (i.e., two global translation vectors between the origin and the bounding box centers of left and right lungs in all ground truth meshes). Regularization Loss: Our network deforms all templates according to input 2D images. Sometimes, the reconstruction results are achieved by tremendous deformations from a template that is not the closest one in the template pool. We introduce a weight regularization term similar to the one in <ref type="bibr" target="#b23">[24]</ref> to encourage the network to give higher weight to the template closer to the ground truth. In this way, the overall performance of the network becomes more rational and intuitive:</p><formula xml:id="formula_17">Lw = n l i=1 wL i PL i 2 2 + nr i=1 wR i PR i 2 2 ,<label>(15)</label></formula><p>where this loss is defined on the deformations of the control points. The total loss is a weighted sum of all the above three kinds of losses as follows:</p><formula xml:id="formula_18">L total = L def orm + λ1Ltrans + λ2Lw,<label>(16)</label></formula><p>where λ1 = 50 and λ2 = 1 in experimental settings, which are determined based on the corresponding order of the magnitude and balanced by the optimal network performance via our extensive experiments.</p><p>It is worth mentioning that through the strategy of integrating the deformation weights in the loss function, the proposed DeepOrganNet can automatically select the proper templates so that the network has a good prior information to start with for each organ. The risk of nonmanifold issue in the reconstructed shape meshes, such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>, is dramatically alleviated. In addition, FFD deforms the templates with a small amount of control points compared with vertex-wise deformation, e.g., 64 vs 10K deformation parameters, which is quite efficient.</p><p>Furthermore, FFD can realize the high-order interpolation for the deformation computations, so that the mesh surface smoothness is well maintained and no additional loss term as in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b51">52]</ref> is required beyond the Chamfer loss (fidelity term) to yield a good inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION DETAILS</head><p>In this section, we introduce our dataset preparation and network training details followed by evaluation metrics which we use to measure the experimental results.</p><p>Dataset Preparation: In order to evaluate the proposed DeepOr-ganNet, we use following phantoms, patient studies in lung imaging and motion datasets. There are two 3D / 4D digital phantoms, i.e., a dynamic NURBS-based cardiac-torso (4D NCAT) phantom (4D images and motions are provided) and 4D extended cardiac-torso (XCAT) <ref type="bibr" target="#b42">[43]</ref>, being used as basis models to generate a reasonable number of 3D lung surface meshes and corresponding 3D / 4D-CBCT projections. They both have 10 breathing phases in 3D volumetric images (e.g., 256 × 256 × 150 with voxel size of 1mm × 1mm × 1mm). We generate 542 pairs of (left and right) lungs with various shapes and different spatial arrangements together with their corresponding 2D single front-view CBCT projections (see Sec. 3.1). We use the first five phases of NCAT and XCAT of 4D-CBCTs to build our training dataset and leave the rest for testing evaluation purpose. All two-lung models are normalized along the sagittal axis and translated to the origin. The bounding box size of all these models is within 1.35 × 1.25 × 1 along the transverse, coronal, and sagittal axes. We then compute the bounding box centers of left and right lungs in the two-lung meshes and translate them to the origin to form the ground truth for the two deformation branches. The input 2D front-view CBCT projections are grayscale images of size 192 × 256 with pixel size of 1mm × 1mm.</p><p>In the experiments, we randomly split the dataset by 446 pairs for training and 96 pairs for testing, respectively. We also test our model performance on deformable image registration (DIR)-Lab (ten lung cancer patient 4D-CT datasets with ten respiration phases each) <ref type="bibr" target="#b5">[6]</ref>, Japanese Society of Radiological Technology (JSRT) database (247 chest X-ray images) <ref type="bibr" target="#b43">[44]</ref> for lung shape reconstruction.</p><p>Training Details: Our task is to generate left and right lung shapes from an image with noisy background and limited dataset, in order to reach a good balance between the prediction accuracy and the network overfitting risk. We set the MobileNets <ref type="bibr" target="#b20">[21]</ref> (pre-trained on ImageNet dataset <ref type="bibr" target="#b10">[11]</ref>) width multiplier to be 0.25 and the width (i.e., channel number) for each layer is shown in <ref type="figure">Fig. 3</ref>. For each lung branch in the network, we have two single lung templates from XCAT and NCAT, respectively. The 3D control point grids for every template from two branches are set to be 4 × 4 × 4, which yields to 64 control points per template. We train the network for 65K steps using Adam optimizer with learning rate as 1 × 10 −3 . The batch size is 32. The total training time is 4 hours on a single Nvidia GTX 1080 GPU with 8 GB GDDR5X.</p><p>Evaluation Metrics: Following the standard 3D shape reconstruction evaluation method, we use five different kinds of numeric metrics to evaluate the performance of our model and compare it with the existing state-of-the-art techniques.</p><p>Chamfer distance (CD) is applied in both training and testing processes. The formal expression is shown in Eq. <ref type="bibr" target="#b11">(12)</ref>. It measures bidirectional overall vertex-wise distance between two meshes.</p><p>Earth mover's distance (EMD) <ref type="bibr" target="#b38">[39]</ref> is designed to compute the minimal sum of distances over all possible one-to-one mappings between points in P and points in Q, where P and Q are two point sets of the same size. The EMD can be written as:</p><formula xml:id="formula_19">EM D(P, Q) = min φ:P→Q p∈P p − φ(p) 2 ,<label>(17)</label></formula><p>where φ is a bijection from P to Q.</p><p>Hausdorff distance (HD) is adopted to measure the largest inconsistency between the reconstruction result and the ground truth. A lot of previous 3D reconstruction works did not list it as their evaluation metric because they mainly focused on point cloud reconstruction, which is insensitive to small amount of outliers. In geometric modeling and computer graphics, Hausdorff distance is a widely-used indicator to check the reconstructed mesh quality since even small amount of outliers may undermine the mesh surface consistency and quality, especially for visualization and rendering. In our experiments, we measure the Hausdorff distance between prediction and ground truth with respect to both point clouds and surface meshes <ref type="bibr" target="#b9">[10]</ref>. Suppose p and q are the points sampled from point clouds (or surface meshes) of P and Q accordingly, the HD in terms of point clouds (or surface meshes) can be written as:</p><formula xml:id="formula_20">HD(P, Q) = max max p∈P min q∈Q p − q 2 , max q∈Q min p∈P q − p 2 .<label>(18)</label></formula><p>F-score <ref type="bibr" target="#b51">[52]</ref> is used as the harmonic mean of precision and recall regarding how many points in prediction or ground truth can find the nearest neighbor from the other within a threshold ( ). We set = 0.001 in our experiments.</p><p>Intersection over union (IoU) is used to examine the volumetric similarity between the voxelized prediction and ground truth. The IoU is defined as:</p><formula xml:id="formula_21">IoU (P, Q) = |P ∩ Q| |P ∪ Q| ,<label>(19)</label></formula><p>where P and Q are the voxelized 3D models. Among the above five metrics, for CD, EMD and HD, the smaller the better; while for F-score and IoU, the larger the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct extensive experiments of our model on the 3D and 4D synthetic data as well as real patient data. The results are qualitatively and quantitatively compared with several state-of-the-art in deep learning based 3D shape generation (from a single-view image) methods and the traditional reconstruction method. It is noted that for comparison experiments, best results in tables are shown in bold font. <ref type="figure" target="#fig_1">Fig. 4</ref> shows the reconstruction results of 3D lungs with different shapes based on the synthetic data. Our network is capable of dealing with drastic variations (even though the real-world medical scenarios are far less challenging). For each input image, our network is able to pick the template which most resembles the ground truth model within the corresponding branch, and predict the accurate spatial arrangement between left and right lungs to generate the final high-fidelity 3D lung shape pair. The reconstruction error (HD on mesh) is mapped into a unified colormap range and it shows that the reconstruction results are pretty good qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">3D Lung Shape Reconstruction from Synthetic Images</head><p>Since our network learns the deformation parameters which are essentially applied on the control points instead of directly on the template model surface, one can generate the final 3D mesh models with arbitrary resolutions in real-time (e.g., 1K vertices: 20 ms, 2.5K vertices: 21 ms, 5K vertices: 22 ms, 10K vertices: 25 ms) according to the users' needs without re-training the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Deep Learning Based Methods</head><p>Comparison with Pixel2Mesh <ref type="bibr" target="#b51">[52]</ref>: Intuitively, according to the Pix-el2Mesh (P2M) experiment setting, we first use their network to predict a single lung from a single-view input image. We train P2M on our synthetic dataset following their training details. The quantitative evaluation result is shown in Tab. 1. We fairly set our output mesh vertex number to be the same as the output of P2M network (i.e., 2466). The CD and EMD are both computed between the uniformly sampled 1024 points from the prediction and ground truth such that the comparison can be made not only between the single lung reconstruction from P2M and our network, but also between the lung pair reconstruction (see <ref type="table">Tab.</ref> 2) and single lung reconstruction of our network. From Tab. 1, our network outperforms P2M on all metrics.</p><p>We also present the qualitative comparison results in <ref type="figure" target="#fig_2">Fig. 5</ref> and <ref type="figure" target="#fig_3">Fig. 6</ref>. Both (P2M and our) networks yield predictions of smooth surface but our model performs better in well preserving the mesh surface geometry without non-manifold issue. The reason may be that the input single-view 3D-CBCT projection image is essentially different from a 2D natural input image. P2M has no mechanism to  deal with the ambiguity caused by such ill-posed problem like PSGN. However, our network have more specific templates to start with so as to rule out some uncertainties or local minima, while the initial ellipsoid template in P2M network is too general for this task; and how to modify their network to fit for an initial lung shape is beyond the scope of this work. We also attempt to infer two lungs together with P2M network by replacing the single ellipsoid with a pair of two ellipsoids. However, the network tends to fuse two separate lungs as a single object. The original weights for each regularization terms need to be further determined to reach a good performance. It seems to be non-trivial to extend P2M framework to multi-object reconstruction scenario.</p><p>Comparison with Point Set Generation Network <ref type="bibr" target="#b13">[14]</ref>: We use our synthetic dataset (i.e., 542 pairs of left and right lungs) with the same training and testing splits to train the Point Set Generation Network (PSGN) <ref type="bibr" target="#b13">[14]</ref> as our model (discussed in Sec. 4) and generate meshes from corresponding prediction point clouds using Ball Pivoting Algorithm <ref type="bibr" target="#b1">[2]</ref>. Since our model can generate meshes with arbitrary densities, we set the output mesh vertex number as 1024 to fairly compare HD with the mesh generated from PSGN predictions. The CD and EMD are both computed between the prediction and uniformly sampled 1024 points from the ground truth (denser isosurface meshes).</p><p>Tab. 2 shows the quantitative evaluation of six different metrics and   In terms of point-wise HD and F-score( ) evaluation, PSGN tends to get slightly better numeric results since the PSGN generates points independently, thus it has more degrees of freedom. However, the EMD of PSGN is much larger since the point cloud inference from PSGN is irregularly distributed, and sometimes the points from one lung are much denser than those from the other. Although PSGN has comparable performance in most of the point-wise evaluation metrics, it does not guarantee a high-quality 3D surface mesh. The mesh-based HD is nearly 50% higher than ours since there are a lot of meshing failures (e.g., self-intersecting triangles, holes, nonmanifold triangles, etc.) and bumpy details in the generated surface meshes. In addition, PSGN learns a lung pair as a single object, when the gap between two lungs is small, the (post-processing) meshing algorithm is difficult to separate them.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with Traditional Reconstruction Method</head><p>Before deep learning methods are applied to 3D reconstruction area, the most common way to acquire 3D organ models from a patient is to first reconstruct 3D-CBCT volumetric image from multiple 2D projections from different views and then segment the organ models from the reconstructed volumetric image. The segmentation quality heavily depends on the number of projections. Very few views severely undermines the reconstructed 3D-CBCT image accuracy, while increasing the views impairs patient health due to a higher imaging dose as well as consumes a longer computational time. Our network offers satisfiable 3D lung shape models with only a single-view 3D-CBCT projection. <ref type="table" target="#tab_3">Tab. 3</ref> shows that the traditional Simultaneous Algebraic Reconstruction Techniques (SART) <ref type="bibr" target="#b0">[1]</ref> requires at least 10-view projections to reconstruct the 3D lung mesh model and up to 50-view projections to reconstruct the 3D-CBCT volumetric image so as to segment the clean, smooth, and complete 3D lung models to reach the comparable result as ours. <ref type="figure">Fig. 8</ref> shows the qualitative comparison between two methods. It is worth mentioning that in clinical studies (or during the therapy), it is common to use hundreds of projections to reconstruct a high-quality 3D volumetric image and then process a good-quality 3D organ model.  </p><formula xml:id="formula_22">Method CD EMD F-score ( / 1.5 ) IoU HD (Mesh) 1-view N/A N/A N/A N/A N/A 5-view N/A N/A N/A N/A N/A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Applications and User Study on Patient Datasets</head><p>To further evaluate the accuracy and usability of the proposed method, our DeepOrganNet has been evaluated in the following studies by some domain experts, including our collaborative radiation oncologists and physicians. The efficiency and accuracy of our method demonstrate its capabilities to explicitly track, reconstruct, and visualize 3D / 4D organ shapes on the fly during the dynamic procedure and therefore it can be employed in the real-time image guided radiation therapy (IGRT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">3D Lung Shape Reconstruction from Patient Images</head><p>We first use ten cases of 4D-CTs from DIR-LAB datasets to evaluate the robustness of our method in real applications. For each case, we select the phase-0 of 4D-CTs to compute the front-view CBCT projection using the method in Sec. 3.1. All the generated front-view projections  <ref type="figure">Fig. 8</ref>: Qualitative comparison between the traditional SART and our method. The reconstructed 3D-CBCT images by SART from one and five views are unable to be used to segment lungs. SART requires CBCT projections from at least 50 views to reconstruct a good-quality 3D volumetric image such that the corresponding segmented lung model is comparable to our result. Failure parts (e.g., wrong connectivities and not-good shape preserving parts) of SART-based meshes are red-cycled.</p><p>are histogram-equalized. We test our network directly on images of all the cases without any fine-tuning. We also further test our model on some single front-view X-ray images from JSRT database <ref type="bibr" target="#b43">[44]</ref> and some ill-positioned single-view CBCT projections from real lung cancer patient datasets. We can see that our network is capable of describing the shape geometric property and providing a reasonable spatial arrangement in real case even though the images appear to be different from synthetic inputs. <ref type="figure">Fig. 9</ref> shows qualitative visualization results of the above datasets, which are examined by domain experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">4D Lung Shape Reconstruction</head><p>Instead of inferring the different 3D lung shapes, our network shows potential capability to track and visualize lung shapes along with the <ref type="figure">Fig. 9</ref>: Top: qualitative visualization results of 3D lung shape reconstruction from single-view phase-0 projections of five cases in DIR-LAB dataset. Bottom: qualitative visualization results of 3D lung shape reconstruction from single-view real X-ray images in JSRT database and real 3D-CBCT patient datasets. These sample results are picked from the challenging cases with large variations of the lung shapes. dynamic process of breathing. It is extremely important for IGRT procedure to understand the anatomical changes and pinpoint the location of the diseased regions on the fly. By sending a series of 2D front-view 4D-CBCT projections with different phases, our network is capable of capturing the minor changes between phases to describe the breathing tendency and maintaining the shape consistency simultaneously. It is interesting to discover that even some occluded deformations (in the natural images) in the diaphragm areas (bottom part of the lungs) can be extracted and reconstructed from the input single-view X-ray or 4D-CBCT projections. To our knowledge, this is the first time that a single-view reconstruction method can capture that. <ref type="figure" target="#fig_0">Fig. 10</ref> and <ref type="figure" target="#fig_0">Fig. 11</ref> show three expiration phases of a phantom case and a real case in DIR-LAB dataset. The colormaps represent the deformation magnitudes during the breathing. The solid surface meshes and wireframe meshes are used to visualize the front-view and occluded (diaphragm) deformations, correspondingly. Furthermore, the proposed method only takes about 22 milliseconds to generate 4D lung meshes with 5K vertices at each phase, which has great potential to be used in an on-the-fly targeting system on dynamic scenes in IGRT; however, there is no current method, which can make it on-the-fly.</p><p>Since our proposed method outperforms the current methods, an official clinical trial is under arrangement with our collaborative hospital. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we have proposed the DeepOrganNet, a deep neural network, to generate and visualize high-fidelity 3D / 4D organ shape geometry from single-view medical images in real time. DeepOrganNet has three major components, i.e., feature encoder block, independent deformation block, and spatial arrangement (translation) block. By using the multi-organ template selection and the smooth FFD strategies in the proposed framework, our method can generate high-quality manifold meshing models, which outperforms the previous deep learning methods as well as the traditional method from the single-view image reconstruction. In medical practice, this work can be used as the key functions for real-time IGRT in order to accurately visualize the patients' organ shapes on the fly, significantly improve the procedure time for patients and doctors, and dramatically reduce the imaging dose during the treatment. Some further interactive techniques based on DeepOrganNet will be developed in collaboration with domain experts. Discussion and Future Work: In the current framework, the lightweight MobileNets are computational efficient but limit the power of feature extraction in the encoder block. In the future, we will explore some more powerful deep neural networks for the encoder part and collect more 4D lung cancer patient datasets to improve the diversity and scalability of the training and testing for our DeepOrganNet. Although the proposed DeepOrganNet aims to reconstruct multiple 3D organs simultaneously, our current work does only implement for left and right lung organs as an example for justifying the feasibility and extendability of the proposed method. We will extend the framework into more organ reconstructions, such as heart, liver, pancreas, etc., in order to build a real fully DeepOrganNet system at a complicated 3D / 4D scene-level reconstruction. As for 4D scenarios, we have reconstructed each phase independently in the current system, and we will consider to use recurrent neural network and attention-based models to construct a 4D dynamic organ shape reconstruction deep neural network. It is worth mentioning that the quality of the reconstructed shapes can be further improved by including 2D-view projections from more viewpoints as the input to alleviate shape over-/ under-estimation; we will accordingly explore how to balance the computational time (imaging dose) and reconstructed accuracy in the clinical study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The flowchart of dataset generation.3.2 Free-Form Deformation (FFD) on MeshA 3D template mesh Ω = (V, F) consists of a set of N verticesV = {v1, v2, ..., vN } and a set of M faces F = {f1, f2, ..., fM }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative reconstruction and visualization results of some lung shapes with drastic variations. The reconstruction error (HD on mesh) is mapped into a unified colormap range (hotter colors indicate larger errors and colder colors indicate smaller errors) and the mesh resolution increases from top to bottom (e.g., 1K, 2.5K, 5K, 10K vertices).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative comparison between P2M and our method on left lung model. Our results generate meshes with no non-manifold issue, while the results from P2M have self-intersections (highlighted in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative comparison between P2M and our method on right lung model. Our results generate meshes with no non-manifold issue, while the results from P2M have self-intersections (highlighted in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7</head><label>7</label><figDesc>provides the qualitative comparison. Our network outperforms PSGN in most metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative comparison between PSGN and ours. Both point clouds and solid surface meshes are given. The failure parts (e.g., self-intersecting triangles, holes, non-manifold triangles, etc.) of PSGN meshes are red-cycled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 Fig. 10 :</head><label>1010</label><figDesc>Three expiration phases of 4D NCAT phantom model. Maximal deformation can be traced according to the red dashed lines across the input 2D images, and the corresponding deformations on the reconstructed 3D mesh models are mapped into a unified colormap range (hotter colors indicate larger deformations). The solid surface and wireframe meshes show the front-view and occluded (diaphragm) deformations, respectively. The deformations of Phases 8 and 10 are computed based on Phase 6 as the reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 Fig. 11 :</head><label>211</label><figDesc>Three expiration phases of Case 8 in 4D-CT DIR-LAB dataset. Maximal deformation can be traced according to the red dashed line across the input 2D images, and the corresponding deformations on the reconstructed 3D mesh models are mapped into a unified colormap range (hotter colors indicate larger deformations). The solid surface and wireframe meshes show the front-view and occluded (diaphragm) deformations, respectively. The deformations of Phases 1 and 2 are computed based on Phase 0 as the reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Right Lung Branch Left Lung Branch Feature Encoder Spatial Arrangement Deformation Branches</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>193 FC</cell><cell>...</cell></row><row><cell></cell><cell>Conv</cell><cell>DW</cell><cell>DW DW</cell><cell>DW</cell><cell>DW</cell><cell></cell><cell></cell><cell></cell><cell>FC</cell><cell>Block</cell><cell>FC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DW</cell><cell>5 x DW</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>256 DW</cell><cell>DW 256</cell><cell>Pool 256</cell><cell>Conv 64</cell><cell>Descriptor</cell><cell>193 FC</cell><cell>Translation Block</cell></row><row><cell></cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell></cell><cell></cell><cell>512</cell><cell>Block</cell><cell>6</cell></row><row><cell>8</cell><cell>1 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>193</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC</cell><cell>...</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>193</cell></row></table><note>Similarly, right lung branch ap- plies the same procedure as the left lung branch to obtain the final right</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison between P2M and our method on our synthetic dataset.</figDesc><table><row><cell>Method</cell><cell>CD</cell><cell>EMD</cell><cell>F-score ( / 1.5 )</cell><cell>IoU</cell><cell>HD (Mesh)</cell></row><row><cell>P2M (Left Lung)</cell><cell cols="2">2.4609 76.2620</cell><cell>0.5983 / 0.7799</cell><cell>0.7190</cell><cell>0.1300</cell></row><row><cell>Ours (Left Lung)</cell><cell cols="2">1.7018 57.0856</cell><cell>0.7293 / 0.8910</cell><cell>0.8352</cell><cell>0.0672</cell></row><row><cell cols="3">P2M (Right Lung) 2.3399 69.7205</cell><cell>0.6111 / 0.8014</cell><cell>0.7661</cell><cell>0.1022</cell></row><row><cell cols="3">Ours (Right Lung) 1.7300 59.9497</cell><cell>0.7293 / 0.8892</cell><cell>0.8423</cell><cell>0.0786</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison between PSGN and our method on our synthetic dataset.</figDesc><table><row><cell>Method CD</cell><cell cols="4">EMD F-score ( / 1.5 ) HD (Point) IoU HD (Mesh)</cell></row><row><cell cols="2">PSGN 3.0122 186.8821 0.4384 / 0.6377</cell><cell>0.0960</cell><cell>0.8002</cell><cell>0.1491</cell></row><row><cell cols="2">Ours 2.8955 70.7083 0.4375 / 0.6650</cell><cell>0.0980</cell><cell>0.8148</cell><cell>0.1000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison between SART (with different numbers of views) and our method.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the reviewers for their valuable comments. This work was partially supported by the NSF under Grant Numbers IIS-1816511, CNS-1647200, OAC-1657364, OAC-1845962, OAC-1910469, the Wayne State University Subaward 4207299A of CNS-1821962, NIH 1R56AG060822-01A1, NIH 1R44HL145826-01A1, and ZJNSF LZ16F020002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous algebraic reconstruction technique (SART): a superior implementation of the ART algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasonic Imaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="94" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ball-pivoting algorithm for surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mittleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="359" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Polygon mesh processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lévy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AK Peters/CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reconstruction of a cone-beam CT image via forward iterative projection matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Docef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6212" to="6220" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lifting object detection datasets into 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1342" to="1355" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A framework for evaluation of deformable image registration spatial accuracy using large landmark point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mcphail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in Medicine &amp; Biology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1849" to="1870" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: an informationrich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prior image constrained compressed sensing (PICCS): a method to accurately reconstruct dynamic CT images from highly undersampled projection data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="660" to="663" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multi-view 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Metro: measuring error on simplified surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cignoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rocchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scopigno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast generation of virtual X-ray images for reconstruction of 3D anatomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ehlke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lamecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2673" to="2682" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tetrahedral mesh generation from volumetric binary and grayscale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1142" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Practical cone-beam algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feldkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kress</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A-Optics Image Science and Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="612" to="619" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonrigid 3-D / 2-D registration of images using statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fleute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavallée</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data-driven 3D primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3392" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-image tomography: 3D volumes from 2D cranial X-rays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rasche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="377" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single-view reconstruction via joint analysis of image and shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">87</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Patient dose from kilovoltage cone beam computed tomography imaging in radiation therapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Purdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Norrlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Siewerdsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jaffray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1573" to="1582" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Part 1</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning free-form deformations for 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Emotion information visualization through learning of 3D morphable face model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="535" to="548" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust 3D face modeling and reconstruction from frontal and side images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Aided Geometric Design</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Radiation dose from cone beam computed tomography for image-guided radiation therapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Radiation Oncology* Biology* Physics</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="272" to="279" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Category-specific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1966" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DeformNet: free-form deformation network for 3D shape reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="858" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reduction of noise-induced streak artifacts in X-ray computed tomography through spline-based penalized-likelihood sinogram smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Billmire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="111" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Atlas-based 3D-shape reconstruction from X-ray images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lamecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wenckebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Pattern Recognition</title>
		<meeting>IEEE International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="371" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Realtime volumetric image reconstruction and 3D tumor localization based on a single X-ray projection image for lung cancer radiotherapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Folkerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2822" to="2826" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Part 1</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Singleprojection based volumetric image reconstruction and 3D tumor localization in real time for lung cancer radiotherapy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Folkerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A wavelet-based single-view reconstruction approach for cone beam X-ray luminescence tomography imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Optics Express</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3848" to="3858" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3D surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image2Mesh: A learning framework for single image 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A novel digital tomosynthesis (DTS) reconstruction method using a deformation field map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thongphiew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7Part1</biblScope>
			<biblScope unit="page" from="3110" to="3115" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Projected tetrahedra revisited: A barycentric formulation applied to digital radiograph reconstruction using higher-order attenuation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sadowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="473" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Make3D: Learning 3D scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Free-form deformation of solid geometric models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sederberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Development and application of the new dynamic NURBSbased Cardiac-Torso (NCAT) phantom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Segars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of North Carolina</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists&apos; detection of pulmonary nodules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shiraishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katsuragawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ikezoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kodera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Roentgenology</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="74" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast calculation of the exact radiological path for a threedimensional CT array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siddon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="252" to="255" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometrics</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11461</idno>
		<title level="m">Exploiting geometric structure for graph-encoded objects</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sparseness prior based iterative image reconstruction for retrospectively gated cardiac micro-CT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Badea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4476" to="4483" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A dose comparison study between XVI and OBI CBCT systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chvetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bhandare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="480" to="486" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">2D/3D deformable registration using a hybrid atlas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Penalized weighted least-squares approach to sinogram noise reduction and image reconstruction for low-dose X-ray computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1272" to="1283" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3D-2D deformable image registration using feature-based nonuniform meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed Research International</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
