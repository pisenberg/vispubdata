<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LassoNet: Deep Lasso-Selection of 3D Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zhutian</forename><surname>Chen</surname></persName>
							<email>zhutian.chen@connect.ust.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
							<email>wei.zeng@siat.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguang</forename><surname>Yang</surname></persName>
							<email>zg.yang@siat.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
							<email>lingyun.yu@rug.nl.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<email>cwfu@cse.cuhk.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
							<email>huamin@cse.ust.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LassoNet: Deep Lasso-Selection of 3D Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934332</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Point Clouds</term>
					<term>Lasso Selection</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Fig. 1. LassoNet enables effective lasso-selection of 3D point clouds based on a latent mapping from viewpoint and lasso to target point clouds. LassoNet is particularly efficient for selecting multiple regions (insets 1, 2, 3) in a complex scene (left), since no viewpoint changing is required to select occluded points. Notice here the insets are viewed from different viewpoints.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Vast amounts of 3D point clouds have been collected from various sources, such as LiDAR scanning and particle simulation. Exploratory analysis and visualization of point clouds show benefits in many applications, including astronomy, autonomous navigation, and scene reconstruction. Selection is a fundamental task in exploratory analysis of point clouds. However, designing effective selection for 3D point clouds is challenging, especially when the visualization is projected onto a planar 2D surface <ref type="bibr" target="#b18">[19]</ref>. The challenge comes from several perspectives: 1) data: a point cloud often consists of a set of unlabeled points, i.e., no information of what label each point holds; 2) visual-local point density <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42]</ref>, or context-aware that further take into account the lasso location and shape <ref type="bibr" target="#b42">[43]</ref>. All methods employ a heuristic that users intend to select regions of higher local point density, which is valid for astronomical datasets where users are typically interested in galaxy cores <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> or halos <ref type="bibr" target="#b29">[30]</ref>. However, many other point clouds do not exhibit this property. In this case, these density-based selection techniques lose their advantages and often select unintended clusters that have higher densities of points than target clusters.</p><p>In this work, we approach lasso-selection of 3D point clouds from a new angle: lasso-selection is regarded as a latent mapping function ( f ) between point clouds (P), viewpoint (V ), and lasso (L), i.e., f (P,V, L) → P s . We hereby introduce LassoNet, a learning-based approach to seek an optimal mapping based on deep learning techniques. LassoNet integrates deliberated modules to tackle challenges of: (i) data heterogeneity induced by 3D point clouds, viewpoint, and 2D lasso − we associate them using 3D coordinate transformation and naive selection (Sec. 4.1); (ii) generalizability caused by widely ranging number of points and varying point densities − we employ an intention filtering and farthest point sampling (FPS) algorithm (Sec. 4.2). We build a hierarchical neural network to learn local and global features (Sec. 4.3) from a dataset consisting of over 30K lasso-selection records on two different point cloud corpora. Model experiments show that LassoNet can effectively learn the mapping (Sec. 5). We also conduct a formal user study showing that LassoNet advances state-of-the-art lasso-selection methods (Sec. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions are as follows</head><p>We develop LassoNet − a deep neural network that models lassoselection of 3D point clouds as a latent mapping from viewpoint and lasso to point cloud regions. To our knowledge, this is a first attempt of learning-based approach that successfully addresses limitations of existing heuristics-based methods.</p><p>We address the challenges of data heterogeneity using 3D coordinate transformation and naive selection, and generalizability using intention filtering and farthest point sampling. We further build a hierarchal neural network to improve network performance.</p><p>We train LassoNet on a new dataset with over 30K lasso-selection records, and release the code and dataset to enable future studies on lasso-selection of point clouds. A formal user study confirms LassoNet fulfills the efficient, effective, and robust requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Lasso-Selection for 3D Point Clouds. Selection is a fundamental task in interactive visualization <ref type="bibr" target="#b37">[38]</ref>. Designing effective interaction theories and methods is regarded as a main challenge for scientific visualization <ref type="bibr" target="#b18">[19]</ref>. Systematic reviews of 3D objects selection techniques can be found in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>. Specifically, lasso-selection is preferable for interacting with 3D point clouds projected on a 2D surface <ref type="bibr" target="#b42">[43]</ref>. An ultimate problem here is how to deduce user-intended region in 3D view frustum from a lasso on 2D surface. Cone/Cylinder-selection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref> is a basic method, which selects all objects within a geometry (i.e., cone or cylinder) extruded from a lasso. The selections can be refined by moving the cone/cylinder <ref type="bibr" target="#b31">[32]</ref>. Owada et al. <ref type="bibr" target="#b25">[26]</ref> improved volume data selection by segmenting the data according to user-drawn stroke. This idea of deducing regions of user intention based on underlying data characteristics inspired structure-aware lasso-selection methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42]</ref>. A series of dedicated methods were developed, including TeddySelection and CloudLasso <ref type="bibr" target="#b41">[42]</ref>. WYSIWYP ('what you see is what you pick') technique <ref type="bibr" target="#b36">[37]</ref> can be integrated to provide instant feedback <ref type="bibr" target="#b29">[30]</ref>. Recently, context-aware methods <ref type="bibr" target="#b42">[43]</ref> that further take into account the lasso position and shape were developed. Limitations of Existing Lasso-Selection Techniques. Conventional CylinderSelection methods select all the points enclosed by the frustum extruded from an input lasso. In case target points are not located in the same area, Boolean operations of multiple lasso-selections are needed. For instance, it is a common task to select both the left and right wings of an airplane. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, users can complete the task by either joining the right and left wing regions with union (top), or removing the body part region from a larger selection with subtraction (bottom). In a more complicated scenario, e.g., to select yellow target points in a more complex scene as in <ref type="figure">Fig. 1</ref>, users need to draw multiple lassos from different viewpoints. Much time will be spent on finding a suitable viewpoint for each target.</p><p>CAST <ref type="bibr" target="#b42">[43]</ref> have been developed for making 3D point cloud selections more intuitive and efficient. The main idea of CAST family techniques is to infer a user-intended selection region based on properties of point clouds and lasso drawings. They employ a series of heuristics based on density information. Therefore, CAST is particularly effective for selections in 3D point cloud datasets with varying point densities, for instance, astronomy simulations of galaxy collisions and N-body problems. However, not all 3D point clouds, e.g., ShapeNet and S3DIS datasets studied in this work (see Sec. 5), exhibit this property. Taking the airplane <ref type="figure" target="#fig_0">(Fig. 2)</ref> extracted from ShapeNet for an example, all parts share almost the same point density. CAST hereof will perform similarly to CylinderSelectionL: all points within the frustum extruded from a lasso will be selected.</p><p>To make lasso-selections more robust and efficient, we should go beyond scalar properties of point density. One possible solution is to add more intrinsic features of point clouds, such as heat kernel signature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>. Nevertheless, the approach would need tremendous trial-and-error processes to find suitable parameters (which may not even exist). Instead, we opt to learning-based approach, as we envision that a deep neural network can effectively capture intrinsic features of point clouds, and eventually learn an optimal mapping f (P,V, L).</p><p>Deep Learning for Interaction. Recent years have witnessed the burst of deep learning techniques, benefiting many fields such as image process and natural language processing. The visualization community has also been contributing to deep learning. On the one hand, many visualization systems have been developed to 'open the black box' of deep learning models through visual understanding, diagnosis, and refinement <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>. On another hand, emerging researches have employed deep learning to address domain-specific tasks, such as to classify chart types <ref type="bibr" target="#b17">[18]</ref>, to measure the similarity between scatterplots <ref type="bibr" target="#b22">[23]</ref>, and even to perceive graphical elements in visualization <ref type="bibr" target="#b10">[11]</ref>. The community has also conducted several pieces of research on exploiting deep learning techniques to facilitate user interactions. Fan and Hauser <ref type="bibr" target="#b5">[6]</ref> modeled user brushing in 2D scatterplot as an image, which can be handled by a convolutional neural network (CNN) to predict selected points. The method greatly improves selection accuracy, meanwhile preserves efficiency. Han et al. <ref type="bibr" target="#b12">[13]</ref> developed a voxel-based CNN framework for processing 3D streamlines, by which clustering and selection of streamlines and stream surfaces are improved.</p><p>Inspired by them, we also model lasso-selection of 3D point clouds using deep learning. However, in our case, point clouds are distributed in 3D space. Thus CNNs (e.g., <ref type="bibr" target="#b5">[6]</ref>) designed for 2D images are not feasible. Point clouds datasets exhibit great diversity, e.g., sparse vs dense, balanced vs imbalanced density. Voxel-based neural network <ref type="bibr" target="#b12">[13]</ref> that divides the volume into a low resolution of 64 3 voxelization can dramatically reduce prediction accuracy. Instead, we employ a featurebased deep neural network (DNN) that has becoming more popular for processing 3D point clouds.</p><p>... ... Deep Learning for Point Clouds. Point cloud is a special type of 3D geometric data that can be processed by deep learning <ref type="bibr" target="#b35">[36]</ref>. Based on how to model 3D shapes into CNN processable units, prior researches can be categorized as: Volumetric CNNs (e.g., <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref>) convert a 3D shape into voxels and apply a 3D CNN over voxels, which faces a critical problem of sparsity, especially for processing non-uniformly distributed point clouds. Multiview CNNs (e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref>) render 3D shapes into 2D images from multiple viewpoints, and then apply 2D CNNs to analyze them. However, in this work viewpoint is a parameter in the latent mapping that we expect the network to learn. Spectral CNNs (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>) learn geometric features defined on 3D manifold meshes, which however, are not easy to construct from 3D point clouds. Many studies (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>) have employed feature-based DNNs that convert 3D data into a vector and apply fully connected network to classify the shape. This approach can be seamlessly integrated with shape features, making it popular for processing 3D shapes now. However, it is extremely challenging to build a suitable DNN for point clouds, as too many features can be derived from enormous points. Qi et al. <ref type="bibr" target="#b26">[27]</ref> successfully addressed the challenge with PointNet, which consists of multiple layers of feature transformation, multi-layer perceptron, and max pooling. Based on PointNet, they further developed a hierarchical neural network <ref type="bibr" target="#b27">[28]</ref> that improves the prediction accuracy. Recently, a series of follow-up works (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>) were conducted to address more complex tasks. We also build LassoNet upon Qi's work. To our knowledge, this is the first extension that has been developed to facilitate user interaction. We show how to tackle challenges of data heterogeneity and scalability using domain knowledge in visualization and human-computer interaction (Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Encoding</head><note type="other">3D</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>The scope of this work is constrained to lasso-selection of 3D point clouds using a 2D input device (e.g., mouse) to draw lassos on a 2D surface (e.g., a desktop monitor). Other input and display devices, such as 3D hand gestures and virtual reality HMDs, are out of the scope. The selection process involves three components:</p><p>Point cloud (P): A point cloud P consists of a set of points {p i ob j :</p><formula xml:id="formula_0">= (x i ob j , y i ob j , z i ob j )} n i=1</formula><p>, where x i ob j , y i ob j , z i ob j indicate position of the point p i ob j in a 3D object space R 3 ob j . n is the total number of points, which can be in a wide range from a few thousand (ShapeNet dataset) to hundreds of thousands (S3DIS and astronomical datasets). Unlike images that are made up of organized pixel arrays, a point cloud holds no specific order of points. Besides, many point clouds in realworld are unsegmented. We would like the selection method being applicable to them. Nevertheless, the unordered and unsegmented properties compound the difficulty of effective lasso-selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Viewpoint (V ):</head><p>A viewpoint V is determined by many factors, including camera position and direction, field of view (FOV), and perspective/orthogonal projection. When a visualization starts, FOV and projection type are usually fixed. Users can control the viewpoint by translating and rotating the camera. Before drawing a lasso on the screen, users tend to find an optimal viewpoint that minimizes occlusion for the target points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lasso (L):</head><p>A lasso L can be represented as an ordered list of points</p><formula xml:id="formula_1">{l i scn := (u i scn , v i scn )} m i=1 , where u i scn , y i scn indicate position of a point l i scn in a 2D screen space R 2</formula><p>scn . The lasso L meets two requirements: 1) Closed: In case the user-drawn stroke is not closed, we enclose it by connecting its start (l 1 scn ) and end (l m scn ). 2) Non-self-intersecting: In case the input stroke self-intersects, we pick its largest closed part starting and ending at the intersection.</p><p>In this work, we regard lasso-selection as a mapping f (P,V, L) that retrieves a subset of points P s ⊆ P based on the current viewpoint V and lasso drawing L. To be effective, the mapping function f should minimize difference between P s and target points P t , i.e., arg min</p><formula xml:id="formula_2">f {d J (P s , P t ) | f (P,V, L) → P s }<label>(1)</label></formula><p>Meanwhile, we would also like the selection to be efficient, which requires the method should be fast enough for fluid interaction, and robust, i.e., the performance remains effective and efficient over various conditions of point clouds (P), viewpoints (V ), and lassos (L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LASSONET</head><p>LassoNet pipeline ( <ref type="figure">Fig. 3</ref>) consists of three main stages:</p><p>1. Interaction Encoding. A primary challenge for this work is implied by data heterogeneity, i.e., to associate viewpoint and lasso information with point cloud. We address the challenge by 3D coordinate transformation that transforms point cloud from object space to camera space, and naive selection that filters a subset of point cloud through CylinderSelection. 2. Filtering and Sampling. The next challenge is to address scalability issue implied by great variability of point clouds (e.g., dense vs sparse) and lasso selection (e.g., small vs large). We employ first an intention filtering mechanism that filters points within an intention area, and then a farthest point sampling (FPS) algorithm that divides dense point clouds into partitions. 3. Network Building. Lastly, we build a hierarchical neural network to learn a latent mapping between point cloud, viewpoint, and lasso. This network structure is inspired by PointNet++ <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> that achieves good performance for segmenting non-uniformly distributed and varying density point clouds.  , and we employ a hierarchical structure that generates more local and global features using (b) abstraction and (c) propagation components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Interaction Encoding</head><p>To couple point clouds with viewpoint and lasso information is a prerequisite before we can train a DNN model. We achieve this in two steps, as illustrated in <ref type="figure">Fig. 3</ref>(b):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">3D Coordinate Transformation:</head><p>The viewpoint is determined by many factors, including camera position and direction, FOV, projection type, etc. The information not available in point clouds, from which we only know point positions in 3D object space R 3 ob j . We encode viewpoint information by transforming coordinates of all point clouds from R 3 ob j to camera space R 3 cam . The transformation can be computed following the graphics pipeline. Specifically, when a user draws the lasso, we record the current position and rotation of the camera, forming a 4×4 projection matrix. We then derive position of a point in the camera space p i cam by multiplying the projection matrix with original position p i ob j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Naive Selection:</head><p>The next question is how to associate a lasso with the point cloud. Notice that the lasso L consists of an ordered list of points in 2D screen space R 2 scn , while the point cloud has been transformed to 3D camera space R <ref type="bibr" target="#b2">3</ref> cam . As indicated in <ref type="figure">Fig. 3</ref>(b), we associate lasso information with the point cloud using a naive lasso selection method. First, we extrude a lasso L in the 2D screen space to a frustum F in 3D camera space, based on the current viewpoint and screen parameters. Now both point cloud and lasso are transformed to camera space R <ref type="bibr" target="#b2">3</ref> cam . Thus we can check if a point p i cam is located inside F. We append a binary value w i to indicate if p i cam falls in F: 1 for inside (red points inside <ref type="figure">Fig. 3</ref>(b)), and 0 for outside (blue points in <ref type="figure">Fig. 3(b)</ref>).</p><p>After these, each point p i ob j is modeled as p</p><formula xml:id="formula_3">i cam := (x i cam , y i cam , z i cam , w i ), where x i cam , y i cam , z i cam indicate position in camera space R 3</formula><p>cam , and w i indicates if the point falls inside the frustum F extruded from a lasso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Filtering and Sampling</head><p>To cope with varying scales implied by point cloud and lasso selection, one simple approach is to add more neurons of a neural network, i.e., scaling up the network width. This, however, will greatly increase the computation time and wreck interactive response. Instead, we employ a filtering-and-sampling approach as in <ref type="figure">Fig. 3</ref>(c):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Intention Filtering:</head><p>We would like to filter out points which users definitely have no intention to select. We employ a heuristic that points distant from the lasso are less intended and can be filtered out. Ideally, the intention can be measured as a parabolic function based on distance to the lasso.</p><p>However, it can be inefficient to compute all point distances to a lasso. Instead, we implement a method that is much simpler, yet gives results as good as the parabolic one. When users draw a lasso on the 2D surface, we first find the lasso's bounding box (yellow dashed rectangle in <ref type="figure">Fig. 3</ref>(c)). We then expand the box 1.2 times outwards (solid purple rectangle in <ref type="figure">Fig. 3(c)</ref>), since the points slightly outside the lasso could also be the intended targets. Points falling outside the expanded box are filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Farthest Point Sampling (FPS):</head><p>Intention filtering can reduce the amount of points in a great extent. However, in cases that the point cloud is dense or the lasso selection is big, there can still be too many filtered points to fit into a single GPU memory. To cope with these situations, we further employ a downsampling algorithm of FPS which can reduce the number of points meanwhile effectively preserve the convex hull of filtered points <ref type="bibr" target="#b4">[5]</ref>. Here, we iteratively split the filtered points into multiple partitions, with each partition consisting of up to thre(FPS) points. All partitions are fed to the network for selection prediction. The predictions of all partitions will be combined together as the final output.</p><p>By these, a point cloud is divided into multiple partitions, each of which consists of a set of sample points {p si cam :</p><formula xml:id="formula_4">= (x si cam , y si cam , z si cam , w i )} thre(FPS) i=1</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Building</head><p>Filtering and sampling step ensures the amount of points is manageable by a neural network in real-time. This, however, can greatly affect the prediction accuracy, which is then addressed by a hierarchical design of neural network as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. A core component here is PointNet (PN) <ref type="bibr" target="#b26">[27]</ref>, which directly consumes an unordered list of points and outputs a representative feature vector. As illustrated in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>, PN first employs a group of fully connected (FC) layers to map each point into a high-dimensional space. The FC layers share parameter weights to reduce the number of parameters and accelerate network convergence. Finally, a pooling layer is used to aggregate the high-dimensional features and output a feature vector that can be regarded as the signature of the input point cloud. However, the pooling layer in PN only remains the global information of the whole point cloud. The relationship between a point and its local neighborhood is missing, which is not desired since it decreases the prediction accuracy. Alternatively, we employ abstraction and propagation hierarchical structures as in PointNet++ <ref type="bibr" target="#b27">[28]</ref> to generate features of both local and global information. Abstraction <ref type="figure" target="#fig_2">(Fig. 4(b)</ref>): We first divide the whole input points into several groups of equal size. Each point group is represented as a level-0 feature vector. We then apply PN to each point group, yielding a level-1 feature vector characterizing local features for each group of points. Each feature vector can actually be modeled as a set of high-dimensional points, which can again be processed by PN to extract correlations among point groups. This grouping and correlation extraction processes are recursively repeated k times. By this, we obtain a level-k feature vector that stores both global and local features of the input point cloud.</p><p>Propagation: The next challenge is how to propagate the level-k feature vector to individual points. Here, we first concatenate levelk with level-(k − 1) feature vectors, and applying a FC layer that generates a propagated layer of feature vector. The process is again repeated k times until each point group is propagated. By this, we generated a final feature vector containing rich information for each point, including its local relation to neighboring points, and its global relation to the whole point cloud.</p><p>Based on the final feature vector, we can predicate for each sample point p si cam a probability value ρ i using FC layers with a softmax function, which falls in [0, 1] indicating the probability that p si cam is selected. If ρ i is larger than 0.5, we regard p si cam as selected; otherwise not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MODEL EXPERIMENTS</head><p>To train LassoNet, we collect a dataset of more than 30K lasso-selection records (Sec. 5.2) annotated on two publicly available point cloud corpora (Sec. 5.1).Then we introduce the training process (Sec. 5.3), and finally report the quantitative evaluation results (Sec. 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Point Cloud Preparation</head><p>We choose two point cloud corpora that have been widely used in many applications, e.g., robotics and scene reconstruction. The first one is ShapeNet <ref type="bibr" target="#b40">[41]</ref>, containing in total over 16K point clouds of CAD models in 16 categories (e.g., airplane, car, bag). Each point cloud consists of several thousand points and point densities from 10K to 6M points/m 3 . The points are divided into two to six parts, e.g., an airplane is divided into body, wing, engine, and tail. The second corpus is Stanford Large-Scale 3D Indoor Spaces (S3DIS) dataset, containing 272 point clouds collected by high-resolution 3D scanning of rooms.</p><p>The point clouds exhibit a wide range of point numbers from 60K to 3M, and point densities ranging from 0.2K to 60K points/m 3 . The points are also divided into parts, e.g., chair, table, and floor.</p><p>To improve the quality of annotation, we first filter out points in the following cases: i) The whole point cloud consists of only one part, e.g., laptop and skateboard point clouds in ShapeNet; ii) The points occluding the view heavily, e.g., ceiling points in S3DIS datasets. Nevertheless, even after filtering, there are still too many point clouds in ShapeNet. Thus, we further randomly select 15% from each category. After filtering and sampling, we retrieve 2,332 point clouds in 14 categories from ShapeNet, and 272 point clouds from S3DIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Lasso-Selection Annotation</head><p>We recruit 20 professional annotators to generate lasso selection records on the point cloud corpora. The annotation is done on a web-based visualization platform that renders target points in yellow and interfering points in blue with a fixed FOV of 60 (see <ref type="figure" target="#fig_3">Fig. 5</ref>). For each point cloud, we randomly allocate one part (e.g., wings of an airplane, or a table in a room) as the target, and the others as interfering points. The platform supports 5-DOF navigation using mouse input.</p><p>The annotators are asked to enclose target points by drawing an appropriate lasso (see red lassos in <ref type="figure" target="#fig_3">Fig. 5</ref>) from a good viewpoint. Then, the target points inside of the lasso are highlighted to indicate how successful the selection is. Thus, for each set of target points, no matter the points are separated or not, we allow for only one lasso selection. Taking the airplane in <ref type="figure" target="#fig_3">Fig. 5</ref> for an example, to select both wings, users are allowed to draw a lasso from different viewpoints as in the first two subfigures, but not to draw two lassos. We encourage the annotators to complete the selection as good as possible, so we do not set an explicit time limit in the annotation. When an annotation is finished, a backend process will record information of point cloud id, target points ids, current camera position &amp; direction, and lasso drawings. To ensure annotation quality, we clean up records that cover less than 70% of the target points or more than 80% non-target points.  <ref type="figure" target="#fig_3">Figure 5</ref> presents some examples of the annotations, which exhibit a wide range of diversities in: 1) point cloud in terms of the whole (e.g., airplane, bag, rooms) and target points (e.g., airplane wings, table, chair); 2) viewpoints in terms of camera position (close by vs far away) and angle (e.g., top, bottom, side); 3) lassos in terms of position and shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Network Training</head><p>Following conventions in machine learning, we randomly split annotations records by point clouds into 9 : 1 for training and testing. In this way, point clouds for testing do not appear in the training set. This yields 2,092 out of 2,332 point clouds from ShapeNet, and 242 out of 272 from S3DIS for training. Loss function. Since our task can be formulated as a per-point binary classification problem (i.e., selected vs non-selected), we adopt a cross entropy loss function to train LassoNet. For a training record, we calculate the loss on each point and then average the losses over all points to update the network by a backward propagation algorithm. The loss for each training record can be calculated as:</p><formula xml:id="formula_5">L = − 1 n n ∑ i=1 (θ 0 s i log(ρ i ) + θ 1 (1 − s i ) log(1 − ρ i )),<label>(2)</label></formula><p>where n is the number of points in a training point cloud P :</p><formula xml:id="formula_6">= {p i } n i=1</formula><p>. s i is a binary value indicating the ground-truth status of a point p i : 0 for interfering points, and 1 for target points. ρ i is the probability value of p i predicted by LassoNet. To improve robustness of LassoNet on point clouds with extremely imbalanced numbers of target and interfering points, we add θ 0 &amp; θ 1 to control weights of the two classes. Specifically, the interfering points are usually much more than target points in S3DIS annotations, thus we set θ 0 = 4 and θ 1 = 1. In contrast, θ 0 &amp; θ 1 are both set to 1 in ShapeNet.</p><p>Hyper parameters. There are two hyper parameters that play important roles in LassoNet, namely, threshold of FPS thre(FPS), and size of a group size(g) in network building (Sec. 4.3). Implementation Details. Adam optimizer is used to optimize the loss of the model. We choose 0.9 for the momentum and 1e-3 for initial learning rate, which is reduced by half per 50 epoch. To avoid overfitting, we employ batch normalization with a decay rate starting from 0.5 and exponentially grows to 0.99, and dropout with keep ratio of 0.7 on the last FC layer. The models are implemented using TensorFlow and run on a server equipped with four NVIDIA GTX1080Ti graphics cards. Each training process contains 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation</head><p>Accuracy performance is a main criterion for lasso selection techniques. As discussed in Sec. 3, the difference between selection points P s and target points P t should be minimized. We measure the difference using Jaccard distance, which is calculated as:</p><formula xml:id="formula_7">d J (P s , P t ) = 1 − |P s ∩ P t | |P s ∪ P t | = 1 − |P s ∩ P t | |P s | + |P t | − |P s ∩ P t |<label>(3)</label></formula><p>We further include F1 score that is often used in measuring binary classification performance. F1 is measured upon true positive (TP = |P s ∩ P t |), false positive (FP = |P s − P s ∩ P t |), and false negative (FN = |P t − P s ∩ P t |): F1 = 2T P/(2T P + FP + FN). In general, F1 score tends to measure average performance, while d J tends to measure the worst case performance. Both d J and F1 are in the range of [0, 1], where 0 indicates best performance for d J but worst performance for F1, and vice versa.</p><p>We compare LassoNet with CylinderSelection -a basic lassoselection method for 3D point clouds. <ref type="table" target="#tab_2">Table 2</ref> presents the comparison results on the testing annotations from ShapeNet and S3DIS separately. Overall, LassoNet achieves much better performance than Cylinder-Selection on both annotation datasets in terms of both F1 score and d J . Specifically, we notice that the performance of CylinderSelection drops much on S3DIS annotations, while LassoNet only drops a bit. We hypothesis this is because S3DIS annotations are more diverse than ShapeNet annotations. To validate the hypothesis, we conduct further evaluations from the following perspectives:</p><p>Scene complexity. We quantify scene complexity using the number of parts in a point cloud. Point clouds in ShapeNet contain a limited number of parts (≤ 6), while S3DIS point clouds usually consist of tens of parts. for S3DIS annotations divided into groups of [0, 10), <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20)</ref>, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30)</ref>, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40)</ref>, and [40, +∞) parts. Again, d J for CylinderSelection remain high in all cases, while LassoNet remains low around 0.2. Surprisingly, we notice that when the number of parts exceeds 40, d J of CylinderSelection drops, while LassoNet increases. Task complexity. We quantify task complexity using the percentage of target points in a point cloud. Big targets (i.e., higher percentage) are typically easier to select than small ones (i.e., small percentage). <ref type="figure" target="#fig_5">Figure 7</ref> shows the comparison results. Targets in ShapeNet are occupying higher percentages than those in S3DIS. We divide ShapeNet annotations into 5 equal ranges, and measure the average Time performance is another criterion for lasso selection techniques. <ref type="table" target="#tab_2">Table 2</ref> also presents a comparison of time costs for CylinderSelection and LassoNet on ShapeNet and S3DIS annotations. Here, CylinderSelection is implemented in WebGL with average time costs of 16.67ms for ShapeNet and 18.86ms for S3DIS. LassoNet requires additional times for network computation, which adds up to 20.47ms and 69.46ms for ShapeNet and S3DIS, respectively. The increments are reasonable given that point clouds in S3DIS contains hundreds of times more points than those in ShapeNet. The time costs are also comparable with state-of-the-art lasso-selection methods such as CloudLasso <ref type="bibr" target="#b41">[42]</ref>. Nevertheless, time costs for accomplishing accurate selection tasks of LassoNet are actually less than those of CylinderSelection and CAST; see <ref type="figure" target="#fig_8">Fig. 10</ref> and Sec. 6.3 for details. Underfitting and overfitting are critical challenges in machine learning <ref type="bibr" target="#b8">[9]</ref>. Underfitting occurs when the model cannot fit the training set, while overfitting occurs when the model fits the training set well but fails to fit the testing set. In network training stage, we have adopted multiple strategies, including dropout, weight decay, and batch normalization, to avoid the issues. Nevertheless, to further investigate whether these issues occur, we examine d J per epoch in the training and testing processes, which are plotted as blue and red lines as shown in <ref type="figure" target="#fig_7">Fig. 8</ref>, respectively. From the figures, we can notice that d J in training process decreases rapidly and smoothly, and d J in the testing process also decreases with a small gap between that in the training process. The observations confirm that our model does not suffer from underfitting and overfitting problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">USER STUDY</head><p>In reality, users typically complete selection of target points using a sequence of lassos. To cope with this fact, we conduct a formal user study to further evaluate the performance of LassoNet in comparison with two lasso-selection methods of conventional CylinerSelection and SpaceCast − a state-of-the-art density-based selection technique. SpaceCast is chosen since it is the only method in the CAST family that is able to select a part of the cluster in case there is no density variation (such as two wings of an airplane). Here, we allow users to refine a selection using Boolean operations of union, intersection, and subtraction for all three interactions.</p><p>This section reports quantitative results of the study in terms of efficiency measured as completion time, and effectiveness measured as Jaccard distance. By comparing efficiency and effectiveness over different datasets, we further evaluate robustness of LassoNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Design</head><p>Participants: We recruited 16 participants (9 males and 7 females) in the study. 13 participants are students from different disciplines such as computer science and biochemistry, while the other three are research staff. All participants had at least a Bachelor's degree. The age of the participants ranges from 22 to 29, with the mean age of 24.69 years (SD = 1.78). All participants reported to be right-handed. Four participants had experience of working with point clouds. Three participants had experience of manipulating 3D objects, and they are familiar with basic 3D interactions, including rotation and zoom-in/-out. All participants completed the experiments in about 90 minutes. Apparatus and Implementation: Testing datasets from ShapeNet and S3DIS were converted into a data format of point positions. A webbased visualization is developed for LassoNet, while CylinderSelection and CAST are running on CAST application developed by the authors. To eliminate bias caused by rendering effects, we adopted the same settings of FOV, background, and point colors with the CAST tool. Target points were rendered in orange color while interfering points and noise points were in blue. LassoNet models ran on a backend server using one NVIDIA GTX1080Ti graphics card. All experiments were performed on a full HD resolution display (1920×1080 px), with a standard mouse as the input device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ShapeNet (D1)</head><p>S3DIS (D2) Astronomy (D3) <ref type="figure">Fig. 9</ref>. Exemplar point clouds employed in the user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We conducted the user study on three different datasets, as shown in <ref type="figure">Fig. 9</ref>. Besides ShapeNet (denoted as D1) and S3DIS (D2) described in Sec. 5.1, we recruited a third dataset of astronomy point clouds that were used in CAST experiments (D3). D3 consists of four point clouds, each of which is made up of 200K to 400K points representing multiple particle clusters. The clusters have equal uniform densities and are surrounded in a low-density noise environment. The target cluster was located either in the center or was partially surrounded by interfering points so that it is tricky to find a clear view to the whole target. We trained a new model for D3, using only 600 lasso-selection records manually annotated by ourselves. The other settings are the same as those used when training S3DIS annotations. Same as <ref type="bibr" target="#b42">[43]</ref>, participants were asked to select some of the small clusters. From each dataset, we selected three different point clouds with one meaningful part as target points. All the point clouds and target points were not used for training. In total, there were 9 assignments (3 point clouds × 3 targets) for participants to complete using each method. Task and Procedure: The task was to select target points marked in orange while avoiding interfering points marked in blue. Selected points would be marked in red. In CylinderSelection and SpaceCast, the participants were allowed to refine the selections by three Boolean operations: union, intersection, and subtraction, in case they were not satisfied by the results. The participants were reminded that completion time is also an evaluation metric. So they were expected to complete the tasks as soon as possible in case they were satisfied with the results. The participants could take a 5-minute break when they felt tired.</p><p>Before actual experiments, we explained to the participants about the principles of the next lasso-selection method. We demonstrated how to change the viewpoint, draw lassos, and select the target points on the screen. To ensure the participants fully understood the interactions, they were asked to practice with three training datasets. In the training trials, we gave the participants as much time as they needed. To suppress learning effects gained from previous assignments, we assigned a sequence of lasso-selection methods pseudo-randomly to each participant. When participants felt satisfied with the results, they proceeded to the next task by pressing a Submission button, and a backend process automatically recorded the completion time and accuracy for the current task. In the end, the participants were asked to complete a questionnaire for user feedback on their satisfactory of each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Hypotheses</head><p>We expected LassoNet would outperform CylinderSelection on all three datasets in terms of completion time and Jaccard distance. We also expected LassoNet would achieve similar performance with SpaceCast on D3, while better performances on D1 and D2 which do not have varying point density. Moreover, since CylinderSelection does not require any additional computation, we expected that CylinderSelection would achieve similar performance on all three datasets. H1: LassoNet would be more efficient, i.e., less completion time, than CylinderSelection on all datasets (H1.1). Compared with SpaceCast, LassoNet would be more efficient on D1 &amp; D2, while equally efficient on D3 (H1.2).</p><p>H2: LassoNet would be more effective, i.e., smaller Jaccard distance, than CylinderSelection on all datasets (H2.1). Compared with Space-Cast, LassoNet would be more effective on D1 &amp; D2, while being equally efficient on D3 (H2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H3:</head><p>CylinderSelection would be the most robust, i.e., similar completion times (H3.1) and Jaccard distances (H3.2), on all three datasets. LassoNet would be more robust than SpaceCast.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Quantitative Results</head><p>We collected in total 432 records (16 participants × 3 techniques × 9 assignments) from the user study. <ref type="figure" target="#fig_8">Figure 10</ref> presents a comparison of mean completion time (left), Jaccard distances (right), and their 95% confidence intervals for each technique conducted on each dataset. At noticed, LassoNet outperformed CylinderSelection on both completion time and accuracy. LassoNet also achieves better performance than SpaceCast on D1 &amp; D2, and similar performance on D3.</p><p>We performed a two-way ANOVA (3 techniques × 3 datasets) on both completion time and Jaccard distance. Before the analysis, we first confirmed that all results of completion time and Jaccard distance in each condition follow normal distribution using a Shapiro-Wilk test. Prerequisites for computing ANOVA are fulfilled for the hypothesis. All hypotheses are confirmed by the analyses. Below we report details of individual analysis.</p><p>Completion Time. As expected, selection technique had a significant effect on completion times (F 2,429 = 82.73, p &lt;.0001). Average completion times <ref type="figure" target="#fig_8">(Fig. 10(left)</ref>) are 37.34s for CylinderSelection (SD = 18.74), 27.14s for SpaceCast (SD = 17.72), and 15.49s for LassoNet (SD = 7.81). Post-hoc tests using Bonferroni correction indicate that LassoNet is significantly faster than both CylinderSelection (p &lt;.0001) and SpaceCast (p &lt;.0001). Through more detailed probes, we found that LassoNet was significantly faster than CylinderSelection (F 1,47 = 69.62, p &lt;.0001), (F 1,47 = 61,51, p &lt;.0001), (F 1,47 = 39.60, p &lt;.0001) on D1, D2, and D3, respectively. This confirms the hypothesis H1.1. We also found that LassoNet was significantly faster than SpaceCast (F 1,47 =72.28 , p &lt;.0001) on D1, and (F 1,47 = 28.28 , p &lt;.0001) on D2. No significant difference is observed for LassoNet and SpaceCast on D3 (F 1,47 =2.30 , p = 0.13). This further confirms the hypothesis H1.2.</p><p>We checked effects of dataset on completion time. Using Bonferroni correction test, we found that dataset shields a significant effect on SpaceCast (F 2,141 = 31.72, p &lt;.0001), while no significant effect on ClyinderSelection (F 2,141 = 1.91, p = 0.15) and LassoNet (F 2,94 = 1.91, p = 0.008). This result confirms the hypothesis H3.1.</p><p>Jaccard Distance. We repeated ANOVA tests on Jaccard distance <ref type="figure" target="#fig_8">(Fig. 10(right)</ref>), by which significant effects imposed by selection techniques were observed (F 2,429 = 29.77, p &lt;.0001). LassoNet achieved a much lower mean Jaccard distance of 7.65% (SD = 6.94), in comparison with CylinderSelection (mean = 14.44%, SD = 11.61) and SpaceCast (mean = 17.95%, SD = 14.68). LassoNet is significantly more effective than CylinderSelection on all three datasets (F = 36.38, p &lt;.0001), which confirms the hypothesis H2.1. Compared to Space-Cast, LassoNet is slightly more but not significant (F=1.87, p=0.18) error-prone on D3, whilst it is significantly effective on D1 (F = 72.28, p &lt;.0001) and D2 (F = 28.28, p &lt;.0001). These results confirm H2.2.</p><p>Though not significant, CylinderSelection achieves the most consistent Jaccard distances on different datasets (F 2,141 = 3.52, p &lt;0.05), in comparison to SpaceCast (F 2,141 = 32.60, p &lt;.0001) and LassoNet (F 2,141 = 10.73, p &lt;.0001). Nevertheless, LassoNet is more stable than SpaceCast. Hypothesis H3.2 hereof is confirmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Qualitative User Feedback</head><p>We also collected qualitative user feedback from the participants after the user study. 13 out of 16 participants prefer LassoNet, due to its simplicity to learn and to use. One participant stated that "a person knowing how to control mouse should feel no difficulty in lasso-selection". They also appreciated that the visual interface returned immediate feedback upon lass selections. The Boolean operations of union, intersection, and subtraction posed some difficulty for them in the beginning, but they fully understood the operations through several trial-and-error trainings finally. Below we summarize their feedback for each method.</p><p>CylinderSelection. All participants felt that results from CylinderSelection are most predictable. Some participants reported that this was highly appreciated because by then they can refine the selections using Boolean operations as expectations. However, we also noticed that the participants showed interests to refine selections at the beginning, but the interests dropped quickly after a few assignments. This reaction was particularly obvious on D2, where the scenes are complex so that participants would need to change viewpoints very often when making refinements. This explains why average completion time of CylinderSelection is slightly less on D2 than on D1.</p><p>SpaceCast. Three participants expressed high praise for SpaceCast on D3, which allowed them to make pretty accurate selections. In fact, most participants would choose SpaceCast as their favorite method if the experiments were conducted on D3. However, all participants felt that SpaceCast was very unpredictable on D1 &amp; D2, even though we had clearly explained the underlying mechanism. Often the results were only a part of what they intended to select. For instance, when the assignment was to select the left wing of an airplane (see <ref type="figure" target="#fig_0">Fig. 2</ref>), SpaceCast often selected only the engine part. We suspect the reason was that the engine has a slightly higher density of points than the wing.</p><p>LassoNet. Most participants favored LassoNet since the selections best match with their intention. "It seems the method can really understand what I want", one participant commented. Nevertheless, the participants also figured out that refinement using LassoNet is not as feasible as CylinderSeleciton. When making refinements, participants often select only a few points at a very close view. In such scenarios, LassoNet tends to select more points that exhibit strong correlations with the target points (e.g., neighboring, symmetric, etc.), see <ref type="figure" target="#fig_0">Fig. 12</ref> for an example. They suggested adding Boolean operations in the technique for refinements. <ref type="figure">Figure 1</ref> presents a typical example of selection task: in a complex room, users need to three regions of different objects. For conventional CylinderSelection method, users need to adjust viewpoints according to the current region of selection, and also need to refine selections using Boolean operations. Instead, LassoNet can complete the task directly from the top view. Even though the targets are partially occluded, LassoNet can still correctly deduce regions of user intention based on the viewpoint and lassos. <ref type="figure" target="#fig_9">Figure 11</ref> presents the comparison of LassoNet with prior methods on two different point clouds. The left side presents a chair in ShapeNet dataset. The task is to select the seat of the chair. The viewpoint is changed to a good position that allows users to draw a lasso. Obviously, from this view direction, CylinderSelection selects the seat and parts the legs, which is not desired. In comparison, LassoNet successfully separates the seat from legs and produces a good selection result. On the right side, the task is to select one from three interlocking rings by drawing a lasso from the viewpoint, as shown in the top-left corner. As expected, CylinderSelection selects all points inside of the lasso, whilst LassoNet and SpaceCast achieve equally good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Examples</head><p>LassoNet can also effectively select multiple separate parts by using only one lasso, as illustrated in <ref type="figure" target="#fig_0">Fig. 12 (left)</ref>. Here, a user draws a lasso to enclose all three target chairs, and LassoNet correctly segments points belonging to the chairs from surrounding points. Notice that the training dataset does not include such cases (for S3DIS data, only one object is selected as the target for each annotation), LassoNet (probably) identifies similarity features among the chairs and select all of them together. However, we also notice that sometimes LassoNet gives unexpected results, as shown in <ref type="figure" target="#fig_0">Fig. 12 (right)</ref>. We assume that LassoNet (again probably) detects symmetric features of trailing edge in the left and right airfoils, which is however not desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Generalizability</head><p>We also tried to train a single model for both ShapeNet and S3DIS annotations, yielding an average d J of 0.214 and F1 score of 0.873. The performance drops in comparison with those achieved by LassoNet using separate models (see <ref type="table">Tab.</ref> 2). The cause for performance dropping mainly comes from large differences between ShapeNet and S3DIS annotations: 1) numbers of points in S3DIS point clouds are almost a hundred times greater than those in ShapeNet point clouds, 2) point densities in the two corpora are very different, as S3DIS point clouds are collected by sensing of real-world indoor rooms, while ShapeNet point clouds are samples from synthetic CAD models, and 3) selections for S3DIS point clouds are usually smaller regions, in comparison to those for ShapeNet point clouds. Nevertheless, the results still outperform basic CylinderSelection.</p><p>Training a single model for multiple datasets is a challenging task. As for now, employing different parameter settings for different datasets is practically more feasible. Many recent deep neural network models for point cloud processing, such as MCCNN <ref type="bibr" target="#b13">[14]</ref>, also trained separate models for different datasets. A potential solution is domain adaptation <ref type="bibr" target="#b1">[2]</ref>, especially multi-source domain adaptation, that has proven beneficial for learning source data from different domains <ref type="bibr" target="#b11">[12]</ref>. Furthermore, domain adaptation can also learn a well-performing model for target data that exhibit different but related distributions with source data, thereby improving generalizability of the model. Thus, we consider domain adaptation as an important direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Limitations and Future Work</head><p>Though the model experiment and user study have demonstrated that LassoNet advances prior methods, there are several limitations.</p><p>All deep neural network models, no matter supervised or unsupervised, require tremendous amounts of training data to generate high prediction accuracy. We tackle this issue using a new dataset with over 30K records generated by professional annotators. It is also feasible to extend this dataset by synthesizing variations from existing records <ref type="bibr" target="#b5">[6]</ref>. Yet, there can still be certain scenarios not covered by the training data.</p><p>When making refinements, users would like to select only a few points. As observed by the participants, LassoNet tends to expand the selection to some closely correlated points. A feasible solution here would be to add a conditional statement in LassoNet: when naive selection detects only a few points being selected, LassoNet automatically returns these points as output.</p><p>We also identify several promising direction for future work:</p><p>A first and foremost work is to update our backbone network to stateof-the-art deep learning models for processing point clouds. For example, Hermosilla et al. proposed MCCNN that utilizes Monte Carlo up-and down-sampling to preserve the original sample density, making it more suitable for non-uniformly distributed point clouds <ref type="bibr" target="#b13">[14]</ref>. We consider MCCNN as an important future improvement to enhance the effectiveness and robustness of LassoNet.</p><p>Second, we would like to develop visual analytics to get a better understanding of what has the network learned, which currently is a 'blackbox'. As for now, we suspect that the network has modeled several intrinsic properties of point clouds, including i) local point density, as astronomic point clouds exhibit; ii) symmetric property, as indicated by airplane wings; iii) heat kernel signature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>, as the network can segment seat and legs of a chair <ref type="figure" target="#fig_9">(Fig. 11 (left)</ref>). The issue calls for more visual analytics to 'open the black box' <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Last but not least, we would like to incorporate more parameters in the mapping function, such as FOV and stereoscopic projection. Up to this point, we have only modeled viewpoint in terms of camera position and direction, but not other parameters. Modeling these parameters would be necessary and interesting, as it can potentially extend the applicability of LassoNet to many other scenarios, e.g., to improve selection in VR/AR environments (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We presented LassoNet, a new learning-based approach of lassoselection for 3D point clouds built upon deep learning. Our approach can be readily applicable to any scenario where one has a set of unordered points (P), a 2D surface for visualizing the points (V ), and a lasso on the surface (L). Essentially, LassoNet can be regarded as an optimization process of finding a functional latent mapping function f (P,V, L) :→ P s such that P s matches best with a user's intention of selection P t . To learn such an optimal mapping, we created a new dataset with over 30K selection records on two distinct point cloud corpora. LassoNet also integrates a series of dedicated modules including coordinate transformation, intention filtering, and furthest point sampling to tackle the challenges of data heterogeneity and scalability. A quantitative comparison with two prior methods demonstrated robustness of LassoNet over various combinations of 3D point clouds, viewpoints, and lassos in terms of effectiveness and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>To select both two wings of the airplane, two sequential lassos from different viewpoints are required, and selection results are combined using either union (top) or subtraction (bottom) Boolean operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Overview of network building. The DNN network is built upon (a) PointNet<ref type="bibr" target="#b27">[28]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Exemplar annotation records for point clouds in ShapeNet (left) and S3DIS (right): target and interfering points are colored in yellow and blue respectively, while lassos are in red color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 comparesFig. 6 .</head><label>66</label><figDesc>CylinderSelection and LassoNet over variations of scene complexity. On the left, average d J are measured for ShapeNet annotations divided into groups of 2 − 6 parts. It can be observed that d J of CylinderSelection increases quickly to ∼0.58 when the number of parts increases to 5, while LassoNet remains to be less than 0.2. On the right, average d J are measured Jaccard distances of CylinderSelection and LassoNet measured upon scene complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Jaccard distances of CylinderSelection and LassoNet measured upon task complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>d J as presented on the right. As expected, d J of both CylinderSelection and LassoNet drop when the percentage of target points increase. The same trend can be observed for CylinderSelection on S3DIS annotations, as shown Fig. 7(left). Since the scene is much complex, we divide the annotations according to target point percentages in the range of [0, 1%), [1%, 2%), [2%, 3%), [3%, 4%), [4%, +∞). In contrast, we can see that LassoNet achieves stable and better performances across the five groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Jaccard distances per epoch in training and testing processes for ShapeNet (left) and S3DIS (right) annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Comparison of completion time (left) and Jaccard distance (right) of CylinderSelection, SpaceCast, and LassoNet on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Comparison of LassoNet with prior methods on two different examples. (Left) LassoNet vs CylinderSelection on a chair in ShapeNet dataset. (Right) LassoNet vs CylinderSelection vs SpaceCast on an artificial dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Typical examples of a good (left) case and a bad (right) case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 3. LassoNet consists of three stages: In Interaction Encoding stage, we associate point cloud with viewpoint and lasso through 3D coordinate transformation and naive selection; In Filtering and Sampling stage, we reduce the amount of points for network processing through intention filtering and farthest point sampling. Lastly, we build a hierarchical neural network in Network Building stage.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>LassoNet</cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell>a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Filtering and Sampling</cell><cell>Network Building</cell><cell></cell></row><row><cell>b</cell><cell>Coordinate Transformation</cell><cell>Naive Selection</cell><cell>c</cell><cell>Attention Filtering</cell><cell>Farthest Point Sampling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of lasso-selection records.Table 1 presents statistics of lasso-selection records. In total, we have collected 19,432 lasso-selection records for 6,297 different parts of target points in ShapeNet point clouds, and 12,944 records for 4,018 different parts of target points in S3DIS point clouds.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Point Clouds #Targets #Records</cell></row><row><cell>ShapeNet</cell><cell>2,332</cell><cell>6,297</cell><cell>19,432</cell></row><row><cell>S3DIS</cell><cell>2,72</cell><cell>4,018</cell><cell>12,944</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance of CylinderSelection and LassoNet on ShapeNet and S3DIS annotations.thre(FPS) controls the maximum number of points fed into the network, which depends on computational resource. In our experiments, we use Nvidia GTX1080Ti GPUs and set thre(FPS) to 20,480.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ShapeNet</cell><cell></cell><cell cols="2">S3DIS</cell></row><row><cell></cell><cell>d J</cell><cell>F1</cell><cell>Time (ms)</cell><cell>d J</cell><cell>F1</cell><cell>Time (ms)</cell></row><row><cell>Cylinder</cell><cell cols="2">0.28 0.84</cell><cell>16.67</cell><cell cols="2">0.61 0.57</cell><cell>18.86</cell></row><row><cell cols="3">LassoNet 0.08 0.95</cell><cell>20.47</cell><cell cols="2">0.17 0.90</cell><cell>69.46</cell></row><row><cell cols="7">size(g) control the receptive fields of the network, ranging from 1</cell></row><row><cell cols="7">to thre(FPS). A smaller size(g) makes the network focus more on</cell></row><row><cell cols="7">local features of a point cloud, but leads to deeper hierarchy and</cell></row><row><cell cols="7">more computational cost. A bigger size(g) allows the network to</cell></row><row><cell cols="7">compute more efficiently, but less accurate predictions caused by</cell></row><row><cell cols="7">the lack of sufficient local details. size(g)should be set based on the</cell></row><row><cell cols="7">characteristics of the target datasets. Empirically, we set size(g) to</cell></row><row><cell cols="7">2048 for ShapeNet annotations, since the point clouds contain only</cell></row><row><cell cols="7">a few thousand points. For S3DIS annotations, we set size(g) to 32</cell></row><row><cell cols="7">that strikes a good balance between effectiveness and efficiency.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors wish to thank the anonymous reviewers for their valuable comments. This work was supported in part by National Natural Science Foundation of China (No.61802388 and No.61602139). This work is partially supported by a grant from MSRA (code: MRA19EG02).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of 3d object selection techniques for virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Argelaguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andujar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="121" to="136" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scale-invariant heat kernel signatures for non-rigid shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1704" to="1711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The farthest point strategy for progressive image sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1305" to="1315" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast and accurate cnn-based brushing in scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="111" to="120" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D Deep Shape Descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2319" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aperture based selection for immersive virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Herndon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeleznik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UIST</title>
		<meeting>UIST</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="95" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3D mesh labeling via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno>3:1-3:12</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating &apos;graphical perception&apos; with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="641" to="650" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dadaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karbalayghareh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9115" to="9124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FlowNet: A Deep Learning Framework for Clustering and Selection of Streamlines and Stream Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pienta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3D segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordeil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alligier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="704" to="714" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ChartSense: Interactive data extraction from chart images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CHI</title>
		<meeting>ACM CHI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6706" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reimagining the scientific visualization interaction paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="51" to="57" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">3D User Interfaces: Theory and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Laviola</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kruijff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Poupyrev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Addison Wesley Longman Publishing Co., Inc</publisher>
			<pubPlace>Redwood City, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Towards Better Analysis of Machine Learning Models: A Visual Analytics Perspective. Visual Informatics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ScatterNet: A deep subjective similarity model for visual analysis of scatterplots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<title level="m">Geodesic Convolutional Neural Networks on Riemannian Manifolds. IEEE ICCVW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="832" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Intel. Conf. Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Owada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<title level="m">Proc. Symp. Interactive 3D Graphics and Games</title>
		<meeting>Symp. Interactive 3D Graphics and Games</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaichun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Leonidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PointNet++ : Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interactive visual exploration of halos in large-scale cosmology simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="156" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards a general model for selection in virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DUI</title>
		<meeting>3DUI<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D Selection Strategies for Head Tracked and Non-Head Tracked Operation of Spatially Immersive Displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IIPT</title>
		<meeting>IIPT</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A concise and provably informative multi-scale signature based on heat diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symp. Geometry Processing</title>
		<meeting>Symp. Geometry essing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glyphlens: View-dependent occlusion management in the interactive glyph visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="891" to="900" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">O-CNN: octreebased convolutional neural networks for 3D shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">WYSIWYP: What You See Is What You Pick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wiebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Vos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2236" to="2244" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selection: 524,288 ways to say &quot;this is interesting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Wills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE InfoVis</title>
		<meeting>IEEE InfoVis</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="54" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="403" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A scalable active framework for region annotation in 3D shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">210</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient Structure-Aware Selection Techniques for 3D Point Cloud Visualizations with 2DOF Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Efstathiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2245" to="2254" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">CAST: Effective and Efficient User Interaction for Context-Aware Selection in 3D Particle Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Efstathiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TVCG</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="886" to="895" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
