<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InSituNet: Deep Image Synthesis for Parameter Space Exploration of Ensemble Simulations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Hanqi</forename><surname>Guo</surname></persName>
							<email>hguo@anl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko-Chih</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Han-Wei</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Raj</surname></persName>
							<email>mraj@anl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><forename type="middle">S G</forename><surname>Nashed</surname></persName>
							<email>ynashed@anl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Tom</forename><surname>Peterka</surname></persName>
							<email>tpeterka@anl.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">InSituNet: Deep Image Synthesis for Parameter Space Exploration of Ensemble Simulations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TVCG.2019.2934312</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-02-19T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>In situ visualization</term>
					<term>ensemble visualization</term>
					<term>parameter space exploration</term>
					<term>deep learning</term>
					<term>image synthesis</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We propose InSituNet, a deep learning based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. In situ visualization, generating visualizations at simulation time, is becoming prevalent in handling large-scale simulations because of the I/O and storage constraints. However, in situ visualization approaches limit the flexibility of post-hoc exploration because the raw simulation data are no longer available. Although multiple image-based approaches have been proposed to mitigate this limitation, those approaches lack the ability to explore the simulation parameters. Our approach allows flexible exploration of parameter space for large-scale ensemble simulations by taking advantage of the recent advances in deep learning. Specifically, we design InSituNet as a convolutional regression model to learn the mapping from the simulation and visualization parameters to the visualization results. With the trained model, users can generate new images for different simulation parameters under various visualization settings, which enables in-depth analysis of the underlying ensemble simulations. We demonstrate the effectiveness of InSituNet in combustion, cosmology, and ocean simulations through quantitative and qualitative evaluations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Ensemble simulations <ref type="bibr" target="#b63">[64]</ref> have been playing an increasingly important role in various scientific and engineering disciplines, such as computational fluid dynamics, cosmology, and weather research. As the computational power of modern supercomputers continues to grow, ensemble simulations are more often conducted with a large number of parameter settings in high spatial and/or temporal resolutions. Despite the advances in accuracy and reliability of simulation results, however, two challenges have emerged: (1) I/O bottleneck for the movement of the large-scale simulation data and (2) effective exploration and analysis of the simulation parameters. In situ visualization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>, which generates visualization at simulation time and stores only the visualization results (that are much smaller than the raw simulation data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>) for post-hoc analysis, addresses the first challenge to some extent. However, it also limits the flexibility of post-hoc exploration and analysis, because the raw simulation data are no long available.</p><p>This study focuses on improving scientists' ability in exploring the in situ visualization results of ensemble simulations and extending their capability in investigating the influence of different simulation parameters. Several pioneering works have been proposed to facilitate post-hoc exploration of in situ visualization results. For example, the Cinema framework <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> visualized the simulation data from different viewpoints in situ and collected images to support post-hoc exploration. The volumetric depth images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> stored ray segments with composited color and opacity values to enable post-hoc exploration of arbitrary viewpoints for volume rendering. However, these approaches focus more on extending the capability to explore the visual mapping parameters (e.g., transfer functions) and view parameters (e.g., view angles) and have little consideration of the simulation parameters, which are important in studying ensemble simulations.</p><p>Simulation parameter space exploration is not trivial, because the relationship between the simulation parameters and outputs is often highly complex. The majority of existing simulation parameter space exploration approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b64">65]</ref> resorted to visualizing a set of simulation parameters and outputs simultaneously and revealing the correspondence between the parameters and outputs through visual linkings. However, these approaches often depend on the raw simulation data that might not be available for large-scale ensemble simulations. Moreover, these approaches have limited ability in inferring simulation outputs with respect to new parameters. Hence, extra simulations have to be conducted for new parameters, which cost enormous computational resources for most scientific simulations.</p><p>In this work, we propose InSituNet, a deep learning based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. Our work is based on the observation that images of high accuracy and fidelity can be generated with deep neural networks for various image synthesis applications, such as super-resolution <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>, inpainting <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b67">68]</ref>, texture synthesis <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b70">71]</ref>, and rendering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>. Specifically, we train InSituNet to learn the end-to-end mapping from the simulation, visual mapping, and view parameters to visualization images. The trained model enables scientists to interactively explore synthesized visualization images for different simulation parameters under various visualization settings without actually executing the expensive simulations. Our approach consists of three major steps.</p><p>1. In situ training data collection from ensemble simulations Given ensemble simulations conducted with different simulation parameters, we visualize the generated simulation data in situ with various visual mapping and view parameters. The resulting visualization images and the corresponding parameters are collected and used for the offline training of InSituNet. 2. Offline training of InSituNet Given the parameters and image pairs, we train InSituNet (i.e., a convolutional regression model) with cutting-edge deep learning techniques on image synthesis to map simulation, visual mapping, and view parameters to visualization images directly. 3. Interactive post-hoc exploration and analysis With the trained InSituNet, we build an interactive visual interface that enables scientists to explore and analyze the simulation from two perspectives: (1) inferring visualization results for arbitrary parameter settings within the parameter space with InSituNet's forward propagations and (2) analyzing the sensitivity of different parameters with InSituNet's backward propagations. We demonstrate the effectiveness of the proposed approach in combustion, cosmology, and ocean simulations, and compare the predicted images of InSituNet with the ground truth and alternative methods. In addition, we evaluate the influence of different hyperparameters of In-SituNet (e.g., the choice of loss functions and the network architectures) and provide guidance in configuring the hyperparameters. In summary, the contributions of this paper are threefold:</p><p>• A deep image synthesis model (i.e., InSituNet) that enables posthoc parameter space exploration of ensemble simulations <ref type="table">• An interactive visual interface to explore and analyze the parameters of ensemble simulations with the trained InSituNet  • A comprehensive study revealing the effects of different hyper-</ref>parameters of InSituNet and providing guidance for applying In-SituNet to other simulations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review related work in image-based in situ visualization, parameter space exploration of ensemble simulations, and deep learning for visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image-Based In Situ Visualization</head><p>Based on the output, in situ visualization can be categorized into image-based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, distribution-based <ref type="bibr" target="#b19">[20]</ref>, compression-based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>, and feature-based <ref type="bibr" target="#b11">[12]</ref> approaches. We regard our work as an image-based approach, which visualizes simulation data in situ and stores images for post-hoc analysis. Tikhonova et al. <ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref> generated images of multiple layers in situ to enable the adjustment of transfer functions in post-hoc analysis. Frey et al. <ref type="bibr" target="#b21">[22]</ref> proposed volumetric depth images, a compact representation of volumetric data that can be rendered efficiently with arbitrary viewpoints. Fernandes et al. <ref type="bibr" target="#b20">[21]</ref> later extended volumetric depth images to handle time-varying volumetric data. Biedert and Garth <ref type="bibr" target="#b7">[8]</ref> combined topology analysis and image-based data representation to preserve flexibility for post-hoc exploration and analysis. Ahrens et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> proposed Cinema, a framework that stores visualization images in situ and performs post-hoc analysis via exploration and composition of those images. Compared with these approaches, our work supports not only the exploration of various visual mapping and view parameters but also the creation of visualizations under new simulation parameters without actually running the simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parameter Space Exploration</head><p>The existing parameter space exploration works for ensemble simulations can generally be reviewed from two perspectives, the adopted visualization techniques and the objective of parameter space exploration. Visualization techniques that designed for high-dimensional data are often borrowed to visualize the parameter space of ensemble simulations, as the simulation parameters are typically treated as multidimensional vectors. These techniques include but are not limited to: parallel coordinate plots <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b64">65]</ref>, radial plots <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, scatter plots <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58]</ref>, line charts <ref type="bibr" target="#b8">[9]</ref>, matrices <ref type="bibr" target="#b47">[48]</ref>, and glyphs <ref type="bibr" target="#b9">[10]</ref>. For the objectives of parameter space exploration, we believe the six tasks sorted out by Sedlmair et al. <ref type="bibr" target="#b54">[55]</ref> could best summarize the literature, which are optimization <ref type="bibr" target="#b61">[62]</ref>, partitioning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b64">65]</ref>, filtering <ref type="bibr" target="#b46">[47]</ref>, outliers <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>, uncertainty <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>, and sensitivity <ref type="bibr" target="#b8">[9]</ref>. We refer the interested readers to the work of Sedlmair et al. <ref type="bibr" target="#b54">[55]</ref> for the detailed definition of each task, as well as the example visualization works.</p><p>The aforementioned parameter visualization techniques and analysis tasks mostly focus on a limited number of simulation inputs and outputs collected from ensemble runs. In this paper, we train a surrogate model to extend our study to arbitrary parameter settings within the parameter space, even if the simulations were not executed with those settings. In addition, our approach is incorporated with in situ visualization, which is widely used in large-scale ensemble simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Learning for Visualization</head><p>The visualization community has started to incorporate deep learning in visualization research. For example, Hong et al. <ref type="bibr" target="#b29">[30]</ref> used long short-term memory <ref type="bibr" target="#b28">[29]</ref> to estimate access pattern for parallel particle tracing. Han et al. <ref type="bibr" target="#b25">[26]</ref> used autoencoders <ref type="bibr" target="#b51">[52]</ref> to cluster streamlines and streamsurfaces. Xie et al. <ref type="bibr" target="#b66">[67]</ref> used neural network embeddings to detect anomalous executions in high performance computing applications. Berger et al. <ref type="bibr" target="#b5">[6]</ref> proposed a deep learning approach to assist transfer function design using generative adversarial networks (GANs) <ref type="bibr" target="#b24">[25]</ref>, which is closely related to our approach. Specifically, we focus on parameter space exploration of ensemble simulations instead of transfer function design for volume rendering.</p><p>Our work is related to deep learning based image synthesis, which has been used in various applications, including super-resolution <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>, denoising <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b69">70]</ref>, inpainting <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b67">68]</ref>, texture synthesis <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b70">71]</ref>, text-to-image synthesis <ref type="bibr" target="#b49">[50]</ref>, style transfer <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b71">72]</ref>, and rendering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>. We investigate and combine different state-of-the-art deep learning techniques on image synthesis (e.g., perpetual losses <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref> and GANs <ref type="bibr" target="#b24">[25]</ref>) to improve the quality of our image synthesis results.  <ref type="figure">Fig. 1</ref>. Workflow of our approach. Ensemble simulations are conducted with different simulation parameters on supercomputers, and visualization images are generated in situ for different visual mapping and view parameters. The generated images and the parameters are collected into an image database. A deep image synthesis model (i.e., InSituNet) is then trained offline based on the collected data, which is later used for parameter space exploration through an interactive visual interface. <ref type="figure">Figure 1</ref> provides the workflow of our approach, which consists of three major components. First, given ensemble simulations conducted with different simulation parameters, we visualize the generated simulation outputs in situ with different visual mapping and view parameters on supercomputers. The three groups of parameters-simulation, visual mapping, and view parameters-along with the corresponding visualization results (i.e., images) are collected to constitute an image database (Section 4). Second, with the collected data pairs between parameters and the corresponding images, we train InSituNet to learn the end-to-end mapping from the simulation inputs to the visualization outputs (Section 5). To improve the accuracy and fidelity of the generated images, we use and combine different state-of-the-art deep learning techniques on image synthesis. Third, with the trained InSi-tuNet, we build an interactive visual interface (Section 6) to explore and analyze the parameters from two aspects: (1) predicting visualization images interactively for arbitrary simulation, visual mapping, and view parameters within the parameter space and (2) investigating the sensitivity of different input parameters to the visualization results.   Given ensemble simulations conducted with different simulation parameters, we perform in situ visualization with a desired set of visual mapping parameters (e.g., isosurfaces extraction with a set of isovalues) and different view parameters (e.g., viewpoints). We denote an instance of simulation, visual mapping, and view parameters as P sim , P vis , and P view , respectively, which corresponds to a visualization image I. The parameters (highlighted in green in <ref type="figure" target="#fig_2">Figure 2</ref>) and the corresponding visualization images (highlighted in blue in <ref type="figure" target="#fig_2">Figure 2</ref>) constitute data pairs, which will be stored and used to train InSituNet. InSituNet learns a function F that maps the three groups of parameters to the corresponding visualization image, which can be defined as F (P sim , P vis , P view ) → I, (1) so that it can predict visualization images for unseen parameters. In the following, we discuss the three groups of parameters in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IN SITU TRAINING DATA COLLECTION</head><p>Simulation parameters P sim are represented as a vector with one or more dimensions, and the value range of each dimension is defined by scientists. By sweeping the parameters within the defined ranges, ensemble simulations are conducted to generate the ensemble data.</p><p>Visual mapping parameters P vis are predefined operations to visualize the generated simulation data, such as pseudo-coloring with predefined color schemes. Note that we limit the users' ability in selecting arbitrary visual mappings to produce and store fewer images.</p><p>View parameters P view are used to control the viewpoints that the images are created from. In this work, we define the viewpoints by a camera rotating around the simulation data, which is controlled by azimuth θ ∈ [0, 360] and elevation φ ∈ [−90, 90]. For panning and zooming, we resort to image-based operations (i.e., panning and resizing the images) as proposed in <ref type="bibr" target="#b1">[2]</ref>). To train a deep learning model that can predict visualization images for arbitrary viewpoints, we sample the azimuth and elevation and generate images from the sampled viewpoints. Based on our study, we found that taking 100 viewpoints for each ensemble member is sufficient to train InSituNet.</p><p>With the specified values for the three groups of parameters, we generate the corresponding visualization images. Our work uses RGB images compressed to the portable network graphics (PNG) format instead of more sophisticated image formats, such as volumetric depth images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> or explorable images <ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref>, for two reasons. First, the benefits of using those sophisticated image formats, such as supporting changing of viewpoints, can be achieved by InSituNet trained on the RGB images. Second, RGB images are more generally applicable for various visualizations and more easily to be handled by neural networks compared with those sophisticated image formats.  In the training stage, InSituNet consists of three subnetworks: a regressor, a feature comparator, and a discriminator. The regressor R ω is a deep neural network (defined by a set of weights ω) modeling the function that maps input parameters to visualization images as defined in Equation 1. To train a regressor that can generate images of high fidelity and accuracy, we introduced the feature comparator F and the discriminator D υ to compute losses by comparing the predicted and the ground truth images. The feature comparator is a pretrained neural network whose convolutional kernels are used to extract and compare image features (e.g., edges, shapes) between the predicted and the ground truth images to obtain a feature reconstruction loss. The discriminator D υ is a deep neural network whose weights υ are updated during training to estimate the divergence between the distributions of the predicted and the ground truth images. The divergence is known as the adversarial loss <ref type="bibr" target="#b24">[25]</ref>, which is combined with the feature reconstruction loss to train R ω . In the inference stage, we need only the trained R ω , which can predict visualization images for parameters that are not in the training data. In the following, we discuss the network architecture, the loss function, and the training process in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INSITUNET ARCHITECTURE AND TRAINING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Network Architecture</head><p>Three subnetworks are involved during training: the regressor R ω , feature comparator F, and discriminator D υ . The regressor R ω and discriminator D υ are two deep residual convolutional neural networks <ref type="bibr" target="#b26">[27]</ref> parameterized by the weights ω and υ, respectively. The architectures of R ω and D υ are designed by following the network architecture proposed by <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>, because the scale of our image synthesis problem is similar to theirs. For the feature comparator F, we use the pretrained VGG-19 model <ref type="bibr" target="#b56">[57]</ref>, which has been widely used in many deep image synthesis approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>.  <ref type="figure">Fig. 4</ref>. Architecture of R ω , which encodes input parameters into a latent vector with fully connected layers and maps the latent vector into an output image with residual blocks. The size of R ω is defined by k, which controls the number of convolutional kernels in the intermediate layers.</p><p>The architecture of R ω is shown in <ref type="figure">Figure 4</ref>, which takes the P sim , P vis , and P view as inputs and outputs a predicted image I. The three types of parameters are first fed into three groups of fully connected layers separately, and the outputs are then concatenated and fed into another fully connected layer to encode them into a latent vector. Note that the parameters could also be concatenated first and then fed into fully connected layers. However, as each parameter is fully connected with all neurons in the next layer, more weights will be introduced in the network and the network size will increase. Next, the latent vector is reshaped into a low-resolution image, which is mapped to a high-resolution output image through residual blocks performing 2D convolutions and upsamplings. Following the commonly used architecture <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>, we use the rectified linear unit (ReLU) activation function <ref type="bibr" target="#b42">[43]</ref> in all layers except the output layer. For the output layer, we use the tanh function to normalize each pixel into [−1, 1].</p><p>Note that we introduce a constant k in the network architecture to control the number of convolutional kernels in the intermediate layers.</p><p>The constant k is used to balance the expressive power and the size and training time of R ω to cope with datasets in different complexities.</p><p>Residual Blocks R ω consists of several residual blocks <ref type="figure">(Figure 4a)</ref>, which are proposed in <ref type="bibr" target="#b26">[27]</ref> to improve the performance of neural networks with increasing depth. We adopted the residual blocks here because R ω often needs to be very deep (i.e., more than 10 convolutional layers) to synthesize images with high-resolutions. Inside each residual block, the input image is first upsampled by using nearest neighbor upsampling. The upsampled image is then fed into two convolutional layers with kernel size 3×3. In the end, the original input image is added to the output, and the result is sent to the next layer. Batch normalizations are performed on the output of each convolutional layer to stabilize the training. Note that if the resolution or the channel number of the input image is not the same as the output, we perform the upsampling and convolution operations on the input image to transform it into the size of the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Discriminator D υ</head><p>The architecture of D υ is shown in <ref type="figure">Figure 5</ref>, which takes a predicted/ground truth image and the corresponding parameters as inputs and produces a likelihood value indicating how likely the input image is a ground truth image conditioning on the given parameters. With the  <ref type="figure">(Figure 5a</ref>) is similar to that in R ω except that downsampling (average pooling in this work) is performed instead of upsampling to transform images into low-resolution representations and no batch normalization is performed, because it often hurts the performance of D υ <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Projection-based condition incorporation We employed the projection-based method <ref type="bibr" target="#b41">[42]</ref> to incorporate the conditional information (i.e., the three groups of parameters) with the input image. This method computes a dot product between the data to be incorporated, which in our work is the latent vector of the input parameters and the latent vector of the image <ref type="figure">(Figure 5b</ref>). Compared with other condition incorporation methods, such as vector concatenation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40]</ref>, the projection-based method improves the quality of conditional image synthesis results, as demonstrated by Miyato and Koyama <ref type="bibr" target="#b41">[42]</ref>. To produce high quality image synthesis results, we also strive to minimize the feature-level difference between the generated and the ground truth image by employing a commonly used feature comparator F, namely the pretrained VGG-19 model <ref type="bibr" target="#b56">[57]</ref> shown in <ref type="figure" target="#fig_6">Figure 6</ref>. F is a convolutional neural network, and the convolutional kernels on each layer have been pretrained to extract certain types of image features, such as edges and shapes. With it, we extract the features from a generated image, as well as its corresponding ground truth image, and minimize the difference between those features to improve the quality of the generated image (see details in Section 5.2.1). Specifically, we use the layer relu1 2 to extract feature maps for comparison based on two observations. First, early layers such as the layer relu1 2 of the VGG-19 network focus on low-level features such as edges and basic shapes, which commonly exist in scientific visualization images. Second, through our experiments we found that artifacts are introduced into the generated image by pooling layers (e.g., pool1 in <ref type="figure" target="#fig_6">Figure 6</ref>). Hence, we use the layer relu1 2 that is before the first pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Feature Comparator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loss Function</head><p>Given an imageÎ generated using R ω and the corresponding ground truth image I, a loss function L is defined by measuring the difference between them. Minimizing L can, therefore, be conducted by updating the parameters ω of R ω over an iterative training process. The most straightforward choice for L is the average of the pixel wise distance betweenÎ and I, such as the mean squared error. As shown in earlier works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>, however, the average pixel wise distance often produces over-smoothed images, lacking high-frequency features.</p><p>In this work, we define L by combining two advanced loss functions: a feature reconstruction loss [32] L F,l f eat and an adversarial loss <ref type="bibr" target="#b24">[25]</ref> </p><formula xml:id="formula_0">L adv R , namely L = L F,l f eat + λ L adv R ,<label>(2)</label></formula><p>where λ is the coefficient between them. L F,l f eat measures the difference between features extracted from the feature comparator F using its convolutional layer l, whereas L adv R quantifies how easily the discriminator D υ can differentiate the generated images from real ones. As can be seen, minimizing L adv R requires training R ω and D υ together in an adversarial manner (i.e., the adversarial theory of GANs <ref type="bibr" target="#b24">[25]</ref>). In order to train D υ , an adversarial loss L adv D is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Feature Reconstruction Loss</head><p>The feature reconstruction loss between imageÎ and I is defined by measuring the difference between their extracted features <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>. Specifically, for a given image I, our feature comparator F (the pretrained VGG-19) is applied on I and extracts a set of feature maps, denoted as F l (I). Here l indicates which layer the feature maps are from (e.g., the relu1 2 of F). The extracted feature maps can be considered as a 3D matrix of dimension h×w×c, where h, w, and c are the height, width, and number of channels, respectively, as shown in <ref type="figure" target="#fig_6">Figure 6</ref>. The feature reconstruction loss betweenÎ and I can, therefore, be defined as the pixel wise mean squared error between F l (Î) and F l (I). Extending this definition to a batch of images, the feature reconstruction loss betweenÎ 0:b−1 and I 0:b−1 (b is the batch size) is</p><formula xml:id="formula_1">L F,l f eat = 1 hwcb b−1 ∑ i=0 F l (I i ) − F l (Î i ) 2 2 .<label>(3)</label></formula><p>Using the feature reconstruction loss enables our regressor to produce images sharing similar feature maps with the corresponding ground truth images, which lead to images with sharper features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Adversarial Loss</head><p>In addition to the feature reconstruction loss described above, we add an adversarial loss L adv R into the loss function. Unlike the feature reconstruction loss, which measures the difference between each pair of images, the adversarial loss focuses on identifying and minimizing the divergence between two image distributions following the adversarial theory of GANs. Specifically, our discriminator D υ is trained along with the regressor R ω to differentiate images generated by R ω with ground truth images. As the regressor R ω becomes stronger over the training, the discriminator D υ is forced to identify more subtle differences between the generated images and the ground truth.</p><p>The adversarial loss can be used as complementary to the feature reconstruction loss for two reasons. First, the feature reconstruction loss focuses on the average difference between images, and the adversarial loss focuses on local features that are the most important to differentiate the predicted and ground truth images. Second, the feature reconstruction loss compares the difference between each pair of the generated and ground truth images, and the adversarial loss measures divergence between two image distributions.</p><p>In this work, we use the standard adversarial loss presented in <ref type="bibr" target="#b24">[25]</ref>, which uses different loss functions for the generator and discriminator. For the generator (i.e., our regressor R ω ), the adversarial loss is</p><formula xml:id="formula_2">L adv R = − 1 b b−1 ∑ i=0 log D υ (Î i ),<label>(4)</label></formula><p>which reaches the minimum when the discriminator cannot differentiate the generated images from the ground truth images. This loss is combined with the feature reconstruction loss to update our regressor (Equation 2). The adversarial loss of the discriminator is defined as</p><formula xml:id="formula_3">L adv D = − 1 b b−1 ∑ i=0 (log D υ (I i ) + log(1 − D υ (Î i ))),<label>(5)</label></formula><p>which estimates the divergence between the distribution of the generated images and the ground truth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Techniques to Stabilize Training</head><p>We use several techniques to stabilize the adversarial training of R ω and D υ . The instability of adversarial trainings is a well-known problem <ref type="bibr" target="#b24">[25]</ref>, especially when the resolution of synthesized images is high <ref type="bibr" target="#b5">[6]</ref>. The previous work <ref type="bibr" target="#b5">[6]</ref> divided the training into two stages for stabilization. In the first stage, the opacity GAN that produces 64×64 opacity images is trained, whereas the opacity-to-color translation GAN is trained in the second stage to produce 256×256 color images, conditioning on the 64×64 opacity images. In this work, we train a single pair of adversarial networks (i.e., R ω and D υ ) that directly produces 256×256 color images with the help of recent techniques in stabilizing the adversarial training, including the spectral normalization <ref type="bibr" target="#b40">[41]</ref> and the two time-scale update rule (TTUR) <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Spectral Normalization</head><p>Spectral normalization <ref type="bibr" target="#b40">[41]</ref> is used to mitigate the instability of the discriminator, which is a major challenge in stabilizing the adversarial training. Spectral normalization is a weight normalization technique, which outperforms other weight normalization techniques in many image synthesis tasks as shown in <ref type="bibr" target="#b33">[34]</ref>. Spectral normalization normalizes the weight matrix of each layer based on the first singular value of the matrix. With spectral normalization, the discriminator is enforced to be Lipschitz continuous, such that the discriminator is constrained and stabilized to some extent. Spectral normalization is applied on each layer of the discriminator without changing the network architecture; hence spectral normalization is not labeled in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Learning Rate</head><p>The learning rates of R ω and D υ are critical for the stability of the adversarial training. This work uses the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> that changes the learning rate of each weight dynamically during training with respect to the momentum of the weight gradients. In detail, the learning rate in the Adam optimizer is controlled by three hyperparmeters: the initial learning rate α, the first-order momentum β 1 , and the second-order momentum β 2 . To stabilize the training, a small α is often preferred; and we found that 5 × 10 −5 stabilized the training in our cases. In addition, we found that a bigger β 1 often cripples the training and set β 1 to 0 as suggested in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b68">69]</ref>. Compared with β 1 , β 2 has less influence on the stability of the training, which is set to 0.999. In previous works on training GANs, we found that people often update the discriminator more frequently than the generator, because they do not want to update the generator based on a discriminator that is not strong enough. Doing so, however, leads to a longer training time. Our work uses the same update frequency for the regressor and discriminator but with different learning rates α D and α R (i.e., the TTUR technique <ref type="bibr" target="#b27">[28]</ref>). Based on the empirical results shown in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b68">69]</ref>, we set the learning rate of the discriminator to be 4 times that of the regressor, that is, α D = 2 × 10 −4 and α R = 5 × 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training Process</head><p>Algorithm 1 Training process of InSituNet. Input: Training data includes parameters {P sim , P vis , P view } 0:N−1 and the corresponding images I 0:N−1 . Initial weights ω and υ of R ω and D υ , respectively. The feature comparator F. Output: Optimized weights ω and υ 1: Repeat:</p><formula xml:id="formula_4">2:</formula><p>{P sim , P vis , P view } 0:b−1 , I 0:b−1 sampled from training data 3:</p><formula xml:id="formula_5">Î 0:b−1 ← R ω ({P sim , P vis , P view } 0:b−1 ) 4: υ ← Adam( ∇ υ L adv D ( I 0:b−1 ,Î 0:b−1 ; υ) , υ, α D , β 1 , β 2 ) 5: ω ← Adam( ∇ ω L( I 0:b−1 ,Î 0:b−1 ; ω) , ω, α R , β 1 , β 2 ) 6: Until exit criterion is satisfied</formula><p>The process of training our regressor and discriminator is shown in Algorithm 1. Given the training data collected in situ, namely, N pairs of paramters {P sim , P vis , P view } 0:N−1 and the corresponding images I 0:N−1 , we first initialize the network weights ω and υ using the orthogonal initialization <ref type="bibr" target="#b53">[54]</ref>. Then, the discriminator and regressor are updated alternatively by using the stochastic gradient descent until the exit criterion is satisfied. The exit criterion used in this work is the maximum number of iterations, which is set to 125,000 because the loss converged in our cases after 125,000 iterations.</p><p>In each iteration, a batch of parameters {P sim , P vis , P view } 0:b−1 and the corresponding images I 0:b−1 are sampled from the training data (line 2), where b is the batch size. Next, the current R ω takes {P sim , P vis , P view } 0:b−1 as inputs and producesÎ 0:b−1 (line 3). According to the loss L adv D defined on I 0:b−1 andÎ 0:b−1 in Equation 5, the weights of the discriminator are updated (line 4). Similarly, the weights of the regressor are updated as well, according to the loss function L (defined in Equations 2, 3, and 4), which is computed using the feature comparator F and the updated discriminator D υ (line 5). When updating the weights υ and ω, the gradients ∇ υ and ∇ ω of the loss functions L adv D and L are computed, respectively. With ∇ υ and ∇ ω , the weights υ and ω are updated through two Adam optimizers using the learning rates discussed in the preceding section.  With the trained InSituNet, users can perform parameter space exploration of ensemble simulations from two perspectives. First, with InSituNet's forward propagations, users can interactively infer the visualization results for arbitrary parameters within the parameter space. Second, using InSituNet's backward propagations, users can investigate the sensitivity of different parameters and thus have better understanding on parameter selections. To support the parameter space exploration, we built an interactive visual interface as shown in <ref type="figure" target="#fig_7">Figure 7</ref>, which contains two views: Parameters View <ref type="figure" target="#fig_7">(Figure 7(a)</ref>) and Visualization View <ref type="figure" target="#fig_7">(Figure 7(b)</ref>). In the following, we explain how users can perform parameter space exploration with this visual interface. <ref type="table">Table 1</ref>. Datasets and timings: k controls the size of InSituNet to cope with datasets in different complexities; diversity <ref type="bibr" target="#b62">[63]</ref> measures how diverse the generated images are; t sim , t vis , and t tr are timings for running ensemble simulations, visualizing data in situ, and training InSituNet, respectively; t f p and t bp are timings for a forward and backward propagation of the trained InSituNet, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PARAMETER SPACE EXPLORATION WITH INSITUNET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Inference of Visualization Results</head><p>InSituNet is able to interactively infer the visualization results for any user-selected parameter values. As shown in <ref type="figure" target="#fig_7">Figure 7(a)</ref>, the three groups of input parameters for the InSituNet are visualized by using different GUI widgets. For the simulation and view parameters, because their values are usually in continuous ranges, we visualize them using slider bars whose ranges are clipped to the corresponding parameters' predefined value ranges. Users are able to select arbitrary parameter values by interacting with those sliders. For the visual mapping parameters, users can switch among a set of predetermined options using the ratio buttons, for example, selecting different isovalues for isosurface visualizations, as shown in <ref type="figure" target="#fig_7">Figure 7(a)</ref>.</p><p>The selected values for the three groups of parameters are fed into the trained InSituNet. Through a forward propagation of the network, which takes around 30 ms, the corresponding visualization image for the given set of parameters is generated and visualized in the Visualization View, as shown in <ref type="figure" target="#fig_7">Figure 7</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sensitivity Analysis on Simulation Parameters</head><p>Because InSituNet is differentiable, users can perform sensitivity analysis for the simulation parameters using the network's backward propagations. Specifically, users can compute the derivative of a scalar value derived from the generated image (e.g., L 1 norm of the pixel values) with respect to a selected simulation parameter. The absolute value of the derivative can be treated as the sensitivity of the parameter, which indicates how much the generated image will change if the parameter gets changed. Note that the sensitivity analysis in this work is used to reflect the changes (with respect to the parameters) in the image space rather than the data space. Inspired by <ref type="bibr" target="#b5">[6]</ref>, our analysis includes overall sensitivity analysis and subregion sensitivity analysis.</p><p>In overall sensitivity analysis, we focus on analyzing the sensitivity of the entire image with respect to each simulation parameter across its value range. To this end, we sweep each parameter across its value range while fixing the values of other parameters. Images are then generated from the selected parameter values and aggregated into a scalar (i.e., the L 1 norm of the pixel values). The aggregated scalar values are then back propagated through the InSituNet to obtain the sensitivity of the selected parameter values. In the end, a list of sensitivity values is returned for each parameter and visualized as a line chart on top of the slider bar corresponding to the parameter <ref type="figure" target="#fig_7">(Figure 7(a1)</ref>) to indicate how sensitive the parameter is across its value range.</p><p>In subregion sensitivity analysis, we analyze the sensitivity of a selected parameter for different subregions of the generated image. This analysis is done by partitioning the visualization image into blocks and computing the sensitive of the parameter for the L 1 norm of the pixel values in each block. The computed sensitivity values are then color coded from white to red and overlaid on top of the visualization image to indicate what regions are more sensitive with respect to the selected parameter (red blocks in <ref type="figure" target="#fig_7">Figure 7</ref>(b1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>We evaluated InSituNet using combustion, cosmology, and ocean simulations (Section 7.1) from four aspects: (1) providing implementation details and analyzing performance (Section 7.2); (2) evaluating the influence of different hyperparameters (Section 7.3); (3) comparing with alternative methods (Section 7.4); and (4) performing parameter space exploration and analysis with case studies (Section 7.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Ensemble Simulations</head><p>We evaluated the proposed approach using three ensemble simulations: SmallPoolFire <ref type="bibr" target="#b65">[66]</ref>, Nyx <ref type="bibr" target="#b3">[4]</ref>, and MPAS-Ocean <ref type="bibr" target="#b50">[51]</ref>. They are sum-marized in <ref type="table">Table 1</ref> (left) and detailed below.</p><p>SmallPoolFire is a 2D combustion simulation from the Open-FOAM simulation package <ref type="bibr" target="#b65">[66]</ref>. We used it as a test case to evaluate InSituNet by studying two parameters: a turbulence parameter Ck∈[0.0925, 0.0975] and a combustion parameter C∈[4.99, 5.01]. We sampled 4,000 parameter settings from the parameter space: 3,900 for training and 100 for testing. Images were generated for the temperature field by using pseudo-coloring with five predefined color schemes. To study how diverse the generated images are, we use the method proposed by Wang et al. <ref type="bibr" target="#b62">[63]</ref>, which measures the diversity as the reciprocal of the average structural similarity (SSIM) between every pair of images. The diversity of the images in this dataset is 2.72, which means the average SSIM is smaller than 0.4.</p><p>Nyx is a cosmological simulation developed by Lawrence Berkeley National Laboratory. Based on the scientists' suggestion, we studied three parameters: the total matter density (OmM ∈ [0.12, 0.155]), the total density of baryons (OmB ∈ [0.0215, 0.0235]), and the Hubble constant (h ∈ [0.55, 0.85]). We sampled 500 parameter settings from the parameter space: 400 for training and 100 for testing. The simulation was conducted with each parameter setting and generated a 256×256×256 volume representing the log density of the dark matters. The volume was visualized in situ by using volume rendering with a predefined transfer function of the wave colormap <ref type="bibr" target="#b0">1</ref> and from 100 different viewpoints. The diversity of the generated images is 1.72.</p><p>MPAS-Ocean is a global ocean simulation developed by Los Alamos National Laboratory. Based on the domain scientists' interest, we studied the parameter that controls the bulk wind stress amplification (BwsA ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>). We generated 300 ensemble members with different BwsA values. We used 270 of them for training and the rest for testing. The isosufaces of the temperature field (with isovalue={15, 20, 25}) were extracted and visualized from 100 different viewpoints for each ensemble member. The isosurfaces were colored based on salinity, using the colormap suggested by Samsel et al. <ref type="bibr" target="#b52">[53]</ref>. The diversity of the generated images is 1.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Implementation and Performance</head><p>The proposed approach consists of three components: the in situ data collection, the training of InSituNet, and the visual exploration and analysis component. We discuss the implementation details and performance of the three components in the following.</p><p>The in situ visualization was implemented by using ParaView Catalyst 2 following the Cinema framework <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The simulations and in situ visualization were conducted on a supercomputer of 648 computation nodes. Each node contains an Intel Xeon E5-2680 CPU with 14 cores and 128 GB of main memory. We used 1, 28, and 128 processes, respectively, for the SmallPoolFire, Nyx, and MPAS-Ocean simulations. InSituNet was implemented in PyTorch 3 and trained with an NVIDIA DGX-1 system, which contains 8 NVIDIA V100 GPUs with NVlink. The visual interface was implemented based on a web server/client framework. The interface was implemented with D3.js on the client side, and the images were generated from a Python server (with the assist of the trained InSituNet) and sent to the client for visualization. The visual exploration and analysis were tested on a desktop with an Intel Core i7-4770 CPU and an NVIDIA 980Ti GPU.</p><p>The space and computation costs using the proposed approach for the three different datasets are listed in <ref type="table">Table 1</ref> (right). The size of InSituNet is less than 1% and 15% of the raw simulation data and the image data, respectively. In terms of data reduction, we also compare our approach with several data compression methods and the results can be found in the supplementary material. The training of InSituNet generally takes more than 10 hours, but the time is much less than actually running the ensemble simulations with extra parameter settings. After training, a forward or backward propagation of InSituNet takes less than one second on a single NVIDIA 980Ti GPU.</p><formula xml:id="formula_6">Nyx SmallPoolFire MPAS-Ocean L mse L feat Ground Truth L adv_R L feat +10 -2 L adv_R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Model Evaluation for Different Hyperparameters</head><p>We evaluated InSituNet trained with different hyperparameters (i.e., loss functions, network architectures, and numbers of training samples) qualitatively and quantitatively using the data that were excluded from the training to study two questions: (1) Is InSituNet able to generate images that are close to the ground truth images? (2) How do the choices of hyperparameters influence the training results? For quantitative evaluations, we used four metrics that focus on different aspects to compare the predicted images with the ground truth images, including peak signal-to-noise ratio (PSNR), SSIM, earth mover's distance (EMD) between color histograms <ref type="bibr" target="#b5">[6]</ref>, and Fréchet inception distance (FID) <ref type="bibr" target="#b27">[28]</ref>.</p><p>PSNR measures the pixel-level difference between two images using the aggregated mean squared error between image pixels. A higher PSNR indicates that the compared images are more similar pixel wise.</p><p>SSIM compares two images based on the regional aggregated statistical information (e.g., mean and standard deviation of small patches) between them. A higher SSIM means the compared images are more similar from a structural point of view.</p><p>EMD is used in <ref type="bibr" target="#b5">[6]</ref> to quantify the distance between the color histograms of two images. A lower EMD means the compared images are more similar according to their color distributions.</p><p>FID approximates the distance between two distributions of images, which is widely used in recent image synthesis works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b68">69]</ref> as a complementary to other metrics. A lower FID suggests the two image collections are more similar statistically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Loss Functions</head><p>We evaluated InSituNet trained with different loss functions including the mean squared error L mse , the feature reconstruction loss L f eat , the adversarial loss L adv R , and the combination of L f eat and L adv R . <ref type="figure" target="#fig_8">Figure 8</ref> compares the images generated by InSituNet trained with different loss functions with the ground truth (The enlarged figure can be found in the supplementary material). We can see that using L mse often generates over-smoothed images lacking high-frequency features, whereas using L f eat can mitigate the problem to some extent. Using L adv R can generate images that are as sharp as the ground truth, but the features are often not introduced in the desired positions. <ref type="table">Table 2</ref>. Quantitative evaluation of InSituNet trained with different loss functions. The model trained with the combination of L f eat and L adv R generates images with the best EMD and FID and only a slightly lower PSNR and SSIM compared with the model trained with By combining L f eat and L adv R , we are able to generate images with sharp features, and those images are also similar to the ground truth. <ref type="table">Table 2</ref> reports the quantitative results from using different loss functions. We found that using L mse gives the best PSNR, because the network using L mse is trained to minimize the mean squared error (i.e., maximize the PSNR). Using L f eat gives the best SSIM in some cases, because it focuses more on the structure of the images. However, using L mse or L f eat often results in poor performance regarding EMD and FID. When training InSituNet with L adv R , the FID value can be improved, but the values of PSNR and SSIM drop a lot. By combining L f eat and L adv R , both EMD and FID improved a lot, though the PSNR and SSIM got slightly worse than using L mse or L f eat . For L f eat , using which layer of the pretrained VGG-19 to extract features from images can affect the image synthesis results. Through empirical studies, we found that using any layers after the first pooling layer of VGG-19 will introduce undesired checkerboard artifacts, because of the "inhomogeneous gradient update" of the pooling layer <ref type="bibr" target="#b2">[3]</ref>, as shown in <ref type="figure">Figure 9</ref>. Hence, we use the last layer right before the first pooling layer, which is the layer relu1 2. We also evaluated the influence of the weight λ for L adv R when combining it with L f eat (defined in Equation 2), and the results are shown in <ref type="table" target="#tab_6">Table 3</ref>. We found that increasing λ over 0.01 cannot improve the accuracy of the generated images any further. In addition, a small λ (i.e., 0.005) will hurt the image accuracy in terms of EMD and FID, although the value of PSNR and SSIM can be improved slightly. We thereby set λ to 0.01 to balance its effects on the four metrics.</p><formula xml:id="formula_7">L mse or L f eat . L mse L f eat L adv R L f eat + 10 −2 L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Network Architectures</head><p>We evaluated InSituNet with different network architectures in terms of the accuracy of predicted images, the network size, and the training time. As mentioned in Section 5.1, the architecture of our network is controlled by a constant k, which controls the number of convolutional kernels in the intermediate layers. In this experiment, we evaluated four k values: 16, 32, 48, and 64. <ref type="figure" target="#fig_10">Figure 10</ref> shows the PSNR and EMD of images generated by InSi-tuNet with the four k values. We can see that InSituNet with larger  k values can generate more (or at least equally) accurate images, because a larger k gives more expressive power to the neural network.</p><p>On the other hand, training InSituNet with a larger k also costs more time, and more storage will be needed to store the networks, as shown in <ref type="table" target="#tab_7">Table 4</ref> using the Nyx dataset as an example. Hence, to balance the accuracy of the generated images and the cost from both computation and storage, we set k to 32, 48, and 48 for the SmallPoolFire, Nyx, and MPAS-Ocean, respectively. We compared InSituNet trained using different numbers of ensemble runs <ref type="table" target="#tab_8">(Table 5)</ref> to study how many ensemble runs will be needed to train a good model for the three simulations. We found that this number is different in different simulations, depending on the complexity of the mapping between simulation parameters and visualization results. Experiment results show that the accuracy of generated images becomes stable when the number of ensemble runs is greater than 2,900, 200, and 210 for the SmallPoolFire, Nyx, and MPAS-Ocean simulation, respectively. As a result, we used 3,900, 400, and 270 runs from the three simulations to train InSituNet for the rest of the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Number of Ensemble Runs used for Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Comparison with Alternative Methods</head><p>We compared our method with two alternative methods including interpolating images from the training data that close to the target image and the GAN-based volume rendering method (GAN-VR) <ref type="bibr" target="#b5">[6]</ref> using the Nyx dataset. For the interpolation method, we sample g images from the training data whose parameter settings are the top g closest to the parameter setting of the test image and interpolate the sampled images using inverse distance weighting interpolation <ref type="bibr" target="#b55">[56]</ref>. We evaluated g from 1 to 5 and present the result of g = 3 in this section because it balances the four metrics (More results are in the supplementary material). For GAN-VR, we incorporated the simulation parameters into both the opacity GAN and the opacity-to-color translation GAN and removed the transfer function related parameters because we used a fixed transfer function for this dataset. For InSituNet, we selected a network architecture whose size is not greater than the size of GAN-VR network, for a fair comparison. <ref type="figure">Figure 11</ref> compares the ground truth images with the images generated by using interpolation, GAN-VR, and InSituNet. With the new</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>GAN-VR InSituNet Interpolation <ref type="figure">Fig. 11</ref>. Comparison of the images generated using interpolation, GAN-VR, and InSituNet with the ground truth images. <ref type="table">Table 6</ref>. Quantitative comparison of images generated with interpolation, GAN-VR, and InSituNet. network architecture (e.g., the projection-based condition incorporation method in Section 5.1), loss functions (e.g., the feature reconstruction loss in Section 5.2), and training strategies (e.g., the spectral normalization in Section 5.3), InSituNet can generate results that better preserve features compared with the other two methods. The quantitative comparisons between the three methods are shown in <ref type="table">Table 6</ref>. InSituNet outperforms the other two methods in all four metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Parameter Space Exploration</head><p>This section demonstrates the effectiveness of our deep image synthesis driven parameter space exploration through case studies on the Nyx and MPAS-Ocean simulations. Our first case study is focused on investigating the influence of different simulation parameters (i.e., OmM, OmB, and h) on the Nyx simulation. The explorations of different visualization settings can be found in our associated video. <ref type="figure" target="#fig_2">Figure 12</ref> shows a selected parameter setting with the predicted visualization image. To understand the influence of each parameter, we computed the sensitivity of the three parameters with respect to the L 1 norm of the predicted image, shown as the three line charts in <ref type="figure">Figure</ref> 12. From the scale of the three charts (i.e., the values along the vertical axes), we see that parameter h is more sensitive to parameter OmM and parameter OmM is more sensitive to parameter OmB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.1">Case Study with the Nyx Simulation</head><p>Focusing on the most sensitive parameter, namely, parameter h, we explored how it affects the visual appearance of the predicted images. <ref type="figure" target="#fig_4">Figure 13</ref> shows five images predicted by using five different h values, while parameter OmM and OmB are fixed at the values shown on the two corresponding slider bars in <ref type="figure" target="#fig_2">Figure 12</ref>. We first evaluate the accuracy of the sensitivity curve (blue curve in <ref type="figure" target="#fig_4">Figure 13</ref>) computed by backpropagation with the central difference method. To this end, we first regularly sample the simulation parameters along the curve (128 samples are drawn) and then generate the visualization images with respect to the sampled simulation parameters. The L1 norm of the generated images is then computed and used to compute the sensitive curve using the central difference method. The result is shown as the orange curve in <ref type="figure" target="#fig_4">Figure 13</ref>. We can see that the sensitivity curves generated with the two methods are similar. From the guidance provided by the line chart in <ref type="figure" target="#fig_4">Figure 13</ref>, we see that parameter h is more sensitive in the first half of its range (i.e., the left side of the dashed line). The three images generated using h values from this range demonstrate a bigger variance compared with the two images shown on the right (which are images generated by using h values from the second half of its range). Our next case study explores different parameter settings for the MPAS-Ocean simulation and demonstrates the subregion sensitivity analysis for the simulation parameter BwsA, which characterizes the bulk wind stress. Note that here we focus only on exploration and analysis of new parameter settings, the comparison between the predicted and ground truth images is discussed in the previous sections. <ref type="figure">Figure 14</ref> shows isosurface visualizations of the temperature field with three different isovalues from six different viewpoints. The value of parameter BwsA is fixed at 1 in this study. The images reasonably reflect the change of view projections and shading effects. More exploration and visualization results can be found in our associated video. <ref type="figure">Figure 15</ref> shows the predicted images when using different BwsA values. All images are generated from the temperature field (iso-value=15) of MPAS-Ocean and from the same viewpoint. The first row of images shows the result of forward inference with different BwsA values, whereas the second row of images overlays the subregion sensitivity maps onto the corresponding images of the first row. The labeled regions (i.e., <ref type="figure">Figure 15a</ref>, b) change the most when adjusting the value of BwsA, and the subregion sensitivity maps on the second row echo these bigger changes, as indicated by the darker red color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2">Case Study with the MPAS-Ocean Simulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">LIMITATIONS, DISCUSSION, AND FUTURE WORK</head><p>This section discusses several directions that we would like to explore in the future: (1) improving the flexibility in exploring arbitrary visual mapping parameters; (2) increasing the accuracy of predicted images; and (3) increasing the resolution of predicted images.</p><p>One limitation of our approach is that we restricted the users' ability in exploring arbitrary visual mapping parameters, for example, exhausting all possible transfer functions for volume rendering. Instead, we allow users to switch only among several predefined visual mappings, for example, the three isovalues when exploring the MPAS-Ocean data. Theoretically, training a deep learning model to predict visualization images for arbitrary simulation and visualization parameters is feasible. However, it will require a large number of training images to cover the joint space of all possible simulation and visualization parameters. For example, in order to train a model that can predict volume rendering results of a single volume data for arbitrary transfer functions, 200,000 training images are required, as shown in <ref type="bibr" target="#b5">[6]</ref>. Consequently, the size of the training data may even exceed the size of the raw simulation data, which offsets the benefit of in situ visualization. Considering this issue, we would like to explore deep learning techniques that do not require a large number of training samples, such as one-or zero-shot learning, to improve the flexibility of exploration.</p><p>Similar to most other machine learning techniques, generating prediction results that are exactly the same as the ground truth is extraordinary difficult. By taking advantage of recent advances in deep learning for image synthesis, the proposed approach has already outperformed other image synthesis based visualization techniques in terms of the fidelity and accuracy of the generated images (see the comparison in Section 7). However, we believe further improvement is still possible, and we would like explore other network architectures and/or other loss functions to improve our deep image synthesis model.</p><p>Our network architecture limits the resolution of output images to 256 × 256, which might not be sufficient for some high-resolution simulation data. We believe that our network architecture has the potential to generate images with higher resolutions by adding more residual blocks, and we will investigate this approach in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this work, we propose InSituNet, a deep learning based image synthesis model supporting the parameter space exploration of large-scale ensemble simulations visualized in situ. The model is trained to learn the mapping from ensemble simulation parameters to visualizations of the corresponding simulation outputs, conditioned on different visualization settings (i.e., visual mapping and view parameters). With a trained InSituNet, users can generate visualizations of simulation outputs with different simulation parameters without actually running the expensive simulation, as well as synthesize new visualizations with different visualization settings that are not used during the runs. Additionally, an interactive visual interface is developed to explore the space of different parameters and investigate their sensitivity using the trained InSituNet. Through both quantitative and qualitative evaluations, we validated the effectiveness of InSituNet in analyzing ensemble simulations that model different physical phenomena.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Manuscript received 31</head><label>31</label><figDesc>Mar. 2019; accepted 1 Aug. 2019. Date of publication 16 Aug. 2019; date of current version 20 Oct. 2019. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2019.2934312</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Our in situ training data collection pipeline. Simulation data, generated with different simulation parameters, are visualized in situ with different visual mapping and view parameters. The in situ visualization generates a large number of images, which are collected along with the corresponding parameters for the training of InSituNet offline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>illustrates our in situ training data collection pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of InSituNet, which is a convolutional regression model that predicts visualization images from input parameters. During training, the regression model is trained based on the losses computed with the assist of a pretrained feature comparator and a discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3</head><label>3</label><figDesc>illustrates the training and inference pipelines of InSituNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Architecture of F (i.e., VGG-19 network), where each layer is labeled with its name. Feature maps are extracted through convolutional layers (e.g., relu1 2) for feature-level comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Visual interface for parameter space exploration. (a) The three groups of parameters: simulation, visual mapping, and view parameters. (b) The predicted visualization image and the sensitivity analysis result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative comparison of InSituNet trained with different loss functions. Combining L f eat and L adv R gives the results of high quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 . 1 .</head><label>91</label><figDesc>Images generated by InSituNet trained with L f eat that uses different layers after the first pooling layer of VGG-19: (a) relu2 1 and (b) relu3 Checkerboard artifacts are introduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Quantitative evaluation of different network architectures controlled by k with (a) PSNR and (b) EMD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Parameter space exploration with the Nyx simulation. For the selected parameter values, the sensitivity of different parameters is estimated and visualized as line charts on the left, whereas the predicted image is visualized on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Comparison of the visual appearance of the predicted images using different h values to see the effect of this simulation parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Predicted images of the MPAS-Ocean dataset for different isosurfaces and viewpoints, which reasonably reflect the change of view projections and shading effects. Forward prediction (top row) and backward subregion sensitivity analysis (bottom row) for different BwsA. Regions that influenced by BwsA (i.e., regions a and b) are highlighted by the sensitivity map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Fig. 5. Architecture of D υ . Input parameters and the predicted/ground truth image are transformed into latent vectors with fully connected layers and residual blocks, respectively. The latent vectors are then incorporated by using the projection-based method<ref type="bibr" target="#b41">[42]</ref> to predict how likely the image is a ground truth image conditioning on the given parameters. Similar to R ω , the size of D υ is controlled by the constant k.likelihood values, an adversarial loss can be defined to update D υ and R ω (details in Section 5.2.2). Similar to R ω , the three types of parameters are encoded into a latent vector in D υ through fully connected layers. Meanwhile, the input image is fed through several residual blocks to derive its intermediate representation that is a latent vector. The latent vectors are then incorporated to obtain the likelihood value conditioning on the three groups of parameters. ReLU activations are used in all layers except the output layer, which instead uses the sigmoid function to derive a likelihood value within [0, 1].Residual blocks The architecture of the residual blocks in D υ</figDesc><table><row><cell>image</cell><cell>(256, 256, 3)</cell><cell>(in=3, out=k)</cell><cell>(128, 128, k)</cell><cell>(in=k, out=2×k)</cell><cell>(64, 64, 2×k)</cell><cell>(in=2×k, out=4×k)</cell><cell cols="2">(32, 32, 4×k)</cell><cell cols="2">(in=4×k, out=8×k)</cell><cell cols="2">(16, 16, 8×k)</cell><cell>(in=8×k, out=8×k)</cell><cell>(8, 8, 8×k)</cell><cell>(in=8×k, out=16×k)</cell><cell>(4, 4, 16×k)</cell><cell>(in=16×k, out=16×k)</cell><cell></cell><cell>global sum</cell><cell>pooling</cell><cell>(16×k)</cell><cell>(16×k, 1)</cell><cell>(1)</cell><cell>b</cell></row><row><cell cols="6">(in, out, 3, 3) (out, out, 3, 3) (in, out, 1, 1) average pooling sum average pooling a</cell><cell cols="2">simulation view visual mapping</cell><cell cols="2">parameters parameters parameters</cell><cell cols="2">(l) (n) (m)</cell><cell>(l, 512) (n, 512) (m, 512)</cell><cell>(512) (512) (512)</cell><cell></cell><cell cols="2">(512, 512) (512, 512) (512, 512)</cell><cell>concat</cell><cell>(1536)</cell><cell>(1536, 16×k)</cell><cell cols="2">(1) fully connected residual block input/output (16×k) dot</cell><cell>sum</cell><cell>sigmoid ReLU others 2D convolution real / fake</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Subregion Sensitivity of: BwsA Visualization View Compute Overall Sensitivity Curve Simulation Parameters</head><label></label><figDesc></figDesc><table><row><cell>Parameters View</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BwsA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>3</cell><cell>3.5</cell><cell>3.8</cell></row><row><cell cols="2">Visual Mapping Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Isovalue of temperature:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15</cell><cell>20</cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>View Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>theta</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>60</cell><cell>120</cell><cell>160</cell><cell>240</cell><cell>300</cell><cell>360</cell></row><row><cell>phi</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-90</cell><cell>-54</cell><cell>-30</cell><cell>0</cell><cell>30</cell><cell>60</cell><cell>90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Network t sim (hr) t vis (hr) t tr (hr) t f p (s) t bp (s)</figDesc><table><row><cell cols="8">Simulation Raw Image SmallPoolFire P sim P vis P view Size (GB) k Diversity Name Number Name Number Ck,C 4,000 pseudo-coloring with 5 color schemes N/A N/A 32 2.72 ≈25.0 0.43</cell><cell>0.06</cell><cell cols="2">Performance 1,420.0 6.45 16.40 0.031 0.19</cell></row><row><cell>Nyx</cell><cell cols="2">OmM, OmB, h 500</cell><cell>volume rendering with a transfer function θ , φ</cell><cell>100</cell><cell>48</cell><cell>1.72</cell><cell>≈30.0 3.92</cell><cell>0.12</cell><cell>537.5</cell><cell>8.47 18.02 0.033 0.22</cell></row><row><cell>MPAS-Ocean</cell><cell>BwsA</cell><cell>300</cell><cell>isosurface visualization with 3 isovalues θ , φ</cell><cell>100</cell><cell>48</cell><cell>1.75</cell><cell>≈300.0 3.46</cell><cell>0.15</cell><cell cols="2">229.5 10.73 18.13 0.033 0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Evaluating the weight λ of L adv R : λ = 0.01 provides the results that balance the PSNR, SSIM, EMD, and FID.</figDesc><table><row><cell></cell><cell>λ = 0.005</cell><cell>λ = 0.01</cell><cell>λ = 0.02</cell><cell>λ = 0.04</cell></row><row><cell>PSNR</cell><cell>30.043</cell><cell>29.366</cell><cell>29.040</cell><cell>27.232</cell></row><row><cell>SSIM</cell><cell>0.8619</cell><cell>0.8336</cell><cell>0.8253</cell><cell>0.7680</cell></row><row><cell>EMD</cell><cell>0.0041</cell><cell>0.0022</cell><cell>0.0023</cell><cell>0.0025</cell></row><row><cell>FID</cell><cell>21.267</cell><cell>6.2694</cell><cell>6.6819</cell><cell>9.8992</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Size and training time of different network architectures controlled by k for the Nyx dataset.</figDesc><table><row><cell></cell><cell>k = 16</cell><cell>k = 32</cell><cell>k = 48</cell><cell>k = 64</cell></row><row><cell>Network Size (MB)</cell><cell>26.4</cell><cell>67.4</cell><cell>125.2</cell><cell>199.6</cell></row><row><cell>Training Time (hr)</cell><cell>13.73</cell><cell>16.42</cell><cell>18.02</cell><cell>20.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Evaluation of the number of ensemble runs used for training.</figDesc><table><row><cell>Simulation</cell><cell># Ensemble Runs</cell><cell>PSNR</cell><cell>SSIM</cell><cell>EMD</cell><cell>FID</cell></row><row><cell></cell><cell>900</cell><cell>21.842</cell><cell>0.8714</cell><cell>0.0040</cell><cell>14.398</cell></row><row><cell>SmallPoolFire</cell><cell>1900 2900</cell><cell>23.192 23.932</cell><cell>0.9016 0.9018</cell><cell>0.0036 0.0037</cell><cell>11.732 9.5813</cell></row><row><cell></cell><cell>3900</cell><cell>24.288</cell><cell>0.9006</cell><cell>0.0037</cell><cell>9.4747</cell></row><row><cell></cell><cell>100</cell><cell>28.108</cell><cell>0.7951</cell><cell>0.0025</cell><cell>9.8818</cell></row><row><cell>Nyx</cell><cell>200 300</cell><cell>29.404 29.398</cell><cell>0.8319 0.8326</cell><cell>0.0022 0.0023</cell><cell>6.5481 6.4239</cell></row><row><cell></cell><cell>400</cell><cell>29.366</cell><cell>0.8336</cell><cell>0.0022</cell><cell>6.2694</cell></row><row><cell></cell><cell>70</cell><cell>24.347</cell><cell>0.8554</cell><cell>0.0016</cell><cell>37.229</cell></row><row><cell>MPAS-Ocean</cell><cell>140 210</cell><cell>24.593 24.732</cell><cell>0.8607 0.8643</cell><cell>0.0017 0.0017</cell><cell>28.380 22.794</cell></row><row><cell></cell><cell>270</cell><cell>24.791</cell><cell>0.8655</cell><cell>0.0017</cell><cell>21.395</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sciviscolor.org 2 https://www.paraview.org/in-situ<ref type="bibr" target="#b2">3</ref> https://pytorch.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by US Department of Energy Los Alamos National Laboratory contract 47145 and UT-Battelle LLC contract 4000159447 program manager Laura Biven.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">In situ MPAS-Ocean image-based visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jourdain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Samsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boeckel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, Visualization &amp; Data Analytics Showcase</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis, Visualization &amp; Data Analytics Showcase</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An image-based approach to extreme scale in situ visualization and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jourdain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02937</idno>
		<title level="m">Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nyx: A massively parallel AMR code for computational cosmology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Almgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Lijewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lukić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astrophysical Journal</title>
		<imprint>
			<biblScope unit="volume">765</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geveci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Moreland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Whitlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Bethel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">situ methods, infrastructures, and applications on high performance computing platforms</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="577" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A generative model for volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1636" to="1650" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ParaGlide: Interactive parameter space partitioning for computer simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Abdolyousefi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1499" to="1512" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contour tree depth images for large data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Biedert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics Symposium on Parallel Graphics and Visualization</title>
		<meeting>Eurographics Symposium on Parallel Graphics and Visualization</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visualization of time-varying weather ensembles across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="841" to="850" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual verification of space weather ensemble simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pembroke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Mays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rastaetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ynnerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015 IEEE Scientific Visualization Conference</title>
		<meeting>2015 IEEE Scientific Visualization Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vismon: Facilitating analysis of trade-offs, uncertainty, and sensitivity in fisheries management decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Booshehrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Peterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactive exploration and analysis of large-scale simulations using topology-based data segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tierny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1307" to="1324" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Result-driven exploration of simulation parameter spaces for visual effects design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1468" to="1476" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multidimensional ensemble data visualization and exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1072" to="1086" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design by dragging: An interface for creative forward and inverse design with simulation ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Erdman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Keefe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2783" to="2791" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast error-bounded lossy HPC data compression with SZ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Parallel and Distributed Processing Symposium</title>
		<meeting>International Parallel and Distributed Processing Symposium</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2015 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1538" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">In situ distribution guided analysis and visualization of transonic jet engine simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heinlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="811" to="820" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Space-time volumetric depth images for in-situ visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2014 IEEE Symposium on Large Data Analysis and Visualization</title>
		<meeting>2014 IEEE Symposium on Large Data Analysis and Visualization</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="59" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explorable volumetric depth images from raycasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ertl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2013 XXVI Conference on Graphics, Patterns and Images</title>
		<meeting>2013 XXVI Conference on Graphics, Patterns and Images</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FlowNet: A deep learning framework for clustering and selection of streamlines and stream surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Early Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Access pattern learning with long shortterm memory for parallel particle tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2018 IEEE Pacific Visualization Symposium</title>
		<meeting>2018 IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep feature consistent variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>2017 IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1133" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04720</idno>
		<title level="m">The GAN landscape: Losses, architectures, regularization, and normalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ISABELA for effective in situ compression of scientific data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ethier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">F</forename><surname>Samatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="524" to="540" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2017 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Are GANs created equal? A large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="700" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">In situ visualization at extreme scale: Challenges and opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interactive visual analysis of complex scientific data as families of data surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gračanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1351" to="1358" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual trends analysis in timevarying ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Obermaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bensema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Joy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2331" to="2342" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Drag and track: A direct manipulation interface for contextualizing data instances within a continuous parameter space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Orban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="256" to="266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hypermoval: Interactive visual validation of regression models for real-time simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="983" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visual reconciliation of alternative similarity spaces in climate modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hargrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Schwalm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Huntzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1923" to="1932" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ensemble-vis: A framework for the statistical visualization of ensemble data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doutriaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Data Mining Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A multi-resolution approach to global ocean modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ringler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Higdon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maltrud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ocean Modelling</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="211" to="232" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<editor>D. E. Rumelhart, J. L. McClelland, and CORPORATE PDP Research Group</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visualization of ocean currents and eddies in a high-resolution global ocean-climate model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Samsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Turton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, Visualization &amp; Data Analytics Showcase</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis, Visualization &amp; Data Analytics Showcase</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visual parameter space analysis: A conceptual framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heinzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Piringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2161" to="2170" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A two-dimensional interpolation function for irregularlyspaced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1968 23rd ACM National Conference</title>
		<meeting>the 1968 23rd ACM National Conference</meeting>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interactive visual steering of hierarchical simulation ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Splechtna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gračanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2015 IEEE Conference on Visual Analytics Science and Technology</title>
		<meeting>2015 IEEE Conference on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Explorable images for visualizing volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tikhonova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2010 IEEE Pacific Visualization Symposium</title>
		<meeting>2010 IEEE Pacific Visualization Symposium</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An exploratory technique for coherent visualization of time-varying volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tikhonova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="783" to="792" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visualization by proxy: A novel framework for deferred interaction with volume data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tikhonova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1551" to="1559" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Tuner: Principled parameter finding for image segmentation algorithms using visual response surface exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Torsney-Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Verbavatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1892" to="1901" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Ganviz: A visual analytics approach to understand the adversarial game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visualization and visual analysis of ensemble data: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Early Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-resolution climate ensemble parameter analysis with nested parallel coordinates plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="90" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A tensorial approach to computational continuum mechanics using object-oriented techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fureby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Physics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="620" to="631" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A visual analytics framework for the detection of anomalous call stack trees in high performance computing applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="224" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Nonstationary texture synthesis by adversarial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno>49:1-49:13</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 International Conference on Computer Vision</title>
		<meeting>2017 International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
